[
    {
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9061",
        "id": "9061",
        "author_site": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, Pieter Abbeel",
        "author": "Haoran Tang; Rein Houthooft; Davis Foote; Adam Stooke; OpenAI Xi Chen; Yan Duan; John Schulman; Filip DeTurck; Pieter Abbeel",
        "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
        "bibtex": "@inproceedings{NIPS2017_3a20f62a,\n author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {\\#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Reviews.html",
        "metareview": "",
        "pdf_size": 6911820,
        "gs_citation": 777,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7719009312505331345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "UC Berkeley, Department of Mathematics; UC Berkeley, Department of Electrical Engineering and Computer Sciences + OpenAI; UC Berkeley, Department of Electrical Engineering and Computer Sciences; UC Berkeley, Department of Electrical Engineering and Computer Sciences; UC Berkeley, Department of Electrical Engineering and Computer Sciences + OpenAI; UC Berkeley, Department of Electrical Engineering and Computer Sciences + OpenAI; OpenAI; Ghent University \u2013 imec, Department of Information Technology; UC Berkeley, Department of Electrical Engineering and Computer Sciences + OpenAI",
        "aff_domain": "math.berkeley.edu;openai.com; ; ; ; ; ; ;",
        "email": "math.berkeley.edu;openai.com; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html",
        "aff_unique_index": "0;0+1;0;0;0+1;0+1;1;2;0+1",
        "aff_unique_norm": "University of California, Berkeley;OpenAI;Ghent University \u2013 imec, Department of Information Technology",
        "aff_unique_dep": "Department of Mathematics;;",
        "aff_unique_url": "https://www.berkeley.edu;https://openai.com;",
        "aff_unique_abbr": "UC Berkeley;OpenAI;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0+0;0;0;0+0;0+0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "A Bayesian Data Augmentation Approach for Learning Deep Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9065",
        "id": "9065",
        "author_site": "Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, Ian Reid",
        "author": "Toan Tran; Trung Pham; Gustavo Carneiro; Lyle Palmer; Ian Reid",
        "abstract": "Data augmentation is an essential part of the training process applied to deep learning models.  The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed.  Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm --- generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above --- the results also show that our approach produces better classification results than similar GAN models.",
        "bibtex": "@inproceedings{NIPS2017_076023ed,\n author = {Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Data Augmentation Approach for Learning Deep Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/076023edc9187cf1ac1f1163470e479a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/076023edc9187cf1ac1f1163470e479a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/076023edc9187cf1ac1f1163470e479a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/076023edc9187cf1ac1f1163470e479a-Reviews.html",
        "metareview": "",
        "pdf_size": 2284870,
        "gs_citation": 306,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2205181983581816790&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science; School of Computer Science; School of Computer Science; School of Public Health; School of Computer Science",
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/076023edc9187cf1ac1f1163470e479a-Abstract.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "School of Computer Science;School of Public Health",
        "aff_unique_dep": "Computer Science;Public Health",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "A Decomposition of Forecast Error in Prediction Markets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9216",
        "id": "9216",
        "author_site": "Miro Dudik, Sebastien Lahaie, Ryan Rogers, Jennifer Wortman Vaughan",
        "author": "Miro Dudik; Sebastien Lahaie; Ryan M Rogers; Jennifer Wortman Vaughan",
        "abstract": "We analyze sources of error in prediction market forecasts in order to bound the difference between a security's price and the ground truth it estimates. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into three components: sampling error, arising because traders only possess noisy estimates of ground truth; market-maker bias, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and convergence error, arising because, at any point in time, market prices may still be in flux. Our goal is to make explicit the tradeoffs between these error components, influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We consider a specific model in which traders have exponential utility and exponential-family beliefs representing noisy estimates of ground truth. In this setting, sampling error vanishes as the number of traders grows, but there is a tradeoff between the other two components. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market's liquidity parameter and into the forecasting benefits of enforcing coherent prices across securities.",
        "bibtex": "@inproceedings{NIPS2017_96671501,\n author = {Dudik, Miro and Lahaie, Sebastien and Rogers, Ryan M and Wortman Vaughan, Jennifer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Decomposition of Forecast Error in Prediction Markets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/96671501524948bc3937b4b30d0e57b9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/96671501524948bc3937b4b30d0e57b9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/96671501524948bc3937b4b30d0e57b9-Reviews.html",
        "metareview": "",
        "pdf_size": 405140,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5503962705305857309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research, New York, NY; Google, New York, NY; University of Pennsylvania, Philadelphia, PA; Microsoft Research, New York, NY",
        "aff_domain": "microsoft.com;google.com;gmail.com;microsoft.com",
        "email": "microsoft.com;google.com;gmail.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/96671501524948bc3937b4b30d0e57b9-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Microsoft Research;Google, New York, NY;University of Pennsylvania",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;;https://www.upenn.edu",
        "aff_unique_abbr": "MSR;;UPenn",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "New York;;Philadelphia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8927",
        "id": "8927",
        "author_site": "Hongteng Xu, Hongyuan Zha",
        "author": "Hongteng Xu; Hongyuan Zha",
        "abstract": "How to cluster event sequences generated via different point processes is an interesting and important problem in statistical machine learning.  To solve this problem, we propose and discuss an effective model-based clustering method based on a novel Dirichlet mixture model of a special but significant type of point processes --- Hawkes process.  The proposed model generates the event sequences with different clusters from the Hawkes processes with different parameters, and uses a Dirichlet process as the prior distribution of the clusters.  We prove the identifiability of our mixture model and propose an effective variational Bayesian inference algorithm to learn our model.  An adaptive inner iteration allocation strategy is designed to accelerate the convergence of our algorithm. Moreover, we investigate the sample complexity and the computational complexity  of our learning algorithm in depth.  Experiments on both synthetic and real-world data show that the clustering method based on our model can learn structural triggering patterns hidden in asynchronous event sequences robustly and achieve superior performance on clustering purity and consistency compared to existing methods.",
        "bibtex": "@inproceedings{NIPS2017_dd8eb9f2,\n author = {Xu, Hongteng and Zha, Hongyuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dd8eb9f23fbd362da0e3f4e70b878c16-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dd8eb9f23fbd362da0e3f4e70b878c16-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dd8eb9f23fbd362da0e3f4e70b878c16-Reviews.html",
        "metareview": "",
        "pdf_size": 1502910,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1236799339504478300&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of ECE, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology",
        "aff_domain": "gmail.com;cc.gatech.edu",
        "email": "gmail.com;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9143",
        "id": "9143",
        "author_site": "Marco Fraccaro, Simon Kamronn, Ulrich Paquet, Ole Winther",
        "author": "Marco Fraccaro; Simon Kamronn; Ulrich Paquet; Ole Winther",
        "abstract": "This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to  generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.",
        "bibtex": "@inproceedings{NIPS2017_7b7a53e2,\n author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7b7a53e239400a13bd6be6c91c4f6c4e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7b7a53e239400a13bd6be6c91c4f6c4e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7b7a53e239400a13bd6be6c91c4f6c4e-Reviews.html",
        "metareview": "",
        "pdf_size": 622178,
        "gs_citation": 382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16282446658958347820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html"
    },
    {
        "title": "A General Framework for Robust Interactive Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9474",
        "id": "9474",
        "author_site": "Ehsan Emamjomeh-Zadeh, David Kempe",
        "author": "Ehsan Emamjomeh-Zadeh; David Kempe",
        "abstract": "We propose a general framework for interactively learning models, such as (binary or non-binary) classifiers, orderings/rankings of items, or clusterings of data points. Our framework is based on a generalization of Angluin's equivalence query model and Littlestone's online learning model: in each iteration, the algorithm proposes a model, and the user either accepts it or reveals a specific mistake in the proposal. The feedback is correct only with probability p > 1/2 (and adversarially incorrect with probability 1 - p), i.e., the algorithm must be able to learn in the presence of arbitrary noise. The algorithm's goal is to learn the ground truth model using few iterations.  Our general framework is based on a graph representation of the models and user feedback. To be able to learn efficiently, it is sufficient that there be a graph G whose nodes are the models, and (weighted) edges capture the user feedback, with the property that if s, s* are the proposed and target models, respectively, then any (correct) user feedback s' must lie on a shortest s-s* path in G. Under this one assumption, there is a natural algorithm, reminiscent of the Multiplicative Weights Update algorithm, which will efficiently learn s* even in the presence of noise in the user's feedback.  From this general result, we rederive with barely any extra effort classic results on learning of classifiers and a recent result on interactive clustering; in addition, we easily obtain new interactive learning algorithms for ordering/ranking.",
        "bibtex": "@inproceedings{NIPS2017_3f647cad,\n author = {Emamjomeh-Zadeh, Ehsan and Kempe, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A General Framework for Robust Interactive Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f647cadf56541fb9513cb63ec370187-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3f647cadf56541fb9513cb63ec370187-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3f647cadf56541fb9513cb63ec370187-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3f647cadf56541fb9513cb63ec370187-Reviews.html",
        "metareview": "",
        "pdf_size": 311256,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16547507698077589797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California",
        "aff_domain": "usc.edu;usc.edu",
        "email": "usc.edu;usc.edu",
        "github": "",
        "project": "https://arxiv.org/abs/1710.05422",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3f647cadf56541fb9513cb63ec370187-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Greedy Approach for Budgeted Maximum Inner Product Search",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9320",
        "id": "9320",
        "author_site": "Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, Inderjit Dhillon",
        "author": "Hsiang-Fu Yu; Cho-Jui Hsieh; Qi Lei; Inderjit S Dhillon",
        "abstract": "Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of low-rank matrix factorization models and deep learning models. Recently, there has been substantial research on how to perform MIPS in sub-linear time, but most of the existing work does not have the flexibility to control the trade-off between search efficiency and search quality. In this paper, we study the important problem of MIPS with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.",
        "bibtex": "@inproceedings{NIPS2017_39d352b0,\n author = {Yu, Hsiang-Fu and Hsieh, Cho-Jui and Lei, Qi and Dhillon, Inderjit S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Greedy Approach for Budgeted Maximum Inner Product Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/39d352b0395ba768e18f042c6e2a8621-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/39d352b0395ba768e18f042c6e2a8621-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/39d352b0395ba768e18f042c6e2a8621-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/39d352b0395ba768e18f042c6e2a8621-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/39d352b0395ba768e18f042c6e2a8621-Reviews.html",
        "metareview": "",
        "pdf_size": 1202273,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1374612491564313911&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Amazon Inc. + The University of Texas at Austin; University of California, Davis; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;ucdavis.edu;ices.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;ucdavis.edu;ices.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/39d352b0395ba768e18f042c6e2a8621-Abstract.html",
        "aff_unique_index": "0+1;2;1;1",
        "aff_unique_norm": "Amazon;University of Texas at Austin;University of California, Davis",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.amazon.com;https://www.utexas.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "Amazon;UT Austin;UC Davis",
        "aff_campus_unique_index": "1;2;1;1",
        "aff_campus_unique": ";Austin;Davis",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A KL-LUCB algorithm for Large-Scale Crowdsourcing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9362",
        "id": "9362",
        "author_site": "Ervin Tanczos, Robert Nowak, Bob Mankoff",
        "author": "Ervin Tanczos; Robert Nowak; Bob Mankoff",
        "abstract": "This paper focuses on best-arm identification in multi-armed bandits with bounded rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB, offering the best qualities of the two algorithms in one method. This is achieved by proving a novel anytime confidence bound for the mean of bounded distributions, which is the analogue of the LIL-type bounds recently developed for sub-Gaussian distributions. We corroborate our theoretical results with numerical experiments based on the New Yorker Cartoon Caption Contest.",
        "bibtex": "@inproceedings{NIPS2017_c02f9de3,\n author = {Tanczos, Ervin and Nowak, Robert and Mankoff, Bob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A KL-LUCB algorithm for Large-Scale Crowdsourcing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c02f9de3c2f3040751818aacc7f60b74-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c02f9de3c2f3040751818aacc7f60b74-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c02f9de3c2f3040751818aacc7f60b74-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c02f9de3c2f3040751818aacc7f60b74-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c02f9de3c2f3040751818aacc7f60b74-Reviews.html",
        "metareview": "",
        "pdf_size": 671050,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3176231933362798822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Wisconsin-Madison; University of Wisconsin-Madison; Former Cartoon Editor of the New Yorker",
        "aff_domain": "wisc.edu;wisc.edu;hearst.com",
        "email": "wisc.edu;wisc.edu;hearst.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c02f9de3c2f3040751818aacc7f60b74-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Wisconsin-Madison;Former Cartoon Editor of the New Yorker",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;",
        "aff_unique_abbr": "UW-Madison;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "A Learning Error Analysis for Structured Prediction with Approximate Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9384",
        "id": "9384",
        "author_site": "Yuanbin Wu, Man Lan, Shiliang Sun, Qi Zhang, Xuanjing Huang",
        "author": "Yuanbin Wu; Man Lan; Shiliang Sun; Qi Zhang; Xuanjing Huang",
        "abstract": "In this work, we try to understand the differences between exact and approximate inference algorithms in structured prediction. We compare the estimation and approximation error of both underestimate and overestimate models. The result shows that, from the perspective of learning errors, performances of approximate inference could be as good as exact inference. The error analyses also suggest a new margin for existing learning algorithms. Empirical evaluations on text classification, sequential labelling and dependency parsing witness the success of approximate inference and the benefit of the proposed margin.",
        "bibtex": "@inproceedings{NIPS2017_e9fb2eda,\n author = {Wu, Yuanbin and Lan, Man and Sun, Shiliang and Zhang, Qi and Huang, Xuanjing},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Learning Error Analysis for Structured Prediction with Approximate Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Reviews.html",
        "metareview": "",
        "pdf_size": 766074,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11735498181828815233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science and Software Engineering, East China Normal University + Shanghai Key Laboratory of Multidimensional Information Processing; School of Computer Science and Software Engineering, East China Normal University + Shanghai Key Laboratory of Multidimensional Information Processing; School of Computer Science and Software Engineering, East China Normal University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",
        "aff_domain": "cs.ecnu.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "cs.ecnu.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Abstract.html",
        "aff_unique_index": "0+1;0+1;0;2;2",
        "aff_unique_norm": "School of Computer Science and Software Engineering, East China Normal University;Shanghai Key Laboratory of Multidimensional Information Processing;Fudan University",
        "aff_unique_dep": ";Multidimensional Information Processing;School of Computer Science",
        "aff_unique_url": ";;https://www.fudan.edu.cn",
        "aff_unique_abbr": ";;Fudan",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "title": "A Linear-Time Kernel Goodness-of-Fit Test",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8823",
        "id": "8823",
        "author_site": "Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur Gretton",
        "author": "Wittawat Jitkrittum; Wenkai Xu; Zoltan Szabo; Kenji Fukumizu; Arthur Gretton",
        "abstract": "We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.",
        "bibtex": "@inproceedings{NIPS2017_979d472a,\n author = {Jitkrittum, Wittawat and Xu, Wenkai and Szabo, Zoltan and Fukumizu, Kenji and Gretton, Arthur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Linear-Time Kernel Goodness-of-Fit Test},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Reviews.html",
        "metareview": "",
        "pdf_size": 1017991,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6152826479788805160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "Gatsby Unit, UCL; Gatsby Unit, UCL; CMAP, \u00c9cole Polytechnique; The Institute of Statistical Mathematics; Gatsby Unit, UCL",
        "aff_domain": "gmail.com;gatsby.ucl.ac.uk;polytechnique.edu;ism.ac.jp;gmail.com",
        "email": "gmail.com;gatsby.ucl.ac.uk;polytechnique.edu;ism.ac.jp;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/979d472a84804b9f647bc185a877a8b5-Abstract.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University College London;\u00c9cole Polytechnique;The Institute of Statistical Mathematics",
        "aff_unique_dep": "Gatsby Unit;CMAP;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.ecp.fr;https://www.ism.ac.jp",
        "aff_unique_abbr": "UCL;\u00c9cole Polytechnique;ISM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "United Kingdom;France;Japan"
    },
    {
        "title": "A Meta-Learning Perspective on Cold-Start Recommendations for Items",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9457",
        "id": "9457",
        "author_site": "Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, Hugo Larochelle",
        "author": "Manasi Vartak; Arvind Thiagarajan; Conrado Miranda; Jeshua Bratman; Hugo Larochelle",
        "abstract": "Matrix factorization (MF) is one of the most popular techniques for product recommendation, but is known to suffer from serious cold-start problems. Item cold-start problems are particularly acute in settings such as Tweet recommendation where new items arrive continuously. In this paper, we present a meta-learning strategy to address item cold-start when new items arrive continuously. We propose two deep neural network architectures that implement our meta-learning strategy. The first architecture learns a linear classifier whose weights are determined by the item history while the second architecture learns a neural network whose biases are instead adjusted. We evaluate our techniques on the real-world problem of Tweet recommendation. On production data at Twitter, we demonstrate that our proposed techniques significantly beat the MF baseline and also outperform production models for Tweet recommendation.",
        "bibtex": "@inproceedings{NIPS2017_51e6d6e6,\n author = {Vartak, Manasi and Thiagarajan, Arvind and Miranda, Conrado and Bratman, Jeshua and Larochelle, Hugo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Meta-Learning Perspective on Cold-Start Recommendations for Items},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/51e6d6e679953c6311757004d8cbbba9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/51e6d6e679953c6311757004d8cbbba9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/51e6d6e679953c6311757004d8cbbba9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/51e6d6e679953c6311757004d8cbbba9-Reviews.html",
        "metareview": "",
        "pdf_size": 539089,
        "gs_citation": 287,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2559890688802765463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology + Twitter Inc.; Twitter Inc.; Twitter Inc.; Twitter Inc.; Google Brain + Twitter Inc.",
        "aff_domain": "csail.mit.edu;twitter.com;twitter.com;twitter.com;google.com",
        "email": "csail.mit.edu;twitter.com;twitter.com;twitter.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/51e6d6e679953c6311757004d8cbbba9-Abstract.html",
        "aff_unique_index": "0+1;1;1;1;2+1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Twitter Inc.;Google",
        "aff_unique_dep": ";;Google Brain",
        "aff_unique_url": "https://web.mit.edu;;https://brain.google.com",
        "aff_unique_abbr": "MIT;;Google Brain",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "A Minimax Optimal Algorithm for Crowdsourcing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9214",
        "id": "9214",
        "author_site": "Thomas Bonald, Richard Combes",
        "author": "Thomas Bonald; Richard Combes",
        "abstract": "We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data.",
        "bibtex": "@inproceedings{NIPS2017_743394be,\n author = {Bonald, Thomas and Combes, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Minimax Optimal Algorithm for Crowdsourcing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/743394beff4b1282ba735e5e3723ed74-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/743394beff4b1282ba735e5e3723ed74-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/743394beff4b1282ba735e5e3723ed74-Reviews.html",
        "metareview": "",
        "pdf_size": 170407,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9689750677068229322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Telecom ParisTech; Centrale-Supelec / L2S",
        "aff_domain": "telecom-paristech.fr;supelec.fr",
        "email": "telecom-paristech.fr;supelec.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/743394beff4b1282ba735e5e3723ed74-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Telecom ParisTech;Centrale-Supelec / L2S",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "A New Alternating Direction Method for Linear Programming",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8939",
        "id": "8939",
        "author_site": "Sinong Wang, Ness Shroff",
        "author": "Sinong Wang; Ness Shroff",
        "abstract": "It is well known that, for a linear program (LP) with constraint matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, the Alternating Direction Method of Multiplier converges globally and linearly at a rate $O((\\|\\mathbf{A}\\|_F^2+mn)\\log(1/\\epsilon))$. However, such a rate is related to the problem dimension and the algorithm exhibits a slow and fluctuating ``tail convergence'' in practice. In this paper, we propose a new variable splitting method of LP and prove that our method has a convergence rate of $O(\\|\\mathbf{A}\\|^2\\log(1/\\epsilon))$. The proof is based on simultaneously estimating the distance from a pair of primal dual iterates to the optimal primal and dual solution set by certain residuals. In practice, we result in a new first-order LP solver that can exploit both the sparsity and the specific structure of matrix $\\mathbf{A}$ and a  significant speedup for important problems such as basis pursuit, inverse covariance matrix estimation, L1 SVM and nonnegative matrix factorization problem compared with current fastest LP solvers.",
        "bibtex": "@inproceedings{NIPS2017_c4b31ce7,\n author = {Wang, Sinong and Shroff, Ness},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A New Alternating Direction Method for Linear Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c4b31ce7d95c75ca70d50c19aef08bf1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c4b31ce7d95c75ca70d50c19aef08bf1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c4b31ce7d95c75ca70d50c19aef08bf1-Reviews.html",
        "metareview": "",
        "pdf_size": 615289,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13369139709221647361&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of ECE, The Ohio State University; Department of ECE and CSE, The Ohio State University",
        "aff_domain": "osu.edu;osu.edu",
        "email": "osu.edu;osu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c4b31ce7d95c75ca70d50c19aef08bf1-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of ECE, The Ohio State University;Department of ECE and CSE, The Ohio State University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "A New Theory for Matrix Completion",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8873",
        "id": "8873",
        "author_site": "Guangcan Liu, Qingshan Liu, Xiaotong Yuan",
        "author": "Guangcan Liu; Qingshan Liu; Xiaotong Yuan",
        "abstract": "Prevalent matrix completion theories reply on an assumption that the locations of the missing data are distributed uniformly and randomly (i.e., uniform sampling). Nevertheless, the reason for observations being missing often depends on the unseen observations themselves, and thus the missing data in practice usually occurs in a nonuniform and deterministic fashion rather than randomly. To break through the limits of random sampling, this paper introduces a new hypothesis called \\emph{isomeric condition}, which is provably weaker than the assumption of uniform sampling and arguably holds even when the missing data is placed irregularly. Equipped with this new tool, we prove a series of theorems for missing data recovery and matrix completion. In particular, we prove that the exact solutions that identify the target matrix are included as critical points by the commonly used nonconvex programs. Unlike the existing theories for nonconvex matrix completion, which are built upon the same condition as convex programs, our theory shows that nonconvex programs have the potential to work with a much weaker condition. Comparing to the existing studies on nonuniform sampling, our setup is more general.",
        "bibtex": "@inproceedings{NIPS2017_07563a3f,\n author = {Liu, Guangcan and Liu, Qingshan and Yuan, Xiaotong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A New Theory for Matrix Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/07563a3fe3bbe7e3ba84431ad9d055af-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/07563a3fe3bbe7e3ba84431ad9d055af-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/07563a3fe3bbe7e3ba84431ad9d055af-Reviews.html",
        "metareview": "",
        "pdf_size": 343932,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9506224486991155271&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html"
    },
    {
        "title": "A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9078",
        "id": "9078",
        "author": "Ben London",
        "abstract": "We study the generalization error of randomized learning algorithms -- focusing on stochastic gradient descent (SGD) -- using a novel combination of PAC-Bayes and algorithmic stability. Importantly, our generalization bounds hold for all posterior distributions on an algorithm's random hyperparameters, including distributions that depend on the training data. This inspires an adaptive sampling algorithm for SGD that optimizes the posterior at runtime. We analyze this algorithm in the context of our generalization bounds and evaluate it on a benchmark dataset. Our experiments demonstrate that adaptive sampling can reduce empirical risk faster than uniform sampling while also improving out-of-sample accuracy.",
        "bibtex": "@inproceedings{NIPS2017_7fea637f,\n author = {London, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7fea637fd6d02b8f0adf6f7dc36aed93-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7fea637fd6d02b8f0adf6f7dc36aed93-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7fea637fd6d02b8f0adf6f7dc36aed93-Reviews.html",
        "metareview": "",
        "pdf_size": 606656,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6751449622289920834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Amazon AI",
        "aff_domain": "amazon.com",
        "email": "amazon.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9226",
        "id": "9226",
        "author_site": "Qinliang Su, xuejun Liao, Lawrence Carin",
        "author": "Qinliang Su; xuejun Liao; Lawrence Carin",
        "abstract": "We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).",
        "bibtex": "@inproceedings{NIPS2017_35936504,\n author = {Su, Qinliang and Liao, xuejun and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/35936504a37d53e03abdfbc7318d9ec7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/35936504a37d53e03abdfbc7318d9ec7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/35936504a37d53e03abdfbc7318d9ec7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/35936504a37d53e03abdfbc7318d9ec7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/35936504a37d53e03abdfbc7318d9ec7-Reviews.html",
        "metareview": "",
        "pdf_size": 495937,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5035269297932430510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA",
        "aff_domain": "duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/35936504a37d53e03abdfbc7318d9ec7-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Durham",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Regularized Framework for Sparse and Structured Neural Attention",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9118",
        "id": "9118",
        "author_site": "Vlad Niculae, Mathieu Blondel",
        "author": "Vlad Niculae; Mathieu Blondel",
        "abstract": "Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input.  We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input.  We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation.  To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization.  Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.",
        "bibtex": "@inproceedings{NIPS2017_2d1b2a5f,\n author = {Niculae, Vlad and Blondel, Mathieu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Regularized Framework for Sparse and Structured Neural Attention},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2d1b2a5ff364606ff041650887723470-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2d1b2a5ff364606ff041650887723470-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2d1b2a5ff364606ff041650887723470-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2d1b2a5ff364606ff041650887723470-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2d1b2a5ff364606ff041650887723470-Reviews.html",
        "metareview": "",
        "pdf_size": 362442,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9476729815953387305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Cornell University; NTT Communication Science Laboratories",
        "aff_domain": "cs.cornell.edu;mblondel.org",
        "email": "cs.cornell.edu;mblondel.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2d1b2a5ff364606ff041650887723470-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;NTT Communication Science Laboratories",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cornell.edu;https://www.ntt-csl.com",
        "aff_unique_abbr": "Cornell;NTT CSL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "A Sample Complexity Measure with Applications to Learning Optimal Auctions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9310",
        "id": "9310",
        "author": "Vasilis Syrgkanis",
        "abstract": "We introduce a new sample complexity measure, which we refer to as split-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size $m$, the split-sample growth rate $\\hat{\\tau}_H(m)$ counts how many different hypotheses can empirical risk minimization output on any sub-sample of $S$ of size $m/2$. We show that the expected generalization error is upper bounded by $O\\left(\\sqrt{\\frac{\\log(\\hat{\\tau}_H(2m))}{m}}\\right)$. Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample.",
        "bibtex": "@inproceedings{NIPS2017_a7a3d70c,\n author = {Syrgkanis, Vasilis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Sample Complexity Measure with Applications to Learning Optimal Auctions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a7a3d70c6d17a73140918996d03c014f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a7a3d70c6d17a73140918996d03c014f-Reviews.html",
        "metareview": "",
        "pdf_size": 254767,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6990758705159473180&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Microsoft Research",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8949",
        "id": "8949",
        "author": "Tor Lattimore",
        "abstract": "Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015a] and of uniform distributions with unknown support [Cowan and Katehakis, 2015b]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.",
        "bibtex": "@inproceedings{NIPS2017_fed33392,\n author = {Lattimore, Tor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fed33392d3a48aa149a87a38b875ba4a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fed33392d3a48aa149a87a38b875ba4a-Reviews.html",
        "metareview": "",
        "pdf_size": 361454,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18340958594262033863&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "DeepMind, London",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fed33392d3a48aa149a87a38b875ba4a-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "A Screening Rule for l1-Regularized Ising Model Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8867",
        "id": "8867",
        "author_site": "Zhaobin Kuang, Sinong Geng, David Page",
        "author": "Zhaobin Kuang; Sinong Geng; David Page",
        "abstract": "We discover a screening rule for l1-regularized Ising model estimation. The simple closed-form screening rule is a necessary and sufficient condition for exactly recovering the blockwise structure of a solution under any given regularization parameters. With enough sparsity, the screening rule can be combined with various optimization procedures to deliver solutions efficiently in practice. The screening rule is especially suitable for large-scale exploratory data analysis, where the number of variables in the dataset can be thousands while we are only interested in the relationship among a handful of variables within moderate-size clusters for interpretability. Experimental results on various datasets demonstrate the efficiency and insights gained from the introduction of the screening rule.",
        "bibtex": "@inproceedings{NIPS2017_74071a67,\n author = {Kuang, Zhaobin and Geng, Sinong and Page, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Screening Rule for l1-Regularized Ising Model Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/74071a673307ca7459bcf75fbd024e09-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/74071a673307ca7459bcf75fbd024e09-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/74071a673307ca7459bcf75fbd024e09-Reviews.html",
        "metareview": "",
        "pdf_size": 403960,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2307079250572223405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Wisconsin; University of Wisconsin; University of Wisconsin",
        "aff_domain": "wisc.edu;wisc.edu;biostat.wisc.edu",
        "email": "wisc.edu;wisc.edu;biostat.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/74071a673307ca7459bcf75fbd024e09-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Wisconsin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9455",
        "id": "9455",
        "author_site": "Kevin Lin, James Sharpnack, Alessandro Rinaldo, Ryan Tibshirani",
        "author": "Kevin Lin; James L Sharpnack; Alessandro Rinaldo; Ryan J Tibshirani",
        "abstract": "In the 1-dimensional multiple changepoint detection problem, we derive a new fast error rate for the fused lasso estimator, under the assumption that the mean vector has a sparse number of changepoints. This rate is seen to be suboptimal (compared to the minimax rate) by only a factor of $\\log\\log{n}$. Our proof technique is centered around a novel construction that we call a lower interpolant. We extend our results to misspecified models and exponential family distributions. We also describe the implications of our error analysis for the approximate screening of changepoints.",
        "bibtex": "@inproceedings{NIPS2017_5abdf8b8,\n author = {Lin, Kevin and Sharpnack, James L and Rinaldo, Alessandro and Tibshirani, Ryan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5abdf8b8520b71f3a528c7547ee92428-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5abdf8b8520b71f3a528c7547ee92428-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5abdf8b8520b71f3a528c7547ee92428-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5abdf8b8520b71f3a528c7547ee92428-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5abdf8b8520b71f3a528c7547ee92428-Reviews.html",
        "metareview": "",
        "pdf_size": 375435,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17413894087936079768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University; University of California, Davis; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;ucdavis.edu;stat.cmu.edu;stat.cmu.edu",
        "email": "andrew.cmu.edu;ucdavis.edu;stat.cmu.edu;stat.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5abdf8b8520b71f3a528c7547ee92428-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;University of California, Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "CMU;UC Davis",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Unified Approach to Interpreting Model Predictions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9253",
        "id": "9253",
        "author_site": "Scott M Lundberg, Su-In Lee",
        "author": "Scott M Lundberg; Su-In Lee",
        "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
        "bibtex": "@inproceedings{NIPS2017_8a20a862,\n author = {Lundberg, Scott M and Lee, Su-In},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Unified Approach to Interpreting Model Predictions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Reviews.html",
        "metareview": "",
        "pdf_size": 927937,
        "gs_citation": 36005,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6828961408019591083&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 23,
        "aff": "Paul G. Allen School of Computer Science, University of Washington, Seattle, WA 98105; Paul G. Allen School of Computer Science, Department of Genome Sciences, University of Washington, Seattle, WA 98105",
        "aff_domain": "cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu",
        "github": "https://github.com/slundberg/shap",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Paul G. Allen School of Computer Science, University of Washington, Seattle, WA 98105;Paul G. Allen School of Computer Science, Department of Genome Sciences, University of Washington, Seattle, WA 98105",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9199",
        "id": "9199",
        "author_site": "Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel",
        "author": "Marc Lanctot; Vinicius Zambaldi; Audrunas Gruslys; Angeliki Lazaridou; Karl Tuyls; Julien Perolat; David Silver; Thore Graepel",
        "abstract": "There has been a resurgence of interest in multiagent reinforcement learning (MARL), due partly to the recent success of deep neural networks. The simplest form of MARL is independent reinforcement learning (InRL), where each agent treats all of its experience as part of its (non stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe a meta-algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game theoretic analysis to compute meta-strategies for policy selection. The meta-algorithm generalizes previous algorithms such as InRL, iterated best response, double oracle, and fictitious play. Then, we propose a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in three partially observable settings: gridworld coordination problems, emergent language games, and poker.",
        "bibtex": "@inproceedings{NIPS2017_3323fe11,\n author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Reviews.html",
        "metareview": "",
        "pdf_size": 512655,
        "gs_citation": 870,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11610542900558381527&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "A Universal Analysis of Large-Scale Regularized Least Squares Solutions",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9122",
        "id": "9122",
        "author_site": "Ashkan Panahi, Babak Hassibi",
        "author": "Ashkan Panahi; Babak Hassibi",
        "abstract": "A problem that has been of recent interest in statistical inference, machine learning and signal processing is that of understanding the asymptotic behavior of regularized least squares solutions under random measurement matrices (or dictionaries). The Least Absolute Shrinkage and Selection Operator (LASSO or least-squares with $\\ell_1$ regularization) is perhaps one of the most interesting examples. Precise expressions for the asymptotic performance of LASSO have been obtained for a number of different cases, in particular when the elements of the dictionary matrix are sampled independently from a Gaussian distribution. It has also been empirically observed that the resulting expressions remain valid when the entries of the dictionary matrix are independently sampled from certain non-Gaussian distributions. In this paper, we confirm these observations theoretically when the distribution is sub-Gaussian. We further generalize the previous expressions for a broader family of regularization functions and under milder conditions on the underlying random, possibly non-Gaussian, dictionary matrix. In particular, we establish the universality of the asymptotic statistics (e.g., the average quadratic risk) of LASSO with non-Gaussian dictionaries.",
        "bibtex": "@inproceedings{NIPS2017_136f9513,\n author = {Panahi, Ashkan and Hassibi, Babak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Universal Analysis of Large-Scale Regularized Least Squares Solutions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/136f951362dab62e64eb8e841183c2a9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/136f951362dab62e64eb8e841183c2a9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/136f951362dab62e64eb8e841183c2a9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/136f951362dab62e64eb8e841183c2a9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/136f951362dab62e64eb8e841183c2a9-Reviews.html",
        "metareview": "",
        "pdf_size": 413543,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9904168788813891374&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering, North Carolina State University; Department of Electrical Engineering, California Institute of Technology",
        "aff_domain": "ncsu.edu;caltech.edu",
        "email": "ncsu.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/136f951362dab62e64eb8e841183c2a9-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "North Carolina State University;California Institute of Technology",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Electrical Engineering",
        "aff_unique_url": "https://www.ncsu.edu;https://www.caltech.edu",
        "aff_unique_abbr": "NCSU;Caltech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9368",
        "id": "9368",
        "author_site": "Fanny Yang, Aaditya Ramdas, Kevin Jamieson, Martin Wainwright",
        "author": "Fanny Yang; Aaditya Ramdas; Kevin G. Jamieson; Martin J. Wainwright",
        "abstract": "We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, which can be continuously monitored by the data scientist. When interleaving the MAB tests with an online false discovery rate (FDR) algorithm, we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.",
        "bibtex": "@inproceedings{NIPS2017_f0204e1d,\n author = {Yang, Fanny and Ramdas, Aaditya and Jamieson, Kevin G and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f0204e1d3ee3e4b05de4e2ddbd39e076-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f0204e1d3ee3e4b05de4e2ddbd39e076-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f0204e1d3ee3e4b05de4e2ddbd39e076-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f0204e1d3ee3e4b05de4e2ddbd39e076-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f0204e1d3ee3e4b05de4e2ddbd39e076-Reviews.html",
        "metareview": "",
        "pdf_size": 738279,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16880375412571841417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f0204e1d3ee3e4b05de4e2ddbd39e076-Abstract.html"
    },
    {
        "title": "A graph-theoretic approach to multitasking",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8998",
        "id": "8998",
        "author_site": "Noga Alon, Daniel Reichman, Igor Shinkar, Tal Wagner, Sebastian Musslick, Jonathan D Cohen, Tom Griffiths, Biswadip dey, Kayhan Ozcimder",
        "author": "Noga Alon; Daniel Reichman; Igor Shinkar; Tal Wagner; Sebastian Musslick; Jonathan D. Cohen; Tom Griffiths; Biswadip dey; Kayhan Ozcimder",
        "abstract": "A key feature of neural network architectures is their ability to support the simultaneous interaction among large numbers of units in the learning and processing of representations. However, how the richness of such interactions trades off against the ability of a network to simultaneously carry out multiple independent processes -- a salient limitation in many domains of human cognition -- remains largely unexplored. In this paper we use a graph-theoretic analysis of network architecture to address this question, where tasks are represented as edges in a bipartite graph $G=(A \\cup B, E)$. We define a new measure of multitasking capacity of such networks, based on the assumptions that tasks that \\emph{need} to be multitasked rely on independent resources, i.e., form a matching, and that tasks \\emph{can} be performed without interference if they form an induced matching. Our main result is an inherent tradeoff between the multitasking capacity and the average degree of the network that holds \\emph{regardless of the network architecture}. These results are also extended to networks of depth greater than $2$. On the positive side, we demonstrate that networks that are random-like (e.g., locally sparse) can have desirable multitasking properties. Our results shed light into the parallel-processing limitations of neural systems and provide insights that may be useful for the analysis and design of parallel architectures.",
        "bibtex": "@inproceedings{NIPS2017_c850371f,\n author = {Alon, Noga and Reichman, Daniel and Shinkar, Igor and Wagner, Tal and Musslick, Sebastian and Cohen, Jonathan D and Griffiths, Tom and dey, Biswadip and Ozcimder, Kayhan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A graph-theoretic approach to multitasking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c850371fda6892fbfd1c5a5b457e5777-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c850371fda6892fbfd1c5a5b457e5777-Reviews.html",
        "metareview": "",
        "pdf_size": 275596,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1079650883167206014&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Tel-Aviv University; UC Berkeley; UC Berkeley; MIT; Princeton University; Princeton University; UC Berkeley; Princeton University; Princeton University",
        "aff_domain": "; ; ; ; ; ; ; ;",
        "email": "; ; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html",
        "aff_unique_index": "0;1;1;2;3;3;1;3;3",
        "aff_unique_norm": "Tel Aviv University;University of California, Berkeley;Massachusetts Institute of Technology;Princeton University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.tau.ac.il;https://www.berkeley.edu;https://web.mit.edu;https://www.princeton.edu",
        "aff_unique_abbr": "TAU;UC Berkeley;MIT;Princeton",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;1;1;1;1;1;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "A multi-agent reinforcement learning model of common-pool resource appropriation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9147",
        "id": "9147",
        "author_site": "Julien P\u00e9rolat, Joel Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel",
        "author": "Julien P\u00e9rolat; Joel Z. Leibo; Vinicius Zambaldi; Charles Beattie; Karl Tuyls; Thore Graepel",
        "abstract": "Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource  appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.",
        "bibtex": "@inproceedings{NIPS2017_2b0f658c,\n author = {P\\'{e}rolat, Julien and Leibo, Joel Z and Zambaldi, Vinicius and Beattie, Charles and Tuyls, Karl and Graepel, Thore},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A multi-agent reinforcement learning model of common-pool resource appropriation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2b0f658cbffd284984fb11d90254081f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2b0f658cbffd284984fb11d90254081f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2b0f658cbffd284984fb11d90254081f-Reviews.html",
        "metareview": "",
        "pdf_size": 883240,
        "gs_citation": 254,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16112079505806296409&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; University of Liverpool + DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2b0f658cbffd284984fb11d90254081f-Abstract.html",
        "aff_unique_index": "0;0;0;0;1+0;0",
        "aff_unique_norm": "DeepMind;University of Liverpool",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://deepmind.com;https://www.liverpool.ac.uk",
        "aff_unique_abbr": "DeepMind;Liv Uni",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "A simple model of recognition and recall memory",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8826",
        "id": "8826",
        "author_site": "Nisheeth Srivastava, Edward Vul",
        "author": "Nisheeth Srivastava; Edward Vul",
        "abstract": "We show that several striking differences in memory performance between recognition and recall tasks are explained by an ecological bias endemic in classic memory experiments - that such experiments universally involve more stimuli than retrieval cues. We show that while it is sensible to think of recall as simply retrieving items when probed with a cue - typically the item list itself -  it is better to think of recognition as retrieving cues when probed with items. To test this theory, by manipulating the number of items and cues in a memory experiment, we show a crossover effect in memory performance within subjects such that recognition performance is superior to recall performance when the number of items is greater than the number of cues and recall performance is better than recognition when the converse holds. We build a simple computational model around this theory, using sampling to approximate an ideal Bayesian observer encoding and retrieving situational co-occurrence frequencies of stimuli and retrieval cues. This model robustly reproduces a number of dissociations in recognition and recall previously used to argue for dual-process accounts of declarative memory.",
        "bibtex": "@inproceedings{NIPS2017_57aeee35,\n author = {Srivastava, Nisheeth and Vul, Edward},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A simple model of recognition and recall memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/57aeee35c98205091e18d1140e9f38cf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/57aeee35c98205091e18d1140e9f38cf-Reviews.html",
        "metareview": "",
        "pdf_size": 267726,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15164169147870509400&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science, IIT Kanpur; Dept of Psychology, UCSD",
        "aff_domain": "cse.iitk.ac.in;ucsd.edu",
        "email": "cse.iitk.ac.in;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/57aeee35c98205091e18d1140e9f38cf-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Computer Science, IIT Kanpur;Dept of Psychology, UCSD",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "A simple neural network module for relational reasoning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9273",
        "id": "9273",
        "author_site": "Adam Santoro, David Raposo, David Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap",
        "author": "Adam Santoro; David Raposo; David G Barrett; Mateusz Malinowski; Razvan Pascanu; Peter Battaglia; Timothy Lillicrap",
        "abstract": "Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamical physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.",
        "bibtex": "@inproceedings{NIPS2017_e6acf4b0,\n author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A simple neural network module for relational reasoning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Reviews.html",
        "metareview": "",
        "pdf_size": 975110,
        "gs_citation": 2023,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7624683168776555686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6acf4b0f69f6f6e60e9a815938aa1ff-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "A unified approach to interpreting model predictions",
        "author": "Scott M Lundberg, Su-In Lee",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/oral/10008",
        "id": "10008"
    },
    {
        "title": "A-NICE-MC: Adversarial Training for MCMC",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9290",
        "id": "9290",
        "author_site": "Jiaming Song, Shengjia Zhao, Stefano Ermon",
        "author": "Jiaming Song; Shengjia Zhao; Stefano Ermon",
        "abstract": "Existing Markov Chain Monte Carlo (MCMC)  methods are either based on general-purpose and domain-agnostic schemes, which can lead to slow convergence, or require hand-crafting of problem-specific proposals by an expert. We propose A-NICE-MC, a novel method to train flexible parametric Markov chain kernels to produce samples with desired properties.   First, we propose an efficient likelihood-free adversarial training method to train a Markov chain and mimic a given data distribution. Then, we leverage flexible volume preserving flows to obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to train efficient Markov Chains to sample from a prescribed posterior distribution by iteratively improving the quality of both the model and the samples. A-NICE-MC provides the first framework to automatically design efficient domain-specific MCMC proposals. Empirical results demonstrate that A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of deep neural networks, and is able to significantly outperform competing methods such as Hamiltonian Monte Carlo.",
        "bibtex": "@inproceedings{NIPS2017_2417dc8a,\n author = {Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A-NICE-MC: Adversarial Training for MCMC},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2417dc8af8570f274e6775d4d60496da-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2417dc8af8570f274e6775d4d60496da-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2417dc8af8570f274e6775d4d60496da-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2417dc8af8570f274e6775d4d60496da-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2417dc8af8570f274e6775d4d60496da-Reviews.html",
        "metareview": "",
        "pdf_size": 2008947,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=178306415831646039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2417dc8af8570f274e6775d4d60496da-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8919",
        "id": "8919",
        "author_site": "Yi Xu, Mingrui Liu, Qihang Lin, Tianbao Yang",
        "author": "Yi Xu; Mingrui Liu; Qihang Lin; Tianbao Yang",
        "abstract": "Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous  problems in machine learning, statistics and signal processing. However, it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization, they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper, we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function, which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side, given the local sharpness characterized by an exponent $\\theta\\in(0, 1]$,  we show that the proposed ADMM enjoys an improved iteration complexity of $\\widetilde O(1/\\epsilon^{1-\\theta})$\\footnote{$\\widetilde O()$ suppresses a logarithmic factor.} in the deterministic setting and an iteration complexity of $\\widetilde O(1/\\epsilon^{2(1-\\theta)})$ in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side, we demonstrate that the proposed algorithms converge comparably to, if not much faster than, ADMM with a fine-tuned fixed penalty parameter.",
        "bibtex": "@inproceedings{NIPS2017_e97ee205,\n author = {Xu, Yi and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e97ee2054defb209c35fe4dc94599061-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e97ee2054defb209c35fe4dc94599061-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e97ee2054defb209c35fe4dc94599061-Reviews.html",
        "metareview": "",
        "pdf_size": 484054,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11383990615466048317&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA; Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA; Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA; Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA",
        "aff_domain": "uiowa.edu;uiowa.edu;uiowa.edu;uiowa.edu",
        "email": "uiowa.edu;uiowa.edu;uiowa.edu;uiowa.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e97ee2054defb209c35fe4dc94599061-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "The University of Iowa;Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uiowa.edu;",
        "aff_unique_abbr": "UIowa;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Iowa City;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9085",
        "id": "9085",
        "author_site": "Marco Cusumano-Towner, Vikash Mansinghka",
        "author": "Marco Cusumano-Towner; Vikash K Mansinghka",
        "abstract": "Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.",
        "bibtex": "@inproceedings{NIPS2017_acab0116,\n author = {Cusumano-Towner, Marco and Mansinghka, Vikash K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/acab0116c354964a558e65bdd07ff047-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/acab0116c354964a558e65bdd07ff047-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/acab0116c354964a558e65bdd07ff047-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/acab0116c354964a558e65bdd07ff047-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/acab0116c354964a558e65bdd07ff047-Reviews.html",
        "metareview": "",
        "pdf_size": 530144,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9768262080996407409&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Probabilistic Computing Project, Massachusetts Institute of Technology; Probabilistic Computing Project, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/acab0116c354964a558e65bdd07ff047-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Probabilistic Computing Project",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9324",
        "id": "9324",
        "author_site": "Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, Lawrence Carin",
        "author": "Chunyuan Li; Hao Liu; Changyou Chen; Yuchen Pu; Liqun Chen; Ricardo Henao; Lawrence Carin",
        "abstract": "We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.",
        "bibtex": "@inproceedings{NIPS2017_ade55409,\n author = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yuchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ade55409d1224074754035a5a937d2e0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ade55409d1224074754035a5a937d2e0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ade55409d1224074754035a5a937d2e0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ade55409d1224074754035a5a937d2e0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ade55409d1224074754035a5a937d2e0-Reviews.html",
        "metareview": "",
        "pdf_size": 2714099,
        "gs_citation": 298,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17478503687355660614&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Duke University; Nanjing University; University at Buffalo; Duke University; Duke University; Duke University; Duke University",
        "aff_domain": "duke.edu; ; ; ; ; ; ",
        "email": "duke.edu; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ade55409d1224074754035a5a937d2e0-Abstract.html",
        "aff_unique_index": "0;1;2;0;0;0;0",
        "aff_unique_norm": "Duke University;Nanjing University;University at Buffalo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.duke.edu;https://www.nju.edu.cn;https://www.buffalo.edu",
        "aff_unique_abbr": "Duke;Nanjing U;UB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9263",
        "id": "9263",
        "author_site": "Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, Licheng Jiao",
        "author": "Yuanyuan Liu; Fanhua Shang; James Cheng; Hong Cheng; Licheng Jiao",
        "abstract": "In this paper, we propose an accelerated first-order method for geodesically convex optimization, which is the generalization of the standard Nesterov's accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular, we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems, which show that our method improves the convergence rate from O((1-\\mu/L)^{k}) to O((1-\\sqrt{\\mu/L})^{k}). Moreover, our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k^{2}). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments.",
        "bibtex": "@inproceedings{NIPS2017_6ef80bb2,\n author = {Liu, Yuanyuan and Shang, Fanhua and Cheng, James and Cheng, Hong and Jiao, Licheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6ef80bb237adf4b6f77d0700e1255907-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6ef80bb237adf4b6f77d0700e1255907-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6ef80bb237adf4b6f77d0700e1255907-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6ef80bb237adf4b6f77d0700e1255907-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6ef80bb237adf4b6f77d0700e1255907-Reviews.html",
        "metareview": "",
        "pdf_size": 771538,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=678308796110062291&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science and Engineering, The Chinese University of Hong Kong; Dept. of Computer Science and Engineering, The Chinese University of Hong Kong; Dept. of Computer Science and Engineering, The Chinese University of Hong Kong; Dept. of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, China",
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk;mail.xidian.edu.cn",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk;mail.xidian.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6ef80bb237adf4b6f77d0700e1255907-Abstract.html",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Dept. of Computer Science and Engineering, The Chinese University of Hong Kong;Dept. of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong;Xidian University",
        "aff_unique_dep": ";;School of Artificial Intelligence",
        "aff_unique_url": ";;http://www.xidian.edu.cn/",
        "aff_unique_abbr": ";;Xidian",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";China"
    },
    {
        "title": "Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9260",
        "id": "9260",
        "author_site": "Chaobing Song, Shaobo Cui, Yong Jiang, Shu-Tao Xia",
        "author": "Chaobing Song; Shaobo Cui; Yong Jiang; Shu-Tao Xia",
        "abstract": "In this paper we study the well-known greedy coordinate descent (GCD)  algorithm to solve $\\ell_1$-regularized problems and improve GCD by the two popular strategies: Nesterov's acceleration and stochastic optimization. Firstly, we propose a new rule for greedy selection based on an $\\ell_1$-norm square approximation  which is nontrivial to solve but convex; then an efficient algorithm called ``SOft ThreshOlding PrOjection (SOTOPO)'' is proposed to exactly solve the $\\ell_1$-regularized $\\ell_1$-norm square approximation problem, which is induced by the new rule.  Based on the new rule and the SOTOPO algorithm, the Nesterov's acceleration and stochastic optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD) has the optimal convergence rate $O(\\sqrt{1/\\epsilon})$;  meanwhile, it reduces the iteration complexity of greedy selection up to a factor of sample size. Both theoretically and empirically, we show that ASGCD has better performance for high-dimensional and dense problems with sparse solution.",
        "bibtex": "@inproceedings{NIPS2017_84b20b1f,\n author = {Song, Chaobing and Cui, Shaobo and Jiang, Yong and Xia, Shu-Tao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/84b20b1f5a0d103f5710bb67a043cd78-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/84b20b1f5a0d103f5710bb67a043cd78-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/84b20b1f5a0d103f5710bb67a043cd78-Reviews.html",
        "metareview": "",
        "pdf_size": 1191669,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7563009902613224841&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;sz.tsinghua.edu.cn;sz.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;sz.tsinghua.edu.cn;sz.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/84b20b1f5a0d103f5710bb67a043cd78-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Accelerated consensus via Min-Sum Splitting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8929",
        "id": "8929",
        "author_site": "Patrick Rebeschini, Sekhar C Tatikonda",
        "author": "Patrick Rebeschini; Sekhar C Tatikonda",
        "abstract": "We apply the Min-Sum message-passing protocol to solve the consensus problem in distributed optimization. We show that while the ordinary Min-Sum algorithm does not converge, a modified version of it known as Splitting yields convergence to the problem solution. We prove that a proper choice of the tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated convergence rates, matching the rates obtained by shift-register methods. The acceleration scheme embodied by Min-Sum Splitting for the consensus problem bears similarities with lifted Markov chains techniques and with multi-step first order methods in convex optimization.",
        "bibtex": "@inproceedings{NIPS2017_024d7f84,\n author = {Rebeschini, Patrick and Tatikonda, Sekhar C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated consensus via Min-Sum Splitting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/024d7f84fff11dd7e8d9c510137a2381-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/024d7f84fff11dd7e8d9c510137a2381-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/024d7f84fff11dd7e8d9c510137a2381-Reviews.html",
        "metareview": "",
        "pdf_size": 396217,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4574169517484672329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics, University of Oxford; Department of Electrical Engineering, Yale University",
        "aff_domain": "stats.ox.ac.uk;yale.edu",
        "email": "stats.ox.ac.uk;yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/024d7f84fff11dd7e8d9c510137a2381-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oxford;Yale University",
        "aff_unique_dep": "Department of Statistics;Department of Electrical Engineering",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.yale.edu",
        "aff_unique_abbr": "Oxford;Yale",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Acceleration and Averaging in Stochastic Descent Dynamics",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9447",
        "id": "9447",
        "author_site": "Walid Krichene, Peter Bartlett",
        "author": "Walid Krichene; Peter L Bartlett",
        "abstract": "We formulate and study a general family of (continuous-time) stochastic dynamics  for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.",
        "bibtex": "@inproceedings{NIPS2017_643de7cf,\n author = {Krichene, Walid and Bartlett, Peter L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Acceleration and Averaging in Stochastic Descent Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Reviews.html",
        "metareview": "",
        "pdf_size": 611374,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12451490547799278044&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Google, Inc.; U.C. Berkeley",
        "aff_domain": "google.com;cs.berkeley.edu",
        "email": "google.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/643de7cf7ba769c7466ccbc4adfd7fac-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.google.com;https://www.berkeley.edu",
        "aff_unique_abbr": "Google;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9043",
        "id": "9043",
        "author_site": "Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, Steven Wu",
        "author": "Katrina Ligett; Seth Neel; Aaron Roth; Bo Waggoner; Steven Z. Wu",
        "abstract": "Traditional approaches to differential privacy assume a fixed privacy requirement \u03b5 for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general \u201cnoise reduction\u201d framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to \u201csearch\u201d the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, and incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objective functions, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger empirical baseline based on binary search.",
        "bibtex": "@inproceedings{NIPS2017_86df7dcf,\n author = {Ligett, Katrina and Neel, Seth and Roth, Aaron and Waggoner, Bo and Wu, Steven Z.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/86df7dcfd896fcaf2674f757a2463eba-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/86df7dcfd896fcaf2674f757a2463eba-Reviews.html",
        "metareview": "",
        "pdf_size": 407722,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11428806656186662146&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Caltech+Hebrew University; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; Microsoft Research",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "https://arxiv.org/abs/1705.10829",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html",
        "aff_unique_index": "0+1;2;2;2;3",
        "aff_unique_norm": "California Institute of Technology;Hebrew University of Jerusalem;University of Pennsylvania;Microsoft Corporation",
        "aff_unique_dep": ";;;Microsoft Research",
        "aff_unique_url": "https://www.caltech.edu;https://www.huji.ac.il;https://www.upenn.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Caltech;HUJI;UPenn;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0+1;0;0;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Action Centered Contextual Bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9370",
        "id": "9370",
        "author_site": "Kristjan Greenewald, Ambuj Tewari, Susan Murphy, Predag Klasnja",
        "author": "Kristjan Greenewald; Ambuj Tewari; Susan Murphy; Predag Klasnja",
        "abstract": "Contextual bandits have become popular as they offer a middle ground between very simple approaches based on multi-armed bandits and very complex approaches using the full power of reinforcement learning. They have demonstrated success in web applications and have a rich body of associated theoretical guarantees. Linear models are well understood theoretically and preferred by practitioners because they are not only easily interpretable but also simple to implement and debug. Furthermore, if the linear model is true, we get very strong performance guarantees. Unfortunately, in emerging applications in mobile health, the time-invariant linear model assumption is untenable. We provide an extension of the linear model for contextual bandits that has two parts: baseline reward and treatment effect. We allow the former to be complex but keep the latter simple. We argue that this model is plausible for mobile health applications. At the same time, it leads to algorithms with strong performance guarantees as in the linear model setting, while still allowing for complex nonlinear baseline modeling. Our theory is supported by experiments on data gathered in a recently concluded mobile health study.",
        "bibtex": "@inproceedings{NIPS2017_4fa177df,\n author = {Greenewald, Kristjan and Tewari, Ambuj and Murphy, Susan and Klasnja, Predag},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Action Centered Contextual Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4fa177df22864518b2d7818d4db5db2d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4fa177df22864518b2d7818d4db5db2d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4fa177df22864518b2d7818d4db5db2d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4fa177df22864518b2d7818d4db5db2d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4fa177df22864518b2d7818d4db5db2d-Reviews.html",
        "metareview": "",
        "pdf_size": 353447,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14464800622061170738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, Harvard University; Department of Statistics, University of Michigan; School of Information, University of Michigan; Departments of Statistics and Computer Science, Harvard University",
        "aff_domain": "fas.harvard.edu;umich.edu;umich.edu;fas.harvard.edu",
        "email": "fas.harvard.edu;umich.edu;umich.edu;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4fa177df22864518b2d7818d4db5db2d-Abstract.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Harvard University;University of Michigan;Departments of Statistics and Computer Science, Harvard University",
        "aff_unique_dep": "Department of Statistics;Department of Statistics;",
        "aff_unique_url": "https://www.harvard.edu;https://www.umich.edu;",
        "aff_unique_abbr": "Harvard;UM;",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;Ann Arbor;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8894",
        "id": "8894",
        "author_site": "Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum",
        "author": "Haw-Shiuan Chang; Erik Learned-Miller; Andrew McCallum",
        "abstract": "Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.",
        "bibtex": "@inproceedings{NIPS2017_2f37d101,\n author = {Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2f37d10131f2a483a8dd005b3d14b0d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2f37d10131f2a483a8dd005b3d14b0d9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2f37d10131f2a483a8dd005b3d14b0d9-Reviews.html",
        "metareview": "",
        "pdf_size": 924820,
        "gs_citation": 443,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9751069988020372940&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2f37d10131f2a483a8dd005b3d14b0d9-Abstract.html"
    },
    {
        "title": "Active Exploration for Learning Symbolic Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9277",
        "id": "9277",
        "author_site": "Garrett Andersen, George Konidaris",
        "author": "Garrett Andersen; George Konidaris",
        "abstract": "We introduce an online active exploration algorithm for data-efficiently learning an abstract symbolic model of an environment. Our algorithm is divided into two parts: the first part quickly generates an intermediate Bayesian symbolic model from the data that the agent has collected so far, which the agent can then use along with the second part to guide its future exploration towards regions of the state space that the model is uncertain about. We show that our algorithm outperforms random and greedy exploration policies on two different computer game domains. The first domain is an Asteroids-inspired game with complex dynamics but basic logical structure. The second is the Treasure Game, with simpler dynamics but more complex logical structure.",
        "bibtex": "@inproceedings{NIPS2017_fb89fd13,\n author = {Andersen, Garrett and Konidaris, George},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Exploration for Learning Symbolic Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fb89fd138b104dcf8e2077ad2a23954d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fb89fd138b104dcf8e2077ad2a23954d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fb89fd138b104dcf8e2077ad2a23954d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fb89fd138b104dcf8e2077ad2a23954d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fb89fd138b104dcf8e2077ad2a23954d-Reviews.html",
        "metareview": "",
        "pdf_size": 1414881,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4690739969026810521&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "PROWLER.io, Cambridge, United Kingdom; Department of Computer Science, Brown University",
        "aff_domain": "prowler.io;cs.brown.edu",
        "email": "prowler.io;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fb89fd138b104dcf8e2077ad2a23954d-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "PROWLER.io;Brown University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://prowler.io;https://www.brown.edu",
        "aff_unique_abbr": ";Brown",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Active Learning from Peers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9467",
        "id": "9467",
        "author_site": "Keerthiram Murugesan, Jaime Carbonell",
        "author": "Keerthiram Murugesan; Jaime Carbonell",
        "abstract": "This paper addresses the challenge of learning from peers in an online multitask setting. Instead of always requesting a label from a human oracle, the proposed method first determines if the learner for each task can acquire that label with sufficient confidence from its peers either as a task-similarity weighted sum, or from the single most similar task.  If so, it saves the oracle query for later use in more difficult cases, and if not it queries the human oracle.  The paper develops the new algorithm to exhibit this behavior and proves a theoretical mistake bound for the method compared to the best linear predictor in hindsight. Experiments over three multitask learning benchmark datasets show clearly superior performance over baselines such as assuming task independence, learning only from the oracle and not learning from peer tasks.",
        "bibtex": "@inproceedings{NIPS2017_b8747078,\n author = {Murugesan, Keerthiram and Carbonell, Jaime},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning from Peers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b87470782489389f344c4fa4ceb5260c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b87470782489389f344c4fa4ceb5260c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b87470782489389f344c4fa4ceb5260c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b87470782489389f344c4fa4ceb5260c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b87470782489389f344c4fa4ceb5260c-Reviews.html",
        "metareview": "",
        "pdf_size": 911996,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16942852942656445175&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b87470782489389f344c4fa4ceb5260c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "AdaGAN: Boosting Generative Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9317",
        "id": "9317",
        "author_site": "Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann SIMON-GABRIEL, Bernhard Sch\u00f6lkopf",
        "author": "Ilya O Tolstikhin; Sylvain Gelly; Olivier Bousquet; Carl-Johann SIMON-GABRIEL; Bernhard Sch\u00f6lkopf",
        "abstract": "Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.",
        "bibtex": "@inproceedings{NIPS2017_d0010a6f,\n author = {Tolstikhin, Ilya O and Gelly, Sylvain and Bousquet, Olivier and SIMON-GABRIEL, Carl-Johann and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {AdaGAN: Boosting Generative Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d0010a6f34908640a4a6da2389772a78-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d0010a6f34908640a4a6da2389772a78-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d0010a6f34908640a4a6da2389772a78-Reviews.html",
        "metareview": "",
        "pdf_size": 503741,
        "gs_citation": 291,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15958765505735872542&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "MPI for Intelligent Systems, T\u00fcbingen, Germany; Google Brain, Z\u00fcrich, Switzerland; Google Brain, Z\u00fcrich, Switzerland; MPI for Intelligent Systems, T\u00fcbingen, Germany; MPI for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "tue.mpg.de;google.com;google.com;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;google.com;google.com;tue.mpg.de;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d0010a6f34908640a4a6da2389772a78-Abstract.html",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Google Brain, Z\u00fcrich, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;",
        "aff_unique_abbr": "MPI-IS;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "title": "Adaptive Accelerated Gradient Converging Method under H\\\"{o}lderian Error Bound Condition",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9095",
        "id": "9095",
        "author_site": "Mingrui Liu, Tianbao Yang",
        "author": "Mingrui Liu; Tianbao Yang",
        "abstract": "Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant  in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, i.e., leveraging  the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the H\\\"{o}lderian error bound (HEB) condition.   {\\it The key technique} for our development is a novel synthesis of  {\\it adaptive regularization and a conditional restarting scheme}, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic, PG's convergence speed is essentially $o(\\frac{1}{t})$, where $t$ is the total number of iterations; (ii) if the objective function consists of an $\\ell_1$, $\\ell_\\infty$, $\\ell_{1,\\infty}$, or huber  norm regularization and a convex smooth piecewise quadratic loss (e.g., square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a {\\it faster linear convergence} than PG without any other assumptions  (e.g., restricted eigen-value condition).   It is notable that  our linear convergence results for the aforementioned problems  are global instead of local.  To the best of our knowledge, these improved results are first shown in this work.",
        "bibtex": "@inproceedings{NIPS2017_2612aa89,\n author = {Liu, Mingrui and Yang, Tianbao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Accelerated Gradient Converging Method under H\\textbackslash \"\\lbrace o\\rbrace lderian Error Bound Condition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2612aa892d962d6f8056b195ca6e550d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2612aa892d962d6f8056b195ca6e550d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2612aa892d962d6f8056b195ca6e550d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2612aa892d962d6f8056b195ca6e550d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2612aa892d962d6f8056b195ca6e550d-Reviews.html",
        "metareview": "",
        "pdf_size": 397275,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15337097133361729190&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, The University of Iowa, Iowa City, IA 52242; Department of Computer Science, The University of Iowa, Iowa City, IA 52242",
        "aff_domain": "uiowa.edu;uiowa.edu",
        "email": "uiowa.edu;uiowa.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2612aa892d962d6f8056b195ca6e550d-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, The University of Iowa, Iowa City, IA 52242",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Adaptive Active Hypothesis Testing under Limited Information",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9184",
        "id": "9184",
        "author_site": "Fabio Cecchi, Nidhi Hegde",
        "author": "Fabio Cecchi; Nidhi Hegde",
        "abstract": "We consider the problem of active sequential hypothesis testing where a Bayesian decision maker must infer the true hypothesis from a set of hypotheses.  The decision maker may choose for a set of actions, where the outcome of an action is corrupted by independent noise.    In this paper we consider a special case where the decision maker has limited knowledge about the distribution of observations for each action, in that only a binary value is observed.  Our objective is to infer the true hypothesis with low error, while minimizing the number of action sampled.  Our main results include the derivation of a lower bound on sample size for our system under limited knowledge and the design of an active learning policy that matches this lower bound and outperforms similar known algorithms.",
        "bibtex": "@inproceedings{NIPS2017_9f44e956,\n author = {Cecchi, Fabio and Hegde, Nidhi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Active Hypothesis Testing under Limited Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Reviews.html",
        "metareview": "",
        "pdf_size": 510443,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12290189891685859294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Eindhoven University of Technology, Eindhoven, The Netherlands; Nokia Bell Labs, Paris-Saclay, France",
        "aff_domain": "tue.nl;nokia-bell-labs.com",
        "email": "tue.nl;nokia-bell-labs.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Eindhoven University of Technology;Nokia Bell Labs, Paris-Saclay, France",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tue.nl;",
        "aff_unique_abbr": "TU/e;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Eindhoven;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Netherlands;"
    },
    {
        "title": "Adaptive Batch Size for Safe Policy Gradients",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9142",
        "id": "9142",
        "author_site": "Matteo Papini, Matteo Pirotta, Marcello Restelli",
        "author": "Matteo Papini; Matteo Pirotta; Marcello Restelli",
        "abstract": "Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.",
        "bibtex": "@inproceedings{NIPS2017_ea6b2efb,\n author = {Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Batch Size for Safe Policy Gradients},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Reviews.html",
        "metareview": "",
        "pdf_size": 444610,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16475723046447632423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "DEIB, Politecnico di Milano, Italy; SequeL Team, Inria Lille, France; DEIB, Politecnico di Milano, Italy",
        "aff_domain": "polimi.it;inria.fr;polimi.it",
        "email": "polimi.it;inria.fr;polimi.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ea6b2efbdd4255a9f1b3bbc6399b58f4-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Politecnico di Milano;SequeL Team, Inria Lille, France",
        "aff_unique_dep": "DEIB;",
        "aff_unique_url": "https://www.polimi.it;",
        "aff_unique_abbr": "Politecnico di Milano;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy;"
    },
    {
        "title": "Adaptive Bayesian Sampling with Monte Carlo EM",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8918",
        "id": "8918",
        "author_site": "Anirban Roychowdhury, Srinivasan Parthasarathy",
        "author": "Anirban Roychowdhury; Srinivasan Parthasarathy",
        "abstract": "We present a novel technique for learning the mass matrices in samplers obtained from discretized dynamics that preserve some energy function. Existing adaptive samplers use Riemannian preconditioning techniques, where the mass matrices are functions of the parameters being sampled. This leads to significant complexities in the energy reformulations and resultant dynamics, often leading to implicit systems of equations and requiring inversion of high-dimensional matrices in the leapfrog steps. Our approach provides a simpler alternative, by using existing dynamics in the sampling step of a Monte Carlo EM framework, and learning the mass matrices in the M step with a novel online technique. We also propose a way to adaptively set the number of samples gathered in the E step, using sampling error estimates from the leapfrog dynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e} dynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as well as newer stochastic algorithms such as SGHMC and SGNHT, and show strong performance on synthetic and real high-dimensional sampling scenarios; we achieve sampling accuracies comparable to Riemannian samplers while being significantly faster.",
        "bibtex": "@inproceedings{NIPS2017_02a32ad2,\n author = {Roychowdhury, Anirban and Parthasarathy, Srinivasan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Bayesian Sampling with Monte Carlo EM},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/02a32ad2669e6fe298e607fe7cc0e1a0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/02a32ad2669e6fe298e607fe7cc0e1a0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/02a32ad2669e6fe298e607fe7cc0e1a0-Reviews.html",
        "metareview": "",
        "pdf_size": 462113,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5o_lZFCI6u4J:scholar.google.com/&scioq=Adaptive+Bayesian+Sampling+with+Monte+Carlo+EM&hl=en&as_sdt=0,33",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University",
        "aff_domain": "osu.edu;cse.ohio-state.edu",
        "email": "osu.edu;cse.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/02a32ad2669e6fe298e607fe7cc0e1a0-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Ohio State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adaptive Classification for Prediction Under a Budget",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9249",
        "id": "9249",
        "author_site": "Feng Nan, Venkatesh Saligrama",
        "author": "Feng Nan; Venkatesh Saligrama",
        "abstract": "We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.",
        "bibtex": "@inproceedings{NIPS2017_d9ff90f4,\n author = {Nan, Feng and Saligrama, Venkatesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Classification for Prediction Under a Budget},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Reviews.html",
        "metareview": "",
        "pdf_size": 836329,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13831041986001169284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Systems Engineering, Boston University; Electrical Engineering, Boston University",
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d9ff90f4000eacd3a6c9cb27f78994cf-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Systems Engineering, Boston University;Electrical Engineering, Boston University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Adaptive Clustering through Semidefinite Programming",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8969",
        "id": "8969",
        "author": "Martin Royer",
        "abstract": "We analyze the clustering problem through a flexible probabilistic model that aims to identify an optimal partition on the sample X1,...,Xn. We perform exact clustering with high probability using a convex semidefinite estimator that interprets as a corrected, relaxed version of K-means. The estimator is analyzed through a non-asymptotic framework and showed to be optimal or near-optimal in recovering the partition. Furthermore, its performances are shown to be adaptive to the problem\u2019s effective dimension, as well as to K the unknown number of groups in this partition. We illustrate the method\u2019s performances in comparison to other classical clustering algorithms with numerical experiments on simulated high-dimensional data.",
        "bibtex": "@inproceedings{NIPS2017_3a15c7d0,\n author = {Royer, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Clustering through Semidefinite Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3a15c7d0bbe60300a39f76f8a5ba6896-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3a15c7d0bbe60300a39f76f8a5ba6896-Reviews.html",
        "metareview": "",
        "pdf_size": 406572,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2806911004717821988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Laboratoire de Math\u00e9matiques d\u2019Orsay, Univ. Paris-Sud, CNRS, Universit\u00e9 Paris-Saclay",
        "aff_domain": "math.u-psud.fr",
        "email": "math.u-psud.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Laboratoire de Math\u00e9matiques d\u2019Orsay, Univ. Paris-Sud, CNRS, Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "title": "Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9112",
        "id": "9112",
        "author_site": "Yi Xu, Qihang Lin, Tianbao Yang",
        "author": "Yi Xu; Qihang Lin; Tianbao Yang",
        "abstract": "Error bound, an inherent property of an optimization problem, has recently revived in the development of algorithms with improved global convergence without strong convexity. The most studied error bound  is the quadratic error bound, which generalizes strong convexity and is satisfied by a large family of machine learning problems. Quadratic error bound have been leveraged to achieve linear convergence in many first-order methods including the stochastic variance reduced gradient (SVRG) method, which is one of the most important stochastic optimization methods in machine learning. However, the studies along this direction face the critical issue that the algorithms must depend on an unknown growth parameter (a generalization of strong convexity modulus) in the error bound. This parameter is difficult to estimate exactly and the algorithms choosing this parameter heuristically do not have theoretical convergence guarantee. To address this issue, we propose novel SVRG methods that automatically search for this unknown parameter on the fly of optimization while still obtain almost the same convergence rate as when this parameter is known. We also analyze the convergence property of SVRG methods under H\\\"{o}lderian error bound, which generalizes the quadratic error bound.",
        "bibtex": "@inproceedings{NIPS2017_09fb05dd,\n author = {Xu, Yi and Lin, Qihang and Yang, Tianbao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/09fb05dd477d4ae6479985ca56c5a12d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/09fb05dd477d4ae6479985ca56c5a12d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/09fb05dd477d4ae6479985ca56c5a12d-Reviews.html",
        "metareview": "",
        "pdf_size": 490875,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18224822175265000818&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA; Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA; Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA",
        "aff_domain": "uiowa.edu;uiowa.edu;uiowa.edu",
        "email": "uiowa.edu;uiowa.edu;uiowa.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/09fb05dd477d4ae6479985ca56c5a12d-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "The University of Iowa;Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uiowa.edu;",
        "aff_unique_abbr": "UIowa;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Iowa City;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Adaptive stimulus selection for optimizing neural population responses",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8931",
        "id": "8931",
        "author_site": "Benjamin Cowley, Ryan Williamson, Katerina Clemens, Matthew Smith, Byron M Yu",
        "author": "Benjamin Cowley; Ryan Williamson; Katerina Clemens; Matthew Smith; Byron M. Yu",
        "abstract": "Adaptive stimulus selection methods in neuroscience have primarily focused on maximizing the firing rate of a single recorded neuron. When recording from a population of neurons, it is usually not possible to find a single stimulus that maximizes the firing rates of all neurons. This motivates optimizing an objective function that takes into account the responses of all recorded neurons together. We propose \u201cAdept,\u201d an adaptive stimulus selection method that can optimize population objective functions. In simulations, we first confirmed that population objective functions elicited more diverse stimulus responses than single-neuron objective functions. Then, we tested Adept in a closed-loop electrophysiological experiment in which population activity was recorded from macaque V4, a cortical area known for mid-level visual processing. To predict neural responses, we used the outputs of a deep convolutional neural network model as feature embeddings. Images chosen by Adept elicited mean neural responses that were 20% larger than those for randomly-chosen natural images, and also evoked a larger diversity of neural responses. Such adaptive stimulus selection methods can facilitate experiments that involve neurons far from the sensory periphery, for which it is often unclear which stimuli to present.",
        "bibtex": "@inproceedings{NIPS2017_892c91e0,\n author = {Cowley, Benjamin and Williamson, Ryan and Clemens, Katerina and Smith, Matthew and Yu, Byron M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive stimulus selection for optimizing neural population responses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/892c91e0a653ba19df81a90f89d99bcd-Reviews.html",
        "metareview": "",
        "pdf_size": 3169420,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=763962317465604567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Dept. + Center for Neural Basis of Cognition + Dept. of Electrical and Computer Engineering + Dept. of Biomedical Engineering, Carnegie Mellon University; Machine Learning Dept. + Center for Neural Basis of Cognition + Dept. of Electrical and Computer Engineering + Dept. of Biomedical Engineering, Carnegie Mellon University + School of Medicine, University of Pittsburgh; Center for Neural Basis of Cognition + Dept. of Neuroscience, University of Pittsburgh; Center for Neural Basis of Cognition + Dept. of Ophthalmology, University of Pittsburgh; Machine Learning Dept. + Center for Neural Basis of Cognition + Dept. of Electrical and Computer Engineering + Dept. of Biomedical Engineering, Carnegie Mellon University",
        "aff_domain": "csNcmuNeduL;pittNeduL;pittNeduL;pittNeduL;cmuNedu",
        "email": "csNcmuNeduL;pittNeduL;pittNeduL;pittNeduL;cmuNedu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html",
        "aff_unique_index": "0+1+2+3;0+1+2+3+4;1+5;1+6;0+1+2+3",
        "aff_unique_norm": "Machine Learning Dept.;Center for Neural Basis of Cognition;University of California, Los Angeles;Dept. of Biomedical Engineering, Carnegie Mellon University;School of Medicine, University of Pittsburgh;Dept. of Neuroscience, University of Pittsburgh;Dept. of Ophthalmology, University of Pittsburgh",
        "aff_unique_dep": ";;Department of Electrical and Computer Engineering;;;;",
        "aff_unique_url": ";;https://www.ucla.edu;;;;",
        "aff_unique_abbr": ";;UCLA;;;;",
        "aff_campus_unique_index": "1;1;;;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "1;1;;;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Adversarial Ranking for Language Generation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9100",
        "id": "9100",
        "author_site": "Kevin Lin, Dianqi Li, Xiaodong He, Ming-ting Sun, Zhengyou Zhang",
        "author": "Kevin Lin; Dianqi Li; Xiaodong He; Zhengyou Zhang; Ming-ting Sun",
        "abstract": "Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.",
        "bibtex": "@inproceedings{NIPS2017_bf201d54,\n author = {Lin, Kevin and Li, Dianqi and He, Xiaodong and Zhang, Zhengyou and Sun, Ming-ting},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adversarial Ranking for Language Generation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bf201d5407a6509fa536afc4b380577e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bf201d5407a6509fa536afc4b380577e-Reviews.html",
        "metareview": "",
        "pdf_size": 412549,
        "gs_citation": 458,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6871069604642164772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Washington; University of Washington; Microsoft Research; Microsoft Research; University of Washington",
        "aff_domain": "uw.edu;uw.edu;microsoft.com;microsoft.com;uw.edu",
        "email": "uw.edu;uw.edu;microsoft.com;microsoft.com;uw.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bf201d5407a6509fa536afc4b380577e-Abstract.html",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Washington;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.washington.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UW;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adversarial Surrogate Losses for Ordinal Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8852",
        "id": "8852",
        "author_site": "Rizal Fathony, Mohammad Ali Bashiri, Brian Ziebart",
        "author": "Rizal Fathony; Mohammad Ali Bashiri; Brian Ziebart",
        "abstract": "Ordinal regression seeks class label predictions when the penalty incurred for mistakes increases according to an ordering over the labels. The absolute error is a canonical example. Many existing methods for this task reduce to binary classification problems and employ surrogate losses, such as the hinge loss. We instead derive uniquely defined surrogate ordinal regression loss functions by seeking the predictor that is robust to the worst-case approximations of training data labels, subject to matching certain provided training data statistics. We demonstrate the advantages of our approach over other surrogate losses based on hinge loss approximations using UCI ordinal prediction tasks.",
        "bibtex": "@inproceedings{NIPS2017_c86a7ee3,\n author = {Fathony, Rizal and Bashiri, Mohammad Ali and Ziebart, Brian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adversarial Surrogate Losses for Ordinal Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Reviews.html",
        "metareview": "",
        "pdf_size": 2878987,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16761050047789534017&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c86a7ee3d8ef0b551ed58e354a836f2b-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adversarial Symmetric Variational Autoencoder",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9212",
        "id": "9212",
        "author_site": "Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, Lawrence Carin",
        "author": "Yuchen Pu; Weiyao Wang; Ricardo Henao; Liqun Chen; Zhe Gan; Chunyuan Li; Lawrence Carin",
        "abstract": "A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmarks datasets.",
        "bibtex": "@inproceedings{NIPS2017_4cb81113,\n author = {Pu, Yuchen and Wang, Weiyao and Henao, Ricardo and Chen, Liqun and Gan, Zhe and Li, Chunyuan and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adversarial Symmetric Variational Autoencoder},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4cb811134b9d39fc3104bd06ce75abad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4cb811134b9d39fc3104bd06ce75abad-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4cb811134b9d39fc3104bd06ce75abad-Reviews.html",
        "metareview": "",
        "pdf_size": 870673,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12383748972223895036&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4cb811134b9d39fc3104bd06ce75abad-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Affine-Invariant Online Optimization and the Low-rank Experts Problem",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9251",
        "id": "9251",
        "author_site": "Tomer Koren, Roi Livni",
        "author": "Tomer Koren; Roi Livni",
        "abstract": "We present a new affine-invariant optimization algorithm called Online Lazy Newton. The regret of Online Lazy Newton is independent of conditioning: the algorithm's performance depends on the best possible preconditioning of the problem in retrospect and on its \\emph{intrinsic} dimensionality. As an application, we show how Online Lazy Newton can be used to achieve an optimal regret of order $\\sqrt{rT}$ for the low-rank experts problem, improving by a $\\sqrt{r}$ factor over the previously best known bound and resolving an open problem posed by Hazan et al (2016).",
        "bibtex": "@inproceedings{NIPS2017_34766559,\n author = {Koren, Tomer and Livni, Roi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Affine-Invariant Online Optimization and the Low-rank Experts Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/347665597cbfaef834886adbb848011f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/347665597cbfaef834886adbb848011f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/347665597cbfaef834886adbb848011f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/347665597cbfaef834886adbb848011f-Reviews.html",
        "metareview": "",
        "pdf_size": 271111,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2648186093763476284&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Google Brain; Princeton University",
        "aff_domain": "google.com;cs.princeton.edu",
        "email": "google.com;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/347665597cbfaef834886adbb848011f-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;Princeton University",
        "aff_unique_dep": "Google Brain;",
        "aff_unique_url": "https://brain.google.com;https://www.princeton.edu",
        "aff_unique_abbr": "Google Brain;Princeton",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Affinity Clustering: Hierarchical Clustering at Scale",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9453",
        "id": "9453",
        "author_site": "Mohammadhossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, Vahab Mirrokni",
        "author": "Mohammadhossein Bateni; Soheil Behnezhad; Mahsa Derakhshan; MohammadTaghi Hajiaghayi; Raimondas Kiveris; Silvio Lattanzi; Vahab Mirrokni",
        "abstract": "Graph clustering is a fundamental task in many data-mining and machine-learning pipelines. In particular, identifying a good hierarchical structure is at the same time a fundamental and challenging problem for several applications. The amount of data to analyze is increasing at an astonishing rate each day. Hence there is a need for new solutions to efficiently compute effective hierarchical clusterings on such huge data.  The main focus of this paper is on minimum spanning tree (MST) based clusterings. In particular, we propose affinity, a novel hierarchical clustering based on Boruvka's MST algorithm. We prove certain theoretical guarantees for affinity (as well as some other classic algorithms) and show that in practice it is superior to several other state-of-the-art clustering algorithms.   Furthermore, we present two MapReduce implementations for affinity. The first one works for the case where the input graph is dense and takes constant rounds. It is based on a Massively Parallel MST algorithm for dense graphs that improves upon the state-of-the-art algorithm of Lattanzi et al. (SPAA 2011). Our second algorithm has no assumption on the density of the input graph and finds the affinity clustering in $O(\\log n)$ rounds using Distributed Hash Tables (DHTs). We show experimentally that our algorithms are scalable for huge data sets, e.g., for graphs with trillions of edges.",
        "bibtex": "@inproceedings{NIPS2017_2e1b24a6,\n author = {Bateni, Mohammadhossein and Behnezhad, Soheil and Derakhshan, Mahsa and Hajiaghayi, MohammadTaghi and Kiveris, Raimondas and Lattanzi, Silvio and Mirrokni, Vahab},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Affinity Clustering: Hierarchical Clustering at Scale},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Reviews.html",
        "metareview": "",
        "pdf_size": 1094750,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=528515049552323216&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Google Research; University of Maryland; University of Maryland; University of Maryland; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;cs.umd.edu;cs.umd.edu;cs.umd.edu;google.com;google.com;google.com",
        "email": "google.com;cs.umd.edu;cs.umd.edu;cs.umd.edu;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2e1b24a664f5e9c18f407b2f9c73e821-Abstract.html",
        "aff_unique_index": "0;1;1;1;0;0;0",
        "aff_unique_norm": "Google;University of Maryland",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www/umd.edu",
        "aff_unique_abbr": "Google Research;UMD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9196",
        "id": "9196",
        "author_site": "Bikash Joshi, Massih R. Amini, Ioannis Partalas, Franck Iutzeler, Yury Maximov",
        "author": "Bikash Joshi; Massih R. Amini; Ioannis Partalas; Franck Iutzeler; Yury Maximov",
        "abstract": "We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority  of  large-scale  multi-class classification problems and to reduce the number of pairs of examples in the expanded data.  We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches.",
        "bibtex": "@inproceedings{NIPS2017_14ea0d5b,\n author = {Joshi, Bikash and Amini, Massih R. and Partalas, Ioannis and Iutzeler, Franck and Maximov, Yury},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/14ea0d5b0cf49525d1866cb1e95ada5d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/14ea0d5b0cf49525d1866cb1e95ada5d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/14ea0d5b0cf49525d1866cb1e95ada5d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/14ea0d5b0cf49525d1866cb1e95ada5d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/14ea0d5b0cf49525d1866cb1e95ada5d-Reviews.html",
        "metareview": "",
        "pdf_size": 771975,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8598088141326389333&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Univ. Grenoble Alps, LIG; Univ. Grenoble Alps, LIG; Expedia EWE; Univ. Grenoble Alps, LJK; Los Alamos National Laboratory and Skolkovo IST",
        "aff_domain": "imag.fr;imag.fr;expedia.com;imag.fr;lanl.gov",
        "email": "imag.fr;imag.fr;expedia.com;imag.fr;lanl.gov",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/14ea0d5b0cf49525d1866cb1e95ada5d-Abstract.html",
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "Univ. Grenoble Alps, LIG;Expedia EWE;Univ. Grenoble Alps, LJK;Los Alamos National Laboratory and Skolkovo IST",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Alternating Estimation for Structured High-Dimensional Multi-Response Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9068",
        "id": "9068",
        "author_site": "Sheng Chen, Arindam Banerjee",
        "author": "Sheng Chen; Arindam Banerjee",
        "abstract": "We consider the problem of learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among different responses, we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector (GDS). Under suitable sample size and resampling assumptions, we show that the error of the estimates generated by AltEst, with high probability, converges linearly to certain minimum achievable level, which can be tersely expressed by a few geometric measures, such as Gaussian width of sets related to the parameter structure. To the best of our knowledge, this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation with general structures.",
        "bibtex": "@inproceedings{NIPS2017_f60bb6bb,\n author = {Chen, Sheng and Banerjee, Arindam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Alternating Estimation for Structured High-Dimensional Multi-Response Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Reviews.html",
        "metareview": "",
        "pdf_size": 524212,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7801416069151772376&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science & Engineering, University of Minnesota, Twin Cities; Dept. of Computer Science & Engineering, University of Minnesota, Twin Cities",
        "aff_domain": "cs.umn.edu;cs.umn.edu",
        "email": "cs.umn.edu;cs.umn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f60bb6bb4c96d4df93c51bd69dcc15a0-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Dept. of Computer Science & Engineering, University of Minnesota, Twin Cities",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Alternating minimization for dictionary learning with random initialization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8988",
        "id": "8988",
        "author_site": "Niladri Chatterji, Peter Bartlett",
        "author": "Niladri Chatterji; Peter L Bartlett",
        "abstract": "We present theoretical guarantees for an alternating minimization algorithm for the dictionary learning/sparse coding problem. The dictionary learning problem is to factorize vector samples $y^{1},y^{2},\\ldots, y^{n}$ into an appropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\\ldots,x^{n*}$. Our algorithm is a simple alternating minimization procedure that switches between $\\ell_1$ minimization and gradient descent in alternate steps. Dictionary learning and specifically alternating minimization algorithms for dictionary learning are well studied both theoretically and empirically. However, in contrast to previous theoretical analyses for this problem, we replace a condition on the operator norm (that is, the largest magnitude singular value) of the true underlying dictionary $A^*$ with a condition on the matrix infinity norm (that is, the largest magnitude term). This not only allows us to get convergence rates for the error of the estimated dictionary measured in the matrix infinity norm, but also ensures that a random initialization will provably converge to the global optimum. Our guarantees are under a reasonable generative model that allows for dictionaries with growing operator norms, and can handle an arbitrary level of overcompleteness, while having sparsity that is information theoretically optimal. We also establish upper bounds on the sample complexity of our algorithm.",
        "bibtex": "@inproceedings{NIPS2017_3210ddbe,\n author = {Chatterji, Niladri and Bartlett, Peter L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Alternating minimization for dictionary learning with random initialization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3210ddbeaa16948a702b6049b8d9a202-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3210ddbeaa16948a702b6049b8d9a202-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3210ddbeaa16948a702b6049b8d9a202-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3210ddbeaa16948a702b6049b8d9a202-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3210ddbeaa16948a702b6049b8d9a202-Reviews.html",
        "metareview": "",
        "pdf_size": 253571,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2373311608261538827&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3210ddbeaa16948a702b6049b8d9a202-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An Empirical Bayes Approach to Optimizing Machine Learning Algorithms",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9057",
        "id": "9057",
        "author": "James McInerney",
        "abstract": "There is rapidly growing interest in using Bayesian optimization to tune model and inference hyperparameters for machine learning algorithms that take a long time to run. For example, Spearmint is a popular software package for selecting the optimal number of layers and learning rate in neural networks. But given that there is uncertainty about which hyperparameters give the best predictive performance, and given that fitting a model for each choice of hyperparameters is costly, it is arguably wasteful to \"throw away\" all but the best result, as per Bayesian optimization. A related issue is the danger of overfitting the validation data when optimizing many hyperparameters. In this paper, we consider an alternative approach that uses more samples from the hyperparameter selection procedure to average over the uncertainty in model hyperparameters. The resulting approach, empirical Bayes for hyperparameter averaging (EB-Hyp) predicts held-out data better than Bayesian optimization in two experiments on latent Dirichlet allocation and deep latent Gaussian models. EB-Hyp suggests a simpler approach to evaluating and deploying machine learning algorithms that does not require a separate validation data set and hyperparameter selection procedure.",
        "bibtex": "@inproceedings{NIPS2017_b60c5ab6,\n author = {McInerney, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Empirical Bayes Approach to Optimizing Machine Learning Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b60c5ab647a27045b462934977ccad9a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b60c5ab647a27045b462934977ccad9a-Reviews.html",
        "metareview": "",
        "pdf_size": 360057,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2896058181749873735&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Spotify Research",
        "aff_domain": "spotify.com",
        "email": "spotify.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b60c5ab647a27045b462934977ccad9a-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Spotify",
        "aff_unique_dep": "Spotify Research",
        "aff_unique_url": "https://www.spotify.com/research",
        "aff_unique_abbr": "Spotify",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "An Empirical Study on The Properties of Random Bases for Kernel Methods",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9062",
        "id": "9062",
        "author_site": "Maximilian Alber, Pieter-Jan Kindermans, Kristof Sch\u00fctt, Klaus-Robert M\u00fcller, Fei Sha",
        "author": "Maximilian Alber; Pieter-Jan Kindermans; Kristof Sch\u00fctt; Klaus-Robert M\u00fcller; Fei Sha",
        "abstract": "Kernel machines as well as neural networks possess universal function approximation properties. Nevertheless in practice their ways of choosing the appropriate function class differ. Specifically neural networks learn a representation by adapting their basis functions to the data and the task at hand, while kernel methods typically use a basis that is not adapted during training. In this work, we contrast random features of approximated kernel machines with learned features of neural networks. Our analysis reveals how these random and adaptive basis functions affect the quality of learning. Furthermore, we present basis adaptation schemes that allow for a more compact representation, while retaining the generalization properties of kernel machines.",
        "bibtex": "@inproceedings{NIPS2017_92af93f7,\n author = {Alber, Maximilian and Kindermans, Pieter-Jan and Sch\\\"{u}tt, Kristof and M\\\"{u}ller, Klaus-Robert and Sha, Fei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Empirical Study on The Properties of Random Bases for Kernel Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/92af93f73faf3cefc129b6bc55a748a9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/92af93f73faf3cefc129b6bc55a748a9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/92af93f73faf3cefc129b6bc55a748a9-Reviews.html",
        "metareview": "",
        "pdf_size": 380007,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5708444052068554738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Technische Universit\u00e4t Berlin; Technische Universit\u00e4t Berlin; Technische Universit\u00e4t Berlin; Technische Universit\u00e4t Berlin+Korea University+Max Planck Institut f\u00fcr Informatik; University of Southern California",
        "aff_domain": "tu-berlin.de; ; ; ;usc.edu",
        "email": "tu-berlin.de; ; ; ;usc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/92af93f73faf3cefc129b6bc55a748a9-Abstract.html",
        "aff_unique_index": "0;0;0;0+1+2;3",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;Korea University;Max Planck Institute for Informatics;University of Southern California",
        "aff_unique_dep": ";;Institute for Informatics;",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.korea.ac.kr;https://mpi-inf.mpg.de;https://www.usc.edu",
        "aff_unique_abbr": "TU Berlin;KU;MPII;USC",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0;0+1+0;2",
        "aff_country_unique": "Germany;South Korea;United States"
    },
    {
        "title": "An Error Detection and Correction Framework for Connectomics",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9449",
        "id": "9449",
        "author_site": "Jonathan Zung, Ignacio Tartavull, Kisuk Lee, H. Sebastian Seung",
        "author": "Jonathan Zung; Ignacio Tartavull; Kisuk Lee; H. Sebastian Seung",
        "abstract": "We define and study error detection and correction tasks that are useful for 3D reconstruction of neurons from electron microscopic imagery, and for image segmentation more generally. Both tasks take as input the raw image and a binary mask representing a candidate object. For the error detection task, the desired output is a map of split and merge errors in the object. For the error correction task, the desired output is the true object. We call this object mask pruning, because the candidate object mask is assumed to be a superset of the true object. We train multiscale 3D convolutional networks to perform both tasks. We find that the error-detecting net can achieve high accuracy. The accuracy of the error-correcting net is enhanced if its input object mask is ``advice'' (union of erroneous objects) from the error-detecting net.",
        "bibtex": "@inproceedings{NIPS2017_4500e403,\n author = {Zung, Jonathan and Tartavull, Ignacio and Lee, Kisuk and Seung, H. Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Error Detection and Correction Framework for Connectomics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4500e4037738e13c0c18db508e18d483-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4500e4037738e13c0c18db508e18d483-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4500e4037738e13c0c18db508e18d483-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4500e4037738e13c0c18db508e18d483-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4500e4037738e13c0c18db508e18d483-Reviews.html",
        "metareview": "",
        "pdf_size": 2014371,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15381867182099601888&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Princeton University; Princeton University; Princeton University + MIT; Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;mit.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;mit.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4500e4037738e13c0c18db508e18d483-Abstract.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "Princeton University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.princeton.edu;https://web.mit.edu",
        "aff_unique_abbr": "Princeton;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An inner-loop free solution to inverse problems using deep neural networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9024",
        "id": "9024",
        "author_site": "Kai Fan, Qi Wei, Lawrence Carin, Katherine Heller",
        "author": "Kai Fan; Qi Wei; Lawrence Carin; Katherine A. Heller",
        "abstract": "We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion.  To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.",
        "bibtex": "@inproceedings{NIPS2017_f016e59c,\n author = {Fan, Kai and Wei, Qi and Carin, Lawrence and Heller, Katherine A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An inner-loop free solution to inverse problems using deep neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f016e59c7ad8b1d72903bb1aa5720d53-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f016e59c7ad8b1d72903bb1aa5720d53-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f016e59c7ad8b1d72903bb1aa5720d53-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f016e59c7ad8b1d72903bb1aa5720d53-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f016e59c7ad8b1d72903bb1aa5720d53-Reviews.html",
        "metareview": "",
        "pdf_size": 1057948,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15165080620323938629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Duke University; Duke University; Duke University; Duke University",
        "aff_domain": "stat.duke.edu;duke.edu;duke.edu;stat.duke.edu",
        "email": "stat.duke.edu;duke.edu;duke.edu;stat.duke.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f016e59c7ad8b1d72903bb1aa5720d53-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9031",
        "id": "9031",
        "author_site": "Yonatan Belinkov, Jim Glass",
        "author": "Yonatan Belinkov; James Glass",
        "abstract": "Neural networks have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.",
        "bibtex": "@inproceedings{NIPS2017_b069b341,\n author = {Belinkov, Yonatan and Glass, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b069b3415151fa7217e870017374de7c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b069b3415151fa7217e870017374de7c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b069b3415151fa7217e870017374de7c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b069b3415151fa7217e870017374de7c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b069b3415151fa7217e870017374de7c-Reviews.html",
        "metareview": "",
        "pdf_size": 1126553,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16838313907198642588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b069b3415151fa7217e870017374de7c-Abstract.html"
    },
    {
        "title": "Approximate Supermodularity Bounds for Experimental Design",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9315",
        "id": "9315",
        "author_site": "Luiz Chamon, Alejandro Ribeiro",
        "author": "Luiz Chamon; Alejandro Ribeiro",
        "abstract": "This work provides performance guarantees for the greedy solution of experimental design problems. In particular, it focuses on A- and E-optimal designs, for which typical guarantees do not apply since the mean-square error and the maximum eigenvalue of the estimation error covariance matrix are not supermodular. To do so, it leverages the concept of approximate supermodularity to derive non-asymptotic worst-case suboptimality bounds for these greedy solutions. These bounds reveal that as the SNR of the experiments decreases, these cost functions behave increasingly as supermodular functions. As such, greedy A- and E-optimal designs approach (1-1/e)-optimality. These results reconcile the empirical success of greedy experimental design with the non-supermodularity of the A- and E-optimality criteria.",
        "bibtex": "@inproceedings{NIPS2017_0d9095b0,\n author = {Chamon, Luiz and Ribeiro, Alejandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximate Supermodularity Bounds for Experimental Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Reviews.html",
        "metareview": "",
        "pdf_size": 400455,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10525966485316267212&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Electrical and Systems Engineering, University of Pennsylvania; Electrical and Systems Engineering, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0d9095b0d6bbe98ea0c9c02b11b59ee3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Electrical and Systems Engineering",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Approximation Algorithms for $\\ell_0$-Low Rank Approximation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9433",
        "id": "9433",
        "author_site": "Karl Bringmann, Pavel Kolev, David Woodruff",
        "author": "Karl Bringmann; Pavel Kolev; David Woodruff",
        "abstract": "We study the $\\ell_0$-Low Rank Approximation Problem, where the goal is,    given an $m \\times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which   $\\|A'-A\\|_0$ is minimized.    Here, for a matrix $B$, $\\|B\\|_0$ denotes the number of its non-zero entries.    This NP-hard variant of low rank approximation is natural for problems    with no underlying metric, and its goal is to minimize the number of disagreeing   data positions.      We provide approximation algorithms which significantly improve the running time    and approximation factor of previous work.    For $k > 1$, we show how to find, in poly$(mn)$ time for every $k$,    a rank $O(k \\log(n/k))$ matrix $A'$ for which $\\|A'-A\\|_0 \\leq O(k^2 \\log(n/k)) \\OPT$.    To the best of our knowledge, this is the first algorithm with provable guarantees    for the $\\ell_0$-Low Rank Approximation Problem for $k > 1$,    even for bicriteria algorithms.       For the well-studied case when $k = 1$, we give a $(2+\\epsilon)$-approximation    in {\\it sublinear time}, which is impossible for other variants of low rank    approximation such as for the  Frobenius norm.    We strengthen this for the well-studied case of binary matrices to obtain    a $(1+O(\\psi))$-approximation in sublinear time,    where $\\psi = \\OPT/\\nnz{A}$.   For small $\\psi$, our approximation factor is $1+o(1)$.",
        "bibtex": "@inproceedings{NIPS2017_e94fe9ac,\n author = {Bringmann, Karl and Kolev, Pavel and Woodruff, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximation Algorithms for \\textbackslash ell\\_0-Low Rank Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e94fe9ac8dc10dd8b9a239e6abee2848-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e94fe9ac8dc10dd8b9a239e6abee2848-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e94fe9ac8dc10dd8b9a239e6abee2848-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e94fe9ac8dc10dd8b9a239e6abee2848-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e94fe9ac8dc10dd8b9a239e6abee2848-Reviews.html",
        "metareview": "",
        "pdf_size": 328434,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15058112305416905189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbr\u00fccken, Germany; Department of Computer Science, Carnegie Mellon University",
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de;cs.cmu.edu",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e94fe9ac8dc10dd8b9a239e6abee2848-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Max Planck Institute for Informatics;Carnegie Mellon University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://www.cmu.edu",
        "aff_unique_abbr": "MPII;CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Saarbr\u00fccken;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9094",
        "id": "9094",
        "author_site": "Benjamin Moseley, Joshua Wang",
        "author": "Benjamin Moseley; Joshua Wang",
        "abstract": "Hierarchical clustering is a data analysis method that has been used for decades. Despite its widespread use, the method has an underdeveloped analytical foundation. Having a well understood foundation would both support the currently used methods and help guide future improvements. The goal of this paper is to give an analytic framework to better understand observations seen in practice. This paper considers the dual of a problem framework for hierarchical clustering introduced by Dasgupta. The main result is that one of the most popular algorithms used in practice, average linkage agglomerative clustering, has a small constant approximation ratio for this objective. Furthermore, this paper establishes that using bisecting k-means divisive clustering has a very poor lower bound on its approximation ratio for the same objective.  However, we show that there are divisive algorithms that perform well with respect to this objective by giving two constant approximation algorithms. This paper is some of the first work to establish guarantees on widely used hierarchical algorithms for a natural objective function.  This objective and analysis give insight into what these popular algorithms are optimizing and when they will perform well.",
        "bibtex": "@inproceedings{NIPS2017_d8d31bd7,\n author = {Moseley, Benjamin and Wang, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d8d31bd778da8bdd536187c36e48892b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d8d31bd778da8bdd536187c36e48892b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d8d31bd778da8bdd536187c36e48892b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d8d31bd778da8bdd536187c36e48892b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d8d31bd778da8bdd536187c36e48892b-Reviews.html",
        "metareview": "",
        "pdf_size": 242877,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17401193523033039465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Carnegie Mellon University; Department of Computer Science Stanford University",
        "aff_domain": "andrew.cmu.edu;cs.stanford.edu",
        "email": "andrew.cmu.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d8d31bd778da8bdd536187c36e48892b-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Department of Computer Science Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Approximation and Convergence Properties of Generative Adversarial Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9329",
        "id": "9329",
        "author_site": "Shuang Liu, Olivier Bousquet, Kamalika Chaudhuri",
        "author": "Shuang Liu; Olivier Bousquet; Kamalika Chaudhuri",
        "abstract": "Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a \"two-player game\" between a generator and a discriminator.  Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence.   In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.",
        "bibtex": "@inproceedings{NIPS2017_4491777b,\n author = {Liu, Shuang and Bousquet, Olivier and Chaudhuri, Kamalika},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximation and Convergence Properties of Generative Adversarial Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4491777b1aa8b5b32c2e8666dbe1a495-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4491777b1aa8b5b32c2e8666dbe1a495-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4491777b1aa8b5b32c2e8666dbe1a495-Reviews.html",
        "metareview": "",
        "pdf_size": 312602,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12379773192790128901&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, San Diego; Google Brain; University of California, San Diego",
        "aff_domain": "ucsd.edu;google.com;cs.ucsd.edu",
        "email": "ucsd.edu;google.com;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;Google",
        "aff_unique_dep": ";Google Brain",
        "aff_unique_url": "https://www.ucsd.edu;https://brain.google.com",
        "aff_unique_abbr": "UCSD;Google Brain",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Associative Embedding: End-to-End Learning for Joint Detection and Grouping",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9015",
        "id": "9015",
        "author_site": "Alejandro Newell, Zhiao Huang, Jia Deng",
        "author": "Alejandro Newell; Zhiao Huang; Jia Deng",
        "abstract": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that  produces pixel-wise predictions. We show how to apply this method to  multi-person pose estimation and report state-of-the-art performance  on the MPII and MS-COCO datasets.",
        "bibtex": "@inproceedings{NIPS2017_8edd7215,\n author = {Newell, Alejandro and Huang, Zhiao and Deng, Jia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Associative Embedding: End-to-End Learning for Joint Detection and Grouping},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Reviews.html",
        "metareview": "",
        "pdf_size": 5929124,
        "gs_citation": 1187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17872365231417177678&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science and Engineering, University of Michigan, Ann Arbor, MI; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China + Computer Science and Engineering, University of Michigan, Ann Arbor, MI; Computer Science and Engineering, University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;mails.tsinghua.edu.cn;umich.edu",
        "email": "umich.edu;mails.tsinghua.edu.cn;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Michigan;Tsinghua University",
        "aff_unique_dep": "Computer Science and Engineering;Institute for Interdisciplinary Information Sciences",
        "aff_unique_url": "https://www.umich.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UM;Tsinghua",
        "aff_campus_unique_index": "0;1+0;0",
        "aff_campus_unique": "Ann Arbor;Beijing",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Asynchronous Coordinate Descent under More Realistic Assumptions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9389",
        "id": "9389",
        "author_site": "Tao Sun, Robert Hannah, Wotao Yin",
        "author": "Tao Sun; Robert Hannah; Wotao Yin",
        "abstract": "Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However, our understanding of these algorithms is limited because the current convergence theory of asynchronous block coordinate descent algorithms is based on somewhat unrealistic assumptions. In particular, the age of the shared optimization variables being used to update blocks is assumed to be independent of the block being updated. Additionally, it is assumed that the updates are applied to randomly chosen blocks.  In this paper, we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions, in particular, always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available, we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex, weakly convex, and strongly convex functions. The convergence theory involves a Lyapunov function that directly incorporates both objective progress and delays. A continuous-time ODE is provided to motivate the construction at a high level.",
        "bibtex": "@inproceedings{NIPS2017_fb2606a5,\n author = {Sun, Tao and Hannah, Robert and Yin, Wotao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Asynchronous Coordinate Descent under More Realistic Assumptions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fb2606a5068901da92473666256e6e5b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fb2606a5068901da92473666256e6e5b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fb2606a5068901da92473666256e6e5b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fb2606a5068901da92473666256e6e5b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fb2606a5068901da92473666256e6e5b-Reviews.html",
        "metareview": "",
        "pdf_size": 362885,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7596761201870206295&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "National University of Defense Technology; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "163.com;math.ucla.edu;math.ucla.edu",
        "email": "163.com;math.ucla.edu;math.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fb2606a5068901da92473666256e6e5b-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "National University of Defense Technology;University of California, Los Angeles",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nudt.edu.cn/;https://www.ucla.edu",
        "aff_unique_abbr": "NUDT;UCLA",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Asynchronous Parallel Coordinate Minimization for MAP Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9347",
        "id": "9347",
        "author_site": "Ofer Meshi, Alex Schwing",
        "author": "Ofer Meshi; Alexander Schwing",
        "abstract": "Finding the maximum a-posteriori (MAP) assignment is a central task in graphical models. Since modern applications give rise to very large problem instances, there is increasing need for efficient solvers. In this work we propose to improve the efficiency of coordinate-minimization-based dual-decomposition solvers by running their updates asynchronously in parallel. In this case message-passing inference is performed by multiple processing units simultaneously without coordination, all reading and writing to shared memory. We analyze the convergence properties of the resulting algorithms and identify settings where speedup gains can be expected. Our numerical evaluations show that this approach indeed achieves significant speedups in common computer vision tasks.",
        "bibtex": "@inproceedings{NIPS2017_5dc126b5,\n author = {Meshi, Ofer and Schwing, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Asynchronous Parallel Coordinate Minimization for MAP Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dc126b503e374b0e08231344a7f493f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5dc126b503e374b0e08231344a7f493f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5dc126b503e374b0e08231344a7f493f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5dc126b503e374b0e08231344a7f493f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5dc126b503e374b0e08231344a7f493f-Reviews.html",
        "metareview": "",
        "pdf_size": 851988,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9845623149747702006&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Google; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign",
        "aff_domain": "google.com;illinois.edu",
        "email": "google.com;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5dc126b503e374b0e08231344a7f493f-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.google.com;https://illinois.edu",
        "aff_unique_abbr": "Google;UIUC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9446",
        "id": "9446",
        "author_site": "Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi",
        "author": "Ritambhara Singh; Jack Lanchantin; Arshdeep Sekhon; Yanjun Qi",
        "abstract": "The past decade has seen a revolution in genomic technologies that enabled a flood of genome-wide profiling of chromatin marks.  Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements.  Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what are the relevant factors and how they work together.  Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach; AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long Short-Term Memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in human. Not only is the proposed architecture more accurate, but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map.",
        "bibtex": "@inproceedings{NIPS2017_d594b1a9,\n author = {Singh, Ritambhara and Lanchantin, Jack and Sekhon, Arshdeep and Qi, Yanjun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Reviews.html",
        "metareview": "",
        "pdf_size": 1018994,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10223105086359261714&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia",
        "aff_domain": "virginia.edu;virginia.edu;virginia.edu;virginia.edu",
        "email": "virginia.edu;virginia.edu;virginia.edu;virginia.edu",
        "github": "",
        "project": "www.deepchrome.org",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d594b1a945b5d645e59e21f88bd2d83b-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Virginia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.virginia.edu",
        "aff_unique_abbr": "UVA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Attention is All you Need",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9372",
        "id": "9372",
        "author_site": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin",
        "author": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin",
        "abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "bibtex": "@inproceedings{NIPS2017_3f5ee243,\n author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \\L ukasz and Polosukhin, Illia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Attention is All you Need},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Reviews.html",
        "metareview": "",
        "pdf_size": 569417,
        "gs_citation": 184033,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2960712678066186980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 73,
        "aff": "Google Brain; Google Brain; Google Research; Google Research; Google Research; University of Toronto + Google Brain; Google Brain; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;cs.toronto.edu;google.com;gmail.com",
        "email": "google.com;google.com;google.com;google.com;google.com;cs.toronto.edu;google.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;1+0;0;0",
        "aff_unique_norm": "Google;University of Toronto",
        "aff_unique_dep": "Google Brain;",
        "aff_unique_url": "https://brain.google.com;https://www.utoronto.ca",
        "aff_unique_abbr": "Google Brain;U of T",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;1+0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Attentional Pooling for Action Recognition",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8801",
        "id": "8801",
        "author_site": "Rohit Girdhar, Deva Ramanan",
        "author": "Rohit Girdhar; Deva Ramanan",
        "abstract": "We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.",
        "bibtex": "@inproceedings{NIPS2017_67c6a1e7,\n author = {Girdhar, Rohit and Ramanan, Deva},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Attentional Pooling for Action Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Reviews.html",
        "metareview": "",
        "pdf_size": 7100328,
        "gs_citation": 447,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2820657376601602725&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "http://rohitgirdhar.github.io/AttentionalPoolingAction",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Avoiding Discrimination through Causal Reasoning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8861",
        "id": "8861",
        "author_site": "Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, Bernhard Sch\u00f6lkopf",
        "author": "Niki Kilbertus; Mateo Rojas Carulla; Giambattista Parascandolo; Moritz Hardt; Dominik Janzing; Bernhard Sch\u00f6lkopf",
        "abstract": "Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively.  Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from \"What is the right fairness criterion?\" to \"What do we want to assume about our model of the causal data generating process?\" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.",
        "bibtex": "@inproceedings{NIPS2017_f5f8590c,\n author = {Kilbertus, Niki and Rojas Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Avoiding Discrimination through Causal Reasoning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Reviews.html",
        "metareview": "",
        "pdf_size": 198061,
        "gs_citation": 792,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1456063515469711783&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Max Planck Institute for Intelligent Systems+University of Cambridge; Max Planck Institute for Intelligent Systems+University of Cambridge; Max Planck Institute for Intelligent Systems+Max Planck ETH Center for Learning Systems; University of California, Berkeley; Max Planck Institute for Intelligent Systems; Max Planck Institute for Intelligent Systems",
        "aff_domain": "tue.mpg.de;tue.mpg.de;tue.mpg.de;berkeley.edu;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de;tue.mpg.de;berkeley.edu;tue.mpg.de;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+2;3;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Cambridge;Max Planck ETH Center for Learning Systems;University of California, Berkeley",
        "aff_unique_dep": "Intelligent Systems;;Center for Learning Systems;",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.cam.ac.uk;https://learning-systems.org;https://www.berkeley.edu",
        "aff_unique_abbr": "MPI-IS;Cambridge;;UC Berkeley",
        "aff_campus_unique_index": "1;1;;2",
        "aff_campus_unique": ";Cambridge;Berkeley",
        "aff_country_unique_index": "0+1;0+1;0+2;3;0;0",
        "aff_country_unique": "Germany;United Kingdom;Switzerland;United States"
    },
    {
        "title": "Balancing information exposure in social networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9243",
        "id": "9243",
        "author_site": "Kiran Garimella, Aristides Gionis, Nikos Parotsidis, Nikolaj Tatti",
        "author": "Kiran Garimella; Aristides Gionis; Nikos Parotsidis; Nikolaj Tatti",
        "abstract": "Social media has brought a revolution on how people are consuming news. Beyond the undoubtedly large number of advantages brought by social-media platforms, a point of criticism has been the creation of echo chambers and filter bubbles, caused by social homophily and algorithmic personalization.  In this paper we address the problem of balancing the information exposure} in a social network. We assume that two opposing campaigns (or viewpoints) are present in the network, and that network nodes have different preferences towards these campaigns. Our goal is to find two sets of nodes to employ in the respective campaigns, so that the overall information exposure for the two campaigns is balanced. We formally define the problem, characterize its hardness, develop approximation algorithms, and present experimental evaluation results.  Our model is inspired by the literature on influence maximization, but we offer significant novelties. First, balance of information exposure is modeled by a symmetric difference function, which is neither monotone nor submodular, and thus, not amenable to existing approaches. Second, while previous papers consider a setting with selfish agents and provide bounds on best response strategies (i.e., move of the last player), we consider a setting with a centralized agent and provide bounds for a global objective function.",
        "bibtex": "@inproceedings{NIPS2017_fc79250f,\n author = {Garimella, Kiran and Gionis, Aristides and Parotsidis, Nikos and Tatti, Nikolaj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Balancing information exposure in social networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fc79250f8c5b804390e8da280b4cf06e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fc79250f8c5b804390e8da280b4cf06e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fc79250f8c5b804390e8da280b4cf06e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fc79250f8c5b804390e8da280b4cf06e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fc79250f8c5b804390e8da280b4cf06e-Reviews.html",
        "metareview": "",
        "pdf_size": 268175,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15845890878616021991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Aalto University & HIIT; Aalto University & HIIT; University of Rome Tor Vergata; Aalto University & HIIT",
        "aff_domain": "aalto.fi;aalto.fi;uniroma2.it;aalto.fi",
        "email": "aalto.fi;aalto.fi;uniroma2.it;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fc79250f8c5b804390e8da280b4cf06e-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Aalto University & HIIT;University of Rome Tor Vergata",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.uniroma2.it",
        "aff_unique_abbr": ";UniRoma2",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tor Vergata",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Italy"
    },
    {
        "title": "Bandits Dueling on Partially Ordered Sets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9001",
        "id": "9001",
        "author_site": "Julien Audiffren, Liva Ralaivola",
        "author": "Julien Audiffren; Liva Ralaivola",
        "abstract": "We address the problem of dueling bandits defined on partially ordered sets, or posets.  In this setting, arms may not be comparable, and there may be several (incomparable) optimal arms.  We propose an algorithm, UnchainedBandits, that efficiently finds the set of optimal arms, or Pareto front, of any poset  even when pairs of comparable arms cannot be a priori distinguished from pairs of incomparable arms,   with a set of minimal assumptions. This means that UnchainedBandits does not require information about comparability and can be used with limited knowledge of the poset. To achieve this, the algorithm relies on the concept of decoys, which stems from social psychology.   We also provide theoretical guarantees on both the regret incurred and the number of comparison required by UnchainedBandits, and we report   compelling empirical results.",
        "bibtex": "@inproceedings{NIPS2017_99566564,\n author = {Audiffren, Julien and Ralaivola, Liva},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bandits Dueling on Partially Ordered Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/995665640dc319973d3173a74a03860c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/995665640dc319973d3173a74a03860c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/995665640dc319973d3173a74a03860c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/995665640dc319973d3173a74a03860c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/995665640dc319973d3173a74a03860c-Reviews.html",
        "metareview": "",
        "pdf_size": 1303168,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16639154099467584350&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "CMLA, ENS Paris-Saclay, CNRS, Universit \u00b4e Paris-Saclay, France; Lab. Informatique Fondamentale de Marseille, CNRS, Aix Marseille University, Institut Universitaire de France, F-13288 Marseille Cedex 9, France",
        "aff_domain": "gmail.com;lif.univ-mrs.fr",
        "email": "gmail.com;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/995665640dc319973d3173a74a03860c-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "CMLA, ENS Paris-Saclay, CNRS, Universit \u00b4e Paris-Saclay, France;Lab. Informatique Fondamentale de Marseille, CNRS, Aix Marseille University, Institut Universitaire de France, F-13288 Marseille Cedex 9, France",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8983",
        "id": "8983",
        "author": "Sergey Ioffe",
        "abstract": "Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small  or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.",
        "bibtex": "@inproceedings{NIPS2017_c54e7837,\n author = {Ioffe, Sergey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c54e7837e0cd0ced286cb5995327d1ab-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c54e7837e0cd0ced286cb5995327d1ab-Reviews.html",
        "metareview": "",
        "pdf_size": 598486,
        "gs_citation": 674,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7372640422096944906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google",
        "aff_domain": "google.com",
        "email": "google.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c54e7837e0cd0ced286cb5995327d1ab-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bayesian Compression for Deep Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9113",
        "id": "9113",
        "author_site": "Christos Louizos, Karen Ullrich, Max Welling",
        "author": "Christos Louizos; Karen Ullrich; Max Welling",
        "abstract": "Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.",
        "bibtex": "@inproceedings{NIPS2017_69d1fc78,\n author = {Louizos, Christos and Ullrich, Karen and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Compression for Deep Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Reviews.html",
        "metareview": "",
        "pdf_size": 412709,
        "gs_citation": 604,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12642526032245768258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Amsterdam + TNO Intelligent Imaging; University of Amsterdam; University of Amsterdam + CIFAR*",
        "aff_domain": "uva.nl;uva.nl;uva.nl",
        "email": "uva.nl;uva.nl;uva.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html",
        "aff_unique_index": "0+1;0;0+2",
        "aff_unique_norm": "University of Amsterdam;TNO;CIFAR*",
        "aff_unique_dep": ";Intelligent Imaging;",
        "aff_unique_url": "https://www.uva.nl;https://www.tno.nl;",
        "aff_unique_abbr": "UvA;TNO;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Netherlands;"
    },
    {
        "title": "Bayesian Dyadic Trees and Histograms for  Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8997",
        "id": "8997",
        "author_site": "St\u00e9phanie van der Pas, Veronika Rockova",
        "author": "St\u00e9phanie van der Pas; Veronika Ro\u010dkov\u00e1",
        "abstract": "Many machine learning  tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance.    In this work,  we shed light on the machinery behind Bayesian variants of these methods.  In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor.   We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at a near-minimax rate. These results {\\sl do not} require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location  of jumps. Our results constitute the first step towards  understanding why Bayesian trees and their ensembles have worked so well in practice.  As an aside, we discuss prior distributions  on balanced interval partitions and how they relate to an old  problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.",
        "bibtex": "@inproceedings{NIPS2017_e0040614,\n author = {van der Pas, St\\'{e}phanie and Ro\\v{c}kov\\'{a}, Veronika},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Dyadic Trees and Histograms for  Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e00406144c1e7e35240afed70f34166a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e00406144c1e7e35240afed70f34166a-Reviews.html",
        "metareview": "",
        "pdf_size": 402326,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3847726848602413647&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Mathematical Institute, Leiden University, Leiden, The Netherlands; Booth School of Business, University of Chicago, Chicago, IL, 60637",
        "aff_domain": "math.leidenuniv.nl;ChicagoBooth.edu",
        "email": "math.leidenuniv.nl;ChicagoBooth.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e00406144c1e7e35240afed70f34166a-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Mathematical Institute, Leiden University, Leiden, The Netherlands;Booth School of Business, University of Chicago, Chicago, IL, 60637",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Bayesian GAN",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9145",
        "id": "9145",
        "author_site": "Yunus Saatci, Andrew Wilson",
        "author": "Yunus Saatci; Andrew G Wilson",
        "abstract": "Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood.  We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs.  Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks.  The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.",
        "bibtex": "@inproceedings{NIPS2017_312351bf,\n author = {Saatci, Yunus and Wilson, Andrew G},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian GAN},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/312351bff07989769097660a56395065-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/312351bff07989769097660a56395065-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/312351bff07989769097660a56395065-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/312351bff07989769097660a56395065-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/312351bff07989769097660a56395065-Reviews.html",
        "metareview": "",
        "pdf_size": 1097640,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8951478904508385000&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Uber AI Labs; Cornell University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/312351bff07989769097660a56395065-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Uber;Cornell University",
        "aff_unique_dep": "Uber AI Labs;",
        "aff_unique_url": "https://www.uber.com;https://www.cornell.edu",
        "aff_unique_abbr": "Uber AI Labs;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bayesian GANs",
        "author": "Yunus Saatci, Andrew Wilson",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10070",
        "id": "10070"
    },
    {
        "title": "Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9126",
        "id": "9126",
        "author_site": "Ahmed M. Alaa, Mihaela van der Schaar",
        "author": "Ahmed M. Alaa; Mihaela van der Schaar",
        "abstract": "Predicated on the increasing abundance of electronic health records, we investigate the problem of inferring individualized treatment effects using observational data. Stemming from the potential outcomes model, we propose a novel multi-task learning framework in which factual and counterfactual outcomes are modeled as the outputs of a function in a vector-valued reproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian method for learning the treatment effects using a multi-task Gaussian process (GP) with a linear coregionalization kernel as a prior over the vvRKHS. The Bayesian approach allows us to compute individualized measures of confidence in our estimates via pointwise credible intervals, which are crucial for realizing the full potential of precision medicine. The impact of selection bias is alleviated via a risk-based empirical Bayes method for adapting the multi-task GP prior, which jointly minimizes the empirical error in factual outcomes and the uncertainty in (unobserved) counterfactual outcomes. We conduct experiments on observational datasets for an interventional social program applied to premature infants, and a left ventricular assist device applied to cardiac patients wait-listed for a heart transplant. In both experiments, we show that our method significantly outperforms the state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2017_6a508a60,\n author = {Alaa, Ahmed M. and van der Schaar, Mihaela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6a508a60aa3bf9510ea6acb021c94b48-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6a508a60aa3bf9510ea6acb021c94b48-Reviews.html",
        "metareview": "",
        "pdf_size": 373919,
        "gs_citation": 383,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16188147846783994283&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Electrical Engineering Department, University of California, Los Angeles; Department of Engineering Science, University of Oxford",
        "aff_domain": "ucla.edu;eng.ox.ac.uk",
        "email": "ucla.edu;eng.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6a508a60aa3bf9510ea6acb021c94b48-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Electrical Engineering Department, University of California, Los Angeles;University of Oxford",
        "aff_unique_dep": ";Department of Engineering Science",
        "aff_unique_url": ";https://www.ox.ac.uk",
        "aff_unique_abbr": ";Oxford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Oxford",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "title": "Bayesian Optimization with Gradients",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9302",
        "id": "9302",
        "author_site": "Jian Wu, Matthias Poloczek, Andrew Wilson, Peter Frazier",
        "author": "Jian Wu; Matthias Poloczek; Andrew G Wilson; Peter Frazier",
        "abstract": "Bayesian optimization has shown success in global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. dKG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the dKG acquisition function and its gradient using a novel fast discretization-free technique. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.",
        "bibtex": "@inproceedings{NIPS2017_64a08e5f,\n author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Optimization with Gradients},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Reviews.html",
        "metareview": "",
        "pdf_size": 806513,
        "gs_citation": 307,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13595185504259297650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html"
    },
    {
        "title": "Best Response Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8941",
        "id": "8941",
        "author_site": "Omer Ben-Porat, Moshe Tennenholtz",
        "author": "Omer Ben-Porat; Moshe Tennenholtz",
        "abstract": "In a regression task, a predictor is given a set of instances, along with a real value for each point. Subsequently, she has to identify the value of a new instance as accurately as possible. In this work, we initiate the study of strategic predictions in machine learning.  We consider a regression task tackled by two players, where the payoff of each player is the proportion of the points she predicts more accurately than the other player. We first revise the probably approximately correct learning framework to deal with the case of a duel between two predictors.  We then devise an algorithm which finds a linear regression predictor that is a best response to any (not necessarily linear) regression algorithm. We show that it has linearithmic sample complexity, and polynomial time complexity when the dimension of the instances domain is fixed. We also test our approach in a high-dimensional setting, and show it significantly defeats classical regression algorithms in the prediction duel. Together, our work introduces a novel machine learning task that lends itself well to current competitive online settings, provides its theoretical foundations, and illustrates its applicability.",
        "bibtex": "@inproceedings{NIPS2017_1ce927f8,\n author = {Ben-Porat, Omer and Tennenholtz, Moshe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Best Response Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Reviews.html",
        "metareview": "",
        "pdf_size": 285833,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17670056835243432236&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Technion - Israel Institute of Technology; Technion - Israel Institute of Technology",
        "aff_domain": "campus.technion.ac.il;ie.technion.ac.il",
        "email": "campus.technion.ac.il;ie.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8828",
        "id": "8828",
        "author_site": "Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra",
        "author": "Jiasen Lu; Anitha Kannan; Jianwei Yang; Devi Parikh; Dhruv Batra",
        "abstract": "We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses like \"I don't know\", \"I can't tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users.   Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, a RNN is augmented with a sequence of GS samplers, which coupled with the straight-through gradient estimator enables end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). The source code can be downloaded from https://github.com/jiasenlu/visDial.pytorch",
        "bibtex": "@inproceedings{NIPS2017_077e29b1,\n author = {Lu, Jiasen and Kannan, Anitha and Yang, Jianwei and Parikh, Devi and Batra, Dhruv},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/077e29b11be80ab57e1a2ecabb7da330-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/077e29b11be80ab57e1a2ecabb7da330-Reviews.html",
        "metareview": "",
        "pdf_size": 961739,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1801945036325317931&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Georgia Institute of Technology; Curai; Georgia Institute of Technology + Facebook AI Research; Georgia Institute of Technology + Facebook AI Research; Georgia Institute of Technology + Facebook AI Research",
        "aff_domain": "gatech.edu;curai.com;gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;curai.com;gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html",
        "aff_unique_index": "0;1;0+2;0+2;0+2",
        "aff_unique_norm": "Georgia Institute of Technology;Curai;Facebook",
        "aff_unique_dep": ";;Facebook AI Research",
        "aff_unique_url": "https://www.gatech.edu;https://www.curai.com;https://research.facebook.com",
        "aff_unique_abbr": "Georgia Tech;Curai;FAIR",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9077",
        "id": "9077",
        "author_site": "Sirui Yao, Bert Huang",
        "author": "Sirui Yao; Bert Huang",
        "abstract": "We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.",
        "bibtex": "@inproceedings{NIPS2017_e6384711,\n author = {Yao, Sirui and Huang, Bert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Parity: Fairness Objectives for Collaborative Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Reviews.html",
        "metareview": "",
        "pdf_size": 288011,
        "gs_citation": 477,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5034533749881188092&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Virginia Tech; Department of Computer Science, Virginia Tech",
        "aff_domain": "vt.edu;vt.edu",
        "email": "vt.edu;vt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Virginia Tech",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.vt.edu",
        "aff_unique_abbr": "VT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9252",
        "id": "9252",
        "author_site": "Omar El Housni, Vineet Goyal",
        "author": "Omar El Housni; Vineet Goyal",
        "abstract": "Affine policies (or control) are widely used as a solution approach in dynamic optimization where computing an optimal adjustable solution is usually intractable. While the worst case performance of affine policies can be significantly bad, the empirical performance is observed to be near-optimal for a large class of problem instances. For instance, in the two-stage dynamic robust optimization problem with linear covering constraints and uncertain right hand side, the worst-case approximation bound for affine policies is $O(\\sqrt m)$ that is also tight (see Bertsimas and Goyal (2012)), whereas observed empirical performance is near-optimal. In this paper, we aim to address this stark-contrast between the worst-case and the empirical performance of affine policies. In particular, we  show that affine policies give a good approximation for the two-stage adjustable robust optimization problem with high probability on random instances where the constraint coefficients are generated i.i.d. from a large class of distributions; thereby, providing a theoretical justification of the observed empirical performance. On the other hand, we also present a distribution such that the performance bound for affine policies on instances generated according to that distribution is $\\Omega(\\sqrt m)$ with high probability; however, the constraint coefficients are not i.i.d.. This demonstrates that the empirical performance of affine policies can depend on the generative model for instances.",
        "bibtex": "@inproceedings{NIPS2017_01a06836,\n author = {El Housni, Omar and Goyal, Vineet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/01a0683665f38d8e5e567b3b15ca98bf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/01a0683665f38d8e5e567b3b15ca98bf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/01a0683665f38d8e5e567b3b15ca98bf-Reviews.html",
        "metareview": "",
        "pdf_size": 393232,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16607501504927745826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "IEOR Department, Columbia University; IEOR Department, Columbia University",
        "aff_domain": "columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/01a0683665f38d8e5e567b3b15ca98bf-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "IEOR Department",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9023",
        "id": "9023",
        "author_site": "Rebecca Morrison, Ricardo Baptista, Youssef Marzouk",
        "author": "Rebecca Morrison; Ricardo Baptista; Youssef Marzouk",
        "abstract": "We present an algorithm to identify sparse dependence structure in continuous and non-Gaussian probability distributions, given a corresponding set of data. The conditional independence structure of an arbitrary distribution can be represented as an undirected graph (or Markov random field), but most algorithms for learning this structure are restricted to the discrete or Gaussian cases. Our new approach allows for more realistic and accurate descriptions of the distribution in question, and in turn better estimates of its sparse Markov structure. Sparsity in the graph is of interest as it can accelerate inference, improve sampling methods, and reveal important dependencies between variables. The algorithm relies on exploiting the connection between the sparsity of the graph and the sparsity of transport maps, which deterministically couple one probability measure to another.",
        "bibtex": "@inproceedings{NIPS2017_ea8fcd92,\n author = {Morrison, Rebecca and Baptista, Ricardo and Marzouk, Youssef},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ea8fcd92d59581717e06eb187f10666d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ea8fcd92d59581717e06eb187f10666d-Reviews.html",
        "metareview": "",
        "pdf_size": 454806,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4363467823228966240&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "MIT; MIT; MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Boltzmann Exploration Done Right",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9399",
        "id": "9399",
        "author_site": "Nicol\u00f2 Cesa-Bianchi, Claudio Gentile, Gergely Neu, Gabor Lugosi",
        "author": "Nicol\u00f2 Cesa-Bianchi; Claudio Gentile; Gabor Lugosi; Gergely Neu",
        "abstract": "Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions for the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon $T$ and the suboptimality gap $\\Delta$). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order $\\frac{K\\log^2 T}{\\Delta}$ and a distribution-independent bound of order $\\sqrt{KT}\\log K$ without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.",
        "bibtex": "@inproceedings{NIPS2017_b299ad86,\n author = {Cesa-Bianchi, Nicol\\`{o} and Gentile, Claudio and Lugosi, Gabor and Neu, Gergely},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Boltzmann Exploration Done Right},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b299ad862b6f12cb57679f0538eca514-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b299ad862b6f12cb57679f0538eca514-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b299ad862b6f12cb57679f0538eca514-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b299ad862b6f12cb57679f0538eca514-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b299ad862b6f12cb57679f0538eca514-Reviews.html",
        "metareview": "",
        "pdf_size": 363654,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13672967594200410602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Universit\u00e0 degli Studi di Milano, Milan, Italy; INRIA Lille \u2013 Nord Europe, Villeneuve d\u2019Ascq, France; ICREA & Universitat Pompeu Fabra, Barcelona, Spain; Universitat Pompeu Fabra, Barcelona, Spain",
        "aff_domain": "unimi.it;gmail.com;gmail.com;gmail.com",
        "email": "unimi.it;gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b299ad862b6f12cb57679f0538eca514-Abstract.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Milano;INRIA Lille \u2013 Nord Europe, Villeneuve d\u2019Ascq, France;Universitat Pompeu Fabra",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unimi.it;;https://www.upf.edu",
        "aff_unique_abbr": "UniMi;;UPF",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "Milan;;Barcelona",
        "aff_country_unique_index": "0;2;2",
        "aff_country_unique": "Italy;;Spain"
    },
    {
        "title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8804",
        "id": "8804",
        "author_site": "Fabian Pedregosa, R\u00e9mi Leblond, Simon Lacoste-Julien",
        "author": "Fabian Pedregosa; R\u00e9mi Leblond; Simon Lacoste-Julien",
        "abstract": "Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints.   In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.",
        "bibtex": "@inproceedings{NIPS2017_072b030b,\n author = {Pedregosa, Fabian and Leblond, R\\'{e}mi and Lacoste-Julien, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/072b030ba126b2f4b2374f342be9ed44-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/072b030ba126b2f4b2374f342be9ed44-Reviews.html",
        "metareview": "",
        "pdf_size": 485036,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11509894414082163596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "INRIA/ENS*; INRIA/ENS*; MILA and DIRO",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "INRIA/ENS*;MILA and DIRO",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9375",
        "id": "9375",
        "author_site": "Zhan Shi, Xinhua Zhang, Yaoliang Yu",
        "author": "Zhan Shi; Xinhua Zhang; Yaoliang Yu",
        "abstract": "Adversarial machines, where a learner competes against an adversary, have regained much recent interest in machine learning. They are naturally in the form of saddle-point optimization, often with separable structure but sometimes also with unmanageably large dimension. In this work we show that adversarial prediction under multivariate losses can be solved much faster than they used to be. We first reduce the problem size exponentially by using appropriate sufficient statistics, and then we adapt the new stochastic variance-reduced algorithm of Balamurugan & Bach (2016) to allow any Bregman divergence. We prove that the same linear rate of convergence is retained and we show that for adversarial prediction using KL-divergence we can further achieve a speedup of #example times compared with the Euclidean alternative. We verify the theoretical findings through extensive experiments on two example applications: adversarial prediction and LPboosting.",
        "bibtex": "@inproceedings{NIPS2017_b6e71087,\n author = {Shi, Zhan and Zhang, Xinhua and Yu, Yaoliang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b6e710870acb098e584277457ba89d68-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b6e710870acb098e584277457ba89d68-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b6e710870acb098e584277457ba89d68-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b6e710870acb098e584277457ba89d68-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b6e710870acb098e584277457ba89d68-Reviews.html",
        "metareview": "",
        "pdf_size": 428102,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11164168207348088596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois at Chicago; University of Illinois at Chicago; University of Waterloo",
        "aff_domain": "uic.edu;uic.edu;uwaterloo.ca",
        "email": "uic.edu;uic.edu;uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b6e710870acb098e584277457ba89d68-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Illinois at Chicago;University of Waterloo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uic.edu;https://uwaterloo.ca",
        "aff_unique_abbr": "UIC;UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9063",
        "id": "9063",
        "author_site": "Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans",
        "author": "Ofir Nachum; Mohammad Norouzi; Kelvin Xu; Dale Schuurmans",
        "abstract": "We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.",
        "bibtex": "@inproceedings{NIPS2017_facf9f74,\n author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/facf9f743b083008a894eee7baa16469-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/facf9f743b083008a894eee7baa16469-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/facf9f743b083008a894eee7baa16469-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/facf9f743b083008a894eee7baa16469-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/facf9f743b083008a894eee7baa16469-Reviews.html",
        "metareview": "",
        "pdf_size": 867949,
        "gs_citation": 589,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10627544355568771547&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Google Brain; Google Brain; Google Brain; Google Brain + University of Alberta",
        "aff_domain": "google.com;google.com;google.com;ualberta.ca",
        "email": "google.com;google.com;google.com;ualberta.ca",
        "github": "https://github.com/tensorflow/models/tree/master/research/pcl_rl",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/facf9f743b083008a894eee7baa16469-Abstract.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Google;University of Alberta",
        "aff_unique_dep": "Google Brain;",
        "aff_unique_url": "https://brain.google.com;https://www.ualberta.ca",
        "aff_unique_abbr": "Google Brain;UAlberta",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9308",
        "id": "9308",
        "author_site": "Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu",
        "author": "Xiangru Lian; Ce Zhang; Huan Zhang; Cho-Jui Hsieh; Wei Zhang; Ji Liu",
        "abstract": "Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart?  Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.",
        "bibtex": "@inproceedings{NIPS2017_f7552665,\n author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Reviews.html",
        "metareview": "",
        "pdf_size": 1409325,
        "gs_citation": 1511,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18334580252706636011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f75526659f31040afeb61cb7133e4e6d-Abstract.html"
    },
    {
        "title": "Causal Effect Inference with Deep Latent-Variable Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9414",
        "id": "9414",
        "author_site": "Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, Max Welling",
        "author": "Christos Louizos; Uri Shalit; Joris M. Mooij; David Sontag; Richard Zemel; Max Welling",
        "abstract": "Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.",
        "bibtex": "@inproceedings{NIPS2017_94b5bde6,\n author = {Louizos, Christos and Shalit, Uri and Mooij, Joris M and Sontag, David and Zemel, Richard and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Causal Effect Inference with Deep Latent-Variable Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Reviews.html",
        "metareview": "",
        "pdf_size": 452522,
        "gs_citation": 972,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12689753489590723797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "University of Amsterdam+TNO Intelligent Imaging; New York University+CIMS; University of Amsterdam; Massachusetts Institute of Technology+CSAIL & IMES; University of Toronto+CIFAR\u2217; University of Amsterdam+CIFAR\u2217",
        "aff_domain": "uva.nl;nyu.edu;uva.nl;mit.edu;cs.toronto.edu;uva.nl",
        "email": "uva.nl;nyu.edu;uva.nl;mit.edu;cs.toronto.edu;uva.nl",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/94b5bde6de888ddf9cde6748ad2523d1-Abstract.html",
        "aff_unique_index": "0+1;2+3;0;4+5;6+7;0+7",
        "aff_unique_norm": "University of Amsterdam;TNO;New York University;CIMS;Massachusetts Institute of Technology;CSAIL & IMES;University of Toronto;CIFAR\u2217",
        "aff_unique_dep": ";Intelligent Imaging;;;;;;",
        "aff_unique_url": "https://www.uva.nl;https://www.tno.nl;https://www.nyu.edu;;https://web.mit.edu;;https://www.utoronto.ca;",
        "aff_unique_abbr": "UvA;TNO;NYU;;MIT;;U of T;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0;1;3;0",
        "aff_country_unique": "Netherlands;United States;;Canada"
    },
    {
        "title": "Certified Defenses for Data Poisoning Attacks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9135",
        "id": "9135",
        "author_site": "Jacob Steinhardt, Pang Wei Koh, Percy Liang",
        "author": "Jacob Steinhardt; Pang Wei W Koh; Percy Liang",
        "abstract": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family  of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for  statistical concentration between train and test error to hold, and (2) that outliers  within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.",
        "bibtex": "@inproceedings{NIPS2017_9d7311ba,\n author = {Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Certified Defenses for Data Poisoning Attacks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Reviews.html",
        "metareview": "",
        "pdf_size": 1283554,
        "gs_citation": 1010,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10474944557566613837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9d7311ba459f9e45ed746755a32dcd11-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9278",
        "id": "9278",
        "author_site": "Andrei-Cristian Barbos, Francois Caron, Jean-Fran\u00e7ois Giovannelli, Arnaud Doucet",
        "author": "Andrei-Cristian Barbos; Francois Caron; Jean-Fran\u00e7ois Giovannelli; Arnaud Doucet",
        "abstract": "We propose a generalized Gibbs sampler algorithm for obtaining samples approximately distributed from a high-dimensional Gaussian distribution. Similarly to Hogwild methods, our approach does not target the original Gaussian distribution of interest, but an approximation to it. Contrary to Hogwild methods, a single parameter allows us to trade bias for variance. We show empirically that our method is very flexible and performs well compared to Hogwild-type algorithms.",
        "bibtex": "@inproceedings{NIPS2017_7876acb6,\n author = {Barbos, Andrei-Cristian and Caron, Francois and Giovannelli, Jean-Fran\\c{c}ois and Doucet, Arnaud},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7876acb66640bad41f1e1371ef30c180-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7876acb66640bad41f1e1371ef30c180-Reviews.html",
        "metareview": "",
        "pdf_size": 3811632,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9800880954694421511&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IMS Laboratory, Univ. Bordeaux - CNRS - BINP; Department of Statistics, University of Oxford; IMS Laboratory, Univ. Bordeaux - CNRS - BINP; Department of Statistics, University of Oxford",
        "aff_domain": "u-bordeaux.fr;stats.ox.ac.uk;ims-bordeaux.fr;stats.ox.ac.uk",
        "email": "u-bordeaux.fr;stats.ox.ac.uk;ims-bordeaux.fr;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "IMS Laboratory, Univ. Bordeaux - CNRS - BINP;University of Oxford",
        "aff_unique_dep": ";Department of Statistics",
        "aff_unique_url": ";https://www.ox.ac.uk",
        "aff_unique_abbr": ";Oxford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Oxford",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "title": "Clustering Billions of Reads for DNA Data Storage",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9120",
        "id": "9120",
        "author_site": "Cyrus Rashtchian, Konstantin Makarychev, Miklos Racz, Siena Ang, Djordje Jevdjic, Sergey Yekhanin, Luis Ceze, Karin Strauss",
        "author": "Cyrus Rashtchian; Konstantin Makarychev; Miklos Racz; Siena Ang; Djordje Jevdjic; Sergey Yekhanin; Luis Ceze; Karin Strauss",
        "abstract": "Storing data in synthetic DNA offers the possibility of improving information density and durability  by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.",
        "bibtex": "@inproceedings{NIPS2017_ab731488,\n author = {Rashtchian, Cyrus and Makarychev, Konstantin and Racz, Miklos and Ang, Siena and Jevdjic, Djordje and Yekhanin, Sergey and Ceze, Luis and Strauss, Karin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering Billions of Reads for DNA Data Storage},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ab7314887865c4265e896c6e209d1cd6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ab7314887865c4265e896c6e209d1cd6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ab7314887865c4265e896c6e209d1cd6-Reviews.html",
        "metareview": "",
        "pdf_size": 545614,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17372486097870747625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research + CSE at University of Washington; EECS at Northwestern University; ORFE at Princeton University; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research + CSE at University of Washington; Microsoft Research",
        "aff_domain": "microsoft.com;northwestern.edu;princeton.edu;microsoft.com;microsoft.com;microsoft.com;cs.washington.edu;microsoft.com",
        "email": "microsoft.com;northwestern.edu;princeton.edu;microsoft.com;microsoft.com;microsoft.com;cs.washington.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ab7314887865c4265e896c6e209d1cd6-Abstract.html",
        "aff_unique_index": "0+1;2;3;0;0;0;0+1;0",
        "aff_unique_norm": "Microsoft Corporation;CSE at University of Washington;EECS at Northwestern University;ORFE at Princeton University",
        "aff_unique_dep": "Microsoft Research;;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;;;",
        "aff_unique_abbr": "MSR;;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Clustering Stable Instances of Euclidean k-means.",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9419",
        "id": "9419",
        "author_site": "Aravindan Vijayaraghavan, Abhratanu Dutta, Alex Wang",
        "author": "Aravindan Vijayaraghavan; Abhratanu Dutta; Alex Wang",
        "abstract": "The Euclidean k-means problem is arguably the most widely-studied clustering problem in machine learning. While the k-means objective is NP-hard in the worst-case, practitioners have enjoyed remarkable success in applying heuristics like Lloyd's algorithm for this problem. To address this disconnect, we study the following question: what properties of real-world instances will enable us to design efficient algorithms and prove guarantees for finding the optimal clustering?   We consider a natural notion called additive perturbation stability that we believe captures many practical instances of Euclidean k-means clustering. Stable instances have unique optimal k-means solutions that does not change even when each point is perturbed a little (in Euclidean distance). This captures the property that k-means optimal solution should be tolerant to measurement errors and uncertainty in the points. We design efficient algorithms that provably recover the optimal clustering for instances that are additive perturbation stable. When the instance has some additional separation, we can design a simple, efficient algorithm with provable guarantees that is also robust to outliers. We also complement these results by studying the amount of stability in real datasets, and demonstrating that our algorithm performs well on these benchmark datasets.",
        "bibtex": "@inproceedings{NIPS2017_d54ce9de,\n author = {Vijayaraghavan, Aravindan and Dutta, Abhratanu and Wang, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering Stable Instances of Euclidean k-means.},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d54ce9de9df77c579775a7b6b1a4bdc0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d54ce9de9df77c579775a7b6b1a4bdc0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d54ce9de9df77c579775a7b6b1a4bdc0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d54ce9de9df77c579775a7b6b1a4bdc0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d54ce9de9df77c579775a7b6b1a4bdc0-Reviews.html",
        "metareview": "",
        "pdf_size": 465808,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3122676643008282238&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Northwestern University; Northwestern University; Carnegie Mellon University + Northwestern University",
        "aff_domain": "u.northwestern.edu;northwestern.edu;u.northwestern.edu",
        "email": "u.northwestern.edu;northwestern.edu;u.northwestern.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d54ce9de9df77c579775a7b6b1a4bdc0-Abstract.html",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Northwestern University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.northwestern.edu;https://www.cmu.edu",
        "aff_unique_abbr": "NU;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Clustering with Noisy Queries",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9352",
        "id": "9352",
        "author_site": "Arya Mazumdar, Barna Saha",
        "author": "Arya Mazumdar; Barna Saha",
        "abstract": "In this paper, we provide a rigorous theoretical study of clustering with noisy queries. Given a set of $n$ elements, our goal is to recover the true clustering by asking minimum number of pairwise queries to an oracle. Oracle can answer queries of the form ``do elements $u$ and $v$ belong to the same cluster?''-the queries can be asked interactively (adaptive queries), or non-adaptively up-front, but its answer can be erroneous with probability $p$. In this paper, we provide the first information theoretic lower bound on the number of queries for clustering with noisy oracle in both situations.  We design novel algorithms that closely match this query complexity lower bound, even when the number of clusters is unknown.  Moreover, we design computationally efficient algorithms both for the adaptive and non-adaptive settings. The problem captures/generalizes multiple application scenarios. It is directly motivated by the growing body of work that use crowdsourcing for {\\em entity resolution}, a fundamental and challenging data mining task aimed to identify all records in a database referring to the same entity. Here crowd represents the noisy oracle, and the number of queries directly relates to the cost of crowdsourcing. Another application comes from the problem of sign edge prediction in social network, where social interactions can be both positive and negative, and one must identify the sign of all pair-wise interactions by  querying a few pairs. Furthermore, clustering with noisy oracle is intimately connected to correlation clustering, leading to improvement therein. Finally, it introduces a new direction of study in the popular stochastic block model where one has an incomplete stochastic block model matrix to recover the clusters.",
        "bibtex": "@inproceedings{NIPS2017_db5cea26,\n author = {Mazumdar, Arya and Saha, Barna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering with Noisy Queries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Reviews.html",
        "metareview": "",
        "pdf_size": 483034,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=655167740123756991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/db5cea26ca37aa09e5365f3e7f5dd9eb-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "College of Information and Computer Sciences",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Coded Distributed Computing for Inverse Problems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8866",
        "id": "8866",
        "author_site": "Yaoqing Yang, Pulkit Grover, Soummya Kar",
        "author": "Yaoqing Yang; Pulkit Grover; Soummya Kar",
        "abstract": "Computationally intensive distributed and parallel computing is often bottlenecked by a small set of slow workers known as stragglers. In this paper, we utilize the  emerging idea of ``coded computation'' to design a novel error-correcting-code inspired technique for solving linear inverse problems under specific iterative methods in a parallelized implementation affected by stragglers. Example machine-learning applications include inverse problems such as personalized PageRank and sampling on graphs. We provably show that our coded-computation technique can reduce the mean-squared error under a computational deadline constraint. In fact, the ratio of mean-squared error of replication-based and coded techniques diverges to infinity as the deadline increases. Our experiments for personalized PageRank performed on real systems and real social networks show that this ratio can be as large as $10^4$. Further, unlike coded-computation techniques proposed thus far, our strategy combines outputs of all workers, including the stragglers, to produce more accurate estimates at the computational deadline. This also ensures that the accuracy degrades ``gracefully'' in the event that the number of stragglers is large.",
        "bibtex": "@inproceedings{NIPS2017_5ef0b4eb,\n author = {Yang, Yaoqing and Grover, Pulkit and Kar, Soummya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Coded Distributed Computing for Inverse Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Reviews.html",
        "metareview": "",
        "pdf_size": 435179,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13142772539701175281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5ef0b4eba35ab2d6180b0bca7e46b6f9-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cold-Start Reinforcement Learning with Softmax Policy Gradient",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9067",
        "id": "9067",
        "author_site": "Nan Ding, Radu Soricut",
        "author": "Nan Ding; Radu Soricut",
        "abstract": "Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.",
        "bibtex": "@inproceedings{NIPS2017_faafda66,\n author = {Ding, Nan and Soricut, Radu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cold-Start Reinforcement Learning with Softmax Policy Gradient},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/faafda66202d234463057972460c04f5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/faafda66202d234463057972460c04f5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/faafda66202d234463057972460c04f5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/faafda66202d234463057972460c04f5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/faafda66202d234463057972460c04f5-Reviews.html",
        "metareview": "",
        "pdf_size": 502437,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16871181589949142718&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Google Inc., Venice, CA 90291; Google Inc., Venice, CA 90291",
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/faafda66202d234463057972460c04f5-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google Inc., Venice, CA 90291",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Collaborative Deep Learning in Fixed Topology Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9363",
        "id": "9363",
        "author_site": "Zhanhong Jiang, Aditya Balu, Chinmay Hegde, Soumik Sarkar",
        "author": "Zhanhong Jiang; Aditya Balu; Chinmay Hegde; Soumik Sarkar",
        "abstract": "There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.",
        "bibtex": "@inproceedings{NIPS2017_a74c3bae,\n author = {Jiang, Zhanhong and Balu, Aditya and Hegde, Chinmay and Sarkar, Soumik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collaborative Deep Learning in Fixed Topology Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Reviews.html",
        "metareview": "",
        "pdf_size": 544415,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13577336808659618893&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mechanical Engineering, Iowa State University; Department of Mechanical Engineering, Iowa State University; Department of Electrical and Computer Engineering, Iowa State University; Department of Mechanical Engineering, Iowa State University",
        "aff_domain": "iastate.edu;iastate.edu;iastate.edu;iastate.edu",
        "email": "iastate.edu;iastate.edu;iastate.edu;iastate.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a74c3bae3e13616104c1b25f9da1f11f-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Iowa State University;Department of Electrical and Computer Engineering, Iowa State University",
        "aff_unique_dep": "Department of Mechanical Engineering;",
        "aff_unique_url": "https://www.iastate.edu;",
        "aff_unique_abbr": "ISU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Collaborative PAC Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9026",
        "id": "9026",
        "author_site": "Avrim Blum, Nika Haghtalab, Ariel Procaccia, Mingda Qiao",
        "author": "Avrim Blum; Nika Haghtalab; Ariel D Procaccia; Mingda Qiao",
        "abstract": "We introduce a collaborative PAC learning model, in which k players attempt to learn the same underlying concept. We ask how much more information is required to learn an accurate classifier for all players simultaneously. We refer to the ratio between the sample complexity of collaborative PAC learning and its non-collaborative (single-player) counterpart as the overhead. We design learning algorithms with O(ln(k)) and O(ln^2(k)) overhead in the personalized and centralized variants our model. This gives an exponential improvement upon the naive algorithm that does not share information among players. We complement our upper bounds with an Omega(ln(k)) overhead lower bound, showing that our results are tight up to a logarithmic factor.",
        "bibtex": "@inproceedings{NIPS2017_186a157b,\n author = {Blum, Avrim and Haghtalab, Nika and Procaccia, Ariel D and Qiao, Mingda},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collaborative PAC Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/186a157b2992e7daed3677ce8e9fe40f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/186a157b2992e7daed3677ce8e9fe40f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/186a157b2992e7daed3677ce8e9fe40f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/186a157b2992e7daed3677ce8e9fe40f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/186a157b2992e7daed3677ce8e9fe40f-Reviews.html",
        "metareview": "",
        "pdf_size": 317392,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14015828076102610405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Toyota Technological Institute at Chicago; Computer Science Department, Carnegie Mellon University; Computer Science Department, Carnegie Mellon University; Institute for Interdisciplinary Information Sciences, Tsinghua University",
        "aff_domain": "ttic.edu;cs.cmu.edu;cs.cmu.edu;mails.tsinghua.edu.cn",
        "email": "ttic.edu;cs.cmu.edu;cs.cmu.edu;mails.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Carnegie Mellon University;Tsinghua University",
        "aff_unique_dep": ";Computer Science Department;Institute for Interdisciplinary Information Sciences",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.cmu.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "TTI Chicago;CMU;Tsinghua",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Chicago;Pittsburgh;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Collapsed variational Bayes for Markov jump processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9157",
        "id": "9157",
        "author_site": "Boqian Zhang, Jiangwei Pan, Vinayak Rao",
        "author": "Boqian Zhang; Jiangwei Pan; Vinayak A Rao",
        "abstract": "Markov jump processes are continuous-time stochastic processes widely used in statistical applications in the natural sciences, and more recently in machine learning. Inference for these models typically proceeds via Markov chain Monte Carlo, and can suffer from various computational challenges. In this work, we propose a novel collapsed variational inference algorithm to address this issue. Our work leverages ideas from discrete-time Markov chains, and exploits a connection between these two through an idea called uniformization. Our algorithm proceeds by marginalizing out the parameters of the Markov jump process, and then approximating the distribution over the trajectory with a factored distribution over segments of a piecewise-constant function. Unlike MCMC schemes that marginalize out transition times of a piecewise-constant process, our scheme optimizes the discretization of time, resulting in significant computational savings. We apply our ideas to synthetic data as well as a dataset of check-in recordings, where we demonstrate superior performance over state-of-the-art MCMC methods.",
        "bibtex": "@inproceedings{NIPS2017_e0a20953,\n author = {Zhang, Boqian and Pan, Jiangwei and Rao, Vinayak A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collapsed variational Bayes for Markov jump processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e0a209539d1e74ab9fe46b9e01a19a97-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e0a209539d1e74ab9fe46b9e01a19a97-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e0a209539d1e74ab9fe46b9e01a19a97-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e0a209539d1e74ab9fe46b9e01a19a97-Reviews.html",
        "metareview": "",
        "pdf_size": 521351,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12197138601578777768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Duke University + Facebook; Department of Statistics, Purdue University; Department of Statistics, Purdue University",
        "aff_domain": "gmail.com;purdue.edu;purdue.edu",
        "email": "gmail.com;purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e0a209539d1e74ab9fe46b9e01a19a97-Abstract.html",
        "aff_unique_index": "0+1;2;2",
        "aff_unique_norm": "Duke University;Facebook, Inc.;Purdue University",
        "aff_unique_dep": "Department of Computer Science;;Department of Statistics",
        "aff_unique_url": "https://www.duke.edu;https://www.facebook.com;https://www.purdue.edu",
        "aff_unique_abbr": "Duke;FB;Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Collecting Telemetry Data Privately",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9140",
        "id": "9140",
        "author_site": "Bolin Ding, Janardhan Kulkarni, Sergey Yekhanin",
        "author": "Bolin Ding; Janardhan Kulkarni; Sergey Yekhanin",
        "abstract": "The collection and analysis of telemetry data from user's devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users' privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.",
        "bibtex": "@inproceedings{NIPS2017_253614bb,\n author = {Ding, Bolin and Kulkarni, Janardhan and Yekhanin, Sergey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collecting Telemetry Data Privately},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/253614bbac999b38b5b60cae531c4969-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/253614bbac999b38b5b60cae531c4969-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/253614bbac999b38b5b60cae531c4969-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/253614bbac999b38b5b60cae531c4969-Reviews.html",
        "metareview": "",
        "pdf_size": 4603238,
        "gs_citation": 958,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2232168025747143296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/253614bbac999b38b5b60cae531c4969-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Communication-Efficient Distributed Learning of Discrete Distributions",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9409",
        "id": "9409",
        "author_site": "Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, Ludwig Schmidt",
        "author": "Ilias Diakonikolas; Elena Grigorescu; Jerry Li; Abhiram Natarajan; Krzysztof Onak; Ludwig Schmidt",
        "abstract": "We initiate a systematic investigation of distribution learning (density estimation) when the data is distributed across multiple servers. The servers must communicate with a referee and the goal is to estimate the underlying distribution with as few bits of communication as possible. We focus on non-parametric density estimation of discrete distributions with respect to the l1 and l2 norms. We provide the first non-trivial upper and lower bounds on the communication complexity of this basic estimation task in various settings of interest. Specifically, our results include the following:   1. When the unknown discrete distribution is unstructured and each server has only one sample, we show that any blackboard protocol (i.e., any protocol in which servers interact arbitrarily using public messages) that learns the distribution must essentially communicate the entire sample.  2. For the case of structured distributions, such as k-histograms and monotone distributions, we design distributed learning algorithms that achieve significantly better communication guarantees than the naive ones, and obtain tight upper and lower bounds in several regimes. Our distributed learning algorithms run in near-linear time and are robust to model misspecification.  Our results provide insights on the interplay between structure and communication efficiency for a range of fundamental distribution estimation tasks.",
        "bibtex": "@inproceedings{NIPS2017_7486cef2,\n author = {Diakonikolas, Ilias and Grigorescu, Elena and Li, Jerry and Natarajan, Abhiram and Onak, Krzysztof and Schmidt, Ludwig},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Communication-Efficient Distributed Learning of Discrete Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7486cef2522ee03547cfb970a404a874-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7486cef2522ee03547cfb970a404a874-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7486cef2522ee03547cfb970a404a874-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7486cef2522ee03547cfb970a404a874-Reviews.html",
        "metareview": "",
        "pdf_size": 282190,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14830023029134472709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "CS, USC; CS, Purdue; EECS & CSAIL, MIT; CS, Purdue; IBM Research, NY; EECS & CSAIL, MIT",
        "aff_domain": "usc.edu;purdue.edu;mit.edu;purdue.edu;us.ibm.com;mit.edu",
        "email": "usc.edu;purdue.edu;mit.edu;purdue.edu;us.ibm.com;mit.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7486cef2522ee03547cfb970a404a874-Abstract.html",
        "aff_unique_index": "0;1;2;1;3;2",
        "aff_unique_norm": "CS, USC;CS, Purdue;Massachusetts Institute of Technology;IBM Research",
        "aff_unique_dep": ";;Electrical Engineering & Computer Science and Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": ";;https://www.mit.edu;https://www.ibm.com/research",
        "aff_unique_abbr": ";;MIT;IBM",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Cambridge;New York",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Communication-Efficient Stochastic Gradient Descent, with Applications to Neural Networks",
        "author": "Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/9992",
        "id": "9992"
    },
    {
        "title": "Compatible Reward Inverse Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8993",
        "id": "8993",
        "author_site": "Alberto Maria Metelli, Matteo Pirotta, Marcello Restelli",
        "author": "Alberto Maria Metelli; Matteo Pirotta; Marcello Restelli",
        "abstract": "Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations.  This paper is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert's reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert's policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is empirically compared to other IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian (LQG) and Car on the Hill environments.",
        "bibtex": "@inproceedings{NIPS2017_e6d8545d,\n author = {Metelli, Alberto Maria and Pirotta, Matteo and Restelli, Marcello},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compatible Reward Inverse Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Reviews.html",
        "metareview": "",
        "pdf_size": 434978,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17802798120697220472&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "DEIB, Politecnico di Milano, Italy; SequeL Team, Inria Lille, France; DEIB, Politecnico di Milano, Italy",
        "aff_domain": "polimi.it;inria.fr;polimi.it",
        "email": "polimi.it;inria.fr;polimi.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Politecnico di Milano;SequeL Team, Inria Lille, France",
        "aff_unique_dep": "DEIB;",
        "aff_unique_url": "https://www.polimi.it;",
        "aff_unique_abbr": "Politecnico di Milano;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy;"
    },
    {
        "title": "Compression-aware Training of Deep Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8880",
        "id": "8880",
        "author_site": "Jose Alvarez, Mathieu Salzmann",
        "author": "Jose M Alvarez; Mathieu Salzmann",
        "abstract": "In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.",
        "bibtex": "@inproceedings{NIPS2017_db85e259,\n author = {Alvarez, Jose M and Salzmann, Mathieu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compression-aware Training of Deep Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/db85e2590b6109813dafa101ceb2faeb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/db85e2590b6109813dafa101ceb2faeb-Reviews.html",
        "metareview": "",
        "pdf_size": 448395,
        "gs_citation": 360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13739947416357254385&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Toyota Research Institute, Los Altos, CA 94022; EPFL - CVLab, Lausanne, Switzerland",
        "aff_domain": "tri.global;epfl.ch",
        "email": "tri.global;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/db85e2590b6109813dafa101ceb2faeb-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Toyota Research Institute, Los Altos, CA 94022;EPFL - CVLab, Lausanne, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Concentration of Multilinear Functions of the Ising Model with Applications to Network Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8799",
        "id": "8799",
        "author_site": "Constantinos Daskalakis, Nishanth Dikkala, Gautam Kamath",
        "author": "Constantinos Daskalakis; Nishanth Dikkala; Gautam Kamath",
        "abstract": "We prove near-tight concentration of measure for polynomial functions of the Ising model, under high temperature, improving the radius of concentration guaranteed by known results by polynomial factors in the dimension (i.e.~the number of nodes in the Ising model). We show that our results are optimal up to logarithmic factors in the dimension. We obtain our results by extending and strengthening the exchangeable-pairs approach used to prove concentration of measure in this setting by Chatterjee. We demonstrate the efficacy of such functions as statistics for testing the strength  of interactions in social networks in both synthetic and real world data.",
        "bibtex": "@inproceedings{NIPS2017_4e732ced,\n author = {Daskalakis, Constantinos and Dikkala, Nishanth and Kamath, Gautam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Concentration of Multilinear Functions of the Ising Model with Applications to Network Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4e732ced3463d06de0ca9a15b6153677-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4e732ced3463d06de0ca9a15b6153677-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4e732ced3463d06de0ca9a15b6153677-Reviews.html",
        "metareview": "",
        "pdf_size": 344406,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9229636606771503967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "EECS & CSAIL, MIT; EECS & CSAIL, MIT; EECS & CSAIL, MIT",
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4e732ced3463d06de0ca9a15b6153677-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Electrical Engineering & Computer Science and Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Concrete Dropout",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9141",
        "id": "9141",
        "author_site": "Yarin Gal, Jiri Hron, Alex Kendall",
        "author": "Yarin Gal; Jiri Hron; Alex Kendall",
        "abstract": "Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary\u2014a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout\u2019s discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.",
        "bibtex": "@inproceedings{NIPS2017_84ddfb34,\n author = {Gal, Yarin and Hron, Jiri and Kendall, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Concrete Dropout},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/84ddfb34126fc3a48ee38d7044e87276-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/84ddfb34126fc3a48ee38d7044e87276-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/84ddfb34126fc3a48ee38d7044e87276-Reviews.html",
        "metareview": "",
        "pdf_size": 2488447,
        "gs_citation": 819,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13564852856591700203&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Cambridge and Alan Turing Institute, London; University of Cambridge; University of Cambridge",
        "aff_domain": "eng.cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "eng.cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/84ddfb34126fc3a48ee38d7044e87276-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cambridge and Alan Turing Institute, London;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cam.ac.uk",
        "aff_unique_abbr": ";Cambridge",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "title": "Conic Scan-and-Cover algorithms for nonparametric topic modeling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9169",
        "id": "9169",
        "author_site": "Mikhail Yurochkin, Aritra Guha, XuanLong Nguyen",
        "author": "Mikhail Yurochkin; Aritra Guha; Xuanlong Nguyen",
        "abstract": "We propose new algorithms for topic modeling when the number of topics is unknown. Our approach relies on an analysis of the concentration of mass and angular geometry of the topic simplex, a convex polytope constructed by taking the convex hull of vertices representing the latent topics. Our algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation, which requires the number of topics be given. Moreover, they are one of the fastest among several state of the art parametric techniques. Statistical consistency of our estimator is established under some conditions.",
        "bibtex": "@inproceedings{NIPS2017_9185f3ec,\n author = {Yurochkin, Mikhail and Guha, Aritra and Nguyen, XuanLong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Conic Scan-and-Cover algorithms for nonparametric topic modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9185f3ec501c674c7c788464a36e7fb3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9185f3ec501c674c7c788464a36e7fb3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9185f3ec501c674c7c788464a36e7fb3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9185f3ec501c674c7c788464a36e7fb3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9185f3ec501c674c7c788464a36e7fb3-Reviews.html",
        "metareview": "",
        "pdf_size": 953522,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9978433151693732795&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, University of Michigan; Department of Statistics, University of Michigan; Department of Statistics, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "https://github.com/moonfolk/Geometric-Topic-Modeling",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Conservative Contextual Linear Bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9172",
        "id": "9172",
        "author_site": "Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi, Benjamin Van Roy",
        "author": "Abbas Kazerouni; Mohammad Ghavamzadeh; Yasin Abbasi Yadkori; Benjamin Van Roy",
        "abstract": "Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized ad recommendation in online marketing. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.",
        "bibtex": "@inproceedings{NIPS2017_bdc4626a,\n author = {Kazerouni, Abbas and Ghavamzadeh, Mohammad and Abbasi Yadkori, Yasin and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Conservative Contextual Linear Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Reviews.html",
        "metareview": "",
        "pdf_size": 536601,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17201902093949963564&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University; DeepMind; Adobe Research; Stanford University",
        "aff_domain": "stanford.edu;google.com;adobe.com;stanford.edu",
        "email": "stanford.edu;google.com;adobe.com;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bdc4626aa1d1df8e14d80d345b2a442d-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Stanford University;DeepMind;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "https://www.stanford.edu;https://deepmind.com;https://research.adobe.com",
        "aff_unique_abbr": "Stanford;DeepMind;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Consistent Multitask Learning with Nonlinear Output Relations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8987",
        "id": "8987",
        "author_site": "Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, Massimiliano Pontil",
        "author": "Carlo Ciliberto; Alessandro Rudi; Lorenzo Rosasco; Massimiliano Pontil",
        "abstract": "Key to multitask learning is exploiting the relationships between different tasks to improve prediction performance. Most previous methods have focused on the case where tasks relations can be modeled as linear operators and regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related is often restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that our algorithm can be efficiently implemented and study its generalization properties, proving universal consistency and learning rates. Our theoretical analysis highlights the benefits of non-linear multitask learning over learning the tasks independently. Encouraging experimental results show the benefits of the proposed method in practice.",
        "bibtex": "@inproceedings{NIPS2017_b24d516b,\n author = {Ciliberto, Carlo and Rudi, Alessandro and Rosasco, Lorenzo and Pontil, Massimiliano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Consistent Multitask Learning with Nonlinear Output Relations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b24d516bb65a5a58079f0f3526c87c57-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b24d516bb65a5a58079f0f3526c87c57-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b24d516bb65a5a58079f0f3526c87c57-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b24d516bb65a5a58079f0f3526c87c57-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b24d516bb65a5a58079f0f3526c87c57-Reviews.html",
        "metareview": "",
        "pdf_size": 671354,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3962041502979064988&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University College London, London, UK + Istituto Italiano di Tecnologia, Genova, Italy; INRIA - Sierra Project-team and \u00c9cole Normale Sup\u00e9rieure, Paris, France + Istituto Italiano di Tecnologia, Genova, Italy; Massachusetts Institute of Technology, Cambridge, USA + Universit\u00e0 degli studi di Genova, Genova, Italy + Istituto Italiano di Tecnologia, Genova, Italy; Department of Computer Science, University College London, London, UK + Istituto Italiano di Tecnologia, Genova, Italy",
        "aff_domain": "ucl.ac.uk;inria.fr;mit.edu;ucl.ac.uk",
        "email": "ucl.ac.uk;inria.fr;mit.edu;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b24d516bb65a5a58079f0f3526c87c57-Abstract.html",
        "aff_unique_index": "0+1;2+1;3+4+1;0+1",
        "aff_unique_norm": "University College London;Istituto Italiano di Tecnologia;INRIA - Sierra Project-team and \u00c9cole Normale Sup\u00e9rieure, Paris, France;Massachusetts Institute of Technology;Universit\u00e0 degli Studi di Genova",
        "aff_unique_dep": "Department of Computer Science;;;;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.iit.it;;https://web.mit.edu;https://www.unige.it",
        "aff_unique_abbr": "UCL;IIT;;MIT;UniGe",
        "aff_campus_unique_index": "0+1;1;3+1+1;0+1",
        "aff_campus_unique": "London;Genova;;Cambridge",
        "aff_country_unique_index": "0+1;1;3+1+1;0+1",
        "aff_country_unique": "United Kingdom;Italy;;United States"
    },
    {
        "title": "Consistent Robust Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8999",
        "id": "8999",
        "author_site": "Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, Purushottam Kar",
        "author": "Kush Bhatia; Prateek Jain; Parameswaran Kamalaruban; Purushottam Kar",
        "abstract": "We present the first efficient and provably consistent estimator for the robust regression problem. The area of robust learning and optimization has generated a significant amount of interest in the learning and statistics communities in recent years owing to its applicability in scenarios with corrupted data, as well as in handling model mis-specifications. In particular, special interest has been devoted to the fundamental problem of robust linear regression where estimators that can tolerate corruption in up to a constant fraction of the response variables are widely studied. Surprisingly however, to this date, we are not aware of a polynomial time estimator that offers a consistent estimate in the presence of dense, unbounded corruptions. In this work we present such an estimator, called CRR. This solves an open problem put forward in the work of (Bhatia et al, 2015). Our consistency analysis requires a novel two-stage proof technique involving a careful analysis of the stability of ordered lists which may be of independent interest. We show that CRR not only offers consistent estimates, but is empirically far superior to several other recently proposed algorithms for the robust regression problem, including extended Lasso and the TORRENT algorithm. In comparison, CRR offers comparable or better model recovery but with runtimes that are faster by an order of magnitude.",
        "bibtex": "@inproceedings{NIPS2017_e702e51d,\n author = {Bhatia, Kush and Jain, Prateek and Kamalaruban, Parameswaran and Kar, Purushottam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Consistent Robust Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Reviews.html",
        "metareview": "",
        "pdf_size": 488329,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10847239951647720465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of California, Berkeley + Microsoft Research India; Microsoft Research, India; EPFL, Switzerland + Microsoft Research India; Indian Institute of Technology, Kanpur",
        "aff_domain": "berkeley.edu;microsoft.com;epfl.ch;cse.iitk.ac.in",
        "email": "berkeley.edu;microsoft.com;epfl.ch;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html",
        "aff_unique_index": "0+1;1;2+1;3",
        "aff_unique_norm": "University of California, Berkeley;Microsoft Research;\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne;Indian Institute of Technology Kanpur",
        "aff_unique_dep": ";Microsoft Research India;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.epfl.ch;https://www.iitk.ac.in",
        "aff_unique_abbr": "UC Berkeley;MSR India;EPFL;IIT Kanpur",
        "aff_campus_unique_index": "0;;2",
        "aff_campus_unique": "Berkeley;;Kanpur",
        "aff_country_unique_index": "0+1;1;2+1;1",
        "aff_country_unique": "United States;India;Switzerland"
    },
    {
        "title": "Context Selection for Embedding Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9258",
        "id": "9258",
        "author_site": "Liping Liu, Francisco Ruiz, Susan Athey, David Blei",
        "author": "Liping Liu; Francisco Ruiz; Susan Athey; David Blei",
        "abstract": "Word embeddings are an effective tool to analyze language. They have been recently extended to model other types of data beyond text, such as items in recommendation systems. Embedding models consider the probability of a target observation (a word or an item) conditioned on the elements in the context (other words or items). In this paper, we show that conditioning on all the elements in the context is not optimal. Instead, we model the probability of the target conditioned on a learned subset of the elements in the context. We use amortized variational inference to automatically choose this subset. Compared to standard embedding models, this method improves predictions and the quality of the embeddings.",
        "bibtex": "@inproceedings{NIPS2017_7884a965,\n author = {Liu, Liping and Ruiz, Francisco and Athey, Susan and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Context Selection for Embedding Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7884a9652e94555c70f96b6be63be216-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7884a9652e94555c70f96b6be63be216-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7884a9652e94555c70f96b6be63be216-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7884a9652e94555c70f96b6be63be216-Reviews.html",
        "metareview": "",
        "pdf_size": 504348,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16653945981633293644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Tufts University + Columbia University; Columbia University + University of Cambridge; Stanford University; Columbia University",
        "aff_domain": "tufts.edu;columbia.edu;stanford.edu;columbia.edu",
        "email": "tufts.edu;columbia.edu;stanford.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7884a9652e94555c70f96b6be63be216-Abstract.html",
        "aff_unique_index": "0+1;1+2;3;1",
        "aff_unique_norm": "Tufts University;Columbia University;University of Cambridge;Stanford University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.tufts.edu;https://www.columbia.edu;https://www.cam.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Tufts;Columbia;Cambridge;Stanford",
        "aff_campus_unique_index": ";1;2",
        "aff_campus_unique": ";Cambridge;Stanford",
        "aff_country_unique_index": "0+0;0+1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Continual Learning with Deep Generative Replay",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9084",
        "id": "9084",
        "author_site": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim",
        "author": "Hanul Shin; Jung Kwon Lee; Jaehong Kim; Jiwon Kim",
        "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\u201cgenerator\u201d) and a task solving model (\u201csolver\u201d). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.",
        "bibtex": "@inproceedings{NIPS2017_0efbe980,\n author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Continual Learning with Deep Generative Replay},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Reviews.html",
        "metareview": "",
        "pdf_size": 1799799,
        "gs_citation": 2609,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5696060555916548471&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Massachusetts Institute of Technology + SK T-Brain; SK T-Brain; SK T-Brain; SK T-Brain",
        "aff_domain": "mit.edu;sktbrain.com;sktbrain.com;sktbrain.com",
        "email": "mit.edu;sktbrain.com;sktbrain.com;sktbrain.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html",
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;SK Telecom",
        "aff_unique_dep": ";T-Brain",
        "aff_unique_url": "https://web.mit.edu;https://www.sktelecom.com",
        "aff_unique_abbr": "MIT;SKT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "Continuous DR-submodular  Maximization: Structure and Algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8845",
        "id": "8845",
        "author_site": "Yatao Bian, Kfir Levy, Andreas Krause, Joachim M Buhmann",
        "author": "An Bian; Kfir Levy; Andreas Krause; Joachim M Buhmann",
        "abstract": "DR-submodular continuous functions are important objectives with wide real-world applications spanning MAP inference in  determinantal point processes (DPPs), and mean-field inference for probabilistic submodular models, amongst others. DR-submodularity captures a subclass of non-convex functions that enables both exact minimization and approximate maximization in polynomial time.  In this work we study the  problem of maximizing  non-monotone DR-submodular continuous functions under general down-closed convex constraints. We start by investigating geometric properties that underlie such objectives, e.g., a strong relation between  (approximately) stationary points and global optimum is proved. These properties are then used to devise two optimization algorithms with provable guarantees. Concretely, we first devise a \"two-phase'' algorithm with 1/4 approximation guarantee. This algorithm allows the use of existing methods for finding (approximately) stationary points as a subroutine, thus, harnessing recent progress in non-convex optimization. Then we present a non-monotone Frank-Wolfe variant with 1/e approximation guarantee and sublinear convergence rate. Finally, we extend our approach to a broader class of generalized DR-submodular continuous functions, which captures a wider spectrum of applications. Our theoretical findings are validated on synthetic and real-world problem instances.",
        "bibtex": "@inproceedings{NIPS2017_58238e9a,\n author = {Bian, An and Levy, Kfir and Krause, Andreas and Buhmann, Joachim M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Continuous DR-submodular  Maximization: Structure and Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/58238e9ae2dd305d79c2ebc8c1883422-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/58238e9ae2dd305d79c2ebc8c1883422-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/58238e9ae2dd305d79c2ebc8c1883422-Reviews.html",
        "metareview": "",
        "pdf_size": 734260,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1028202507889067983&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/58238e9ae2dd305d79c2ebc8c1883422-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Contrastive Learning for Image Captioning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8884",
        "id": "8884",
        "author_site": "Bo Dai, Dahua Lin",
        "author": "Bo Dai; Dahua Lin",
        "abstract": "Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.",
        "bibtex": "@inproceedings{NIPS2017_46922a08,\n author = {Dai, Bo and Lin, Dahua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Contrastive Learning for Image Captioning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/46922a0880a8f11f8f69cbb52b1396be-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/46922a0880a8f11f8f69cbb52b1396be-Reviews.html",
        "metareview": "",
        "pdf_size": 1203359,
        "gs_citation": 224,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5560551834119010465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong",
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Information Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Controllable Invariance through Adversarial Feature Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8854",
        "id": "8854",
        "author_site": "Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig",
        "author": "Qizhe Xie; Zihang Dai; Yulun Du; Eduard Hovy; Graham Neubig",
        "abstract": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.",
        "bibtex": "@inproceedings{NIPS2017_8cb22bdd,\n author = {Xie, Qizhe and Dai, Zihang and Du, Yulun and Hovy, Eduard and Neubig, Graham},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Controllable Invariance through Adversarial Feature Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Reviews.html",
        "metareview": "",
        "pdf_size": 1131578,
        "gs_citation": 332,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5672971224339713468&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8cb22bdd0b7ba1ab13d742e22eed8da2-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8855",
        "id": "8855",
        "author_site": "Yuanzhi Li, Yang Yuan",
        "author": "Yuanzhi Li; Yang Yuan",
        "abstract": "In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing.  In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called \"identity mapping\". We prove that, if input follows from Gaussian distribution, with standard $O(1/\\sqrt{d})$ initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the \"identity mapping\" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.  Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in \"two phases\": In phase I, the gradient points to the wrong direction, however, a potential function $g$ gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.",
        "bibtex": "@inproceedings{NIPS2017_a96b65a7,\n author = {Li, Yuanzhi and Yuan, Yang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergence Analysis of Two-layer Neural Networks with ReLU Activation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Reviews.html",
        "metareview": "",
        "pdf_size": 1783192,
        "gs_citation": 846,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=212149420094118161&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, Princeton University; Computer Science Department, Cornell University",
        "aff_domain": "cs.princeton.edu;cs.cornell.edu",
        "email": "cs.princeton.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;Cornell University",
        "aff_unique_dep": "Computer Science Department;Computer Science Department",
        "aff_unique_url": "https://www.princeton.edu;https://www.cornell.edu",
        "aff_unique_abbr": "Princeton;Cornell",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Princeton;Ithaca",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Convergence of Gradient EM on Multi-component Mixture of Gaussians",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9462",
        "id": "9462",
        "author_site": "Bowei Yan, Mingzhang Yin, Purnamrita Sarkar",
        "author": "Bowei Yan; Mingzhang Yin; Purnamrita Sarkar",
        "abstract": "In this paper, we study convergence properties of the gradient variant of Expectation-Maximization algorithm~\\cite{lange1995gradient} for Gaussian Mixture Models for arbitrary number of clusters and mixing coefficients. We derive the convergence rate depending on the mixing coefficients, minimum and maximum pairwise distances between the true centers, dimensionality and number of components; and obtain a near-optimal local contraction radius. While there have been some recent notable works that derive local convergence rates for EM in the two symmetric mixture of Gaussians, in the more general case, the derivations need structurally different and non-trivial arguments. We use recent tools from learning theory and empirical processes to achieve our theoretical results.",
        "bibtex": "@inproceedings{NIPS2017_dfeb9598,\n author = {Yan, Bowei and Yin, Mingzhang and Sarkar, Purnamrita},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergence of Gradient EM on Multi-component Mixture of Gaussians},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dfeb9598fbfb97cc6bbcc0aff2c785d6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dfeb9598fbfb97cc6bbcc0aff2c785d6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dfeb9598fbfb97cc6bbcc0aff2c785d6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dfeb9598fbfb97cc6bbcc0aff2c785d6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dfeb9598fbfb97cc6bbcc0aff2c785d6-Reviews.html",
        "metareview": "",
        "pdf_size": 342993,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16392164369704805133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Texas at Austin; University of Texas at Austin; University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;austin.utexas.edu",
        "email": "utexas.edu;utexas.edu;austin.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dfeb9598fbfb97cc6bbcc0aff2c785d6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Convergence rates of a partition based Bayesian multivariate density estimation method",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9250",
        "id": "9250",
        "author_site": "Linxi Liu, Dangna Li, Wing Hung Wong",
        "author": "Linxi Liu; Dangna Li; Wing Hung Wong",
        "abstract": "We study a class of non-parametric density estimators under Bayesian settings. The estimators are obtained by adaptively partitioning the sample space. Under a suitable prior, we analyze the concentration rate of the posterior distribution, and demonstrate that the rate does not directly depend on the dimension of the problem in several special cases. Another advantage of this class of Bayesian density estimators is that it can adapt to the unknown smoothness of the true density function, thus achieving the optimal convergence rate without artificial conditions on the density. We also validate the theoretical results on a variety of simulated data sets.",
        "bibtex": "@inproceedings{NIPS2017_f55cadb9,\n author = {Liu, Linxi and Li, Dangna and Wong, Wing Hung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergence rates of a partition based Bayesian multivariate density estimation method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f55cadb97eaff2ba1980e001b0bd9842-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f55cadb97eaff2ba1980e001b0bd9842-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f55cadb97eaff2ba1980e001b0bd9842-Reviews.html",
        "metareview": "",
        "pdf_size": 1208290,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=910398399151537606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, Columbia University + Stanford University; ICME, Stanford University; Department of Statistics, Stanford University",
        "aff_domain": "columbia.edu;stanford.edu;stanford.edu",
        "email": "columbia.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f55cadb97eaff2ba1980e001b0bd9842-Abstract.html",
        "aff_unique_index": "0+1;2;1",
        "aff_unique_norm": "Columbia University;Stanford University;ICME, Stanford University",
        "aff_unique_dep": "Department of Statistics;;",
        "aff_unique_url": "https://www.columbia.edu;https://www.stanford.edu;",
        "aff_unique_abbr": "Columbia;Stanford;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8962",
        "id": "8962",
        "author_site": "Ziming Zhang, Matthew Brand",
        "author": "Ziming Zhang; Matthew Brand",
        "abstract": "By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.",
        "bibtex": "@inproceedings{NIPS2017_6a2feef8,\n author = {Zhang, Ziming and Brand, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Reviews.html",
        "metareview": "",
        "pdf_size": 429395,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9242368973456076248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6a2feef8ed6a9fe76d6b3f30f02150b4-Abstract.html"
    },
    {
        "title": "Convolutional Gaussian Processes",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9069",
        "id": "9069",
        "author_site": "Mark van der Wilk, Carl Edward Rasmussen, James Hensman",
        "author": "Mark van der Wilk; Carl Edward Rasmussen; James Hensman",
        "abstract": "We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, where we obtain significant improvements over existing Gaussian process models. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. This illustration of the usefulness of the marginal likelihood may help automate discovering architectures in larger models.",
        "bibtex": "@inproceedings{NIPS2017_1c54985e,\n author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convolutional Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1c54985e4f95b7819ca0357c0cb9a09f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1c54985e4f95b7819ca0357c0cb9a09f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1c54985e4f95b7819ca0357c0cb9a09f-Reviews.html",
        "metareview": "",
        "pdf_size": 413647,
        "gs_citation": 175,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6134307582305750233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, University of Cambridge, UK; Department of Engineering, University of Cambridge, UK; prowler.io, Cambridge, UK",
        "aff_domain": "cam.ac.uk;cam.ac.uk;prowler.io",
        "email": "cam.ac.uk;cam.ac.uk;prowler.io",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1c54985e4f95b7819ca0357c0cb9a09f-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Cambridge;prowler.io, Cambridge, UK",
        "aff_unique_dep": "Department of Engineering;",
        "aff_unique_url": "https://www.cam.ac.uk;",
        "aff_unique_abbr": "Cambridge;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "title": "Convolutional Phase Retrieval",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9380",
        "id": "9380",
        "author_site": "Qing Qu, Yuqian Zhang, Yonina Eldar, John Wright",
        "author": "Qing Qu; Yuqian Zhang; Yonina Eldar; John Wright",
        "abstract": "We study the convolutional phase retrieval problem, which asks us to recover an unknown signal ${\\mathbf x} $ of length $n$ from $m$ measurements consisting of the magnitude of its cyclic convolution with a known kernel $\\mathbf a$ of length $m$. This model is motivated by applications to channel estimation, optics, and underwater acoustic communication, where the signal of interest is acted on by a given channel/filter, and phase information is difficult or impossible to acquire. We show that when $\\mathbf a$ is random and $m \\geq \\Omega(\\frac{ \\| \\mathbf C_{\\mathbf x}\\|^2}{ \\|\\mathbf x\\|^2 }  n \\mathrm{poly} \\log n)$, $\\mathbf x$ can be efficiently recovered up to a global phase using a combination of spectral initialization and generalized gradient descent. The main challenge is coping with dependencies in the measurement operator; we overcome this challenge by using ideas from decoupling theory, suprema of chaos processes and the restricted isometry property of random circulant matrices, and recent analysis for alternating minimizing methods.",
        "bibtex": "@inproceedings{NIPS2017_6ad4174e,\n author = {Qu, Qing and Zhang, Yuqian and Eldar, Yonina and Wright, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convolutional Phase Retrieval},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6ad4174eba19ecb5fed17411a34ff5e6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6ad4174eba19ecb5fed17411a34ff5e6-Reviews.html",
        "metareview": "",
        "pdf_size": 1107815,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7398991236636486743&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Columbia University; Columbia University; Technion; Columbia University",
        "aff_domain": "columbia.edu;columbia.edu;ee.technion.ac.il;columbia.edu",
        "email": "columbia.edu;columbia.edu;ee.technion.ac.il;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6ad4174eba19ecb5fed17411a34ff5e6-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Columbia University;Technion - Israel Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Columbia;Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Cortical microcircuits as gated-recurrent neural networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8824",
        "id": "8824",
        "author_site": "Rui Costa, Yannis Assael, Brendan Shillingford, Nando de Freitas, TIm Vogels",
        "author": "Rui Costa; Ioannis Alexandros Assael; Brendan Shillingford; Nando de Freitas; TIm Vogels",
        "abstract": "Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM).  We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits.   Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.",
        "bibtex": "@inproceedings{NIPS2017_45fbc6d3,\n author = {Costa, Rui and Assael, Ioannis Alexandros and Shillingford, Brendan and de Freitas, Nando and Vogels, TIm},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cortical microcircuits as gated-recurrent neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/45fbc6d3e05ebd93369ce542e8f2322d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/45fbc6d3e05ebd93369ce542e8f2322d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/45fbc6d3e05ebd93369ce542e8f2322d-Reviews.html",
        "metareview": "",
        "pdf_size": 274231,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9584250990031503609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Centre for Neural Circuits and Behaviour+Dept. of Physiology, Anatomy and Genetics, University of Oxford, Oxford, UK; Dept. of Computer Science, University of Oxford, Oxford, UK+DeepMind, London, UK; Dept. of Computer Science, University of Oxford, Oxford, UK+DeepMind, London, UK; DeepMind, London, UK; Centre for Neural Circuits and Behaviour+Dept. of Physiology, Anatomy and Genetics, University of Oxford, Oxford, UK",
        "aff_domain": "cncb.ox.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;google.com;cncb.ox.ac.uk",
        "email": "cncb.ox.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;google.com;cncb.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html",
        "aff_unique_index": "0+1;2+3;2+3;3;0+1",
        "aff_unique_norm": "Centre for Neural Circuits and Behaviour;Dept. of Physiology, Anatomy and Genetics, University of Oxford, Oxford, UK;Dept. of Computer Science, University of Oxford, Oxford, UK;DeepMind",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;https://deepmind.com",
        "aff_unique_abbr": ";;;DeepMind",
        "aff_campus_unique_index": ";1;1;1;",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": ";1;1;1;",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "title": "Cost efficient gradient boosting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8946",
        "id": "8946",
        "author_site": "Sven Peter, Ferran Diego, Fred Hamprecht, Boaz Nadler",
        "author": "Sven Peter; Ferran Diego; Fred A. Hamprecht; Boaz Nadler",
        "abstract": "Many applications require learning classifiers or regressors that are both accurate and cheap to evaluate. Prediction cost can be drastically reduced if the learned predictor is constructed such that on the majority of the inputs, it uses cheap features and fast evaluations. The main challenge is to do so with little loss in accuracy. In this work we propose a budget-aware strategy based on deep boosted regression trees. In contrast to previous approaches to learning with cost penalties, our method can grow very deep trees that on average are nonetheless cheap to compute. We evaluate our method on a number of datasets and find that it outperforms the current state of the art by a large margin. Our algorithm is easy to implement and its learning time is comparable to that of the original gradient boosting. Source code is made available at http://github.com/svenpeter42/LightGBM-CEGB.",
        "bibtex": "@inproceedings{NIPS2017_4fac9ba1,\n author = {Peter, Sven and Diego, Ferran and Hamprecht, Fred A and Nadler, Boaz},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cost efficient gradient boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Reviews.html",
        "metareview": "",
        "pdf_size": 1221254,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13990906581536296774&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Heidelberg Collaboratory for Image Processing+Interdisciplinary Center for Scienti\ufb01c Computing+University of Heidelberg; Robert Bosch GmbH; Heidelberg Collaboratory for Image Processing+Interdisciplinary Center for Scienti\ufb01c Computing+University of Heidelberg; Department of Computer Science+Weizmann Institute of Science",
        "aff_domain": "iwr.uni-heidelberg.de;de.bosch.com;iwr.uni-heidelberg.de;weizmann.ac.il",
        "email": "iwr.uni-heidelberg.de;de.bosch.com;iwr.uni-heidelberg.de;weizmann.ac.il",
        "github": "http://github.com/svenpeter42/LightGBM-CEGB",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html",
        "aff_unique_index": "0+1+2;3;0+1+2;4+5",
        "aff_unique_norm": "Heidelberg University;Interdisciplinary Center for Scienti\ufb01c Computing;University of Heidelberg;Robert Bosch GmbH;Unknown Institution;Weizmann Institute of Science",
        "aff_unique_dep": "Heidelberg Collaboratory for Image Processing;;;;Department of Computer Science;",
        "aff_unique_url": "https://www.kip.uni-heidelberg.de;;https://www.uni-heidelberg.de;https://www.bosch.com;;https://www.weizmann.org.il",
        "aff_unique_abbr": "KIP;;Uni Heidelberg;Bosch;;Weizmann",
        "aff_campus_unique_index": "0;0;",
        "aff_campus_unique": "Heidelberg;",
        "aff_country_unique_index": "0+0;0;0+0;2",
        "aff_country_unique": "Germany;;Israel"
    },
    {
        "title": "Counterfactual Fairness",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9187",
        "id": "9187",
        "author_site": "Matt Kusner, Joshua Loftus, Chris Russell, Ricardo Silva",
        "author": "Matt J Kusner; Joshua Loftus; Chris Russell; Ricardo Silva",
        "abstract": "Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.  In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.  Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it  the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",
        "bibtex": "@inproceedings{NIPS2017_a486cd07,\n author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Counterfactual Fairness},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html",
        "metareview": "",
        "pdf_size": 759710,
        "gs_citation": 2376,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13115459093902017069&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "The Alan Turing Institute + University of Warwick; New York University; The Alan Turing Institute + University of Surrey; The Alan Turing Institute + University College London",
        "aff_domain": "turing.ac.uk;nyu.edu;turing.ac.uk;stats.ucl.ac.uk",
        "email": "turing.ac.uk;nyu.edu;turing.ac.uk;stats.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html",
        "aff_unique_index": "0+1;2;0+3;0+4",
        "aff_unique_norm": "The Alan Turing Institute;University of Warwick;New York University;University of Surrey;University College London",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.turing.ac.uk;https://www.warwick.ac.uk;https://www.nyu.edu;https://www.surrey.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "ATI;Warwick;NYU;Surrey;UCL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;0+0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Countering Feedback Delays in Multi-Agent Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9388",
        "id": "9388",
        "author_site": "Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter W Glynn, Claire Tomlin",
        "author": "Zhengyuan Zhou; Panayotis Mertikopoulos; Nicholas Bambos; Peter W. Glynn; Claire Tomlin",
        "abstract": "We consider a model of game-theoretic learning based on online mirror descent (OMD) with asynchronous and delayed feedback information. Instead of focusing on specific games, we consider a broad class of continuous games defined by the general equilibrium stability notion, which we call \u03bb-variational stability. Our first contribution is that, in this class of games, the actual sequence of play induced by OMD-based learning converges to Nash equilibria provided that the feedback delays faced by the players are synchronous and bounded. Subsequently, to tackle fully decentralized, asynchronous environments with (possibly) unbounded delays between actions and feedback, we propose a variant of OMD which we call delayed mirror descent (DMD), and which relies on the repeated leveraging of past information. With this modification, the algorithm converges to Nash equilibria with no feedback synchronicity assumptions and even when the delays grow superlinearly relative to the horizon of play.",
        "bibtex": "@inproceedings{NIPS2017_2e0aca89,\n author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Glynn, Peter W and Tomlin, Claire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Countering Feedback Delays in Multi-Agent Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2e0aca891f2a8aedf265edf533a6d9a8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2e0aca891f2a8aedf265edf533a6d9a8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2e0aca891f2a8aedf265edf533a6d9a8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2e0aca891f2a8aedf265edf533a6d9a8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2e0aca891f2a8aedf265edf533a6d9a8-Reviews.html",
        "metareview": "",
        "pdf_size": 341481,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7925037974517783496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Univ. Grenoble Alpes, CNRS, Inria, LIG; Stanford University; Stanford University; UC Berkeley",
        "aff_domain": "stanford.edu;imag.fr;stanford.edu;stanford.edu;eecs.berkeley.edu",
        "email": "stanford.edu;imag.fr;stanford.edu;stanford.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2e0aca891f2a8aedf265edf533a6d9a8-Abstract.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Stanford University;Universit\u00e9 Grenoble Alpes;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.univ-grenoble-alpes.fr;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;UGA;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Stanford;;Berkeley",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Cross-Spectral Factor Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9451",
        "id": "9451",
        "author_site": "Neil Gallagher, Kyle Ulrich, Austin Talbot, Kafui Dzirasa, Lawrence Carin, David Carlson",
        "author": "Neil Gallagher; Kyle R Ulrich; Austin Talbot; Kafui Dzirasa; Lawrence Carin; David E Carlson",
        "abstract": "In neuropsychiatric disorders such as schizophrenia or depression, there is often a disruption in the way that regions of the brain synchronize with one another. To facilitate understanding of network-level synchronization between brain regions, we introduce a novel model of multisite low-frequency neural recordings, such as local field potentials (LFPs) and electroencephalograms (EEGs). The proposed model, named Cross-Spectral Factor Analysis (CSFA), breaks the observed signal into factors defined by unique spatio-spectral properties. These properties are granted to the factors via a Gaussian process formulation in a multiple kernel learning framework. In this way, the LFP signals can be mapped to a lower dimensional space in a way that retains information of relevance to neuroscientists.  Critically, the factors are interpretable. The proposed approach empirically allows similar performance in classifying mouse genotype and behavioral context when compared to commonly used approaches that lack the interpretability of CSFA. We also introduce a semi-supervised approach, termed discriminative CSFA (dCSFA). CSFA and dCSFA provide useful tools for understanding neural dynamics, particularly by aiding in the design of causal follow-up experiments.",
        "bibtex": "@inproceedings{NIPS2017_5b970a1d,\n author = {Gallagher, Neil and Ulrich, Kyle R and Talbot, Austin and Dzirasa, Kafui and Carin, Lawrence and Carlson, David E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cross-Spectral Factor Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5b970a1d9be0fd100063fd6cd688b73e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5b970a1d9be0fd100063fd6cd688b73e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5b970a1d9be0fd100063fd6cd688b73e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5b970a1d9be0fd100063fd6cd688b73e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5b970a1d9be0fd100063fd6cd688b73e-Reviews.html",
        "metareview": "",
        "pdf_size": 1223494,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2922453998020020970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Neurobiology; Department of Electrical and Computer Engineering; Department of Statistical Science; Department of Neurobiology+Department of Psychiatry and Behavioral Sciences; Department of Electrical and Computer Engineering; Department of Civil and Environmental Engineering+Department of Biostatistics and Bioinformatics",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5b970a1d9be0fd100063fd6cd688b73e-Abstract.html",
        "aff_unique_index": "0;1;2;0+3;1;4+5",
        "aff_unique_norm": "Department of Neurobiology;Unknown Institution;Department of Statistical Science;Department of Psychiatry and Behavioral Sciences;Department of Civil and Environmental Engineering;Department of Biostatistics and Bioinformatics",
        "aff_unique_dep": ";Department of Electrical and Computer Engineering;;;;",
        "aff_unique_url": ";;;;;",
        "aff_unique_abbr": ";;;;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "title": "DPSCREEN: Dynamic Personalized Screening",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8924",
        "id": "8924",
        "author_site": "Kartik Ahuja, William Zame, Mihaela van der Schaar",
        "author": "Kartik Ahuja; William Zame; Mihaela van der Schaar",
        "abstract": "Screening is important for the diagnosis and treatment of a wide variety of diseases.  A good screening  policy should be personalized to the disease, to the features of the patient and to the dynamic history of the patient (including the history of screening).  The growth of electronic health records data  has led to the development of many  models to  predict the onset and progression of different diseases. However, there has been limited work to address the personalized screening for these different diseases. In this work, we develop the first framework to construct screening policies for a large class of disease models. The disease is modeled as a finite state stochastic process with an absorbing disease state. The patient observes an  external information process (for instance, self-examinations, discovering comorbidities, etc.) which can trigger the patient to arrive at the clinician earlier than scheduled screenings. The clinician carries out the tests; based on the test results and the  external information it schedules the next arrival.  Computing the exactly optimal screening policy that balances the delay in the detection against the frequency of screenings is computationally intractable; this paper provides a computationally tractable construction of an approximately optimal policy.  As an illustration, we make use of a large breast cancer data set.  The constructed policy screens patients more or less often according to their initial risk -- it is personalized to the features of the patient -- and according to the results of previous screens \u2013 it is personalized to the  history of the patient. In comparison with existing clinical policies, the constructed policy leads to large reductions (28-68 %) in the number of screens performed while achieving the same expected delays in disease detection.",
        "bibtex": "@inproceedings{NIPS2017_22fb0cee,\n author = {Ahuja, Kartik and Zame, William and van der Schaar, Mihaela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {DPSCREEN: Dynamic Personalized Screening},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/22fb0cee7e1f3bde58293de743871417-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/22fb0cee7e1f3bde58293de743871417-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/22fb0cee7e1f3bde58293de743871417-Reviews.html",
        "metareview": "",
        "pdf_size": 334049,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11509674164574124648&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Electrical and Computer Engineering Department, University of California, Los Angeles; Economics Department, University of California, Los Angeles; Engineering Science Department, University of Oxford+Electrical and Computer Engineering Department, University of California, Los Angeles",
        "aff_domain": "ucla.edu;econ.ucla.edu;oxford-man.ox.ac.uk",
        "email": "ucla.edu;econ.ucla.edu;oxford-man.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "University of California, Los Angeles;Economics Department, University of California, Los Angeles;Engineering Science Department, University of Oxford",
        "aff_unique_dep": "Electrical and Computer Engineering Department;;",
        "aff_unique_url": "https://www.ucla.edu;;",
        "aff_unique_abbr": "UCLA;;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8992",
        "id": "8992",
        "author_site": "Rowan McAllister, Carl Edward Rasmussen",
        "author": "Rowan McAllister; Carl Edward Rasmussen",
        "abstract": "We present a data-efficient reinforcement learning method for continuous state-action systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.",
        "bibtex": "@inproceedings{NIPS2017_5eac43ac,\n author = {McAllister, Rowan and Rasmussen, Carl Edward},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5eac43aceba42c8757b54003a58277b5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5eac43aceba42c8757b54003a58277b5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5eac43aceba42c8757b54003a58277b5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5eac43aceba42c8757b54003a58277b5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5eac43aceba42c8757b54003a58277b5-Reviews.html",
        "metareview": "",
        "pdf_size": 390999,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12324505803263203906&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Engineering, Cambridge University; Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5eac43aceba42c8757b54003a58277b5-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Engineering, Cambridge University;University of Cambridge",
        "aff_unique_dep": ";Department of Engineering",
        "aff_unique_url": ";https://www.cam.ac.uk",
        "aff_unique_abbr": ";Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "title": "Deanonymization in the Bitcoin P2P Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8928",
        "id": "8928",
        "author_site": "Giulia Fanti, Pramod Viswanath",
        "author": "Giulia Fanti; Pramod Viswanath",
        "abstract": "Recent attacks on Bitcoin's peer-to-peer (P2P) network demonstrated that its transaction-flooding protocols, which are used to ensure network consistency, may enable user deanonymization---the linkage of a user's IP address with her pseudonym in the Bitcoin network. In 2015, the Bitcoin community responded to these attacks by changing the network's flooding mechanism to a different protocol, known as diffusion. However, it is unclear if diffusion actually improves the system's anonymity. In this paper, we model the Bitcoin networking stack and analyze its anonymity properties, both pre- and post-2015. The core problem is one of epidemic source inference over graphs, where the observational model and spreading mechanisms are informed by Bitcoin's implementation; notably, these models have not been studied in the epidemic source detection literature before. We identify and analyze near-optimal source estimators. This analysis suggests that Bitcoin's networking protocols (both pre- and post-2015) offer poor anonymity properties on networks with a regular-tree topology. We confirm this claim in simulation on a 2015 snapshot of the real Bitcoin P2P network topology.",
        "bibtex": "@inproceedings{NIPS2017_6c3cf77d,\n author = {Fanti, Giulia and Viswanath, Pramod},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deanonymization in the Bitcoin P2P Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6c3cf77d52820cd0fe646d38bc2145ca-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6c3cf77d52820cd0fe646d38bc2145ca-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6c3cf77d52820cd0fe646d38bc2145ca-Reviews.html",
        "metareview": "",
        "pdf_size": 524964,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12486601712116374979&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "ECE Department at Carnegie Mellon University; ECE Department at the University of Illinois at Urbana-Champaign",
        "aff_domain": "andrew.cmu.edu;illinois.edu",
        "email": "andrew.cmu.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ECE Department at Carnegie Mellon University;ECE Department at the University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Decoding with Value Networks for Neural Machine Translation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8815",
        "id": "8815",
        "author_site": "Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, Tie-Yan Liu",
        "author": "Di He; Hanqing Lu; Yingce Xia; Tao Qin; Liwei Wang; Tie-Yan Liu",
        "abstract": "Neural Machine Translation (NMT) has become a popular technology in recent years, and beam search is its de facto decoding method due to the shrunk search space and reduced computational complexity. However, since it only searches for local optima at each time step through one-step forward looking, it usually cannot output the best target sentence. Inspired by the success and methodology of AlphaGo, in this paper we propose using a prediction network to improve beam search, which takes the source sentence $x$, the currently available decoding output $y_1,\\cdots, y_{t-1}$ and a candidate word $w$ at step $t$ as inputs and predicts the long-term value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT model. Following the practice in reinforcement learning, we call this prediction network \\emph{value network}. Specifically, we propose a recurrent structure for the value network, and train its parameters from bilingual data. During the test time, when  choosing a word $w$ for decoding, we consider both its conditional probability given by the NMT model and its long-term value predicted by the value network. Experiments show that such an approach can significantly improve the translation accuracy on several translation tasks.",
        "bibtex": "@inproceedings{NIPS2017_2b24d495,\n author = {He, Di and Lu, Hanqing and Xia, Yingce and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decoding with Value Networks for Neural Machine Translation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Reviews.html",
        "metareview": "",
        "pdf_size": 740543,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9924066051536654397&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Key Laboratory of Machine Perception, MOE, School of EECS, Peking University; Carnegie Mellon University; University of Science and Technology of China; Microsoft Research; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University + Center for Data Science, Peking University, Beijing Institute of Big Data Research; Microsoft Research",
        "aff_domain": "pku.edu.cn;cmu.edu;mail.ustc.edu.cn;microsoft.com;cis.pku.edu.cn;microsoft.com",
        "email": "pku.edu.cn;cmu.edu;mail.ustc.edu.cn;microsoft.com;cis.pku.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html",
        "aff_unique_index": "0;1;2;3;0+0;3",
        "aff_unique_norm": "Peking University;Carnegie Mellon University;University of Science and Technology of China;Microsoft Corporation",
        "aff_unique_dep": "School of EECS;;;Microsoft Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.cmu.edu;http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Peking U;CMU;USTC;MSR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;1;0;1;0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Decomposable Submodular Function Minimization: Discrete and Continuous",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9072",
        "id": "9072",
        "author_site": "Alina Ene, Huy Nguyen, L\u00e1szl\u00f3 A. V\u00e9gh",
        "author": "Alina Ene; Huy Nguyen; L\u00e1szl\u00f3 A. V\u00e9gh",
        "abstract": "This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.",
        "bibtex": "@inproceedings{NIPS2017_c1fea270,\n author = {Ene, Alina and Nguyen, Huy and V\\'{e}gh, L\\'{a}szl\\'{o} A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decomposable Submodular Function Minimization: Discrete and Continuous},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c1fea270c48e8079d8ddf7d06d26ab52-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c1fea270c48e8079d8ddf7d06d26ab52-Reviews.html",
        "metareview": "",
        "pdf_size": 437467,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9685882419704729977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Boston University; College of Computer and Information Science, Northeastern University; Department of Mathematics, London School of Economics",
        "aff_domain": "bu.edu;northeastern.edu;lse.ac.uk",
        "email": "bu.edu;northeastern.edu;lse.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Boston University;Northeastern University;London School of Economics",
        "aff_unique_dep": "Department of Computer Science;College of Computer and Information Science;Department of Mathematics",
        "aff_unique_url": "https://www.bu.edu;https://www.northeastern.edu;https://www.lse.ac.uk",
        "aff_unique_abbr": "BU;NU;LSE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9055",
        "id": "9055",
        "author_site": "Mohammad Ali Bashiri, Xinhua Zhang",
        "author": "Mohammad Ali Bashiri; Xinhua Zhang",
        "abstract": "Frank-Wolfe (FW) algorithms with linear convergence rates have recently achieved great efficiency in many applications. Garber and Meshi (2016) designed a new decomposition-invariant pairwise FW variant with favorable dependency on the domain geometry. Unfortunately, it applies only to a restricted class of polytopes and cannot achieve theoretical and practical efficiency at the same time. In this paper, we show that by employing an away-step update, similar rates can be generalized to arbitrary polytopes with strong empirical performance. A new \"condition number\" of the domain is introduced which allows leveraging the sparsity of the solution. We applied the method to a reformulation of SVM, and the linear convergence rate depends, for the first time, on the number of support vectors.",
        "bibtex": "@inproceedings{NIPS2017_99adff45,\n author = {Bashiri, Mohammad Ali and Zhang, Xinhua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/99adff456950dd9629a5260c4de21858-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/99adff456950dd9629a5260c4de21858-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/99adff456950dd9629a5260c4de21858-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/99adff456950dd9629a5260c4de21858-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/99adff456950dd9629a5260c4de21858-Reviews.html",
        "metareview": "",
        "pdf_size": 483807,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15256728121067537572&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/99adff456950dd9629a5260c4de21858-Abstract.html"
    },
    {
        "title": "Deconvolutional Paragraph Representation Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9197",
        "id": "9197",
        "author_site": "Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, Lawrence Carin",
        "author": "Yizhe Zhang; Dinghan Shen; Guoyin Wang; Zhe Gan; Ricardo Henao; Lawrence Carin",
        "abstract": "Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.",
        "bibtex": "@inproceedings{NIPS2017_4b4edc26,\n author = {Zhang, Yizhe and Shen, Dinghan and Wang, Guoyin and Gan, Zhe and Henao, Ricardo and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deconvolutional Paragraph Representation Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4b4edc2630fe75800ddc29a7b4070add-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4b4edc2630fe75800ddc29a7b4070add-Reviews.html",
        "metareview": "",
        "pdf_size": 457451,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2697182705803790945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical & Computer Engineering, Duke University; Department of Electrical & Computer Engineering, Duke University; Department of Electrical & Computer Engineering, Duke University; Department of Electrical & Computer Engineering, Duke University; Department of Electrical & Computer Engineering, Duke University; Department of Electrical & Computer Engineering, Duke University",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4b4edc2630fe75800ddc29a7b4070add-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Department of Electrical & Computer Engineering, Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Decoupling \"when to update\" from \"how to update\"",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8890",
        "id": "8890",
        "author_site": "Eran Malach, Shai Shalev-Shwartz",
        "author": "Eran Malach; Shai Shalev-Shwartz",
        "abstract": "Deep learning requires data. A useful approach to obtain data is to  be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple",
        "bibtex": "@inproceedings{NIPS2017_58d4d1e7,\n author = {Malach, Eran and Shalev-Shwartz, Shai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decoupling \"when to update\" from \"how to update\"},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/58d4d1e7b1e97b258c9ed0b37e02d087-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/58d4d1e7b1e97b258c9ed0b37e02d087-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/58d4d1e7b1e97b258c9ed0b37e02d087-Reviews.html",
        "metareview": "",
        "pdf_size": 463846,
        "gs_citation": 729,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14625023666792081100&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science, The Hebrew University, Israel; School of Computer Science, The Hebrew University, Israel",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il",
        "email": "mail.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/58d4d1e7b1e97b258c9ed0b37e02d087-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Hebrew University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Deep Dynamic Poisson Factorization Model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8957",
        "id": "8957",
        "author_site": "Chengyue Gong, win-bin huang",
        "author": "Chengyue Gong; win-bin huang",
        "abstract": "A new model, named as deep dynamic poisson factorization model, is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks, representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution, and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.",
        "bibtex": "@inproceedings{NIPS2017_27ed0fb9,\n author = {Gong, Chengyue and huang, win-bin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Dynamic Poisson Factorization Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/27ed0fb950b856b06e1273989422e7d3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/27ed0fb950b856b06e1273989422e7d3-Reviews.html",
        "metareview": "",
        "pdf_size": 939918,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16920620413319962623&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Information Management, Peking University; Department of Information Management, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/27ed0fb950b856b06e1273989422e7d3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Department of Information Management",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Hyperalignment",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8951",
        "id": "8951",
        "author_site": "Muhammad Yousefnezhad, Daoqiang Zhang",
        "author": "Muhammad Yousefnezhad; Daoqiang Zhang",
        "abstract": "This paper proposes Deep Hyperalignment (DHA) as a regularized, deep extension, scalable Hyperalignment (HA) method, which is well-suited for applying functional alignment to fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. Unlink previous methods, DHA is not limited by a restricted fixed kernel function. Further, it uses a parametric approach, rank-m Singular Value Decomposition (SVD), and stochastic gradient descent for optimization. Therefore, DHA has a suitable time complexity for large datasets, and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.",
        "bibtex": "@inproceedings{NIPS2017_0768281a,\n author = {Yousefnezhad, Muhammad and Zhang, Daoqiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Hyperalignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0768281a05da9f27df178b5c39a51263-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0768281a05da9f27df178b5c39a51263-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0768281a05da9f27df178b5c39a51263-Reviews.html",
        "metareview": "",
        "pdf_size": 745352,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11596966776210190786&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics",
        "aff_domain": "nuaa.edu.cn;nuaa.edu.cn",
        "email": "nuaa.edu.cn;nuaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0768281a05da9f27df178b5c39a51263-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanjing University of Aeronautics and Astronautics",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "http://www.nuaa.edu.cn",
        "aff_unique_abbr": "NUAA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Hyperspherical Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9176",
        "id": "9176",
        "author_site": "Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, Le Song",
        "author": "Weiyang Liu; Yan-Ming Zhang; Xingguo Li; Zhiding Yu; Bo Dai; Tuo Zhao; Le Song",
        "abstract": "Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.",
        "bibtex": "@inproceedings{NIPS2017_5227b6aa,\n author = {Liu, Weiyang and Zhang, Yan-Ming and Li, Xingguo and Yu, Zhiding and Dai, Bo and Zhao, Tuo and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Hyperspherical Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5227b6aaf294f5f027273aebf16015f2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5227b6aaf294f5f027273aebf16015f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5227b6aaf294f5f027273aebf16015f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5227b6aaf294f5f027273aebf16015f2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5227b6aaf294f5f027273aebf16015f2-Reviews.html",
        "metareview": "",
        "pdf_size": 1256459,
        "gs_citation": 170,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12520533596453682546&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Georgia Institute of Technology; Institute of Automation, Chinese Academy of Sciences; University of Minnesota+Georgia Institute of Technology; Carnegie Mellon University; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;nlpr.ia.ac.cn;umn.edu;andrew.cmu.edu; gatech.edu; gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;nlpr.ia.ac.cn;umn.edu;andrew.cmu.edu; gatech.edu; gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5227b6aaf294f5f027273aebf16015f2-Abstract.html",
        "aff_unique_index": "0;1;2+0;3;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;Chinese Academy of Sciences;University of Minnesota;Carnegie Mellon University",
        "aff_unique_dep": ";Institute of Automation;;",
        "aff_unique_url": "https://www.gatech.edu;http://www.ia.cas.cn;https://www.minnesota.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Georgia Tech;CAS;UMN;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Deep Lattice Networks and Partial Monotonic Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9083",
        "id": "9083",
        "author_site": "Seungil You, David Ding, Kevin Canini, Jan Pfeifer, Maya Gupta",
        "author": "Seungil You; David Ding; Kevin Canini; Jan Pfeifer; Maya Gupta",
        "abstract": "We propose learning deep models that are monotonic with respect to a user-specified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the Adam optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.",
        "bibtex": "@inproceedings{NIPS2017_464d828b,\n author = {You, Seungil and Ding, David and Canini, Kevin and Pfeifer, Jan and Gupta, Maya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Lattice Networks and Partial Monotonic Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Reviews.html",
        "metareview": "",
        "pdf_size": 471735,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14018878392272886639&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html"
    },
    {
        "title": "Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9336",
        "id": "9336",
        "author_site": "Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun WOO",
        "author": "Xingjian Shi; Zhihan Gao; Leonard Lausen; Hao Wang; Dit-Yan Yeung; Wai-kin Wong; Wang-chun WOO",
        "abstract": "With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.",
        "bibtex": "@inproceedings{NIPS2017_a6db4ed0,\n author = {Shi, Xingjian and Gao, Zhihan and Lausen, Leonard and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and WOO, Wang-chun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a6db4ed04f1621a119799fd3d7545d3d-Reviews.html",
        "metareview": "",
        "pdf_size": 743270,
        "gs_citation": 1128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11455223307443262721&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Hong Kong Observatory, Hong Kong, China; Hong Kong Observatory, Hong Kong, China",
        "aff_domain": "cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk;hko.gov.hk;hko.gov.hk",
        "email": "cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk;hko.gov.hk;hko.gov.hk",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a6db4ed04f1621a119799fd3d7545d3d-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;1;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Hong Kong Observatory, Hong Kong, China",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.ust.hk;",
        "aff_unique_abbr": "HKUST;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Deep Learning with Topological Signatures",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8954",
        "id": "8954",
        "author_site": "Christoph Hofer, Roland Kwitt, Marc Niethammer, Andreas Uhl",
        "author": "Christoph Hofer; Roland Kwitt; Marc Niethammer; Andreas Uhl",
        "abstract": "Inferring topological and geometrical information from data can offer an alternative perspective in machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often  come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.",
        "bibtex": "@inproceedings{NIPS2017_883e881b,\n author = {Hofer, Christoph and Kwitt, Roland and Niethammer, Marc and Uhl, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Learning with Topological Signatures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Reviews.html",
        "metareview": "",
        "pdf_size": 752345,
        "gs_citation": 342,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9292469144097805148&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Salzburg, Austria; University of Salzburg, Austria; UNC Chapel Hill, NC, USA; University of Salzburg, Austria",
        "aff_domain": "cosy.sbg.ac.at;sbg.ac.at;cs.unc.edu;cosy.sbg.ac.at",
        "email": "cosy.sbg.ac.at;sbg.ac.at;cs.unc.edu;cosy.sbg.ac.at",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/883e881bb4d22a7add958f2d6b052c9f-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Salzburg;UNC Chapel Hill, NC, USA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-salzburg.at;",
        "aff_unique_abbr": "USAL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria;"
    },
    {
        "title": "Deep Mean-Shift Priors for Image Restoration",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8871",
        "id": "8871",
        "author_site": "Siavash Arjomand Bigdeli, Matthias Zwicker, Paolo Favaro, Meiguang Jin",
        "author": "Siavash Arjomand Bigdeli; Matthias Zwicker; Paolo Favaro; Meiguang Jin",
        "abstract": "In this paper we introduce a natural image prior that directly represents a Gaussian-smoothed version of the natural image distribution. We include our prior in a formulation of image restoration as a Bayes estimator that also allows us to solve noise-blind image restoration problems. We show that the gradient of our prior corresponds to the mean-shift vector on the natural image distribution. In addition, we learn the mean-shift vector field using denoising autoencoders, and use it in a gradient descent approach to perform Bayes risk minimization. We demonstrate competitive results for noise-blind deblurring, super-resolution, and demosaicing.",
        "bibtex": "@inproceedings{NIPS2017_38913e1d,\n author = {Arjomand Bigdeli, Siavash and Zwicker, Matthias and Favaro, Paolo and Jin, Meiguang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Mean-Shift Priors for Image Restoration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/38913e1d6a7b94cb0f55994f679f5956-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/38913e1d6a7b94cb0f55994f679f5956-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/38913e1d6a7b94cb0f55994f679f5956-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/38913e1d6a7b94cb0f55994f679f5956-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/38913e1d6a7b94cb0f55994f679f5956-Reviews.html",
        "metareview": "",
        "pdf_size": 2594478,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14711119136266528170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Bern; University of Bern; University of Bern; University of Bern + University of Maryland, College Park",
        "aff_domain": "inf.unibe.ch;inf.unibe.ch;inf.unibe.ch;cs.umd.edu",
        "email": "inf.unibe.ch;inf.unibe.ch;inf.unibe.ch;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/38913e1d6a7b94cb0f55994f679f5956-Abstract.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "University of Bern;University of Maryland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unibe.ch;https://www/umd.edu",
        "aff_unique_abbr": "UniBE;UMD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";College Park",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9020",
        "id": "9020",
        "author_site": "Ahmed M. Alaa, Mihaela van der Schaar",
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14850046686238058514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "author": "",
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Abstract.html"
    },
    {
        "title": "Deep Networks for Decoding Natural Images from Retinal Signals",
        "author": "Nikhil Parthasarathy, Eleanor Batty, William Falcon, Thomas Rutten, Mohit Rajpal, E.J. Chichilnisky, Liam Paninski",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10145",
        "id": "10145"
    },
    {
        "title": "Deep Recurrent Neural Network-Based Identification of Precursor microRNAs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9074",
        "id": "9074",
        "author_site": "Seunghyun Park, Seonwoo Min, Hyun-Soo Choi, Sungroh Yoon",
        "author": "Seunghyun Park; Seonwoo Min; Hyun-Soo Choi; Sungroh Yoon",
        "abstract": "MicroRNAs (miRNAs) are small non-coding ribonucleic acids (RNAs) which play key roles in post-transcriptional gene regulation. Direct identification of mature miRNAs is infeasible due to their short lengths, and researchers instead aim at identifying precursor miRNAs (pre-miRNAs). Many of the known pre-miRNAs have distinctive stem-loop secondary structure, and structure-based filtering is usually the first step to predict the possibility of a given sequence being a pre-miRNA. To identify new pre-miRNAs that often have non-canonical structure, however, we need to consider additional features other than structure. To obtain such additional characteristics, existing computational methods rely on manual feature extraction, which inevitably limits the efficiency, robustness, and generalization of computational identification. To address the limitations of existing approaches, we propose a pre-miRNA identification method that incorporates (1) a deep recurrent neural network (RNN) for automated feature learning and classification, (2) multimodal architecture for seamless integration of prior knowledge (secondary structure), (3) an attention mechanism for improving long-term dependence modeling, and (4) an RNN-based class activation mapping for highlighting the learned representations that can contrast pre-miRNAs and non-pre-miRNAs. In our experiments with recent benchmarks, the proposed approach outperformed the compared state-of-the-art alternatives in terms of various performance metrics.",
        "bibtex": "@inproceedings{NIPS2017_b2531e7b,\n author = {Park, Seunghyun and Min, Seonwoo and Choi, Hyun-Soo and Yoon, Sungroh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Recurrent Neural Network-Based Identification of Precursor microRNAs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Reviews.html",
        "metareview": "",
        "pdf_size": 927363,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2653090677047600888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Electrical and Computer Engineering, Seoul National University, Seoul 08826, Korea + School of Electrical Engineering, Korea University, Seoul 02841, Korea; Electrical and Computer Engineering, Seoul National University, Seoul 08826, Korea; Electrical and Computer Engineering, Seoul National University, Seoul 08826, Korea; Electrical and Computer Engineering, Seoul National University, Seoul 08826, Korea",
        "aff_domain": "snu.ac.kr; ; ;snu.ac.kr",
        "email": "snu.ac.kr; ; ;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Electrical and Computer Engineering, Seoul National University, Seoul 08826, Korea;School of Electrical Engineering, Korea University, Seoul 02841, Korea",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Deep Reinforcement Learning from Human Preferences",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9209",
        "id": "9209",
        "author_site": "Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei",
        "author": "Paul F Christiano; Jan Leike; Tom Brown; Miljan Martic; Shane Legg; Dario Amodei",
        "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.",
        "bibtex": "@inproceedings{NIPS2017_d5e2c0ad,\n author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Reinforcement Learning from Human Preferences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Reviews.html",
        "metareview": "",
        "pdf_size": 2518627,
        "gs_citation": 4197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16666410803638838470&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "OpenAI; DeepMind; Google Brain+OpenAI; DeepMind; DeepMind; OpenAI",
        "aff_domain": "openai.com;google.com;google.com;google.com;google.com;openai.com",
        "email": "openai.com;google.com;google.com;google.com;google.com;openai.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html",
        "aff_unique_index": "0;1;2+0;1;1;0",
        "aff_unique_norm": "OpenAI;DeepMind;Google",
        "aff_unique_dep": ";;Google Brain",
        "aff_unique_url": "https://openai.com;https://deepmind.com;https://brain.google.com",
        "aff_unique_abbr": "OpenAI;DeepMind;Google Brain",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0+0;1;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Deep Sets",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9123",
        "id": "9123",
        "author_site": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, Alexander Smola",
        "author": "Manzil Zaheer; Satwik Kottur; Siamak Ravanbakhsh; Barnabas Poczos; Ruslan Salakhutdinov; Alexander J Smola",
        "abstract": "We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.",
        "bibtex": "@inproceedings{NIPS2017_f22e4747,\n author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Reviews.html",
        "metareview": "",
        "pdf_size": 1990691,
        "gs_citation": 3299,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2295404778383262980&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University+Amazon Web Services; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University+Amazon Web Services",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html",
        "aff_unique_index": "0+1;0;0;0;0;0+1",
        "aff_unique_norm": "Carnegie Mellon University;Amazon Web Services",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://aws.amazon.com",
        "aff_unique_abbr": "CMU;AWS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Subspace Clustering Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8800",
        "id": "8800",
        "author_site": "Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, Ian Reid",
        "author": "Pan Ji; Tong Zhang; Hongdong Li; Mathieu Salzmann; Ian Reid",
        "abstract": "We present a novel deep neural network architecture for unsupervised subspace clustering. This architecture is built upon deep auto-encoders, which non-linearly map the input data into a latent space. Our key idea is to introduce a novel self-expressive layer between the encoder and the decoder to mimic the \"self-expressiveness\" property that has proven effective in traditional subspace clustering. Being differentiable, our new self-expressive layer provides a simple but effective way to learn pairwise affinities between all data points through a standard back-propagation procedure. Being nonlinear, our neural-network based method is able to cluster data points having complex (often nonlinear) structures. We further propose pre-training and fine-tuning strategies that let us effectively learn the parameters of our subspace clustering networks. Our experiments show that the proposed method significantly outperforms the state-of-the-art unsupervised subspace clustering methods.",
        "bibtex": "@inproceedings{NIPS2017_e369853d,\n author = {Ji, Pan and Zhang, Tong and Li, Hongdong and Salzmann, Mathieu and Reid, Ian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Subspace Clustering Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e369853df766fa44e1ed0ff613f563bd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e369853df766fa44e1ed0ff613f563bd-Reviews.html",
        "metareview": "",
        "pdf_size": 1838161,
        "gs_citation": 704,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8850420811021780590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "University of Adelaide; Australian National University; Australian National University; EPFL - CVLab; University of Adelaide",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e369853df766fa44e1ed0ff613f563bd-Abstract.html",
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "University of Adelaide;Australian National University;EPFL - CVLab",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.anu.edu.au;",
        "aff_unique_abbr": "Adelaide;ANU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia;"
    },
    {
        "title": "Deep Supervised Discrete Hashing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9035",
        "id": "9035",
        "author_site": "Qi Li, Zhenan Sun, Ran He, Tieniu Tan",
        "author": "Qi Li; Zhenan Sun; Ran He; Tieniu Tan",
        "abstract": "With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.",
        "bibtex": "@inproceedings{NIPS2017_e94f63f5,\n author = {Li, Qi and Sun, Zhenan and He, Ran and Tan, Tieniu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Supervised Discrete Hashing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Reviews.html",
        "metareview": "",
        "pdf_size": 265524,
        "gs_citation": 386,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8474262354400892550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Research on Intelligent Perception and Computing; National Laboratory of Pattern Recognition; CAS Center for Excellence in Brain Science and Intelligence Technology; Institute of Automation, Chinese Academy of Sciences",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e94f63f579e05cb49c05c2d050ead9c0-Abstract.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Center for Research on Intelligent Perception and Computing;National Laboratory of Pattern Recognition;Chinese Academy of Sciences",
        "aff_unique_dep": ";;Center for Excellence in Brain Science and Intelligence Technology",
        "aff_unique_url": ";http://www.nlpr.ia.ac.cn/en.html;http://www.cas.cn/",
        "aff_unique_abbr": ";NLPR;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9081",
        "id": "9081",
        "author_site": "Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou",
        "author": "Andrew Gibiansky; Sercan Arik; Gregory Diamos; John Miller; Kainan Peng; Wei Ping; Jonathan Raiman; Yanqi Zhou",
        "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with low-dimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-of-the-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.",
        "bibtex": "@inproceedings{NIPS2017_c59b469d,\n author = {Gibiansky, Andrew and Arik, Sercan and Diamos, Gregory and Miller, John and Peng, Kainan and Ping, Wei and Raiman, Jonathan and Zhou, Yanqi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Voice 2: Multi-Speaker Neural Text-to-Speech},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Reviews.html",
        "metareview": "",
        "pdf_size": 888447,
        "gs_citation": 452,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=188402189689268087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab; Baidu Silicon Valley Artificial Intelligence Lab",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c59b469d724f7919b7d35514184fdc0f-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Artificial Intelligence Lab",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu AI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Silicon Valley",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deliberation Networks: Sequence Generation Beyond One-Pass Decoding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8968",
        "id": "8968",
        "author_site": "Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, Tie-Yan Liu",
        "author": "Yingce Xia; Fei Tian; Lijun Wu; Jianxin Lin; Tao Qin; Nenghai Yu; Tie-Yan Liu",
        "abstract": "The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.",
        "bibtex": "@inproceedings{NIPS2017_c6036a69,\n author = {Xia, Yingce and Tian, Fei and Wu, Lijun and Lin, Jianxin and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deliberation Networks: Sequence Generation Beyond One-Pass Decoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Reviews.html",
        "metareview": "",
        "pdf_size": 392113,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5359968740795634948&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China; Sun Yat-sen University, Guangzhou, China; University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China",
        "aff_domain": "gmail.com;microsoft.com;mail2.sysu.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn;microsoft.com",
        "email": "gmail.com;microsoft.com;mail2.sysu.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c6036a69be21cb660499b75718a3ef24-Abstract.html",
        "aff_unique_index": "0;1;2;0;1;0;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft Research;Sun Yat-sen University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "USTC;MSR;SYSU",
        "aff_campus_unique_index": "0;1;2;0;1;0;1",
        "aff_campus_unique": "Hefei;Beijing;Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Detrended Partial Cross Correlation for Brain Connectivity Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8883",
        "id": "8883",
        "author_site": "Jaime Ide, F\u00e1bio Cappabianco, Fabio Faria, Chiang-shan R Li",
        "author": "Jaime Ide; F\u00e1bio Cappabianco; Fabio Faria; Chiang-shan R Li",
        "abstract": "Brain connectivity analysis is a critical component of ongoing human connectome projects to decipher the healthy and diseased brain. Recent work has highlighted the power-law (multi-time scale) properties of brain signals; however, there remains a lack of methods to specifically quantify short- vs. long- time range brain connections. In this paper, using detrended partial cross-correlation analysis (DPCCA), we propose a novel functional connectivity measure to delineate brain interactions at multiple time scales, while controlling for covariates. We use a rich simulated fMRI dataset to validate the proposed method, and apply it to a real fMRI dataset in a cocaine dependence prediction task. We show that, compared to extant methods, the DPCCA-based approach not only distinguishes short and long memory functional connectivity but also improves feature extraction and enhances classification accuracy. Together, this paper contributes broadly to new computational methodologies in understanding neural information processing.",
        "bibtex": "@inproceedings{NIPS2017_ffeabd22,\n author = {Ide, Jaime and Cappabianco, F\\'{a}bio and Faria, Fabio and Li, Chiang-shan R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Detrended Partial Cross Correlation for Brain Connectivity Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ffeabd223de0d4eacb9a3e6e53e5448d-Reviews.html",
        "metareview": "",
        "pdf_size": 1535630,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9673598060188437852&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Yale University; Federal University of Sao Paulo; Federal University of Sao Paulo; Yale University",
        "aff_domain": "yale.edu;unifesp.br;unifesp.br; chiang-shan.li-yale.edu",
        "email": "yale.edu;unifesp.br;unifesp.br; chiang-shan.li-yale.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Yale University;Federal University of Sao Paulo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yale.edu;",
        "aff_unique_abbr": "Yale;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9019",
        "id": "9019",
        "author_site": "Fan Yang, Zhilin Yang, William Cohen",
        "author": "Fan Yang; Zhilin Yang; William W. Cohen",
        "abstract": "We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.",
        "bibtex": "@inproceedings{NIPS2017_0e55666a,\n author = {Yang, Fan and Yang, Zhilin and Cohen, William W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Differentiable Learning of Logical Rules for Knowledge Base Reasoning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Reviews.html",
        "metareview": "",
        "pdf_size": 629908,
        "gs_citation": 826,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14303079091209893677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Science; School of Computer Science; School of Computer Science",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0e55666a4ad822e0e34299df3591d979-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "School of Computer Science",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Differentiable Learning of Submodular Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8895",
        "id": "8895",
        "author_site": "Josip Djolonga, Andreas Krause",
        "author": "Josip Djolonga; Andreas Krause",
        "abstract": "Can we incorporate discrete optimization algorithms within modern machine learning models? For example, is it possible to use in deep architectures a layer whose output is the minimal cut of a parametrized graph? Given that these models are trained end-to-end by leveraging gradient information, the introduction of such layers seems very challenging due to their non-continuous output. In this paper we focus on the problem of submodular minimization, for which we show that such layers are indeed possible. The key idea is that we can continuously relax the output without sacrificing guarantees. We provide an easily computable approximation to the Jacobian complemented with a complete theoretical analysis. Finally, these contributions let us experimentally learn probabilistic log-supermodular models via a bi-level variational inference formulation.",
        "bibtex": "@inproceedings{NIPS2017_192fc044,\n author = {Djolonga, Josip and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Differentiable Learning of Submodular Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/192fc044e74dffea144f9ac5dc9f3395-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/192fc044e74dffea144f9ac5dc9f3395-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/192fc044e74dffea144f9ac5dc9f3395-Reviews.html",
        "metareview": "",
        "pdf_size": 671964,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17143970511880665307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich",
        "aff_domain": "inf.ethz.ch;ethz.ch",
        "email": "inf.ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/192fc044e74dffea144f9ac5dc9f3395-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Differentially Private Empirical Risk Minimization Revisited: Faster and More General",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9058",
        "id": "9058",
        "author_site": "Di Wang, Minwei Ye, Jinhui Xu",
        "author": "Di Wang; Minwei Ye; Jinhui Xu",
        "abstract": "In this paper we study differentially private Empirical Risk Minimization(ERM) in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization, we give algorithms which achieve either optimal or near optimal utility bound with less gradient complexity compared with previous work.  For ERM with smooth convex loss function in high-dimension($p\\gg n$) setting, we give an algorithm which achieves the upper bound with less gradient complexity than previous ones. At last, we generalize the expected excess empirical risk from convex to Polyak-Lojasiewicz condition and give a tighter upper bound of the utility comparing with the result in \\cite{DBLP:journals/corr/ZhangZMW17}.",
        "bibtex": "@inproceedings{NIPS2017_f337d999,\n author = {Wang, Di and Ye, Minwei and Xu, Jinhui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Differentially Private Empirical Risk Minimization Revisited: Faster and More General},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Reviews.html",
        "metareview": "",
        "pdf_size": 307733,
        "gs_citation": 343,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10721287206685315265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Computer Science and Engineering, State University of New York at Buffalo; Dept. of Computer Science and Engineering, State University of New York at Buffalo; Dept. of Computer Science and Engineering, State University of New York at Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f337d999d9ad116a7b4f3d409fcc6480-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Dept. of Computer Science and Engineering, State University of New York at Buffalo",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Differentially private Bayesian learning on distributed data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9107",
        "id": "9107",
        "author_site": "Mikko Heikkil\u00e4, Eemil Lagerspetz, Samuel Kaski, Kana Shimizu, Sasu Tarkoma, Antti Honkela",
        "author": "Mikko Heikkil\u00e4; Eemil Lagerspetz; Samuel Kaski; Kana Shimizu; Sasu Tarkoma; Antti Honkela",
        "abstract": "Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness, or add prohibitive amounts of noise. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.",
        "bibtex": "@inproceedings{NIPS2017_dfce0680,\n author = {Heikkil\\\"{a}, Mikko and Lagerspetz, Eemil and Kaski, Samuel and Shimizu, Kana and Tarkoma, Sasu and Honkela, Antti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Differentially private Bayesian learning on distributed data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dfce06801e1a85d6d06f1fdd4475dacd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dfce06801e1a85d6d06f1fdd4475dacd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dfce06801e1a85d6d06f1fdd4475dacd-Reviews.html",
        "metareview": "",
        "pdf_size": 360980,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11768270138760952075&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Helsinki Institute for Information Technology HIIT, Department of Mathematics and Statistics, University of Helsinki; Helsinki Institute for Information Technology HIIT, Department of Computer Science, University of Helsinki; Helsinki Institute for Information Technology HIIT, Department of Computer Science, Aalto University; Department of Computer Science and Engineering, Waseda University; Helsinki Institute for Information Technology HIIT, Department of Computer Science, University of Helsinki; Helsinki Institute for Information Technology HIIT, Department of Mathematics and Statistics, University of Helsinki + Department of Public Health, University of Helsinki",
        "aff_domain": "helsinki.fi;helsinki.fi;aalto.fi;gmail.com;helsinki.fi;helsinki.fi",
        "email": "helsinki.fi;helsinki.fi;aalto.fi;gmail.com;helsinki.fi;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dfce06801e1a85d6d06f1fdd4475dacd-Abstract.html",
        "aff_unique_index": "0;1;2;3;1;0+4",
        "aff_unique_norm": "Helsinki Institute for Information Technology HIIT, Department of Mathematics and Statistics, University of Helsinki;University of Helsinki;Aalto University;Waseda University;Department of Public Health, University of Helsinki",
        "aff_unique_dep": ";Department of Computer Science;Department of Computer Science;Department of Computer Science and Engineering;",
        "aff_unique_url": ";https://www.helsinki.fi;https://www.aalto.fi;https://www.waseda.jp/top;",
        "aff_unique_abbr": ";UH;Aalto;Waseda;",
        "aff_campus_unique_index": "1;1;1;",
        "aff_campus_unique": ";Helsinki",
        "aff_country_unique_index": "1;1;2;1;",
        "aff_country_unique": ";Finland;Japan"
    },
    {
        "title": "Diffusion Approximations for Online Principal Component Estimation and Global Convergence",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8860",
        "id": "8860",
        "author_site": "Chris Junchi Li, Mengdi Wang, Tong Zhang",
        "author": "Chris Junchi Li; Mengdi Wang; Han Liu; Tong Zhang",
        "abstract": "In this paper, we propose to adopt the diffusion approximation tools to study the dynamics of Oja's iteration which is an online stochastic gradient method for the principal component analysis. Oja's iteration maintains a running estimate of the true principal component from streaming data and enjoys less temporal and spatial complexities. We show that the Oja's iteration for the top eigenvector generates a continuous-state discrete-time Markov chain over the unit sphere. We characterize the Oja's iteration in three phases using diffusion approximation and weak convergence tools. Our three-phase analysis further provides a finite-sample error bound for the running estimate, which matches the minimax information lower bound for PCA under the additional assumption of bounded samples.",
        "bibtex": "@inproceedings{NIPS2017_13f3cf8c,\n author = {Li, Chris Junchi and Wang, Mengdi and Liu, Han and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Diffusion Approximations for Online Principal Component Estimation and Global Convergence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/13f3cf8c531952d72e5847c4183e6910-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/13f3cf8c531952d72e5847c4183e6910-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/13f3cf8c531952d72e5847c4183e6910-Reviews.html",
        "metareview": "",
        "pdf_size": 1245742,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8088226551167322627&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Princeton University; Princeton University; Princeton University; Tencent AI Lab",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;tongzhang-ml.org",
        "email": "princeton.edu;princeton.edu;princeton.edu;tongzhang-ml.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/13f3cf8c531952d72e5847c4183e6910-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Princeton University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.princeton.edu;https://ai.tencent.com",
        "aff_unique_abbr": "Princeton;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Dilated Recurrent Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8806",
        "id": "8806",
        "author_site": "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, Thomas Huang",
        "author": "Shiyu Chang; Yang Zhang; Wei Han; Mo Yu; Xiaoxiao Guo; Wei Tan; Xiaodong Cui; Michael Witbrock; Mark A Hasegawa-Johnson; Thomas S. Huang",
        "abstract": "Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task.  There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DilatedRNN, which simultaneously tackles all of these challenges.  The proposed architecture is characterized by multi-resolution dilated recurrent skip connections and can be combined flexibly with diverse RNN cells.  Moreover, the DilatedRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies.  To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures.  We rigorously prove the advantages of the DilatedRNN over other recurrent neural architectures.  The code for our method is publicly available at https://github.com/code-terminator/DilatedRNN.",
        "bibtex": "@inproceedings{NIPS2017_32bb90e8,\n author = {Chang, Shiyu and Zhang, Yang and Han, Wei and Yu, Mo and Guo, Xiaoxiao and Tan, Wei and Cui, Xiaodong and Witbrock, Michael and Hasegawa-Johnson, Mark A and Huang, Thomas S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dilated Recurrent Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/32bb90e8976aab5298d5da10fe66f21d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/32bb90e8976aab5298d5da10fe66f21d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/32bb90e8976aab5298d5da10fe66f21d-Reviews.html",
        "metareview": "",
        "pdf_size": 2198023,
        "gs_citation": 422,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4803057263751666378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IBM Thomas J. Watson Research Center; IBM Thomas J. Watson Research Center; University of Illinois at Urbana-Champaign; IBM Thomas J. Watson Research Center; IBM Thomas J. Watson Research Center; IBM Thomas J. Watson Research Center; IBM Thomas J. Watson Research Center; IBM Thomas J. Watson Research Center; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "ibm.com;ibm.com;illinois.edu;us.ibm.com;ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;illinois.edu;illinois.edu",
        "email": "ibm.com;ibm.com;illinois.edu;us.ibm.com;ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;illinois.edu;illinois.edu",
        "github": "https://github.com/code-terminator/DilatedRNN",
        "project": "",
        "author_num": 10,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/32bb90e8976aab5298d5da10fe66f21d-Abstract.html",
        "aff_unique_index": "0;0;1;0;0;0;0;0;1;1",
        "aff_unique_norm": "IBM;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Research;",
        "aff_unique_url": "https://www.ibm.com/research;https://illinois.edu",
        "aff_unique_abbr": "IBM;UIUC",
        "aff_campus_unique_index": "0;0;1;0;0;0;0;0;1;1",
        "aff_campus_unique": "Yorktown Heights;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discovering Potential Correlations via Hypercontractivity",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9235",
        "id": "9235",
        "author_site": "Hyeji Kim, Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",
        "author": "Hyeji Kim; Weihao Gao; Sreeram Kannan; Sewoong Oh; Pramod Viswanath",
        "abstract": "Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical examples of potential correlations.",
        "bibtex": "@inproceedings{NIPS2017_dcf6070a,\n author = {Kim, Hyeji and Gao, Weihao and Kannan, Sreeram and Oh, Sewoong and Viswanath, Pramod},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discovering Potential Correlations via Hypercontractivity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dcf6070a4ab7f3afbfd2809173e0824b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dcf6070a4ab7f3afbfd2809173e0824b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dcf6070a4ab7f3afbfd2809173e0824b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dcf6070a4ab7f3afbfd2809173e0824b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dcf6070a4ab7f3afbfd2809173e0824b-Reviews.html",
        "metareview": "",
        "pdf_size": 608463,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3347146102242836468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of Illinois at Urbana Champaign+Coordinated Science Lab and Department of Electrical and Computer Engineering; University of Illinois at Urbana Champaign+Coordinated Science Lab and Department of Electrical and Computer Engineering; University of Washington+Department of Electrical Engineering; University of Illinois at Urbana Champaign+Coordinated Science Lab and Department of Industrial and Enterprise Systems Engineering; University of Illinois at Urbana Champaign+Coordinated Science Lab and Department of Electrical and Computer Engineering",
        "aff_domain": "illinois.edu;illinois.edu;uw.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;uw.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dcf6070a4ab7f3afbfd2809173e0824b-Abstract.html",
        "aff_unique_index": "0+1;0+1;2+3;0+4;0+1",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Coordinated Science Lab and Department of Electrical and Computer Engineering;University of Washington;Institution not specified;Coordinated Science Lab and Department of Industrial and Enterprise Systems Engineering",
        "aff_unique_dep": ";;;Department of Electrical Engineering;",
        "aff_unique_url": "https://illinois.edu;;https://www.washington.edu;;",
        "aff_unique_abbr": "UIUC;;UW;;",
        "aff_campus_unique_index": "0;0;;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Discriminative State Space Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9341",
        "id": "9341",
        "author_site": "Vitaly Kuznetsov, Mehryar Mohri",
        "author": "Vitaly Kuznetsov; Mehryar Mohri",
        "abstract": "In this paper, we introduce and analyze Discriminative State-Space Models for forecasting non-stationary time series. We provide data-dependent generalization guarantees for learning these models based on the recently introduced notion of discrepancy. We provide an in-depth analysis of the complexity of such models. Finally, we also study the generalization guarantees for several structural risk minimization approaches to this problem and provide an efficient implementation for one of them which is based on a convex objective.",
        "bibtex": "@inproceedings{NIPS2017_6fe13163,\n author = {Kuznetsov, Vitaly and Mohri, Mehryar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative State Space Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6fe131632103526e3a6e8114c78eb1e1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6fe131632103526e3a6e8114c78eb1e1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6fe131632103526e3a6e8114c78eb1e1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6fe131632103526e3a6e8114c78eb1e1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6fe131632103526e3a6e8114c78eb1e1-Reviews.html",
        "metareview": "",
        "pdf_size": 647597,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9370592374259838918&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; Courant Institute and Google Research",
        "aff_domain": "cims.nyu.edu;cims.nyu.edu",
        "email": "cims.nyu.edu;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6fe131632103526e3a6e8114c78eb1e1-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;Courant Institute",
        "aff_unique_dep": "Google Research;Courant Institute",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu",
        "aff_unique_abbr": "Google Research;Courant",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Distral: Robust multitask reinforcement learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9227",
        "id": "9227",
        "author_site": "Yee Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, Razvan Pascanu",
        "author": "Yee Teh; Victor Bapst; Wojciech M. Czarnecki; John Quan; James Kirkpatrick; Raia Hadsell; Nicolas Heess; Razvan Pascanu",
        "abstract": "Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however,  this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new  approach for joint training of multiple tasks, which we refer to as Distral (DIStill & TRAnsfer Learning). Instead of sharing parameters between the different workers, we propose to share a distilled policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.",
        "bibtex": "@inproceedings{NIPS2017_0abdc563,\n author = {Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M. and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distral: Robust multitask reinforcement learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Reviews.html",
        "metareview": "",
        "pdf_size": 1558854,
        "gs_citation": 677,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5010350299610830290&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0abdc563a06105aee3c6136871c9f4d1-Abstract.html"
    },
    {
        "title": "Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9349",
        "id": "9349",
        "author_site": "Liwei Wang, Alex Schwing, Svetlana Lazebnik",
        "author": "Liwei Wang; Alexander Schwing; Svetlana Lazebnik",
        "abstract": "This paper explores image caption generation using conditional variational auto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield descriptions with too little variability. Instead, we propose two models that explicitly structure the latent space around K components corresponding to different types of image content, and combine components to create priors for images that contain multiple types of content simultaneously (e.g., several kinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior, while the second one defines a novel Additive Gaussian (AG) prior that linearly combines component means. We show that both models produce captions that are more diverse and more accurate than a strong LSTM baseline or a \u201cvanilla\u201d CVAE with a fixed Gaussian prior, with AG-CVAE showing particular promise.",
        "bibtex": "@inproceedings{NIPS2017_4b21cf96,\n author = {Wang, Liwei and Schwing, Alexander and Lazebnik, Svetlana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4b21cf96d4cf612f239a6c322b10c8fe-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4b21cf96d4cf612f239a6c322b10c8fe-Reviews.html",
        "metareview": "",
        "pdf_size": 7431006,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6278326098998968588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4b21cf96d4cf612f239a6c322b10c8fe-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Diving into the shallows: a computational perspective on large-scale shallow learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9160",
        "id": "9160",
        "author_site": "SIYUAN MA, Mikhail Belkin",
        "author": "SIYUAN MA; Mikhail Belkin",
        "abstract": "Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been  particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have  encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow  architecture.   In this paper we  identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data.  To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a  significant performance  boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.",
        "bibtex": "@inproceedings{NIPS2017_bf424cb7,\n author = {MA, SIYUAN and Belkin, Mikhail},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Diving into the shallows: a computational perspective on large-scale shallow learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Reviews.html",
        "metareview": "",
        "pdf_size": 321029,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12456041046322710643&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University",
        "aff_domain": "cse.ohio-state.edu;cse.ohio-state.edu",
        "email": "cse.ohio-state.edu;cse.ohio-state.edu",
        "github": "",
        "project": "https://arxiv.org/abs/1703.10622",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bf424cb7b0dea050a42b9739eb261a3a-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Ohio State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Do Deep Neural Networks Suffer from Crowding?",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9337",
        "id": "9337",
        "author_site": "Anna Volokitin, Gemma Roig, Tomaso Poggio",
        "author": "Anna Volokitin; Gemma Roig; Tomaso A Poggio",
        "abstract": "Crowding is a visual effect suffered by humans, in which an object that can be recognized in isolation can no longer be recognized when other objects, called flankers, are placed close to it. In this work, we study the effect of crowding in artificial Deep Neural Networks (DNNs) for object recognition. We analyze both deep convolutional neural networks  (DCNNs) as well as an extension of DCNNs that are multi-scale and that change the receptive field size of the convolution filters with their position in the image.  The latter  networks, that we call eccentricity-dependent, have been proposed for modeling the feedforward path of the primate visual cortex. Our results reveal that the eccentricity-dependent model, trained on target objects in isolation, can recognize such targets in the presence of flankers, if the targets are near the center of the image, whereas DCNNs cannot. Also, for all tested networks, when trained on targets in isolation, we find that recognition accuracy of the networks decreases the closer the flankers are to the target and the more flankers there are. We find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding. Additionally, we show that incorporating flankers  into the images of the training set for learning the DNNs does not lead to robustness against configurations not seen at training.",
        "bibtex": "@inproceedings{NIPS2017_c61f571d,\n author = {Volokitin, Anna and Roig, Gemma and Poggio, Tomaso A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Do Deep Neural Networks Suffer from Crowding?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c61f571dbd2fb949d3fe5ae1608dd48b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c61f571dbd2fb949d3fe5ae1608dd48b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c61f571dbd2fb949d3fe5ae1608dd48b-Reviews.html",
        "metareview": "",
        "pdf_size": 4166455,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14081730771172982967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Brains, Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA + Computer Vision Laboratory, ETH Zurich, Switzerland; Center for Brains, Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA + Istituto Italiano di Tecnologia at Massachusetts Institute of Technology, Cambridge, MA + Singapore University of Technology and Design, Singapore; Center for Brains, Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA + Istituto Italiano di Tecnologia at Massachusetts Institute of Technology, Cambridge, MA",
        "aff_domain": "vision.ee.ethz.ch;mit.edu;csail.mit.edu",
        "email": "vision.ee.ethz.ch;mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html",
        "aff_unique_index": "0+1;0+2+3;0+2",
        "aff_unique_norm": "Center for Brains, Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA;ETH Zurich;Istituto Italiano di Tecnologia at Massachusetts Institute of Technology, Cambridge, MA;Singapore University of Technology and Design",
        "aff_unique_dep": ";Computer Vision Laboratory;;",
        "aff_unique_url": ";https://www.ethz.ch;;https://www.sutd.edu.sg",
        "aff_unique_abbr": ";ETHZ;;SUTD",
        "aff_campus_unique_index": "1;;",
        "aff_campus_unique": ";Zurich",
        "aff_country_unique_index": "1;2;",
        "aff_country_unique": ";Switzerland;Singapore"
    },
    {
        "title": "Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8856",
        "id": "8856",
        "author_site": "Tomoya Murata, Taiji Suzuki",
        "author": "Tomoya Murata; Taiji Suzuki",
        "abstract": "We develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches has become a golden standard in the machine learning community, because the mini-batch techniques stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new ``double acceleration'' technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods, and essentially only needs size $\\sqrt{n}$ mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives, where $n$ is the training set size. Further, we show that even in non-mini-batch settings, our method achieves the best known convergence rate for non-strongly convex and strongly convex objectives.",
        "bibtex": "@inproceedings{NIPS2017_75fc093c,\n author = {Murata, Tomoya and Suzuki, Taiji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/75fc093c0ee742f6dddaa13fff98f104-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/75fc093c0ee742f6dddaa13fff98f104-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/75fc093c0ee742f6dddaa13fff98f104-Reviews.html",
        "metareview": "",
        "pdf_size": 774555,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8088389602030456454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "NTT DATA Mathematical Systems Inc., Tokyo, Japan; Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan + PRESTO, Japan Science and Technology Agency, Japan + Center for Advanced Integrated Intelligence Research, RIKEN, Tokyo, Japan",
        "aff_domain": "msi.co.jp;mist.i.u-tokyo.ac.jp",
        "email": "msi.co.jp;mist.i.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html",
        "aff_unique_index": "0;1+2+3",
        "aff_unique_norm": "NTT DATA Mathematical Systems Inc.;Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan;Japan Science and Technology Agency;RIKEN",
        "aff_unique_dep": ";;PRESTO;Center for Advanced Integrated Intelligence Research",
        "aff_unique_url": "https://www.ntt-data.com/;;https://www.jst.go.jp;https://www.riken.jp",
        "aff_unique_abbr": ";;JST;RIKEN",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Japan;"
    },
    {
        "title": "Doubly Stochastic Variational Inference for Deep Gaussian Processes",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9236",
        "id": "9236",
        "author_site": "Hugh Salimbeni, Marc Deisenroth",
        "author": "Hugh Salimbeni; Marc Deisenroth",
        "abstract": "Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.",
        "bibtex": "@inproceedings{NIPS2017_82089746,\n author = {Salimbeni, Hugh and Deisenroth, Marc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Doubly Stochastic Variational Inference for Deep Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Reviews.html",
        "metareview": "",
        "pdf_size": 415968,
        "gs_citation": 541,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10004359812932944966&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Imperial College London + PROWLER.io; Imperial College London + PROWLER.io",
        "aff_domain": "ic.ac.uk;imperial.ac.uk",
        "email": "ic.ac.uk;imperial.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8208974663db80265e9bfe7b222dcb18-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Imperial College London;PROWLER.io",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.imperial.ac.uk;https://prowler.io",
        "aff_unique_abbr": "ICL;PROWLER.io",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "DropoutNet: Addressing Cold Start in Recommender Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9272",
        "id": "9272",
        "author_site": "Maksims Volkovs, Guangwei Yu, Tomi Poutanen",
        "author": "Maksims Volkovs; Guangwei Yu; Tomi Poutanen",
        "abstract": "Latent models have become the default choice for recommender systems due to their performance and scalability. However, research in this area has primarily focused on modeling user-item interactions,  and few latent models have been developed for cold start. Deep learning has recently achieved remarkable success showing excellent results for diverse input types. Inspired by these results we propose a neural network based latent model called DropoutNet to address the cold start problem in recommender systems. Unlike existing approaches that incorporate additional content-based objective terms, we instead focus on the optimization and show that neural network models can be explicitly trained for cold start through dropout. Our model can  be applied on top of any existing latent model effectively providing cold start capabilities, and full power of deep architectures. Empirically we demonstrate  state-of-the-art accuracy on publicly available benchmarks. Code is available at  https://github.com/layer6ai-labs/DropoutNet.",
        "bibtex": "@inproceedings{NIPS2017_dbd22ba3,\n author = {Volkovs, Maksims and Yu, Guangwei and Poutanen, Tomi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {DropoutNet: Addressing Cold Start in Recommender Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dbd22ba3bd0df8f385bdac3e9f8be207-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dbd22ba3bd0df8f385bdac3e9f8be207-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dbd22ba3bd0df8f385bdac3e9f8be207-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dbd22ba3bd0df8f385bdac3e9f8be207-Reviews.html",
        "metareview": "",
        "pdf_size": 815883,
        "gs_citation": 396,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16794824413531209323&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "layer6.ai; layer6.ai; layer6.ai",
        "aff_domain": "layer6.ai;layer6.ai;layer6.ai",
        "email": "layer6.ai;layer6.ai;layer6.ai",
        "github": "https://github.com/layer6ai-labs/DropoutNet",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dbd22ba3bd0df8f385bdac3e9f8be207-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "layer6.ai",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Dual Discriminator Generative Adversarial Nets",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9053",
        "id": "9053",
        "author_site": "Tu Nguyen, Trung Le, Hung Vu, Dinh Phung",
        "author": "Tu Nguyen; Trung Le; Hung Vu; Dinh Phung",
        "abstract": "We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.",
        "bibtex": "@inproceedings{NIPS2017_e60e81c4,\n author = {Nguyen, Tu and Le, Trung and Vu, Hung and Phung, Dinh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dual Discriminator Generative Adversarial Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e60e81c4cbe5171cd654662d9887aec2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e60e81c4cbe5171cd654662d9887aec2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e60e81c4cbe5171cd654662d9887aec2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e60e81c4cbe5171cd654662d9887aec2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e60e81c4cbe5171cd654662d9887aec2-Reviews.html",
        "metareview": "",
        "pdf_size": 1643284,
        "gs_citation": 431,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5998408572298019483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Deakin University, Geelong, Australia; Deakin University, Geelong, Australia; Deakin University, Geelong, Australia; Deakin University, Geelong, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e60e81c4cbe5171cd654662d9887aec2-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Deakin University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.deakin.edu.au",
        "aff_unique_abbr": "Deakin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Geelong",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Dual Path Networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9224",
        "id": "9224",
        "author_site": "Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng",
        "author": "Yunpeng Chen; Jianan Li; Huaxin Xiao; Xiaojie Jin; Shuicheng Yan; Jiashi Feng",
        "abstract": "In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.",
        "bibtex": "@inproceedings{NIPS2017_f7e0b956,\n author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dual Path Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f7e0b956540676a129760a3eae309294-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f7e0b956540676a129760a3eae309294-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f7e0b956540676a129760a3eae309294-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f7e0b956540676a129760a3eae309294-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f7e0b956540676a129760a3eae309294-Reviews.html",
        "metareview": "",
        "pdf_size": 948161,
        "gs_citation": 1090,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11426486763420133960&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f7e0b956540676a129760a3eae309294-Abstract.html"
    },
    {
        "title": "Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8805",
        "id": "8805",
        "author_site": "Jian Zhao, Lin Xiong, Panasonic Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Panasonic Sugiri Pranata, Panasonic Shengmei Shen, Shuicheng Yan, Jiashi Feng",
        "author": "Jian Zhao; Lin Xiong; Panasonic Karlekar Jayashree; Jianshu Li; Fang Zhao; Zhecan Wang; Panasonic Sugiri Pranata; Panasonic Shengmei Shen; Shuicheng Yan; Jiashi Feng",
        "abstract": "Synthesizing realistic profile faces is promising for more efficiently  training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively.",
        "bibtex": "@inproceedings{NIPS2017_7cbbc409,\n author = {Zhao, Jian and Xiong, Lin and Karlekar Jayashree, Panasonic and Li, Jianshu and Zhao, Fang and Wang, Zhecan and Sugiri Pranata, Panasonic and Shengmei Shen, Panasonic and Yan, Shuicheng and Feng, Jiashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Reviews.html",
        "metareview": "",
        "pdf_size": 1298170,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1295332317556669493&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "National University of Singapore+National University of Defense Technology; Panasonic R&D Center Singapore; Panasonic R&D Center Singapore; National University of Singapore; National University of Singapore; Franklin. W. Olin College of Engineering; Panasonic R&D Center Singapore; Panasonic R&D Center Singapore; National University of Singapore+Qihoo 360 AI Institute; National University of Singapore",
        "aff_domain": "u.nus.edu;u.nus.edu;sg.panasonic.com;sg.panasonic.com;sg.panasonic.com;sg.panasonic.com;students.olin.edu;u.nus.edu;u.nus.edu;u.nus.edu",
        "email": "u.nus.edu;u.nus.edu;sg.panasonic.com;sg.panasonic.com;sg.panasonic.com;sg.panasonic.com;students.olin.edu;u.nus.edu;u.nus.edu;u.nus.edu",
        "github": "https://zhaoj9014.github.io/",
        "project": "",
        "author_num": 10,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7cbbc409ec990f19c78c75bd1e06f215-Abstract.html",
        "aff_unique_index": "0+1;2;2;0;0;3;2;2;0+4;0",
        "aff_unique_norm": "National University of Singapore;National University of Defense Technology;Panasonic;Franklin. W. Olin College of Engineering;Qihoo 360",
        "aff_unique_dep": ";;R&D Center;;AI Institute",
        "aff_unique_url": "https://www.nus.edu.sg;http://www.nudt.edu.cn/;https://www.panasonic.com.sg;;https://www.qihoo.net",
        "aff_unique_abbr": "NUS;NUDT;Panasonic;;Qihoo",
        "aff_campus_unique_index": ";1;1;1;1;",
        "aff_campus_unique": ";Singapore",
        "aff_country_unique_index": "0+1;0;0;0;0;0;0;0+1;0",
        "aff_country_unique": "Singapore;China;"
    },
    {
        "title": "Dualing GANs",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9335",
        "id": "9335",
        "author_site": "Yujia Li, Alex Schwing, Kuan-Chieh Wang, Richard Zemel",
        "author": "Yujia Li; Alexander Schwing; Kuan-Chieh Wang; Richard Zemel",
        "abstract": "Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its saddle point formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this \u2018dualing GAN\u2019 act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.",
        "bibtex": "@inproceedings{NIPS2017_12a1d073,\n author = {Li, Yujia and Schwing, Alexander and Wang, Kuan-Chieh and Zemel, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dualing GANs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/12a1d073d5ed3fa12169c67c4e2ce415-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/12a1d073d5ed3fa12169c67c4e2ce415-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/12a1d073d5ed3fa12169c67c4e2ce415-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/12a1d073d5ed3fa12169c67c4e2ce415-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/12a1d073d5ed3fa12169c67c4e2ce415-Reviews.html",
        "metareview": "",
        "pdf_size": 2374554,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5910580075782327238&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Toronto + Vector Institute; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Toronto + Vector Institute; Department of Computer Science, University of Toronto + Vector Institute",
        "aff_domain": "cs.toronto.edu;illinois.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;illinois.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/12a1d073d5ed3fa12169c67c4e2ce415-Abstract.html",
        "aff_unique_index": "0+1;2;0+1;0+1",
        "aff_unique_norm": "University of Toronto;Vector Institute;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science;;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utoronto.ca;https://vectorinstitute.ai/;https://illinois.edu",
        "aff_unique_abbr": "U of T;Vector Institute;UIUC",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Toronto;;Urbana-Champaign",
        "aff_country_unique_index": "0+0;1;0+0;0+0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8848",
        "id": "8848",
        "author_site": "Ryan Tibshirani",
        "author": "Ryan J Tibshirani",
        "abstract": "We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the penalty is a separable sum of support functions, is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra's algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.",
        "bibtex": "@inproceedings{NIPS2017_5ef698cd,\n author = {Tibshirani, Ryan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dykstra\\textquotesingle s Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5ef698cd9fe650923ea331c15af3b160-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5ef698cd9fe650923ea331c15af3b160-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5ef698cd9fe650923ea331c15af3b160-Reviews.html",
        "metareview": "",
        "pdf_size": 1606667,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17314064465194884193&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics and Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "stat.cmu.edu",
        "email": "stat.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Statistics and Machine Learning Department, Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "title": "Dynamic Importance Sampling for Anytime Bounds of the Partition Function",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9104",
        "id": "9104",
        "author_site": "Qi Lou, Rina Dechter, Alexander Ihler",
        "author": "Qi Lou; Rina Dechter; Alex Ihler",
        "abstract": "Computing the partition function is a key inference task in many graphical models. In this paper, we propose a dynamic importance sampling scheme that provides  anytime finite-sample bounds for the partition function. Our algorithm balances the advantages of the three major inference strategies, heuristic search, variational bounds, and Monte Carlo methods, blending sampling with search to refine a variationally defined proposal. Our algorithm combines and generalizes recent work on anytime search and probabilistic bounds of the partition function. By using an intelligently chosen weighted average over the samples, we construct an unbiased estimator of the partition function with strong finite-sample confidence intervals that inherit both the rapid early improvement rate of sampling and the long-term benefits of an improved proposal from search. This gives significantly improved anytime behavior, and more flexible trade-offs between memory, time, and solution quality. We demonstrate the effectiveness of our approach empirically  on real-world problem instances taken from recent UAI competitions.",
        "bibtex": "@inproceedings{NIPS2017_1f1baa5b,\n author = {Lou, Qi and Dechter, Rina and Ihler, Alexander T},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Importance Sampling for Anytime Bounds of the Partition Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1f1baa5b8edac74eb4eaa329f14a0361-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1f1baa5b8edac74eb4eaa329f14a0361-Reviews.html",
        "metareview": "",
        "pdf_size": 705071,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16006217850535069800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science, Univ. of California, Irvine; Computer Science, Univ. of California, Irvine; Computer Science, Univ. of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Computer Science, Univ. of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Dynamic Revenue Sharing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9054",
        "id": "9054",
        "author_site": "Santiago Balseiro, Max Lin, Vahab Mirrokni, Renato Leme, IIIS Song Zuo",
        "author": "Santiago Balseiro; Max Lin; Vahab Mirrokni; Renato Leme; IIIS Song Zuo",
        "abstract": "Many online platforms act as intermediaries between a seller and a set of buyers. Examples of such settings include online retailers (such as Ebay) selling items on behalf of sellers to buyers, or advertising exchanges (such as AdX) selling pageviews on behalf of publishers to advertisers. In such settings, revenue sharing is a central part of running such a marketplace for the intermediary, and fixed-percentage revenue sharing schemes are often used to split the revenue among the platform and the sellers. In particular, such revenue sharing schemes require the platform to (i) take at most a constant fraction \\alpha of the revenue from auctions and (ii) pay the seller at least the seller declared opportunity cost c for each item sold. A straightforward way to satisfy the constraints is to set a reserve price at c / (1 - \\alpha) for each item, but it is not the optimal solution on maximizing the profit of the intermediary.  While previous studies (by Mirrokni and Gomes, and by Niazadeh et al) focused on revenue-sharing schemes in static double auctions, in this paper, we take advantage of the repeated nature of the auctions. In particular, we introduce dynamic revenue sharing schemes where we balance the two constraints over different auctions to achieve higher profit and seller revenue. This is directly motivated by the practice of advertising exchanges where the fixed-percentage revenue-share should be met across all auctions and not in each auction. In this paper, we characterize the optimal revenue sharing scheme that satisfies both constraints in expectation. Finally, we empirically evaluate our revenue sharing scheme on real data.",
        "bibtex": "@inproceedings{NIPS2017_cb8acb1d,\n author = {Balseiro, Santiago and Lin, Max and Mirrokni, Vahab and Leme, Renato and Song Zuo, IIIS},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Revenue Sharing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cb8acb1dc9821bf74e6ca9068032d623-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cb8acb1dc9821bf74e6ca9068032d623-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cb8acb1dc9821bf74e6ca9068032d623-Reviews.html",
        "metareview": "",
        "pdf_size": 376029,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5528050197242510414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Columbia University; Google; Google; Google; Tsinghua University",
        "aff_domain": "columbia.edu;google.com;google.com;google.com;gmail.com",
        "email": "columbia.edu;google.com;google.com;google.com;gmail.com",
        "github": "",
        "project": "https://ssrn.com/abstract=2956715",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cb8acb1dc9821bf74e6ca9068032d623-Abstract.html",
        "aff_unique_index": "0;1;1;1;2",
        "aff_unique_norm": "Columbia University;Google;Tsinghua University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.columbia.edu;https://www.google.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Columbia;Google;THU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Dynamic Routing Between Capsules",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9167",
        "id": "9167",
        "author_site": "Sara Sabour, Nicholas Frosst, Geoffrey E Hinton",
        "author": "Sara Sabour; Nicholas Frosst; Geoffrey E. Hinton",
        "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules.  When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",
        "bibtex": "@inproceedings{NIPS2017_2cad8fa4,\n author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Routing Between Capsules},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Reviews.html",
        "metareview": "",
        "pdf_size": 839709,
        "gs_citation": 6611,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5914955692202761908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 31,
        "aff": "Google Brain; Google Brain; Google Brain",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Brain",
        "aff_unique_url": "https://brain.google.com",
        "aff_unique_abbr": "Google Brain",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8811",
        "id": "8811",
        "author_site": "El Mahdi El-Mhamdi, Rachid Guerraoui, Hadrien Hendrikx, Alexandre Maurer",
        "author": "El Mahdi El Mhamdi; Rachid Guerraoui; Hadrien Hendrikx; Alexandre Maurer",
        "abstract": "In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to interrupt an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined safe interruptibility for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces dynamic safe interruptibility, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: joint action learners and independent learners. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.",
        "bibtex": "@inproceedings{NIPS2017_812b4ba2,\n author = {El Mhamdi, El Mahdi and Guerraoui, Rachid and Hendrikx, Hadrien and Maurer, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Reviews.html",
        "metareview": "",
        "pdf_size": 241081,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2836514742555030210&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "EPFL, Switzerland; EPFL, Switzerland; Ecole Polytechnique, France; EPFL, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;gmail.com;epfl.ch",
        "email": "epfl.ch;epfl.ch;gmail.com;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne;Ecole Polytechnique",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.ec-polytechnique.fr",
        "aff_unique_abbr": "EPFL;X",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;France"
    },
    {
        "title": "Dynamic-Depth Context Tree Weighting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9117",
        "id": "9117",
        "author_site": "Joao V Messias, Shimon Whiteson",
        "author": "Joao V Messias; Shimon Whiteson",
        "abstract": "Reinforcement learning (RL) in partially observable settings is challenging because the agent\u2019s observations are not Markov. Recently proposed methods can learn variable-order Markov models of the underlying process but have steep memory requirements and are sensitive to aliasing between observation histories due to sensor noise. This paper proposes dynamic-depth context tree weighting (D2-CTW), a model-learning method that addresses these limitations. D2-CTW dynamically expands a suffix tree while ensuring that the size of the model, but not its depth, remains bounded. We show that D2-CTW approximately matches the performance of state-of-the-art alternatives at stochastic time-series prediction while using at least an order of magnitude less memory. We also apply D2-CTW to model-based RL, showing that, on tasks that require memory of past observations, D2-CTW can learn without prior knowledge of a good state representation, or even the length of history upon which such a representation should depend.",
        "bibtex": "@inproceedings{NIPS2017_c366c2c9,\n author = {Messias, Joao V and Whiteson, Shimon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic-Depth Context Tree Weighting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c366c2c97d47b02b24c3ecade4c40a01-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c366c2c97d47b02b24c3ecade4c40a01-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c366c2c97d47b02b24c3ecade4c40a01-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c366c2c97d47b02b24c3ecade4c40a01-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c366c2c97d47b02b24c3ecade4c40a01-Reviews.html",
        "metareview": "",
        "pdf_size": 1102300,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1435831851185233663&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Morpheus Labs+University of Amsterdam; University of Oxford",
        "aff_domain": "morpheuslabs.co.uk;cs.ox.ac.uk",
        "email": "morpheuslabs.co.uk;cs.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c366c2c97d47b02b24c3ecade4c40a01-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Morpheus Labs;University of Amsterdam;University of Oxford",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.uva.nl;https://www.ox.ac.uk",
        "aff_unique_abbr": ";UvA;Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";Netherlands;United Kingdom"
    },
    {
        "title": "EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9312",
        "id": "9312",
        "author_site": "Yogatheesan Varatharajah, Min Jin Chong, Krishnakant Saboo, Brent M Berry, Benjamin Brinkmann, Gregory Worrell, Ravishankar Iyer",
        "author": "Yogatheesan Varatharajah; Min Jin Chong; Krishnakant Saboo; Brent Berry; Benjamin Brinkmann; Gregory Worrell; Ravishankar Iyer",
        "abstract": "This paper presents a probabilistic-graphical model that can be used to infer characteristics of instantaneous brain activity by jointly analyzing spatial and temporal dependencies observed in electroencephalograms (EEG). Specifically, we describe a factor-graph-based model with customized factor-functions defined based on domain knowledge, to infer pathologic brain activity with the goal of identifying seizure-generating brain regions in epilepsy patients. We utilize an inference technique based on the graph-cut algorithm to exactly solve graph inference in polynomial time. We validate the model by using clinically collected intracranial EEG data from 29 epilepsy patients to show that the model correctly identifies seizure-generating brain regions. Our results indicate that our model outperforms two conventional approaches used for seizure-onset localization (5-7% better AUC: 0.72, 0.67, 0.65) and that the proposed inference technique provides 3-10% gain in AUC (0.72, 0.62, 0.69) compared to sampling-based alternatives.",
        "bibtex": "@inproceedings{NIPS2017_fb3f7685,\n author = {Varatharajah, Yogatheesan and Chong, Min Jin and Saboo, Krishnakant and Berry, Brent and Brinkmann, Benjamin and Worrell, Gregory and Iyer, Ravishankar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fb3f76858cb38e5b7fd113e0bc1c0721-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fb3f76858cb38e5b7fd113e0bc1c0721-Reviews.html",
        "metareview": "",
        "pdf_size": 956302,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5686984252440597558&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Neurology, Mayo Clinic; Department of Neurology, Mayo Clinic; Department of Neurology, Mayo Clinic; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;mayo.edu;mayo.edu;mayo.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;mayo.edu;mayo.edu;mayo.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fb3f76858cb38e5b7fd113e0bc1c0721-Abstract.html",
        "aff_unique_index": "0;0;0;1;1;1;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Department of Neurology, Mayo Clinic",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://illinois.edu;",
        "aff_unique_abbr": "UIUC;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9052",
        "id": "9052",
        "author_site": "Yuandong Tian, Qucheng Gong, Wendy Shang, Yuxin Wu, Larry Zitnick",
        "author": "Yuandong Tian; Qucheng Gong; Wenling Shang; Yuxin Wu; C. Lawrence Zitnick",
        "abstract": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 165K frame-per-second (FPS) on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open-sourced at https://github.com/facebookresearch/ELF.",
        "bibtex": "@inproceedings{NIPS2017_3fb451ca,\n author = {Tian, Yuandong and Gong, Qucheng and Shang, Wenling and Wu, Yuxin and Zitnick, C. Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Reviews.html",
        "metareview": "",
        "pdf_size": 2457251,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1819620389360858116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Facebook AI Research; Facebook AI Research; Oculus; Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com;oculus.com;fb.com;fb.com",
        "email": "fb.com;fb.com;oculus.com;fb.com;fb.com",
        "github": "https://github.com/facebookresearch/ELF",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3fb451ca2e89b3a13095b059d8705b15-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Facebook;Oculus VR",
        "aff_unique_dep": "Facebook AI Research;",
        "aff_unique_url": "https://research.facebook.com;https://www.oculus.com",
        "aff_unique_abbr": "FAIR;Oculus",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9044",
        "id": "9044",
        "author_site": "Justin Fu, John Co-Reyes, Sergey Levine",
        "author": "Justin Fu; John Co-Reyes; Sergey Levine",
        "abstract": "Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.",
        "bibtex": "@inproceedings{NIPS2017_1baff70e,\n author = {Fu, Justin and Co-Reyes, John and Levine, Sergey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {EX2: Exploration with Exemplar Models for Deep Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1baff70e2669e8376347efd3a874a341-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1baff70e2669e8376347efd3a874a341-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1baff70e2669e8376347efd3a874a341-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1baff70e2669e8376347efd3a874a341-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1baff70e2669e8376347efd3a874a341-Reviews.html",
        "metareview": "",
        "pdf_size": 850964,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1374245061310968783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of California Berkeley; University of California Berkeley; University of California Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1baff70e2669e8376347efd3a874a341-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Early stopping for kernel boosting algorithms: A general analysis with localized complexities",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9378",
        "id": "9378",
        "author_site": "Yuting Wei, Fanny Yang, Martin Wainwright",
        "author": "Yuting Wei; Fanny Yang; Martin J. Wainwright",
        "abstract": "Early stopping of iterative algorithms is a widely-used form of regularization in statistical learning, commonly used in conjunction with boosting and related gradient-type algorithms. Although consistency results have been established in some settings, such estimators are less well-understood than their analogues based on penalized regularization.  In this paper, for a relatively broad   class of loss functions and boosting algorithms (including   $L^2$-boost, LogitBoost and AdaBoost, among others), we connect the performance of a stopped iterate to the localized  Rademacher/Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis, now standard in the analysis of penalized estimators, can be used to derive optimal stopping rules.  We derive such stopping rules in detail for various kernel classes, and illustrate the correspondence of our theory with practice for Sobolev kernel classes.",
        "bibtex": "@inproceedings{NIPS2017_a081cab4,\n author = {Wei, Yuting and Yang, Fanny and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Early stopping for kernel boosting algorithms: A general analysis with localized complexities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a081cab429ff7a3b96e0a07319f1049e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a081cab429ff7a3b96e0a07319f1049e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a081cab429ff7a3b96e0a07319f1049e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a081cab429ff7a3b96e0a07319f1049e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a081cab429ff7a3b96e0a07319f1049e-Reviews.html",
        "metareview": "",
        "pdf_size": 485696,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16435634078503584382&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics1 + Department of Electrical Engineering and Computer Sciences2; Department of Statistics1 + Department of Electrical Engineering and Computer Sciences2; Department of Statistics1 + Department of Electrical Engineering and Computer Sciences2",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a081cab429ff7a3b96e0a07319f1049e-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Department of Statistics1;Department of Electrical Engineering and Computer Sciences2",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;",
        "aff_country_unique": ""
    },
    {
        "title": "Effective Parallelisation for Machine Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9417",
        "id": "9417",
        "author_site": "Michael Kamp, Mario Boley, Olana Missura, Thomas G\u00e4rtner",
        "author": "Michael Kamp; Mario Boley; Olana Missura; Thomas G\u00e4rtner",
        "abstract": "We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios.",
        "bibtex": "@inproceedings{NIPS2017_38811c52,\n author = {Kamp, Michael and Boley, Mario and Missura, Olana and G\\\"{a}rtner, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Effective Parallelisation for Machine Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/38811c5285e34e2e3319ab7d9f2cfa5b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/38811c5285e34e2e3319ab7d9f2cfa5b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/38811c5285e34e2e3319ab7d9f2cfa5b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/38811c5285e34e2e3319ab7d9f2cfa5b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/38811c5285e34e2e3319ab7d9f2cfa5b-Reviews.html",
        "metareview": "",
        "pdf_size": 393633,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11531272345461396411&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "University of Bonn + Fraunhofer IAIS; Max Planck Institute for Informatics + Saarland University; Google Inc.; University of Nottingham",
        "aff_domain": "cs.uni-bonn.de;mpi-inf.mpg.de;google.com;nottingham.ac.uk",
        "email": "cs.uni-bonn.de;mpi-inf.mpg.de;google.com;nottingham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/38811c5285e34e2e3319ab7d9f2cfa5b-Abstract.html",
        "aff_unique_index": "0+1;2+3;4;5",
        "aff_unique_norm": "University of Bonn;Fraunhofer Institute for Applied Information Technology;Max Planck Institute for Informatics;Saarland University;Google;University of Nottingham",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.uni-bonn.de/;https://www.iais.fraunhofer.de/;https://mpi-inf.mpg.de;https://www.uni-saarland.de;https://www.google.com;https://www.nottingham.ac.uk",
        "aff_unique_abbr": "UBonn;Fraunhofer IAIS;MPII;UdS;Google;UoN",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0+0;1;2",
        "aff_country_unique": "Germany;United States;United Kingdom"
    },
    {
        "title": "Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9460",
        "id": "9460",
        "author_site": "Muhammad Farhan, Juvaria Tariq, Arif Zaman, Mudassir Shabbir, Imdad Ullah Khan",
        "author": "Muhammad Farhan; Juvaria Tariq; Arif Zaman; Mudassir Shabbir; Imdad Ullah Khan",
        "abstract": "Sequence classification algorithms, such as SVM, require a definition of distance (similarity) measure between two sequences. A commonly used notion of similarity is the number of matches between k-mers (k-length subsequences) in the two sequences. Extending this definition, by considering two k-mers to match if their distance is at most m, yields better classification performance. This, however, makes the problem computationally much more complex. Known algorithms to compute this similarity have computational complexity that render them applicable only for small values of k and m. In this work, we develop novel techniques to efficiently and accurately estimate the pairwise similarity score, which enables us to use much larger values of k and m, and get higher predictive accuracy. This opens up a broad avenue of applying this classification approach to audio, images, and text sequences. Our algorithm achieves excellent approximation performance with theoretical guarantees. In the process we solve an open combinatorial problem, which was posed as a major hindrance to the scalability of existing solutions. We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.",
        "bibtex": "@inproceedings{NIPS2017_ddcbe259,\n author = {Farhan, Muhammad and Tariq, Juvaria and Zaman, Arif and Shabbir, Mudassir and Khan, Imdad Ullah},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ddcbe25988981920c872c1787382f04d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ddcbe25988981920c872c1787382f04d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ddcbe25988981920c872c1787382f04d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ddcbe25988981920c872c1787382f04d-Reviews.html",
        "metareview": "",
        "pdf_size": 643603,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9773097594122488498&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, School of Science and Engineering, Lahore University of Management Sciences, Lahore, Pakistan; Department of Mathematics, School of Science and Engineering, Lahore University of Management Sciences, Lahore, Pakistan; Department of Computer Science, School of Science and Engineering, Lahore University of Management Sciences, Lahore, Pakistan; Department of Computer Science, Information Technology University, Lahore, Pakistan; Department of Computer Science, School of Science and Engineering, Lahore University of Management Sciences, Lahore, Pakistan",
        "aff_domain": "lums.edu.pk;emory.edu;lums.edu.pk;itu.edu.pk;lums.edu.pk",
        "email": "lums.edu.pk;emory.edu;lums.edu.pk;itu.edu.pk;lums.edu.pk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ddcbe25988981920c872c1787382f04d-Abstract.html",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Department of Computer Science, School of Science and Engineering, Lahore University of Management Sciences, Lahore, Pakistan;Department of Mathematics, School of Science and Engineering, Lahore University of Management Sciences, Lahore, Pakistan;Department of Computer Science, Information Technology University, Lahore, Pakistan",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9289",
        "id": "9289",
        "author_site": "Zhenwen Dai, Mauricio \u00c1lvarez, Neil Lawrence",
        "author": "Zhenwen Dai; Mauricio \u00c1lvarez; Neil Lawrence",
        "abstract": "Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID.  How could we build a model that captures the latent information related to  these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.",
        "bibtex": "@inproceedings{NIPS2017_1680e9fa,\n author = {Dai, Zhenwen and \\'{A}lvarez, Mauricio and Lawrence, Neil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Reviews.html",
        "metareview": "",
        "pdf_size": 1135124,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9063652582200914697&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Inferentia Limited\u2021; Dept. of Computer Science, University of Shef\ufb01eld, Shef\ufb01eld, UK\u2020; Dept. of Computer Science, University of Shef\ufb01eld, Shef\ufb01eld, UK\u2020+Amazon.com\u2021",
        "aff_domain": "amazon.com;sheffield.ac.uk;amazon.com",
        "email": "amazon.com;sheffield.ac.uk;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1680e9fa7b4dd5d62ece800239bb53bd-Abstract.html",
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "Inferentia Limited\u2021;Dept. of Computer Science, University of Shef\ufb01eld, Shef\ufb01eld, UK\u2020;Amazon.com\u2021",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Efficient Online Linear Optimization with Approximation Algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8858",
        "id": "8858",
        "author": "Dan Garber",
        "abstract": "We revisit the problem of Online Linear Optimization in case the set of feasible actions is accessible through an approximated linear optimization oracle with a factor $\\alpha$ multiplicative approximation guarantee. This setting is in particular interesting since it captures natural online extensions of well-studied offline linear optimization problems which are NP-hard, yet admit efficient approximation algorithms. The goal here is to minimize the $\\alpha$-regret which is the natural extension of the standard regret in online learning to this setting.   We present new  algorithms with significantly improved oracle complexity for both the full information and bandit variants of the problem. Mainly, for both variants, we present $\\alpha$-regret bounds of $O(T^{-1/3})$, were $T$ is the number of prediction rounds, using only $O(\\log(T))$ calls to the approximation oracle per iteration, on average. These are the first results to obtain both average oracle complexity of $O(\\log(T))$ (or even poly-logarithmic in $T$) and $\\alpha$-regret bound $O(T^{-c})$ for a positive constant $c$, for both variants.",
        "bibtex": "@inproceedings{NIPS2017_a49e9411,\n author = {Garber, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Online Linear Optimization with Approximation Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a49e9411d64ff53eccfdd09ad10a15b3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a49e9411d64ff53eccfdd09ad10a15b3-Reviews.html",
        "metareview": "",
        "pdf_size": 438198,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7580520439374529200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Technion - Israel Institute of Technology",
        "aff_domain": "technion.ac.il",
        "email": "technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9128",
        "id": "9128",
        "author_site": "Wenbing Huang, Mehrtash Harandi, Tong Zhang, Lijie Fan, Fuchun Sun, Junzhou Huang",
        "author": "Wenbing Huang; Mehrtash Harandi; Tong Zhang; Lijie Fan; Fuchun Sun; Junzhou Huang",
        "abstract": "Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-temporal data in various disciplines. Though rich in modeling, analyzing LDSs is not free of difficulty, mainly because LDSs do not comply with Euclidean geometry and hence conventional learning techniques can not be applied directly. In this paper, we propose an efficient projected gradient descent method to minimize a general form of a loss function and demonstrate how clustering and sparse coding with LDSs can be solved by the proposed method efficiently. To this end, we first derive a novel canonical form for representing the parameters of an LDS, and then show how gradient-descent updates through the projection on the space of LDSs can be achieved dexterously. In contrast to previous studies, our solution avoids any approximation in LDS modeling or during the optimization process. Extensive experiments reveal the superior performance of the proposed method in terms of the convergence and classification accuracy over state-of-the-art techniques.",
        "bibtex": "@inproceedings{NIPS2017_e3408432,\n author = {Huang, Wenbing and Harandi, Mehrtash and Zhang, Tong and Fan, Lijie and Sun, Fuchun and Huang, Junzhou},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e3408432c1a48a52fb6c74d926b38886-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e3408432c1a48a52fb6c74d926b38886-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e3408432c1a48a52fb6c74d926b38886-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e3408432c1a48a52fb6c74d926b38886-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e3408432c1a48a52fb6c74d926b38886-Reviews.html",
        "metareview": "",
        "pdf_size": 390537,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12288144567327371302&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e3408432c1a48a52fb6c74d926b38886-Abstract.html"
    },
    {
        "title": "Efficient Second-Order Online Kernel Learning with Adaptive Embedding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9385",
        "id": "9385",
        "author_site": "Daniele Calandriello, Alessandro Lazaric, Michal Valko",
        "author": "Daniele Calandriello; Alessandro Lazaric; Michal Valko",
        "abstract": "Online kernel learning (OKL) is a flexible framework to approach prediction problems, since the large approximation space provided by reproducing kernel Hilbert spaces can contain an accurate function for the problem. Nonetheless, optimizing over this space is computationally expensive. Not only first order methods accumulate $\\O(\\sqrt{T})$ more loss than the optimal function, but the curse of kernelization results in a $\\O(t)$ per step complexity. Second-order methods get closer to the optimum much faster, suffering only $\\O(\\log(T))$ regret, but second-order updates are even more expensive, with a $\\O(t^2)$ per-step cost. Existing approximate OKL methods try to reduce this complexity either by limiting the Support Vectors (SV) introduced in the predictor, or by avoiding the kernelization process altogether using embedding. Nonetheless, as long as the size of the approximation space or the number of SV does not grow over time, an adversary can always exploit the approximation process. In this paper, we propose PROS-N-KONS, a method that combines Nystrom sketching to project the input point in a small, accurate embedded space, and performs efficient second-order updates in this space. The embedded space is continuously updated to guarantee that the embedding remains accurate, and we show that the per-step cost only grows with the effective dimension of the problem and not with $T$. Moreover, the second-order updated allows us to achieve the logarithmic regret. We empirically compare our algorithm on recent large-scales benchmarks and show it performs favorably.",
        "bibtex": "@inproceedings{NIPS2017_366f0bc7,\n author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Second-Order Online Kernel Learning with Adaptive Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/366f0bc7bd1d4bf414073cabbadfdfcd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/366f0bc7bd1d4bf414073cabbadfdfcd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/366f0bc7bd1d4bf414073cabbadfdfcd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/366f0bc7bd1d4bf414073cabbadfdfcd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/366f0bc7bd1d4bf414073cabbadfdfcd-Reviews.html",
        "metareview": "",
        "pdf_size": 416819,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4535736878120363144&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "SequeL team, INRIA Lille - Nord Europe, France; SequeL team, INRIA Lille - Nord Europe, France; SequeL team, INRIA Lille - Nord Europe, France",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/366f0bc7bd1d4bf414073cabbadfdfcd-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA Lille - Nord Europe",
        "aff_unique_dep": "SequeL team",
        "aff_unique_url": "https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9190",
        "id": "9190",
        "author_site": "Shinji Ito, Daisuke Hatano, Hanna Sumita, Akihiro Yabe, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi",
        "author": "Shinji Ito; Daisuke Hatano; Hanna Sumita; Akihiro Yabe; Takuro Fukunaga; Naonori Kakimura; Ken-Ichi Kawarabayashi",
        "abstract": "Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP$\\subseteq$BPP, and only an exponential-time sublinear-regret algorithm has been found. In this paper, we introduce mild assumptions to solve the problem. Under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.",
        "bibtex": "@inproceedings{NIPS2017_6e5025cc,\n author = {Ito, Shinji and Hatano, Daisuke and Sumita, Hanna and Yabe, Akihiro and Fukunaga, Takuro and Kakimura, Naonori and Kawarabayashi, Ken-Ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6e5025ccc7d638ae4e724da8938450a6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6e5025ccc7d638ae4e724da8938450a6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6e5025ccc7d638ae4e724da8938450a6-Reviews.html",
        "metareview": "",
        "pdf_size": 508327,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2917426497126324849&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "NEC Corporation; National Institute of Informatics; National Institute of Informatics; NEC Corporation; JST, PRESTO; Keio University; National Institute of Informatics",
        "aff_domain": "me.jp.nec.com;nii.ac.jp;nii.ac.jp;cq.jp.nec.com;nii.ac.jp;math.keio.ac.jp;nii.ac.jp",
        "email": "me.jp.nec.com;nii.ac.jp;nii.ac.jp;cq.jp.nec.com;nii.ac.jp;math.keio.ac.jp;nii.ac.jp",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6e5025ccc7d638ae4e724da8938450a6-Abstract.html",
        "aff_unique_index": "0;1;1;0;2;3;1",
        "aff_unique_norm": "NEC Corporation;National Institute of Informatics;Japan Science and Technology Agency;Keio University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nec.com;https://www.nii.ac.jp/;https://www.jst.go.jp;https://www.keio.ac.jp",
        "aff_unique_abbr": "NEC;NII;JST;Keio",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9205",
        "id": "9205",
        "author_site": "Celestine D\u00fcnner, Thomas Parnell, Martin Jaggi",
        "author": "Celestine D\u00fcnner; Thomas Parnell; Martin Jaggi",
        "abstract": "We propose a generic algorithmic building block to accelerate training of  machine learning models on heterogeneous compute systems. Our scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique is built upon novel theoretical insights regarding primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on a large-scale dataset exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.",
        "bibtex": "@inproceedings{NIPS2017_e0f7a4d0,\n author = {D\\\"{u}nner, Celestine and Parnell, Thomas and Jaggi, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Reviews.html",
        "metareview": "",
        "pdf_size": 3423646,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10745772020784136864&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Research - Zurich; IBM Research - Zurich; EPFL",
        "aff_domain": "zurich.ibm.com;zurich.ibm.com;epfl.ch",
        "email": "zurich.ibm.com;zurich.ibm.com;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "IBM Research;Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": "Research;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.epfl.ch",
        "aff_unique_abbr": "IBM;EPFL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Zurich;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Efficient and Flexible Inference for Stochastic Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9465",
        "id": "9465",
        "author_site": "Stefan Bauer, Nico S Gorbach, Djordje Miladinovic, Joachim M Buhmann",
        "author": "Stefan Bauer; Nico S Gorbach; Djordje Miladinovic; Joachim M Buhmann",
        "abstract": "Many real world dynamical systems are described by stochastic differential equations. Thus parameter inference is a challenging and important problem in many disciplines. We provide a grid free and flexible algorithm offering parameter and state inference for stochastic systems and compare our approch based on variational approximations to state of the art methods showing significant advantages both in runtime and accuracy.",
        "bibtex": "@inproceedings{NIPS2017_e0126439,\n author = {Bauer, Stefan and Gorbach, Nico S and Miladinovic, Djordje and Buhmann, Joachim M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient and Flexible Inference for Stochastic Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e0126439e08ddfbdf4faa952dc910590-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e0126439e08ddfbdf4faa952dc910590-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e0126439e08ddfbdf4faa952dc910590-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e0126439e08ddfbdf4faa952dc910590-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e0126439e08ddfbdf4faa952dc910590-Reviews.html",
        "metareview": "",
        "pdf_size": 1225852,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1323047274116895257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e0126439e08ddfbdf4faa952dc910590-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Eigen-Distortions of Hierarchical Representations",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9136",
        "id": "9136",
        "author_site": "Alexander Berardino, Valero Laparra, Johannes Ball\u00e9, Eero Simoncelli",
        "author": "Alexander Berardino; Valero Laparra; Johannes Ball\u00e9; Eero Simoncelli",
        "abstract": "We develop a method for comparing hierarchical image representations in terms of their ability to explain perceptual sensitivity in humans. Specifically, we utilize Fisher information to establish a model-derived prediction of sensitivity to local perturbations of an image. For a given image, we compute the eigenvectors of the Fisher information matrix with largest and smallest eigenvalues, corresponding to the model-predicted most- and least-noticeable image distortions, respectively. For human subjects, we then measure the amount of each distortion that can be reliably detected when added to the image. We use this method to test the ability of a variety of representations to mimic human perceptual sensitivity. We find that the early layers of VGG16, a deep neural network optimized for object recognition, provide a better match to human perception than later layers, and a better match than a 4-stage convolutional neural network (CNN) trained on a database of human ratings of distorted image quality. On the other hand, we find that simple models of early visual processing, incorporating one or more stages of local gain control, trained on the same database of distortion ratings, provide substantially better predictions of human sensitivity than either the CNN, or any combination of layers of VGG16.",
        "bibtex": "@inproceedings{NIPS2017_c5a4e7e6,\n author = {Berardino, Alexander and Laparra, Valero and Ball\\'{e}, Johannes and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Eigen-Distortions of Hierarchical Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c5a4e7e6882845ea7bb4d9462868219b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c5a4e7e6882845ea7bb4d9462868219b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c5a4e7e6882845ea7bb4d9462868219b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c5a4e7e6882845ea7bb4d9462868219b-Reviews.html",
        "metareview": "",
        "pdf_size": 2776969,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14395689940071339246&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Center for Neural Science, New York University; Center for Neural Science, New York University*; Image Processing Laboratory, Universitat de Val\u00e8ncia; Howard Hughes Medical Institute, Center for Neural Science and Courant Institute of Mathematical Sciences, New York University",
        "aff_domain": "nyu.edu;nyu.edu;uv.es;nyu.edu",
        "email": "nyu.edu;nyu.edu;uv.es;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c5a4e7e6882845ea7bb4d9462868219b-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Center for Neural Science, New York University;Center for Neural Science, New York University*;Image Processing Laboratory, Universitat de Val\u00e8ncia;Howard Hughes Medical Institute, Center for Neural Science and Courant Institute of Mathematical Sciences, New York University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9007",
        "id": "9007",
        "author_site": "Surbhi Goel, Adam Klivans",
        "author": "Surbhi Goel; Adam Klivans",
        "abstract": "We consider the problem of learning function classes computed by   neural networks with various activations (e.g. ReLU or Sigmoid), a   task believed to be computationally intractable in the worst-case.   A major open problem is to understand the minimal assumptions under   which these classes admit provably efficient algorithms. In this work we show   that a natural distributional assumption corresponding to {\\em     eigenvalue decay} of the Gram matrix yields polynomial-time   algorithms in the non-realizable setting for expressive classes of   networks (e.g. feed-forward networks of ReLUs).  We make no    assumptions on the structure of the network or the labels.  Given   sufficiently-strong eigenvalue decay, we obtain {\\em     fully}-polynomial time algorithms in {\\em all} the relevant   parameters with respect to square-loss.  This is the first purely   distributional assumption that leads to polynomial-time algorithms   for networks of ReLUs.  Further, unlike   prior distributional assumptions (e.g., the marginal distribution is   Gaussian), eigenvalue decay has been observed in practice on common   data sets.",
        "bibtex": "@inproceedings{NIPS2017_50905d7b,\n author = {Goel, Surbhi and Klivans, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/50905d7b2216bfeccb5b41016357176b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/50905d7b2216bfeccb5b41016357176b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/50905d7b2216bfeccb5b41016357176b-Reviews.html",
        "metareview": "",
        "pdf_size": 353549,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=136915817041080923&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/50905d7b2216bfeccb5b41016357176b-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Elementary Symmetric Polynomials for Optimal Experimental Design",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9002",
        "id": "9002",
        "author_site": "Zelda Mariet, Suvrit Sra",
        "author": "Zelda E. Mariet; Suvrit Sra",
        "abstract": "We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture \"partial volumes\" and offer a graded interpolation between the widely used A-optimal and D-optimal design models, obtaining each of them as special cases. We analyze properties of our models, and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy algorithm. Finally, as a byproduct, we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.",
        "bibtex": "@inproceedings{NIPS2017_1cecc7a7,\n author = {Mariet, Zelda E. and Sra, Suvrit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Elementary Symmetric Polynomials for Optimal Experimental Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1cecc7a77928ca8133fa24680a88d2f9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1cecc7a77928ca8133fa24680a88d2f9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1cecc7a77928ca8133fa24680a88d2f9-Reviews.html",
        "metareview": "",
        "pdf_size": 167236,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6232537905722100261&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "csail.mit.edu;mit.edu",
        "email": "csail.mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9003",
        "id": "9003",
        "author_site": "Serhii Havrylov, Ivan Titov",
        "author": "Serhii Havrylov; Ivan Titov",
        "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success  exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages.    As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model  and study properties of the resulting protocol.",
        "bibtex": "@inproceedings{NIPS2017_70222949,\n author = {Havrylov, Serhii and Titov, Ivan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/70222949cc0db89ab32c9969754d4758-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/70222949cc0db89ab32c9969754d4758-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/70222949cc0db89ab32c9969754d4758-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/70222949cc0db89ab32c9969754d4758-Reviews.html",
        "metareview": "",
        "pdf_size": 1087524,
        "gs_citation": 370,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17308624474306270808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "ILCC, School of Informatics, University of Edinburgh; ILCC, School of Informatics, University of Edinburgh + ILLC, University of Amsterdam",
        "aff_domain": "inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/70222949cc0db89ab32c9969754d4758-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of Edinburgh;University of Amsterdam",
        "aff_unique_dep": "School of Informatics;ILLC",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.uva.nl",
        "aff_unique_abbr": "Edinburgh;UvA",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Edinburgh;Amsterdam",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United Kingdom;Netherlands"
    },
    {
        "title": "End-to-End Differentiable Proving",
        "author": "Tim Rockt\u00e4schel, Sebastian Riedel",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9161",
        "id": "9161"
    },
    {
        "title": "End-to-end Differentiable Proving",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/oral/10058",
        "id": "10058",
        "author_site": "Tim Rockt\u00e4schel, Sebastian Riedel",
        "author": "Tim Rockt\u00e4schel; Sebastian Riedel",
        "abstract": "We introduce deep neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols.  These neural networks are recursively constructed by following the backward chaining algorithm as used in Prolog.  Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations.  The resulting neural network can be trained to infer facts from a given incomplete knowledge base using gradient descent.  By doing so, it learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) it can use provided and induced logical rules for complex multi-hop reasoning.  On four benchmark knowledge bases we demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, while at the same time inducing interpretable function-free first-order logic rules.",
        "bibtex": "@inproceedings{NIPS2017_b2ab0019,\n author = {Rockt\\\"{a}schel, Tim and Riedel, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {End-to-end Differentiable Proving},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Reviews.html",
        "metareview": "",
        "pdf_size": 595321,
        "gs_citation": 519,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13103968366733865463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "University of Oxford; University College London & Bloomsbury AI",
        "aff_domain": "cs.ox.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ox.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oxford;University College London & Bloomsbury AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ox.ac.uk;",
        "aff_unique_abbr": "Oxford;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "title": "Ensemble Sampling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9110",
        "id": "9110",
        "author_site": "Xiuyuan Lu, Benjamin Van Roy",
        "author": "Xiuyuan Lu; Benjamin Van Roy",
        "abstract": "Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.",
        "bibtex": "@inproceedings{NIPS2017_49ad23d1,\n author = {Lu, Xiuyuan and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ensemble Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Reviews.html",
        "metareview": "",
        "pdf_size": 490560,
        "gs_citation": 166,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=61709017893123848&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9215",
        "id": "9215",
        "author_site": "Emmanouil Platanios, Hoifung Poon, Tom M Mitchell, Eric Horvitz",
        "author": "Emmanouil Platanios; Hoifung Poon; Tom M. Mitchell; Eric J Horvitz",
        "abstract": "We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.",
        "bibtex": "@inproceedings{NIPS2017_95f8d990,\n author = {Platanios, Emmanouil and Poon, Hoifung and Mitchell, Tom M and Horvitz, Eric J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/95f8d9901ca8878e291552f001f67692-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/95f8d9901ca8878e291552f001f67692-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/95f8d9901ca8878e291552f001f67692-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/95f8d9901ca8878e291552f001f67692-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/95f8d9901ca8878e291552f001f67692-Reviews.html",
        "metareview": "",
        "pdf_size": 533341,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15410994600507266723&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Carnegie Mellon University; Microsoft Research; Carnegie Mellon University; Microsoft Research",
        "aff_domain": "cs.cmu.edu;microsoft.com;cs.cmu.edu;microsoft.com",
        "email": "cs.cmu.edu;microsoft.com;cs.cmu.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/95f8d9901ca8878e291552f001f67692-Abstract.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CMU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein\u2019s Lemma",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9381",
        "id": "9381",
        "author_site": "Zhuoran Yang, Krishnakumar Balasubramanian, Zhaoran Wang, Han Liu",
        "author": "Zhuoran Yang; Krishnakumar Balasubramanian; Zhaoran Wang; Han Liu",
        "abstract": "We consider estimating the parametric components of semiparametric multi-index models in high dimensions. To bypass the requirements of Gaussianity or elliptical symmetry of covariates in existing methods, we propose to leverage a second-order Stein\u2019s method with score function-based corrections. We prove that our estimator achieves a near-optimal statistical rate of convergence even when the score function or the response variable is heavy-tailed. To establish the key concentration results, we develop a data-driven truncation argument that may be of independent interest. We supplement our theoretical findings with simulations.",
        "bibtex": "@inproceedings{NIPS2017_4db0f8b0,\n author = {Yang, Zhuoran and Balasubramanian, Krishnakumar and Wang, Zhaoran and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein\u2019s Lemma},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4db0f8b0fc895da263fd77fc8aecabe4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4db0f8b0fc895da263fd77fc8aecabe4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4db0f8b0fc895da263fd77fc8aecabe4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4db0f8b0fc895da263fd77fc8aecabe4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4db0f8b0fc895da263fd77fc8aecabe4-Reviews.html",
        "metareview": "",
        "pdf_size": 478055,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11081853185999472815&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Princeton University; Princeton University; Tencent AI Lab + Northwestern University; Tencent AI Lab + Northwestern University",
        "aff_domain": "princeton.edu;princeton.edu;gmail.com;gmail.com",
        "email": "princeton.edu;princeton.edu;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4db0f8b0fc895da263fd77fc8aecabe4-Abstract.html",
        "aff_unique_index": "0;0;1+2;1+2",
        "aff_unique_norm": "Princeton University;Tencent;Northwestern University",
        "aff_unique_dep": ";Tencent AI Lab;",
        "aff_unique_url": "https://www.princeton.edu;https://ai.tencent.com;https://www.northwestern.edu",
        "aff_unique_abbr": "Princeton;Tencent AI Lab;NU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0;1+0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Estimating Mutual Information for Discrete-Continuous Mixtures",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9371",
        "id": "9371",
        "author_site": "Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",
        "author": "Weihao Gao; Sreeram Kannan; Sewoong Oh; Pramod Viswanath",
        "abstract": "Estimation of mutual information from observed samples is a basic primitive in machine learning, useful in several learning tasks including correlation mining, information bottleneck, Chow-Liu tree, and conditional independence testing in (causal) graphical models. While mutual information is a quantity well-defined for general probability spaces, estimators have been developed only in the special case of discrete or continuous pairs of random variables. Most of these estimators operate using the 3H -principle, i.e., by calculating the three (differential) entropies of X, Y and the pair (X,Y). However, in general mixture spaces, such individual entropies  are not well defined, even though mutual information is.  In this paper, we develop a novel estimator for estimating mutual information in discrete-continuous mixtures. We prove the consistency of this estimator theoretically as well as demonstrate its excellent empirical performance. This problem is relevant in a wide-array of applications, where some variables are discrete, some continuous, and others are a mixture between continuous and discrete components.",
        "bibtex": "@inproceedings{NIPS2017_ef72d539,\n author = {Gao, Weihao and Kannan, Sreeram and Oh, Sewoong and Viswanath, Pramod},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating Mutual Information for Discrete-Continuous Mixtures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Reviews.html",
        "metareview": "",
        "pdf_size": 360233,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13209933127623266330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of ECE, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Department of Electrical Engineering, University of Washington; Department of IESE, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Department of ECE, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;uw.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;uw.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ef72d53990bc4805684c9b61fa64a102-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Department of ECE, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign;University of Washington;Department of IESE, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";Department of Electrical Engineering;",
        "aff_unique_url": ";https://www.washington.edu;",
        "aff_unique_abbr": ";UW;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Estimation of the covariance structure of heavy-tailed distributions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9070",
        "id": "9070",
        "author_site": "Xiaohan Wei, Stanislav Minsker",
        "author": "Xiaohan Wei; Stanislav Minsker",
        "abstract": "We propose and analyze a new estimator of the covariance matrix that admits strong theoretical guarantees under weak assumptions on the underlying distribution, such as existence of moments of only low order. While estimation of covariance matrices corresponding to sub-Gaussian distributions is well-understood, much less in known in the case of heavy-tailed data.  As K. Balasubramanian and M. Yuan write,",
        "bibtex": "@inproceedings{NIPS2017_10c272d0,\n author = {Wei, Xiaohan and Minsker, Stanislav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimation of the covariance structure of heavy-tailed distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/10c272d06794d3e5785d5e7c5356e9ff-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/10c272d06794d3e5785d5e7c5356e9ff-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/10c272d06794d3e5785d5e7c5356e9ff-Reviews.html",
        "metareview": "",
        "pdf_size": 324344,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2956359065972991386&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematics, University of Southern California; Department of Electrical Engineering, University of Southern California",
        "aff_domain": "usc.edu;usc.edu",
        "email": "usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/10c272d06794d3e5785d5e7c5356e9ff-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Southern California;Department of Electrical Engineering, University of Southern California",
        "aff_unique_dep": "Department of Mathematics;",
        "aff_unique_url": "https://www.usc.edu;",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9291",
        "id": "9291",
        "author_site": "Rishit Sheth, Roni Khardon",
        "author": "Rishit Sheth; Roni Khardon",
        "abstract": "Bayesian models are established as one of the main successful paradigms for complex problems in machine learning. To handle intractable inference, research in this area has developed new approximation methods that are fast and effective. However, theoretical analysis of the performance of such approximations is not well developed. The paper furthers such analysis by providing bounds on the excess risk of variational inference algorithms and related regularized loss minimization algorithms for a large class of latent variable models with Gaussian latent variables. We strengthen previous results for variational algorithms by showing they are competitive with any point-estimate predictor. Unlike previous work, we also provide bounds on the risk of the \\emph{Bayesian} predictor and not just the risk of the Gibbs predictor for the same approximate posterior. The bounds are applied in complex models including sparse Gaussian processes and correlated topic models. Theoretical results are complemented by identifying novel approximations to the Bayesian objective that attempt to minimize the risk directly. An empirical evaluation compares the variational and new algorithms shedding further light on their performance.",
        "bibtex": "@inproceedings{NIPS2017_7edccc66,\n author = {Sheth, Rishit and Khardon, Roni},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7edccc661418aeb5761dbcdc06ad490c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7edccc661418aeb5761dbcdc06ad490c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7edccc661418aeb5761dbcdc06ad490c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7edccc661418aeb5761dbcdc06ad490c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7edccc661418aeb5761dbcdc06ad490c-Reviews.html",
        "metareview": "",
        "pdf_size": 420221,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3605208678929074163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Tufts University; Department of Computer Science, Tufts University",
        "aff_domain": "tufts.edu;cs.tufts.edu",
        "email": "tufts.edu;cs.tufts.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7edccc661418aeb5761dbcdc06ad490c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tufts University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tufts.edu",
        "aff_unique_abbr": "Tufts",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Expectation Propagation for t-Exponential Family Using q-Algebra",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9012",
        "id": "9012",
        "author_site": "Futoshi Futami, Issei Sato, Masashi Sugiyama",
        "author": "Futoshi Futami; Issei Sato; Masashi Sugiyama",
        "abstract": "Exponential family distributions are highly useful in machine learning since their calculation can be performed efficiently through natural parameters. The exponential family has recently been extended to the t-exponential family, which contains Student-t distributions as family members and thus allows us to handle noisy data well. However, since the t-exponential family is defined by the deformed exponential, an efficient learning algorithm for the t-exponential family such as expectation propagation (EP) cannot be derived in the same way as the ordinary exponential family. In this paper, we borrow the mathematical tools of q-algebra from statistical physics and show that the pseudo additivity of distributions allows us to perform calculation of t-exponential family distributions through natural parameters. We then develop an expectation propagation (EP) algorithm for the t-exponential family, which provides a deterministic approximation to the posterior or predictive distribution with simple moment matching. We finally apply the proposed EP algorithm to the Bayes point machine and Student-t process classification, and demonstrate their performance numerically.",
        "bibtex": "@inproceedings{NIPS2017_17fafe5f,\n author = {Futami, Futoshi and Sato, Issei and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expectation Propagation for t-Exponential Family Using q-Algebra},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Reviews.html",
        "metareview": "",
        "pdf_size": 2319241,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3632272668678840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Tokyo + RIKEN; The University of Tokyo + RIKEN; RIKEN + The University of Tokyo",
        "aff_domain": "ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html",
        "aff_unique_index": "0+1;0+1;1+0",
        "aff_unique_norm": "University of Tokyo;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "UTokyo;RIKEN",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8991",
        "id": "8991",
        "author_site": "Le Fang, Fan Yang, Wen Dong, Tong Guan, Chunming Qiao",
        "author": "Le Fang; Fan Yang; Wen Dong; Tong Guan; Chunming Qiao",
        "abstract": "Technological breakthroughs allow us to collect data with increasing spatio-temporal resolution from complex interaction systems. The combination of high-resolution observations, expressive dynamic models, and efficient machine learning algorithms can lead to crucial insights into complex interaction dynamics and the functions of these systems. In this paper, we formulate the dynamics of a complex interacting network as a stochastic process driven by a sequence of events, and develop expectation propagation algorithms to make inferences from noisy observations. To avoid getting stuck at a local optimum, we formulate the problem of minimizing Bethe free energy as a constrained primal problem and take advantage of the concavity of dual problem in the feasible domain of dual variables guaranteed by duality theorem. Our expectation propagation algorithms demonstrate better performance in inferring the interaction dynamics in complex transportation networks than competing models such as particle filter, extended Kalman filter, and deep neural networks.",
        "bibtex": "@inproceedings{NIPS2017_d3890178,\n author = {Fang, Le and Yang, Fan and Dong, Wen and Guan, Tong and Qiao, Chunming},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d38901788c533e8286cb6400b40b386d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d38901788c533e8286cb6400b40b386d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d38901788c533e8286cb6400b40b386d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d38901788c533e8286cb6400b40b386d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d38901788c533e8286cb6400b40b386d-Reviews.html",
        "metareview": "",
        "pdf_size": 492752,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17440093836983064497&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, University at Buffalo; Department of Computer Science and Engineering, University at Buffalo; Department of Computer Science and Engineering, University at Buffalo; Department of Computer Science and Engineering, University at Buffalo; Department of Computer Science and Engineering, University at Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu;buffalo.edu;buffalo.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;buffalo.edu;buffalo.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d38901788c533e8286cb6400b40b386d-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University at Buffalo",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.buffalo.edu",
        "aff_unique_abbr": "UB",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Buffalo",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Experimental Design for Learning Causal Graphs with Latent Variables",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9468",
        "id": "9468",
        "author_site": "Murat Kocaoglu, Karthikeyan Shanmugam, Elias Bareinboim",
        "author": "Murat Kocaoglu; Karthikeyan Shanmugam; Elias Bareinboim",
        "abstract": "We consider the problem of learning causal structures with latent variables using interventions. Our objective is not only to learn the causal graph between the observed variables, but to locate unobserved variables that could confound the relationship between observables. Our approach is stage-wise: We first learn the observable graph, i.e., the induced graph between observable variables. Next we learn the existence and location of the latent variables given the observable graph. We propose an efficient randomized algorithm that can learn the observable graph using O(d\\log^2 n) interventions where d is the degree of the graph. We further propose an efficient deterministic variant which uses O(log n + l) interventions, where l is the longest directed path in the graph. Next, we propose an algorithm that uses only O(d^2 log n) interventions that can learn the latents between both non-adjacent and adjacent variables. While a naive baseline approach would require O(n^2) interventions, our combined algorithm can learn the causal graph with latents using O(d log^2 n + d^2 log (n)) interventions.",
        "bibtex": "@inproceedings{NIPS2017_291d43c6,\n author = {Kocaoglu, Murat and Shanmugam, Karthikeyan and Bareinboim, Elias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Experimental Design for Learning Causal Graphs with Latent Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/291d43c696d8c3704cdbe0a72ade5f6c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/291d43c696d8c3704cdbe0a72ade5f6c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/291d43c696d8c3704cdbe0a72ade5f6c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/291d43c696d8c3704cdbe0a72ade5f6c-Reviews.html",
        "metareview": "",
        "pdf_size": 463494,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9876098745951443974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin, USA; IBM Research NY, USA; Department of Computer Science and Statistics, Purdue University, USA",
        "aff_domain": "utexas.edu;ibm.com;purdue.edu",
        "email": "utexas.edu;ibm.com;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/291d43c696d8c3704cdbe0a72ade5f6c-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "The University of Texas at Austin;IBM Research NY, USA;Department of Computer Science and Statistics, Purdue University, USA",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;;",
        "aff_unique_url": "https://www.utexas.edu;;",
        "aff_unique_abbr": "UT Austin;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Exploring Generalization in Deep Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9367",
        "id": "9367",
        "author_site": "Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nati Srebro",
        "author": "Behnam Neyshabur; Srinadh Bhojanapalli; David Mcallester; Nati Srebro",
        "abstract": "With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory.  We then investigate how well the measures explain different observed phenomena.",
        "bibtex": "@inproceedings{NIPS2017_10ce03a1,\n author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploring Generalization in Deep Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Reviews.html",
        "metareview": "",
        "pdf_size": 659240,
        "gs_citation": 1601,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16285731102067380229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/10ce03a1ed01077e3e289f3e53c72813-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI Chicago",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9344",
        "id": "9344",
        "author_site": "Marcel Nonnenmacher, Srinivas C Turaga, Jakob H Macke",
        "author": "Marcel Nonnenmacher; Srinivas C. Turaga; Jakob H Macke",
        "abstract": "A powerful approach for understanding neural population dynamics is to extract low-dimensional trajectories from population recordings using dimensionality reduction methods. Current approaches for dimensionality reduction on neural data are limited to single population recordings, and can not identify dynamics embedded across multiple measurements. We propose an approach for extracting low-dimensional dynamics from multiple, sequential recordings. Our algorithm scales to data comprising millions of observed dimensions, making it possible to access dynamics distributed across large populations or multiple brain areas. Building on subspace-identification approaches for dynamical systems, we perform parameter estimation by minimizing a moment-matching objective using a scalable stochastic gradient descent algorithm: The model is optimized to predict temporal covariations across neurons and across time. We show how this approach naturally handles missing data and multiple partial recordings, and can identify dynamics and predict correlations even in the presence of severe subsampling and small overlap between recordings. We demonstrate the effectiveness of the approach both on simulated data and a whole-brain larval zebrafish imaging dataset.",
        "bibtex": "@inproceedings{NIPS2017_c7558e9d,\n author = {Nonnenmacher, Marcel and Turaga, Srinivas C and Macke, Jakob H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Reviews.html",
        "metareview": "",
        "pdf_size": 1819453,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10290272460220051312&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "research center caesar, an associate of the Max Planck Society, Bonn, Germany; HHMI Janelia Research Campus, Ashburn, V A; research center caesar, an associate of the Max Planck Society, Bonn, Germany + Centre for Cognitive Science, Technical University Darmstadt",
        "aff_domain": "caesar.de;janelia.hhmi.org;caesar.de",
        "email": "caesar.de;janelia.hhmi.org;caesar.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c7558e9d1f956b016d1fdba7ea132378-Abstract.html",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "research center caesar, an associate of the Max Planck Society, Bonn, Germany;HHMI Janelia Research Campus;Centre for Cognitive Science, Technical University Darmstadt",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.janelia.org;",
        "aff_unique_abbr": ";Janelia;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Ashburn",
        "aff_country_unique_index": "1;",
        "aff_country_unique": ";United States"
    },
    {
        "title": "ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9124",
        "id": "9124",
        "author_site": "Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Mr. Prabhat, Chris Pal",
        "author": "Evan Racah; Christopher Beckham; Tegan Maharaj; Samira Ebrahimi Kahou; Mr. Prabhat; Chris Pal",
        "abstract": "Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.",
        "bibtex": "@inproceedings{NIPS2017_519c8415,\n author = {Racah, Evan and Beckham, Christopher and Maharaj, Tegan and Ebrahimi Kahou, Samira and Prabhat, Mr. and Pal, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/519c84155964659375821f7ca576f095-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/519c84155964659375821f7ca576f095-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/519c84155964659375821f7ca576f095-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/519c84155964659375821f7ca576f095-Reviews.html",
        "metareview": "",
        "pdf_size": 5757481,
        "gs_citation": 284,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9025298096891885137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "MILA, Universit\u00e9 de Montr\u00e9al + Lawrence Berkeley National Lab, Berkeley, CA; MILA, Universit\u00e9 de Montr\u00e9al + \u00c9cole Polytechnique de Montr\u00e9al; MILA, Universit\u00e9 de Montr\u00e9al + \u00c9cole Polytechnique de Montr\u00e9al; Microsoft Maluuba; Lawrence Berkeley National Lab, Berkeley, CA; MILA, Universit\u00e9 de Montr\u00e9al + \u00c9cole Polytechnique de Montr\u00e9al",
        "aff_domain": "umontreal.ca;polymtl.ca;polymtl.ca;microsoft.com;lbl.gov;polymtl.ca",
        "email": "umontreal.ca;polymtl.ca;polymtl.ca;microsoft.com;lbl.gov;polymtl.ca",
        "github": "https://github.com/eracah/hur-detect",
        "project": "extremeweatherdataset.github.io",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/519c84155964659375821f7ca576f095-Abstract.html",
        "aff_unique_index": "0+1;0+2;0+2;3;1;0+2",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Lawrence Berkeley National Lab, Berkeley, CA;\u00c9cole Polytechnique de Montr\u00e9al;Microsoft Maluuba",
        "aff_unique_dep": "MILA;;;",
        "aff_unique_url": "https://www.umontreal.ca;;https://www.polymtl.ca;",
        "aff_unique_abbr": "UdeM;;Polytechnique Montr\u00e9al;",
        "aff_campus_unique_index": "0;0+0;0+0;0+0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;0+0;0+0;0+0",
        "aff_country_unique": "Canada;"
    },
    {
        "title": "FALKON: An Optimal Large Scale Kernel Method",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9170",
        "id": "9170",
        "author_site": "Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco",
        "author": "Alessandro Rudi; Luigi Carratino; Lorenzo Rosasco",
        "abstract": "Kernel methods provide a principled way to perform non linear, nonparametric learning. They rely on solid functional analytic foundations and enjoy optimal statistical properties. However, at least in their basic form,  they have limited  applicability in large scale scenarios because of stringent computational requirements  in terms of time and especially memory. In this paper, we take a substantial step in scaling up kernel methods, proposing FALKON, a novel algorithm that allows to efficiently process millions of points. FALKON is derived combining several algorithmic principles, namely stochastic subsampling, iterative solvers and preconditioning. Our theoretical analysis shows that  optimal statistical accuracy  is achieved  requiring essentially $O(n)$ memory and $O(n\\sqrt{n})$  time. An extensive experimental analysis on large scale datasets shows that, even with a single machine,  FALKON   outperforms  previous state of the art solutions, which exploit parallel/distributed architectures.",
        "bibtex": "@inproceedings{NIPS2017_05546b0e,\n author = {Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {FALKON: An Optimal Large Scale Kernel Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/05546b0e38ab9175cd905eebcc6ebb76-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/05546b0e38ab9175cd905eebcc6ebb76-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/05546b0e38ab9175cd905eebcc6ebb76-Reviews.html",
        "metareview": "",
        "pdf_size": 464764,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6578743372888343188&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "INRIA \u2013 Sierra Project-team, \u00c9cole Normale Sup\u00e9rieure, Paris; University of Genoa; University of Genoa, LCSL, IIT & MIT",
        "aff_domain": "inria.fr; ; ",
        "email": "inria.fr; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "INRIA \u2013 Sierra Project-team, \u00c9cole Normale Sup\u00e9rieure, Paris;University of Genoa;University of Genoa, LCSL, IIT & MIT",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.unige.it;",
        "aff_unique_abbr": ";UniGe;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Italy"
    },
    {
        "title": "Fader Networks:Manipulating Images by Sliding Attributes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9369",
        "id": "9369",
        "author_site": "Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic DENOYER, Marc'Aurelio Ranzato",
        "author": "Guillaume Lample; Neil Zeghidour; Nicolas Usunier; Antoine Bordes; Ludovic DENOYER; Marc'Aurelio Ranzato",
        "abstract": "This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image. This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.",
        "bibtex": "@inproceedings{NIPS2017_3fd60983,\n author = {Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and DENOYER, Ludovic and Ranzato, Marc\\textquotesingle Aurelio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fader Networks:Manipulating Images by Sliding Attributes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3fd60983292458bf7dee75f12d5e9e05-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3fd60983292458bf7dee75f12d5e9e05-Reviews.html",
        "metareview": "",
        "pdf_size": 18671141,
        "gs_citation": 635,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7253031520963581223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "1Facebook AI Research + 2Sorbonne Universit\u00e9s, UPMC Univ Paris 06, UMR 7606, LIP6; 1Facebook AI Research + 3LSCP, ENS, EHESS, CNRS, PSL Research University, INRIA; 1Facebook AI Research; 1Facebook AI Research; 2Sorbonne Universit\u00e9s, UPMC Univ Paris 06, UMR 7606, LIP6; 1Facebook AI Research",
        "aff_domain": "fb.com;fb.com;fb.com;fb.com;lip6.fr;fb.com",
        "email": "fb.com;fb.com;fb.com;fb.com;lip6.fr;fb.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3fd60983292458bf7dee75f12d5e9e05-Abstract.html",
        "aff_unique_index": "0+1;0+2;0;0;1;0",
        "aff_unique_norm": "1Facebook AI Research;2Sorbonne Universit\u00e9s, UPMC Univ Paris 06, UMR 7606, LIP6;3LSCP, ENS, EHESS, CNRS, PSL Research University, INRIA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "title": "Fair Clustering Through Fairlets",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9279",
        "id": "9279",
        "author_site": "Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Sergei Vassilvitskii",
        "author": "Flavio Chierichetti; Ravi Kumar; Silvio Lattanzi; Sergei Vassilvitskii",
        "abstract": "We study the question of fair clustering under the {\\em disparate impact} doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions---for instance a point may no longer be assigned to its nearest cluster center!  En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective.  We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms.  While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow.  We empirically demonstrate the \\emph{price of fairness} by quantifying the value of fair clustering on real-world datasets with sensitive attributes.",
        "bibtex": "@inproceedings{NIPS2017_978fce5b,\n author = {Chierichetti, Flavio and Kumar, Ravi and Lattanzi, Silvio and Vassilvitskii, Sergei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fair Clustering Through Fairlets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Reviews.html",
        "metareview": "",
        "pdf_size": 358914,
        "gs_citation": 612,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4765776252275399208&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/978fce5bcc4eccc88ad48ce3914124a2-Abstract.html"
    },
    {
        "title": "Fast Black-box Variational Inference through Stochastic Trust-Region Optimization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9027",
        "id": "9027",
        "author_site": "Jeffrey Regier, Michael Jordan, Jon McAuliffe",
        "author": "Jeffrey Regier; Michael I Jordan; Jon McAuliffe",
        "abstract": "We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the reparameterization trick. At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implemented TrustVI in the Stan framework and compared it to two alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is based on stochastic first-order optimization. The latter uses second-order information, but lacks convergence guarantees. TrustVI typically converged at least one order of magnitude faster than ADVI, demonstrating the value of stochastic second-order information. TrustVI often found substantially better variational distributions than HFSGVI, demonstrating that our convergence theory can matter in practice.",
        "bibtex": "@inproceedings{NIPS2017_9a1756fd,\n author = {Regier, Jeffrey and Jordan, Michael I and McAuliffe, Jon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Black-box Variational Inference through Stochastic Trust-Region Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9a1756fd0c741126d7bbd4b692ccbd91-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9a1756fd0c741126d7bbd4b692ccbd91-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9a1756fd0c741126d7bbd4b692ccbd91-Reviews.html",
        "metareview": "",
        "pdf_size": 444314,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11834810547171481838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;stat.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9a1756fd0c741126d7bbd4b692ccbd91-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9010",
        "id": "9010",
        "author_site": "Quentin Berthet, Vianney Perchet",
        "author": "Quentin Berthet; Vianney Perchet",
        "abstract": "We consider the problem of bandit optimization, inspired by stochastic optimization and online learning problems with bandit feedback. In this problem, the objective is to minimize a global loss function of all the actions, not necessarily a cumulative loss. This framework allows us to study a very general class of problems, with applications in statistics, machine learning, and other fields. To solve this problem, we analyze the Upper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and convex optimization. We give theoretical guarantees for the performance of this algorithm over various classes of functions, and discuss the optimality of these results.",
        "bibtex": "@inproceedings{NIPS2017_dc960c46,\n author = {Berthet, Quentin and Perchet, Vianney},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dc960c46c38bd16e953d97cdeefdbc68-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dc960c46c38bd16e953d97cdeefdbc68-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dc960c46c38bd16e953d97cdeefdbc68-Reviews.html",
        "metareview": "",
        "pdf_size": 282368,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18006557669386933492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Cambridge; ENS Paris-Saclay + Criteo Research, Paris",
        "aff_domain": "statslab.cam.ac.uk;normalesup.org",
        "email": "statslab.cam.ac.uk;normalesup.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of Cambridge;\u00c9cole Normale Sup\u00e9rieure Paris-Saclay;Criteo",
        "aff_unique_dep": ";;Criteo Research",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ensparis-saclay.fr;https://research.criteo.com",
        "aff_unique_abbr": "Cambridge;ENS Paris-Saclay;Criteo",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Cambridge;Paris-Saclay;Paris",
        "aff_country_unique_index": "0;1+1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "title": "Fast amortized inference of neural activity from calcium imaging data with variational autoencoders",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9183",
        "id": "9183",
        "author_site": "Artur Speiser, Jinyao Yan, Evan Archer, Lars Buesing, Srinivas C Turaga, Jakob H Macke",
        "author": "Artur Speiser; Jinyao Yan; Evan W Archer; Lars Buesing; Srinivas C. Turaga; Jakob H Macke",
        "abstract": "Calcium imaging permits optical measurement of neural activity. Since intracellular calcium concentration is an indirect measurement of neural activity, computational tools are necessary to infer the true underlying spiking activity from fluorescence measurements. Bayesian model inversion can be used to solve this problem, but typically requires either computationally expensive MCMC sampling, or faster but approximate maximum-a-posteriori optimization.  Here, we introduce a flexible algorithmic framework for fast, efficient and accurate extraction of neural spikes from imaging data. Using the framework of variational autoencoders, we propose to amortize inference by training a deep neural network to perform model inversion efficiently. The recognition network is trained to produce samples from the posterior distribution over spike trains.  Once trained, performing inference amounts to a fast single forward pass through the network, without the need for iterative optimization or sampling. We show that amortization can be applied flexibly to a wide range of nonlinear generative models and significantly improves upon the state of the art in computation time, while achieving competitive accuracy.  Our framework is also able to represent posterior distributions over spike-trains. We demonstrate the generality of our method by proposing the first probabilistic approach for separating backpropagating action potentials from putative synaptic inputs in calcium imaging of dendritic spines.",
        "bibtex": "@inproceedings{NIPS2017_5d6646aa,\n author = {Speiser, Artur and Yan, Jinyao and Archer, Evan W and Buesing, Lars and Turaga, Srinivas C and Macke, Jakob H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast amortized inference of neural activity from calcium imaging data with variational autoencoders},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5d6646aad9bcc0be55b2c82f69750387-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5d6646aad9bcc0be55b2c82f69750387-Reviews.html",
        "metareview": "",
        "pdf_size": 551471,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8152329658450946407&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5d6646aad9bcc0be55b2c82f69750387-Abstract.html"
    },
    {
        "title": "Fast, Sample-Efficient Algorithms for Structured Phase Retrieval",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9268",
        "id": "9268",
        "author_site": "Gauri Jagatap, Chinmay Hegde",
        "author": "Gauri Jagatap; Chinmay Hegde",
        "abstract": "We consider the problem of recovering a signal x in R^n, from magnitude-only measurements, y",
        "bibtex": "@inproceedings{NIPS2017_c3a690be,\n author = {Jagatap, Gauri and Hegde, Chinmay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast, Sample-Efficient Algorithms for Structured Phase Retrieval},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c3a690be93aa602ee2dc0ccab5b7b67e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c3a690be93aa602ee2dc0ccab5b7b67e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c3a690be93aa602ee2dc0ccab5b7b67e-Reviews.html",
        "metareview": "",
        "pdf_size": 161411,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15406611780086614580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Electrical and Computer Engineering, Iowa State University; Electrical and Computer Engineering, Iowa State University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c3a690be93aa602ee2dc0ccab5b7b67e-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Electrical and Computer Engineering, Iowa State University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Fast-Slow Recurrent Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9364",
        "id": "9364",
        "author_site": "Asier Mujika, Florian Meier, Angelika Steger",
        "author": "Asier Mujika; Florian Meier; Angelika Steger",
        "abstract": "Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both  multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales  and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character based language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where  we improve state of the art results to  1.19 and 1.25 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on  Hutter Prize Wikipedia outperforming  the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture,  and thus can be flexibly applied to different tasks.",
        "bibtex": "@inproceedings{NIPS2017_e4a93f03,\n author = {Mujika, Asier and Meier, Florian and Steger, Angelika},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast-Slow Recurrent Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e4a93f0332b2519177ed55741ea4e5e7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e4a93f0332b2519177ed55741ea4e5e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e4a93f0332b2519177ed55741ea4e5e7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e4a93f0332b2519177ed55741ea4e5e7-Reviews.html",
        "metareview": "",
        "pdf_size": 364257,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16335266062519837134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland",
        "aff_domain": "ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e4a93f0332b2519177ed55741ea4e5e7-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Z\u00fcrich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9225",
        "id": "9225",
        "author_site": "Cong Fang, Feng Cheng, Zhouchen Lin",
        "author": "Cong Fang; Feng Cheng; Zhouchen Lin",
        "abstract": "We study stochastic convex optimization subjected to linear equality constraints. Traditional Stochastic Alternating Direction Method of Multipliers and its Nesterov's acceleration scheme can only achieve ergodic O(1/\\sqrt{K}) convergence rates, where K is the number of iteration. By introducing Variance Reduction (VR) techniques, the convergence rates improve to ergodic O(1/K). In this paper, we propose a new stochastic ADMM which elaborately integrates Nesterov's extrapolation and VR techniques. With Nesterov\u2019s extrapolation, our algorithm can achieve a non-ergodic O(1/K) convergence rate which is optimal for separable linearly constrained non-smooth convex problems, while the convergence rates of VR based ADMM methods are actually tight O(1/\\sqrt{K}) in non-ergodic sense. To the best of our knowledge, this is the first work that achieves a truly accelerated, stochastic convergence rate for constrained convex problems. The experimental results demonstrate that our algorithm is significantly faster than the existing state-of-the-art stochastic ADMM methods.",
        "bibtex": "@inproceedings{NIPS2017_7e3b7a5b,\n author = {Fang, Cong and Cheng, Feng and Lin, Zhouchen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Reviews.html",
        "metareview": "",
        "pdf_size": 567394,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15126333659464310723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Abstract.html"
    },
    {
        "title": "Federated Multi-Task Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9220",
        "id": "9220",
        "author_site": "Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S Talwalkar",
        "author": "Virginia Smith; Chao-Kai Chiang; Maziar Sanjabi; Ameet S Talwalkar",
        "abstract": "Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.",
        "bibtex": "@inproceedings{NIPS2017_6211080f,\n author = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Federated Multi-Task Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Reviews.html",
        "metareview": "",
        "pdf_size": 626453,
        "gs_citation": 2457,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2729989706953112243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford; USC; USC; CMU",
        "aff_domain": "stanford.edu;usc.edu;gmail.com;cmu.edu",
        "email": "stanford.edu;usc.edu;gmail.com;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Stanford University;University of Southern California;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.usc.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Stanford;USC;CMU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Stanford;Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Few-Shot Adversarial Domain Adaptation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9435",
        "id": "9435",
        "author_site": "Saeid Motiian, Quinn Jones, Seyed Iranmanesh, Gianfranco Doretto",
        "author": "Saeid Motiian; Quinn Jones; Seyed Iranmanesh; Gianfranco Doretto",
        "abstract": "This work provides a framework for addressing the problem of supervised domain adaptation with deep models. The main idea is to exploit adversarial learning to learn an embedded subspace that simultaneously maximizes the confusion between two domains while semantically aligning their embedding. The supervised setting becomes attractive especially when there are only a few target data samples that need to be labeled. In this few-shot learning scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by carefully designing a training scheme whereby the typical binary adversarial discriminator is augmented to distinguish between four different classes, it is possible to effectively address the supervised adaptation problem. In addition, the approach has a high \u201cspeed\u201d of adaptation, i.e. it requires an extremely low number of labeled target training samples, even one per category can be effective. We then extensively compare this approach to the state of the art in domain adaptation in two experiments: one using datasets for handwritten digit recognition, and one using datasets for visual object recognition.",
        "bibtex": "@inproceedings{NIPS2017_21c5bba1,\n author = {Motiian, Saeid and Jones, Quinn and Iranmanesh, Seyed and Doretto, Gianfranco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Few-Shot Adversarial Domain Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Reviews.html",
        "metareview": "",
        "pdf_size": 1419215,
        "gs_citation": 522,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6767514726454604430&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Lane Department of Computer Science and Electrical Engineering; Lane Department of Computer Science and Electrical Engineering; Lane Department of Computer Science and Electrical Engineering; Lane Department of Computer Science and Electrical Engineering",
        "aff_domain": "mix.wvu.edu;mix.wvu.edu;mix.wvu.edu;mix.wvu.edu",
        "email": "mix.wvu.edu;mix.wvu.edu;mix.wvu.edu;mix.wvu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/21c5bba1dd6aed9ab48c2b34c1a0adde-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Lane Department of Computer Science and Electrical Engineering",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Few-Shot Learning Through an Information Retrieval  Lens",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9013",
        "id": "9013",
        "author_site": "Eleni Triantafillou, Richard Zemel, Raquel Urtasun",
        "author": "Eleni Triantafillou; Richard Zemel; Raquel Urtasun",
        "abstract": "Few-shot learning refers to understanding new concepts from only a few examples. We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime. We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously. In particular, we view each batch point as a `query' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.",
        "bibtex": "@inproceedings{NIPS2017_01e9565c,\n author = {Triantafillou, Eleni and Zemel, Richard and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Few-Shot Learning Through an Information Retrieval  Lens},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/01e9565cecc4e989123f9620c1d09c09-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/01e9565cecc4e989123f9620c1d09c09-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/01e9565cecc4e989123f9620c1d09c09-Reviews.html",
        "metareview": "",
        "pdf_size": 737824,
        "gs_citation": 291,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6438993040212686070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html"
    },
    {
        "title": "Filtering Variational Objectives",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9426",
        "id": "9426",
        "author_site": "Chris Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Teh",
        "author": "Chris J Maddison; John Lawson; George Tucker; Nicolas Heess; Mohammad Norouzi; Andriy Mnih; Arnaud Doucet; Yee Teh",
        "abstract": "When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.",
        "bibtex": "@inproceedings{NIPS2017_fa84632d,\n author = {Maddison, Chris J and Lawson, John and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Filtering Variational Objectives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fa84632d742f2729dc32ce8cb5d49733-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fa84632d742f2729dc32ce8cb5d49733-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fa84632d742f2729dc32ce8cb5d49733-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fa84632d742f2729dc32ce8cb5d49733-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fa84632d742f2729dc32ce8cb5d49733-Reviews.html",
        "metareview": "",
        "pdf_size": 318782,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6016385180093191918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "DeepMind; Google Brain; University of Oxford; DeepMind; Google Brain; DeepMind; University of Oxford; DeepMind",
        "aff_domain": "google.com;google.com;google.com; ; ; ; ; ",
        "email": "google.com;google.com;google.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fa84632d742f2729dc32ce8cb5d49733-Abstract.html",
        "aff_unique_index": "0;1;2;0;1;0;2;0",
        "aff_unique_norm": "DeepMind;Google;University of Oxford",
        "aff_unique_dep": ";Google Brain;",
        "aff_unique_url": "https://deepmind.com;https://brain.google.com;https://www.ox.ac.uk",
        "aff_unique_abbr": "DeepMind;Google Brain;Oxford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0;0;1;0;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9325",
        "id": "9325",
        "author_site": "Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, Tie-Yan Liu",
        "author": "Yue Wang; Wei Chen; Yuting Liu; Zhi-Ming Ma; Tie-Yan Liu",
        "abstract": "In reinforcement learning (RL), one of the key components is policy evaluation, which aims to estimate the value function (i.e., expected long-term accumulated reward) of a policy. With a good policy evaluation method, the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous \\emph{Gradient-based Temporal Difference(GTD)} policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming, a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated, previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting  them into  convex-concave saddle point problems. However, it is well-known that, the data are generated from Markov processes rather than i.i.d in RL problems.. In this paper, in the realistic Markov setting, we derive the finite sample bounds for the general convex-concave saddle point problems, and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size, GTD algorithms converge. (2) The convergence rate is determined by the step size, with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix, the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process.  To the best of our knowledge, our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.",
        "bibtex": "@inproceedings{NIPS2017_353de269,\n author = {Wang, Yue and Chen, Wei and Liu, Yuting and Ma, Zhi-Ming and Liu, Tie-Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/353de26971b93af88da102641069b440-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/353de26971b93af88da102641069b440-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/353de26971b93af88da102641069b440-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/353de26971b93af88da102641069b440-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/353de26971b93af88da102641069b440-Reviews.html",
        "metareview": "",
        "pdf_size": 840607,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17413953397336783081&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Science, Beijing Jiaotong University; Microsoft Research; School of Science, Beijing Jiaotong University; Academy of Mathematics and Systems Science, Chinese Academy of Sciences; Microsoft Research",
        "aff_domain": "bjtu.edu.cn;microsoft.com;bjtu.edu.cn;amt.ac.cn;microsoft.com",
        "email": "bjtu.edu.cn;microsoft.com;bjtu.edu.cn;amt.ac.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/353de26971b93af88da102641069b440-Abstract.html",
        "aff_unique_index": "0;1;0;2;1",
        "aff_unique_norm": "School of Science, Beijing Jiaotong University;Microsoft Corporation;Chinese Academy of Sciences",
        "aff_unique_dep": ";Microsoft Research;Academy of Mathematics and Systems Science",
        "aff_unique_url": ";https://www.microsoft.com/en-us/research;http://www.amss.cas.cn",
        "aff_unique_abbr": ";MSR;AMSS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;1",
        "aff_country_unique": ";United States;China"
    },
    {
        "title": "First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8994",
        "id": "8994",
        "author_site": "Aryan Mokhtari, Alejandro Ribeiro",
        "author": "Aryan Mokhtari; Alejandro Ribeiro",
        "abstract": "This paper studies empirical risk minimization (ERM) problems for large-scale datasets and incorporates the idea of adaptive sample size methods to improve the guaranteed convergence bounds for first-order stochastic and deterministic methods. In contrast to traditional methods that attempt to solve the ERM problem corresponding to the full dataset directly, adaptive sample size schemes start with a small number of samples and solve the corresponding ERM problem to its statistical accuracy. The sample size is then grown geometrically -- e.g., scaling by a factor of two -- and use the solution of the previous ERM as a warm start for the new ERM. Theoretical analyses show that the use of adaptive sample size methods reduces the overall computational cost of achieving the statistical accuracy of the whole dataset for a broad range of deterministic and stochastic first-order methods. The gains are specific to the choice of method. When particularized to, e.g., accelerated gradient descent and stochastic variance reduce gradient, the computational cost advantage is a logarithm of the number of training samples. Numerical experiments on various datasets confirm theoretical claims and showcase the gains of using the proposed adaptive sample size scheme.",
        "bibtex": "@inproceedings{NIPS2017_81e5f81d,\n author = {Mokhtari, Aryan and Ribeiro, Alejandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/81e5f81db77c596492e6f1a5a792ed53-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/81e5f81db77c596492e6f1a5a792ed53-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/81e5f81db77c596492e6f1a5a792ed53-Reviews.html",
        "metareview": "",
        "pdf_size": 383063,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6328795500918536551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fisher GAN",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9038",
        "id": "9038",
        "author_site": "Youssef Mroueh, Tom Sercu",
        "author": "Youssef Mroueh; Tom Sercu",
        "abstract": "Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN that fits within the Integral Probability Metrics (IPM) framework for  training GANs. Fisher GAN defines a data dependent constraint on the second order moments of the critic. We show in this paper that Fisher GAN allows for stable and time efficient  training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.",
        "bibtex": "@inproceedings{NIPS2017_07042ac7,\n author = {Mroueh, Youssef and Sercu, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fisher GAN},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/07042ac7d03d3b9911a00da43ce0079a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/07042ac7d03d3b9911a00da43ce0079a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/07042ac7d03d3b9911a00da43ce0079a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/07042ac7d03d3b9911a00da43ce0079a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/07042ac7d03d3b9911a00da43ce0079a-Reviews.html",
        "metareview": "",
        "pdf_size": 3896232,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7342548181871041432&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "AI Foundations, IBM Research AI+IBM T.J Watson Research Center; AI Foundations, IBM Research AI+IBM T.J Watson Research Center",
        "aff_domain": "us.ibm.com;ibm.com",
        "email": "us.ibm.com;ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/07042ac7d03d3b9911a00da43ce0079a-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "AI Foundations, IBM Research AI;IBM T.J Watson Research Center",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "title": "Fitting Low-Rank Tensors in Constant Time",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9034",
        "id": "9034",
        "author_site": "Kohei Hayashi, Yuichi Yoshida",
        "author": "Kohei Hayashi; Yuichi Yoshida",
        "abstract": "In this paper, we develop an algorithm that approximates the residual error of Tucker decomposition, one of the most popular tensor decomposition methods, with a provable guarantee.  Given an order-$K$ tensor $X\\in\\mathbb{R}^{N_1\\times\\cdots\\times N_K}$, our algorithm randomly samples a constant number $s$ of indices for each mode and creates a ``mini'' tensor $\\tilde{X}\\in\\mathbb{R}^{s\\times\\cdots\\times s}$, whose elements are given by the intersection of the sampled indices on $X$.  Then, we show that the residual error of the Tucker decomposition of $\\tilde{X}$ is sufficiently close to that of $X$ with high probability.  This result implies that we can figure out how much we can fit a low-rank tensor to $X$ \\emph{in constant time}, regardless of the size of $X$. This is useful for guessing the favorable rank of Tucker decomposition.  Finally, we demonstrate how the sampling method works quickly and accurately using multiple real datasets.",
        "bibtex": "@inproceedings{NIPS2017_c930eecd,\n author = {Hayashi, Kohei and Yoshida, Yuichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fitting Low-Rank Tensors in Constant Time},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c930eecd01935feef55942cc445f708f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c930eecd01935feef55942cc445f708f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c930eecd01935feef55942cc445f708f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c930eecd01935feef55942cc445f708f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c930eecd01935feef55942cc445f708f-Reviews.html",
        "metareview": "",
        "pdf_size": 421132,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3619717544084323999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "National Institute of Advanced Industrial Science and Technology+RIKEN AIP; National Institute of Informatics",
        "aff_domain": "gmail.com;nii.ac.jp",
        "email": "gmail.com;nii.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c930eecd01935feef55942cc445f708f-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;RIKEN;National Institute of Informatics",
        "aff_unique_dep": ";Advanced Institute for Computational Science;",
        "aff_unique_url": "https://www.aist.go.jp;https://www.aip.riken.jp;https://www.nii.ac.jp/",
        "aff_unique_abbr": "AIST;RIKEN AIP;NII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8915",
        "id": "8915",
        "author_site": "Joel A Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher",
        "author": "Joel A Tropp; Alp Yurtsever; Madeleine Udell; Volkan Cevher",
        "abstract": "Several important applications, such as streaming PCA and semidefinite programming, involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates.  Because of storage limitations, it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nystr\u00f6m approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix.  Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.",
        "bibtex": "@inproceedings{NIPS2017_4558dbb6,\n author = {Tropp, Joel A and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Reviews.html",
        "metareview": "",
        "pdf_size": 455103,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12593849299192833291&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Caltech; EPFL; Cornell; EPFL",
        "aff_domain": "caltech.edu;epfl.ch;cornell.edu;epfl.ch",
        "email": "caltech.edu;epfl.ch;cornell.edu;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4558dbb6f6f8bb2e16d03b85bde76e2c-Abstract.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "California Institute of Technology;Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne;Cornell University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.caltech.edu;https://www.epfl.ch;https://www.cornell.edu",
        "aff_unique_abbr": "Caltech;EPFL;Cornell",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "title": "Flexible statistical inference for mechanistic models of neural dynamics",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8921",
        "id": "8921",
        "author_site": "Jan-Matthis Lueckmann, Pedro Goncalves, Giacomo Bassetto, Kaan \u00d6cal, Marcel Nonnenmacher, Jakob H Macke",
        "author": "Jan-Matthis Lueckmann; Pedro J Goncalves; Giacomo Bassetto; Kaan \u00d6cal; Marcel Nonnenmacher; Jakob H Macke",
        "abstract": "Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.",
        "bibtex": "@inproceedings{NIPS2017_addfa9b7,\n author = {Lueckmann, Jan-Matthis and Goncalves, Pedro J and Bassetto, Giacomo and \\\"{O}cal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Flexible statistical inference for mechanistic models of neural dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Reviews.html",
        "metareview": "",
        "pdf_size": 1759779,
        "gs_citation": 322,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1774681204531368581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "1research center caesar, an associate of the Max Planck Society, Bonn, Germany; 1research center caesar, an associate of the Max Planck Society, Bonn, Germany; 1research center caesar, an associate of the Max Planck Society, Bonn, Germany; 1research center caesar, an associate of the Max Planck Society, Bonn, Germany + 2Mathematical Institute, University of Bonn, Bonn, Germany; 1research center caesar, an associate of the Max Planck Society, Bonn, Germany; 1research center caesar, an associate of the Max Planck Society, Bonn, Germany",
        "aff_domain": "caesar.de;caesar.de;caesar.de;caesar.de;caesar.de;caesar.de",
        "email": "caesar.de;caesar.de;caesar.de;caesar.de;caesar.de;caesar.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html",
        "aff_unique_index": "0;0;0;0+1;0;0",
        "aff_unique_norm": "1research center caesar, an associate of the Max Planck Society, Bonn, Germany;2Mathematical Institute, University of Bonn, Bonn, Germany",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8964",
        "id": "8964",
        "author_site": "Urs K\u00f6ster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal, William Constable, Oguz Elibol, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J Pai, Naveen Rao",
        "author": "Urs K\u00f6ster; Tristan Webb; Xin Wang; Marcel Nassar; Arjun K Bansal; William Constable; Oguz Elibol; Scott Gray; Stewart Hall; Luke Hornof; Amir Khosrowshahi; Carey Kloss; Ruby J Pai; Naveen Rao",
        "abstract": "Deep neural networks are commonly developed and trained in 32-bit floating point format. Significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit floating point format training and inference, designed to support modern deep network topologies without modifications. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range. We validate Flexpoint by training AlexNet, a deep residual network and a generative adversarial network, using a simulator implemented with the \\emph{neon} deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit floating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.",
        "bibtex": "@inproceedings{NIPS2017_a0160709,\n author = {K\\\"{o}ster, Urs and Webb, Tristan and Wang, Xin and Nassar, Marcel and Bansal, Arjun K and Constable, William and Elibol, Oguz and Gray, Scott and Hall, Stewart and Hornof, Luke and Khosrowshahi, Amir and Kloss, Carey and Pai, Ruby J and Rao, Naveen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a0160709701140704575d499c997b6ca-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a0160709701140704575d499c997b6ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a0160709701140704575d499c997b6ca-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a0160709701140704575d499c997b6ca-Reviews.html",
        "metareview": "",
        "pdf_size": 1358672,
        "gs_citation": 352,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16063878598472599920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation + OpenAI; Artificial Intelligence Products Group, Intel Corporation + Cerebras Systems; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation; Artificial Intelligence Products Group, Intel Corporation",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 14,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a0160709701140704575d499c997b6ca-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0+1;0+2;0;0;0;0;0",
        "aff_unique_norm": "Artificial Intelligence Products Group, Intel Corporation;OpenAI;Cerebras Systems",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://openai.com;https://www.cerebras.com",
        "aff_unique_abbr": ";OpenAI;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9014",
        "id": "9014",
        "author_site": "Matthias Hein, Maksym Andriushchenko",
        "author": "Matthias Hein; Maksym Andriushchenko",
        "abstract": "Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified  input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific \\emph{lower bounds} on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.",
        "bibtex": "@inproceedings{NIPS2017_e077e1a5,\n author = {Hein, Matthias and Andriushchenko, Maksym},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Reviews.html",
        "metareview": "",
        "pdf_size": 677074,
        "gs_citation": 662,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7126535280922614101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Mathematics and Computer Science, Saarland University, Saarbr\u00fccken Informatics Campus, Germany; Department of Mathematics and Computer Science, Saarland University, Saarbr\u00fccken Informatics Campus, Germany",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e077e1a544eec4f0307cf5c3c721d944-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Mathematics and Computer Science, Saarland University, Saarbr\u00fccken Informatics Campus, Germany",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "From Bayesian Sparsity to Gated Recurrent Nets",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9330",
        "id": "9330",
        "author_site": "Hao He, Bo Xin, Satoshi Ikehata, David Wipf",
        "author": "Hao He; Bo Xin; Satoshi Ikehata; David Wipf",
        "abstract": "The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with pre-specified weights.  This observation has prompted the development of learning-based approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data.  For example, important NP-hard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations.  Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorization-minimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction.  As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences.  The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems.   The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.",
        "bibtex": "@inproceedings{NIPS2017_e6cbc650,\n author = {He, Hao and Xin, Bo and Ikehata, Satoshi and Wipf, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From Bayesian Sparsity to Gated Recurrent Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e6cbc650cd5798a05dfd0f51d14cde5c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6cbc650cd5798a05dfd0f51d14cde5c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6cbc650cd5798a05dfd0f51d14cde5c-Reviews.html",
        "metareview": "",
        "pdf_size": 431661,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15555342371870385188&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Massachusetts Institute of Technology; Microsoft Research, Beijing, China; National Institute of Informatics; Microsoft Research, Beijing, China",
        "aff_domain": "mit.edu;gmail.com;gmail.com;gmail.com",
        "email": "mit.edu;gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6cbc650cd5798a05dfd0f51d14cde5c-Abstract.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Microsoft Research;National Institute of Informatics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-asia;https://www.nii.ac.jp/",
        "aff_unique_abbr": "MIT;MSR;NII",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;1;2;1",
        "aff_country_unique": "United States;China;Japan"
    },
    {
        "title": "From Parity to Preference-based Notions of Fairness in Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8820",
        "id": "8820",
        "author_site": "Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, Adrian Weller",
        "author": "Muhammad Bilal Zafar; Isabel Valera; Manuel Rodriguez; Krishna Gummadi; Adrian Weller",
        "abstract": "The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.",
        "bibtex": "@inproceedings{NIPS2017_82161242,\n author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel and Gummadi, Krishna and Weller, Adrian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From Parity to Preference-based Notions of Fairness in Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/82161242827b703e6acf9c726942a1e4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/82161242827b703e6acf9c726942a1e4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/82161242827b703e6acf9c726942a1e4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/82161242827b703e6acf9c726942a1e4-Reviews.html",
        "metareview": "",
        "pdf_size": 897361,
        "gs_citation": 293,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2455293981432384177&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "MPI-SWS; MPI-IS; MPI-SWS; MPI-SWS; University of Cambridge & Alan Turing Institute",
        "aff_domain": "mpi-sws.org;tue.mpg.de;mpi-sws.org;mpi-sws.org;cam.ac.uk",
        "email": "mpi-sws.org;tue.mpg.de;mpi-sws.org;mpi-sws.org;cam.ac.uk",
        "github": "",
        "project": "http://fate-computing.mpi-sws.org/",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/82161242827b703e6acf9c726942a1e4-Abstract.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Max Planck Institute for Software Systems;Max Planck Institute for Intelligent Systems;University of Cambridge & Alan Turing Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mpi-sws.org;https://www.mpituebingen.mpg.de;",
        "aff_unique_abbr": "MPI-SWS;MPI-IS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "title": "From which world is your graph",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8938",
        "id": "8938",
        "author_site": "Cheng Li, Felix MF Wong, Zhenming Liu, Varun Kanade",
        "author": "Cheng Li; Felix MF Wong; Zhenming Liu; Varun Kanade",
        "abstract": "Discovering statistical structure from links is a fundamental problem in the analysis of social networks. Choosing a misspecified model, or equivalently, an incorrect inference algorithm will result in an invalid analysis or even falsely uncover patterns that are in fact artifacts of the model. This work focuses on unifying two of the most widely used link-formation models: the stochastic block model (SBM) and the small world (or latent space) model (SWM). Integrating techniques from kernel learning, spectral graph theory, and nonlinear dimensionality reduction, we develop the first statistically sound polynomial-time algorithm to discover latent patterns in sparse graphs for both models. When the network comes from an SBM, the algorithm outputs a block structure. When it is from an SWM, the algorithm outputs estimates of each node's latent position.",
        "bibtex": "@inproceedings{NIPS2017_b55ec28c,\n author = {Li, Cheng and Wong, Felix MF and Liu, Zhenming and Kanade, Varun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From which world is your graph},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b55ec28c52d5f6205684a473a2193564-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b55ec28c52d5f6205684a473a2193564-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b55ec28c52d5f6205684a473a2193564-Reviews.html",
        "metareview": "",
        "pdf_size": 755303,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3330001269780482292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html"
    },
    {
        "title": "Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9079",
        "id": "9079",
        "author_site": "Roel Dobbe, David Fridovich-Keil, Claire Tomlin",
        "author": "Roel Dobbe; David Fridovich-Keil; Claire Tomlin",
        "abstract": "Learning cooperative policies for multi-agent systems is often challenged by partial observability and a lack of coordination. In some settings, the structure of a problem allows a distributed solution with limited communication. Here, we consider a scenario where no communication is available, and instead we learn local policies for all agents that collectively mimic the solution to a centralized multi-agent static optimization problem. Our main contribution is an information theoretic framework based on rate distortion theory which facilitates analysis of how well the resulting fully decentralized policies are able to reconstruct the optimal solution. Moreover, this framework provides a natural extension that addresses which nodes an agent should communicate with to improve the  performance of its individual policy.",
        "bibtex": "@inproceedings{NIPS2017_8bb88f80,\n author = {Dobbe, Roel and Fridovich-Keil, David and Tomlin, Claire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8bb88f80d334b1869781beb89f7b73be-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8bb88f80d334b1869781beb89f7b73be-Reviews.html",
        "metareview": "",
        "pdf_size": 736453,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4459332321023986930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Electrical Engineering and Computer Science, University of California, Berkeley; Electrical Engineering and Computer Science, University of California, Berkeley; Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8bb88f80d334b1869781beb89f7b73be-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9431",
        "id": "9431",
        "author_site": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter",
        "author": "Martin Heusel; Hubert Ramsauer; Thomas Unterthiner; Bernhard Nessler; Sepp Hochreiter",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fr\u00e9chet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
        "bibtex": "@inproceedings{NIPS2017_8a1d6947,\n author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Reviews.html",
        "metareview": "",
        "pdf_size": 1736086,
        "gs_citation": 17120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15143899073250151317&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html"
    },
    {
        "title": "GP CaKe: Effective brain connectivity with causal kernels",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8889",
        "id": "8889",
        "author_site": "Luca Ambrogioni, Max Hinne, Marcel Van Gerven, Eric Maris",
        "author": "Luca Ambrogioni; Max Hinne; Marcel Van Gerven; Eric Maris",
        "abstract": "A fundamental goal in network neuroscience is to understand how activity in one brain region drives activity elsewhere, a process referred to as effective connectivity. Here we propose to model this causal interaction using integro-differential equations and causal kernels that allow for a rich analysis of effective connectivity. The approach combines the tractability and flexibility of autoregressive modeling with the biophysical interpretability of dynamic causal modeling. The causal kernels are learned nonparametrically using Gaussian process regression, yielding an efficient framework for causal inference. We construct a novel class of causal covariance functions that enforce the desired properties of the causal kernels, an approach which we call GP CaKe. By construction, the model and its hyperparameters have biophysical meaning and are therefore easily interpretable. We demonstrate the efficacy of GP CaKe on a number of simulations and give an example of a realistic application on magnetoencephalography (MEG) data.",
        "bibtex": "@inproceedings{NIPS2017_9cf81d80,\n author = {Ambrogioni, Luca and Hinne, Max and Van Gerven, Marcel and Maris, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {GP CaKe: Effective brain connectivity with causal kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9cf81d8026a9018052c429cc4e56739b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9cf81d8026a9018052c429cc4e56739b-Reviews.html",
        "metareview": "",
        "pdf_size": 3796772,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15022246851777834148&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Radboud University; Radboud University; Radboud University; Radboud University",
        "aff_domain": "donders.ru.nl;donders.ru.nl;donders.ru.nl;donders.ru.nl",
        "email": "donders.ru.nl;donders.ru.nl;donders.ru.nl;donders.ru.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9cf81d8026a9018052c429cc4e56739b-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Radboud University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ru.nl",
        "aff_unique_abbr": "RU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "title": "Gated Recurrent Convolution Neural Network for OCR",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8830",
        "id": "8830",
        "author_site": "Jianfeng Wang, Xiaolin Hu",
        "author": "Jianfeng Wang; Xiaolin Hu",
        "abstract": "Optical Character Recognition (OCR) aims to recognize text in natural images. Inspired by a recently proposed model for general image classification, Recurrent Convolution Neural Network (RCNN), we propose a new architecture named Gated RCNN (GRCNN) for solving this problem. Its critical component, Gated Recurrent Convolution Layer (GRCL), is constructed by adding a gate to the Recurrent Convolution Layer (RCL), the critical component of RCNN. The gate controls the context modulation in RCL and balances the feed-forward information and the recurrent information. In addition, an efficient Bidirectional Long Short-Term Memory (BLSTM) is built for sequence modeling. The GRCNN is combined with BLSTM to recognize text in natural images. The entire GRCNN-BLSTM model can be trained end-to-end. Experiments show that the proposed model outperforms existing methods on several benchmark datasets including the IIIT-5K, Street View Text (SVT) and ICDAR.",
        "bibtex": "@inproceedings{NIPS2017_c24cd76e,\n author = {Wang, Jianfeng and Hu, Xiaolin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gated Recurrent Convolution Neural Network for OCR},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c24cd76e1ce41366a4bbe8a49b02a028-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c24cd76e1ce41366a4bbe8a49b02a028-Reviews.html",
        "metareview": "",
        "pdf_size": 1525752,
        "gs_citation": 242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14509961177242148875&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Beijing University of Posts and Telecommunications+Tsinghua University; Tsinghua National Laboratory for Information Science and Technology (TNList)+Department of Computer Science and Technology+Center for Brain-Inspired Computing Research (CBICR)+Tsinghua University",
        "aff_domain": "gmail.com;tsinghua.edu.cn",
        "email": "gmail.com;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html",
        "aff_unique_index": "0+1;1+2+3+1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Tsinghua University;University of Cambridge;Center for Brain-Inspired Computing Research (CBICR)",
        "aff_unique_dep": ";;Department of Computer Science and Technology;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.tsinghua.edu.cn;https://www.cam.ac.uk;",
        "aff_unique_abbr": "BUPT;THU;Cambridge;",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Beijing;;Cambridge",
        "aff_country_unique_index": "0+0;0+1+0",
        "aff_country_unique": "China;United Kingdom;"
    },
    {
        "title": "Gauging Variational Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9073",
        "id": "9073",
        "author_site": "Sungsoo Ahn, Michael Chertkov, Jinwoo Shin",
        "author": "Sung-Soo Ahn; Michael Chertkov; Jinwoo Shin",
        "abstract": "Computing partition function is the most important statistical inference task arising in applications of Graphical Models (GM). Since it is computationally intractable, approximate  methods have been used in practice, where mean-field  (MF) and belief propagation (BP) are arguably the most popular and successful approaches of a variational type. In this paper, we propose two new variational schemes, coined Gauged-MF (G-MF) and Gauged-BP (G-BP), improving MF and BP, respectively. Both provide lower bounds for the partition function by utilizing the so-called gauge transformation which modifies factors of GM while keeping the partition function invariant. Moreover, we prove that both G-MF and G-BP are exact for GMs with a single loop of a special structure, even though the bare MF and BP perform badly in this case. Our extensive experiments indeed confirm that the proposed algorithms outperform and generalize MF and BP.",
        "bibtex": "@inproceedings{NIPS2017_8d420fa3,\n author = {Ahn, Sung-Soo and Chertkov, Michael and Shin, Jinwoo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gauging Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8d420fa35754d1f1c19969c88780314d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8d420fa35754d1f1c19969c88780314d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8d420fa35754d1f1c19969c88780314d-Reviews.html",
        "metareview": "",
        "pdf_size": 559546,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2773129985697404992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Theoretical Division, T-4 & Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos, NM 87545, USA + Skolkovo Institute of Science and Technology, 143026 Moscow, Russia; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea",
        "aff_domain": "kaist.ac.kr;lanl.gov;kaist.ac.kr",
        "email": "kaist.ac.kr;lanl.gov;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8d420fa35754d1f1c19969c88780314d-Abstract.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Los Alamos National Laboratory;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": "School of Electrical Engineering;Theoretical Division, T-4 & Center for Nonlinear Studies;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.lanl.gov;https://www.skoltech.ru",
        "aff_unique_abbr": "KAIST;LANL;Skoltech",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Daejeon;Los Alamos;",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "South Korea;United States;Russia"
    },
    {
        "title": "Gaussian Quadrature for Kernel Features",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9382",
        "id": "9382",
        "author_site": "Tri Dao, Christopher M De Sa, Christopher R\u00e9",
        "author": "Tri Dao; Christopher M De Sa; Christopher R\u00e9",
        "abstract": "Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(\\epsilon^{-2})$ samples are required to achieve an approximation error of at most $\\epsilon$. We investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any $\\gamma > 0$, to achieve error $\\epsilon$ with $O(e^{e^\\gamma} + \\epsilon^{-1/\\gamma})$ samples as $\\epsilon$ goes to 0. Our method works particularly well with sparse ANOVA kernels, which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.",
        "bibtex": "@inproceedings{NIPS2017_62f91ce9,\n author = {Dao, Tri and De Sa, Christopher M and R\\'{e}, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian Quadrature for Kernel Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/62f91ce9b820a491ee78c108636db089-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/62f91ce9b820a491ee78c108636db089-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/62f91ce9b820a491ee78c108636db089-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/62f91ce9b820a491ee78c108636db089-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/62f91ce9b820a491ee78c108636db089-Reviews.html",
        "metareview": "",
        "pdf_size": 328788,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12500705582150721187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Cornell University; Department of Computer Science, Stanford University",
        "aff_domain": "stanford.edu;cs.cornell.edu;cs.stanford.edu",
        "email": "stanford.edu;cs.cornell.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/62f91ce9b820a491ee78c108636db089-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;Cornell University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu;https://www.cornell.edu",
        "aff_unique_abbr": "Stanford;Cornell",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gaussian process based nonlinear latent structure discovery in multivariate spike train data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9133",
        "id": "9133",
        "author_site": "Anqi Wu, Nicholas Roy, Stephen Keeley, Jonathan Pillow",
        "author": "Anqi Wu; Nicholas A. Roy; Stephen Keeley; Jonathan W Pillow",
        "abstract": "A large body of recent work focuses on methods for extracting low-dimensional latent structure from multi-neuron spike train data. Most such methods employ either linear latent dynamics or linear mappings from latent space to log spike rates. Here we propose a doubly nonlinear latent variable model that can identify low-dimensional structure underlying apparently high-dimensional spike train data. We introduce the Poisson Gaussian-Process Latent Variable Model (P-GPLVM), which consists of Poisson spiking observations and two underlying Gaussian processes\u2014one governing a temporal latent variable and another governing a set of nonlinear tuning curves. The use of nonlinear tuning curves enables discovery of low-dimensional latent structure even when spike responses exhibit high linear dimensionality (e.g., as found in hippocampal place cell codes). To learn the model from data, we introduce the decoupled Laplace approximation, a fast approximate inference method that allows us to efficiently optimize the latent path while marginalizing over tuning curves. We show that this method outperforms previous Laplace-approximation-based inference methods in both the speed of convergence and accuracy. We apply the model to spike trains recorded from hippocampal place cells and show that it compares favorably to a variety of previous methods for latent structure discovery, including variational auto-encoder (VAE) based methods that parametrize the nonlinear mapping from latent space to spike rates with a deep neural network.",
        "bibtex": "@inproceedings{NIPS2017_b3b4d2db,\n author = {Wu, Anqi and Roy, Nicholas A. and Keeley, Stephen and Pillow, Jonathan W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian process based nonlinear latent structure discovery in multivariate spike train data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b3b4d2dbedc99fe843fd3dedb02f086f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b3b4d2dbedc99fe843fd3dedb02f086f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b3b4d2dbedc99fe843fd3dedb02f086f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b3b4d2dbedc99fe843fd3dedb02f086f-Reviews.html",
        "metareview": "",
        "pdf_size": 1976365,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1826634259356524330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b3b4d2dbedc99fe843fd3dedb02f086f-Abstract.html"
    },
    {
        "title": "Generalization Properties of Learning with Random Features",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9106",
        "id": "9106",
        "author_site": "Alessandro Rudi, Lorenzo Rosasco",
        "author": "Alessandro Rudi; Lorenzo Rosasco",
        "abstract": "We study the generalization properties of ridge regression with random features in the statistical learning  framework. We show for the first time that $O(1/\\sqrt{n})$ learning bounds can be achieved with only  $O(\\sqrt{n}\\log n)$  random features rather than $O({n})$  as suggested by previous results. Further,  we prove  faster learning rates and show that they might require more random features, unless they are sampled according to  a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential  effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties.",
        "bibtex": "@inproceedings{NIPS2017_61b1fb3f,\n author = {Rudi, Alessandro and Rosasco, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalization Properties of Learning with Random Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Reviews.html",
        "metareview": "",
        "pdf_size": 671287,
        "gs_citation": 434,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17029451222814059921&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "INRIA - Sierra Project-team, \u00b4Ecole Normale Sup \u00b4erieure, Paris, 75012 Paris, France + Laboratory of Computational and Statistical Learning (Istituto Italiano di Tecnologia); University of Genova, Istituto Italiano di Tecnologia, Massachusetts Institute of Technology",
        "aff_domain": "inria.fr;mit.edu",
        "email": "inria.fr;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/61b1fb3f59e28c67f3925f3c79be81a1-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "INRIA - Sierra Project-team, \u00b4Ecole Normale Sup \u00b4erieure, Paris, 75012 Paris, France;Laboratory of Computational and Statistical Learning (Istituto Italiano di Tecnologia);University of Genova, Istituto Italiano di Tecnologia, Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Generalized Linear Model Regression under Distance-to-set Penalties",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8930",
        "id": "8930",
        "author_site": "Jason Xu, Eric Chi, Kenneth Lange",
        "author": "Jason Xu; Eric Chi; Kenneth Lange",
        "abstract": "Estimation in generalized linear models (GLM) is complicated by the presence of constraints. One can handle constraints by maximizing a penalized log-likelihood. Penalties such as the lasso are effective in high dimensions but often lead to severe shrinkage. This paper explores instead penalizing the squared distance to constraint sets. Distance penalties are more flexible than algebraic and regularization penalties, and avoid the drawback of shrinkage. To optimize distance penalized objectives, we make use of the majorization-minimization principle. Resulting algorithms constructed within this framework are amenable to acceleration and come with global convergence guarantees. Applications to shape constraints, sparse regression, and rank-restricted matrix regression on synthetic and real data showcase the strong empirical performance of distance penalization, even under non-convex constraints.",
        "bibtex": "@inproceedings{NIPS2017_061412e4,\n author = {Xu, Jason and Chi, Eric and Lange, Kenneth},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized Linear Model Regression under Distance-to-set Penalties},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/061412e4a03c02f9902576ec55ebbe77-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/061412e4a03c02f9902576ec55ebbe77-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/061412e4a03c02f9902576ec55ebbe77-Reviews.html",
        "metareview": "",
        "pdf_size": 717418,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5297565236080298949&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Los Angeles; North Carolina State University; University of California, Los Angeles",
        "aff_domain": "ucla.edu;ncsu.edu;ucla.edu",
        "email": "ucla.edu;ncsu.edu;ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Los Angeles;North Carolina State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.ncsu.edu",
        "aff_unique_abbr": "UCLA;NCSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Generalizing GANs: A Turing Perspective",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9402",
        "id": "9402",
        "author_site": "Roderich Gross, Yue Gu, Wei Li, Melvin Gauci",
        "author": "Roderich Gross; Yue Gu; Wei Li; Melvin Gauci",
        "abstract": "Recently, a new class of machine learning algorithms has emerged, where models and discriminators are generated in a competitive setting. The most prominent example is Generative Adversarial Networks (GANs). In this paper we examine how these algorithms relate to the Turing test, and derive what - from a Turing perspective - can be considered their defining features. Based on these features, we outline directions for generalizing GANs - resulting in the family of algorithms referred to as Turing Learning. One such direction is to allow the discriminators to interact with the processes from which the data samples are obtained, making them \"interrogators\", as in the Turing test. We validate this idea using two case studies. In the first case study, a computer infers the behavior of an agent while controlling its environment. In the second case study, a robot infers its own sensor configuration while controlling its movements. The results confirm that by allowing discriminators to interrogate, the accuracy of models is improved.",
        "bibtex": "@inproceedings{NIPS2017_73e5080f,\n author = {Gross, Roderich and Gu, Yue and Li, Wei and Gauci, Melvin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalizing GANs: A Turing Perspective},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/73e5080f0f3804cb9cf470a8ce895dac-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/73e5080f0f3804cb9cf470a8ce895dac-Reviews.html",
        "metareview": "",
        "pdf_size": 567111,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14027978725417113299&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Automatic Control and Systems Engineering, The University of Shef\ufb01eld; Department of Automatic Control and Systems Engineering, The University of Shef\ufb01eld; Department of Electronics, The University of York; Wyss Institute for Biologically Inspired Engineering, Harvard University",
        "aff_domain": "sheffield.ac.uk;sheffield.ac.uk;york.ac.uk;g.harvard.edu",
        "email": "sheffield.ac.uk;sheffield.ac.uk;york.ac.uk;g.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/73e5080f0f3804cb9cf470a8ce895dac-Abstract.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Department of Automatic Control and Systems Engineering, The University of Shef\ufb01eld;Department of Electronics, The University of York;Wyss Institute for Biologically Inspired Engineering, Harvard University",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Generating steganographic images via adversarial training",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8984",
        "id": "8984",
        "author_site": "Jamie Hayes, George Danezis",
        "author": "Jamie Hayes; George Danezis",
        "abstract": "Adversarial training has proved to be competitive against supervised learning methods on computer vision tasks. However, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing the existence of information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques. We also show that supervised training of our adversarial model produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques.",
        "bibtex": "@inproceedings{NIPS2017_fe2d0103,\n author = {Hayes, Jamie and Danezis, George},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generating steganographic images via adversarial training},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fe2d010308a6b3799a3d9c728ee74244-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fe2d010308a6b3799a3d9c728ee74244-Reviews.html",
        "metareview": "",
        "pdf_size": 1157836,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11633977877366166394&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University College London; University College London + The Alan Turing Institute",
        "aff_domain": "cs.ucl.ac.uk;ucl.ac.uk",
        "email": "cs.ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "https://www.eff.org/files/2014/05/29/unnecessary_and_disproportionate.pdf",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fe2d010308a6b3799a3d9c728ee74244-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University College London;The Alan Turing Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "UCL;ATI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Generative Local Metric Learning for Kernel Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9032",
        "id": "9032",
        "author_site": "Yung-Kyun Noh, Masashi Sugiyama, Kee-Eung Kim, Frank Park, Daniel Lee",
        "author": "Yung-Kyun Noh; Masashi Sugiyama; Kee-Eung Kim; Frank Park; Daniel D Lee",
        "abstract": "This paper shows how metric learning can be used with Nadaraya-Watson (NW) kernel regression.  Compared with standard approaches, such as bandwidth selection, we show how metric learning can significantly reduce the mean square error (MSE) in kernel regression, particularly for high-dimensional data.  We propose a method for efficiently learning a good metric function based upon analyzing the performance of the NW estimator for Gaussian-distributed data.  A key feature of our approach is that the NW estimator with a learned metric uses information from both the global and local structure of the training data.  Theoretical and empirical results confirm that the learned metric can considerably reduce the bias and MSE for kernel regression even when the data are not confined to Gaussian.",
        "bibtex": "@inproceedings{NIPS2017_f69e505b,\n author = {Noh, Yung-Kyun and Sugiyama, Masashi and Kim, Kee-Eung and Park, Frank and Lee, Daniel D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generative Local Metric Learning for Kernel Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f69e505b08403ad2298b9f262659929a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f69e505b08403ad2298b9f262659929a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f69e505b08403ad2298b9f262659929a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f69e505b08403ad2298b9f262659929a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f69e505b08403ad2298b9f262659929a-Reviews.html",
        "metareview": "",
        "pdf_size": 715869,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12012765802130526372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Seoul National University, Rep. of Korea; RIKEN / The University of Tokyo, Japan; KAIST, Rep. of Korea; Seoul National University, Rep. of Korea; University of Pennsylvania, USA",
        "aff_domain": "snu.ac.kr;k.u-tokyo.ac.jp;cs.kaist.ac.kr;snu.ac.kr;seas.upenn.edu",
        "email": "snu.ac.kr;k.u-tokyo.ac.jp;cs.kaist.ac.kr;snu.ac.kr;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f69e505b08403ad2298b9f262659929a-Abstract.html",
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "Seoul National University, Rep. of Korea;RIKEN / The University of Tokyo, Japan;Korea Advanced Institute of Science and Technology;University of Pennsylvania",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;https://www.kaist.ac.kr;https://www.upenn.edu",
        "aff_unique_abbr": ";;KAIST;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";South Korea;United States"
    },
    {
        "title": "Geometric Descent Method for Convex Composite Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8859",
        "id": "8859",
        "author_site": "Shixiang Chen, Shiqian Ma, Wei Liu",
        "author": "Shixiang Chen; Shiqian Ma; Wei Liu",
        "abstract": "In this paper, we extend the geometric descent method recently proposed by Bubeck, Lee and Singh to tackle nonsmooth and strongly convex composite problems. We prove that our proposed algorithm, dubbed geometric proximal gradient method (GeoPG), converges with a linear rate $(1-1/\\sqrt{\\kappa})$ and thus achieves the optimal rate among first-order methods, where $\\kappa$ is the condition number of the problem. Numerical results on linear regression and logistic regression with elastic net regularization show that GeoPG compares favorably with Nesterov's accelerated proximal gradient method, especially when the problem is ill-conditioned.",
        "bibtex": "@inproceedings{NIPS2017_a8abb4bb,\n author = {Chen, Shixiang and Ma, Shiqian and Liu, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Geometric Descent Method for Convex Composite Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a8abb4bb284b5b27aa7cb790dc20f80b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a8abb4bb284b5b27aa7cb790dc20f80b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a8abb4bb284b5b27aa7cb790dc20f80b-Reviews.html",
        "metareview": "",
        "pdf_size": 539243,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3233061875496748235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of SEEM, The Chinese University of Hong Kong, Hong Kong; Department of Mathematics, UC Davis, USA; Tencent AI Lab, China",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a8abb4bb284b5b27aa7cb790dc20f80b-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "The Chinese University of Hong Kong;Department of Mathematics, UC Davis, USA;",
        "aff_unique_dep": "Department of SEEM;;",
        "aff_unique_url": "https://www.cuhk.edu.hk;;",
        "aff_unique_abbr": "CUHK;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9152",
        "id": "9152",
        "author_site": "Federico Monti, Michael Bronstein, Xavier Bresson",
        "author": "Federico Monti; Michael Bronstein; Xavier Bresson",
        "abstract": "Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationary structures on user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets, showing that it outperforms state-of-the-art matrix completion techniques.",
        "bibtex": "@inproceedings{NIPS2017_2eace51d,\n author = {Monti, Federico and Bronstein, Michael and Bresson, Xavier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2eace51d8f796d04991c831a07059758-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2eace51d8f796d04991c831a07059758-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2eace51d8f796d04991c831a07059758-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2eace51d8f796d04991c831a07059758-Reviews.html",
        "metareview": "",
        "pdf_size": 9068917,
        "gs_citation": 711,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12720117978042601586&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Universit\u00e0 della Svizzera italiana, Lugano, Switzerland; Universit\u00e0 della Svizzera italiana, Lugano, Switzerland; School of Computer Science and Engineering, NTU, Singapore",
        "aff_domain": "usi.ch;usi.ch;ntu.edu.sg",
        "email": "usi.ch;usi.ch;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2eace51d8f796d04991c831a07059758-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Universit\u00e0 della Svizzera italiana;Nanyang Technological University",
        "aff_unique_dep": ";School of Computer Science and Engineering",
        "aff_unique_url": "https://www.usi.ch;https://www.ntu.edu.sg",
        "aff_unique_abbr": "USI;NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lugano;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Switzerland;Singapore"
    },
    {
        "title": "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9285",
        "id": "9285",
        "author_site": "Alex Lamb, R Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron Courville, Yoshua Bengio",
        "author": "Alex M Lamb; Devon Hjelm; Yaroslav Ganin; Joseph Paul Cohen; Aaron C. Courville; Yoshua Bengio",
        "abstract": "Directed latent variable models that formulate the joint distribution as $p(x,z) = p(z) p(x \\mid z)$ have the advantage of fast and exact sampling. However, these models have the weakness of needing to specify $p(z)$, often with a simple fixed prior that limits the expressiveness of the model.  Undirected latent variable models discard the requirement that $p(z)$ be specified with a prior, yet sampling from them generally requires an iterative procedure such as blocked Gibbs-sampling that may require many steps to draw samples from the joint distribution $p(x, z)$.  We propose a novel approach to learning the joint distribution between the data and a latent code which uses an adversarially learned iterative procedure to gradually refine the joint distribution, $p(x, z)$, to better match with the data distribution on each step.  GibbsNet is the best of both worlds both in theory and in practice.  Achieving the speed and simplicity of a directed latent variable model, it is guaranteed (assuming the adversarial game reaches the virtual training criteria global minimum) to produce samples from $p(x, z)$ with only a few sampling iterations.  Achieving the expressiveness and flexibility of an undirected latent variable model, GibbsNet does away with the need for an explicit $p(z)$ and has the ability to do attribute prediction, class-conditional generation, and joint image-attribute modeling in a single model which is not trained for any of these specific tasks.  We show empirically that GibbsNet is able to learn a more complex $p(z)$ and show that this leads to improved inpainting and iterative refinement of $p(x, z)$ for dozens of steps and stable generation without collapse for thousands of steps, despite being trained on only a few steps.",
        "bibtex": "@inproceedings{NIPS2017_30f8f6b9,\n author = {Lamb, Alex M and Hjelm, Devon and Ganin, Yaroslav and Cohen, Joseph Paul and Courville, Aaron C and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {GibbsNet: Iterative Adversarial Inference for Deep Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Reviews.html",
        "metareview": "",
        "pdf_size": 3790252,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16908737664975000661&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/30f8f6b940d1073d8b6a5eebc46dd6e5-Abstract.html"
    },
    {
        "title": "Good Semi-supervised Learning That Requires a Bad GAN",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9420",
        "id": "9420",
        "author_site": "Zihang Dai, Zhilin Yang, Fan Yang, William Cohen, Ruslan Salakhutdinov",
        "author": "Zihang Dai; Zhilin Yang; Fan Yang; William W. Cohen; Ruslan Salakhutdinov",
        "abstract": "Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically we show that given the discriminator objective, good semi-supervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets.",
        "bibtex": "@inproceedings{NIPS2017_79514e88,\n author = {Dai, Zihang and Yang, Zhilin and Yang, Fan and Cohen, William W and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Good Semi-supervised Learning That Requires a Bad GAN},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/79514e888b8f2acacc68738d0cbb803e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/79514e888b8f2acacc68738d0cbb803e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/79514e888b8f2acacc68738d0cbb803e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/79514e888b8f2acacc68738d0cbb803e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/79514e888b8f2acacc68738d0cbb803e-Reviews.html",
        "metareview": "",
        "pdf_size": 2536060,
        "gs_citation": 599,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9678350544464313908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science; School of Computer Science; School of Computer Science; School of Computer Science; School of Computer Science",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/kimiyoung/ssl_bad_gan",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/79514e888b8f2acacc68738d0cbb803e-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "School of Computer Science",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Gradient Descent Can Take Exponential Time to Escape Saddle Points",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8900",
        "id": "8900",
        "author_site": "Simon Du, Chi Jin, Jason D Lee, Michael Jordan, Aarti Singh, Barnabas Poczos",
        "author": "Simon S Du; Chi Jin; Jason Lee; Michael I Jordan; Aarti Singh; Barnabas Poczos",
        "abstract": "Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points\u2014it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.",
        "bibtex": "@inproceedings{NIPS2017_f79921bb,\n author = {Du, Simon S and Jin, Chi and Lee, Jason D and Jordan, Michael I and Singh, Aarti and Poczos, Barnabas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradient Descent Can Take Exponential Time to Escape Saddle Points},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Reviews.html",
        "metareview": "",
        "pdf_size": 539225,
        "gs_citation": 324,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6167885709417307539&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Carnegie Mellon University; University of California, Berkeley; University of Southern California; University of California, Berkeley; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;berkeley.edu;marshall.usc.edu;cs.berkeley.edu;cs.cmu.edu;cmu.edu",
        "email": "cs.cmu.edu;berkeley.edu;marshall.usc.edu;cs.berkeley.edu;cs.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html",
        "aff_unique_index": "0;1;2;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;University of California, Berkeley;University of Southern California",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.berkeley.edu;https://www.usc.edu",
        "aff_unique_abbr": "CMU;UC Berkeley;USC",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Berkeley;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gradient Episodic Memory for Continual Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9416",
        "id": "9416",
        "author_site": "David Lopez-Paz, Marc'Aurelio Ranzato",
        "author": "David Lopez-Paz; Marc'Aurelio Ranzato",
        "abstract": "One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2017_f8752278,\n author = {Lopez-Paz, David and Ranzato, Marc\\textquotesingle Aurelio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradient Episodic Memory for Continual Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Reviews.html",
        "metareview": "",
        "pdf_size": 418148,
        "gs_citation": 3418,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17788496987696818587&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Facebook Artificial Intelligence Research; Facebook Artificial Intelligence Research",
        "aff_domain": "fb.com;fb.com",
        "email": "fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f87522788a2be2d171666752f97ddebb-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Facebook",
        "aff_unique_dep": "Artificial Intelligence Research",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gradient Methods for Submodular Maximization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9357",
        "id": "9357",
        "author_site": "Hamed Hassani, Mahdi Soltanolkotabi, Amin Karbasi",
        "author": "Hamed Hassani; Mahdi Soltanolkotabi; Amin Karbasi",
        "abstract": "In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor $1/2$ approximation to the global maxima. We also study stochastic gradient methods and show that after $\\mathcal{O}(1/\\epsilon^2)$ iterations these methods reach solutions which achieve in expectation objective values exceeding $(\\frac{\\text{OPT}}{2}-\\epsilon)$. An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor $1/2$ approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient ascent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.",
        "bibtex": "@inproceedings{NIPS2017_24b43fb0,\n author = {Hassani, Hamed and Soltanolkotabi, Mahdi and Karbasi, Amin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradient Methods for Submodular Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/24b43fb034a10d78bec71274033b4096-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/24b43fb034a10d78bec71274033b4096-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/24b43fb034a10d78bec71274033b4096-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/24b43fb034a10d78bec71274033b4096-Reviews.html",
        "metareview": "",
        "pdf_size": 601289,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2693761823324381655&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "ESE Department, University of Pennsylvania, Philadelphia, PA; EE Department, University of Southern California, Los Angeles, CA; ECE Department, Yale University, New Haven, CT",
        "aff_domain": "seas.upenn.edu;usc.edu;yale.edu",
        "email": "seas.upenn.edu;usc.edu;yale.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/24b43fb034a10d78bec71274033b4096-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "ESE Department, University of Pennsylvania, Philadelphia, PA;EE Department, University of Southern California, Los Angeles, CA;ECE Department, Yale University, New Haven, CT",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Gradient descent GAN optimization is locally stable",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9333",
        "id": "9333",
        "author_site": "Vaishnavh Nagarajan, J. Zico Kolter",
        "author": "Vaishnavh Nagarajan; J. Zico Kolter",
        "abstract": "Despite the growing prominence of generative adversarial   networks (GANs), optimization in GANs is still a poorly understood topic.  In this paper, we analyze   the ``gradient descent'' form of GAN optimization, i.e., the natural   setting where we simultaneously take small gradient steps in both generator   and discriminator parameters.  We show that even though GAN optimization   does \\emph{not}    correspond to a convex-concave game (even for simple parameterizations), under   proper conditions, equilibrium points of this  optimization    procedure are still \\emph{locally asymptotically stable} for the traditional   GAN formulation. On the other hand, we show that the recently proposed   Wasserstein GAN can have non-convergent limit cycles near equilibrium.   Motivated by this stability analysis, we    propose an additional regularization term for gradient descent GAN updates,   which \\emph{is} able to guarantee local stability for both the WGAN and the   traditional GAN, and also shows practical promise in speeding up   convergence and addressing mode collapse.",
        "bibtex": "@inproceedings{NIPS2017_7e0a0209,\n author = {Nagarajan, Vaishnavh and Kolter, J. Zico},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradient descent GAN optimization is locally stable},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7e0a0209b929d097bd3e8ef30567a5c1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7e0a0209b929d097bd3e8ef30567a5c1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7e0a0209b929d097bd3e8ef30567a5c1-Reviews.html",
        "metareview": "",
        "pdf_size": 3622634,
        "gs_citation": 410,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15799922175856134689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA 15213; Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7e0a0209b929d097bd3e8ef30567a5c1-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA 15213",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9346",
        "id": "9346",
        "author_site": "John T Halloran, David M Rocke",
        "author": "John T Halloran; David M Rocke",
        "abstract": "Tandem mass spectrometry (MS/MS) is a high-throughput technology used to identify the proteins in a complex biological sample, such as a drop of blood. A collection of spectra is generated at the output of the process, each spectrum of which is representative of a peptide (protein subsequence) present in the original complex sample. In this work, we leverage the log-likelihood gradients of generative models to improve the identification of such spectra. In particular, we show that the gradient of a recently proposed dynamic Bayesian network (DBN) may be naturally employed by a kernel-based discriminative classifier. The resulting Fisher kernel substantially improves upon recent attempts to combine generative and discriminative models for post-processing analysis, outperforming all other methods on the evaluated datasets. We extend the improved accuracy offered by the Fisher kernel framework to other search algorithms by introducing Theseus, a DBN representating a large number of widely used MS/MS scoring functions. Furthermore, with gradient ascent and max-product inference at hand, we use Theseus to learn model parameters without any supervision.",
        "bibtex": "@inproceedings{NIPS2017_a4666cd9,\n author = {Halloran, John T and Rocke, David M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Reviews.html",
        "metareview": "",
        "pdf_size": 1494457,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18180404496377041396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Public Health Sciences, University of California, Davis; Department of Public Health Sciences, University of California, Davis",
        "aff_domain": "ucdavis.edu;ucdavis.edu",
        "email": "ucdavis.edu;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a4666cd9e1ab0e4abf05a0fb232f4ad3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Public Health Sciences, University of California, Davis",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Graph Matching via Multiplicative Update Algorithm",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9103",
        "id": "9103",
        "author_site": "Bo Jiang, Jin Tang, Chris Ding, Yihong Gong, Bin Luo",
        "author": "Bo Jiang; Jin Tang; Chris Ding; Yihong Gong; Bin Luo",
        "abstract": "As a fundamental problem in computer vision, graph matching problem can usually be formulated as a Quadratic Programming (QP) problem with doubly stochastic and discrete (integer) constraints. Since it is NP-hard, approximate algorithms are required. In this paper, we present a new algorithm, called Multiplicative Update Graph Matching (MPGM), that develops a multiplicative update technique to solve the QP matching problem. MPGM has three main benefits: (1) theoretically, MPGM solves the general QP problem with doubly stochastic constraint naturally whose convergence and KKT optimality are guaranteed. (2) Em- pirically, MPGM generally returns a sparse solution and thus can also incorporate the discrete constraint approximately. (3) It is efficient and simple to implement. Experimental results show the benefits of MPGM algorithm.",
        "bibtex": "@inproceedings{NIPS2017_d1e946f4,\n author = {Jiang, Bo and Tang, Jin and Ding, Chris and Gong, Yihong and Luo, Bin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graph Matching via Multiplicative Update Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d1e946f4e67db4b362ad23818a6fb78a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d1e946f4e67db4b362ad23818a6fb78a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d1e946f4e67db4b362ad23818a6fb78a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d1e946f4e67db4b362ad23818a6fb78a-Reviews.html",
        "metareview": "",
        "pdf_size": 1818611,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7516695194071289965&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, Anhui University, China; School of Computer Science and Technology, Anhui University, China; CSE Department, University of Texas at Arlington, Arlington, USA; School of Electronic and Information Engineering, Xi\u2019an Jiaotong University, China; School of Computer Science and Technology, Anhui University, China",
        "aff_domain": "ahu.edu.cn;ahu.edu.cn;uta.edu;mail.xjtu.edu.cn;ahu.edu.cn",
        "email": "ahu.edu.cn;ahu.edu.cn;uta.edu;mail.xjtu.edu.cn;ahu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d1e946f4e67db4b362ad23818a6fb78a-Abstract.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Anhui University;CSE Department, University of Texas at Arlington, Arlington, USA;Xi'an Jiaotong University",
        "aff_unique_dep": "School of Computer Science and Technology;;School of Electronic and Information Engineering",
        "aff_unique_url": "http://www.ahu.edu.cn/;;http://www.xjtu.edu.cn",
        "aff_unique_abbr": ";;XJTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Xi'an",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8872",
        "id": "8872",
        "author_site": "Francesco Locatello, Michael Tschannen, Gunnar Ratsch, Martin Jaggi",
        "author": "Francesco Locatello; Michael Tschannen; Gunnar Raetsch; Martin Jaggi",
        "abstract": "Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e^{-t})) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.",
        "bibtex": "@inproceedings{NIPS2017_63538fe6,\n author = {Locatello, Francesco and Tschannen, Michael and Raetsch, Gunnar and Jaggi, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/63538fe6ef330c13a05a3ed7e599d5f7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/63538fe6ef330c13a05a3ed7e599d5f7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/63538fe6ef330c13a05a3ed7e599d5f7-Reviews.html",
        "metareview": "",
        "pdf_size": 442169,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13872485763376662831&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "MPI for Intelligent Systems - ETH Zurich; ETH Zurich; ETH Zurich; EPFL",
        "aff_domain": "ethz.ch;nari.ee.ethz.ch;inf.ethz.ch;epfl.ch",
        "email": "ethz.ch;nari.ee.ethz.ch;inf.ethz.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/63538fe6ef330c13a05a3ed7e599d5f7-Abstract.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "MPI for Intelligent Systems - ETH Zurich;ETH Zurich;Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.ethz.ch;https://www.epfl.ch",
        "aff_unique_abbr": ";ETHZ;EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";Switzerland"
    },
    {
        "title": "Group Additive Structure Identification for Kernel Nonparametric Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9267",
        "id": "9267",
        "author_site": "Chao Pan, Michael Zhu",
        "author": "Chao Pan; Michael Zhu",
        "abstract": "The additive model is one of the most popularly used models for high dimensional nonparametric regression analysis. However, its main drawback is that it neglects possible interactions between predictor variables. In this paper, we reexamine the group additive model proposed in the literature, and rigorously define the intrinsic group additive structure for the relationship between the response variable $Y$ and the predictor vector $\\vect{X}$, and further develop an effective structure-penalized kernel method for simultaneous identification of the intrinsic group additive structure and nonparametric function estimation. The method utilizes a novel complexity measure we derive for group additive structures. We show that the proposed method is consistent in identifying the intrinsic group additive structure.  Simulation study and real data applications demonstrate the effectiveness of the proposed method as a general tool for high dimensional nonparametric regression.",
        "bibtex": "@inproceedings{NIPS2017_2aedcba6,\n author = {Pan, Chao and Zhu, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Group Additive Structure Identification for Kernel Nonparametric Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2aedcba61ca55ceb62d785c6b7f10a83-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2aedcba61ca55ceb62d785c6b7f10a83-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2aedcba61ca55ceb62d785c6b7f10a83-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2aedcba61ca55ceb62d785c6b7f10a83-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2aedcba61ca55ceb62d785c6b7f10a83-Reviews.html",
        "metareview": "",
        "pdf_size": 475528,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8085459208293861667&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Statistics, Purdue University; Department of Statistics, Purdue University + Center for Statistical Science, Department of Industrial Engineering, Tsinghua University",
        "aff_domain": "gmail.com;purdue.edu",
        "email": "gmail.com;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2aedcba61ca55ceb62d785c6b7f10a83-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Purdue University;Tsinghua University",
        "aff_unique_dep": "Department of Statistics;Department of Industrial Engineering",
        "aff_unique_url": "https://www.purdue.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Purdue;Tsinghua",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Group Sparse Additive Machine",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8817",
        "id": "8817",
        "author_site": "Hong Chen, Xiaoqian Wang, Cheng Deng, Heng Huang",
        "author": "Hong Chen; Xiaoqian Wang; Cheng Deng; Heng Huang",
        "abstract": "A family of learning algorithms generated from additive models have attracted much attention recently for their flexibility and interpretability in high dimensional data analysis. Among them, learning models with grouped variables have shown competitive performance for prediction and variable selection. However, the previous works mainly focus on the least squares regression problem, not the classification task. Thus, it is desired to design the new additive classification model with variable selection capability for many real-world applications which focus on high-dimensional data classification. To address this challenging problem, in this paper, we investigate the classification with group sparse additive models in reproducing kernel Hilbert spaces. A novel classification method, called as \\emph{group sparse additive machine} (GroupSAM), is proposed to explore and utilize the structure information among the input variables. Generalization error bound is derived and proved by integrating the sample error analysis with empirical covering numbers and the hypothesis error estimate with the stepping stone technique. Our new bound shows that GroupSAM can achieve a satisfactory learning rate with polynomial decay. Experimental results on synthetic data and seven benchmark datasets consistently show the effectiveness of our new approach.",
        "bibtex": "@inproceedings{NIPS2017_7e7757b1,\n author = {Chen, Hong and Wang, Xiaoqian and Deng, Cheng and Huang, Heng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Group Sparse Additive Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7e7757b1e12abcb736ab9a754ffb617a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7e7757b1e12abcb736ab9a754ffb617a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7e7757b1e12abcb736ab9a754ffb617a-Reviews.html",
        "metareview": "",
        "pdf_size": 319262,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2970171374987494476&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering, University of Pittsburgh, USA; Department of Electrical and Computer Engineering, University of Pittsburgh, USA; School of Electronic Engineering, Xidian University, China; Department of Electrical and Computer Engineering, University of Pittsburgh, USA",
        "aff_domain": "mail.hzau.edu.cn;gmail.com;mail.xidian.edu.cn;pitt.edu",
        "email": "mail.hzau.edu.cn;gmail.com;mail.xidian.edu.cn;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7e7757b1e12abcb736ab9a754ffb617a-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Pittsburgh;Xidian University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;School of Electronic Engineering",
        "aff_unique_url": "https://www.pitt.edu;http://www.xidian.edu.cn",
        "aff_unique_abbr": "Pitt;Xidian",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Hash Embeddings for Efficient Word Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9269",
        "id": "9269",
        "author_site": "Dan Tito Svenstrup, Jonas Hansen, Ole Winther",
        "author": "Dan Tito Svenstrup; Jonas Hansen; Ole Winther",
        "abstract": "We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight vector. The final $d$ dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of $B$ embedding vectors.  Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.",
        "bibtex": "@inproceedings{NIPS2017_f0f6ba4b,\n author = {Tito Svenstrup, Dan and Hansen, Jonas and Winther, Ole},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hash Embeddings for Efficient Word Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Reviews.html",
        "metareview": "",
        "pdf_size": 283433,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6998624806225669969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department for Applied Mathematics and Computer Science, Technical University of Denmark (DTU); FindZebra; Department for Applied Mathematics and Computer Science, Technical University of Denmark (DTU)",
        "aff_domain": "dtu.dk;findzebra.com;dtu.dk",
        "email": "dtu.dk;findzebra.com;dtu.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f0f6ba4b5e0000340312d33c212c3ae8-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department for Applied Mathematics and Computer Science, Technical University of Denmark (DTU);FindZebra",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Hiding Images in Plain Sight: Deep Steganography",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8995",
        "id": "8995",
        "author": "Shumeet Baluja",
        "abstract": "Steganography is the practice of concealing a secret message within another, ordinary, message.  Commonly, steganography is used to unobtrusively hide a small message within the noisy regions of a larger image.  In this study, we attempt to place a full size color image within another image of the same size.  Deep neural networks are simultaneously trained to create the hiding and revealing processes and are designed to specifically work as a pair.  The system is trained on images drawn randomly from the ImageNet database, and works well on natural images from a wide variety of sources.  Beyond demonstrating the successful application of deep learning to hiding images, we carefully examine how the result is achieved and explore extensions.  Unlike many popular steganographic methods that encode the secret message within the least significant bits of the carrier image, our approach compresses and distributes the secret image's representation across all of the available bits.",
        "bibtex": "@inproceedings{NIPS2017_838e8afb,\n author = {Baluja, Shumeet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hiding Images in Plain Sight: Deep Steganography},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/838e8afb1ca34354ac209f53d90c3a43-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/838e8afb1ca34354ac209f53d90c3a43-Reviews.html",
        "metareview": "",
        "pdf_size": 8234496,
        "gs_citation": 707,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18152019609842686409&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/838e8afb1ca34354ac209f53d90c3a43-Abstract.html"
    },
    {
        "title": "Hierarchical Attentive Recurrent Tracking",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9090",
        "id": "9090",
        "author_site": "Adam Kosiorek, Alex Bewley, Ingmar Posner",
        "author": "Adam Kosiorek; Alex Bewley; Ingmar Posner",
        "abstract": "Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate",
        "bibtex": "@inproceedings{NIPS2017_752d25a1,\n author = {Kosiorek, Adam and Bewley, Alex and Posner, Ingmar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Attentive Recurrent Tracking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/752d25a1f8dbfb2d656bac3094bfb81c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/752d25a1f8dbfb2d656bac3094bfb81c-Reviews.html",
        "metareview": "",
        "pdf_size": 8042161,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10597913779613577710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/752d25a1f8dbfb2d656bac3094bfb81c-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Hierarchical Clustering Beyond the Worst-Case",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9391",
        "id": "9391",
        "author_site": "Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn",
        "author": "Vincent Cohen-Addad; Varun Kanade; Frederik Mallmann-Trenn",
        "abstract": "Hiererachical clustering, that is computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is a fundamental problem in data analysis. Although hierarchical clustering has mostly been studied through procedures such as linkage algorithms, or top-down heuristics, rather than as optimization problems, recently Dasgupta [1] proposed an objective function for hierarchical clustering and initiated a line of work developing algorithms that explicitly optimize an objective (see also [2, 3, 4]). In this paper, we consider a fairly general random graph model for hierarchical clustering, called the hierarchical stochastic blockmodel (HSBM), and show that in certain regimes the SVD approach of McSherry [5] combined with specific linkage methods results in a clustering that give an O(1)-approximation to Dasgupta\u2019s cost function. We also show that an approach based on SDP relaxations for balanced cuts based on the work of Makarychev et al. [6], combined with the recursive sparsest cut algorithm of Dasgupta, yields an O(1) approximation in slightly larger regimes and also in the semi-random setting, where an adversary may remove edges from the random graph generated according to an HSBM. Finally, we report empirical evaluation on synthetic and real-world  data showing that our proposed SVD-based method does indeed achieve a better cost than other widely-used heurstics and also results in a better classification accuracy when the underlying problem was that of multi-class classification.",
        "bibtex": "@inproceedings{NIPS2017_e8bf0f27,\n author = {Cohen-Addad, Vincent and Kanade, Varun and Mallmann-Trenn, Frederik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Clustering Beyond the Worst-Case},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e8bf0f27d70d480d3ab793bb7619aaa5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e8bf0f27d70d480d3ab793bb7619aaa5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e8bf0f27d70d480d3ab793bb7619aaa5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e8bf0f27d70d480d3ab793bb7619aaa5-Reviews.html",
        "metareview": "",
        "pdf_size": 504985,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11340911430999800596&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Copenhagen; University of Oxford + Alan Turing Institute; MIT",
        "aff_domain": "gmail.com;cs.ox.ac.uk;mit.edu",
        "email": "gmail.com;cs.ox.ac.uk;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html",
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "University of Copenhagen;University of Oxford;Alan Turing Institute;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ku.dk;https://www.ox.ac.uk;https://www.turing.ac.uk;https://web.mit.edu",
        "aff_unique_abbr": "UCPH;Oxford;ATI;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1;2",
        "aff_country_unique": "Denmark;United Kingdom;United States"
    },
    {
        "title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9327",
        "id": "9327",
        "author_site": "Dustin Tran, Rajesh Ranganath, David Blei",
        "author": "Dustin Tran; Rajesh Ranganath; David Blei",
        "abstract": "Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for models which encompass our understanding of the physical word. Despite this fundamental nature, the use of implicit models remains limited due to challenge in positing complex latent structure in them, and the ability to inference in such models with large data sets. In this paper, we first introduce the hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for symbol generation.",
        "bibtex": "@inproceedings{NIPS2017_6f1d0705,\n author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Implicit Models and Likelihood-Free Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Reviews.html",
        "metareview": "",
        "pdf_size": 877141,
        "gs_citation": 272,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17165298103982419096&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6f1d0705c91c2145201df18a1a0c7345-Abstract.html"
    },
    {
        "title": "Hierarchical Methods of Moments",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8979",
        "id": "8979",
        "author_site": "Matteo Ruffini, Guillaume Rabusseau, Borja Balle",
        "author": "Matteo Ruffini; Guillaume Rabusseau; Borja Balle",
        "abstract": "Spectral methods of moments provide a powerful tool for learning the parameters of latent variable models. Despite their theoretical appeal, the applicability of these methods to real data is still limited due to a lack of robustness to model misspecification. In this paper we present a hierarchical approach to methods of moments to circumvent such limitations. Our method is based on replacing the tensor decomposition step used in previous algorithms with approximate joint diagonalization. Experiments on topic modeling show that our method outperforms previous tensor decomposition methods in terms of speed and model quality.",
        "bibtex": "@inproceedings{NIPS2017_c44e5038,\n author = {Ruffini, Matteo and Rabusseau, Guillaume and Balle, Borja},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Methods of Moments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c44e503833b64e9f27197a484f4257c0-Reviews.html",
        "metareview": "",
        "pdf_size": 1251008,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7764640439438092594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Universitat Polit\u00e8cnica de Catalunya; McGill University; Amazon Research Cambridge",
        "aff_domain": "cs.upc.edu;mail.mcgill.ca;amazon.co.uk",
        "email": "cs.upc.edu;mail.mcgill.ca;amazon.co.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c44e503833b64e9f27197a484f4257c0-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Catalunya;McGill University;Amazon Research Cambridge",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.upc.edu;https://www.mcgill.ca;",
        "aff_unique_abbr": "UPC;McGill;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Spain;Canada;"
    },
    {
        "title": "High-Order Attention Models for Visual Question Answering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9149",
        "id": "9149",
        "author_site": "Idan Schwartz, Alex Schwing, Tamir Hazan",
        "author": "Idan Schwartz; Alexander Schwing; Tamir Hazan",
        "abstract": "The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they  take into account different data modalities,  such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.",
        "bibtex": "@inproceedings{NIPS2017_05192834,\n author = {Schwartz, Idan and Schwing, Alexander and Hazan, Tamir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {High-Order Attention Models for Visual Question Answering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/051928341be67dcba03f0e04104d9047-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/051928341be67dcba03f0e04104d9047-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/051928341be67dcba03f0e04104d9047-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/051928341be67dcba03f0e04104d9047-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/051928341be67dcba03f0e04104d9047-Reviews.html",
        "metareview": "",
        "pdf_size": 18288297,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13901095197481337133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Technion; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Industrial Engineering & Management, Technion",
        "aff_domain": "cs.technion.ac.il;illinois.edu;gmail.com",
        "email": "cs.technion.ac.il;illinois.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/051928341be67dcba03f0e04104d9047-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Department of Computer Science, Technion;University of Illinois at Urbana-Champaign;Department of Industrial Engineering & Management, Technion",
        "aff_unique_dep": ";Department of Electrical and Computer Engineering;",
        "aff_unique_url": ";https://illinois.edu;",
        "aff_unique_abbr": ";UIUC;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9353",
        "id": "9353",
        "author_site": "Veeranjaneyulu Sadhanala, Yu-Xiang Wang, James Sharpnack, Ryan Tibshirani",
        "author": "Veeranjaneyulu Sadhanala; Yu-Xiang Wang; James L Sharpnack; Ryan J Tibshirani",
        "abstract": "We consider the problem of estimating the values of a function over $n$ nodes of a $d$-dimensional grid graph (having equal side lengths $n^{1/d}$) from noisy observations. The function is assumed to be smooth, but is allowed to exhibit different amounts of smoothness at different regions in the grid. Such heterogeneity eludes classical measures of smoothness from nonparametric statistics, such as Holder smoothness. Meanwhile, total variation (TV) smoothness classes allow for heterogeneity, but are restrictive in another sense: only constant functions count as perfectly smooth (achieve zero TV). To move past this, we define two new higher-order TV classes, based on two ways of compiling the discrete derivatives of a parameter across the nodes. We relate these two new classes to Holder classes, and derive lower bounds on their minimax errors. We also analyze two naturally associated trend filtering methods; when $d=2$, each is seen to be rate optimal over the appropriate class.",
        "bibtex": "@inproceedings{NIPS2017_3e60e09c,\n author = {Sadhanala, Veeranjaneyulu and Wang, Yu-Xiang and Sharpnack, James L and Tibshirani, Ryan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3e60e09c222f206c725385f53d7e567c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3e60e09c222f206c725385f53d7e567c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3e60e09c222f206c725385f53d7e567c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3e60e09c222f206c725385f53d7e567c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3e60e09c222f206c725385f53d7e567c-Reviews.html",
        "metareview": "",
        "pdf_size": 616928,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12682248976416956713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Carnegie Mellon University; Carnegie Mellon University+Amazon AI; University of California, Davis; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;amazon.com;ucdavis.edu;stat.cmu.edu",
        "email": "cs.cmu.edu;amazon.com;ucdavis.edu;stat.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3e60e09c222f206c725385f53d7e567c-Abstract.html",
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Amazon;University of California, Davis",
        "aff_unique_dep": ";Amazon AI;",
        "aff_unique_url": "https://www.cmu.edu;https://www.amazon.com;https://www.ucdavis.edu",
        "aff_unique_abbr": "CMU;Amazon;UC Davis",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Hindsight Experience Replay",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9281",
        "id": "9281",
        "author_site": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, Wojciech Zaremba",
        "author": "Marcin Andrychowicz; Filip Wolski; Alex Ray; Jonas Schneider; Rachel Fong; Peter Welinder; Bob McGrew; Josh Tobin; OpenAI Pieter Abbeel; Wojciech Zaremba",
        "abstract": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.",
        "bibtex": "@inproceedings{NIPS2017_453fadbd,\n author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hindsight Experience Replay},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Reviews.html",
        "metareview": "",
        "pdf_size": 2040774,
        "gs_citation": 3290,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14733084267697271284&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "OpenAI; OpenAI; OpenAI; OpenAI; OpenAI; OpenAI; OpenAI; OpenAI; OpenAI\u2020; OpenAI\u2020",
        "aff_domain": "openai.com; ; ; ; ; ; ; ; ; ",
        "email": "openai.com; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://goo.gl/SMrQnI",
        "author_num": 10,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0;1;1",
        "aff_unique_norm": "OpenAI;OpenAI\u2020",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://openai.com;",
        "aff_unique_abbr": "OpenAI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9464",
        "id": "9464",
        "author_site": "Moustapha Cisse, Yossi Adi, Natalia Neverova, Joseph Keshet",
        "author": "Moustapha M Cisse; Yossi Adi; Natalia Neverova; Joseph Keshet",
        "abstract": "Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.",
        "bibtex": "@inproceedings{NIPS2017_d494020f,\n author = {Cisse, Moustapha M and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d494020ff8ec181ef98ed97ac3f25453-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d494020ff8ec181ef98ed97ac3f25453-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d494020ff8ec181ef98ed97ac3f25453-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d494020ff8ec181ef98ed97ac3f25453-Reviews.html",
        "metareview": "",
        "pdf_size": 1135651,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12819341474220200650&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Facebook AI Research; Bar-Ilan University, Israel + Facebook AI Research; Facebook AI Research; Bar-Ilan University, Israel",
        "aff_domain": "fb.com;gmail.com;fb.com;cs.biu.ac.il",
        "email": "fb.com;gmail.com;fb.com;cs.biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d494020ff8ec181ef98ed97ac3f25453-Abstract.html",
        "aff_unique_index": "0;1+0;0;1",
        "aff_unique_norm": "Facebook;Bar-Ilan University",
        "aff_unique_dep": "Facebook AI Research;",
        "aff_unique_url": "https://research.facebook.com;https://www.biu.ac.il",
        "aff_unique_abbr": "FAIR;BIU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "How regularization affects the critical points in linear networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9037",
        "id": "9037",
        "author_site": "Amirhossein Taghvaei, Jin W Kim, Prashant Mehta",
        "author": "Amirhossein Taghvaei; Jin W Kim; Prashant Mehta",
        "abstract": "This paper is concerned with the problem of representing and learning a linear transformation using a linear neural network.  In recent years, there is a growing interest in the study of such networks, in part due to the successes of deep learning.  The main question of this body of research (and also of our paper) is related to the existence and optimality properties of the critical points of the mean-squared loss function.  An additional primary concern of our paper pertains to the robustness of these critical points in the face of (a small amount of) regularization.  An optimal control model is introduced for this purpose and a learning algorithm (backprop with weight decay) derived for the same using the Hamilton's formulation of optimal control.  The formulation is used to provide a complete characterization of the critical points in terms of the solutions of a nonlinear matrix-valued equation, referred to as the characteristic equation.  Analytical and numerical tools from bifurcation theory are used to compute the critical points via the solutions of the characteristic equation.",
        "bibtex": "@inproceedings{NIPS2017_1abb1e1e,\n author = {Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How regularization affects the critical points in linear networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1abb1e1ea5f481b589da52303b091cbb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1abb1e1ea5f481b589da52303b091cbb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1abb1e1ea5f481b589da52303b091cbb-Reviews.html",
        "metareview": "",
        "pdf_size": 512593,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13939539897800533023&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Coordinated Science Laboratory",
        "aff_unique_url": "https://www illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8807",
        "id": "8807",
        "author_site": "Saurabh Verma, Zhi-Li Zhang",
        "author": "Saurabh Verma; Zhi-Li Zhang",
        "abstract": "For the purpose of learning on graphs, we hunt for a graph feature representation that exhibit certain uniqueness, stability and sparsity properties while also being amenable to fast computation. This leads to the discovery of family of graph spectral distances (denoted as FGSD) and their based graph feature representations, which we prove to possess most of these desired properties. To both evaluate the quality of graph features produced by FGSD and demonstrate their utility, we apply them to the graph classification problem. Through extensive experiments, we show that a simple SVM based classification algorithm, driven with our powerful FGSD based graph features, significantly outperforms all the more sophisticated state-of-art algorithms on the unlabeled node datasets in terms of both accuracy and speed; it also yields very competitive results on the labeled datasets - despite the fact it does not utilize any node label information.",
        "bibtex": "@inproceedings{NIPS2017_d2ddea18,\n author = {Verma, Saurabh and Zhang, Zhi-Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d2ddea18f00665ce8623e36bd4e3c7c5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d2ddea18f00665ce8623e36bd4e3c7c5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d2ddea18f00665ce8623e36bd4e3c7c5-Reviews.html",
        "metareview": "",
        "pdf_size": 908344,
        "gs_citation": 166,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5949277258591641202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Minnesota, Twin Cities; Department of Computer Science, University of Minnesota, Twin Cities",
        "aff_domain": "cs.umn.edu;cs.umn.edu",
        "email": "cs.umn.edu;cs.umn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Twin Cities",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Hybrid Reward Architecture for Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9314",
        "id": "9314",
        "author_site": "Harm Van Seijen, Mehdi Fatemi, Romain Laroche, Joshua Romoff, Tavian Barnes, Jeffrey Tsang",
        "author": "Harm Van Seijen; Mehdi Fatemi; Joshua Romoff; Romain Laroche; Tavian Barnes; Jeffrey Tsang",
        "abstract": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network.  While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",
        "bibtex": "@inproceedings{NIPS2017_1264a061,\n author = {Van Seijen, Harm and Fatemi, Mehdi and Romoff, Joshua and Laroche, Romain and Barnes, Tavian and Tsang, Jeffrey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hybrid Reward Architecture for Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1264a061d82a2edae1574b07249800d6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1264a061d82a2edae1574b07249800d6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1264a061d82a2edae1574b07249800d6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1264a061d82a2edae1574b07249800d6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1264a061d82a2edae1574b07249800d6-Reviews.html",
        "metareview": "",
        "pdf_size": 549885,
        "gs_citation": 331,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15837142664236653199&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Maluuba, Montreal, Canada; Microsoft Maluuba, Montreal, Canada; Microsoft Maluuba, Montreal, Canada + McGill University, Montreal, Canada; Microsoft Maluuba, Montreal, Canada; Microsoft Maluuba, Montreal, Canada; Microsoft Maluuba, Montreal, Canada",
        "aff_domain": "microsoft.com;microsoft.com;mail.mcgill.ca;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;mail.mcgill.ca;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1264a061d82a2edae1574b07249800d6-Abstract.html",
        "aff_unique_index": "0;0;0+1;0;0;0",
        "aff_unique_norm": "Microsoft;McGill University",
        "aff_unique_dep": "Microsoft Maluuba;",
        "aff_unique_url": "https://www.microsoft.com;https://www.mcgill.ca",
        "aff_unique_abbr": "Microsoft;McGill",
        "aff_campus_unique_index": "0;0;0+0;0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Hypothesis Transfer Learning via Transformation Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8853",
        "id": "8853",
        "author_site": "Simon Du, Jayanth Koushik, Aarti Singh, Barnabas Poczos",
        "author": "Simon S Du; Jayanth Koushik; Aarti Singh; Barnabas Poczos",
        "abstract": "We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation functions, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. We accompany this framework with an analysis of cross-validation for HTL to search for the best transfer technique and gracefully reduce to non-transfer learning when HTL is not helpful. Experiments on robotics and neural imaging data demonstrate the effectiveness of our framework.",
        "bibtex": "@inproceedings{NIPS2017_352fe25d,\n author = {Du, Simon S and Koushik, Jayanth and Singh, Aarti and Poczos, Barnabas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hypothesis Transfer Learning via Transformation Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/352fe25daf686bdb4edca223c921acea-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/352fe25daf686bdb4edca223c921acea-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/352fe25daf686bdb4edca223c921acea-Reviews.html",
        "metareview": "",
        "pdf_size": 576274,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9349264185707241797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cmu.edu;cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cmu.edu;cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/352fe25daf686bdb4edca223c921acea-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Identification of Gaussian Process State Space Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9306",
        "id": "9306",
        "author_site": "Stefanos Eleftheriadis, Tom Nicholson, Marc Deisenroth, James Hensman",
        "author": "Stefanos Eleftheriadis; Tom Nicholson; Marc Deisenroth; James Hensman",
        "abstract": "The Gaussian process state space model (GPSSM) is a non-linear dynamical system, where unknown transition and/or measurement mappings are described by GPs. Most research in GPSSMs has focussed on the state estimation problem, i.e., computing a posterior of the latent state given the model. However, the key challenge in GPSSMs has not been satisfactorily addressed yet: system identification, i.e., learning the model. To address this challenge, we impose a structured Gaussian variational posterior distribution over the latent states, which is parameterised by a recognition model in the form of a bi-directional recurrent neural network. Inference with this structure allows us to recover a posterior smoothed over sequences of data. We provide a practical algorithm for efficiently computing a lower bound on the marginal likelihood using the reparameterisation trick. This further allows for the use of arbitrary kernels within the GPSSM. We demonstrate that the learnt GPSSM can efficiently generate plausible future trajectories of the identified system after only observing a small number of episodes from the true system.",
        "bibtex": "@inproceedings{NIPS2017_1006ff12,\n author = {Eleftheriadis, Stefanos and Nicholson, Tom and Deisenroth, Marc and Hensman, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identification of Gaussian Process State Space Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1006ff12c465532f8c574aeaa4461b16-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1006ff12c465532f8c574aeaa4461b16-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1006ff12c465532f8c574aeaa4461b16-Reviews.html",
        "metareview": "",
        "pdf_size": 2094246,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9340840003303506814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "PROWLER.io; PROWLER.io; PROWLER.io+Imperial College London; PROWLER.io",
        "aff_domain": "prowler.io;prowler.io;prowler.io;prowler.io",
        "email": "prowler.io;prowler.io;prowler.io;prowler.io",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "PROWLER.io;Imperial College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://prowler.io;https://www.imperial.ac.uk",
        "aff_unique_abbr": "PROWLER.io;ICL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Identifying Outlier Arms in Multi-Armed Bandit",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9296",
        "id": "9296",
        "author_site": "Honglei Zhuang, Chi Wang, Yifan Wang",
        "author": "Honglei Zhuang; Chi Wang; Yifan Wang",
        "abstract": "We study a novel problem lying at the intersection of two areas: multi-armed bandit and outlier detection. Multi-armed bandit is a useful tool to model the process of incrementally collecting data for multiple objects in a decision space. Outlier detection is a powerful method to narrow down the attention to a few objects after the data for them are collected. However, no one has studied how to detect outlier objects while incrementally collecting data for them, which is necessary when data collection is expensive. We formalize this problem as identifying outlier arms in a multi-armed bandit. We propose two sampling strategies with theoretical guarantee, and analyze their sampling efficiency. Our experimental results on both synthetic and real data show that our solution saves 70-99% of data collection cost from baseline while having nearly perfect accuracy.",
        "bibtex": "@inproceedings{NIPS2017_dcda54e2,\n author = {Zhuang, Honglei and Wang, Chi and Wang, Yifan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifying Outlier Arms in Multi-Armed Bandit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dcda54e29207294d8e7e1b537338b1c0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dcda54e29207294d8e7e1b537338b1c0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dcda54e29207294d8e7e1b537338b1c0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dcda54e29207294d8e7e1b537338b1c0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dcda54e29207294d8e7e1b537338b1c0-Reviews.html",
        "metareview": "",
        "pdf_size": 624975,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9657197011775382485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois at Urbana-Champaign; Microsoft Research, Redmond; Tsinghua University",
        "aff_domain": "illinois.edu;microsoft.com;mails.tsinghua.edu.cn",
        "email": "illinois.edu;microsoft.com;mails.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dcda54e29207294d8e7e1b537338b1c0-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Microsoft Research;Tsinghua University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://illinois.edu;https://www.microsoft.com/en-us/research;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UIUC;MSR;THU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Urbana-Champaign;Redmond;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9343",
        "id": "9343",
        "author_site": "S\u00e9bastien Racani\u00e8re, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri\u00e0 Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra",
        "author": "S\u00e9bastien Racani\u00e8re; Theophane Weber; David Reichert; Lars Buesing; Arthur Guez; Danilo Jimenez Rezende; Adri\u00e0 Puigdom\u00e8nech Badia; Oriol Vinyals; Nicolas Heess; Yujia Li; Razvan Pascanu; Peter Battaglia; Demis Hassabis; David Silver; Daan Wierstra",
        "abstract": "We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects.   In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a trained environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several strong baselines.",
        "bibtex": "@inproceedings{NIPS2017_9e82757e,\n author = {Racani\\`{e}re, S\\'{e}bastien and Weber, Theophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdom\\`{e}nech Badia, Adri\\`{a} and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Imagination-Augmented Agents for Deep Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Reviews.html",
        "metareview": "",
        "pdf_size": 627158,
        "gs_citation": 763,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13959845767068252955&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com; ; ; ; ; ; ; ; ; ; ; ; ",
        "email": "google.com;google.com;google.com; ; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 15,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Implicit Regularization in Matrix Factorization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9386",
        "id": "9386",
        "author_site": "Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, Nati Srebro",
        "author": "Suriya Gunasekar; Blake E Woodworth; Srinadh Bhojanapalli; Behnam Neyshabur; Nati Srebro",
        "abstract": "We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix $X$ with gradient descent on a factorization of X.  We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.",
        "bibtex": "@inproceedings{NIPS2017_58191d2a,\n author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Implicit Regularization in Matrix Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Reviews.html",
        "metareview": "",
        "pdf_size": 517361,
        "gs_citation": 602,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9586763707057704617&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "TTI at Chicago; TTI at Chicago; TTI at Chicago; TTI at Chicago; TTI at Chicago",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu;ttic.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/58191d2a914c6dae66371c9dcdc91b41-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "TTI at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Improved Dynamic Regret for Non-degenerate Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8868",
        "id": "8868",
        "author_site": "Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, Zhi-Hua Zhou",
        "author": "Lijun Zhang; Tianbao Yang; Jinfeng Yi; Rong Jin; Zhi-Hua Zhou",
        "abstract": "Recently, there has been a growing research interest in the analysis of dynamic regret, which measures the performance of an online learner against a sequence of local minimizers. By exploiting the strong convexity, previous studies have shown that the dynamic regret can be upper bounded by the path-length of the comparator sequence. In this paper, we illustrate that the dynamic regret can be further improved by allowing the learner to query the gradient of the function multiple times, and meanwhile the strong convexity can be weakened to other non-degenerate conditions. Specifically, we introduce the squared path-length, which could be much smaller than the path-length, as a new regularity of the comparator sequence. When multiple gradients are accessible to the learner, we first demonstrate that the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. We then extend our theoretical guarantee to functions that are semi-strongly convex or self-concordant. To the best of our knowledge, this is the first time that semi-strong convexity and self-concordance are utilized to tighten the dynamic regret.",
        "bibtex": "@inproceedings{NIPS2017_cfee3986,\n author = {Zhang, Lijun and Yang, Tianbao and Yi, Jinfeng and Jin, Rong and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improved Dynamic Regret for Non-degenerate Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cfee398643cbc3dc5eefc89334cacdc1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cfee398643cbc3dc5eefc89334cacdc1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cfee398643cbc3dc5eefc89334cacdc1-Reviews.html",
        "metareview": "",
        "pdf_size": 181856,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8874417129457000216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, The University of Iowa, Iowa City, USA; AI Foundations Lab, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Alibaba Group, Seattle, USA; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",
        "aff_domain": "lamda.nju.edu.cn;uiowa.edu;tencent.com;alibaba-inc.com;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;uiowa.edu;tencent.com;alibaba-inc.com;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Nanjing University;The University of Iowa;AI Foundations Lab, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA;Alibaba Group",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Department of Computer Science;;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.uiowa.edu;;https://www.alibaba.com",
        "aff_unique_abbr": "Nanjing U;UIowa;;Alibaba",
        "aff_campus_unique_index": "0;1;3;0",
        "aff_campus_unique": "Nanjing;Iowa City;;Seattle",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "China;United States;"
    },
    {
        "title": "Improved Graph Laplacian via Geometric Self-Consistency",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9223",
        "id": "9223",
        "author_site": "Dominique Perrault-Joncas, Marina Meila, James McQueen",
        "author": "Dominique Joncas; Marina Meila; James McQueen",
        "abstract": "We address the problem of setting the kernel bandwidth, epps, used by Manifold Learning algorithms to construct the graph Laplacian. Exploiting the connection between manifold geometry, represented by the Riemannian metric, and the Laplace-Beltrami operator, we set epps by optimizing the Laplacian's ability to preserve the geometry of the data. Experiments show that this principled approach is effective and robust",
        "bibtex": "@inproceedings{NIPS2017_619205da,\n author = {Joncas, Dominique and Meila, Marina and McQueen, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improved Graph Laplacian via Geometric Self-Consistency},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/619205da514e83f869515c782a328d3c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/619205da514e83f869515c782a328d3c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/619205da514e83f869515c782a328d3c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/619205da514e83f869515c782a328d3c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/619205da514e83f869515c782a328d3c-Reviews.html",
        "metareview": "",
        "pdf_size": 584167,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10837327838312430405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Google, Inc.; Department of Statistics, University of Washington; Amazon",
        "aff_domain": "google.com;uw.edu;amazon.com",
        "email": "google.com;uw.edu;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/619205da514e83f869515c782a328d3c-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;University of Washington;Amazon.com, Inc.",
        "aff_unique_dep": ";Department of Statistics;",
        "aff_unique_url": "https://www.google.com;https://www.washington.edu;https://www.amazon.com",
        "aff_unique_abbr": "Google;UW;Amazon",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Seattle;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improved Training of Wasserstein GANs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9350",
        "id": "9350",
        "author_site": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",
        "author": "Ishaan Gulrajani; Faruk Ahmed; Martin Arjovsky; Vincent Dumoulin; Aaron C. Courville",
        "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
        "bibtex": "@inproceedings{NIPS2017_892c3b1c,\n author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improved Training of Wasserstein GANs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Reviews.html",
        "metareview": "",
        "pdf_size": 3472880,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Montreal Institute for Learning Algorithms+Google Brain; Montreal Institute for Learning Algorithms; Courant Institute of Mathematical Sciences; Montreal Institute for Learning Algorithms; Montreal Institute for Learning Algorithms+CIFAR Fellow",
        "aff_domain": "gmail.com;umontreal.ca;nyu.edu;umontreal.ca;umontreal.ca",
        "email": "gmail.com;umontreal.ca;nyu.edu;umontreal.ca;umontreal.ca",
        "github": "https://github.com/igul222/improved_wgan_training",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html",
        "aff_unique_index": "0+1;0;2;0;0+3",
        "aff_unique_norm": "Montreal Institute for Learning Algorithms;Google;Courant Institute of Mathematical Sciences;CIFAR",
        "aff_unique_dep": "Artificial Intelligence;Google Brain;Mathematical Sciences;",
        "aff_unique_url": "https://mila.quebec;https://brain.google.com;https://cims.nyu.edu;https://www.cifar.ca",
        "aff_unique_abbr": "MILA;Google Brain;CIMS;CIFAR",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Montreal;Mountain View;",
        "aff_country_unique_index": "0+1;0;1;0;0+0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8909",
        "id": "8909",
        "author_site": "Qinshi Wang, Wei Chen",
        "author": "Qinshi Wang; Wei Chen",
        "abstract": "We study combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of 1/p",
        "bibtex": "@inproceedings{NIPS2017_a8e864d0,\n author = {Wang, Qinshi and Chen, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a8e864d04c95572d1aece099af852d0a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a8e864d04c95572d1aece099af852d0a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a8e864d04c95572d1aece099af852d0a-Reviews.html",
        "metareview": "",
        "pdf_size": 349558,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9608064996276341727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Princeton University; Microsoft Research",
        "aff_domain": "princeton.edu;microsoft.com",
        "email": "princeton.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Princeton;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improving the Expected Improvement Algorithm",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9313",
        "id": "9313",
        "author_site": "Chao Qin, Diego Klabjan, Daniel Russo",
        "author": "Chao Qin; Diego Klabjan; Daniel Russo",
        "abstract": "The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.",
        "bibtex": "@inproceedings{NIPS2017_b19aa25f,\n author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improving the Expected Improvement Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b19aa25ff58940d974234b48391b9549-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b19aa25ff58940d974234b48391b9549-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b19aa25ff58940d974234b48391b9549-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b19aa25ff58940d974234b48391b9549-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b19aa25ff58940d974234b48391b9549-Reviews.html",
        "metareview": "",
        "pdf_size": 307967,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2008346532887770998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Columbia Business School; Northwestern University; Columbia Business School",
        "aff_domain": "gsb.columbia.edu;northwestern.edu;gsb.columbia.edu",
        "email": "gsb.columbia.edu;northwestern.edu;gsb.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b19aa25ff58940d974234b48391b9549-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Columbia University;Northwestern University",
        "aff_unique_dep": "Business School;",
        "aff_unique_url": "https://www8.gsb.columbia.edu;https://www.northwestern.edu",
        "aff_unique_abbr": "CBS;NU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Incorporating Side Information by Adaptive Convolution",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9168",
        "id": "9168",
        "author_site": "Di Kang, Debarun Dhar, Antoni Chan",
        "author": "Di Kang; Debarun Dhar; Antoni Chan",
        "abstract": "Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in counting systems based on deep learning. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolution filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold within the high-dimensional space of filter weights. The filter weights are generated using a learned ``filter manifold'' sub-network, whose input is the side information. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context (e.g. camera perspective, noise level, blur kernel parameters). We demonstrate the effectiveness of  ACNN incorporating side information on 3 tasks: crowd counting, corrupted digit recognition, and image deblurring. Our experiments show that ACNN improves the performance compared to a plain CNN with a similar number of parameters. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information.",
        "bibtex": "@inproceedings{NIPS2017_e7e23670,\n author = {Kang, Di and Dhar, Debarun and Chan, Antoni},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Incorporating Side Information by Adaptive Convolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Reviews.html",
        "metareview": "",
        "pdf_size": 3942502,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15043857276285903357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong",
        "aff_domain": "my.cityu.edu.hk;my.cityu.edu.hk;cityu.edu.hk",
        "email": "my.cityu.edu.hk;my.cityu.edu.hk;cityu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e7e23670481ac78b3c4122a99ba60573-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "City University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cityu.edu.hk",
        "aff_unique_abbr": "CityU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Independence clustering (without a matrix)",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9182",
        "id": "9182",
        "author": "Daniil Ryabko",
        "abstract": "The  independence clustering problem is considered in the following formulation: given a set $S$ of random variables,  it is required to find the finest partitioning $\\{U_1,\\dots,U_k\\}$ of  $S$ into clusters  such that the clusters $U_1,\\dots,U_k$ are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable.   The distribution of the random variables in $S$ is, in general, unknown, but a sample  is available.  Thus, the problem is cast in terms of time series.  Two forms of sampling are considered: i.i.d.\\ and stationary  time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of fascinating open directions for further research are outlined.",
        "bibtex": "@inproceedings{NIPS2017_37d097ca,\n author = {Ryabko, Daniil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Independence clustering (without a matrix)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/37d097caf1299d9aa79c2c2b843d2d78-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/37d097caf1299d9aa79c2c2b843d2d78-Reviews.html",
        "metareview": "",
        "pdf_size": 317108,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7159308677360078742&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "INRIA Lillle",
        "aff_domain": "ryabko.net",
        "email": "ryabko.net",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/37d097caf1299d9aa79c2c2b843d2d78-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "INRIA Lillle",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "title": "Inductive Representation Learning on Large Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8896",
        "id": "8896",
        "author_site": "Will Hamilton, Zhitao Ying, Jure Leskovec",
        "author": "Will Hamilton; Zhitao Ying; Jure Leskovec",
        "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings.  Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
        "bibtex": "@inproceedings{NIPS2017_5dd9db5e,\n author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inductive Representation Learning on Large Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Reviews.html",
        "metareview": "",
        "pdf_size": 923482,
        "gs_citation": 20519,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10802896480404413344&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Inference in Graphical Models via Semidefinite Programming Hierarchies",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8838",
        "id": "8838",
        "author_site": "Murat Erdogdu, Yash Deshpande, Andrea Montanari",
        "author": "Murat A Erdogdu; Yash Deshpande; Andrea Montanari",
        "abstract": "Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured  combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation  within the Sherali-Adams hierarchy. Despite the popularity of these algorithms,  it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with $n$ vertices require solving an SDP with $n^{\\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In practice, for $d\\ge 4$, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation.",
        "bibtex": "@inproceedings{NIPS2017_8d3bba74,\n author = {Erdogdu, Murat A and Deshpande, Yash and Montanari, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inference in Graphical Models via Semidefinite Programming Hierarchies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8d3bba7425e7c98c50f52ca1b52d3735-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8d3bba7425e7c98c50f52ca1b52d3735-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8d3bba7425e7c98c50f52ca1b52d3735-Reviews.html",
        "metareview": "",
        "pdf_size": 1719974,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4589942741424099742&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Microsoft Research; MIT and Microsoft Research; Stanford University",
        "aff_domain": "cs.toronto.edu;mit.edu;stanford.edu",
        "email": "cs.toronto.edu;mit.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft Corporation;MIT and Microsoft Research;Stanford University",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;;https://www.stanford.edu",
        "aff_unique_abbr": "MSR;;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Inferring Generative Model Structure with Static Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8821",
        "id": "8821",
        "author_site": "Paroma Varma, Bryan He, Payal Bajaj, Nishith Khandwala, Imon Banerjee, Daniel Rubin, Christopher R\u00e9",
        "author": "Paroma Varma; Bryan D He; Payal Bajaj; Nishith Khandwala; Imon Banerjee; Daniel Rubin; Christopher R\u00e9",
        "abstract": "Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects the quality of the training labels, but is difficult to learn without any ground truth labels. We instead rely on weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus significantly reducing the amount of data required to learn structure. We prove that Coral's sample complexity scales quasilinearly with the number of heuristics and number of relations identified, improving over the standard sample complexity, which is exponential in n for learning n-th degree relations. Empirically, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.",
        "bibtex": "@inproceedings{NIPS2017_cedebb6e,\n author = {Varma, Paroma and He, Bryan D and Bajaj, Payal and Khandwala, Nishith and Banerjee, Imon and Rubin, Daniel and R\\'{e}, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring Generative Model Structure with Static Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cedebb6e872f539bef8c3f919874e9d7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cedebb6e872f539bef8c3f919874e9d7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cedebb6e872f539bef8c3f919874e9d7-Reviews.html",
        "metareview": "",
        "pdf_size": 834946,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9204073630384117344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Electrical Engineering; Computer Science; Computer Science; Computer Science; Biomedical Data Science; Biomedical Data Science+Radiology; Computer Science",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html",
        "aff_unique_index": "0;1;1;1;2;2+3;1",
        "aff_unique_norm": "Electrical Engineering Department;Computer Science;Biomedical Data Science;Radiology",
        "aff_unique_dep": "Electrical Engineering;Computer Science Department;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Influence Maximization with $\\varepsilon$-Almost Submodular Threshold Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9162",
        "id": "9162",
        "author_site": "Qiang Li, Wei Chen, Institute of Computing Xiaoming Sun, Institute of Computing Jialin Zhang",
        "author": "Qiang Li; Wei Chen; Institute of Computing Xiaoming Sun; Institute of Computing Jialin Zhang",
        "abstract": "Influence maximization is the problem of selecting $k$ nodes in a social network to maximize their influence spread. The problem has been extensively studied but most works focus on the submodular influence diffusion models. In this paper, motivated by empirical evidences, we explore influence maximization in the non-submodular regime. In particular, we study the general threshold model in which a fraction of nodes have non-submodular threshold functions, but their threshold functions are closely upper- and lower-bounded by some submodular functions (we call them $\\varepsilon$-almost submodular). We first show a strong hardness result: there is no $1/n^{\\gamma/c}$ approximation for influence maximization (unless P = NP) for all networks with up to $n^{\\gamma}$ $\\varepsilon$-almost submodular nodes, where $\\gamma$ is in (0,1) and $c$ is a parameter depending on $\\varepsilon$. This indicates that influence maximization is still hard to approximate even though threshold functions are close to submodular. We then provide $(1-\\varepsilon)^{\\ell}(1-1/e)$ approximation algorithms when the number of $\\varepsilon$-almost submodular nodes is $\\ell$. Finally, we conduct experiments on a number of real-world datasets, and the results demonstrate that our approximation algorithms outperform other baseline algorithms.",
        "bibtex": "@inproceedings{NIPS2017_cf2226dd,\n author = {Li, Qiang and Chen, Wei and Xiaoming Sun, Institute of Computing and Jialin Zhang, Institute of Computing},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Influence Maximization with \\textbackslash varepsilon-Almost Submodular Threshold Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cf2226ddd41b1a2d0ae51dab54d32c36-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cf2226ddd41b1a2d0ae51dab54d32c36-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cf2226ddd41b1a2d0ae51dab54d32c36-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cf2226ddd41b1a2d0ae51dab54d32c36-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cf2226ddd41b1a2d0ae51dab54d32c36-Reviews.html",
        "metareview": "",
        "pdf_size": 565703,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17833834846317943857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Microsoft Research; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;microsoft.com;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;microsoft.com;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cf2226ddd41b1a2d0ae51dab54d32c36-Abstract.html",
        "aff_unique_index": "0+1;2;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Microsoft Corporation",
        "aff_unique_dep": "Institute of Computing Technology;;Microsoft Research",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CAS;UCAS;MSR",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9163",
        "id": "9163",
        "author_site": "Yunzhu Li, Jiaming Song, Stefano Ermon",
        "author": "Yunzhu Li; Jiaming Song; Stefano Ermon",
        "abstract": "The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",
        "bibtex": "@inproceedings{NIPS2017_2cd4e8a2,\n author = {Li, Yunzhu and Song, Jiaming and Ermon, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Reviews.html",
        "metareview": "",
        "pdf_size": 3501053,
        "gs_citation": 481,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15461722063571103922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "MIT; Stanford University; Stanford University",
        "aff_domain": "mit.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "mit.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2cd4e8a2ce081c3d7c32c3cde4312ef7-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.stanford.edu",
        "aff_unique_abbr": "MIT;Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9033",
        "id": "9033",
        "author_site": "Linus Hamilton, Frederic Koehler, Ankur Moitra",
        "author": "Linus Hamilton; Frederic Koehler; Ankur Moitra",
        "abstract": "Markov random fields are a popular model for high-dimensional probability distributions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoherence assumptions. Bresler gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models.   Here we take a more conceptual approach to proving lower bounds on the mutual information. Our proof generalizes well beyond Ising models, to arbitrary Markov random fields with higher order interactions. As an application, we obtain algorithms for learning Markov random fields on bounded degree graphs on $n$ nodes with $r$-order interactions in $n^r$ time and $\\log n$ sample complexity. Our algorithms also extend to various partial observation models.",
        "bibtex": "@inproceedings{NIPS2017_8fb5f8be,\n author = {Hamilton, Linus and Koehler, Frederic and Moitra, Ankur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Reviews.html",
        "metareview": "",
        "pdf_size": 323652,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8007677950177363910&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Massachusetts Institute of Technology. Department of Mathematics; Massachusetts Institute of Technology. Department of Mathematics; Massachusetts Institute of Technology. Department of Mathematics and the Computer Science and Artificial Intelligence Lab",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8fb5f8be2aa9d6c64a04e3ab9f63feee-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Information-theoretic analysis of generalization capability of learning algorithms",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9039",
        "id": "9039",
        "author_site": "Aolin Xu, Maxim Raginsky",
        "author": "Aolin Xu; Maxim Raginsky",
        "abstract": "We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information.  We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.",
        "bibtex": "@inproceedings{NIPS2017_ad71c82b,\n author = {Xu, Aolin and Raginsky, Maxim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Information-theoretic analysis of generalization capability of learning algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Reviews.html",
        "metareview": "",
        "pdf_size": 510021,
        "gs_citation": 517,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7269121981215267730&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801, USA; Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801, USA",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801, USA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Inhomogeneous Hypergraph Clustering with Applications",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9018",
        "id": "9018",
        "author_site": "Pan Li, Olgica Milenkovic",
        "author": "Pan Li; Olgica Milenkovic",
        "abstract": "Hypergraph partitioning is an important problem in machine learning, computer vision and network analytics. A widely used method for hypergraph partitioning relies on minimizing a normalized sum of the costs of partitioning hyperedges across clusters. Algorithmic solutions based on this approach assume that different partitions of a hyperedge incur the same cost. However, this assumption fails to leverage the fact that different subsets of vertices within the same hyperedge may have different structural importance. We hence propose a new hypergraph clustering technique, termed inhomogeneous hypergraph partitioning, which assigns different costs to different hyperedge cuts. We prove that inhomogeneous partitioning produces a quadratic approximation to the optimal solution if the inhomogeneous costs satisfy submodularity constraints. Moreover, we demonstrate that inhomogenous partitioning offers significant performance improvements in applications such as structure learning of rankings, subspace segmentation and motif clustering.",
        "bibtex": "@inproceedings{NIPS2017_a50abba8,\n author = {Li, Pan and Milenkovic, Olgica},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inhomogeneous Hypergraph Clustering with Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a50abba8132a77191791390c3eb19fe7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a50abba8132a77191791390c3eb19fe7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a50abba8132a77191791390c3eb19fe7-Reviews.html",
        "metareview": "",
        "pdf_size": 1253020,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6318640772864900578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department ECE, UIUC; Department ECE, UIUC",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department ECE, UIUC",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Inhomogoenous Hypergraph Clustering with Applications",
        "author": "Pan Li, Olgica Milenkovic",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/9994",
        "id": "9994"
    },
    {
        "title": "Integration Methods and Optimization Algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8904",
        "id": "8904",
        "author_site": "Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d'Aspremont",
        "author": "Damien Scieur; Vincent Roulet; Francis Bach; Alexandre d'Aspremont",
        "abstract": "We show that accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. Compared with recent advances in this vein, the differential equation considered here is the basic gradient flow, and we derive a class of multi-step schemes which includes accelerated algorithms, using classical conditions from numerical analysis. Multi-step schemes integrate the differential equation using larger step sizes, which intuitively explains the acceleration phenomenon.",
        "bibtex": "@inproceedings{NIPS2017_bf62768c,\n author = {Scieur, Damien and Roulet, Vincent and Bach, Francis and d\\textquotesingle Aspremont, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Integration Methods and Optimization Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/bf62768ca46b6c3b5bea9515d1a1fc45-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bf62768ca46b6c3b5bea9515d1a1fc45-Reviews.html",
        "metareview": "",
        "pdf_size": 304153,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10015563318196948836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "INRIA, ENS, PSL Research University, Paris France; INRIA, ENS, PSL Research University, Paris France; INRIA, ENS, PSL Research University, Paris France; CNRS, ENS, PSL Research University, Paris France",
        "aff_domain": "inria.fr;inria.fr;inria.fr;ens.fr",
        "email": "inria.fr;inria.fr;inria.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "INRIA, ENS, PSL Research University, Paris France;CNRS, ENS, PSL Research University, Paris France",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Interactive Submodular Bandit",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8812",
        "id": "8812",
        "author_site": "Lin Chen, Andreas Krause, Amin Karbasi",
        "author": "Lin Chen; Andreas Krause; Amin Karbasi",
        "abstract": "In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible. In real life situations, however, the utility function is not fully known in advance and can only be estimated via interactions. For instance, whether a user likes a movie or not can be reliably evaluated only after it was shown to her. Or, the range of influence of a user in a social network can be estimated only after she is selected to advertise the product. We model such problems as an interactive submodular bandit optimization, where in each round we receive a context (e.g., previously selected movies) and have to choose an action (e.g., propose a new movie). We then receive a noisy feedback about the utility of the action (e.g., ratings) which we model as a submodular function over the context-action space. We develop SM-UCB that efficiently trades off exploration (collecting more data) and exploration (proposing a good action given gathered data) and achieves a $O(\\sqrt{T})$ regret bound after $T$ rounds of interaction. Given a bounded-RKHS norm kernel over the context-action-payoff space that governs the smoothness of the utility function, SM-UCB keeps an upper-confidence bound on the payoff function that allows it to asymptotically achieve no-regret. Finally, we evaluate our results on four concrete applications, including movie recommendation (on the MovieLense data set), news recommendation (on Yahoo! Webscope dataset), interactive influence maximization (on a subset of the Facebook network), and personalized data summarization (on Reuters Corpus). In all these applications, we observe that SM-UCB consistently outperforms the prior art.",
        "bibtex": "@inproceedings{NIPS2017_f0935e4c,\n author = {Chen, Lin and Krause, Andreas and Karbasi, Amin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Interactive Submodular Bandit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Reviews.html",
        "metareview": "",
        "pdf_size": 659434,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2557675188871966492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering + Yale Institute for Network Science, Yale University; Department of Computer Science, ETH Z\u00fcrich; Department of Electrical Engineering + Yale Institute for Network Science, Yale University",
        "aff_domain": "yale.edu;ethz.ch;yale.edu",
        "email": "yale.edu;ethz.ch;yale.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f0935e4cd5920aa6c7c996a5ee53a70f-Abstract.html",
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Institution not specified;Yale University;ETH Z\u00fcrich",
        "aff_unique_dep": "Department of Electrical Engineering;Yale Institute for Network Science;Department of Computer Science",
        "aff_unique_url": ";https://www.yale.edu;https://www.ethz.ch",
        "aff_unique_abbr": ";Yale;ETHZ",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";New Haven",
        "aff_country_unique_index": "1;2;1",
        "aff_country_unique": ";United States;Switzerland"
    },
    {
        "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9166",
        "id": "9166",
        "author_site": "Shixiang (Shane) Gu, Timothy Lillicrap, Richard Turner, Zoubin Ghahramani, Bernhard Sch\u00f6lkopf, Sergey Levine",
        "author": "Shixiang (Shane) Gu; Timothy Lillicrap; Richard E Turner; Zoubin Ghahramani; Bernhard Sch\u00f6lkopf; Sergey Levine",
        "abstract": "Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning.  Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.",
        "bibtex": "@inproceedings{NIPS2017_a1d7311f,\n author = {Gu, Shixiang (Shane) and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch\\\"{o}lkopf, Bernhard and Levine, Sergey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a1d7311f2a312426d710e1c617fcbc8c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a1d7311f2a312426d710e1c617fcbc8c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a1d7311f2a312426d710e1c617fcbc8c-Reviews.html",
        "metareview": "",
        "pdf_size": 602276,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18031173648112197230&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "University of Cambridge + Max Planck Institute; DeepMind; University of Cambridge + Uber AI Labs; University of Cambridge; Max Planck Institute; UC Berkeley",
        "aff_domain": "cam.ac.uk;google.com;eng.cam.ac.uk;cam.ac.uk;tuebingen.mpg.de;eecs.berkeley.edu",
        "email": "cam.ac.uk;google.com;eng.cam.ac.uk;cam.ac.uk;tuebingen.mpg.de;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a1d7311f2a312426d710e1c617fcbc8c-Abstract.html",
        "aff_unique_index": "0+1;2;0+3;0;1;4",
        "aff_unique_norm": "University of Cambridge;Max Planck Institute;DeepMind;Uber;University of California, Berkeley",
        "aff_unique_dep": ";;;Uber AI Labs;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.mpiwg-berlin.mpg.de;https://deepmind.com;https://www.uber.com;https://www.berkeley.edu",
        "aff_unique_abbr": "Cambridge;MPI;DeepMind;Uber AI Labs;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Cambridge;;Berkeley",
        "aff_country_unique_index": "0+1;0;0+2;0;1;2",
        "aff_country_unique": "United Kingdom;Germany;United States"
    },
    {
        "title": "Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8980",
        "id": "8980",
        "author_site": "Raymond A. Yeh, Jinjun Xiong, Wen-Mei Hwu, Minh Do, Alex Schwing",
        "author": "Raymond Yeh; Jinjun Xiong; Wen-Mei Hwu; Minh Do; Alexander Schwing",
        "abstract": "Textual grounding is an important but challenging task for human-computer inter- action, robotics and knowledge mining. Existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. In this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. Hence, the method is able to consider significantly more proposals and doesn\u2019t rely on a successful first stage hypothesizing bounding box proposals. Beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. Lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77% respectively.",
        "bibtex": "@inproceedings{NIPS2017_52292e0c,\n author = {Yeh, Raymond and Xiong, Jinjun and Hwu, Wen-Mei and Do, Minh and Schwing, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/52292e0c763fd027c6eba6b8f494d2eb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/52292e0c763fd027c6eba6b8f494d2eb-Reviews.html",
        "metareview": "",
        "pdf_size": 6588896,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6308209609622125613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, University of Illinois at Urbana-Champaign; IBM Thomas J. Watson Research Center+Department of Electrical Engineering, University of Illinois at Urbana-Champaign; Department of Electrical Engineering, University of Illinois at Urbana-Champaign; Department of Electrical Engineering, University of Illinois at Urbana-Champaign; Department of Electrical Engineering, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;us.ibm.com;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;us.ibm.com;illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/52292e0c763fd027c6eba6b8f494d2eb-Abstract.html",
        "aff_unique_index": "0;1+0;0;0;0",
        "aff_unique_norm": "Department of Electrical Engineering, University of Illinois at Urbana-Champaign;IBM",
        "aff_unique_dep": ";Research",
        "aff_unique_url": ";https://www.ibm.com/research",
        "aff_unique_abbr": ";IBM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Yorktown Heights",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Introspective Classification with Convolutional Nets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8877",
        "id": "8877",
        "author_site": "Long Jin, Justin Lazarow, Zhuowen Tu",
        "author": "Long Jin; Justin Lazarow; Zhuowen Tu",
        "abstract": "We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.",
        "bibtex": "@inproceedings{NIPS2017_11b921ef,\n author = {Jin, Long and Lazarow, Justin and Tu, Zhuowen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Introspective Classification with Convolutional Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Reviews.html",
        "metareview": "",
        "pdf_size": 823013,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4040878993826950365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "UC San Diego; UC San Diego; UC San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/11b921ef080f7736089c757404650e40-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Invariance and Stability of Deep Convolutional Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9392",
        "id": "9392",
        "author_site": "Alberto Bietti, Julien Mairal",
        "author": "Alberto Bietti; Julien Mairal",
        "abstract": "In this paper, we study deep signal representations that are near-invariant to groups of transformations and stable to the action of diffeomorphisms without losing signal information. This is achieved by generalizing the multilayer kernel introduced in the context of convolutional kernel networks and by studying the geometry of the corresponding reproducing kernel Hilbert space. We show that the signal representation is stable, and that models from this functional space, such as a large class of convolutional neural networks, may enjoy the same stability.",
        "bibtex": "@inproceedings{NIPS2017_38ed162a,\n author = {Bietti, Alberto and Mairal, Julien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Invariance and Stability of Deep Convolutional Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/38ed162a0dbef7b3fe0f628aa08b90e7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/38ed162a0dbef7b3fe0f628aa08b90e7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/38ed162a0dbef7b3fe0f628aa08b90e7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/38ed162a0dbef7b3fe0f628aa08b90e7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/38ed162a0dbef7b3fe0f628aa08b90e7-Reviews.html",
        "metareview": "",
        "pdf_size": 195089,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1512479080454759460&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Inria*; Inria*",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/38ed162a0dbef7b3fe0f628aa08b90e7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Inria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Inverse Filtering for Hidden Markov Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9200",
        "id": "9200",
        "author_site": "Robert Mattila, Cristian Rojas, Vikram Krishnamurthy, Bo Wahlberg",
        "author": "Robert Mattila; Cristian Rojas; Vikram Krishnamurthy; Bo Wahlberg",
        "abstract": "This paper considers a number of related inverse filtering problems for hidden Markov models (HMMs). In particular, given a sequence of state posteriors and the system dynamics; i) estimate the corresponding sequence of observations, ii) estimate the observation likelihoods, and iii) jointly estimate the observation likelihoods and the observation sequence. We show how to avoid a computationally expensive mixed integer linear program (MILP) by exploiting the algebraic structure of the HMM filter using simple linear algebra operations, and provide conditions for when the quantities can be uniquely reconstructed. We also propose a solution to the more general case where the posteriors are noisily observed. Finally, the proposed inverse filtering algorithms are evaluated on real-world polysomnographic data used for automatic sleep segmentation.",
        "bibtex": "@inproceedings{NIPS2017_01894d6f,\n author = {Mattila, Robert and Rojas, Cristian and Krishnamurthy, Vikram and Wahlberg, Bo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inverse Filtering for Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/01894d6f048493d2cacde3c579c315a3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/01894d6f048493d2cacde3c579c315a3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/01894d6f048493d2cacde3c579c315a3-Reviews.html",
        "metareview": "",
        "pdf_size": 333538,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15193515221546064744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Automatic Control, KTH Royal Institute of Technology; Department of Automatic Control, KTH Royal Institute of Technology; Cornell Tech, Cornell University; Department of Automatic Control, KTH Royal Institute of Technology",
        "aff_domain": "kth.se;kth.se;cornell.edu;kth.se",
        "email": "kth.se;kth.se;cornell.edu;kth.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Department of Automatic Control, KTH Royal Institute of Technology;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://cornell.edu",
        "aff_unique_abbr": ";Cornell",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cornell Tech",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Inverse Reward Design",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9444",
        "id": "9444",
        "author_site": "Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, Anca Dragan",
        "author": "Dylan Hadfield-Menell; Smitha Milli; Pieter Abbeel; Stuart Russell; Anca Dragan",
        "abstract": "Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we  want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",
        "bibtex": "@inproceedings{NIPS2017_32fdab65,\n author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inverse Reward Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/32fdab6559cdfa4f167f8c31b9199643-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/32fdab6559cdfa4f167f8c31b9199643-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/32fdab6559cdfa4f167f8c31b9199643-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/32fdab6559cdfa4f167f8c31b9199643-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/32fdab6559cdfa4f167f8c31b9199643-Reviews.html",
        "metareview": "",
        "pdf_size": 860286,
        "gs_citation": 525,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9434034481007357683&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley; OpenAI, International Computer Science Institute (ICSI); Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/32fdab6559cdfa4f167f8c31b9199643-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of California, Berkeley;OpenAI, International Computer Science Institute (ICSI)",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;",
        "aff_unique_url": "https://www.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9221",
        "id": "9221",
        "author_site": "Cameron Musco, David Woodruff",
        "author": "Cameron Musco; David Woodruff",
        "abstract": "Low-rank approximation is a common tool used to accelerate kernel methods: the $n \\times n$ kernel matrix $K$ is approximated via a rank-$k$ matrix $\\tilde K$ which can be stored in much less space and processed more quickly. In this work we study the limits of computationally efficient low-rank kernel approximation. We show that for a broad class of kernels, including the popular Gaussian and polynomial kernels, computing a relative error $k$-rank approximation to $K$ is at least as difficult as multiplying the input data matrix $A \\in R^{n \\times d}$ by an arbitrary matrix $C \\in R^{d \\times k}$. Barring a breakthrough in fast matrix multiplication, when $k$ is not too large, this requires $\\Omega(nnz(A)k)$ time where $nnz(A)$ is the number of non-zeros in $A$. This lower bound matches, in many parameter regimes, recent work on subquadratic time algorithms for low-rank approximation of general kernels [MM16,MW17], demonstrating that these algorithms are unlikely to be significantly improved, in particular to $O(nnz(A))$ input sparsity runtimes. At the same time there is hope: we show for the first time that $O(nnz(A))$ time approximation is possible for general radial basis function kernels (e.g., the Gaussian kernel) for the closely related problem of low-rank approximation of the kernelized dataset.",
        "bibtex": "@inproceedings{NIPS2017_69dafe8b,\n author = {Musco, Cameron and Woodruff, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/69dafe8b58066478aea48f3d0f384820-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/69dafe8b58066478aea48f3d0f384820-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/69dafe8b58066478aea48f3d0f384820-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/69dafe8b58066478aea48f3d0f384820-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/69dafe8b58066478aea48f3d0f384820-Reviews.html",
        "metareview": "",
        "pdf_size": 457685,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1010449421687471804&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "MIT; Carnegie Mellon University",
        "aff_domain": "mit.edu;cs.cmu.edu",
        "email": "mit.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/69dafe8b58066478aea48f3d0f384820-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.cmu.edu",
        "aff_unique_abbr": "MIT;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Is the Bellman residual a bad proxy?",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9105",
        "id": "9105",
        "author_site": "Matthieu Geist, Bilal Piot, Olivier Pietquin",
        "author": "Matthieu Geist; Bilal Piot; Olivier Pietquin",
        "abstract": "This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual $\\|T_* v_\\pi - v_\\pi\\|_{1,\\nu}$ over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.",
        "bibtex": "@inproceedings{NIPS2017_e0ab531e,\n author = {Geist, Matthieu and Piot, Bilal and Pietquin, Olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Is the Bellman residual a bad proxy?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e0ab531ec312161511493b002f9be2ee-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e0ab531ec312161511493b002f9be2ee-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e0ab531ec312161511493b002f9be2ee-Reviews.html",
        "metareview": "",
        "pdf_size": 639071,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16361699271701606518&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Universit\u00e9 de Lorraine & CNRS, LIEC, UMR 7360, Metz, F-57070 France; Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 - CRIStAL, F-59000 Lille, France + Now with Google DeepMind, London, United Kingdom; Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 - CRIStAL, F-59000 Lille, France + Now with Google DeepMind, London, United Kingdom",
        "aff_domain": "univ-lorraine.fr;univ-lille1.fr;univ-lille1.fr",
        "email": "univ-lorraine.fr;univ-lille1.fr;univ-lille1.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e0ab531ec312161511493b002f9be2ee-Abstract.html",
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "Universit\u00e9 de Lorraine & CNRS, LIEC, UMR 7360, Metz, F-57070 France;University of Lille;Now with Google DeepMind, London, United Kingdom",
        "aff_unique_dep": ";UMR 9189 - CRIStAL;",
        "aff_unique_url": ";https://www.univ-lille.fr;",
        "aff_unique_abbr": ";Univ. Lille;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";France"
    },
    {
        "title": "Joint distribution optimal transportation for domain adaptation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9155",
        "id": "9155",
        "author_site": "Nicolas Courty, R\u00e9mi Flamary, Amaury Habrard, Alain Rakotomamonjy",
        "author": "Nicolas Courty; R\u00e9mi Flamary; Amaury Habrard; Alain Rakotomamonjy",
        "abstract": "This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a  non-linear transformation between the joint feature/label space distributions of the two domain $\\ps$ and $\\pt$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target $\\pt^f=(X,f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.",
        "bibtex": "@inproceedings{NIPS2017_0070d23b,\n author = {Courty, Nicolas and Flamary, R\\'{e}mi and Habrard, Amaury and Rakotomamonjy, Alain},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Joint distribution optimal transportation for domain adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0070d23b06b1486a538c0eaa45dd167a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0070d23b06b1486a538c0eaa45dd167a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0070d23b06b1486a538c0eaa45dd167a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0070d23b06b1486a538c0eaa45dd167a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0070d23b06b1486a538c0eaa45dd167a-Reviews.html",
        "metareview": "",
        "pdf_size": 460751,
        "gs_citation": 700,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3838459407382849555&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Universit\u00e9 de Bretagne Sud, IRISA, UMR 6074, CNRS; Universit\u00e9 C\u00f4te d\u2019Azur, Lagrange, UMR 7293, CNRS, OCA; Univ Lyon, UJM-Saint-Etienne, CNRS, Lab. Hubert Curien UMR 5516, F-42023; Normandie Universite, Universit\u00e9 de Rouen, LITIS EA 4108",
        "aff_domain": "univ-ubs.fr;unice.fr;univ-st-etienne.fr;insa-rouen.fr",
        "email": "univ-ubs.fr;unice.fr;univ-st-etienne.fr;insa-rouen.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0070d23b06b1486a538c0eaa45dd167a-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Universit\u00e9 de Bretagne Sud, IRISA, UMR 6074, CNRS;Universit\u00e9 C\u00f4te d\u2019Azur, Lagrange, UMR 7293, CNRS, OCA;Univ Lyon, UJM-Saint-Etienne, CNRS, Lab. Hubert Curien UMR 5516, F-42023;Normandie Universite, Universit\u00e9 de Rouen, LITIS EA 4108",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "K-Medoids For K-Means Seeding",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9295",
        "id": "9295",
        "author_site": "James Newling, Fran\u00e7ois Fleuret",
        "author": "James Newling; Fran\u00e7ois Fleuret",
        "abstract": "We show experimentally that the algorithm CLARANS of Ng and Han (1994) finds better K-medoids solutions than the Voronoi iteration algorithm of Hastie et al. (2001). This finding, along with the similarity between the Voronoi iteration algorithm and Lloyd's K-means algorithm, motivates us to use CLARANS as a K-means initializer. We show that CLARANS outperforms other algorithms on 23/23 datasets with a mean decrease over k-means++ of 30% for initialization mean squared error (MSE) and 3% for final MSE. We introduce algorithmic improvements to CLARANS which improve its complexity and runtime, making it a viable initialization scheme for large datasets.",
        "bibtex": "@inproceedings{NIPS2017_a8345c3b,\n author = {Newling, James and Fleuret, Fran\\c{c}ois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {K-Medoids For K-Means Seeding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a8345c3bb9e3896ea538ce77ffaf2c20-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a8345c3bb9e3896ea538ce77ffaf2c20-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a8345c3bb9e3896ea538ce77ffaf2c20-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a8345c3bb9e3896ea538ce77ffaf2c20-Reviews.html",
        "metareview": "",
        "pdf_size": 2056062,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4660181003556663567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Idiap Research Institue + \u00b4Ecole polytechnique f\u00b4ed\u00b4erale de Lausanne; Idiap Research Institue + \u00b4Ecole polytechnique f\u00b4ed\u00b4erale de Lausanne",
        "aff_domain": "idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a8345c3bb9e3896ea538ce77ffaf2c20-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Idiap Research Institue;\u00b4Ecole polytechnique f\u00b4ed\u00b4erale de Lausanne",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "title": "Kernel Feature Selection via Conditional Covariance Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9461",
        "id": "9461",
        "author_site": "Jianbo Chen, Mitchell Stern, Martin J Wainwright, Michael Jordan",
        "author": "Jianbo Chen; Mitchell Stern; Martin J. Wainwright; Michael I Jordan",
        "abstract": "We propose a method for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. Building on past work in kernel dimension reduction, we show how to perform feature selection via a constrained optimization problem involving the trace of the conditional covariance operator. We prove various consistency results for this procedure, and also demonstrate that our method compares favorably with other state-of-the-art algorithms on a variety of synthetic and real data sets.",
        "bibtex": "@inproceedings{NIPS2017_b7fede84,\n author = {Chen, Jianbo and Stern, Mitchell and Wainwright, Martin J and Jordan, Michael I},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Feature Selection via Conditional Covariance Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b7fede84c2be02ccb9c77107956560eb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b7fede84c2be02ccb9c77107956560eb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b7fede84c2be02ccb9c77107956560eb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b7fede84c2be02ccb9c77107956560eb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b7fede84c2be02ccb9c77107956560eb-Reviews.html",
        "metareview": "",
        "pdf_size": 729142,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17144839773957485390&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b7fede84c2be02ccb9c77107956560eb-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Kernel functions based on triplet comparisons",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9448",
        "id": "9448",
        "author_site": "Matth\u00e4us Kleindessner, Ulrike von Luxburg",
        "author": "Matth\u00e4us Kleindessner; Ulrike von Luxburg",
        "abstract": "Given only information in the form of similarity triplets \"Object A is more similar to object B than to object C\" about a data set, we propose two ways of defining a kernel function on the data set. While previous approaches construct a low-dimensional Euclidean embedding of the data set that reflects the given similarity triplets, we aim at defining kernel functions that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply any kernel method to the data set.",
        "bibtex": "@inproceedings{NIPS2017_07211688,\n author = {Kleindessner, Matth\\\"{a}us and von Luxburg, Ulrike},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel functions based on triplet comparisons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/07211688a0869d995947a8fb11b215d6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/07211688a0869d995947a8fb11b215d6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/07211688a0869d995947a8fb11b215d6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/07211688a0869d995947a8fb11b215d6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/07211688a0869d995947a8fb11b215d6-Reviews.html",
        "metareview": "",
        "pdf_size": 1828096,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7384071618069132377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Rutgers University; Department of Computer Science, University of T\u00fcbingen + Max Planck Institute for Intelligent Systems, T\u00fcbingen",
        "aff_domain": "cs.rutgers.edu;informatik.uni-tuebingen.de",
        "email": "cs.rutgers.edu;informatik.uni-tuebingen.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/07211688a0869d995947a8fb11b215d6-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Rutgers University;University of T\u00fcbingen;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;",
        "aff_unique_url": "https://www.rutgers.edu;https://www.uni-tuebingen.de/;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "Rutgers;;MPI-IS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0;1+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Label Distribution Learning Forests",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8878",
        "id": "8878",
        "author_site": "Wei Shen, KAI ZHAO, Yilu Guo, Alan Yuille",
        "author": "Wei Shen; KAI ZHAO; Yilu Guo; Alan L. Yuille",
        "abstract": "Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.",
        "bibtex": "@inproceedings{NIPS2017_6e2713a6,\n author = {Shen, Wei and ZHAO, KAI and Guo, Yilu and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Label Distribution Learning Forests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Reviews.html",
        "metareview": "",
        "pdf_size": 804952,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7832599317037602798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University + Department of Computer Science, Johns Hopkins University; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University;Johns Hopkins University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";https://www.jhu.edu",
        "aff_unique_abbr": ";JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Label Efficient Learning of Transferable Representations acrosss Domains and Tasks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8814",
        "id": "8814",
        "author_site": "Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei",
        "author": "Zelun Luo; Yuliang Zou; Judy Hoffman; Li F Fei-Fei",
        "abstract": "We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.",
        "bibtex": "@inproceedings{NIPS2017_a8baa565,\n author = {Luo, Zelun and Zou, Yuliang and Hoffman, Judy and Fei-Fei, Li F},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Label Efficient Learning of Transferable Representations acrosss Domains and Tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Reviews.html",
        "metareview": "",
        "pdf_size": 1626287,
        "gs_citation": 360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10486375725591601278&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Stanford University; Virginia Tech; University of California, Berkeley; Stanford University",
        "aff_domain": "stanford.edu;vt.edu;eecs.berkeley.edu;cs.stanford.edu",
        "email": "stanford.edu;vt.edu;eecs.berkeley.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Stanford University;Virginia Tech;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.vt.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;VT;UC Berkeley",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Stanford;;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8857",
        "id": "8857",
        "author_site": "Nanyang Ye, Zhanxing Zhu, Rafal Mantiuk",
        "author": "Nanyang Ye; Zhanxing Zhu; Rafal Mantiuk",
        "abstract": "Minimizing non-convex and high-dimensional objective functions is challenging, especially when training modern deep neural networks.  In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the `fat'' modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed ``temperature dynamics''.  These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments.",
        "bibtex": "@inproceedings{NIPS2017_019d385e,\n author = {Ye, Nanyang and Zhu, Zhanxing and Mantiuk, Rafal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/019d385eb67632a7e958e23f24bd07d7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/019d385eb67632a7e958e23f24bd07d7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/019d385eb67632a7e958e23f24bd07d7-Reviews.html",
        "metareview": "",
        "pdf_size": 3358997,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15195820813569086952&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html"
    },
    {
        "title": "Language Modeling with Recurrent Highway Hypernetworks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9111",
        "id": "9111",
        "author": "Joseph Suarez",
        "abstract": "We present extensive experimental and theoretical support for the efficacy of recurrent highway networks (RHNs) and recurrent hypernetworks complimentary to the original works. Where the original RHN work primarily provides theoretical treatment of the subject, we demonstrate experimentally that RHNs benefit from far better gradient flow than LSTMs in addition to their improved task accuracy. The original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved--we consider these in depth and frame several feasible solutions that we believe will yield further gains in the future. We demonstrate that these approaches are complementary: by combining RHNs and hypernetworks, we make a significant improvement over current state-of-the-art character-level language modeling performance on Penn Treebank while relying on much simpler regularization. Finally, we argue for RHNs as a drop-in replacement for LSTMs (analogous to LSTMs for vanilla RNNs) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.",
        "bibtex": "@inproceedings{NIPS2017_f9d11525,\n author = {Suarez, Joseph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Language Modeling with Recurrent Highway Hypernetworks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f9d1152547c0bde01830b7e8bd60024c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f9d1152547c0bde01830b7e8bd60024c-Reviews.html",
        "metareview": "",
        "pdf_size": 366285,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=806768304380098117&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University",
        "aff_domain": "stanford.edu",
        "email": "stanford.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9017",
        "id": "9017",
        "author_site": "Kinjal Basu, Ankan Saha, Shaunak Chatterjee",
        "author": "Kinjal Basu; Ankan Saha; Shaunak Chatterjee",
        "abstract": "We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem, they are mostly not scalable. In this paper, we develop a method that transforms the quadratic constraint into a linear form by a sampling a set of low-discrepancy points. The transformed problem can then be solved by applying any state-of-the-art large-scale solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability in practice.",
        "bibtex": "@inproceedings{NIPS2017_d10ec7c1,\n author = {Basu, Kinjal and Saha, Ankan and Chatterjee, Shaunak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Reviews.html",
        "metareview": "",
        "pdf_size": 570777,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17828794289360623823&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d10ec7c16cbe9de8fbb1c42787c3ec26-Abstract.html"
    },
    {
        "title": "Learned D-AMP: Principled Neural Network based Compressive Image Recovery",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8967",
        "id": "8967",
        "author_site": "Chris Metzler, Ali Mousavi, Richard Baraniuk",
        "author": "Chris Metzler; Ali Mousavi; Richard Baraniuk",
        "abstract": "Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix.  It was recently demonstrated that iterative sparse-signal-recovery algorithms can be ``unrolled\u2019' to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network {\\em Learned} D-AMP (LDAMP).  The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over $50\\times$ faster than BM3D-AMP and hundreds of times faster than NLR-CS.",
        "bibtex": "@inproceedings{NIPS2017_8597a6cf,\n author = {Metzler, Chris and Mousavi, Ali and Baraniuk, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learned D-AMP: Principled Neural Network based Compressive Image Recovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Reviews.html",
        "metareview": "",
        "pdf_size": 2090932,
        "gs_citation": 332,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2459406187339435364&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Rice University; Rice University; Rice University",
        "aff_domain": "rice.edu;rice.edu;rice.edu",
        "email": "rice.edu;rice.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8597a6cfa74defcbde3047c891d78f90-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Rice University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rice.edu",
        "aff_unique_abbr": "Rice",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learned in Translation: Contextualized Word Vectors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9400",
        "id": "9400",
        "author_site": "Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher",
        "author": "Bryan McCann; James Bradbury; Caiming Xiong; Richard Socher",
        "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
        "bibtex": "@inproceedings{NIPS2017_20c86a62,\n author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learned in Translation: Contextualized Word Vectors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Reviews.html",
        "metareview": "",
        "pdf_size": 412761,
        "gs_citation": 1344,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12356231721397988330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Salesforce; Salesforce; Salesforce; Salesforce",
        "aff_domain": "salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "email": "salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Salesforce",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.salesforce.com",
        "aff_unique_abbr": "Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning A Structured Optimal Bipartite Graph for Co-Clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9193",
        "id": "9193",
        "author_site": "Feiping Nie, Xiaoqian Wang, Cheng Deng, Heng Huang",
        "author": "Feiping Nie; Xiaoqian Wang; Cheng Deng; Heng Huang",
        "abstract": "Co-clustering methods have been widely applied to document clustering and gene expression analysis. These methods make use of the duality between features and samples such that the co-occurring structure of sample and feature clusters can be extracted. In graph based co-clustering methods, a bipartite graph is constructed to depict the relation between features and samples. Most existing co-clustering methods conduct clustering on the graph achieved from the original data matrix, which doesn\u2019t have explicit cluster structure, thus they require a post-processing step to obtain the clustering results. In this paper, we propose a novel co-clustering method to learn a bipartite graph with exactly k connected components, where k is the number of clusters. The new bipartite graph learned in our model approximates the original graph but maintains an explicit cluster structure, from which we can immediately get the clustering results without post-processing. Extensive empirical results are presented to verify the effectiveness and robustness of our model.",
        "bibtex": "@inproceedings{NIPS2017_00a03ec6,\n author = {Nie, Feiping and Wang, Xiaoqian and Deng, Cheng and Huang, Heng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning A Structured Optimal Bipartite Graph for Co-Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/00a03ec6533ca7f5c644d198d815329c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/00a03ec6533ca7f5c644d198d815329c-Reviews.html",
        "metareview": "",
        "pdf_size": 579987,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18257128901798460069&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Center for OPTIMAL, Northwestern Polytechnical University, China; Department of Electrical and Computer Engineering, University of Pittsburgh, USA; School of Electronic Engineering, Xidian University, China; Department of Electrical and Computer Engineering, University of Pittsburgh, USA",
        "aff_domain": "gmail.com;gmail.com;mail.xidian.edu.cn;pitt.edu",
        "email": "gmail.com;gmail.com;mail.xidian.edu.cn;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/00a03ec6533ca7f5c644d198d815329c-Abstract.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "School of Computer Science, Center for OPTIMAL, Northwestern Polytechnical University, China;University of Pittsburgh;Xidian University",
        "aff_unique_dep": ";Department of Electrical and Computer Engineering;School of Electronic Engineering",
        "aff_unique_url": ";https://www.pitt.edu;http://www.xidian.edu.cn",
        "aff_unique_abbr": ";Pitt;Xidian",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;1",
        "aff_country_unique": ";United States;China"
    },
    {
        "title": "Learning Active Learning from Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9202",
        "id": "9202",
        "author_site": "Ksenia Konyushkova, Raphael Sznitman, Pascal Fua",
        "author": "Ksenia Konyushkova; Raphael Sznitman; Pascal Fua",
        "abstract": "In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can  be learnt either from simple synthetic 2D datasets or  from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.",
        "bibtex": "@inproceedings{NIPS2017_8ca8da41,\n author = {Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Active Learning from Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Reviews.html",
        "metareview": "",
        "pdf_size": 810467,
        "gs_citation": 411,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16475067584131096563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "CVLab, EPFL; ARTORG Center, University of Bern; CVLab, EPFL",
        "aff_domain": "epfl.ch;artorg.unibe.ch;epfl.ch",
        "email": "epfl.ch;artorg.unibe.ch;epfl.ch",
        "github": "",
        "project": "http://ksenia.konyushkova.com",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne;University of Bern",
        "aff_unique_dep": "CVLab;ARTORG Center",
        "aff_unique_url": "https://cvlab.epfl.ch;https://www.unibe.ch",
        "aff_unique_abbr": "EPFL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Learning Affinity via Spatial Propagation Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8943",
        "id": "8943",
        "author_site": "Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong, Ming-Hsuan Yang, Jan Kautz",
        "author": "Sifei Liu; Shalini De Mello; Jinwei Gu; Guangyu Zhong; Ming-Hsuan Yang; Jan Kautz",
        "abstract": "In this paper, we propose a spatial propagation networks for learning affinity matrix. We show that by constructing a row/column linear propagation model, the spatially variant transformation matrix constitutes an affinity matrix that models dense, global pairwise similarities of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix where all elements can be the output from a deep CNN, but (b) results in a dense affinity matrix that is effective to model any task-specific pairwise similarity. Instead of designing the similarity kernels according to image features of two points, we can directly output all similarities in a pure data-driven manner. The spatial propagation network is a generic framework that can be applied to numerous tasks, which traditionally benefit from designed affinity, e.g., image matting, colorization, and guided filtering, to name a few. Furthermore, the model can also learn semantic-aware affinity for high-level vision tasks due to the learning capability of the deep model. We validate the proposed framework by refinement of object segmentation. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides general, effective and efficient solutions for generating high-quality segmentation results.",
        "bibtex": "@inproceedings{NIPS2017_c22abfa3,\n author = {Liu, Sifei and De Mello, Shalini and Gu, Jinwei and Zhong, Guangyu and Yang, Ming-Hsuan and Kautz, Jan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Affinity via Spatial Propagation Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Reviews.html",
        "metareview": "",
        "pdf_size": 4291874,
        "gs_citation": 338,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14073264167496277995&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "UC Merced + NVIDIA; NVIDIA; NVIDIA; Dalian University of Technology; UC Merced + NVIDIA; NVIDIA",
        "aff_domain": "ucmerced.edu;nvidia.com;nvidia.com;dlut.edu.cn;ucmerced.edu;nvidia.com",
        "email": "ucmerced.edu;nvidia.com;nvidia.com;dlut.edu.cn;ucmerced.edu;nvidia.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c22abfa379f38b5b0411bc11fa9bf92f-Abstract.html",
        "aff_unique_index": "0+1;1;1;2;0+1;1",
        "aff_unique_norm": "University of California, Merced;NVIDIA Corporation;Dalian University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucmerced.edu;https://www.nvidia.com;http://www.dlut.edu.cn/",
        "aff_unique_abbr": "UCM;NVIDIA;DUT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0+0;0;0;1;0+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Learning Causal Structures Using Regression Invariance",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9086",
        "id": "9086",
        "author_site": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang",
        "author": "AmirEmad Ghassami; Saber Salehkaleybar; Negar Kiyavash; Kun Zhang",
        "abstract": "We study causal discovery in a multi-environment setting, in which the functional relations for producing the variables from their direct causes remain the same across environments, while the distribution of exogenous noises may vary. We introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments for structure learning. We define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm. Additionally, we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm. Experiment results show that the proposed algorithm outperforms the other existing algorithms.",
        "bibtex": "@inproceedings{NIPS2017_62889e73,\n author = {Ghassami, AmirEmad and Salehkaleybar, Saber and Kiyavash, Negar and Zhang, Kun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Causal Structures Using Regression Invariance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/62889e73828c756c961c5a6d6c01a463-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/62889e73828c756c961c5a6d6c01a463-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/62889e73828c756c961c5a6d6c01a463-Reviews.html",
        "metareview": "",
        "pdf_size": 621343,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3174975700400375866&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of ECE, University of Illinois at Urbana-Champaign + Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Department of ECE, University of Illinois at Urbana-Champaign + Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Department of ECE, University of Illinois at Urbana-Champaign + Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Department of Philosophy, Carnegie Mellon University",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;cmu.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/62889e73828c756c961c5a6d6c01a463-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1;2",
        "aff_unique_norm": "Department of ECE, University of Illinois at Urbana-Champaign;University of Illinois at Urbana-Champaign;Carnegie Mellon University",
        "aff_unique_dep": ";Coordinated Science Laboratory;Department of Philosophy",
        "aff_unique_url": ";https://www illinois.edu;https://www.cmu.edu",
        "aff_unique_abbr": ";UIUC;CMU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Learning Chordal Markov Networks via Branch and Bound",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8974",
        "id": "8974",
        "author_site": "Kari Rantanen, Antti Hyttinen, Matti J\u00e4rvisalo",
        "author": "Kari Rantanen; Antti Hyttinen; Matti J\u00e4rvisalo",
        "abstract": "We present a new algorithmic approach for the task of finding a chordal Markov network structure that maximizes a given scoring function. The algorithm is based on branch and bound and integrates dynamic programming for both domain pruning and for obtaining strong bounds for search-space pruning. Empirically, we show that the approach dominates in terms of running times a recent integer programming approach (and thereby also a recent constraint optimization approach) for the problem. Furthermore, our algorithm scales at times further with respect to the number of variables than a state-of-the-art dynamic programming algorithm for the problem, with the potential of reaching 20 variables and at the same time circumventing the tight exponential lower bounds on memory consumption of the pure dynamic programming approach.",
        "bibtex": "@inproceedings{NIPS2017_e2231217,\n author = {Rantanen, Kari and Hyttinen, Antti and J\\\"{a}rvisalo, Matti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Chordal Markov Networks via Branch and Bound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e22312179bf43e61576081a2f250f845-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e22312179bf43e61576081a2f250f845-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e22312179bf43e61576081a2f250f845-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e22312179bf43e61576081a2f250f845-Reviews.html",
        "metareview": "",
        "pdf_size": 313970,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2457808704390056393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e22312179bf43e61576081a2f250f845-Abstract.html"
    },
    {
        "title": "Learning Combinatorial Optimization Algorithms over Graphs",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9405",
        "id": "9405",
        "author_site": "Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, Le Song",
        "author": "Elias Khalil; Hanjun Dai; Yuyu Zhang; Bistra Dilkina; Le Song",
        "abstract": "The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems.  In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.",
        "bibtex": "@inproceedings{NIPS2017_d9896106,\n author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Combinatorial Optimization Algorithms over Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Reviews.html",
        "metareview": "",
        "pdf_size": 1166289,
        "gs_citation": 2018,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6935751850601868852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology + Ant Financial",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu;cc.gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;cc.gatech.edu;cc.gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d9896106ca98d3d05b8cbdf4fd8b13a1-Abstract.html",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Georgia Institute of Technology;Ant Financial",
        "aff_unique_dep": "College of Computing;",
        "aff_unique_url": "https://www.gatech.edu;https://www.antgroup.com",
        "aff_unique_abbr": "Georgia Tech;Ant Financial",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9177",
        "id": "9177",
        "author_site": "Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang, Nicu Sebe",
        "author": "Dan Xu; Wanli Ouyang; Xavier Alameda-Pineda; Elisa Ricci; Xiaogang Wang; Nicu Sebe",
        "abstract": "Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.",
        "bibtex": "@inproceedings{NIPS2017_a869ccbc,\n author = {Xu, Dan and Ouyang, Wanli and Alameda-Pineda, Xavier and Ricci, Elisa and Wang, Xiaogang and Sebe, Nicu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a869ccbcbd9568808b8497e28275c7c8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a869ccbcbd9568808b8497e28275c7c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a869ccbcbd9568808b8497e28275c7c8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a869ccbcbd9568808b8497e28275c7c8-Reviews.html",
        "metareview": "",
        "pdf_size": 2490099,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15724893170316859166&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "The University of Trento; The University of Sydney; Perception Group, INRIA; University of Perugia; The Chinese University of Hong Kong; The University of Trento",
        "aff_domain": "unitn.it;sydney.edu.au;inria.fr;unipg.it;ee.cuhk.edu.hk;unitn.it",
        "email": "unitn.it;sydney.edu.au;inria.fr;unipg.it;ee.cuhk.edu.hk;unitn.it",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a869ccbcbd9568808b8497e28275c7c8-Abstract.html",
        "aff_unique_index": "0;1;2;3;4;0",
        "aff_unique_norm": "University of Trento;University of Sydney;Perception Group, INRIA;University of Perugia;The Chinese University of Hong Kong",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.unitn.it;https://www.sydney.edu.au;;https://www.unipg.it;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "UniTN;USYD;;Unipg;CUHK",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1;0;3;0",
        "aff_country_unique": "Italy;Australia;;China"
    },
    {
        "title": "Learning Disentangled Representations with Semi-Supervised Deep Generative Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9365",
        "id": "9365",
        "author_site": "Siddharth Narayanaswamy, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, Philip Torr",
        "author": "Siddharth N; Brooks Paige; Jan-Willem van de Meent; Alban Desmaison; Noah Goodman; Pushmeet Kohli; Frank Wood; Philip Torr",
        "abstract": "Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.",
        "bibtex": "@inproceedings{NIPS2017_9cb9ed4f,\n author = {N, Siddharth and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah and Kohli, Pushmeet and Wood, Frank and Torr, Philip},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Reviews.html",
        "metareview": "",
        "pdf_size": 2175196,
        "gs_citation": 426,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=884149417524188355&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "University of Oxford; Alan Turing Institute + University of Cambridge; Northeastern University; University of Oxford; Stanford University; Deepmind; University of Oxford; University of Oxford",
        "aff_domain": "robots.ox.ac.uk;turing.ac.uk;northeastern.edu;robots.ox.ac.uk;stanford.edu;google.com;robots.ox.ac.uk;eng.ox.ac.uk",
        "email": "robots.ox.ac.uk;turing.ac.uk;northeastern.edu;robots.ox.ac.uk;stanford.edu;google.com;robots.ox.ac.uk;eng.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9cb9ed4f35cf7c2f295cc2bc6f732a84-Abstract.html",
        "aff_unique_index": "0;1+2;3;0;4;5;0;0",
        "aff_unique_norm": "University of Oxford;Alan Turing Institute;University of Cambridge;Northeastern University;Stanford University;DeepMind",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.turing.ac.uk;https://www.cam.ac.uk;https://www.northeastern.edu;https://www.stanford.edu;https://deepmind.com",
        "aff_unique_abbr": "Oxford;ATI;Cambridge;NEU;Stanford;DeepMind",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Cambridge;Stanford",
        "aff_country_unique_index": "0;0+0;1;0;1;0;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Learning Efficient Object Detection Models with Knowledge Distillation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8869",
        "id": "8869",
        "author_site": "Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, Manmohan Chandraker",
        "author": "Guobin Chen; Wongun Choi; Xiang Yu; Tony Han; Manmohan Chandraker",
        "abstract": "Despite significant accuracy improvement in convolutional neural networks (CNN) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast ob- ject detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous la- bels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distribu- tions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including PASCAL, KITTI, ILSVRC and MS-COCO. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.",
        "bibtex": "@inproceedings{NIPS2017_e1e32e23,\n author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Efficient Object Detection Models with Knowledge Distillation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Reviews.html",
        "metareview": "",
        "pdf_size": 341402,
        "gs_citation": 1342,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9704115036838184098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "NEC Labs America+University of Missouri; NEC Labs America; NEC Labs America; University of Missouri; NEC Labs America+University of California, San Diego",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html",
        "aff_unique_index": "0+1;0;0;1;0+2",
        "aff_unique_norm": "NEC Labs America;University of Missouri;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nec-labs.com;https://www.missouri.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "NEC LA;MU;UCSD",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Graph Representations with Embedding Propagation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9288",
        "id": "9288",
        "author_site": "Alberto Garcia Duran, Mathias Niepert",
        "author": "Alberto Garcia Duran; Mathias Niepert",
        "abstract": "We propose EP, Embedding Propagation, an unsupervised learning framework for graph-structured data. EP learns vector representations of graphs by passing two types of messages between neighboring nodes. Forward messages consist of label representations such as representations of words and other attributes associated with the nodes. Backward messages consist of gradients that result from aggregating the label representations and applying a reconstruction loss. Node representations are finally computed from the representation of their labels. With significantly fewer parameters and hyperparameters, an instance of EP is competitive with and often outperforms state of the art unsupervised and semi-supervised learning methods on a range of benchmark data sets.",
        "bibtex": "@inproceedings{NIPS2017_e0688d13,\n author = {Garcia Duran, Alberto and Niepert, Mathias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Graph Representations with Embedding Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e0688d13958a19e087e123148555e4b4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e0688d13958a19e087e123148555e4b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e0688d13958a19e087e123148555e4b4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e0688d13958a19e087e123148555e4b4-Reviews.html",
        "metareview": "",
        "pdf_size": 567887,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12875076665867005923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "NEC Labs Europe, Heidelberg, Germany; NEC Labs Europe, Heidelberg, Germany",
        "aff_domain": "neclab.eu;neclab.eu",
        "email": "neclab.eu;neclab.eu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e0688d13958a19e087e123148555e4b4-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Labs Europe",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec-labs.eu",
        "aff_unique_abbr": "NEC LE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Learning Hierarchical Information Flow with Recurrent Neural Modules",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9440",
        "id": "9440",
        "author_site": "Danijar Hafner, Alexander Irpan, James Davidson, Nicolas Heess",
        "author": "Danijar Hafner; Alexander Irpan; James Davidson; Nicolas Heess",
        "abstract": "We propose ThalNet, a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. Our model outperforms standard recurrent neural networks on several sequential benchmarks.",
        "bibtex": "@inproceedings{NIPS2017_6c349155,\n author = {Hafner, Danijar and Irpan, Alexander and Davidson, James and Heess, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Hierarchical Information Flow with Recurrent Neural Modules},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c349155b122aa8ad5c877007e05f24f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6c349155b122aa8ad5c877007e05f24f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6c349155b122aa8ad5c877007e05f24f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6c349155b122aa8ad5c877007e05f24f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6c349155b122aa8ad5c877007e05f24f-Reviews.html",
        "metareview": "",
        "pdf_size": 510919,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3196170843802548160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Google Brain; Google Brain; Google Brain; Google DeepMind",
        "aff_domain": "danijar.com;google.com;google.com;google.com",
        "email": "danijar.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6c349155b122aa8ad5c877007e05f24f-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Brain",
        "aff_unique_url": "https://brain.google.com",
        "aff_unique_abbr": "Google Brain",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9415",
        "id": "9415",
        "author_site": "Asish Ghoshal, Jean Honorio",
        "author": "Asish Ghoshal; Jean Honorio",
        "abstract": "Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many non-identifiability and hardness results are known. In this paper we propose a provably polynomial-time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance --- a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data --- under high-dimensional settings. We show that $O(k^4 \\log p)$ number of samples suffices for our method to recover the true DAG structure with high probability, where $p$ is the number of variables and $k$ is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called \\emph{restricted strong adjacency faithfulness} (RSAF), which is strictly weaker than strong faithfulness --- a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on $p$. We validate our theoretical findings through synthetic experiments.",
        "bibtex": "@inproceedings{NIPS2017_907edb0a,\n author = {Ghoshal, Asish and Honorio, Jean},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/907edb0aa6986220dbffb79a788596ee-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/907edb0aa6986220dbffb79a788596ee-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/907edb0aa6986220dbffb79a788596ee-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/907edb0aa6986220dbffb79a788596ee-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/907edb0aa6986220dbffb79a788596ee-Reviews.html",
        "metareview": "",
        "pdf_size": 592097,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11362607407010670767&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Purdue University, West Lafayette, IN - 47906; Department of Computer Science, Purdue University, West Lafayette, IN - 47906",
        "aff_domain": "purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/907edb0aa6986220dbffb79a788596ee-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "West Lafayette",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8906",
        "id": "8906",
        "author_site": "Naoya Takeishi, Yoshinobu Kawahara, Takehisa Yairi",
        "author": "Naoya Takeishi; Yoshinobu Kawahara; Takehisa Yairi",
        "abstract": "Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.",
        "bibtex": "@inproceedings{NIPS2017_3a835d32,\n author = {Takeishi, Naoya and Kawahara, Yoshinobu and Yairi, Takehisa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Reviews.html",
        "metareview": "",
        "pdf_size": 1250330,
        "gs_citation": 505,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8832228543043392776&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Aeronautics and Astronautics, The University of Tokyo; The Institute of Scientific and Industrial Research, Osaka University + RIKEN Center for Advanced Intelligence Project; Department of Aeronautics and Astronautics, The University of Tokyo",
        "aff_domain": "ailab.t.u-tokyo.ac.jp;sanken.osaka-u.ac.jp;ailab.t.u-tokyo.ac.jp",
        "email": "ailab.t.u-tokyo.ac.jp;sanken.osaka-u.ac.jp;ailab.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3a835d3215755c435ef4fe9965a3f2a0-Abstract.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Department of Aeronautics and Astronautics, The University of Tokyo;The Institute of Scientific and Industrial Research, Osaka University;RIKEN",
        "aff_unique_dep": ";;Center for Advanced Intelligence Project",
        "aff_unique_url": ";;https://www.riken.jp/en/",
        "aff_unique_abbr": ";;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Japan"
    },
    {
        "title": "Learning Linear Dynamical Systems via Spectral Filtering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9438",
        "id": "9438",
        "author_site": "Elad Hazan, Karan Singh, Cyril Zhang",
        "author": "Elad Hazan; Karan Singh; Cyril Zhang",
        "abstract": "We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.",
        "bibtex": "@inproceedings{NIPS2017_165a59f7,\n author = {Hazan, Elad and Singh, Karan and Zhang, Cyril},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Linear Dynamical Systems via Spectral Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/165a59f7cf3b5c4396ba65953d679f17-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/165a59f7cf3b5c4396ba65953d679f17-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/165a59f7cf3b5c4396ba65953d679f17-Reviews.html",
        "metareview": "",
        "pdf_size": 615566,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8832177801009596139&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/165a59f7cf3b5c4396ba65953d679f17-Abstract.html"
    },
    {
        "title": "Learning Low-Dimensional Metrics",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9194",
        "id": "9194",
        "author_site": "Blake Mason, Lalit Jain, Robert Nowak",
        "author": "Blake Mason; Lalit Jain; Robert Nowak",
        "abstract": "This paper investigates the theoretical foundations of metric learning, focused on three key questions that are not fully addressed in prior work:  1) we consider learning general low-dimensional (low-rank) metrics as well as sparse metrics;2) we develop upper and lower (minimax) bounds on the generalization error; 3)we quantify the sample complexity of metric learning in terms of the dimension of the feature space and the dimension/rank of the underlying metric; 4) we also bound the accuracy of the learned metric relative to the underlying true generative metric. All the results involve novel mathematical approaches to the metric learning problem, and also shed new light on the special case of ordinal embedding (aka non-metric multidimensional scaling).",
        "bibtex": "@inproceedings{NIPS2017_f12ee973,\n author = {Mason, Blake and Jain, Lalit and Nowak, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Low-Dimensional Metrics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Reviews.html",
        "metareview": "",
        "pdf_size": 1443169,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12743141621551846037&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Michigan; University of Wisconsin; University of Wisconsin",
        "aff_domain": "umich.edu;wisc.edu;wisc.edu",
        "email": "umich.edu;wisc.edu;wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f12ee9734e1edf70ed02d9829018b3d9-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Michigan;University of Wisconsin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.wisc.edu",
        "aff_unique_abbr": "UM;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Mixture of Gaussians with Streaming Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9429",
        "id": "9429",
        "author_site": "Aditi Raghunathan, Prateek Jain, Ravishankar Krishnawamy",
        "author": "Aditi Raghunathan; Prateek Jain; Ravishankar Krishnawamy",
        "abstract": "In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of $N$ points in $d$ dimensions generated by an unknown mixture of $k$ spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians  accurately if they are sufficiently separated. Assuming each pair of centers are $C\\sigma$ distant with $C=\\Omega((k\\log k)^{1/4}\\sigma)$ and where $\\sigma^2$ is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians \\citep{vempalawang}. For finite samples, we show that a bias term based on the initial estimate decreases at $O(1/{\\rm poly}(N))$ rate while variance decreases at nearly optimal rate of $\\sigma^2 d/N$. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal $d\\cdot k$ while space complexity of our algorithm is $O(dk\\log k)$.  In addition to the bias and variance terms which tend to $0$, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an \\emph{approximation error} that cannot be avoided. However, by using a streaming version of the classical \\emph{(soft-thresholding-based)} EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to $0$ for $N\\rightarrow \\infty$.",
        "bibtex": "@inproceedings{NIPS2017_f24ad6f7,\n author = {Raghunathan, Aditi and Jain, Prateek and Krishnawamy, Ravishankar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Mixture of Gaussians with Streaming Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f24ad6f72d6cc4cb51464f2b29ab69d3-Reviews.html",
        "metareview": "",
        "pdf_size": 359402,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12182463806593109139&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University; Microsoft Research, India; Microsoft Research, India",
        "aff_domain": "stanford.edu;microsoft.com;microsoft.com",
        "email": "stanford.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f24ad6f72d6cc4cb51464f2b29ab69d3-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Stanford University;Microsoft Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.microsoft.com/en-us/research/group/india.aspx",
        "aff_unique_abbr": "Stanford;MSR India",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Learning Multiple Tasks with Multilinear Relationship Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8950",
        "id": "8950",
        "author_site": "Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, Philip S Yu",
        "author": "Mingsheng Long; ZHANGJIE CAO; Jianmin Wang; Philip S Yu",
        "abstract": "Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying  parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.",
        "bibtex": "@inproceedings{NIPS2017_03e0704b,\n author = {Long, Mingsheng and CAO, ZHANGJIE and Wang, Jianmin and Yu, Philip S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Multiple Tasks with Multilinear Relationship Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/03e0704b5690a2dee1861dc3ad3316c9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/03e0704b5690a2dee1861dc3ad3316c9-Reviews.html",
        "metareview": "",
        "pdf_size": 3659633,
        "gs_citation": 399,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10591504582934255729&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Software, Tsinghua University, Beijing 100084, China; School of Software, Tsinghua University, Beijing 100084, China; School of Software, Tsinghua University, Beijing 100084, China; ",
        "aff_domain": "tsinghua.edu.cn;gmail.com;tsinghua.edu.cn;uic.edu",
        "email": "tsinghua.edu.cn;gmail.com;tsinghua.edu.cn;uic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/03e0704b5690a2dee1861dc3ad3316c9-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "School of Software, Tsinghua University, Beijing 100084, China",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Learning Neural Representations of Human Cognition across Many fMRI Studies",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9361",
        "id": "9361",
        "author_site": "Arthur Mensch, Julien Mairal, Danilo Bzdok, Bertrand Thirion, Gael Varoquaux",
        "author": "Arthur Mensch; Julien Mairal; Danilo Bzdok; Bertrand Thirion; Gael Varoquaux",
        "abstract": "Cognitive neuroscience is enjoying rapid increase in extensive public brain-imaging datasets. It opens the door to large-scale statistical models. Finding a unified perspective for all available data calls for scalable and automated solutions to an old challenge: how to aggregate heterogeneous information on brain function into a universal cognitive system that relates mental operations/cognitive processes/psychological tasks to brain networks? We cast this challenge in a machine-learning approach to predict conditions from statistical brain maps across different studies. For this, we leverage multi-task learning and multi-scale dimension reduction to learn low-dimensional representations of brain images that carry cognitive information and can be robustly associated with psychological stimuli. Our multi-dataset classification model achieves the best prediction performance on several large reference datasets, compared to models without cognitive-aware low-dimension representations; it brings a substantial performance boost to the analysis of small datasets, and can be introspected to identify universal template cognitive concepts.",
        "bibtex": "@inproceedings{NIPS2017_908c9a56,\n author = {Mensch, Arthur and Mairal, Julien and Bzdok, Danilo and Thirion, Bertrand and Varoquaux, Gael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Neural Representations of Human Cognition across Many fMRI Studies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/908c9a564a86426585b29f5335b619bc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/908c9a564a86426585b29f5335b619bc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/908c9a564a86426585b29f5335b619bc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/908c9a564a86426585b29f5335b619bc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/908c9a564a86426585b29f5335b619bc-Reviews.html",
        "metareview": "",
        "pdf_size": 1755137,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15844013424158153700&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Inria; Inria; Department of Psychiatry, RWTH; Inria+CEA+Universit\u00e9 Paris-Saclay; Inria+Univ. Grenoble Alpes+CNRS+Grenoble INP+LJK",
        "aff_domain": "m4x.org;inria.fr;rwth-aachen.de;inria.fr;inria.fr",
        "email": "m4x.org;inria.fr;rwth-aachen.de;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/908c9a564a86426585b29f5335b619bc-Abstract.html",
        "aff_unique_index": "0;0;1;0+2+3;0+4+5+6+7",
        "aff_unique_norm": "Inria;Department of Psychiatry, RWTH;Commissariat \u00e0 l'\u00e9nergie atomique et aux \u00e9nergies alternatives;Universit\u00e9 Paris-Saclay;Universit\u00e9 Grenoble Alpes;Centre National de la Recherche Scientifique;Grenoble INP;Laboratoire Jean Kuntzmann",
        "aff_unique_dep": ";;;;;;;",
        "aff_unique_url": "https://www.inria.fr;;https://www cea fr;https://www.universite-paris-saclay.fr;https://www.univ-grenoble-alpes.fr;https://www.cnrs.fr;https://www.grenoble-inp.fr;https://ljk.ensimag.fr",
        "aff_unique_abbr": "Inria;;CEA;UPSaclay;UGA;CNRS;Grenoble INP;LJK",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0+0;0+0+0+0+0",
        "aff_country_unique": "France;"
    },
    {
        "title": "Learning Overcomplete HMMs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8888",
        "id": "8888",
        "author_site": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant",
        "author": "Vatsal Sharan; Sham M. Kakade; Percy Liang; Gregory Valiant",
        "abstract": "We study the basic problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable-learning setting and the intractable setting. We show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned and have small probability mass on short cycles. We also show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.",
        "bibtex": "@inproceedings{NIPS2017_6aca9700,\n author = {Sharan, Vatsal and Kakade, Sham M and Liang, Percy S and Valiant, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Overcomplete HMMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6aca97005c68f1206823815f66102863-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6aca97005c68f1206823815f66102863-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6aca97005c68f1206823815f66102863-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6aca97005c68f1206823815f66102863-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6aca97005c68f1206823815f66102863-Reviews.html",
        "metareview": "",
        "pdf_size": 589794,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=852490216353562653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; University of Washington; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;cs.washington.edu;cs.stanford.edu;stanford.edu",
        "email": "stanford.edu;cs.washington.edu;cs.stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6aca97005c68f1206823815f66102863-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Stanford University;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.washington.edu",
        "aff_unique_abbr": "Stanford;UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Populations of Parameters",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9351",
        "id": "9351",
        "author_site": "Kevin Tian, Weihao Kong, Gregory Valiant",
        "author": "Kevin Tian; Weihao Kong; Gregory Valiant",
        "abstract": "Consider the following estimation problem: there are $n$ entities, each with an unknown parameter $p_i \\in [0,1]$, and we observe $n$ independent random variables, $X_1,\\ldots,X_n$, with $X_i \\sim $ Binomial$(t, p_i)$.  How accurately can one recover the ``histogram'' (i.e. cumulative density function) of the $p_i$'s?   While the empirical estimates would recover the histogram to earth mover distance $\\Theta(\\frac{1}{\\sqrt{t}})$ (equivalently, $\\ell_1$ distance between the CDFs), we show that, provided $n$ is sufficiently large, we can achieve error $O(\\frac{1}{t})$ which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters.  Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, sports analytics, and variation in the gender ratio of offspring.",
        "bibtex": "@inproceedings{NIPS2017_bc4e356f,\n author = {Tian, Kevin and Kong, Weihao and Valiant, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Populations of Parameters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bc4e356fee1972242c8f7eabf4dff517-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bc4e356fee1972242c8f7eabf4dff517-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/bc4e356fee1972242c8f7eabf4dff517-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bc4e356fee1972242c8f7eabf4dff517-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bc4e356fee1972242c8f7eabf4dff517-Reviews.html",
        "metareview": "",
        "pdf_size": 429115,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17761058569341903387&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bc4e356fee1972242c8f7eabf4dff517-Abstract.html"
    },
    {
        "title": "Learning ReLUs via Gradient Descent",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8989",
        "id": "8989",
        "author": "Mahdi Soltanolkotabi",
        "abstract": "In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form $\\vct{x}\\mapsto \\max(0,\\langle \\vct{w},\\vct{x}\\rangle)$ with $\\vct{w}\\in\\R^d$ denoting the weight vector.  We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialized at $\\vct{0}$, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.",
        "bibtex": "@inproceedings{NIPS2017_e034fb6b,\n author = {Soltanolkotabi, Mahdi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning ReLUs via Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e034fb6b66aacc1d48f445ddfb08da98-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e034fb6b66aacc1d48f445ddfb08da98-Reviews.html",
        "metareview": "",
        "pdf_size": 450818,
        "gs_citation": 212,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10541082377184066121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html"
    },
    {
        "title": "Learning Spherical Convolution for Fast Features from 360\u00b0 Imagery",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8849",
        "id": "8849",
        "author_site": "Yu-Chuan Su, Kristen Grauman",
        "author": "Yu-Chuan Su; Kristen Grauman",
        "abstract": "While 360\u00b0 cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield \u201cflat\" filters, yet 360\u00b0 images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360\u00b0 imagery directly in its equirectangular projection. Our approach learns to reproduce the flat filter outputs on 360\u00b0 data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360\u00b0 images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art \u201cflat\" object detector to 360\u00b0 data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.",
        "bibtex": "@inproceedings{NIPS2017_0c74b7f7,\n author = {Su, Yu-Chuan and Grauman, Kristen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Spherical Convolution for Fast Features from 360\\textdegree  Imagery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Reviews.html",
        "metareview": "",
        "pdf_size": 2328439,
        "gs_citation": 344,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9944174591271884548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Unknown Markov Decision Processes: A Thompson Sampling Approach",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8925",
        "id": "8925",
        "author_site": "Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, Rahul Jain",
        "author": "Yi Ouyang; Mukul Gagrani; Ashutosh Nayyar; Rahul Jain",
        "abstract": "We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish $\\tilde O(HS\\sqrt{AT})$ bounds on expected regret under a Bayesian setting, where $S$ and $A$ are the sizes of the state and action spaces, $T$ is time, and $H$ is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.",
        "bibtex": "@inproceedings{NIPS2017_51ef186e,\n author = {Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Unknown Markov Decision Processes: A Thompson Sampling Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/51ef186e18dc00c2d31982567235c559-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/51ef186e18dc00c2d31982567235c559-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/51ef186e18dc00c2d31982567235c559-Reviews.html",
        "metareview": "",
        "pdf_size": 285631,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=901911793868695857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Berkeley; University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "berkeley.edu;usc.edu;usc.edu;usc.edu",
        "email": "berkeley.edu;usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/51ef186e18dc00c2d31982567235c559-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of California, Berkeley;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.usc.edu",
        "aff_unique_abbr": "UC Berkeley;USC",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Berkeley;Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning a Multi-View Stereo Machine",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8833",
        "id": "8833",
        "author_site": "Abhishek Kar, Christian H\u00e4ne, Jitendra Malik",
        "author": "Abhishek Kar; Christian H\u00e4ne; Jitendra Malik",
        "abstract": "We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.",
        "bibtex": "@inproceedings{NIPS2017_9c838d2e,\n author = {Kar, Abhishek and H\\\"{a}ne, Christian and Malik, Jitendra},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning a Multi-View Stereo Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9c838d2e45b2ad1094d42f4ef36764f6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9c838d2e45b2ad1094d42f4ef36764f6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9c838d2e45b2ad1094d42f4ef36764f6-Reviews.html",
        "metareview": "",
        "pdf_size": 3608415,
        "gs_citation": 640,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1104621812979246968&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning from Complementary Labels",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9338",
        "id": "9338",
        "author_site": "Takashi Ishida, Gang Niu, Weihua Hu, Masashi Sugiyama",
        "author": "Takashi Ishida; Gang Niu; Weihua Hu; Masashi Sugiyama",
        "abstract": "Collecting labeled data is costly and thus a critical bottleneck in real-world classification tasks. To mitigate this problem, we propose a novel setting, namely learning from complementary labels for multi-class classification. A complementary label specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than collecting ordinary labels, since users do not have to carefully choose the correct class from a long list of candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from them. In this paper, we show that an unbiased estimator to the classification risk can be obtained only from complementarily labeled data, if a loss function satisfies a particular symmetric condition. We derive estimation error bounds for the proposed method and prove that the optimal parametric convergence rate is achieved. We further show that learning from complementary labels can be easily combined with learning from ordinary labels (i.e., ordinary supervised learning), providing a highly practical implementation of the proposed method. Finally, we experimentally demonstrate the usefulness of the proposed methods.",
        "bibtex": "@inproceedings{NIPS2017_1dba5eed,\n author = {Ishida, Takashi and Niu, Gang and Hu, Weihua and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from Complementary Labels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1dba5eed8838571e1c80af145184e515-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1dba5eed8838571e1c80af145184e515-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1dba5eed8838571e1c80af145184e515-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1dba5eed8838571e1c80af145184e515-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1dba5eed8838571e1c80af145184e515-Reviews.html",
        "metareview": "",
        "pdf_size": 562528,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18328494346514836056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Sumitomo Mitsui Asset Management, Tokyo, Japan + The University of Tokyo, Tokyo, Japan + RIKEN, Tokyo, Japan; The University of Tokyo, Tokyo, Japan + RIKEN, Tokyo, Japan; The University of Tokyo, Tokyo, Japan + RIKEN, Tokyo, Japan; The University of Tokyo, Tokyo, Japan + RIKEN, Tokyo, Japan",
        "aff_domain": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1dba5eed8838571e1c80af145184e515-Abstract.html",
        "aff_unique_index": "0+1+2;1+2;1+2;1+2",
        "aff_unique_norm": "Sumitomo Mitsui Asset Management, Tokyo, Japan;The University of Tokyo;RIKEN",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.u-tokyo.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": ";UTokyo;RIKEN",
        "aff_campus_unique_index": "1+1;1+1;1+1;1+1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "1+1;1+1;1+1;1+1",
        "aff_country_unique": ";Japan"
    },
    {
        "title": "Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9340",
        "id": "9340",
        "author_site": "Anton Mallasto, Aasa Feragen",
        "author": "Anton Mallasto; Aasa Feragen",
        "abstract": "We introduce a novel framework for statistical analysis of populations of non-degenerate Gaussian processes (GPs), which are natural representations of uncertain curves. This allows inherent variation or uncertainty in function-valued data to be properly incorporated in the population analysis. Using the 2-Wasserstein metric we geometrize the space of GPs with L2 mean and covariance functions over compact index spaces. We prove uniqueness of the barycenter of a population of GPs, as well as convergence of the metric and the barycenter of their finite-dimensional counterparts. This justifies practical computations. Finally, we demonstrate our framework through experimental validation on GP datasets representing brain connectivity and climate development. A Matlab library for relevant computations will be published at https://sites.google.com/view/antonmallasto/software.",
        "bibtex": "@inproceedings{NIPS2017_7a006957,\n author = {Mallasto, Anton and Feragen, Aasa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a006957be65e608e863301eb98e1808-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7a006957be65e608e863301eb98e1808-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7a006957be65e608e863301eb98e1808-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7a006957be65e608e863301eb98e1808-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7a006957be65e608e863301eb98e1808-Reviews.html",
        "metareview": "",
        "pdf_size": 869396,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10263584477959551650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Copenhagen; Department of Computer Science, University of Copenhagen",
        "aff_domain": "di.ku.dk;di.ku.dk",
        "email": "di.ku.dk;di.ku.dk",
        "github": "",
        "project": "https://sites.google.com/view/antonmallasto/software",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7a006957be65e608e863301eb98e1808-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Copenhagen",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ku.dk",
        "aff_unique_abbr": "UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "title": "Learning multiple visual domains with residual adapters",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8847",
        "id": "8847",
        "author_site": "Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi",
        "author": "Sylvestre-Alvise Rebuffi; Hakan Bilen; Andrea Vedaldi",
        "abstract": "There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of  representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.",
        "bibtex": "@inproceedings{NIPS2017_e7b24b11,\n author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning multiple visual domains with residual adapters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Reviews.html",
        "metareview": "",
        "pdf_size": 358453,
        "gs_citation": 1111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2217008455459281578&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Visual Geometry Group, University of Oxford + School of Informatics, University of Edinburgh; Visual Geometry Group, University of Oxford; Visual Geometry Group, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "University of Oxford;University of Edinburgh",
        "aff_unique_dep": "Visual Geometry Group;School of Informatics",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.ed.ac.uk",
        "aff_unique_abbr": "Oxford;Edinburgh",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Oxford;Edinburgh",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8908",
        "id": "8908",
        "author_site": "St\u00e9phanie ALLASSONNIERE, Juliette Chevallier, Stephane Oudard",
        "author": "St\u00e9phanie ALLASSONNIERE; Juliette Chevallier; Stephane Oudard",
        "abstract": "We introduce a hierarchical model which allows to estimate a group-average piecewise-geodesic trajectory in the Riemannian space of measurements and individual variability. This model falls into the well defined mixed-effect models. The subject-specific trajectories are defined through spatial and temporal transformations of the group-average piecewise-geodesic path, component by component. Thus we can apply our model to a wide variety of situations. Due to the non-linearity of the model, we use the Stochastic Approximation Expectation-Maximization algorithm to estimate the model parameters. Experiments on synthetic data validate this choice. The model is then applied to the metastatic renal cancer chemotherapy monitoring: we run estimations on RECIST scores of treated patients and estimate the time they escape from the treatment. Experiments highlight the role of the different parameters on the response to treatment.",
        "bibtex": "@inproceedings{NIPS2017_4e0928de,\n author = {ALLASSONNIERE, St\\'{e}phanie and Chevallier, Juliette and Oudard, Stephane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4e0928de075538c593fbdabb0c5ef2c3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4e0928de075538c593fbdabb0c5ef2c3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4e0928de075538c593fbdabb0c5ef2c3-Reviews.html",
        "metareview": "",
        "pdf_size": 448727,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10935386464276214277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "CMAP, \u00c9cole polytechnique; USPC, AP-HP, HEGP; CRC, Universit\u00e9 Paris Descartes",
        "aff_domain": "polytechnique.edu; ;parisdescartes.fr",
        "email": "polytechnique.edu; ;parisdescartes.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "CMAP, \u00c9cole polytechnique;USPC, AP-HP, HEGP;CRC, Universit\u00e9 Paris Descartes",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8903",
        "id": "8903",
        "author_site": "Mainak Jas, Tom Dupr\u00e9 la Tour, Umut Simsekli, Alexandre Gramfort",
        "author": "Mainak Jas; Tom Dupr\u00e9 la Tour; Umut Simsekli; Alexandre Gramfort",
        "abstract": "Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such `shift-invariant' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call $\\alpha$CSC, lies a family of heavy-tailed distributions called $\\alpha$-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we  develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides, $\\alpha$CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series.",
        "bibtex": "@inproceedings{NIPS2017_6f2268bd,\n author = {Jas, Mainak and Dupr\\'{e} la Tour, Tom and Simsekli, Umut and Gramfort, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6f2268bd1d3d3ebaabb04d6b5d099425-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6f2268bd1d3d3ebaabb04d6b5d099425-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6f2268bd1d3d3ebaabb04d6b5d099425-Reviews.html",
        "metareview": "",
        "pdf_size": 644785,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12817073862662181846&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html"
    },
    {
        "title": "Learning to Compose Domain-Specific Transformations for Data Augmentation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9108",
        "id": "9108",
        "author_site": "Alexander Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher R\u00e9",
        "author": "Alexander J Ratner; Henry Ehrenberg; Zeshan Hussain; Jared Dunnmon; Christopher R\u00e9",
        "abstract": "Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.",
        "bibtex": "@inproceedings{NIPS2017_f26dab9b,\n author = {Ratner, Alexander J and Ehrenberg, Henry and Hussain, Zeshan and Dunnmon, Jared and R\\'{e}, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Compose Domain-Specific Transformations for Data Augmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Reviews.html",
        "metareview": "",
        "pdf_size": 1081703,
        "gs_citation": 451,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9561278480749432385&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Inpaint for Image Compression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8917",
        "id": "8917",
        "author_site": "Mohammad Haris Baig, Vladlen Koltun, Lorenzo Torresani",
        "author": "Mohammad Haris Baig; Vladlen Koltun; Lorenzo Torresani",
        "abstract": "We study the design of deep architectures for lossy image compression. We present two architectural recipes in the context of multi-stage progressive encoders  and empirically demonstrate their importance on compression performance. Specifically, we show that: 1) predicting the original image data from residuals in a multi-stage progressive architecture facilitates learning and leads to improved performance at approximating the original content and 2) learning to inpaint (from neighboring image pixels) before performing compression reduces the amount of information that must be stored to achieve a high-quality approximation. Incorporating these design choices in a baseline progressive encoder yields an average reduction of over 60% in file size with similar quality compared to the original residual encoder.",
        "bibtex": "@inproceedings{NIPS2017_013a006f,\n author = {Baig, Mohammad Haris and Koltun, Vladlen and Torresani, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Inpaint for Image Compression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/013a006f03dbc5392effeb8f18fda755-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/013a006f03dbc5392effeb8f18fda755-Reviews.html",
        "metareview": "",
        "pdf_size": 605130,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4137491831327602703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Dartmouth College; Intel Labs; Department of Computer Science, Dartmouth College",
        "aff_domain": "cs.dartmouth.edu;intel.com;cs.dartmouth.edu",
        "email": "cs.dartmouth.edu;intel.com;cs.dartmouth.edu",
        "github": "",
        "project": "http://www.cs.dartmouth.edu/ haris/compression",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Dartmouth College;Intel Corporation",
        "aff_unique_dep": "Department of Computer Science;Intel Labs",
        "aff_unique_url": "https://dartmouth.edu;https://www.intel.com",
        "aff_unique_abbr": "Dartmouth;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Model the Tail",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9469",
        "id": "9469",
        "author_site": "Yu-Xiong Wang, Deva Ramanan, Martial Hebert",
        "author": "Yu-Xiong Wang; Deva Ramanan; Martial Hebert",
        "abstract": "We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings.  Here, the challenge is to learn accurate \"few-shot'' models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters.  Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the \"body'', and from the \"body'' to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting.",
        "bibtex": "@inproceedings{NIPS2017_147ebe63,\n author = {Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Model the Tail},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/147ebe637038ca50a1265abac8dea181-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/147ebe637038ca50a1265abac8dea181-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/147ebe637038ca50a1265abac8dea181-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/147ebe637038ca50a1265abac8dea181-Reviews.html",
        "metareview": "",
        "pdf_size": 2189215,
        "gs_citation": 835,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4875809028335803238&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/147ebe637038ca50a1265abac8dea181-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Pivot with Adversarial Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8892",
        "id": "8892",
        "author_site": "Gilles Louppe, Michael Kagan, Kyle Cranmer",
        "author": "Gilles Louppe; Michael Kagan; Kyle Cranmer",
        "abstract": "Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work,  we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.",
        "bibtex": "@inproceedings{NIPS2017_48ab2f9b,\n author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Pivot with Adversarial Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Reviews.html",
        "metareview": "",
        "pdf_size": 532816,
        "gs_citation": 316,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1302349342444410739&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "New York University; SLAC National Accelerator Laboratory; New York University",
        "aff_domain": "nyu.edu;slac.stanford.edu;nyu.edu",
        "email": "nyu.edu;slac.stanford.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/48ab2f9b45957ab574cf005eb8a76760-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "New York University;SLAC National Accelerator Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nyu.edu;https://www.slac.stanford.edu",
        "aff_unique_abbr": "NYU;SLAC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9262",
        "id": "9262",
        "author_site": "Xin Dong, Shangyu Chen, Sinno Pan",
        "author": "Xin Dong; Shangyu Chen; Sinno Pan",
        "abstract": "How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. By controlling layer-wise errors properly, one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods. Codes of our work are released at: https://github.com/csyhhu/L-OBS.",
        "bibtex": "@inproceedings{NIPS2017_c5dc3e08,\n author = {Dong, Xin and Chen, Shangyu and Pan, Sinno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Reviews.html",
        "metareview": "",
        "pdf_size": 462078,
        "gs_citation": 620,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3026095024723003965&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "github": "https://github.com/csyhhu/L-OBS",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c5dc3e08849bec07e33ca353de62ea04-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Learning to See Physics via Visual De-animation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8813",
        "id": "8813",
        "author_site": "Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, Josh Tenenbaum",
        "author": "Jiajun Wu; Erika Lu; Pushmeet Kohli; Bill Freeman; Josh Tenenbaum",
        "abstract": "We introduce a paradigm for understanding physical scenes without human annotations. At the core of our system is a physical world representation that is first recovered by a perception module and then utilized by physics and graphics engines. During training, the perception module and the generative models learn by visual de-animation --- interpreting and reconstructing the visual information stream. During testing, the system first recovers the physical world state, and then uses the generative models for reasoning and future prediction.  Even more so than forward simulation, inverting a physics or graphics engine is a computationally hard problem; we overcome this challenge by using a convolutional inversion network. Our system quickly recognizes the physical world state from appearance and motion cues, and has the flexibility to incorporate both differentiable and non-differentiable physics and graphics engines. We evaluate our system on both synthetic and real datasets involving multiple physical scenes, and demonstrate that our system performs well on both physical state estimation and reasoning problems. We further show that the knowledge learned on the synthetic dataset generalizes to constrained real images.",
        "bibtex": "@inproceedings{NIPS2017_4c56ff4c,\n author = {Wu, Jiajun and Lu, Erika and Kohli, Pushmeet and Freeman, Bill and Tenenbaum, Josh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to See Physics via Visual De-animation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4c56ff4ce4aaf9573aa5dff913df997a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4c56ff4ce4aaf9573aa5dff913df997a-Reviews.html",
        "metareview": "",
        "pdf_size": 1165838,
        "gs_citation": 238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10624233443563026466&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "MIT CSAIL; University of Oxford; DeepMind; MIT CSAIL, Google Research; MIT CSAIL",
        "aff_domain": "mit.edu;oxford.ac.uk;deepmind.com;mit.edu;mit.edu",
        "email": "mit.edu;oxford.ac.uk;deepmind.com;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Oxford;DeepMind;MIT CSAIL, Google Research",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;;;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.ox.ac.uk;https://deepmind.com;",
        "aff_unique_abbr": "MIT CSAIL;Oxford;DeepMind;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;United Kingdom;"
    },
    {
        "title": "Learning with Average Top-k Loss",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8846",
        "id": "8846",
        "author_site": "Yanbo Fan, Siwei Lyu, Yiming Ying, Baogang Hu",
        "author": "Yanbo Fan; Siwei Lyu; Yiming Ying; Baogang Hu",
        "abstract": "In this work, we introduce the average top-$k$ (\\atk) loss as a new ensemble loss for supervised learning. The \\atk loss provides a natural generalization of the two widely used ensemble losses, namely the average loss and the maximum loss. Furthermore, the \\atk loss combines the advantages of them and can alleviate their corresponding drawbacks to better adapt to different data distributions. We show that the \\atk loss affords an intuitive interpretation that reduces the penalty of continuous and convex individual losses on correctly classified data. The \\atk loss can lead to convex optimization problems that can be solved effectively with conventional sub-gradient based method. We further study the Statistical Learning Theory of \\matk by establishing its classification calibration and statistical consistency of \\matk which provide useful insights on the practical choice of the  parameter $k$. We demonstrate the applicability of \\matk learning combined with different individual loss functions for binary and multi-class classification and regression using synthetic and real datasets.",
        "bibtex": "@inproceedings{NIPS2017_6c524f9d,\n author = {Fan, Yanbo and Lyu, Siwei and Ying, Yiming and Hu, Baogang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Average Top-k Loss},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6c524f9d5d7027454a783c841250ba71-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6c524f9d5d7027454a783c841250ba71-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6c524f9d5d7027454a783c841250ba71-Reviews.html",
        "metareview": "",
        "pdf_size": 545778,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3993632801765861118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University at Albany, SUNY + National Laboratory of Pattern Recognition, CASIA + University of Chinese Academy of Sciences (UCAS); Department of Computer Science, University at Albany, SUNY; Department of Mathematics and Statistics, University at Albany, SUNY; National Laboratory of Pattern Recognition, CASIA + University of Chinese Academy of Sciences (UCAS)",
        "aff_domain": "nlpr.ia.ac.cn;albany.edu;albany.edu;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;albany.edu;albany.edu;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html",
        "aff_unique_index": "0+1+2;0;0;1+2",
        "aff_unique_norm": "University at Albany, SUNY;Chinese Academy of Sciences, Institute of Automation;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Computer Science;National Laboratory of Pattern Recognition;",
        "aff_unique_url": "https://www.albany.edu;http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "UAlbany;CASIA;UCAS",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "Albany;",
        "aff_country_unique_index": "0+1+1;0;0;1+1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Learning with Bandit Feedback in Potential Games",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9407",
        "id": "9407",
        "author_site": "Am\u00e9lie H\u00e9liou, Johanne Cohen, Panayotis Mertikopoulos",
        "author": "Am\u00e9lie Heliou; Johanne Cohen; Panayotis Mertikopoulos",
        "abstract": "This paper examines the equilibrium convergence properties of no-regret learning with exponential weights in potential games. To establish convergence with minimal information requirements on the players' side, we focus on two frameworks: the semi-bandit case (where players have access to a noisy estimate of their payoff vectors, including strategies they did not play), and the bandit case (where players are only able to observe their in-game, realized payoffs). In the semi-bandit case, we show that the induced sequence of play converges almost surely to a Nash equilibrium at a quasi-exponential rate. In the bandit case, the same result holds for approximate Nash equilibria if we introduce a constant exploration factor that guarantees that action choice probabilities never become arbitrarily small. In particular, if the algorithm is run with a suitably decreasing exploration factor, the sequence of play converges to a bona fide Nash equilibrium with probability 1.",
        "bibtex": "@inproceedings{NIPS2017_39ae2ed1,\n author = {Heliou, Am\\'{e}lie and Cohen, Johanne and Mertikopoulos, Panayotis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Bandit Feedback in Potential Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Reviews.html",
        "metareview": "",
        "pdf_size": 341564,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17987254554773305932&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "LRI-CNRS, Universit\u00e9 Paris-Sud,Universit\u00e9 Paris-Saclay, France; LIX, Ecole Polytechnique, CNRS, AMIBio, Inria, Universit\u00e9 Paris-Saclay; Univ. Grenoble Alpes, CNRS, Inria, LIG, F-38000, Grenoble, France",
        "aff_domain": "lri.fr;polytechnique.edu;imag.fr",
        "email": "lri.fr;polytechnique.edu;imag.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/39ae2ed11b14a4ccb41d35e9d1ba5d11-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "LRI-CNRS, Universit\u00e9 Paris-Sud,Universit\u00e9 Paris-Saclay, France;LIX, Ecole Polytechnique, CNRS, AMIBio, Inria, Universit\u00e9 Paris-Saclay;Univ. Grenoble Alpes, CNRS, Inria, LIG, F-38000, Grenoble, France",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Learning with Feature Evolvable Streams",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8933",
        "id": "8933",
        "author_site": "Bo-Jian Hou, Lijun Zhang, Zhi-Hua Zhou",
        "author": "Bo-Jian Hou; Lijun Zhang; Zhi-Hua Zhou",
        "abstract": "Learning with streaming data has attracted much attention during the past few years.Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited lifespan sensors will change when these sensors are substituted by new ones. In this paper, we propose a novel learning paradigm: Feature Evolvable Streaming Learning where old features would vanish and new features would occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with the assistance of old features, the performance on new features can be improved. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal.",
        "bibtex": "@inproceedings{NIPS2017_7634ea65,\n author = {Hou, Bo-Jian and Zhang, Lijun and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Feature Evolvable Streams},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7634ea65a4e6d9041cfd3f7de18e334a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7634ea65a4e6d9041cfd3f7de18e334a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7634ea65a4e6d9041cfd3f7de18e334a-Reviews.html",
        "metareview": "",
        "pdf_size": 1035483,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13684248617928063976&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China",
        "aff_domain": "lamda.nju.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9099",
        "id": "9099",
        "author_site": "Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu",
        "author": "Guolin Ke; Qi Meng; Thomas Finley; Taifeng Wang; Wei Chen; Weidong Ma; Qiwei Ye; Tie-Yan Liu",
        "abstract": "Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \\emph{Gradient-based One-Side Sampling} (GOSS) and \\emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \\emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.",
        "bibtex": "@inproceedings{NIPS2017_6449f44a,\n author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Reviews.html",
        "metareview": "",
        "pdf_size": 366352,
        "gs_citation": 18091,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4012588654259138758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Microsoft Research; Peking University; Microsoft Redmond; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html",
        "aff_unique_index": "0;1;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft Corporation;Peking University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;http://www.pku.edu.cn",
        "aff_unique_abbr": "MSR;Peking U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9137",
        "id": "9137",
        "author": "Yossi Arjevani",
        "abstract": "We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sums  problems. First, we show that perhaps surprisingly, the finite sum structure, by itself, is not sufficient for obtaining a complexity bound of $\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly convex finite sums - one must also know exactly which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sums algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' complexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing $L$-smooth and non-strongly convex finite sums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming that (on average) the same update rule is used for any iteration, and $\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise.",
        "bibtex": "@inproceedings{NIPS2017_a00e5eb0,\n author = {Arjevani, Yossi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a00e5eb0973d24649a4a920fc53d9564-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a00e5eb0973d24649a4a920fc53d9564-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a00e5eb0973d24649a4a920fc53d9564-Reviews.html",
        "metareview": "",
        "pdf_size": 312346,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2513203709875804196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel",
        "aff_domain": "weizmann.ac.il",
        "email": "weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a00e5eb0973d24649a4a920fc53d9564-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "title": "Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9390",
        "id": "9390",
        "author_site": "Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, Yuanzhi Li",
        "author": "Zeyuan Allen-Zhu; Elad Hazan; Wei Hu; Yuanzhi Li",
        "abstract": "We propose a rank-k variant of the classical Frank-Wolfe algorithm to solve convex optimization over a trace-norm ball. Our algorithm replaces the top singular-vector computation (1-SVD) in Frank-Wolfe with a top-k singular-vector computation (k-SVD), which can be done by repeatedly applying 1-SVD k times. Alternatively, our algorithm can be viewed as a rank-k restricted version of projected gradient descent. We show that our algorithm has a linear convergence rate when the objective function is smooth and strongly convex, and the optimal solution has rank at most k. This improves the convergence rate and the total time complexity of the Frank-Wolfe method and its variants.",
        "bibtex": "@inproceedings{NIPS2017_8b838818,\n author = {Allen-Zhu, Zeyuan and Hazan, Elad and Hu, Wei and Li, Yuanzhi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8b8388180314a337c9aa3c5aa8e2f37a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8b8388180314a337c9aa3c5aa8e2f37a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8b8388180314a337c9aa3c5aa8e2f37a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8b8388180314a337c9aa3c5aa8e2f37a-Reviews.html",
        "metareview": "",
        "pdf_size": 527022,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15274377099173581558&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Microsoft Research, Redmond; Princeton University; Princeton University; Princeton University",
        "aff_domain": "csail.mit.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "csail.mit.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "https://arxiv.org/abs/1708.02105",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8b8388180314a337c9aa3c5aa8e2f37a-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Microsoft Research;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.princeton.edu",
        "aff_unique_abbr": "MSR;Princeton",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Redmond;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Linear Time Computation of Moments in Sum-Product Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9456",
        "id": "9456",
        "author_site": "Han Zhao, Geoffrey Gordon",
        "author": "Han Zhao; Geoffrey J. Gordon",
        "abstract": "Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear. We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs.",
        "bibtex": "@inproceedings{NIPS2017_473447ac,\n author = {Zhao, Han and Gordon, Geoffrey J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear Time Computation of Moments in Sum-Product Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/473447ac58e1cd7e96172575f48dca3b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/473447ac58e1cd7e96172575f48dca3b-Reviews.html",
        "metareview": "",
        "pdf_size": 337627,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18134010668631546689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/473447ac58e1cd7e96172575f48dca3b-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Linear regression without correspondence",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8944",
        "id": "8944",
        "author_site": "Daniel Hsu, Kevin Shi, Xiaorui Sun",
        "author": "Daniel J. Hsu; Kevin Shi; Xiaorui Sun",
        "abstract": "This article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. First, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. Next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. Finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.",
        "bibtex": "@inproceedings{NIPS2017_c32d9bf2,\n author = {Hsu, Daniel J and Shi, Kevin and Sun, Xiaorui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear regression without correspondence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c32d9bf27a3da7ec8163957080c8628e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c32d9bf27a3da7ec8163957080c8628e-Reviews.html",
        "metareview": "",
        "pdf_size": 613224,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17865379055204271215&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Columbia University; Columbia University; Microsoft Research",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c32d9bf27a3da7ec8163957080c8628e-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Columbia University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.columbia.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Columbia;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Linearly constrained Gaussian processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8914",
        "id": "8914",
        "author_site": "Carl Jidling, Niklas Wahlstr\u00f6m, Adrian Wills, Thomas Sch\u00f6n",
        "author": "Carl Jidling; Niklas Wahlstr\u00f6m; Adrian Wills; Thomas B Sch\u00f6n",
        "abstract": "We consider a modification of the covariance function in Gaussian processes to correctly account for known linear constraints. By modelling the target function as a transformation of an underlying function, the constraints are explicitly incorporated in the model such that they are guaranteed to be fulfilled by any sample drawn or prediction made. We also propose a constructive procedure for designing the transformation operator and illustrate the result on both simulated and real-data examples.",
        "bibtex": "@inproceedings{NIPS2017_71ad16ad,\n author = {Jidling, Carl and Wahlstr\\\"{o}m, Niklas and Wills, Adrian and Sch\\\"{o}n, Thomas B},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linearly constrained Gaussian processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/71ad16ad2c4d81f348082ff6c4b20768-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/71ad16ad2c4d81f348082ff6c4b20768-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/71ad16ad2c4d81f348082ff6c4b20768-Reviews.html",
        "metareview": "",
        "pdf_size": 685718,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7639238739567410251&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Information Technology, Uppsala University, Sweden; Department of Information Technology, Uppsala University, Sweden; School of Engineering, University of Newcastle, Australia; Department of Information Technology, Uppsala University, Sweden",
        "aff_domain": "it.uu.se;it.uu.se;newcastle.edu.au;it.uu.se",
        "email": "it.uu.se;it.uu.se;newcastle.edu.au;it.uu.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/71ad16ad2c4d81f348082ff6c4b20768-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Uppsala University;School of Engineering, University of Newcastle, Australia",
        "aff_unique_dep": "Department of Information Technology;",
        "aff_unique_url": "https://www.uu.se;",
        "aff_unique_abbr": "UU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Sweden;"
    },
    {
        "title": "Local Aggregative Games",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9309",
        "id": "9309",
        "author_site": "Vikas Garg, Tommi Jaakkola",
        "author": "Vikas Garg; Tommi Jaakkola",
        "abstract": "Aggregative games provide a rich abstraction to model strategic multi-agent interactions. We focus on learning local aggregative games, where the payoff of each player is a function of its own action and the aggregate behavior of its neighbors in a connected digraph. We show the existence of a pure strategy epsilon-Nash equilibrium in such games when the payoff functions are convex or sub-modular. We prove an information theoretic lower bound, in a value oracle model, on approximating the structure of the digraph with non-negative monotone sub-modular cost functions on the edge set cardinality. We also introduce gamma-aggregative games that generalize local aggregative games, and admit epsilon-Nash equilibrium that are stable with respect to small changes in some specified graph property. Moreover, we provide estimation algorithms for the game theoretic model that can meaningfully recover the underlying structure and payoff functions from real voting data.",
        "bibtex": "@inproceedings{NIPS2017_44ac09ac,\n author = {Garg, Vikas and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Local Aggregative Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/44ac09ac6a149136a4102ee4b4103ae6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/44ac09ac6a149136a4102ee4b4103ae6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/44ac09ac6a149136a4102ee4b4103ae6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/44ac09ac6a149136a4102ee4b4103ae6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/44ac09ac6a149136a4102ee4b4103ae6-Reviews.html",
        "metareview": "",
        "pdf_size": 454306,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17152834995155449130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/44ac09ac6a149136a4102ee4b4103ae6-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8970",
        "id": "8970",
        "author_site": "Liangpeng Zhang, Ke Tang, Xin Yao",
        "author": "Liangpeng Zhang; Ke Tang; Xin Yao",
        "abstract": "Under/overestimation of state/action values are harmful for reinforcement learning agents. In this paper, we show that a state/action value estimated using the Bellman equation can be decomposed to a weighted sum of path-wise values that follow log-normal distributions. Since log-normal distributions are skewed, the distribution of estimated state/action values can also be skewed, leading to an imbalanced likelihood of under/overestimation. The degree of such imbalance can vary greatly among actions and policies within a single problem instance, making the agent prone to select actions/policies that have inferior expected return and higher likelihood of overestimation. We present a comprehensive analysis to such skewness, examine its factors and impacts through both theoretical and empirical results, and discuss the possible ways to reduce its undesirable effects.",
        "bibtex": "@inproceedings{NIPS2017_69a5b599,\n author = {Zhang, Liangpeng and Tang, Ke and Yao, Xin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/69a5b5995110b36a9a347898d97a610e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/69a5b5995110b36a9a347898d97a610e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/69a5b5995110b36a9a347898d97a610e-Reviews.html",
        "metareview": "",
        "pdf_size": 496568,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8792721849743379146&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/69a5b5995110b36a9a347898d97a610e-Abstract.html"
    },
    {
        "title": "Lookahead  Bayesian Optimization with Inequality Constraints",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8978",
        "id": "8978",
        "author_site": "Remi Lam, Karen Willcox",
        "author": "Remi Lam; Karen Willcox",
        "abstract": "We consider the task of optimizing an objective function subject to inequality constraints when both the objective and the constraints are expensive to evaluate. Bayesian optimization (BO) is a popular way to tackle optimization problems with expensive objective function evaluations, but has mostly been applied to unconstrained problems. Several BO approaches have been proposed to address expensive constraints but are limited to greedy strategies maximizing immediate reward. To address this limitation, we propose a lookahead approach that selects the next evaluation in order to maximize the long-term feasible reduction of the objective function. We present numerical experiments demonstrating the performance improvements of such a lookahead approach compared to several greedy BO algorithms, including constrained expected improvement (EIC) and predictive entropy search with constraint (PESC).",
        "bibtex": "@inproceedings{NIPS2017_83f97f48,\n author = {Lam, Remi and Willcox, Karen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lookahead  Bayesian Optimization with Inequality Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/83f97f4825290be4cb794ec6a234595f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/83f97f4825290be4cb794ec6a234595f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/83f97f4825290be4cb794ec6a234595f-Reviews.html",
        "metareview": "",
        "pdf_size": 497548,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12013634604684718050&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/83f97f4825290be4cb794ec6a234595f-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Lower bounds on the robustness to adversarial perturbations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8875",
        "id": "8875",
        "author_site": "Jonathan Peck, Joris Roels, Bart Goossens, Yvan Saeys",
        "author": "Jonathan Peck; Joris Roels; Bart Goossens; Yvan Saeys",
        "abstract": "The input-output mappings learned by state-of-the-art neural networks are significantly discontinuous. It is possible to cause a neural network used for image recognition to misclassify its input by applying very specific, hardly perceptible perturbations to the input, called adversarial perturbations. Many hypotheses have been proposed to explain the existence of these peculiar samples as well as several methods to mitigate them. A proven explanation remains elusive, however. In this work, we take steps towards a formal characterization of adversarial perturbations by deriving lower bounds on the magnitudes of perturbations necessary to change the classification of neural networks. The bounds are experimentally verified on the MNIST and CIFAR-10 data sets.",
        "bibtex": "@inproceedings{NIPS2017_298f95e1,\n author = {Peck, Jonathan and Roels, Joris and Goossens, Bart and Saeys, Yvan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lower bounds on the robustness to adversarial perturbations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/298f95e1bf9136124592c8d4825a06fc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/298f95e1bf9136124592c8d4825a06fc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/298f95e1bf9136124592c8d4825a06fc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/298f95e1bf9136124592c8d4825a06fc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/298f95e1bf9136124592c8d4825a06fc-Reviews.html",
        "metareview": "",
        "pdf_size": 361659,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7338416494202727538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/298f95e1bf9136124592c8d4825a06fc-Abstract.html"
    },
    {
        "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9008",
        "id": "9008",
        "author_site": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnabas Poczos",
        "author": "Chun-Liang Li; Wei-Cheng Chang; Yu Cheng; Yiming Yang; Barnabas Poczos",
        "abstract": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing {\\it adversarial kernel learning} techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak$^*$ topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.",
        "bibtex": "@inproceedings{NIPS2017_dfd7468a,\n author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and Poczos, Barnabas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MMD GAN: Towards Deeper Understanding of Moment Matching Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dfd7468ac613286cdbb40872c8ef3b06-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dfd7468ac613286cdbb40872c8ef3b06-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dfd7468ac613286cdbb40872c8ef3b06-Reviews.html",
        "metareview": "",
        "pdf_size": 8872612,
        "gs_citation": 924,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13688446556700022690&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; AI Foundations, IBM Research; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;us.ibm.com;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;us.ibm.com;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dfd7468ac613286cdbb40872c8ef3b06-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;AI Foundations, IBM Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8810",
        "id": "8810",
        "author_site": "Peva Blanchard, El Mahdi El-Mhamdi, Rachid Guerraoui, Julien Stainer",
        "author": "Peva Blanchard; El Mahdi El Mhamdi; Rachid Guerraoui; Julien Stainer",
        "abstract": "We study the resilience  to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially  arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of $n$ workers, up to $f$  being Byzantine,  we ask how resilient  can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience  property of the aggregation rule capturing the basic requirements to guarantee convergence despite $f$   Byzantine workers. We propose \\emph{Krum}, an aggregation rule that satisfies our resilience  property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.",
        "bibtex": "@inproceedings{NIPS2017_f4b9ec30,\n author = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Reviews.html",
        "metareview": "",
        "pdf_size": 356077,
        "gs_citation": 2435,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14556431758739127101&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "EPFL, Switzerland; EPFL, Switzerland; EPFL, Switzerland; EPFL, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Mapping distinct timescales of functional interactions among brain networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9191",
        "id": "9191",
        "author_site": "Mali Sundaresan, Arshed Nabeel, Devarajan Sridharan",
        "author": "Mali Sundaresan; Arshed Nabeel; Devarajan Sridharan",
        "abstract": "Brain processes occur at various timescales, ranging from milliseconds (neurons) to minutes and hours (behavior). Characterizing functional coupling among brain regions at these diverse timescales is key to understanding how the brain produces behavior. Here, we apply instantaneous and lag-based measures of conditional linear dependence, based on Granger-Geweke causality (GC), to infer network connections at distinct timescales from functional magnetic resonance imaging (fMRI) data. Due to the slow sampling rate of fMRI, it is widely held that GC produces spurious and unreliable estimates of functional connectivity when applied to fMRI data. We challenge this claim with simulations and a novel machine learning approach. First, we show, with simulated fMRI data, that instantaneous and lag-based GC identify distinct timescales and complementary patterns of functional connectivity. Next, we analyze fMRI scans from 500 subjects and show that a linear classifier trained on either instantaneous or lag-based GC connectivity reliably distinguishes task versus rest brain states, with ~80-85% cross-validation accuracy. Importantly, instantaneous and lag-based GC exploit markedly different spatial and temporal patterns of connectivity to achieve robust classification. Our approach enables identifying functionally connected networks that operate at distinct timescales in the brain.",
        "bibtex": "@inproceedings{NIPS2017_8929c70f,\n author = {Sundaresan, Mali and Nabeel, Arshed and Sridharan, Devarajan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mapping distinct timescales of functional interactions among brain networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8929c70f8d710e412d38da624b21c3c8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8929c70f8d710e412d38da624b21c3c8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8929c70f8d710e412d38da624b21c3c8-Reviews.html",
        "metareview": "",
        "pdf_size": 1614122,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8981473434860049598&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Neuroscience, Indian Institute of Science, Bangalore + Department of Computer Science and Automation, Indian Institute of Science, Bangalore; Department of Computer Science and Automation, Indian Institute of Science, Bangalore; Center for Neuroscience, Indian Institute of Science, Bangalore + Department of Computer Science and Automation, Indian Institute of Science, Bangalore",
        "aff_domain": "gmail.com;iisc.ac.in;iisc.ac.in",
        "email": "gmail.com;iisc.ac.in;iisc.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html",
        "aff_unique_index": "0+1;1;0+1",
        "aff_unique_norm": "Center for Neuroscience, Indian Institute of Science, Bangalore;Indian Institute of Science",
        "aff_unique_dep": ";Department of Computer Science and Automation",
        "aff_unique_url": ";https://www.iisc.ac.in",
        "aff_unique_abbr": ";IISc",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Bangalore",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";India"
    },
    {
        "title": "MarrNet: 3D Shape Reconstruction via 2.5D Sketches",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8850",
        "id": "8850",
        "author_site": "Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill Freeman, Josh Tenenbaum",
        "author": "Jiajun Wu; Yifan Wang; Tianfan Xue; Xingyuan Sun; Bill Freeman; Josh Tenenbaum",
        "abstract": "3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenge for learning-based approaches, as 3D object annotations in real images are scarce. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from the domain adaptation issue when tested on real data.  In this work, we propose an end-to-end trainable framework, sequentially estimating 2.5D sketches and 3D object shapes. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image, and to transfer from synthetic to real data. Second, for 3D reconstruction from the 2.5D sketches, we can easily transfer the learned model on synthetic data to real images, as rendered 2.5D sketches are invariant to object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches, making the framework end-to-end trainable on real images, requiring no real-image annotations. Our framework achieves state-of-the-art performance on 3D shape reconstruction.",
        "bibtex": "@inproceedings{NIPS2017_ad972f10,\n author = {Wu, Jiajun and Wang, Yifan and Xue, Tianfan and Sun, Xingyuan and Freeman, Bill and Tenenbaum, Josh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MarrNet: 3D Shape Reconstruction via 2.5D Sketches},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ad972f10e0800b49d76fed33a21f6698-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ad972f10e0800b49d76fed33a21f6698-Reviews.html",
        "metareview": "",
        "pdf_size": 6556686,
        "gs_citation": 536,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9068422335111880777&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "MIT CSAIL; ShanghaiTech University; MIT CSAIL; Shanghai Jiao Tong University; MIT CSAIL, Google Research; MIT CSAIL",
        "aff_domain": "mit.edu;shanghaitech.edu.cn;mit.edu;sjtu.edu.cn;google.com;mit.edu",
        "email": "mit.edu;shanghaitech.edu.cn;mit.edu;sjtu.edu.cn;google.com;mit.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html",
        "aff_unique_index": "0;1;0;2;3;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;ShanghaiTech University;Shanghai Jiao Tong University;MIT CSAIL, Google Research",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;;;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.shanghaitech.edu.cn;https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "MIT CSAIL;ShanghaiTech;SJTU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "United States;China;"
    },
    {
        "title": "MaskRNN: Instance Level Video Object Segmentation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8829",
        "id": "8829",
        "author_site": "Yuan-Ting Hu, Jia-Bin Huang, Alex Schwing",
        "author": "Yuan-Ting Hu; Jia-Bin Huang; Alexander Schwing",
        "abstract": "Instance level video object segmentation is an important technique for video editing and compression. To capture the temporal coherence, in this paper, we develop MaskRNN, a recurrent neural net approach which fuses in each frame the output of two deep nets for each object instance - a binary segmentation net providing a mask and a localization net providing a bounding box. Due to the recurrent component and the localization component, our method is able to take advantage of long-term temporal structures of the video data as well as rejecting outliers. We validate the proposed algorithm on three challenging benchmark datasets, the DAVIS-2016 dataset, the DAVIS-2017 dataset, and the Segtrack v2 dataset, achieving state-of-the-art performance on all of them.",
        "bibtex": "@inproceedings{NIPS2017_6c9882bb,\n author = {Hu, Yuan-Ting and Huang, Jia-Bin and Schwing, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MaskRNN: Instance Level Video Object Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6c9882bbac1c7093bd25041881277658-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6c9882bbac1c7093bd25041881277658-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6c9882bbac1c7093bd25041881277658-Reviews.html",
        "metareview": "",
        "pdf_size": 12821148,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17148526768420846958&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "UIUC; Virginia Tech; UIUC",
        "aff_domain": "illinois.edu;vt.edu;illinois.edu",
        "email": "illinois.edu;vt.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6c9882bbac1c7093bd25041881277658-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Virginia Tech",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www illinois.edu;https://www.vt.edu",
        "aff_unique_abbr": "UIUC;VT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Masked Autoregressive Flow for Density Estimation",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9021",
        "id": "9021",
        "author_site": "George Papamakarios, Iain Murray, Theo Pavlakou",
        "author": "George Papamakarios; Theo Pavlakou; Iain Murray",
        "abstract": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.",
        "bibtex": "@inproceedings{NIPS2017_6c1da886,\n author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Masked Autoregressive Flow for Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Reviews.html",
        "metareview": "",
        "pdf_size": 561483,
        "gs_citation": 1735,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11200642106543542089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Edinburgh; University of Edinburgh; University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Matching neural paths: transfer from recognition to correspondence search",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8913",
        "id": "8913",
        "author_site": "Nikolay Savinov, Lubor Ladicky, Marc Pollefeys",
        "author": "Nikolay Savinov; Lubor Ladicky; Marc Pollefeys",
        "abstract": "Many machine learning tasks require finding per-part correspondences between objects. In this work we focus on low-level correspondences --- a highly ambiguous matching problem. We propose to use a hierarchical semantic representation of the objects, coming from a convolutional neural network, to solve this ambiguity. Training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain. We show how transfer from recognition can be used to avoid such training. Our idea is to mark parts as \"matching\" if their features are close to each other at all the levels of convolutional feature hierarchy (neural paths). Although the overall number of such paths is exponential in the number of layers, we propose a polynomial algorithm for aggregating all of them in a single backward pass. The empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data.",
        "bibtex": "@inproceedings{NIPS2017_a8ecbaba,\n author = {Savinov, Nikolay and Ladicky, Lubor and Pollefeys, Marc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matching neural paths: transfer from recognition to correspondence search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a8ecbabae151abacba7dbde04f761c37-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a8ecbabae151abacba7dbde04f761c37-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a8ecbabae151abacba7dbde04f761c37-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a8ecbabae151abacba7dbde04f761c37-Reviews.html",
        "metareview": "",
        "pdf_size": 3206414,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16422199516119717822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science at ETH Zurich; Department of Computer Science at ETH Zurich; Department of Computer Science at ETH Zurich + Microsoft",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a8ecbabae151abacba7dbde04f761c37-Abstract.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "ETH Zurich;Microsoft Corporation",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ethz.ch;https://www.microsoft.com",
        "aff_unique_abbr": "ETHZ;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Matching on Balanced Nonlinear Representations for Treatment Effects Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8887",
        "id": "8887",
        "author_site": "Sheng Li, Yun Fu",
        "author": "Sheng Li; Yun Fu",
        "abstract": "Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classification problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be significantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.",
        "bibtex": "@inproceedings{NIPS2017_b2eeb736,\n author = {Li, Sheng and Fu, Yun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matching on Balanced Nonlinear Representations for Treatment Effects Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Reviews.html",
        "metareview": "",
        "pdf_size": 361097,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7403776824594014208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Adobe Research, San Jose, CA; Northeastern University, Boston, MA",
        "aff_domain": "adobe.com;ece.neu.edu",
        "email": "adobe.com;ece.neu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b2eeb7362ef83deff5c7813a67e14f0a-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Adobe Research;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://research.adobe.com;https://www.northeastern.edu",
        "aff_unique_abbr": "Adobe;NEU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "San Jose;Boston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Matrix Norm Estimation from a Few Entries",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9412",
        "id": "9412",
        "author_site": "Ashish Khetan, Sewoong Oh",
        "author": "Ashish Khetan; Sewoong Oh",
        "abstract": "Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis,  we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering various spectral properties of the underlying matrix from a sampling of its entries. We propose a framework of first estimating the Schatten $k$-norms of a matrix for several values of $k$, and using these as surrogates for estimating spectral properties of interest, such as   the spectrum itself or the rank.  This paper focuses on the technical challenges in accurately estimating the Schatten norms from a sampling of a matrix. We introduce  a novel unbiased estimator based on counting small structures in a graph and provide  guarantees that match its empirical performances.  Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.",
        "bibtex": "@inproceedings{NIPS2017_89d4402d,\n author = {Khetan, Ashish and Oh, Sewoong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matrix Norm Estimation from a Few Entries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/89d4402dc03d3b7318bbac10203034ab-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/89d4402dc03d3b7318bbac10203034ab-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/89d4402dc03d3b7318bbac10203034ab-Reviews.html",
        "metareview": "",
        "pdf_size": 1160696,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7524897319793859643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of ISE, University of Illinois Urbana-Champaign; Department of ISE, University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/89d4402dc03d3b7318bbac10203034ab-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of ISE, University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Max-Margin Invariant Features from Transformed Unlabelled Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8935",
        "id": "8935",
        "author_site": "Dipan Pal, Ashwin Kannan, Gautam Arakalgud, Marios Savvides",
        "author": "Dipan Pal; Ashwin Kannan; Gautam Arakalgud; Marios Savvides",
        "abstract": "The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to a unitary group while having theoretical guarantees in addressing the important practical issue of unavailability of transformed versions of labelled data. A problem we call the Unlabeled Transformation Problem which is a special form of semi-supervised learning and one-shot learning. We present a theoretically motivated alternate approach to the invariant kernel SVM based on which we propose Max-Margin Invariant Features (MMIF) to solve this problem. As an illustration, we design an framework for face recognition and demonstrate the efficacy of our approach on a large scale semi-synthetic dataset with 153,000 images and a new challenging protocol on Labelled Faces in the Wild (LFW) while out-performing strong baselines.",
        "bibtex": "@inproceedings{NIPS2017_6d0f8463,\n author = {Pal, Dipan and Kannan, Ashwin and Arakalgud, Gautam and Savvides, Marios},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Max-Margin Invariant Features from Transformed Unlabelled Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6d0f846348a856321729a2f36734d1a7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6d0f846348a856321729a2f36734d1a7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6d0f846348a856321729a2f36734d1a7-Reviews.html",
        "metareview": "",
        "pdf_size": 603028,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18381397943475159903&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering, Carnegie Mellon University; Department of Electrical and Computer Engineering, Carnegie Mellon University; Department of Electrical and Computer Engineering, Carnegie Mellon University; Department of Electrical and Computer Engineering, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu;cmu.edu;cmu.edu",
        "email": "cmu.edu;cmu.edu;cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6d0f846348a856321729a2f36734d1a7-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9316",
        "id": "9316",
        "author_site": "Jinseok Nam, Eneldo Loza Menc\u00eda, Hyunwoo J Kim, Johannes F\u00fcrnkranz",
        "author": "Jinseok Nam; Eneldo Loza Menc\u00eda; Hyunwoo J Kim; Johannes F\u00fcrnkranz",
        "abstract": "Multi-label classification is the task of predicting a set of labels for a given input instance. Classifier chains are a state-of-the-art method for tackling such problems, which essentially converts this problem into a sequential prediction problem, where the labels are first ordered in an arbitrary fashion, and the task is to predict a sequence of binary values for these labels. In this paper, we replace classifier chains with recurrent neural networks, a sequence-to-sequence prediction algorithm which has recently been successfully applied to sequential prediction tasks in many domains. The key advantage of this approach is that it allows to focus on the prediction of the positive labels only, a much smaller set than the full set of possible labels. Moreover, parameter sharing across all classifiers allows to better exploit information of previous decisions. As both, classifier chains and recurrent neural networks depend on a fixed ordering of the labels, which is typically not part of a multi-label problem specification, we also compare different ways of ordering the label set, and give some recommendations on suitable ordering strategies.",
        "bibtex": "@inproceedings{NIPS2017_2eb5657d,\n author = {Nam, Jinseok and Loza Menc\\'{\\i}a, Eneldo and Kim, Hyunwoo J and F\\\"{u}rnkranz, Johannes},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2eb5657d37f474e4c4cf01e4882b8962-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2eb5657d37f474e4c4cf01e4882b8962-Reviews.html",
        "metareview": "",
        "pdf_size": 670516,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15026336315272636279&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2eb5657d37f474e4c4cf01e4882b8962-Abstract.html"
    },
    {
        "title": "Maximum Margin Interval Trees",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9271",
        "id": "9271",
        "author_site": "Alexandre Drouin, Toby Hocking, Francois Laviolette",
        "author": "Alexandre Drouin; Toby Hocking; Francois Laviolette",
        "abstract": "Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.",
        "bibtex": "@inproceedings{NIPS2017_2288f691,\n author = {Drouin, Alexandre and Hocking, Toby and Laviolette, Francois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximum Margin Interval Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Reviews.html",
        "metareview": "",
        "pdf_size": 663728,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17638985653174080082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel, Universit\u00e9 Laval, Qu\u00e9bec, Canada; McGill Genome Center, McGill University, Montr\u00e9al, Canada; D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel, Universit\u00e9 Laval, Qu\u00e9bec, Canada",
        "aff_domain": "ulaval.ca;r-project.org;ift.ulaval.ca",
        "email": "ulaval.ca;r-project.org;ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Universit\u00e9 Laval;McGill Genome Center, McGill University, Montr\u00e9al, Canada",
        "aff_unique_dep": "D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel;",
        "aff_unique_url": "https://www.universite-laval.ca;",
        "aff_unique_abbr": "UL;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Qu\u00e9bec;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "title": "Maxing and Ranking with Few Assumptions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9472",
        "id": "9472",
        "author_site": "Moein Falahatgar, Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati, Vaishakh Ravindrakumar",
        "author": "Moein Falahatgar; Yi Hao; Alon Orlitsky; Venkatadheeraj Pichapati; Vaishakh Ravindrakumar",
        "abstract": "PAC maximum                                                                       selection (maxing) and ranking of $n$ elements via random pairwise comparisons have diverse applications and have been studied under many models and assumptions. With just one simple natural assumption: strong stochastic transitivity, we show that maxing can be performed with linearly many comparisons yet ranking requires quadratically many.                                                                  With no assumptions at all, we show that for the Borda-score metric, maximum selection can be performed with linearly many comparisons and ranking can be performed with $\\mathcal{O}(n\\log n)$ comparisons.",
        "bibtex": "@inproceedings{NIPS2017_db98dc0d,\n author = {Falahatgar, Moein and Hao, Yi and Orlitsky, Alon and Pichapati, Venkatadheeraj and Ravindrakumar, Vaishakh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maxing and Ranking with Few Assumptions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/db98dc0dbafde48e8f74c0de001d35e4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/db98dc0dbafde48e8f74c0de001d35e4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/db98dc0dbafde48e8f74c0de001d35e4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/db98dc0dbafde48e8f74c0de001d35e4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/db98dc0dbafde48e8f74c0de001d35e4-Reviews.html",
        "metareview": "",
        "pdf_size": 820992,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14863742717044803945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, San Deigo; University of California, San Deigo; University of California, San Deigo; University of California, San Deigo; University of California, San Deigo",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/db98dc0dbafde48e8f74c0de001d35e4-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, San Deigo",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Mean Field Residual Networks: On the Edge of Chaos",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9071",
        "id": "9071",
        "author_site": "Ge Yang, Samuel Schoenholz",
        "author": "Ge Yang; Samuel Schoenholz",
        "abstract": "We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the \"edge of chaos\" hypothesis, these subexponential and polynomial laws allow residual networks to \"hover over the boundary between stability and chaos,\" thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.",
        "bibtex": "@inproceedings{NIPS2017_81c650ca,\n author = {Yang, Ge and Schoenholz, Samuel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mean Field Residual Networks: On the Edge of Chaos},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Reviews.html",
        "metareview": "",
        "pdf_size": 1907224,
        "gs_citation": 225,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9326324086433018808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research AI + Harvard University; Google Brain",
        "aff_domain": "microsoft.com;google.com",
        "email": "microsoft.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Microsoft Research;Harvard University;Google",
        "aff_unique_dep": "AI;;Google Brain",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.harvard.edu;https://brain.google.com",
        "aff_unique_abbr": "MSR;Harvard;Google Brain",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8912",
        "id": "8912",
        "author_site": "Antti Tarvainen, Harri Valpola",
        "author": "Antti Tarvainen; Harri Valpola",
        "abstract": "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.",
        "bibtex": "@inproceedings{NIPS2017_68053af2,\n author = {Tarvainen, Antti and Valpola, Harri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Reviews.html",
        "metareview": "",
        "pdf_size": 1087729,
        "gs_citation": 6860,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3256042804843589088&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The Curious AI Company; The Curious AI Company",
        "aff_domain": "cai.fi;cai.fi",
        "email": "cai.fi;cai.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Curious AI Company",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Min-Max Propagation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9331",
        "id": "9331",
        "author_site": "Christopher Srinivasa, Inmar Givoni, Siamak Ravanbakhsh, Brendan J Frey",
        "author": "Christopher Srinivasa; Inmar Givoni; Siamak Ravanbakhsh; Brendan J. Frey",
        "abstract": "We study the application of min-max propagation, a variation of belief propagation, for approximate min-max inference in factor graphs. We show that for \u201cany\u201d high-order function that can be minimized in O(\u03c9), the min-max message update can be obtained using an efficient O(K(\u03c9 + log(K)) procedure, where K is the number of variables. We demonstrate how this generic procedure, in combination with efficient updates for a family of high-order constraints, enables the application of min-max propagation to efficiently approximate the NP-hard problem of makespan minimization, which seeks to distribute a set of tasks on machines, such that the worst case load is minimized.",
        "bibtex": "@inproceedings{NIPS2017_327708dd,\n author = {Srinivasa, Christopher and Givoni, Inmar and Ravanbakhsh, Siamak and Frey, Brendan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Min-Max Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/327708dd10d68b1361ad3addbaca01f2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/327708dd10d68b1361ad3addbaca01f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/327708dd10d68b1361ad3addbaca01f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/327708dd10d68b1361ad3addbaca01f2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/327708dd10d68b1361ad3addbaca01f2-Reviews.html",
        "metareview": "",
        "pdf_size": 398588,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8844262012887412911&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Toronto + Borealis AI; University of Toronto; University of British Columbia; University of Toronto + Vector Institute + Deep Genomics",
        "aff_domain": "gmail.com;gmail.com;cs.ubc.ca;psi.toronto.edu",
        "email": "gmail.com;gmail.com;cs.ubc.ca;psi.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/327708dd10d68b1361ad3addbaca01f2-Abstract.html",
        "aff_unique_index": "0+1;0;2;0+3+4",
        "aff_unique_norm": "University of Toronto;Borealis AI;University of British Columbia;Vector Institute;Deep Genomics",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.borealisai.com;https://www.ubc.ca;https://vectorinstitute.ai/;https://www.deepgenomics.com",
        "aff_unique_abbr": "U of T;Borealis AI;UBC;Vector Institute;Deep Genomics",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Minimal Exploration in Structured Stochastic Bandits",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8966",
        "id": "8966",
        "author_site": "Richard Combes, Stefan Magureanu, Alexandre Proutiere",
        "author": "Richard Combes; Stefan Magureanu; Alexandre Proutiere",
        "abstract": "This paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties. Most existing structures (e.g. linear, lipschitz, unimodal, combinatorial, dueling,...) are covered by our framework. We derive an asymptotic instance-specific regret lower bound for these problems, and develop OSSB, an algorithm whose regret matches this fundamental limit. OSSB is not based on the classical principle of ``optimism in the face of uncertainty'' or on Thompson sampling, and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound. We illustrate the efficiency of OSSB using numerical experiments in the case of the linear bandit problem and show that OSSB outperforms existing algorithms, including Thompson sampling",
        "bibtex": "@inproceedings{NIPS2017_e19347e1,\n author = {Combes, Richard and Magureanu, Stefan and Proutiere, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimal Exploration in Structured Stochastic Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e19347e1c3ca0c0b97de5fb3b690855a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e19347e1c3ca0c0b97de5fb3b690855a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e19347e1c3ca0c0b97de5fb3b690855a-Reviews.html",
        "metareview": "",
        "pdf_size": 302534,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9579326560683466451&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Centrale-Supelec / L2S; KTH, EE School / ACL; KTH, EE School / ACL",
        "aff_domain": "supelec.fr;kth.se;kth.se",
        "email": "supelec.fr;kth.se;kth.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Centrale-Supelec / L2S;KTH, EE School / ACL",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Minimax Estimation of Bandable Precision Matrices",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9265",
        "id": "9265",
        "author_site": "Addison Hu, Sahand Negahban",
        "author": "Addison Hu; Sahand Negahban",
        "abstract": "The inverse covariance matrix provides considerable insight for understanding statistical models in the multivariate setting.  In particular, when the distribution over variables is assumed to be multivariate normal, the sparsity pattern in the inverse covariance matrix, commonly referred to as the precision matrix, corresponds to the adjacency matrix representation of the Gauss-Markov graph, which encodes conditional independence statements between variables.  Minimax results under the spectral norm have previously been established for covariance matrices, both sparse and banded, and for sparse precision matrices.  We establish minimax estimation bounds for estimating banded precision matrices under the spectral norm. Our results greatly improve upon the existing bounds; in particular, we find that the minimax rate for estimating banded precision matrices matches that of estimating banded covariance matrices.  The key insight in our analysis is that we are able to obtain barely-noisy estimates of $k \\times k$ subblocks of the precision matrix by inverting slightly wider blocks of the empirical covariance matrix along the diagonal.  Our theoretical results are complemented by experiments demonstrating the sharpness of our bounds.",
        "bibtex": "@inproceedings{NIPS2017_070dbb60,\n author = {Hu, Addison and Negahban, Sahand},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimax Estimation of Bandable Precision Matrices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/070dbb6024b5ef93784428afc71f2146-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/070dbb6024b5ef93784428afc71f2146-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/070dbb6024b5ef93784428afc71f2146-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/070dbb6024b5ef93784428afc71f2146-Reviews.html",
        "metareview": "",
        "pdf_size": 369219,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17534374839759353507&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics and Data Science, Yale University; Department of Statistics and Data Science, Yale University",
        "aff_domain": "yale.edu;yale.edu",
        "email": "yale.edu;yale.edu",
        "github": "",
        "project": "http://huisaddison.com/",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/070dbb6024b5ef93784428afc71f2146-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Statistics and Data Science",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Minimizing a Submodular Function from Samples",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8876",
        "id": "8876",
        "author_site": "Eric Balkanski, Yaron Singer",
        "author": "Eric Balkanski; Yaron Singer",
        "abstract": "In this paper we consider the problem of minimizing a submodular function from training data. Submodular functions can be efficiently minimized and are conse- quently heavily applied in machine learning. There are many cases, however, in which we do not know the function we aim to optimize, but rather have access to training data that is used to learn the function. In this paper we consider the question of whether submodular functions can be minimized in such cases. We show that even learnable submodular functions cannot be minimized within any non-trivial approximation when given access to polynomially-many samples. Specifically, we show that there is a class of submodular functions with range in [0, 1] such that, despite being PAC-learnable and minimizable in polynomial-time, no algorithm can obtain an approximation strictly better than 1/2 \u2212 o(1) using polynomially-many samples drawn from any distribution. Furthermore, we show that this bound is tight using a trivial algorithm that obtains an approximation of 1/2.",
        "bibtex": "@inproceedings{NIPS2017_c75b6f11,\n author = {Balkanski, Eric and Singer, Yaron},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimizing a Submodular Function from Samples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c75b6f114c23a4d7ea11331e7c00e73c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c75b6f114c23a4d7ea11331e7c00e73c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c75b6f114c23a4d7ea11331e7c00e73c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c75b6f114c23a4d7ea11331e7c00e73c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c75b6f114c23a4d7ea11331e7c00e73c-Reviews.html",
        "metareview": "",
        "pdf_size": 572413,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2060823883863034679&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Harvard University; Harvard University",
        "aff_domain": "g.harvard.edu;seas.harvard.edu",
        "email": "g.harvard.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c75b6f114c23a4d7ea11331e7c00e73c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Mixture-Rank Matrix Approximation for Collaborative Filtering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8844",
        "id": "8844",
        "author_site": "Dongsheng Li, Chao Chen, Wei Liu, Tun Lu, Ning Gu, Stephen Chu",
        "author": "Dongsheng Li; Chao Chen; Wei Liu; Tun Lu; Ning Gu; Stephen Chu",
        "abstract": "Low-rank matrix approximation (LRMA) methods have achieved excellent accuracy among today's collaborative filtering (CF) methods. In existing LRMA methods, the rank of user/item feature matrices is typically fixed, i.e., the same rank is adopted to describe all users/items. However, our studies show that submatrices with different ranks could coexist in the same user-item rating matrix, so that approximations with fixed ranks cannot perfectly describe the internal structures of the rating matrix, therefore leading to inferior recommendation accuracy. In this paper, a mixture-rank matrix approximation (MRMA) method is proposed, in which user-item ratings can be characterized by a mixture of LRMA models with different ranks. Meanwhile, a learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to MRMA. Experimental studies on MovieLens and Netflix datasets demonstrate that MRMA can outperform six state-of-the-art LRMA-based CF methods in terms of recommendation accuracy.",
        "bibtex": "@inproceedings{NIPS2017_3dd48ab3,\n author = {Li, Dongsheng and Chen, Chao and Liu, Wei and Lu, Tun and Gu, Ning and Chu, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mixture-Rank Matrix Approximation for Collaborative Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Reviews.html",
        "metareview": "",
        "pdf_size": 262633,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8523342601828460235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "IBM Research - China; IBM Research - China; Tencent AI Lab, China; School of Computer Science, Fudan University, China + Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science, Fudan University, China + Shanghai Key Laboratory of Data Science, Fudan University, China; IBM Research - China",
        "aff_domain": "cn.ibm.com;cn.ibm.com;ee.columbia.edu;fudan.edu.cn;fudan.edu.cn;cn.ibm.com",
        "email": "cn.ibm.com;cn.ibm.com;ee.columbia.edu;fudan.edu.cn;fudan.edu.cn;cn.ibm.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html",
        "aff_unique_index": "0;0;2+3;2+3;0",
        "aff_unique_norm": "IBM Research;;Fudan University;Shanghai Key Laboratory of Data Science, Fudan University, China",
        "aff_unique_dep": "Research;;School of Computer Science;",
        "aff_unique_url": "https://www.ibm.com/research;;https://www.fudan.edu.cn;",
        "aff_unique_abbr": "IBM;;Fudan;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Model evidence from nonequilibrium simulations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8965",
        "id": "8965",
        "author": "Michael Habeck",
        "abstract": "The marginal likelihood, or model evidence, is a key quantity in Bayesian parameter estimation and model comparison. For many probabilistic models, computation of the marginal likelihood is challenging, because it involves a sum or integral over an enormous parameter space. Markov chain Monte Carlo (MCMC) is a powerful approach to compute marginal likelihoods. Various MCMC algorithms and evidence estimators have been proposed in the literature. Here we discuss the use of nonequilibrium techniques for estimating the marginal likelihood. Nonequilibrium estimators build on recent developments in statistical physics and are known as annealed importance sampling (AIS) and reverse AIS in probabilistic machine learning. We introduce estimators for the model evidence that combine forward and backward simulations and show for various challenging models that the evidence estimators outperform forward and reverse AIS.",
        "bibtex": "@inproceedings{NIPS2017_4da04049,\n author = {Habeck, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Model evidence from nonequilibrium simulations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4da04049a062f5adfe81b67dd755cecc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4da04049a062f5adfe81b67dd755cecc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4da04049a062f5adfe81b67dd755cecc-Reviews.html",
        "metareview": "",
        "pdf_size": 394621,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10491415212395271036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Statistical Inverse Problems in Biophysics, Max Planck Institute for Biophysical Chemistry + Institute for Mathematical Stochastics, University of G\u00f6ttingen",
        "aff_domain": "gwdg.de",
        "email": "gwdg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4da04049a062f5adfe81b67dd755cecc-Abstract.html",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Statistical Inverse Problems in Biophysics, Max Planck Institute for Biophysical Chemistry;Institute for Mathematical Stochastics, University of G\u00f6ttingen",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Model-Powered Conditional Independence Test",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9080",
        "id": "9080",
        "author_site": "Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alex Dimakis, Sanjay Shakkottai",
        "author": "Rajat Sen; Ananda Theertha Suresh; Karthikeyan Shanmugam; Alexandros G Dimakis; Sanjay Shakkottai",
        "abstract": "We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we determine whether $X \\independent Y \\vert Z$. We approach this by converting the conditional independence test into a classification problem.  This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks.  These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution $f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X \\independent Y \\vert Z.$ -- when given access only to i.i.d.  samples from the true joint distribution $f(x,y,z)$.  To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $f^{CI}$ in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d \\textit{near-independent} samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.",
        "bibtex": "@inproceedings{NIPS2017_02f03905,\n author = {Sen, Rajat and Suresh, Ananda Theertha and Shanmugam, Karthikeyan and Dimakis, Alexandros G and Shakkottai, Sanjay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Model-Powered Conditional Independence Test},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Reviews.html",
        "metareview": "",
        "pdf_size": 2651965,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1309653055284373774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "The University of Texas at Austin; Google, New York; IBM Research, Thomas J. Watson Center; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/02f039058bd48307e6f653a2005c9dd2-Abstract.html",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of Texas at Austin;Google, New York;IBM Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;;https://www.ibm.com/research",
        "aff_unique_abbr": "UT Austin;;IBM",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Austin;;Thomas J. Watson Center",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9132",
        "id": "9132",
        "author_site": "Laurence Aitchison, Lloyd Russell, Adam Packer, Jinyao Yan, Philippe Castonguay, Michael Hausser, Srinivas C Turaga",
        "author": "Laurence Aitchison; Lloyd Russell; Adam M Packer; Jinyao Yan; Philippe Castonguay; Michael Hausser; Srinivas C. Turaga",
        "abstract": "Population activity measurement by calcium imaging can be combined with cellular resolution optogenetic activity perturbations to enable the mapping of neural connectivity in vivo. This requires accurate inference of perturbed and unperturbed neural activity from calcium imaging measurements, which are noisy and indirect, and can also be contaminated by photostimulation artifacts. We have developed a new fully Bayesian approach to jointly inferring spiking activity and neural connectivity from in vivo all-optical perturbation experiments. In contrast to standard approaches that perform spike inference and analysis in two separate maximum-likelihood phases, our joint model is able to propagate uncertainty in spike inference to the inference of connectivity and vice versa. We use the framework of variational autoencoders to model spiking activity using discrete latent variables, low-dimensional latent common input, and sparse spike-and-slab generalized linear coupling between neurons. Additionally, we model two properties of the optogenetic perturbation: off-target photostimulation and photostimulation transients. Using this model, we were able to fit models on 30 minutes of data in just 10 minutes. We performed an all-optical circuit mapping experiment in primary visual cortex of the awake mouse, and use our approach to predict neural connectivity between excitatory neurons in layer 2/3. Predicted connectivity is sparse and consistent with known correlations with stimulus tuning, spontaneous correlation and distance.",
        "bibtex": "@inproceedings{NIPS2017_f80bf055,\n author = {Aitchison, Laurence and Russell, Lloyd and Packer, Adam M and Yan, Jinyao and Castonguay, Philippe and Hausser, Michael and Turaga, Srinivas C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f80bf05527157a8c2a7bb63b22f49aaa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f80bf05527157a8c2a7bb63b22f49aaa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f80bf05527157a8c2a7bb63b22f49aaa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f80bf05527157a8c2a7bb63b22f49aaa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f80bf05527157a8c2a7bb63b22f49aaa-Reviews.html",
        "metareview": "",
        "pdf_size": 1084244,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8471831208252101763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge; University College London; University College London; Janelia Research Campus; Janelia Research Campus; University College London; Janelia Research Campus",
        "aff_domain": "gmail.com;gmail.com;gmail.com;janelia.hhmi.org;gmail.com;ucl.ac.uk;janelia.hhmi.org",
        "email": "gmail.com;gmail.com;gmail.com;janelia.hhmi.org;gmail.com;ucl.ac.uk;janelia.hhmi.org",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f80bf05527157a8c2a7bb63b22f49aaa-Abstract.html",
        "aff_unique_index": "0;1;1;2;2;1;2",
        "aff_unique_norm": "University of Cambridge;University College London;HHMI Janelia Research Campus",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ucl.ac.uk;https://www.janelia.org",
        "aff_unique_abbr": "Cambridge;UCL;Janelia",
        "aff_campus_unique_index": "0;2;2;2",
        "aff_campus_unique": "Cambridge;;Ashburn",
        "aff_country_unique_index": "0;0;0;1;1;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Modulating early visual processing by language",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9428",
        "id": "9428",
        "author_site": "Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, Aaron Courville",
        "author": "Harm de Vries; Florian Strub; Jeremie Mary; Hugo Larochelle; Olivier Pietquin; Aaron C. Courville",
        "abstract": "It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the \\emph{entire visual processing} by a linguistic input. Specifically, we introduce Conditional Batch Normalization (CBN) as an efficient mechanism to modulate convolutional feature maps by a linguistic embedding. We apply CBN to a pre-trained Residual Network (ResNet), leading to the MODulatEd ResNet (\\MRN) architecture, and show that this significantly improves strong baselines on two visual question answering tasks. Our ablation study confirms that modulating from the early stages of the visual processing is beneficial.",
        "bibtex": "@inproceedings{NIPS2017_6fab6e3a,\n author = {de Vries, Harm and Strub, Florian and Mary, Jeremie and Larochelle, Hugo and Pietquin, Olivier and Courville, Aaron C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modulating early visual processing by language},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6fab6e3aa34248ec1e34a4aeedecddc8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6fab6e3aa34248ec1e34a4aeedecddc8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6fab6e3aa34248ec1e34a4aeedecddc8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6fab6e3aa34248ec1e34a4aeedecddc8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6fab6e3aa34248ec1e34a4aeedecddc8-Reviews.html",
        "metareview": "",
        "pdf_size": 513067,
        "gs_citation": 617,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4007282315491669669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of Montreal; Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 CRIStAL; Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 CRIStAL + Criteo; Google Brain; DeepMind; University of Montreal",
        "aff_domain": "harmdevries.com;inria.fr;univ-lille3.fr;google.com;google.com;gmail.com",
        "email": "harmdevries.com;inria.fr;univ-lille3.fr;google.com;google.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6fab6e3aa34248ec1e34a4aeedecddc8-Abstract.html",
        "aff_unique_index": "0;1;1+2;3;4;0",
        "aff_unique_norm": "University of Montreal;University of Lille;Criteo;Google;DeepMind",
        "aff_unique_dep": ";;;Google Brain;",
        "aff_unique_url": "https://wwwumontreal.ca;https://www.univ-lille.fr;https://www.criteo.com;https://brain.google.com;https://deepmind.com",
        "aff_unique_abbr": "UM;Univ. Lille;Criteo;Google Brain;DeepMind",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;1+1;2;3;0",
        "aff_country_unique": "Canada;France;United States;United Kingdom"
    },
    {
        "title": "Monte-Carlo Tree Search by Best Arm Identification",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9266",
        "id": "9266",
        "author_site": "Emilie Kaufmann, Wouter Koolen",
        "author": "Emilie Kaufmann; Wouter M. Koolen",
        "abstract": "Recent advances in bandit tools and techniques for sequential learning are steadily enabling new applications and are promising the resolution of a range of challenging related problems. We study the game tree search problem, where the goal is to quickly identify the optimal move in a given game tree by sequentially sampling its stochastic payoffs. We develop new algorithms for trees of arbitrary depth, that operate by summarizing all deeper levels of the tree into confidence intervals at depth one, and applying a best arm identification procedure at the root. We prove new sample complexity guarantees with a refined dependence on the problem instance. We show experimentally that our algorithms outperform existing elimination-based algorithms and match  previous special-purpose methods for depth-two trees.",
        "bibtex": "@inproceedings{NIPS2017_a6d259bf,\n author = {Kaufmann, Emilie and Koolen, Wouter M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Monte-Carlo Tree Search by Best Arm Identification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a6d259bfbfa2062843ef543e21d7ec8e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a6d259bfbfa2062843ef543e21d7ec8e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a6d259bfbfa2062843ef543e21d7ec8e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a6d259bfbfa2062843ef543e21d7ec8e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a6d259bfbfa2062843ef543e21d7ec8e-Reviews.html",
        "metareview": "",
        "pdf_size": 381880,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16697402696513247945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "CNRS & Univ. Lille, UMR 9189 (CRIStAL), Inria SequeL; Centrum Wiskunde & Informatica",
        "aff_domain": "univ-lille1.fr;cwi.nl",
        "email": "univ-lille1.fr;cwi.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a6d259bfbfa2062843ef543e21d7ec8e-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "CNRS & Univ. Lille, UMR 9189 (CRIStAL), Inria SequeL;Centrum Wiskunde & Informatica",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cwi.nl/",
        "aff_unique_abbr": ";CWI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Netherlands"
    },
    {
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9408",
        "id": "9408",
        "author_site": "Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, Igor Mordatch",
        "author": "Ryan Lowe; YI WU; Aviv Tamar; Jean Harb; OpenAI Pieter Abbeel; Igor Mordatch",
        "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.",
        "bibtex": "@inproceedings{NIPS2017_68a97503,\n author = {Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Reviews.html",
        "metareview": "",
        "pdf_size": 1375021,
        "gs_citation": 6315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11728876531627940507&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "McGill University+OpenAI; UC Berkeley; UC Berkeley; McGill University+OpenAI; UC Berkeley+OpenAI; OpenAI",
        "aff_domain": "cs.mcgill.ca;gmail.com; ; ;openai.com; ",
        "email": "cs.mcgill.ca;gmail.com; ; ;openai.com; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html",
        "aff_unique_index": "0+1;2;2;0+1;2+1;1",
        "aff_unique_norm": "McGill University;OpenAI;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mcgill.ca;https://openai.com;https://www.berkeley.edu",
        "aff_unique_abbr": "McGill;OpenAI;UC Berkeley",
        "aff_campus_unique_index": ";1;1;;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+1;1;1;0+1;1+1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Multi-Armed Bandits with Metric Movement Costs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9192",
        "id": "9192",
        "author_site": "Tomer Koren, Roi Livni, Yishay Mansour",
        "author": "Tomer Koren; Roi Livni; Yishay Mansour",
        "abstract": "We consider the non-stochastic Multi-Armed Bandit problem in a setting where there is a fixed and known metric on the action space that determines a cost for switching between any pair of actions. The loss of the online learner has two components: the first is the usual loss of the selected actions, and the second is an additional loss due to switching between actions.  Our main contribution gives a tight characterization of the expected minimax regret in this setting, in terms of a complexity measure $\\mathcal{C}$ of the underlying metric which depends on its covering numbers. In finite metric spaces with $k$ actions, we give an efficient algorithm that achieves regret of the form $\\widetilde(\\max\\set{\\mathcal{C}^{1/3}T^{2/3},\\sqrt{kT}})$, and show that this is the best possible. Our regret bound generalizes previous known regret bounds for some special cases: (i) the unit-switching cost regret $\\widetilde{\\Theta}(\\max\\set{k^{1/3}T^{2/3},\\sqrt{kT}})$ where $\\mathcal{C}=\\Theta(k)$, and (ii) the interval metric with regret $\\widetilde{\\Theta}(\\max\\set{T^{2/3},\\sqrt{kT}})$ where $\\mathcal{C}=\\Theta(1)$.  For infinite metrics spaces with Lipschitz loss functions, we derive a tight regret bound of $\\widetilde{\\Theta}(T^{\\frac{d+1}{d+2}})$ where $d \\ge 1$ is the Minkowski dimension of the space, which is known to be tight even when there are no switching costs.",
        "bibtex": "@inproceedings{NIPS2017_bd0cc810,\n author = {Koren, Tomer and Livni, Roi and Mansour, Yishay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Armed Bandits with Metric Movement Costs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bd0cc810b580b35884bd9df37c0e8b0f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bd0cc810b580b35884bd9df37c0e8b0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bd0cc810b580b35884bd9df37c0e8b0f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bd0cc810b580b35884bd9df37c0e8b0f-Reviews.html",
        "metareview": "",
        "pdf_size": 275225,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7071947117586361368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Google Brain; Princeton University; Tel Aviv University + Google",
        "aff_domain": "google.com;cs.princeton.edu;cs.tau.ac.il",
        "email": "google.com;cs.princeton.edu;cs.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bd0cc810b580b35884bd9df37c0e8b0f-Abstract.html",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "Google;Princeton University;Tel Aviv University",
        "aff_unique_dep": "Google Brain;;",
        "aff_unique_url": "https://brain.google.com;https://www.princeton.edu;https://www.tau.ac.il",
        "aff_unique_abbr": "Google Brain;Princeton;TAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Multi-Information Source Optimization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9208",
        "id": "9208",
        "author_site": "Matthias Poloczek, Jialei Wang, Peter Frazier",
        "author": "Matthias Poloczek; Jialei Wang; Peter Frazier",
        "abstract": "We consider Bayesian methods for multi-information source optimization (MISO), in which we seek to optimize an expensive-to-evaluate black-box objective function while also accessing cheaper but biased and noisy approximations (\"information sources\"). We present a novel algorithm that outperforms the state of the art for this problem by using a Gaussian process covariance kernel better suited to MISO than those used by previous approaches, and an acquisition function based on a one-step optimality analysis supported by efficient parallelization. We also provide a novel technique to guarantee the asymptotic quality of the solution provided by this algorithm. Experimental evaluations demonstrate that this algorithm consistently finds designs of higher value at less cost than previous approaches.",
        "bibtex": "@inproceedings{NIPS2017_df1f1d20,\n author = {Poloczek, Matthias and Wang, Jialei and Frazier, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Information Source Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Reviews.html",
        "metareview": "",
        "pdf_size": 441215,
        "gs_citation": 287,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2340371535840028584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Systems and Industrial Engineering, University of Arizona; Chief Analytics Office, IBM; School of Operations Research and Information Engineering, Cornell University",
        "aff_domain": "email.arizona.edu;cornell.edu;cornell.edu",
        "email": "email.arizona.edu;cornell.edu;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/df1f1d20ee86704251795841e6a9405a-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Department of Systems and Industrial Engineering, University of Arizona;Chief Analytics Office, IBM;Cornell University",
        "aff_unique_dep": ";;School of Operations Research and Information Engineering",
        "aff_unique_url": ";;https://www.cornell.edu",
        "aff_unique_abbr": ";;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8916",
        "id": "8916",
        "author_site": "Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, Joseph Lim",
        "author": "Karol Hausman; Yevgen Chebotar; Stefan Schaal; Gaurav Sukhatme; Joseph J. Lim",
        "abstract": "Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy.",
        "bibtex": "@inproceedings{NIPS2017_632cee94,\n author = {Hausman, Karol and Chebotar, Yevgen and Schaal, Stefan and Sukhatme, Gaurav and Lim, Joseph J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/632cee946db83e7a52ce5e8d6f0fed35-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/632cee946db83e7a52ce5e8d6f0fed35-Reviews.html",
        "metareview": "",
        "pdf_size": 3329166,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18069097869971364401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Southern California, Los Angeles, CA, USA+Max-Planck-Institute for Intelligent Systems, T\u00fcbingen, Germany; University of Southern California, Los Angeles, CA, USA+Max-Planck-Institute for Intelligent Systems, T\u00fcbingen, Germany; University of Southern California, Los Angeles, CA, USA+Max-Planck-Institute for Intelligent Systems, T\u00fcbingen, Germany; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "http://sites.google.com/view/nips17intentiongan",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/632cee946db83e7a52ce5e8d6f0fed35-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1;0;0",
        "aff_unique_norm": "University of Southern California;Max-Planck-Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "USC;MPI-IS",
        "aff_campus_unique_index": "0+1;0+1;0+1;0;0",
        "aff_campus_unique": "Los Angeles;T\u00fcbingen",
        "aff_country_unique_index": "0+1;0+1;0+1;0;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Multi-Objective Non-parametric Sequential Prediction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9121",
        "id": "9121",
        "author_site": "Guy Uziel, Ran El-Yaniv",
        "author": "Guy Uziel; Ran El-Yaniv",
        "abstract": "Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented.  In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus  allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while  fulfilling  any continuous and convex constraining criterion.",
        "bibtex": "@inproceedings{NIPS2017_b432f34c,\n author = {Uziel, Guy and El-Yaniv, Ran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Objective Non-parametric Sequential Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b432f34c5a997c8e7c806a895ecc5e25-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b432f34c5a997c8e7c806a895ecc5e25-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b432f34c5a997c8e7c806a895ecc5e25-Reviews.html",
        "metareview": "",
        "pdf_size": 260517,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15866286852741796312&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, Technion - Israel Institute of Technology; Computer Science Department, Technion - Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b432f34c5a997c8e7c806a895ecc5e25-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Multi-Task Learning for Contextual Bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9261",
        "id": "9261",
        "author_site": "Aniket Anand Deshmukh, Urun Dogan, Clay Scott",
        "author": "Aniket Anand Deshmukh; Urun Dogan; Clay Scott",
        "abstract": "Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets.",
        "bibtex": "@inproceedings{NIPS2017_b06f50d1,\n author = {Deshmukh, Aniket Anand and Dogan, Urun and Scott, Clay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Task Learning for Contextual Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Reviews.html",
        "metareview": "",
        "pdf_size": 1126262,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17597500231184318994&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of EECS, University of Michigan Ann Arbor; Microsoft Research; Department of EECS, University of Michigan Ann Arbor",
        "aff_domain": "umich.edu;skype.net;umich.edu",
        "email": "umich.edu;skype.net;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b06f50d1f89bd8b2a0fb771c1a69c2b0-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department of EECS, University of Michigan Ann Arbor;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": ";https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": ";MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Multi-View Decision Processes: The Helper-AI Problem",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9319",
        "id": "9319",
        "author_site": "Christos Dimitrakakis, David Parkes, Goran Radanovic, Paul Tylkin",
        "author": "Christos Dimitrakakis; David C. Parkes; Goran Radanovic; Paul Tylkin",
        "abstract": "We consider a  two-player sequential game in which agents have the same reward function but may disagree on the transition probabilities of an underlying Markovian model of the world. By committing to play a specific policy, the agent with the correct model can steer the behavior of the other agent, and seek to improve utility. We model this setting as a multi-view decision process, which we use to formally analyze the positive effect of steering policies. Furthermore, we develop an algorithm for computing the agents' achievable joint policy, and we experimentally show that it can lead to a large utility increase when the agents' models diverge.",
        "bibtex": "@inproceedings{NIPS2017_227f6afd,\n author = {Dimitrakakis, Christos and Parkes, David C and Radanovic, Goran and Tylkin, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-View Decision Processes: The Helper-AI Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/227f6afd3b7f89b96c4bb91f95d50f6d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/227f6afd3b7f89b96c4bb91f95d50f6d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/227f6afd3b7f89b96c4bb91f95d50f6d-Reviews.html",
        "metareview": "",
        "pdf_size": 1084549,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3709560109021407984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html"
    },
    {
        "title": "Multi-output Polynomial Networks and Factorization Machines",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9119",
        "id": "9119",
        "author_site": "Mathieu Blondel, Vlad Niculae, Takuma Otsuka, Naonori Ueda",
        "author": "Mathieu Blondel; Vlad Niculae; Takuma Otsuka; Naonori Ueda",
        "abstract": "Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.",
        "bibtex": "@inproceedings{NIPS2017_dea9ddb2,\n author = {Blondel, Mathieu and Niculae, Vlad and Otsuka, Takuma and Ueda, Naonori},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-output Polynomial Networks and Factorization Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dea9ddb25cbf2352cf4dec30222a02a5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dea9ddb25cbf2352cf4dec30222a02a5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dea9ddb25cbf2352cf4dec30222a02a5-Reviews.html",
        "metareview": "",
        "pdf_size": 357160,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16420397875527670292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "NTT Communication Science Laboratories, Kyoto, Japan; Cornell University, Ithaca, NY + NTT Communication Science Laboratories, Kyoto, Japan; NTT Communication Science Laboratories, Kyoto, Japan; NTT Communication Science Laboratories, Kyoto, Japan + RIKEN",
        "aff_domain": "mblondel.org;cs.cornell.edu;lab.ntt.co.jp;lab.ntt.co.jp",
        "email": "mblondel.org;cs.cornell.edu;lab.ntt.co.jp;lab.ntt.co.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dea9ddb25cbf2352cf4dec30222a02a5-Abstract.html",
        "aff_unique_index": "0;1+0;0;0+2",
        "aff_unique_norm": "NTT Communication Science Laboratories;Cornell University;RIKEN",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ntt-csl.com;https://www.cornell.edu;https://www.riken.jp",
        "aff_unique_abbr": "NTT CSL;Cornell;RIKEN",
        "aff_campus_unique_index": "0;1+0;0;0",
        "aff_campus_unique": "Kyoto;Ithaca;",
        "aff_country_unique_index": "0;1+0;0;0+0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Multi-view Matrix Factorization for Linear Dynamical System Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9475",
        "id": "9475",
        "author_site": "Mahdi Karami, Martha White, Dale Schuurmans, Csaba Szepesvari",
        "author": "Mahdi Karami; Martha White; Dale Schuurmans; Csaba Szepesvari",
        "abstract": "We consider maximum likelihood estimation of linear dynamical systems with generalized-linear observation models. Maximum likelihood is typically considered to be hard in this setting since latent states and transition parameters must be inferred jointly. Given that expectation-maximization does not scale and is prone to local minima, moment-matching approaches from the subspace identification literature have become standard, despite known statistical efficiency issues. In this paper, we instead reconsider likelihood maximization and develop an optimization based strategy for recovering the latent states and transition parameters. Key to the approach is a two-view reformulation of maximum likelihood estimation for linear dynamical systems that enables the use of global optimization algorithms for matrix factorization. We show that the proposed estimation strategy outperforms widely-used identification algorithms such as subspace identification methods, both in terms of accuracy and runtime.",
        "bibtex": "@inproceedings{NIPS2017_c2964caa,\n author = {Karami, Mahdi and White, Martha and Schuurmans, Dale and Szepesvari, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-view Matrix Factorization for Linear Dynamical System Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c2964caac096f26db222cb325aa267cb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c2964caac096f26db222cb325aa267cb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c2964caac096f26db222cb325aa267cb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c2964caac096f26db222cb325aa267cb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c2964caac096f26db222cb325aa267cb-Reviews.html",
        "metareview": "",
        "pdf_size": 681473,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=720931204645881642&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Alberta, Edmonton, AB, Canada; Department of Computer Science, University of Alberta, Edmonton, AB, Canada; Department of Computer Science, University of Alberta, Edmonton, AB, Canada; Department of Computer Science, University of Alberta, Edmonton, AB, Canada",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c2964caac096f26db222cb325aa267cb-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Multi-way Interacting Regression via Factorization Machines",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9046",
        "id": "9046",
        "author_site": "Mikhail Yurochkin, XuanLong Nguyen, nikolaos Vasiloglou",
        "author": "Mikhail Yurochkin; Xuanlong Nguyen; nikolaos Vasiloglou",
        "abstract": "We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.",
        "bibtex": "@inproceedings{NIPS2017_fcdf25d6,\n author = {Yurochkin, Mikhail and Nguyen, XuanLong and Vasiloglou, nikolaos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-way Interacting Regression via Factorization Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Reviews.html",
        "metareview": "",
        "pdf_size": 318612,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10837053888013855615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, University of Michigan; Department of Statistics, University of Michigan; LogicBlox",
        "aff_domain": "umich.edu;umich.edu;logicblox.com",
        "email": "umich.edu;umich.edu;logicblox.com",
        "github": "https://github.com/moonfolk/MiFM",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fcdf25d6e191893e705819b177cddea0-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Michigan;LogicBlox",
        "aff_unique_dep": "Department of Statistics;",
        "aff_unique_url": "https://www.umich.edu;",
        "aff_unique_abbr": "UM;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Multimodal Learning and Reasoning for Visual Question Answering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8851",
        "id": "8851",
        "author_site": "Ilija Ilievski, Jiashi Feng",
        "author": "Ilija Ilievski; Jiashi Feng",
        "abstract": "Reasoning about entities and their relationships from multimodal data is a key goal of Artificial General Intelligence. The visual question answering (VQA) problem is an excellent way to test such reasoning capabilities of an AI model and its multimodal representation learning. However, the current VQA models are over-simplified deep neural networks, comprised of a long short-term memory (LSTM) unit for question comprehension and a convolutional neural network (CNN) for learning single image representation. We argue that the single visual representation contains a limited and general information about the image contents and thus limits the model reasoning capabilities. In this work we introduce a modular neural network model that learns a multimodal and multifaceted representation of the image and the question. The proposed model learns to use the multimodal representation to reason about the image entities and achieves a new state-of-the-art performance on both VQA benchmark datasets, VQA v1.0 and v2.0, by a wide margin.",
        "bibtex": "@inproceedings{NIPS2017_f61d6947,\n author = {Ilievski, Ilija and Feng, Jiashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multimodal Learning and Reasoning for Visual Question Answering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f61d6947467ccd3aa5af24db320235dd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f61d6947467ccd3aa5af24db320235dd-Reviews.html",
        "metareview": "",
        "pdf_size": 740160,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14645160374700826557&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Integrative Sciences and Engineering, National University of Singapore; Electrical and Computer Engineering, National University of Singapore",
        "aff_domain": "u.nus.edu;nus.edu.sg",
        "email": "u.nus.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Integrative Sciences and Engineering, National University of Singapore;National University of Singapore",
        "aff_unique_dep": ";Electrical and Computer Engineering",
        "aff_unique_url": ";https://www.nus.edu.sg",
        "aff_unique_abbr": ";NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Singapore"
    },
    {
        "title": "Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9360",
        "id": "9360",
        "author_site": "Gerasimos Palaiopanos, Ioannis Panageas, Georgios Piliouras",
        "author": "Gerasimos Palaiopanos; Ioannis Panageas; Georgios Piliouras",
        "abstract": "The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to action $\\gamma$ is multiplied by $(1 -\\epsilon C(\\gamma))>0$ where $C(\\gamma)$ is the ``cost\" of action $\\gamma$ and then rescaled to ensure that the new values form a distribution.  We analyze MWU in congestion games where agents use \\textit{arbitrary admissible constants} as learning rates $\\epsilon$ and prove convergence to \\textit{exact Nash equilibria}. Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to action $\\gamma$ is multiplied by $(1 -\\epsilon)^{C(\\gamma)}$ even for the simplest case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior.",
        "bibtex": "@inproceedings{NIPS2017_e93028bd,\n author = {Palaiopanos, Gerasimos and Panageas, Ioannis and Piliouras, Georgios},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e93028bdc1aacdfb3687181f2031765d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e93028bdc1aacdfb3687181f2031765d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e93028bdc1aacdfb3687181f2031765d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e93028bdc1aacdfb3687181f2031765d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e93028bdc1aacdfb3687181f2031765d-Reviews.html",
        "metareview": "",
        "pdf_size": 456025,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12540715957381424937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "SUTD; MIT; SUTD",
        "aff_domain": "yahoo.com;csail.mit.edu;sutd.edu.sg",
        "email": "yahoo.com;csail.mit.edu;sutd.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e93028bdc1aacdfb3687181f2031765d-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Singapore University of Technology and Design;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sutd.edu.sg;https://web.mit.edu",
        "aff_unique_abbr": "SUTD;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "title": "Multiresolution Kernel Approximation for Gaussian Process Regression",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9156",
        "id": "9156",
        "author_site": "Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler",
        "author": "Yi Ding; Risi Kondor; Jonathan Eskreis-Winkler",
        "abstract": "Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix, $K$, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm.  Important points about MKA are that it is memory efficient, and it is a direct method,  which means that it also makes it easy to approximate $K^{-1}$ and $\\mathop{\\textrm{det}}(K)$.",
        "bibtex": "@inproceedings{NIPS2017_850af92f,\n author = {Ding, Yi and Kondor, Risi and Eskreis-Winkler, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiresolution Kernel Approximation for Gaussian Process Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/850af92f8d9903e7a4e0559a98ecc857-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/850af92f8d9903e7a4e0559a98ecc857-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/850af92f8d9903e7a4e0559a98ecc857-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/850af92f8d9903e7a4e0559a98ecc857-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/850af92f8d9903e7a4e0559a98ecc857-Reviews.html",
        "metareview": "",
        "pdf_size": 369633,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17153739431605727645&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science; Department of Computer Science + Department of Statistics; Department of Statistics",
        "aff_domain": "uchicago.edu;uchicago.edu;uchicago.edu",
        "email": "uchicago.edu;uchicago.edu;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/850af92f8d9903e7a4e0559a98ecc857-Abstract.html",
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "Unknown Institution;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Multiscale Quantization for Fast Similarity Search",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9348",
        "id": "9348",
        "author_site": "Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Daniel Holtmann-Rice, David Simcha, Felix Yu",
        "author": "Xiang Wu; Ruiqi Guo; Ananda Theertha Suresh; Sanjiv Kumar; Daniel N Holtmann-Rice; David Simcha; Felix Yu",
        "abstract": "We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real- world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2017_b6617980,\n author = {Wu, Xiang and Guo, Ruiqi and Suresh, Ananda Theertha and Kumar, Sanjiv and Holtmann-Rice, Daniel N and Simcha, David and Yu, Felix},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiscale Quantization for Fast Similarity Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b6617980ce90f637e68c3ebe8b9be745-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b6617980ce90f637e68c3ebe8b9be745-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b6617980ce90f637e68c3ebe8b9be745-Reviews.html",
        "metareview": "",
        "pdf_size": 900776,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2303959089206882891&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Google Research, New York; Google Research, New York; Google Research, New York; Google Research, New York; Google Research, New York; Google Research, New York; Google Research, New York",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b6617980ce90f637e68c3ebe8b9be745-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8881",
        "id": "8881",
        "author_site": "Daniel Milstein, Jason Pacheco, Leigh Hochberg, John D Simeral, Beata Jarosiewicz, Erik Sudderth",
        "author": "Daniel Milstein; Jason Pacheco; Leigh Hochberg; John D Simeral; Beata Jarosiewicz; Erik Sudderth",
        "abstract": "Intracortical brain-computer interfaces (iBCIs) have allowed people with tetraplegia to control a computer cursor by imagining the movement of their paralyzed arm or hand. State-of-the-art decoders deployed in human iBCIs are derived from a Kalman filter that assumes Markov dynamics on the angle of intended movement, and a unimodal dependence on intended angle for each channel of neural activity. Due to errors made in the decoding of noisy neural data, as a user attempts to move the cursor to a goal, the angle between cursor and goal positions may change rapidly. We propose a dynamic Bayesian network that includes the on-screen goal position as part of its latent state, and thus allows the person\u2019s intended angle of movement to be aggregated over a much longer history of neural activity. This multiscale model explicitly captures the relationship between instantaneous angles of motion and long-term goals, and incorporates semi-Markov dynamics for motion trajectories. We also introduce a multimodal likelihood model for recordings of neural populations which can be rapidly calibrated for clinical applications. In offline experiments with recorded neural data, we demonstrate significantly improved prediction of motion directions compared to the Kalman filter. We derive an efficient online inference algorithm, enabling a clinical trial participant with tetraplegia to control a computer cursor with neural activity in real time. The observed kinematics of cursor movement are objectively straighter and smoother than prior iBCI decoding models without loss of responsiveness.",
        "bibtex": "@inproceedings{NIPS2017_99c5e07b,\n author = {Milstein, Daniel and Pacheco, Jason and Hochberg, Leigh and Simeral, John D and Jarosiewicz, Beata and Sudderth, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Reviews.html",
        "metareview": "",
        "pdf_size": 3310991,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2093066680694944137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Brown University, Providence, RI, USA; Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA, USA; School of Engineering, Brown University, Providence, RI, USA + Department of Neurology, Massachusetts General Hospital, Boston, MA, USA + Rehabilitation R&D Service, Department of Veterans Affairs Medical Center, Providence, RI, USA + Brown Institute for Brain Science, Brown University, Providence, RI, USA + Department of Neurology, Harvard Medical School, Boston, MA, USA; School of Engineering, Brown University, Providence, RI, USA + Department of Neurology, Massachusetts General Hospital, Boston, MA, USA + Rehabilitation R&D Service, Department of Veterans Affairs Medical Center, Providence, RI, USA + Brown Institute for Brain Science, Brown University, Providence, RI, USA; Department of Neuroscience, Brown University, Providence, RI, USA + Dept. of Neurosurgery, Stanford University, Stanford, CA, USA; Department of Computer Science, University of California, Irvine, CA, USA",
        "aff_domain": "alumni.brown.edu;mit.edu;brown.edu;brown.edu;stanford.edu;uci.edu",
        "email": "alumni.brown.edu;mit.edu;brown.edu;brown.edu;stanford.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/99c5e07b4d5de9d18c350cdf64c5aa3d-Abstract.html",
        "aff_unique_index": "0;1;0+2+3+4+5;0+2+3+4;6+7;8",
        "aff_unique_norm": "Brown University;Massachusetts Institute of Technology;Department of Neurology, Massachusetts General Hospital, Boston, MA, USA;Rehabilitation R&D Service, Department of Veterans Affairs Medical Center, Providence, RI, USA;Brown Institute for Brain Science, Brown University, Providence, RI, USA;Department of Neurology, Harvard Medical School, Boston, MA, USA;Department of Neuroscience, Brown University, Providence, RI, USA;Dept. of Neurosurgery, Stanford University, Stanford, CA, USA;University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science;Computer Science and Artificial Intelligence Laboratory;;;;;;;Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu;https://web.mit.edu;;;;;;;https://www.uci.edu",
        "aff_unique_abbr": "Brown;MIT;;;;;;;UCI",
        "aff_campus_unique_index": "0;1;0;0;;3",
        "aff_campus_unique": "Providence;Cambridge;;Irvine",
        "aff_country_unique_index": "0;0;0;0;;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Multitask Spectral Learning of Weighted Automata",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9045",
        "id": "9045",
        "author_site": "Guillaume Rabusseau, Borja Balle, Joelle Pineau",
        "author": "Guillaume Rabusseau; Borja Balle; Joelle Pineau",
        "abstract": "We consider the problem of estimating multiple related functions computed by weighted  automata~(WFA). We first present a natural notion of relatedness between WFAs by considering to which extent several WFAs can share a common underlying representation. We then introduce the model of vector-valued WFA which conveniently helps us formalize this notion of relatedness. Finally, we propose a spectral learning algorithm for vector-valued WFAs to tackle the multitask learning problem. By jointly learning multiple tasks in the form of a vector-valued WFA, our algorithm enforces the discovery of a representation space shared between tasks. The benefits of the proposed multitask approach are theoretically motivated and showcased through experiments on both synthetic and real world datasets.",
        "bibtex": "@inproceedings{NIPS2017_e655c771,\n author = {Rabusseau, Guillaume and Balle, Borja and Pineau, Joelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multitask Spectral Learning of Weighted Automata},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e655c7716a4b3ea67f48c6322fc42ed6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e655c7716a4b3ea67f48c6322fc42ed6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e655c7716a4b3ea67f48c6322fc42ed6-Reviews.html",
        "metareview": "",
        "pdf_size": 435821,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14964202436279467264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "McGill University; Amazon Research Cambridge; McGill University",
        "aff_domain": "mail.mcgill.ca;amazon.co.uk;cs.mcgill.ca",
        "email": "mail.mcgill.ca;amazon.co.uk;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e655c7716a4b3ea67f48c6322fc42ed6-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "McGill University;Amazon Research Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mcgill.ca;",
        "aff_unique_abbr": "McGill;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "title": "Natural Value Approximators: Learning when to Trust Past Estimates",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9000",
        "id": "9000",
        "author_site": "Zhongwen Xu, Joseph Modayil, Hado van Hasselt, Andre Barreto, David Silver, Tom Schaul",
        "author": "Zhongwen Xu; Joseph Modayil; Hado P van Hasselt; Andre Barreto; David Silver; Tom Schaul",
        "abstract": "Neural networks have a smooth initial inductive bias, such that small changes in input do not lead to large changes in output. However, in reinforcement learning domains with sparse rewards, value functions have non-smooth structure with a characteristic asymmetric discontinuity whenever rewards arrive. We propose a mechanism that learns an interpolation between a direct value estimate and a projected value estimate computed from the encountered reward and the previous estimate. This reduces the need to learn about discontinuities, and thus improves the value function approximation. Furthermore, as the interpolation is learned and state-dependent, our method can deal with heterogeneous observability. We demonstrate that this one change leads to significant improvements on multiple Atari games, when applied to the state-of-the-art A3C algorithm.",
        "bibtex": "@inproceedings{NIPS2017_fb60d411,\n author = {Xu, Zhongwen and Modayil, Joseph and van Hasselt, Hado P and Barreto, Andre and Silver, David and Schaul, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Natural Value Approximators: Learning when to Trust Past Estimates},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Reviews.html",
        "metareview": "",
        "pdf_size": 434601,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15083994832294630533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9088",
        "id": "9088",
        "author_site": "Yasin Abbasi Yadkori, Peter Bartlett, Victor Gabillon",
        "author": "Yasin Abbasi Yadkori; Peter L Bartlett; Victor Gabillon",
        "abstract": "We study minimax strategies for the online prediction problem with expert advice. It has been conjectured that a simple adversary strategy, called COMB, is near optimal in this game for any number of experts. Our results and new insights make progress in this direction by showing that, up to a small additive term, COMB is minimax optimal in the finite-time three expert problem. In addition, we provide for this setting a new near minimax optimal COMB-based learner. Prior to this work, in this problem, learners obtaining the optimal multiplicative constant in their regret rate were known only when $K=2$ or $K\\rightarrow\\infty$. We characterize, when $K=3$, the regret of the game scaling as $\\sqrt{8/(9\\pi)T}\\pm \\log(T)^2$ which gives for the first time the optimal constant in the leading ($\\sqrt{T}$) term of the regret.",
        "bibtex": "@inproceedings{NIPS2017_851300ee,\n author = {Abbasi Yadkori, Yasin and Bartlett, Peter L and Gabillon, Victor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/851300ee84c2b80ed40f51ed26d866fc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/851300ee84c2b80ed40f51ed26d866fc-Reviews.html",
        "metareview": "",
        "pdf_size": 481869,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14986823799157251243&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/851300ee84c2b80ed40f51ed26d866fc-Abstract.html"
    },
    {
        "title": "Near Optimal Sketching of Low-Rank Tensor Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9130",
        "id": "9130",
        "author_site": "Xingguo Li, Jarvis Haupt, David Woodruff",
        "author": "Xingguo Li; Jarvis Haupt; David Woodruff",
        "abstract": "We study the least squares regression problem $\\min_{\\Theta \\in \\RR^{p_1 \\times \\cdots \\times p_D}} \\| \\cA(\\Theta) -  b \\|_2^2$, where $\\Theta$ is a low-rank tensor, defined as $\\Theta = \\sum_{r=1}^{R} \\theta_1^{(r)} \\circ \\cdots \\circ \\theta_D^{(r)}$, for vectors $\\theta_d^{(r)} \\in \\mathbb{R}^{p_d}$ for all $r \\in [R]$ and $d \\in [D]$.    %$R$ is small compared with $p_1,\\ldots,p_D$,   Here, $\\circ$ denotes the outer product of vectors, and $\\cA(\\Theta)$ is a linear function on $\\Theta$. This problem is motivated by the fact that the number of parameters in $\\Theta$ is only $R \\cdot \\sum_{d=1}^D p_D$, which is significantly smaller than the $\\prod_{d=1}^{D} p_d$ number of parameters in ordinary least squares regression. We consider the above CP decomposition model of tensors $\\Theta$, as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on {\\it sparse} random projections $\\Phi \\in \\RR^{m \\times n}$, with $m \\ll n$, to reduce the problem to a much smaller problem $\\min_{\\Theta} \\|\\Phi \\cA(\\Theta) - \\Phi b\\|_2^2$, for which $\\|\\Phi \\cA(\\Theta) - \\Phi b\\|_2^2 = (1 \\pm \\varepsilon) \\| \\cA(\\Theta) -  b \\|_2^2$ holds simultaneously for all $\\Theta$. We obtain a significantly smaller dimension and sparsity in the randomized linear mapping $\\Phi$ than is possible for ordinary least squares regression. Finally, we give a number of numerical simulations supporting our theory.",
        "bibtex": "@inproceedings{NIPS2017_c4de8ced,\n author = {Li, Xingguo and Haupt, Jarvis and Woodruff, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near Optimal Sketching of Low-Rank Tensor Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c4de8ced6214345614d33fb0b16a8acd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c4de8ced6214345614d33fb0b16a8acd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c4de8ced6214345614d33fb0b16a8acd-Reviews.html",
        "metareview": "",
        "pdf_size": 1874075,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10634171619783180783&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Minnesota; University of Minnesota+Georgia Tech; Carnegie Mellon University",
        "aff_domain": "umn.edu;umn.edu;cs.cmu.edu",
        "email": "umn.edu;umn.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c4de8ced6214345614d33fb0b16a8acd-Abstract.html",
        "aff_unique_index": "0;0+1;2",
        "aff_unique_norm": "University of Minnesota;Georgia Institute of Technology;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.minnesota.edu;https://www.gatech.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UMN;Georgia Tech;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9240",
        "id": "9240",
        "author_site": "Sanjiban Choudhury, Shervin Javdani, Siddhartha Srinivasa, Sebastian Scherer",
        "author": "Sanjiban Choudhury; Shervin Javdani; Siddhartha Srinivasa; Sebastian Scherer",
        "abstract": "Robotic motion-planning problems, such as a UAV flying fast in a partially-known environment or a robot arm moving around cluttered objects, require finding collision-free paths quickly. Typically, this is solved by constructing a graph, where vertices represent robot configurations and edges represent potentially valid movements of the robot between theses configurations. The main computational bottlenecks are expensive edge evaluations to check for collisions. State of the art planning methods do not reason about the optimal sequence of edges to evaluate in order to find a collision free path quickly. In this paper, we do so by drawing a novel equivalence between motion planning and the Bayesian active learning paradigm of decision region determination (DRD). Unfortunately, a straight application of ex- isting methods requires computation exponential in the number of edges in a graph. We present BISECT, an efficient and near-optimal algorithm to solve the DRD problem when edges are independent Bernoulli random variables. By leveraging this property, we are able to significantly reduce computational complexity from exponential to linear in the number of edges. We show that BISECT outperforms several state of the art algorithms on a spectrum of planning problems for mobile robots, manipulators, and real flight data collected from a full scale helicopter. Open-source code and details can be found here: https://github.com/sanjibac/matlab",
        "bibtex": "@inproceedings{NIPS2017_e139c454,\n author = {Choudhury, Sanjiban and Javdani, Shervin and Srinivasa, Siddhartha and Scherer, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e139c454239bfde741e893edb46a06cc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e139c454239bfde741e893edb46a06cc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e139c454239bfde741e893edb46a06cc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e139c454239bfde741e893edb46a06cc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e139c454239bfde741e893edb46a06cc-Reviews.html",
        "metareview": "",
        "pdf_size": 1623784,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9173797743578320316&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/sanjibac/matlab_learning_collision_checking",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e139c454239bfde741e893edb46a06cc-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8985",
        "id": "8985",
        "author_site": "Jason Altschuler, Jonathan Niles-Weed, Philippe Rigollet",
        "author": "Jason Altschuler; Jonathan Niles-Weed; Philippe Rigollet",
        "abstract": "Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iterations, which also directly suggests a new greedy coordinate descent algorithm Greenkhorn with the same theoretical guarantees. Numerical simulations  illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice.",
        "bibtex": "@inproceedings{NIPS2017_491442df,\n author = {Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Reviews.html",
        "metareview": "",
        "pdf_size": 795000,
        "gs_citation": 754,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16247840395851587233&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "MIT; MIT; MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/491442df5f88c6aa018e86dac21d3606-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8948",
        "id": "8948",
        "author_site": "Aryeh Kontorovich, Sivan Sabato, Roi Weiss",
        "author": "Aryeh Kontorovich; Sivan Sabato; Roi Weiss",
        "abstract": "We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.",
        "bibtex": "@inproceedings{NIPS2017_934815ad,\n author = {Kontorovich, Aryeh and Sabato, Sivan and Weiss, Roi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Reviews.html",
        "metareview": "",
        "pdf_size": 393569,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16346775316509631805&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Ben-Gurion University of the Negev; Department of Computer Science, Ben-Gurion University of the Negev; Department of Computer Science and Applied Mathematics, Weizmann Institute of Science",
        "aff_domain": "cs.bgu.ac.il;bgu.ac.il;weizmann.ac.il",
        "email": "cs.bgu.ac.il;bgu.ac.il;weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Ben-Gurion University of the Negev;Weizmann Institute of Science",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science and Applied Mathematics",
        "aff_unique_url": "https://www.bgu.ac.il;https://www.weizmann.ac.il",
        "aff_unique_abbr": "BGU;Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9102",
        "id": "9102",
        "author_site": "Alireza Aghasi, Afshin Abdi, Nam Nguyen, Justin Romberg",
        "author": "Alireza Aghasi; Afshin Abdi; Nam Nguyen; Justin Romberg",
        "abstract": "We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance.  Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program.  This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model.  The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm.  While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner.  In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model.  To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length $N$ as inputs,  we show that if the network response can be described using a maximum number of $s$ non-zero weights per node, these weights can be learned from $\\mathcal{O}(s\\log N)$ samples.",
        "bibtex": "@inproceedings{NIPS2017_3fab5890,\n author = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3fab5890d8113d0b5a4178201dc842ad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3fab5890d8113d0b5a4178201dc842ad-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3fab5890d8113d0b5a4178201dc842ad-Reviews.html",
        "metareview": "",
        "pdf_size": 12185572,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15335507507028124599&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Institute for Insight, Georgia State University + IBM TJ Watson; Department of ECE, Georgia Tech; IBM TJ Watson; Department of ECE, Georgia Tech",
        "aff_domain": "gsu.edu;gatech.edu;us.ibm.com;ece.gatech.edu",
        "email": "gsu.edu;gatech.edu;us.ibm.com;ece.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html",
        "aff_unique_index": "0+1;2;1;2",
        "aff_unique_norm": "Institute for Insight, Georgia State University;IBM TJ Watson;Department of ECE, Georgia Tech",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Neural Discrete Representation Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9401",
        "id": "9401",
        "author_site": "Aaron van den Oord, Oriol Vinyals, koray kavukcuoglu",
        "author": "Aaron van den Oord; Oriol Vinyals; koray kavukcuoglu",
        "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ``posterior collapse'' -\u2014 where the latents are ignored when they are paired with a powerful autoregressive decoder -\u2014 typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
        "bibtex": "@inproceedings{NIPS2017_7a98af17,\n author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Discrete Representation Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Reviews.html",
        "metareview": "",
        "pdf_size": 3122746,
        "gs_citation": 6158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9141153084529999933&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Neural Expectation Maximization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9437",
        "id": "9437",
        "author_site": "Klaus Greff, Sjoerd van Steenkiste, J\u00fcrgen Schmidhuber",
        "author": "Klaus Greff; Sjoerd van Steenkiste; J\u00fcrgen Schmidhuber",
        "abstract": "Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network.  Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities.  We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects.  We demonstrate that the learned representations are useful for next-step prediction.",
        "bibtex": "@inproceedings{NIPS2017_d2cd33e9,\n author = {Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber, J\\\"{u}rgen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Expectation Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Reviews.html",
        "metareview": "",
        "pdf_size": 680763,
        "gs_citation": 343,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2161715145700594706&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "IDSIA; IDSIA; IDSIA",
        "aff_domain": "idsia.ch;idsia.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute of Digital Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idsia.ch",
        "aff_unique_abbr": "IDSIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9413",
        "id": "9413",
        "author_site": "Nikhil Parthasarathy, Eleanor Batty, William Falcon, Thomas Rutten, Mohit Rajpal, E.J. Chichilnisky, Liam Paninski",
        "author": "Nikhil Parthasarathy; Eleanor Batty; William Falcon; Thomas Rutten; Mohit Rajpal; E. J. Chichilnisky; Liam Paninski",
        "abstract": "Decoding sensory stimuli from neural signals can be used to reveal how we sense our physical environment, and is valuable for the design of brain-machine interfaces.  However, existing linear techniques for neural decoding may not fully reveal or exploit the fidelity of  the neural signal. Here we develop a new approximate Bayesian method for decoding natural images from the spiking activity of  populations of retinal ganglion cells (RGCs). We sidestep known computational challenges with Bayesian inference by exploiting artificial neural networks developed for computer vision, enabling fast nonlinear decoding that incorporates natural scene statistics implicitly.  We use a decoder architecture that first linearly reconstructs an image from RGC spikes, then applies a convolutional autoencoder to enhance the image. The resulting decoder, trained on natural images and simulated neural responses, significantly outperforms linear decoding, as well as simple point-wise nonlinear decoding. These results provide a tool for the assessment and optimization of retinal prosthesis technologies, and reveal that the retina may provide a more accurate representation of the visual scene than previously appreciated.",
        "bibtex": "@inproceedings{NIPS2017_b0169350,\n author = {Parthasarathy, Nikhil and Batty, Eleanor and Falcon, William and Rutten, Thomas and Rajpal, Mohit and Chichilnisky, E.J. and Paninski, Liam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b0169350cd35566c47ba83c6ec1d6f82-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b0169350cd35566c47ba83c6ec1d6f82-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b0169350cd35566c47ba83c6ec1d6f82-Reviews.html",
        "metareview": "",
        "pdf_size": 5725720,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2294685670721778138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Stanford University; Columbia University; Columbia University; Columbia University; Columbia University; Stanford University+Columbia University; Columbia University",
        "aff_domain": "gmail.com;columbia.edu;columbia.edu;columbia.edu;columbia.edu;stanford.edu;stat.columbia.edu",
        "email": "gmail.com;columbia.edu;columbia.edu;columbia.edu;columbia.edu;stanford.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b0169350cd35566c47ba83c6ec1d6f82-Abstract.html",
        "aff_unique_index": "0;1;1;1;1;0+1;1",
        "aff_unique_norm": "Stanford University;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.columbia.edu",
        "aff_unique_abbr": "Stanford;Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Neural Program Meta-Induction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8996",
        "id": "8996",
        "author_site": "Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli",
        "author": "Jacob Devlin; Rudy R Bunel; Rishabh Singh; Matthew Hausknecht; Pushmeet Kohli",
        "abstract": "Most recently proposed methods for Neural Program induction work under the assumption of having a large set of input/output (I/O) examples for learning any given input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two novel approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a $k$-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance.",
        "bibtex": "@inproceedings{NIPS2017_3bf55bba,\n author = {Devlin, Jacob and Bunel, Rudy R and Singh, Rishabh and Hausknecht, Matthew and Kohli, Pushmeet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Program Meta-Induction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3bf55bbad370a8fcad1d09b005e278c2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3bf55bbad370a8fcad1d09b005e278c2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3bf55bbad370a8fcad1d09b005e278c2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3bf55bbad370a8fcad1d09b005e278c2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3bf55bbad370a8fcad1d09b005e278c2-Reviews.html",
        "metareview": "",
        "pdf_size": 1300454,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7738686145684941641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Google; University of Oxford; Microsoft Research; Microsoft Research; DeepMind",
        "aff_domain": "google.com;robots.ox.ac.uk;microsoft.com;microsoft.com;google.com",
        "email": "google.com;robots.ox.ac.uk;microsoft.com;microsoft.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3bf55bbad370a8fcad1d09b005e278c2-Abstract.html",
        "aff_unique_index": "0;1;2;2;3",
        "aff_unique_norm": "Google;University of Oxford;Microsoft Corporation;DeepMind",
        "aff_unique_dep": ";;Microsoft Research;",
        "aff_unique_url": "https://www.google.com;https://www.ox.ac.uk;https://www.microsoft.com/en-us/research;https://deepmind.com",
        "aff_unique_abbr": "Google;Oxford;MSR;DeepMind",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Neural Variational Inference and Learning in Undirected Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9441",
        "id": "9441",
        "author_site": "Volodymyr Kuleshov, Stefano Ermon",
        "author": "Volodymyr Kuleshov; Stefano Ermon",
        "abstract": "Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the log-partition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.",
        "bibtex": "@inproceedings{NIPS2017_14e422f0,\n author = {Kuleshov, Volodymyr and Ermon, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Variational Inference and Learning in Undirected Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/14e422f05b68cc0139988e128ee880df-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/14e422f05b68cc0139988e128ee880df-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/14e422f05b68cc0139988e128ee880df-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/14e422f05b68cc0139988e128ee880df-Reviews.html",
        "metareview": "",
        "pdf_size": 726202,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10981559974257453550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/14e422f05b68cc0139988e128ee880df-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Neural system identification for large populations separating \u201cwhat\u201d and \u201cwhere\u201d",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9134",
        "id": "9134",
        "author_site": "David Klindt, Alexander Ecker, Thomas Euler, Matthias Bethge",
        "author": "David Klindt; Alexander S Ecker; Thomas Euler; Matthias Bethge",
        "abstract": "Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of \u201cwhat\u201d and \u201cwhere\u201d.  Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space.  Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations \u2013 a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.",
        "bibtex": "@inproceedings{NIPS2017_8c249675,\n author = {Klindt, David and Ecker, Alexander S and Euler, Thomas and Bethge, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural system identification for large populations separating \\textquotedblleft what\\textquotedblright  and \\textquotedblleft where\\textquotedblright },\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Reviews.html",
        "metareview": "",
        "pdf_size": 934337,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18290773688746704685&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, University of T\u00fcbingen, Germany+Institute for Ophthalmic Research, University of T\u00fcbingen, Germany+Institute for Theoretical Physics, University of T\u00fcbingen, Germany+Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany+Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, USA; Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, University of T\u00fcbingen, Germany+Institute for Theoretical Physics, University of T\u00fcbingen, Germany+Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany+Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, USA; Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, University of T\u00fcbingen, Germany+Institute for Ophthalmic Research, University of T\u00fcbingen, Germany; Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, University of T\u00fcbingen, Germany+Institute for Theoretical Physics, University of T\u00fcbingen, Germany+Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany",
        "aff_domain": "gmail.com;uni-tuebingen.de;cin.uni-tuebingen.de;bethgelab.org",
        "email": "gmail.com;uni-tuebingen.de;cin.uni-tuebingen.de;bethgelab.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8c249675aea6c3cbd91661bbae767ff1-Abstract.html",
        "aff_unique_index": "0+1+0+2+3+4;0+1+2+3+4;0+1+0;0+1+2+3",
        "aff_unique_norm": "University of T\u00fcbingen;Bernstein Center for Computational Neuroscience, University of T\u00fcbingen, Germany;Institute for Theoretical Physics, University of T\u00fcbingen, Germany;Max Planck Institute for Biological Cybernetics;Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, USA",
        "aff_unique_dep": "Centre for Integrative Neuroscience;;;;",
        "aff_unique_url": "https://www.uni-tuebingen.de;;;https://www.biocybernetics.mpg.de;",
        "aff_unique_abbr": ";;;MPIBC;",
        "aff_campus_unique_index": "1;1;;1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0+0",
        "aff_country_unique": "Germany;"
    },
    {
        "title": "NeuralFDR: Learning Discovery Thresholds from Hypothesis Features",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8945",
        "id": "8945",
        "author_site": "Fei Xia, Martin J Zhang, James Zou, David Tse",
        "author": "Fei Xia; Martin J Zhang; James Y Zou; David Tse",
        "abstract": "As datasets grow richer, an important challenge is to leverage the full features in the data to maximize the number of useful discoveries while controlling for false positives. We address this problem in the context of multiple hypotheses testing, where for each hypothesis, we observe a p-value along with a set of features specific to that hypothesis. For example, in genetic association studies, each hypothesis tests the correlation between a variant and the trait. We have a rich set of features for each variant (e.g. its location, conservation, epigenetics etc.) which could inform how likely the variant is to have a true association. However popular testing approaches, such as Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting (IHW), either ignore these features or assume that the features are categorical. We propose a new algorithm, NeuralFDR, which automatically learns a discovery threshold as a function of all the hypothesis features. We parametrize the discovery threshold as a neural network, which enables flexible handling of multi-dimensional discrete and continuous features as well as efficient end-to-end optimization. We prove that NeuralFDR has strong false discovery rate (FDR) guarantees, and show that it makes substantially more discoveries in synthetic and real datasets. Moreover, we demonstrate that the learned discovery threshold is directly interpretable.",
        "bibtex": "@inproceedings{NIPS2017_fec8d47d,\n author = {Xia, Fei and Zhang, Martin J and Zou, James Y and Tse, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {NeuralFDR: Learning Discovery Thresholds from Hypothesis Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fec8d47d412bcbeece3d9128ae855a7a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fec8d47d412bcbeece3d9128ae855a7a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fec8d47d412bcbeece3d9128ae855a7a-Reviews.html",
        "metareview": "",
        "pdf_size": 4244118,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15344774166931421548&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fec8d47d412bcbeece3d9128ae855a7a-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Noise-Tolerant Interactive Learning Using Pairwise Comparisons",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9030",
        "id": "9030",
        "author_site": "Yichong Xu, Hongyang Zhang, Aarti Singh, Artur Dubrawski, Kyle Miller",
        "author": "Yichong Xu; Hongyang Zhang; Kyle Miller; Aarti Singh; Artur Dubrawski",
        "abstract": "We study the problem of interactively learning a binary classifier using noisy labeling and pairwise comparison oracles, where the comparison oracle answers which one in the given two instances is more likely to be positive. Learning from such oracles has multiple applications where obtaining direct labels is harder but pairwise comparisons are easier, and the algorithm can leverage both types of oracles. In this paper, we attempt to characterize how the access to an easier comparison oracle helps in improving the label and total query complexity. We show that the comparison oracle reduces the learning problem to that of learning a threshold function. We then present an algorithm that interactively queries the label and comparison oracles and we characterize its query complexity under Tsybakov and adversarial noise conditions for the comparison and labeling oracles. Our lower bounds show that our label and total query complexity is almost optimal.",
        "bibtex": "@inproceedings{NIPS2017_e11943a6,\n author = {Xu, Yichong and Zhang, Hongyang and Miller, Kyle and Singh, Aarti and Dubrawski, Artur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Noise-Tolerant Interactive Learning Using Pairwise Comparisons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e11943a6031a0e6114ae69c257617980-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e11943a6031a0e6114ae69c257617980-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e11943a6031a0e6114ae69c257617980-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e11943a6031a0e6114ae69c257617980-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e11943a6031a0e6114ae69c257617980-Reviews.html",
        "metareview": "",
        "pdf_size": 397727,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2388398484066581873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Department, Carnegie Mellon University, USA; Machine Learning Department, Carnegie Mellon University, USA; Auton Lab, Carnegie Mellon University, USA; Machine Learning Department, Carnegie Mellon University, USA; Auton Lab, Carnegie Mellon University, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;andrew.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;andrew.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e11943a6031a0e6114ae69c257617980-Abstract.html",
        "aff_unique_index": "0;0;1;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Auton Lab, Carnegie Mellon University, USA",
        "aff_unique_dep": "Machine Learning Department;",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Non-Stationary Spectral Kernels",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9241",
        "id": "9241",
        "author_site": "Sami Remes, Markus Heinonen, Samuel Kaski",
        "author": "Sami Remes; Markus Heinonen; Samuel Kaski",
        "abstract": "We propose non-stationary spectral kernels for Gaussian process regression by modelling the spectral density of a non-stationary kernel function as a mixture of input-dependent Gaussian process frequency density surfaces. We solve the generalised Fourier transform with such a model, and present a family of non-stationary and non-monotonic kernels that can learn input-dependent and potentially long-range, non-monotonic covariances between inputs. We derive efficient inference using model whitening and marginalized posterior, and show with case studies that these kernels are necessary when modelling even rather simple time series, image or geospatial data with non-stationary characteristics.",
        "bibtex": "@inproceedings{NIPS2017_c65d7bd7,\n author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-Stationary Spectral Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Reviews.html",
        "metareview": "",
        "pdf_size": 2520578,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12471934552222782598&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Helsinki Institute for Information Technology HIIT + Department of Computer Science, Aalto University; Helsinki Institute for Information Technology HIIT + Department of Computer Science, Aalto University; Helsinki Institute for Information Technology HIIT + Department of Computer Science, Aalto University",
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c65d7bd70fe3e5e3a2f3de681edc193d-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Helsinki Institute for Information Technology;Aalto University",
        "aff_unique_dep": "HIIT;Department of Computer Science",
        "aff_unique_url": "https://www.hiit.fi;https://www.aalto.fi",
        "aff_unique_abbr": "HIIT;Aalto",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Non-convex Finite-Sum Optimization Via SCSG Methods",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9022",
        "id": "9022",
        "author_site": "Lihua Lei, Cheng Ju, Jianbo Chen, Michael Jordan",
        "author": "Lihua Lei; Cheng Ju; Jianbo Chen; Michael I Jordan",
        "abstract": "We develop a class of algorithms, as variants of the stochastically controlled stochastic gradient (SCSG) methods , for the smooth nonconvex finite-sum optimization problem. Only assuming the smoothness of each component, the complexity of SCSG to reach a stationary point with $E \\|\\nabla f(x)\\|^{2}\\le \\epsilon$ is $O(\\min\\{\\epsilon^{-5/3}, \\epsilon^{-1}n^{2/3}\\})$, which strictly outperforms the stochastic gradient descent. Moreover, SCSG is never worse than the state-of-the-art methods based on variance reduction and it significantly outperforms them when the target accuracy is low. A similar acceleration is also achieved when the functions satisfy the Polyak-Lojasiewicz condition. Empirical experiments demonstrate that SCSG outperforms stochastic gradient methods on training multi-layers neural networks in terms of both training and validation loss.",
        "bibtex": "@inproceedings{NIPS2017_81ca0262,\n author = {Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-convex Finite-Sum Optimization Via SCSG Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Reviews.html",
        "metareview": "",
        "pdf_size": 558203,
        "gs_citation": 304,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6411260473563589154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;stat.berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/81ca0262c82e712e50c580c032d99b60-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Non-parametric Structured Output Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9201",
        "id": "9201",
        "author_site": "Andreas Lehrmann, Leonid Sigal",
        "author": "Andreas Lehrmann; Leonid Sigal",
        "abstract": "Deep neural networks (DNNs) and probabilistic graphical models (PGMs) are the two main tools for statistical modeling. While DNNs provide the ability to model rich and complex relationships between input and output variables, PGMs provide the ability to encode dependencies among the output variables themselves. End-to-end training methods for models with structured graphical dependencies on top of neural predictions have recently emerged as a principled way of combining these two paradigms. While these models have proven to be powerful in discriminative settings with discrete outputs, extensions to structured continuous spaces, as well as performing efficient inference in these spaces, are lacking. We propose non-parametric structured output networks (NSON), a modular approach that cleanly separates a non-parametric, structured posterior representation from a discriminative inference scheme but allows joint end-to-end training of both components. Our experiments evaluate the ability of NSONs to capture structured posterior densities (modeling) and to compute complex statistics of those densities (inference). We compare our model to output spaces of varying expressiveness and popular variational and sampling-based inference algorithms.",
        "bibtex": "@inproceedings{NIPS2017_04048aec,\n author = {Lehrmann, Andreas and Sigal, Leonid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-parametric Structured Output Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/04048aeca2c0f5d84639358008ed2ae7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/04048aeca2c0f5d84639358008ed2ae7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/04048aeca2c0f5d84639358008ed2ae7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/04048aeca2c0f5d84639358008ed2ae7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/04048aeca2c0f5d84639358008ed2ae7-Reviews.html",
        "metareview": "",
        "pdf_size": 7831435,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4509579955518202056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Disney Research; Disney Research",
        "aff_domain": "disneyresearch.com;disneyresearch.com",
        "email": "disneyresearch.com;disneyresearch.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/04048aeca2c0f5d84639358008ed2ae7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Disney Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://research.disney.com",
        "aff_unique_abbr": "Disney Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Nonbacktracking Bounds on the Influence in Independent Cascade Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8932",
        "id": "8932",
        "author_site": "Emmanuel Abbe, Sanjeev Kulkarni, Eun Jee Lee",
        "author": "Emmanuel Abbe; Sanjeev Kulkarni; Eun Jee Lee",
        "abstract": "This paper develops upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit nonbacktracking walks, Fortuin-Kasteleyn-Ginibre type inequalities, and are computed by message passing algorithms. Nonbacktracking walks have recently allowed for headways in community detection, and this paper shows that their use can also impact the influence computation. Further, we provide parameterized versions of the bounds that control the trade-off between the efficiency and the accuracy. Finally, the tightness of the bounds is illustrated with simulations on various network models.",
        "bibtex": "@inproceedings{NIPS2017_8b5040a8,\n author = {Abbe, Emmanuel and Kulkarni, Sanjeev and Lee, Eun Jee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonbacktracking Bounds on the Influence in Independent Cascade Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8b5040a8a5baf3e0e67386c2e3a9b903-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8b5040a8a5baf3e0e67386c2e3a9b903-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8b5040a8a5baf3e0e67386c2e3a9b903-Reviews.html",
        "metareview": "",
        "pdf_size": 836256,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2309933400911198711&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Program in Applied and Computational Mathematics+The Department of Electrical Engineering; The Department of Electrical Engineering; Program in Applied and Computational Mathematics",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "Program in Applied and Computational Mathematics;The Department of Electrical Engineering",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Nonlinear Acceleration of Stochastic Algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9179",
        "id": "9179",
        "author_site": "Damien Scieur, Francis Bach, Alexandre d'Aspremont",
        "author": "Damien Scieur; Francis Bach; Alexandre d'Aspremont",
        "abstract": "Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased  perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.",
        "bibtex": "@inproceedings{NIPS2017_fca0789e,\n author = {Scieur, Damien and Bach, Francis and d\\textquotesingle Aspremont, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonlinear Acceleration of Stochastic Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fca0789e7891cbc0583298a238316122-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fca0789e7891cbc0583298a238316122-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fca0789e7891cbc0583298a238316122-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fca0789e7891cbc0583298a238316122-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fca0789e7891cbc0583298a238316122-Reviews.html",
        "metareview": "",
        "pdf_size": 1391546,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9854834411719375520&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "INRIA, ENS, PSL Research University, Paris France; INRIA, ENS, PSL Research University, Paris France; CNRS, ENS, PSL Research University, Paris France",
        "aff_domain": "inria.fr;inria.fr;ens.fr",
        "email": "inria.fr;inria.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fca0789e7891cbc0583298a238316122-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "INRIA, ENS, PSL Research University, Paris France;CNRS, ENS, PSL Research University, Paris France",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Nonlinear random matrix theory for deep learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9050",
        "id": "9050",
        "author_site": "Jeffrey Pennington, Pratik Worah",
        "author": "Jeffrey Pennington; Pratik Worah",
        "abstract": "Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix $Y^TY$, $Y=f(WX)$, where $W$ is a random weight matrix, $X$ is a random data matrix, and $f$ is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature methods on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.",
        "bibtex": "@inproceedings{NIPS2017_0f3d014e,\n author = {Pennington, Jeffrey and Worah, Pratik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonlinear random matrix theory for deep learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Reviews.html",
        "metareview": "",
        "pdf_size": 714477,
        "gs_citation": 264,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8328385245052817432&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Google Brain; Google Research",
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Brain",
        "aff_unique_url": "https://brain.google.com",
        "aff_unique_abbr": "Google Brain",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Nonparametric Online Regression while Learning the Metric",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8862",
        "id": "8862",
        "author_site": "Ilja Kuzborskij, Nicol\u00f2 Cesa-Bianchi",
        "author": "Ilja Kuzborskij; Nicol\u00f2 Cesa-Bianchi",
        "abstract": "We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix $\\boldsymbol{G}$ of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret ---on the same data sequence--- in terms of the spectrum of $\\boldsymbol{G}$. As a preliminary step in our analysis, we extend a nonparametric online learning algorithm by Hazan and Megiddo enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.",
        "bibtex": "@inproceedings{NIPS2017_821fa74b,\n author = {Kuzborskij, Ilja and Cesa-Bianchi, Nicol\\`{o}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Online Regression while Learning the Metric},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/821fa74b50ba3f7cba1e6c53e8fa6845-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/821fa74b50ba3f7cba1e6c53e8fa6845-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/821fa74b50ba3f7cba1e6c53e8fa6845-Reviews.html",
        "metareview": "",
        "pdf_size": 455106,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14607381491772842353&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "EPFL, Switzerland; Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano, Milano 20135, Italy",
        "aff_domain": "gmail.com;unimi.it",
        "email": "gmail.com;unimi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne;Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano, Milano 20135, Italy",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;",
        "aff_unique_abbr": "EPFL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "title": "Off-policy evaluation for slate recommendation",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9146",
        "id": "9146",
        "author_site": "Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, Imed Zitouni",
        "author": "Adith Swaminathan; Akshay Krishnamurthy; Alekh Agarwal; Miro Dudik; John Langford; Damien Jose; Imed Zitouni",
        "abstract": "This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy's performance.  A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learning-to-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased---these conditions are weaker than prior heuristics for slate evaluation---and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators.",
        "bibtex": "@inproceedings{NIPS2017_5352696a,\n author = {Swaminathan, Adith and Krishnamurthy, Akshay and Agarwal, Alekh and Dudik, Miro and Langford, John and Jose, Damien and Zitouni, Imed},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Off-policy evaluation for slate recommendation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5352696a9ca3397beb79f116f3a33991-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5352696a9ca3397beb79f116f3a33991-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5352696a9ca3397beb79f116f3a33991-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5352696a9ca3397beb79f116f3a33991-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5352696a9ca3397beb79f116f3a33991-Reviews.html",
        "metareview": "",
        "pdf_size": 972270,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2695893491325474358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Research, Redmond; University of Massachusetts, Amherst; Microsoft Research, New York; Microsoft Research, New York; Microsoft Research, New York; Microsoft, Redmond; Microsoft, Redmond",
        "aff_domain": "microsoft.com;cs.umass.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;cs.umass.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5352696a9ca3397beb79f116f3a33991-Abstract.html",
        "aff_unique_index": "0;1;0;0;0;2;2",
        "aff_unique_norm": "Microsoft Research;University of Massachusetts Amherst;Microsoft Corporation",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.umass.edu;https://www.microsoft.com",
        "aff_unique_abbr": "MSR;UMass Amherst;Microsoft",
        "aff_campus_unique_index": "0;1;2;2;2;0;0",
        "aff_campus_unique": "Redmond;Amherst;New York",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Blackbox Backpropagation and Jacobian Sensing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9421",
        "id": "9421",
        "author_site": "Krzysztof Choromanski, Vikas Sindhwani",
        "author": "Krzysztof M Choromanski; Vikas Sindhwani",
        "abstract": "From a small number of calls to a given \u201cblackbox\" on random input perturbations, we show how to efficiently recover its unknown Jacobian, or estimate the left action of its Jacobian on a given vector. Our methods are based on a novel combination of compressed sensing and graph coloring techniques, and provably exploit structural prior knowledge about the Jacobian such as sparsity and symmetry while being noise robust. We demonstrate efficient backpropagation through noisy blackbox layers in a deep neural net, improved data-efficiency in the task of linearizing the dynamics of a rigid body system, and the generic ability to handle a rich class of input-output dependency structures in Jacobian estimation problems.",
        "bibtex": "@inproceedings{NIPS2017_9c8661be,\n author = {Choromanski, Krzysztof M and Sindhwani, Vikas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Blackbox Backpropagation and Jacobian Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9c8661befae6dbcd08304dbf4dcaf0db-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9c8661befae6dbcd08304dbf4dcaf0db-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9c8661befae6dbcd08304dbf4dcaf0db-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9c8661befae6dbcd08304dbf4dcaf0db-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9c8661befae6dbcd08304dbf4dcaf0db-Reviews.html",
        "metareview": "",
        "pdf_size": 762839,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3416463847019729538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google Brain, New York, NY 10011; Google Brain, New York, NY 10011",
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9c8661befae6dbcd08304dbf4dcaf0db-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google Brain, New York, NY 10011",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "On Fairness and Calibration",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9342",
        "id": "9342",
        "author_site": "Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Weinberger",
        "author": "Geoff Pleiss; Manish Raghavan; Felix Wu; Jon Kleinberg; Kilian Q. Weinberger",
        "abstract": "The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.",
        "bibtex": "@inproceedings{NIPS2017_b8b9c74a,\n author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Fairness and Calibration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Reviews.html",
        "metareview": "",
        "pdf_size": 1786812,
        "gs_citation": 1206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17739317978844923548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Cornell University, Department of Computer Science; Cornell University, Department of Computer Science; Cornell University, Department of Computer Science; Cornell University, Department of Computer Science; Cornell University, Department of Computer Science",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;cornell.edu;cs.cornell.edu;cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu;cornell.edu;cs.cornell.edu;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Frank-Wolfe and Equilibrium Computation",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9427",
        "id": "9427",
        "author_site": "Jacob D Abernethy, Jun-Kun Wang",
        "author": "Jacob D. Abernethy; Jun-Kun Wang",
        "abstract": "We consider the Frank-Wolfe (FW) method for constrained convex optimization, and we show that this classical technique can be interpreted from a different perspective: FW emerges as the computation of an equilibrium (saddle point) of a special convex-concave zero sum game. This saddle-point trick relies on the existence of no-regret online learning to both generate a sequence of iterates but also to provide a proof of convergence through vanishing regret. We show that our stated equivalence has several nice properties, as it exhibits a modularity that gives rise to various old and new algorithms. We explore a few such resulting methods, and provide experimental results to demonstrate correctness and efficiency.",
        "bibtex": "@inproceedings{NIPS2017_7371364b,\n author = {Abernethy, Jacob D and Wang, Jun-Kun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Frank-Wolfe and Equilibrium Computation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Reviews.html",
        "metareview": "",
        "pdf_size": 331555,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7411446942546761869&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7371364b3d72ac9a3ed8638e6f0be2c9-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Optimal Generalizability in Parametric Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9129",
        "id": "9129",
        "author_site": "Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, Vahid Tarokh",
        "author": "Ahmad Beirami; Meisam Razaviyayn; Shahin Shahrampour; Vahid Tarokh",
        "abstract": "We consider the parametric learning problem, where the objective of the learner is determined by a parametric loss function. Employing empirical risk minimization with possibly regularization, the inferred parameter vector will be biased toward the training samples. Such bias is measured by the cross validation procedure in practice where the data set is partitioned into a training set used for training and a validation set, which is not used in training and is left to measure the out-of-sample performance. A classical cross validation strategy is the leave-one-out cross validation (LOOCV) where one sample is left out for validation and training is done on the rest of the samples that are presented to the learner, and this process is repeated on all  of the samples. LOOCV is rarely used in practice due to the high computational complexity. In this paper, we first develop a computationally efficient approximate LOOCV (ALOOCV) and provide theoretical guarantees for its performance. Then we use ALOOCV to provide an optimization algorithm for finding the regularizer in the empirical risk minimization framework. In our numerical experiments, we illustrate the accuracy and efficiency of ALOOCV  as well as our proposed framework for the optimization of the regularizer.",
        "bibtex": "@inproceedings{NIPS2017_a82d922b,\n author = {Beirami, Ahmad and Razaviyayn, Meisam and Shahrampour, Shahin and Tarokh, Vahid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Optimal Generalizability in Parametric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a82d922b133be19c1171534e6594f754-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a82d922b133be19c1171534e6594f754-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a82d922b133be19c1171534e6594f754-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a82d922b133be19c1171534e6594f754-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a82d922b133be19c1171534e6594f754-Reviews.html",
        "metareview": "",
        "pdf_size": 588675,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6485572717206130590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Engineering and Applied Sciences, Harvard University; Department of Industrial and Systems Engineering, University of Southern California; School of Engineering and Applied Sciences, Harvard University; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "seas.harvard.edu;usc.edu;seas.harvard.edu;seas.harvard.edu",
        "email": "seas.harvard.edu;usc.edu;seas.harvard.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a82d922b133be19c1171534e6594f754-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Harvard University;Department of Industrial and Systems Engineering, University of Southern California",
        "aff_unique_dep": "School of Engineering and Applied Sciences;",
        "aff_unique_url": "https://www.harvard.edu;",
        "aff_unique_abbr": "Harvard;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9060",
        "id": "9060",
        "author_site": "Xingguo Li, Lin Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao",
        "author": "Xingguo Li; Lin Yang; Jason Ge; Jarvis Haupt; Tong Zhang; Tuo Zhao",
        "abstract": "We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming,  and enjoys both strong computational and statistical guarantees. Specifically, by leveraging a sophisticated characterization of sparse modeling structures (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory.",
        "bibtex": "@inproceedings{NIPS2017_351b3358,\n author = {Li, Xingguo and Yang, Lin and Ge, Jason and Haupt, Jarvis and Zhang, Tong and Zhao, Tuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/351b33587c5fdd93bd42ef7ac9995a28-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/351b33587c5fdd93bd42ef7ac9995a28-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/351b33587c5fdd93bd42ef7ac9995a28-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/351b33587c5fdd93bd42ef7ac9995a28-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/351b33587c5fdd93bd42ef7ac9995a28-Reviews.html",
        "metareview": "",
        "pdf_size": 3072279,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17276749210785113330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Minnesota; Princeton University; Princeton University; University of Minnesota; Tencent AI Lab; Georgia Tech",
        "aff_domain": "umn.edu; ; ; ; ;isye.gatech.edu",
        "email": "umn.edu; ; ; ; ;isye.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/351b33587c5fdd93bd42ef7ac9995a28-Abstract.html",
        "aff_unique_index": "0;1;1;0;2;3",
        "aff_unique_norm": "University of Minnesota;Princeton University;Tencent;Georgia Institute of Technology",
        "aff_unique_dep": ";;Tencent AI Lab;",
        "aff_unique_url": "https://www.minnesota.edu;https://www.princeton.edu;https://ai.tencent.com;https://www.gatech.edu",
        "aff_unique_abbr": "UMN;Princeton;Tencent AI Lab;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9471",
        "id": "9471",
        "author_site": "Adarsh Prasad, Alexandru Niculescu-Mizil, Pradeep Ravikumar",
        "author": "Adarsh Prasad; Alexandru Niculescu-Mizil; Pradeep K Ravikumar",
        "abstract": "We revisit the classical analysis of generative vs discriminative models for general exponential families, and high-dimensional settings. Towards this, we develop novel technical machinery, including a notion of separability of general loss functions, which allow us to provide a general framework to obtain l\u221e convergence rates for general M-estimators. We use this machinery to analyze l\u221e and l2 convergence rates of generative and discriminative models, and provide insights into their nuanced behaviors in high-dimensions. Our results are also applicable to differential parameter estimation, where the quantity of interest is the difference between generative model parameters.",
        "bibtex": "@inproceedings{NIPS2017_50cf0763,\n author = {Prasad, Adarsh and Niculescu-Mizil, Alexandru and Ravikumar, Pradeep K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/50cf0763d8eb871776d4f28b39deb564-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/50cf0763d8eb871776d4f28b39deb564-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/50cf0763d8eb871776d4f28b39deb564-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/50cf0763d8eb871776d4f28b39deb564-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/50cf0763d8eb871776d4f28b39deb564-Reviews.html",
        "metareview": "",
        "pdf_size": 4393874,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3503771692371204179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Dept., CMU; NEC Laboratories America, Princeton, NJ, USA; Machine Learning Dept., CMU",
        "aff_domain": "andrew.cmu.edu;nec-labs.com;cs.cmu.edu",
        "email": "andrew.cmu.edu;nec-labs.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/50cf0763d8eb871776d4f28b39deb564-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Machine Learning Dept., CMU;NEC Laboratories America",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.nec-labs.com",
        "aff_unique_abbr": ";NEC Labs America",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Princeton",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "On Structured Prediction Theory with Calibrated Convex Surrogate Losses",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8827",
        "id": "8827",
        "author_site": "Anton Osokin, Francis Bach, Simon Lacoste-Julien",
        "author": "Anton Osokin; Francis Bach; Simon Lacoste-Julien",
        "abstract": "We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called \"calibration function\" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.",
        "bibtex": "@inproceedings{NIPS2017_38db3aed,\n author = {Osokin, Anton and Bach, Francis and Lacoste-Julien, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Structured Prediction Theory with Calibrated Convex Surrogate Losses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Reviews.html",
        "metareview": "",
        "pdf_size": 460972,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2424435697769737774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html"
    },
    {
        "title": "On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9174",
        "id": "9174",
        "author_site": "Masaaki Imaizumi, Takanori Maehara, Kohei Hayashi",
        "author": "Masaaki Imaizumi; Takanori Maehara; Kohei Hayashi",
        "abstract": "Tensor train (TT) decomposition provides a space-efficient representation for higher-order tensors. Despite its advantage, we face two crucial limitations when we apply the TT decomposition to machine learning problems: the lack of statistical theory and of scalable algorithms. In this paper, we address the limitations. First, we introduce a convex relaxation of the TT decomposition problem and derive its error bound for the tensor completion task. Next, we develop a randomized optimization method, in which the time complexity is as efficient as the space complexity is. In experiments, we numerically confirm the derived bounds and empirically demonstrate the performance of our method with a real higher-order tensor.",
        "bibtex": "@inproceedings{NIPS2017_1b5230e3,\n author = {Imaizumi, Masaaki and Maehara, Takanori and Hayashi, Kohei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1b5230e3ea6d7123847ad55a1e06fffd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1b5230e3ea6d7123847ad55a1e06fffd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1b5230e3ea6d7123847ad55a1e06fffd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1b5230e3ea6d7123847ad55a1e06fffd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1b5230e3ea6d7123847ad55a1e06fffd-Reviews.html",
        "metareview": "",
        "pdf_size": 516889,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10040169691760117100&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Institute of Statistical Mathematics+RIKEN Center for Advanced Intelligence Project; RIKEN Center for Advanced Intelligence Project; National Institute of Advanced Industrial Science and Technology+RIKEN Center for Advanced Intelligence Project",
        "aff_domain": "ism.ac.jp;riken.jp;gmail.com",
        "email": "ism.ac.jp;riken.jp;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1b5230e3ea6d7123847ad55a1e06fffd-Abstract.html",
        "aff_unique_index": "0+1;1;2+1",
        "aff_unique_norm": "Institute of Statistical Mathematics;RIKEN;National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": ";Center for Advanced Intelligence Project;",
        "aff_unique_url": "https://www.ism.ac.jp;https://www.riken.jp/en/;https://www.aist.go.jp",
        "aff_unique_abbr": "ISM;RIKEN;AIST",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "On clustering network-valued data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9473",
        "id": "9473",
        "author_site": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, Lizhen Lin",
        "author": "Soumendu Sundar Mukherjee; Purnamrita Sarkar; Lizhen Lin",
        "abstract": "Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the  research community. While being able to cluster within a network is important, there are emerging needs to be able to \\emph{cluster multiple networks}. This is largely motivated by the routine collection of network data that are generated from potentially different populations. These networks may or may not have node correspondence. When node correspondence is present, we cluster networks by summarizing a network by its graphon estimate, whereas when node correspondence is not present, we propose a novel solution for clustering such networks by associating a computationally feasible feature vector to each network based on trace of powers of the adjacency matrix. We illustrate our methods using both simulated and real data sets, and theoretical justifications are provided in terms of consistency.",
        "bibtex": "@inproceedings{NIPS2017_018dd1e0,\n author = {Mukherjee, Soumendu Sundar and Sarkar, Purnamrita and Lin, Lizhen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On clustering network-valued data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/018dd1e07a2de4a08e6612341bf2323e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/018dd1e07a2de4a08e6612341bf2323e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/018dd1e07a2de4a08e6612341bf2323e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/018dd1e07a2de4a08e6612341bf2323e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/018dd1e07a2de4a08e6612341bf2323e-Reviews.html",
        "metareview": "",
        "pdf_size": 454023,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5845503677770104618&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, University of California, Berkeley; Department of Statistics and Data Sciences, University of Texas, Austin; Department of Applied and Computational Mathematics and Statistics, Univeristy of Notre Dame",
        "aff_domain": "berkeley.edu;austin.utexas.edu;nd.edu",
        "email": "berkeley.edu;austin.utexas.edu;nd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/018dd1e07a2de4a08e6612341bf2323e-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of California, Berkeley;University of Texas at Austin;Department of Applied and Computational Mathematics and Statistics, Univeristy of Notre Dame",
        "aff_unique_dep": "Department of Statistics;Department of Statistics and Data Sciences;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.utexas.edu;",
        "aff_unique_abbr": "UC Berkeley;UT Austin;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Austin;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "On the Complexity of Learning Neural Networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9326",
        "id": "9326",
        "author_site": "Le Song, Santosh Vempala, John Wilmes, Bo Xie",
        "author": "Le Song; Santosh Vempala; John Wilmes; Bo Xie",
        "abstract": "The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take, in the face of existing complexity-theoretic lower bounds? A first step might be to show that data generated by neural networks with a single hidden layer, smooth activation functions and benign input distributions can be learned efficiently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used), and inputs drawn from any logconcave distribution, there is a family of one-hidden-layer functions whose output is a sum gate that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover, this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.",
        "bibtex": "@inproceedings{NIPS2017_a78482ce,\n author = {Song, Le and Vempala, Santosh and Wilmes, John and Xie, Bo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Complexity of Learning Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a78482ce76496fcf49085f2190e675b4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a78482ce76496fcf49085f2190e675b4-Reviews.html",
        "metareview": "",
        "pdf_size": 485415,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7645008239206763796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "cc.gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "email": "cc.gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Consistency of Quick Shift",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8802",
        "id": "8802",
        "author": "Heinrich Jiang",
        "abstract": "Quick Shift is a popular mode-seeking and clustering algorithm. We present finite sample statistical consistency guarantees for Quick Shift on mode and cluster recovery under mild distributional assumptions. We then apply our results to construct a consistent modal regression algorithm.",
        "bibtex": "@inproceedings{NIPS2017_f457c545,\n author = {Jiang, Heinrich},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Consistency of Quick Shift},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f457c545a9ded88f18ecee47145a72c0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f457c545a9ded88f18ecee47145a72c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f457c545a9ded88f18ecee47145a72c0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f457c545a9ded88f18ecee47145a72c0-Reviews.html",
        "metareview": "",
        "pdf_size": 327440,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12148384363645195259&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google Inc.",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f457c545a9ded88f18ecee47145a72c0-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9210",
        "id": "9210",
        "author_site": "Arturs Backurs, Piotr Indyk, Ludwig Schmidt",
        "author": "Arturs Backurs; Piotr Indyk; Ludwig Schmidt",
        "abstract": "Empirical risk minimization (ERM) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various ERM problems, the exact computational complexity of ERM is still not understood. We address this issue for multiple popular ERM problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on complexity-theoretic assumptions such as the Strong Exponential Time Hypothesis. Under these assumptions, we show that there are no algorithms that solve the aforementioned ERM problems to high accuracy in sub-quadratic time. We also give similar hardness results for computing the gradient of the empirical loss, which is the main computational burden in many non-convex learning tasks.",
        "bibtex": "@inproceedings{NIPS2017_635440af,\n author = {Backurs, Arturs and Indyk, Piotr and Schmidt, Ludwig},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/635440afdfc39fe37995fed127d7df4f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/635440afdfc39fe37995fed127d7df4f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/635440afdfc39fe37995fed127d7df4f-Reviews.html",
        "metareview": "",
        "pdf_size": 331412,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=393317033043917826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CSAIL MIT; CSAIL MIT; CSAIL MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/635440afdfc39fe37995fed127d7df4f-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Model Shrinkage Effect of Gamma Process Edge Partition Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8836",
        "id": "8836",
        "author_site": "Iku Ohama, Issei Sato, Takuya Kida, Hiroki Arimura",
        "author": "Iku Ohama; Issei Sato; Takuya Kida; Hiroki Arimura",
        "abstract": "The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process ($\\Gamma$P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal $\\Gamma$P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the $\\Gamma$P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed.",
        "bibtex": "@inproceedings{NIPS2017_ef0d3930,\n author = {Ohama, Iku and Sato, Issei and Kida, Takuya and Arimura, Hiroki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Model Shrinkage Effect of Gamma Process Edge Partition Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ef0d3930a7b6c95bd2b32ed45989c61f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ef0d3930a7b6c95bd2b32ed45989c61f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ef0d3930a7b6c95bd2b32ed45989c61f-Reviews.html",
        "metareview": "",
        "pdf_size": 552217,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3979398812946418699&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Panasonic Corp., Japan\u2021The Univ. of Tokyo, Japan; The Univ. of Tokyo, Japan\u2021Hokkaido Univ., Japan; Hokkaido Univ., Japan; Hokkaido Univ., Japan",
        "aff_domain": "jp.panasonic.com;k.u-tokyo.ac.jp;ist.hokudai.ac.jp;ist.hokudai.ac.jp",
        "email": "jp.panasonic.com;k.u-tokyo.ac.jp;ist.hokudai.ac.jp;ist.hokudai.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Panasonic Corp., Japan\u2021The Univ. of Tokyo, Japan;The Univ. of Tokyo, Japan\u2021Hokkaido Univ., Japan;Hokkaido Univ., Japan",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "On the Optimization Landscape of Tensor Decompositions",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9148",
        "id": "9148",
        "author_site": "Rong Ge, Tengyu Ma",
        "author": "Rong Ge; Tengyu Ma",
        "abstract": "Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that ``all local optima are (approximately) global optima'', and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult.   In this paper, we analyze the optimization landscape of the random over-complete  tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant $\\epsilon > 0$, among the set of points with function values $(1+\\epsilon)$-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem.   Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.",
        "bibtex": "@inproceedings{NIPS2017_a4856405,\n author = {Ge, Rong and Ma, Tengyu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Optimization Landscape of Tensor Decompositions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a48564053b3c7b54800246348c7fa4a0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a48564053b3c7b54800246348c7fa4a0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a48564053b3c7b54800246348c7fa4a0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a48564053b3c7b54800246348c7fa4a0-Reviews.html",
        "metareview": "",
        "pdf_size": 336935,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13334388756853232809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Duke University; Facebook AI Research",
        "aff_domain": "cs.duke.edu;cs.stanford.edu",
        "email": "cs.duke.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a48564053b3c7b54800246348c7fa4a0-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Duke University;Facebook",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.duke.edu;https://research.facebook.com",
        "aff_unique_abbr": "Duke;FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Power of Truncated SVD for General High-rank Matrix Estimation Problems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8841",
        "id": "8841",
        "author_site": "Simon Du, Yining Wang, Aarti Singh",
        "author": "Simon S Du; Yining Wang; Aarti Singh",
        "abstract": "We show that given an estimate $\\widehat{\\mat A}$ that is close to a general high-rank positive semi-definite (PSD) matrix $\\mat A$ in spectral norm (i.e., $\\|\\widehat{\\mat A}-\\mat A\\|_2 \\leq \\delta$), the simple truncated Singular Value Decomposition of $\\widehat{\\mat A}$ produces a multiplicative approximation of $\\mat A$ in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems: 1.High-rank matrix completion: we show that it is possible to recover a {general high-rank matrix} $\\mat A$ up to $(1+\\varepsilon)$ relative error in Frobenius norm from partial observations, with sample complexity independent of the spectral gap of $\\mat A$. 2.High-rank matrix denoising: we design algorithms that recovers a matrix $\\mat A$ with relative error in Frobenius norm from its noise-perturbed observations, without assuming $\\mat A$ is exactly low-rank. 3.Low-dimensional estimation of high-dimensional covariance: given $N$ i.i.d.~samples of dimension $n$ from $\\mathcal N_n(\\mat 0,\\mat A)$, we show that it is possible to estimate the covariance matrix $\\mat A$ with relative error in Frobenius norm with $N\\approx n$,improving over classical covariance estimation results which requires $N\\approx n^2$.",
        "bibtex": "@inproceedings{NIPS2017_89f0fd5c,\n author = {Du, Simon S and Wang, Yining and Singh, Aarti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Power of Truncated SVD for General High-rank Matrix Estimation Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Reviews.html",
        "metareview": "",
        "pdf_size": 427426,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15132111208730761206&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/89f0fd5c927d466d6ec9a21b9ac34ffa-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On-the-fly Operation Batching in Dynamic Computation Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9178",
        "id": "9178",
        "author_site": "Graham Neubig, Yoav Goldberg, Chris Dyer",
        "author": "Graham Neubig; Yoav Goldberg; Chris Dyer",
        "abstract": "Dynamic neural networks toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance data-parallel algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, in computationally efficient batches. On a variety of tasks, we obtain throughput similar to manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.",
        "bibtex": "@inproceedings{NIPS2017_c902b497,\n author = {Neubig, Graham and Goldberg, Yoav and Dyer, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On-the-fly Operation Batching in Dynamic Computation Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c902b497eb972281fb5b4e206db38ee6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c902b497eb972281fb5b4e206db38ee6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c902b497eb972281fb5b4e206db38ee6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c902b497eb972281fb5b4e206db38ee6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c902b497eb972281fb5b4e206db38ee6-Reviews.html",
        "metareview": "",
        "pdf_size": 2390383,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7147131640745506772&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Computer Science Department, Bar-Ilan University; DeepMind",
        "aff_domain": "cs.cmu.edu;cs.biu.ac.il;google.com",
        "email": "cs.cmu.edu;cs.biu.ac.il;google.com",
        "github": "",
        "project": "http://dynet.io/",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c902b497eb972281fb5b4e206db38ee6-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;Bar-Ilan University;DeepMind",
        "aff_unique_dep": "Language Technologies Institute;Computer Science Department;",
        "aff_unique_url": "https://www.cmu.edu;https://www.biu.ac.il;https://deepmind.com",
        "aff_unique_abbr": "CMU;BIU;DeepMind",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United States;Israel;United Kingdom"
    },
    {
        "title": "OnACID: Online Analysis of Calcium Imaging Data in Real Time",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9025",
        "id": "9025",
        "author_site": "Andrea Giovannucci, Johannes Friedrich, Matt Kaufman, Anne Churchland, Dmitri Chklovskii, Liam Paninski, Eftychios Pnevmatikakis",
        "author": "Andrea Giovannucci; Johannes Friedrich; Matt Kaufman; Anne Churchland; Dmitri Chklovskii; Liam Paninski; Eftychios A Pnevmatikakis",
        "abstract": "Optical imaging methods using calcium indicators are critical for monitoring the activity of large neuronal populations in vivo. Imaging experiments typically generate a large amount of data that needs to be processed to extract the activity of the imaged neuronal sources. While deriving such processing algorithms is an active area of research, most existing methods require the processing of large amounts of data at a time, rendering them vulnerable to the volume of the recorded data, and preventing real-time experimental interrogation. Here we introduce OnACID, an Online framework for the Analysis of streaming Calcium Imaging Data, including i) motion artifact correction, ii) neuronal source extraction, and iii) activity denoising and deconvolution. Our approach combines and extends previous work on online dictionary learning and calcium imaging data analysis, to deliver an automated pipeline that can discover and track the activity of hundreds of cells in real time, thereby enabling new types of closed-loop experiments. We apply our algorithm on two large scale experimental datasets, benchmark its performance on manually annotated data, and show that it outperforms a popular offline approach.",
        "bibtex": "@inproceedings{NIPS2017_4edaa105,\n author = {Giovannucci, Andrea and Friedrich, Johannes and Kaufman, Matt and Churchland, Anne and Chklovskii, Dmitri and Paninski, Liam and Pnevmatikakis, Eftychios A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {OnACID: Online Analysis of Calcium Imaging Data in Real Time},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4edaa105d5f53590338791951e38c3ad-Reviews.html",
        "metareview": "",
        "pdf_size": 3061668,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2446845384793849357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Flatiron Institute; Flatiron Institute + Columbia University; Cold Spring Harbor Laboratory; Cold Spring Harbor Laboratory; Flatiron Institute + Columbia University; Columbia University; Flatiron Institute",
        "aff_domain": "flatironinstitute.org;flatironinstitute.org;cshl.edu;cshl.edu;flatironinstitute.org;stat.columbia.edu;flatironinstitute.org",
        "email": "flatironinstitute.org;flatironinstitute.org;cshl.edu;cshl.edu;flatironinstitute.org;stat.columbia.edu;flatironinstitute.org",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4edaa105d5f53590338791951e38c3ad-Abstract.html",
        "aff_unique_index": "0;0+1;2;2;0+1;1;0",
        "aff_unique_norm": "Flatiron Institute;Columbia University;Cold Spring Harbor Laboratory",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://flatironinstitute.org;https://www.columbia.edu;https://www.cshl.edu",
        "aff_unique_abbr": "Flatiron;Columbia;CSHL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "One-Shot Imitation Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8902",
        "id": "8902",
        "author_site": "Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba",
        "author": "Yan Duan; Marcin Andrychowicz; Bradly Stadie; OpenAI Jonathan Ho; Jonas Schneider; Ilya Sutskever; Pieter Abbeel; Wojciech Zaremba",
        "abstract": "Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.  Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations.  For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states.  At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks.  A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.",
        "bibtex": "@inproceedings{NIPS2017_ba386660,\n author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Jonathan Ho, OpenAI and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {One-Shot Imitation Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Reviews.html",
        "metareview": "",
        "pdf_size": 811935,
        "gs_citation": 870,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3870865527815598360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Berkeley AI Research Lab+OpenAI; OpenAI; Berkeley AI Research Lab+OpenAI; Berkeley AI Research Lab+OpenAI; OpenAI; OpenAI; Berkeley AI Research Lab+OpenAI; OpenAI",
        "aff_domain": "eecs.berkeley.edu;openai.com;openai.com;eecs.berkeley.edu;openai.com;openai.com;eecs.berkeley.edu;openai.com",
        "email": "eecs.berkeley.edu;openai.com;openai.com;eecs.berkeley.edu;openai.com;openai.com;eecs.berkeley.edu;openai.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ba3866600c3540f67c1e9575e213be0a-Abstract.html",
        "aff_unique_index": "0+1;1;0+1;0+1;1;1;0+1;1",
        "aff_unique_norm": "Berkeley AI Research Lab;OpenAI",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://openai.com",
        "aff_unique_abbr": ";OpenAI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1;1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "One-Sided Unsupervised Domain Mapping",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8870",
        "id": "8870",
        "author_site": "Sagie Benaim, Lior Wolf",
        "author": "Sagie Benaim; Lior Wolf",
        "abstract": "In unsupervised domain mapping, the learner is given two unmatched datasets $A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample in $A$ to the analog sample in $B$. Recent approaches have shown that when learning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$, convincing mappings are obtained. In this work, we present a method of learning $G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at~\\url{https://github.com/sagiebenaim/DistanceGAN}.",
        "bibtex": "@inproceedings{NIPS2017_59b90e10,\n author = {Benaim, Sagie and Wolf, Lior},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {One-Sided Unsupervised Domain Mapping},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Reviews.html",
        "metareview": "",
        "pdf_size": 2518658,
        "gs_citation": 368,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11923049027832532523&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University, Israel+Facebook AI Research; The Blavatnik School of Computer Science, Tel Aviv University, Israel+Facebook AI Research",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/sagiebenaim/DistanceGAN",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Tel Aviv University;Facebook",
        "aff_unique_dep": "Blavatnik School of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.tau.ac.il;https://research.facebook.com",
        "aff_unique_abbr": "TAU;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Online Convex Optimization with Stochastic Constraints",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8934",
        "id": "8934",
        "author_site": "Hao Yu, Michael Neely, Xiaohan Wei",
        "author": "Hao Yu; Michael Neely; Xiaohan Wei",
        "abstract": "This paper considers online convex optimization  (OCO) with stochastic constraints, which generalizes Zinkevich's OCO over a  known simple fixed set by introducing multiple stochastic functional constraints that are i.i.d. generated at each round and are disclosed to the decision maker only after the decision is made. This formulation arises naturally when decisions are restricted by stochastic environments or deterministic environments with noisy observations. It also includes many important problems  as special case, such as OCO with long term constraints, stochastic constrained convex optimization, and deterministic constrained convex optimization.  To solve this problem, this paper proposes a new algorithm that achieves $O(\\sqrt{T})$ expected regret and constraint violations and $O(\\sqrt{T}\\log(T))$ high probability regret and constraint violations. Experiments on a real-world data center scheduling problem further verify the performance of the new algorithm.",
        "bibtex": "@inproceedings{NIPS2017_da0d1111,\n author = {Yu, Hao and Neely, Michael and Wei, Xiaohan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Convex Optimization with Stochastic Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/da0d1111d2dc5d489242e60ebcbaf988-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/da0d1111d2dc5d489242e60ebcbaf988-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/da0d1111d2dc5d489242e60ebcbaf988-Reviews.html",
        "metareview": "",
        "pdf_size": 975783,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2644540333326208032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html"
    },
    {
        "title": "Online Dynamic Programming",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9476",
        "id": "9476",
        "author_site": "Holakou Rahmanian, Manfred K. Warmuth",
        "author": "Holakou Rahmanian; Manfred K. Warmuth",
        "abstract": "We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find a good binary search tree in a changing environment. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n + 1 gaps between keys at the leaves. The learner is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hindsight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a general methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge and Component Hedge to a significantly wider class of combinatorial objects than was possible before.",
        "bibtex": "@inproceedings{NIPS2017_7a6a74cb,\n author = {Rahmanian, Holakou and Warmuth, Manfred K. K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7a6a74cbe87bc60030a4bd041dd47b78-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7a6a74cbe87bc60030a4bd041dd47b78-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7a6a74cbe87bc60030a4bd041dd47b78-Reviews.html",
        "metareview": "",
        "pdf_size": 2522323,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17033086567940370099&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of California Santa Cruz; Department of Computer Science, University of California Santa Cruz",
        "aff_domain": "ucsc.edu;ucsc.edu",
        "email": "ucsc.edu;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7a6a74cbe87bc60030a4bd041dd47b78-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, University of California Santa Cruz",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9087",
        "id": "9087",
        "author_site": "Zheng Wen, Branislav Kveton, Michal Valko, Sharan Vaswani",
        "author": "Zheng Wen; Branislav Kveton; Michal Valko; Sharan Vaswani",
        "abstract": "We study the online influence maximization problem in social networks under the independent cascade model. Specifically, we aim to learn the set of \"best influencers\" in a social network online while repeatedly interacting with it. We address the challenges of (i) combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and (ii) limited feedback, since only the influenced portion of the network is observed. Under a stochastic semi-bandit feedback, we propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our bounds on the cumulative regret are polynomial in all quantities of interest, achieve near-optimal dependence on the number of interactions and reflect the topology of the network and the activation probabilities of its edges, thereby giving insights on the problem complexity. To the best of our knowledge, these are the first such results. Our experiments show that in several representative graph topologies, the regret of IMLinUCB scales as suggested by our upper bounds. IMLinUCB permits linear generalization and thus is both statistically and computationally suitable for large-scale problems. Our experiments also show that IMLinUCB with linear generalization can lead to low regret in real-world online influence maximization.",
        "bibtex": "@inproceedings{NIPS2017_7137debd,\n author = {Wen, Zheng and Kveton, Branislav and Valko, Michal and Vaswani, Sharan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7137debd45ae4d0ab9aa953017286b20-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7137debd45ae4d0ab9aa953017286b20-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7137debd45ae4d0ab9aa953017286b20-Reviews.html",
        "metareview": "",
        "pdf_size": 582598,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7446479287144489431&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Adobe Research; Adobe Research; SequeL team, INRIA Lille - Nord Europe; University of British Columbia",
        "aff_domain": "adobe.com;adobe.com;inria.fr;cs.ubc.ca",
        "email": "adobe.com;adobe.com;inria.fr;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Adobe;INRIA Lille - Nord Europe;University of British Columbia",
        "aff_unique_dep": "Adobe Research;SequeL team;",
        "aff_unique_url": "https://research.adobe.com;https://www.inria.fr/en/centre/lille-nord-europe;https://www.ubc.ca",
        "aff_unique_abbr": "Adobe;INRIA;UBC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "United States;France;Canada"
    },
    {
        "title": "Online Learning for Multivariate Hawkes Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9270",
        "id": "9270",
        "author_site": "Yingxiang Yang, Jalal Etesami, Niao He, Negar Kiyavash",
        "author": "Yingxiang Yang; Jalal Etesami; Niao He; Negar Kiyavash",
        "abstract": "We develop a nonparametric and online learning algorithm that estimates the triggering functions of a multivariate Hawkes process (MHP). The approach we take approximates the triggering function $f_{i,j}(t)$ by functions in a reproducing kernel Hilbert space (RKHS), and maximizes a time-discretized version of the log-likelihood, with Tikhonov regularization. Theoretically, our algorithm achieves an $\\calO(\\log T)$ regret bound. Numerical results show that our algorithm offers a competing performance to that of the nonparametric batch learning algorithm, with a run time comparable to the parametric online learning algorithm.",
        "bibtex": "@inproceedings{NIPS2017_92a0e7a4,\n author = {Yang, Yingxiang and Etesami, Jalal and He, Niao and Kiyavash, Negar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning for Multivariate Hawkes Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/92a0e7a415d64ebafcb16a8ca817cde4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/92a0e7a415d64ebafcb16a8ca817cde4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/92a0e7a415d64ebafcb16a8ca817cde4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/92a0e7a415d64ebafcb16a8ca817cde4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/92a0e7a415d64ebafcb16a8ca817cde4-Reviews.html",
        "metareview": "",
        "pdf_size": 558728,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7941538929282747305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering\u2217; Department of Industrial and Enterprise Systems Engineering\u2020; Department of Industrial and Enterprise Systems Engineering\u2020; Department of Electrical and Computer Engineering\u2217\u2020",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/92a0e7a415d64ebafcb16a8ca817cde4-Abstract.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Department of Electrical and Computer Engineering\u2217;Department of Industrial and Enterprise Systems Engineering\u2020;Department of Electrical and Computer Engineering\u2217\u2020",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Online Learning of Linear Dynamical Systems",
        "author": "Elad Hazan, Karan Singh, Cyril Zhang",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10050",
        "id": "10050"
    },
    {
        "title": "Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9228",
        "id": "9228",
        "author_site": "M. Sevi Baltaoglu, Lang Tong, Qing Zhao",
        "author": "M. Sevi Baltaoglu; Lang Tong; Qing Zhao",
        "abstract": "We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his T-period payoff, the bidder determines the optimal allocation of his budget among his bids for $K$ goods at each period. As a bidding strategy, we propose a polynomial-time algorithm, inspired by the dynamic programming approach to the knapsack problem. The proposed algorithm, referred to as dynamic programming on discrete set (DPDS), achieves a regret order of $O(\\sqrt{T\\log{T}})$. By showing that the regret is lower bounded by $\\Omega(\\sqrt{T})$ for any strategy, we conclude that DPDS is order optimal up to a $\\sqrt{\\log{T}}$ term. We evaluate the performance of DPDS empirically in the context of virtual trading in wholesale electricity markets by using historical data from the New York market. Empirical results show that DPDS consistently outperforms benchmark heuristic methods that are derived from machine learning and online learning approaches.",
        "bibtex": "@inproceedings{NIPS2017_23af4b45,\n author = {Baltaoglu, M. Sevi and Tong, Lang and Zhao, Qing},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/23af4b45f1e166141a790d1a3126e77a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/23af4b45f1e166141a790d1a3126e77a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/23af4b45f1e166141a790d1a3126e77a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/23af4b45f1e166141a790d1a3126e77a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/23af4b45f1e166141a790d1a3126e77a-Reviews.html",
        "metareview": "",
        "pdf_size": 522891,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6365470385300780732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Cornell University; Cornell University; Cornell University",
        "aff_domain": "cornell.edu;cornell.edu;cornell.edu",
        "email": "cornell.edu;cornell.edu;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/23af4b45f1e166141a790d1a3126e77a-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online Learning with Transductive Regret",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9297",
        "id": "9297",
        "author_site": "Scott Yang, Mehryar Mohri",
        "author": "Mehryar Mohri; Scott Yang",
        "abstract": "We study online learning with the general notion of transductive regret, that is regret with modification rules applying to expert sequences (as opposed to single experts) that are representable by weighted finite-state transducers. We show how transductive regret generalizes existing notions of regret, including: (1) external regret; (2) internal regret; (3) swap regret; and (4) conditional swap regret. We present a general and efficient online learning algorithm for minimizing transductive regret. We further extend that to design efficient algorithms for the time-selection and sleeping expert settings. A by-product of our study is an algorithm for swap regret, which, under mild assumptions, is more efficient than existing ones, and a substantially more efficient algorithm for time selection swap regret.",
        "bibtex": "@inproceedings{NIPS2017_8420d359,\n author = {Mohri, Mehryar and Yang, Scott},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning with Transductive Regret},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8420d359404024567b5aefda1231af24-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8420d359404024567b5aefda1231af24-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8420d359404024567b5aefda1231af24-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8420d359404024567b5aefda1231af24-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8420d359404024567b5aefda1231af24-Reviews.html",
        "metareview": "",
        "pdf_size": 1436758,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1514027962353538430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Courant Institute and Google Research, New York, NY; D. E. Shaw & Co., New York, NY + Courant Institute of Mathematical Sciences",
        "aff_domain": "cims.nyu.edu;cims.nyu.edu",
        "email": "cims.nyu.edu;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8420d359404024567b5aefda1231af24-Abstract.html",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Courant Institute of Mathematical Sciences;D. E. Shaw & Co.",
        "aff_unique_dep": "Mathematical Sciences;",
        "aff_unique_url": "https://courant.nyu.edu;https://www.deshaw.com",
        "aff_unique_abbr": "Courant;DES",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online Learning with a Hint",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9305",
        "id": "9305",
        "author_site": "Ofer Dekel, arthur flajolet, Nika Haghtalab, Patrick Jaillet",
        "author": "Ofer Dekel; arthur flajolet; Nika Haghtalab; Patrick Jaillet",
        "abstract": "We study a variant of online linear optimization where the player receives a hint about the loss function at the beginning of each round. The hint is given in the form of a vector that is weakly correlated with the loss vector on that round. We show that the player can benefit from such a hint if the set of feasible actions is sufficiently round. Specifically, if the set is strongly convex, the hint can be used to guarantee a regret of O(log(T)), and if the set is q-uniformly convex for q\\in(2,3), the hint can be used to guarantee a regret of o(sqrt{T}). In contrast, we establish Omega(sqrt{T}) lower bounds on regret when the set of feasible actions is a polyhedron.",
        "bibtex": "@inproceedings{NIPS2017_22b1f2e0,\n author = {Dekel, Ofer and flajolet, arthur and Haghtalab, Nika and Jaillet, Patrick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning with a Hint},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/22b1f2e0983160db6f7bb9f62f4dbb39-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/22b1f2e0983160db6f7bb9f62f4dbb39-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/22b1f2e0983160db6f7bb9f62f4dbb39-Reviews.html",
        "metareview": "",
        "pdf_size": 563573,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14685251142161233019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Research; Operations Research Center, Massachusetts Institute of Technology; Computer Science Department, Carnegie Mellon University; EECS, LIDS, ORC, Massachusetts Institute of Technology",
        "aff_domain": "microsoft.com;mit.edu;cmu.edu;mit.edu",
        "email": "microsoft.com;mit.edu;cmu.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/22b1f2e0983160db6f7bb9f62f4dbb39-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Microsoft Corporation;Operations Research Center, Massachusetts Institute of Technology;Carnegie Mellon University;EECS, LIDS, ORC, Massachusetts Institute of Technology",
        "aff_unique_dep": "Microsoft Research;;Computer Science Department;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;;https://www.cmu.edu;",
        "aff_unique_abbr": "MSR;;CMU;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Online Prediction with Selfish Experts",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8922",
        "id": "8922",
        "author_site": "Tim Roughgarden, Okke Schrijvers",
        "author": "Tim Roughgarden; Okke Schrijvers",
        "abstract": "We consider the problem of binary prediction with expert advice in settings where experts have agency and seek to maximize their credibility. This paper makes three main contributions.  First, it defines a model to reason formally about settings with selfish experts, and demonstrates that  ``incentive compatible'' (IC) algorithms are closely related to the design of proper scoring rules.   Second, we design IC algorithms with good performance guarantees for the absolute loss function. Third, we give a formal separation between the power of online prediction with selfish experts and online prediction with honest experts by proving lower bounds for both IC and non-IC algorithms.  In particular, with selfish experts and the absolute loss function, there is no (randomized) algorithm for online prediction---IC or otherwise---with asymptotically vanishing regret.",
        "bibtex": "@inproceedings{NIPS2017_3b3dbaf6,\n author = {Roughgarden, Tim and Schrijvers, Okke},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Prediction with Selfish Experts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3b3dbaf68507998acd6a5a5254ab2d76-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3b3dbaf68507998acd6a5a5254ab2d76-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3b3dbaf68507998acd6a5a5254ab2d76-Reviews.html",
        "metareview": "",
        "pdf_size": 736178,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9308316304765935301&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online Reinforcement Learning in Stochastic Games",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9275",
        "id": "9275",
        "author_site": "Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu",
        "author": "Chen-Yu Wei; Yi-Te Hong; Chi-Jen Lu",
        "abstract": "We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the \\textsc{UCSG} algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the \\textit{diameter}, which is an intrinsic value related to the mixing property of SGs. Slightly extended, \\textsc{UCSG} finds an $\\varepsilon$-maximin stationary policy with a sample complexity of $\\tilde{\\mathcal{O}}\\left(\\text{poly}(1/\\varepsilon)\\right)$, where $\\varepsilon$ is the error parameter. To the best of our knowledge, this extended result is the first in the average-reward setting. In the analysis, we develop Markov chain's perturbation bounds for mean first passage times and techniques to deal with non-stationary opponents, which may be of interest in their own right.",
        "bibtex": "@inproceedings{NIPS2017_36e729ec,\n author = {Wei, Chen-Yu and Hong, Yi-Te and Lu, Chi-Jen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Reinforcement Learning in Stochastic Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/36e729ec173b94133d8fa552e4029f8b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/36e729ec173b94133d8fa552e4029f8b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/36e729ec173b94133d8fa552e4029f8b-Reviews.html",
        "metareview": "",
        "pdf_size": 338774,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3105474820949737643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Institute of Information Science, Academia Sinica, Taiwan; Institute of Information Science, Academia Sinica, Taiwan; Institute of Information Science, Academia Sinica, Taiwan",
        "aff_domain": "iis.sinica.edu.tw;iis.sinica.edu.tw;iis.sinica.edu.tw",
        "email": "iis.sinica.edu.tw;iis.sinica.edu.tw;iis.sinica.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/36e729ec173b94133d8fa552e4029f8b-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Academia Sinica",
        "aff_unique_dep": "Institute of Information Science",
        "aff_unique_url": "https://www.sinica.edu.tw",
        "aff_unique_abbr": "AS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Online control of the false discovery rate with decaying memory",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9339",
        "id": "9339",
        "author_site": "Aaditya Ramdas, Fanny Yang, Martin Wainwright, Michael Jordan",
        "author": "Aaditya Ramdas; Fanny Yang; Martin J. Wainwright; Michael I Jordan",
        "abstract": "In the online multiple testing problem, p-values corresponding to different null hypotheses are presented one by one, and the decision of whether to reject a hypothesis must be made immediately, after which the next p-value is presented. Alpha-investing algorithms to control the false discovery rate were first formulated by Foster and Stine and have since been generalized and applied to various settings, varying from quality-preserving databases for science to multiple A/B tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways : (a) we show how to uniformly improve the power of the entire class of GAI procedures under independence by awarding more alpha-wealth for each rejection, giving a near win-win resolution to a dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be null or non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more meaningful/important than others, (d) we define a new quantity called the \\emph{decaying memory false discovery rate, or $\\memfdr$} that may be more meaningful for applications with an explicit time component, using a discount factor to incrementally forget past decisions and alleviate some potential problems that we describe and name ``piggybacking'' and ``alpha-death''. Our GAI++ algorithms incorporate all four generalizations (a, b, c, d) simulatenously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity.",
        "bibtex": "@inproceedings{NIPS2017_7f018eb7,\n author = {Ramdas, Aaditya and Yang, Fanny and Wainwright, Martin J and Jordan, Michael I},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online control of the false discovery rate with decaying memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7f018eb7b301a66658931cb8a93fd6e8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7f018eb7b301a66658931cb8a93fd6e8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7f018eb7b301a66658931cb8a93fd6e8-Reviews.html",
        "metareview": "",
        "pdf_size": 458549,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13806088968210889359&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online multiclass boosting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8886",
        "id": "8886",
        "author_site": "Young H Jung, Jack Goetz, Ambuj Tewari",
        "author": "Young Hun Jung; Jack Goetz; Ambuj Tewari",
        "abstract": "Recent work has extended the theoretical analysis of boosting algorithms to multiclass problems and to online settings. However, the multiclass extension is in the batch setting and the online extensions only consider binary classification. We fill this gap in the literature by defining, and justifying, a weak learning condition for online multiclass boosting. This condition leads to an optimal boosting algorithm that requires the minimal number of weak learners to achieve a certain accuracy. Additionally, we propose an adaptive algorithm which is near optimal and enjoys an excellent performance on real data due to its adaptive property.",
        "bibtex": "@inproceedings{NIPS2017_08b255a5,\n author = {Jung, Young Hun and Goetz, Jack and Tewari, Ambuj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online multiclass boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/08b255a5d42b89b0585260b6f2360bdd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/08b255a5d42b89b0585260b6f2360bdd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/08b255a5d42b89b0585260b6f2360bdd-Reviews.html",
        "metareview": "",
        "pdf_size": 323636,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13918200850165073601&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, University of Michigan; Department of Statistics, University of Michigan; Department of Statistics, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online to Offline Conversions, Universality and Adaptive Minibatch Sizes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8952",
        "id": "8952",
        "author": "Kfir Levy",
        "abstract": "We present an approach towards convex optimization that relies on a novel scheme which  converts  adaptive online algorithms into offline methods. In the offline optimization setting, our derived methods are shown to obtain favourable adaptive  guarantees which depend on the harmonic sum of the queried gradients.  We further  show that our methods implicitly adapt to the objective's structure: in the smooth case fast  convergence rates are ensured without any prior knowledge of the smoothness parameter, while  still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting, resulting in a lazy version of SGD  (stochastic GD), where minibathces are chosen adaptively depending on the magnitude of  the gradients. Thus providing a principled approach towards choosing minibatch sizes.",
        "bibtex": "@inproceedings{NIPS2017_ce5140df,\n author = {Levy, Kfir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online to Offline Conversions, Universality and Adaptive Minibatch Sizes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ce5140df15d046a66883807d18d0264b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ce5140df15d046a66883807d18d0264b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ce5140df15d046a66883807d18d0264b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ce5140df15d046a66883807d18d0264b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ce5140df15d046a66883807d18d0264b-Reviews.html",
        "metareview": "",
        "pdf_size": 411842,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2894046247782880800&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch",
        "email": "inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ce5140df15d046a66883807d18d0264b-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "ETH Z\u00fcrich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Optimal Sample Complexity of M-wise Data for Top-K Ranking",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8959",
        "id": "8959",
        "author_site": "Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh",
        "author": "Minje Jang; Sunghyun Kim; Changho Suh; Sewoong Oh",
        "abstract": "We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight $\\ell_\\infty$ estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in $\\ell_\\infty$ error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model.",
        "bibtex": "@inproceedings{NIPS2017_b4d168b4,\n author = {Jang, Minje and Kim, Sunghyun and Suh, Changho and Oh, Sewoong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Sample Complexity of M-wise Data for Top-K Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b4d168b48157c623fbd095b4a565b5bb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b4d168b48157c623fbd095b4a565b5bb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b4d168b48157c623fbd095b4a565b5bb-Reviews.html",
        "metareview": "",
        "pdf_size": 634923,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4777209490990701964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Electrical Engineering, KAIST; Electronics and Telecommunications Research Institute, Daejeon, Korea; School of Electrical Engineering, KAIST; Industrial and Enterprise Systems Engineering Department, UIUC",
        "aff_domain": "kaist.ac.kr;etri.re.kr;kaist.ac.kr;illinois.edu",
        "email": "kaist.ac.kr;etri.re.kr;kaist.ac.kr;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b4d168b48157c623fbd095b4a565b5bb-Abstract.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "KAIST;Electronics and Telecommunications Research Institute;Industrial and Enterprise Systems Engineering Department, UIUC",
        "aff_unique_dep": "School of Electrical Engineering;;",
        "aff_unique_url": "https://www.kaist.ac.kr;http://www.etri.re.kr;",
        "aff_unique_abbr": "KAIST;ETRI;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Daejeon",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "title": "Optimal Shrinkage of Singular Values Under Random Data Contamination",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9387",
        "id": "9387",
        "author_site": "Danny Barash, Matan Gavish",
        "author": "Danny Barash; Matan Gavish",
        "abstract": "A low rank matrix X has been contaminated by uniformly distributed noise, missing values, outliers and corrupt entries. Reconstruction of X from the singular values and singular vectors of the  contaminated matrix Y is a key problem in machine learning, computer vision and data science.  In this paper we show that common contamination models   (including arbitrary combinations of uniform noise, missing values, outliers and corrupt entries) can be described efficiently using a single framework. We develop an asymptotically optimal algorithm that estimates X by manipulation of the singular values of Y, which applies to any of the contamination models considered.  Finally, we find an explicit signal-to-noise cutoff, below which estimation of X from the singular value decomposition of Y must fail, in a well-defined sense.",
        "bibtex": "@inproceedings{NIPS2017_f231f210,\n author = {Barash, Danny and Gavish, Matan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Shrinkage of Singular Values Under Random Data Contamination},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f231f2107df69eab0a3862d50018a9b2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f231f2107df69eab0a3862d50018a9b2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f231f2107df69eab0a3862d50018a9b2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f231f2107df69eab0a3862d50018a9b2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f231f2107df69eab0a3862d50018a9b2-Reviews.html",
        "metareview": "",
        "pdf_size": 379459,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9410556851428048632&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel; School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il",
        "email": "mail.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f231f2107df69eab0a3862d50018a9b2-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Optimistic posterior sampling for reinforcement learning: worst-case regret bounds",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8911",
        "id": "8911",
        "author_site": "Shipra Agrawal, Randy Jia",
        "author": "Shipra Agrawal; Randy Jia",
        "abstract": "We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\\tilde{O}(D\\sqrt{SAT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$, when $T\\ge S^5A$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result improves over the best previously known upper bound of $\\tilde{O}(DS\\sqrt{AT})$ achieved by any algorithm in this setting, and matches the dependence on $S$ in the established lower bound of $\\Omega(\\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.",
        "bibtex": "@inproceedings{NIPS2017_3621f145,\n author = {Agrawal, Shipra and Jia, Randy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Reviews.html",
        "metareview": "",
        "pdf_size": 312538,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2944369107297388349&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Columbia University; Columbia University",
        "aff_domain": "columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Optimized Pre-Processing for Discrimination Prevention",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9180",
        "id": "9180",
        "author_site": "Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, Kush Varshney",
        "author": "Flavio Calmon; Dennis Wei; Bhanukiran Vinzamuri; Karthikeyan Natesan Ramamurthy; Kush R Varshney",
        "abstract": "Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.",
        "bibtex": "@inproceedings{NIPS2017_9a49a25d,\n author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimized Pre-Processing for Discrimination Prevention},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Reviews.html",
        "metareview": "",
        "pdf_size": 568552,
        "gs_citation": 1131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14078492046583529272&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Harvard University; IBM Research AI; IBM Research AI; IBM Research AI; IBM Research AI",
        "aff_domain": "seas.harvard.edu;us.ibm.com;ibm.com;us.ibm.com;us.ibm.com",
        "email": "seas.harvard.edu;us.ibm.com;ibm.com;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Harvard University;IBM Research",
        "aff_unique_dep": ";AI",
        "aff_unique_url": "https://www.harvard.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "Harvard;IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9242",
        "id": "9242",
        "author_site": "Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, Byoung-Tak Zhang",
        "author": "Sang-Woo Lee; Jin-Hwa Kim; Jaehyun Jun; Jung-Woo Ha; Byoung-Tak Zhang",
        "abstract": "Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.",
        "bibtex": "@inproceedings{NIPS2017_f708f064,\n author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Overcoming Catastrophic Forgetting by Incremental Moment Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Reviews.html",
        "metareview": "",
        "pdf_size": 1558033,
        "gs_citation": 792,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6355108647719259214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Seoul National University1; Seoul National University1; Seoul National University1; Clova AI Research, NAVER Corp2; Seoul National University1+Surromind Robotics3",
        "aff_domain": "bi.snu.ac.kr;bi.snu.ac.kr;bi.snu.ac.kr;navercorp.com;bi.snu.ac.kr",
        "email": "bi.snu.ac.kr;bi.snu.ac.kr;bi.snu.ac.kr;navercorp.com;bi.snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html",
        "aff_unique_index": "0;0;0;1;0+2",
        "aff_unique_norm": "Seoul National University;Clova AI Research, NAVER Corp2;Surromind Robotics3",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.snu.ac.kr;;",
        "aff_unique_abbr": "SNU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "title": "PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9144",
        "id": "9144",
        "author_site": "Jonathan Huggins, Ryan Adams, Tamara Broderick",
        "author": "Jonathan Huggins; Ryan P. Adams; Tamara Broderick",
        "abstract": "Generalized linear models (GLMs)---such as logistic regression, Poisson regression, and robust regression---provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of  speed and multiple measures of accuracy---including on an advertising data set with 40 million data points and 20,000 covariates.",
        "bibtex": "@inproceedings{NIPS2017_07811dc6,\n author = {Huggins, Jonathan and Adams, Ryan P and Broderick, Tamara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/07811dc6c422334ce36a09ff5cd6fe71-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/07811dc6c422334ce36a09ff5cd6fe71-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/07811dc6c422334ce36a09ff5cd6fe71-Reviews.html",
        "metareview": "",
        "pdf_size": 642601,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14550085053193588567&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "CSAIL, MIT; Google Brain and Princeton; CSAIL, MIT",
        "aff_domain": "mit.edu;princeton.edu;csail.mit.edu",
        "email": "mit.edu;princeton.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Google Brain and Princeton",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.csail.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "PRUNE: Preserving Proximity and Global Ranking for Network Embedding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9301",
        "id": "9301",
        "author_site": "Yi-An Lai, Chin-Chi Hsu, Wen Hao Chen, Mi-Yen Yeh, Shou-De Lin",
        "author": "Yi-An Lai; Chin-Chi Hsu; Wen Hao Chen; Mi-Yen Yeh; Shou-De Lin",
        "abstract": "We investigate an unsupervised generative approach for network embedding. A multi-task Siamese neural network structure is formulated to connect embedding vectors and our objective to preserve the global node ranking and local proximity of nodes. We provide deeper analysis to connect the proposed proximity objective to link prediction and community detection in the network. We show our model can satisfy the following design properties: scalability, asymmetry, unity and simplicity. Experiment results not only verify the above design properties but also demonstrate the superior performance in learning-to-rank, classification, regression, and link prediction tasks.",
        "bibtex": "@inproceedings{NIPS2017_cdd96eed,\n author = {Lai, Yi-An and Hsu, Chin-Chi and Chen, Wen Hao and Yeh, Mi-Yen and Lin, Shou-De},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PRUNE: Preserving Proximity and Global Ranking for Network Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Reviews.html",
        "metareview": "",
        "pdf_size": 882030,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3620015543666050977&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "National Taiwan University\u2021; Academia Sinica\u2020\u2021; National Taiwan University\u2217; Academia Sinica\u2020; National Taiwan University\u2217",
        "aff_domain": "ntu.edu.tw;iis.sinica.edu.tw;ntu.edu.tw;iis.sinica.edu.tw;csie.ntu.edu.tw",
        "email": "ntu.edu.tw;iis.sinica.edu.tw;ntu.edu.tw;iis.sinica.edu.tw;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html",
        "aff_unique_index": "0;1;2;3;2",
        "aff_unique_norm": "National Taiwan University\u2021;Academia Sinica\u2020\u2021;National Taiwan University\u2217;Academia Sinica\u2020",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Parallel Streaming Wasserstein Barycenters",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9051",
        "id": "9051",
        "author_site": "Matt Staib, Sebastian Claici, Justin Solomon, Stefanie Jegelka",
        "author": "Matthew Staib; Sebastian Claici; Justin M Solomon; Stefanie Jegelka",
        "abstract": "Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.",
        "bibtex": "@inproceedings{NIPS2017_253f7b5d,\n author = {Staib, Matthew and Claici, Sebastian and Solomon, Justin M and Jegelka, Stefanie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Streaming Wasserstein Barycenters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/253f7b5d921338af34da817c00f42753-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/253f7b5d921338af34da817c00f42753-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/253f7b5d921338af34da817c00f42753-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/253f7b5d921338af34da817c00f42753-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/253f7b5d921338af34da817c00f42753-Reviews.html",
        "metareview": "",
        "pdf_size": 6118362,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6428359143478031539&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "MIT CSAIL; MIT CSAIL; MIT CSAIL; MIT CSAIL",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/253f7b5d921338af34da817c00f42753-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Parameter-Free Online Learning via Model Selection",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9374",
        "id": "9374",
        "author_site": "Dylan J Foster, Satyen Kale, Mehryar Mohri, Karthik Sridharan",
        "author": "Dylan J Foster; Satyen Kale; Mehryar Mohri; Karthik Sridharan",
        "abstract": "We introduce an efficient algorithmic framework for model selection in online learning, also known as parameter-free online learning. Departing from previous work, which has focused on highly structured function classes such as nested balls in Hilbert space, we propose a generic meta-algorithm framework that achieves online model selection oracle inequalities under minimal structural assumptions. We give the first computationally efficient parameter-free algorithms that work in arbitrary Banach spaces under mild smoothness assumptions; previous results applied only to Hilbert spaces. We further derive new oracle inequalities for matrix classes, non-nested convex sets, and $\\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these results by providing oracle inequalities for arbitrary non-linear classes in the online supervised learning model.    These results are all derived through a unified meta-algorithm scheme using a novel \"multi-scale\" algorithm for prediction with expert advice based on random playout, which may be of independent interest.",
        "bibtex": "@inproceedings{NIPS2017_a2186aa7,\n author = {Foster, Dylan J and Kale, Satyen and Mohri, Mehryar and Sridharan, Karthik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parameter-Free Online Learning via Model Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a2186aa7c086b46ad4e8bf81e2a3a19b-Reviews.html",
        "metareview": "",
        "pdf_size": 1357731,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2546730392669022181&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a2186aa7c086b46ad4e8bf81e2a3a19b-Abstract.html"
    },
    {
        "title": "Parametric Simplex Method for Sparse Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8816",
        "id": "8816",
        "author_site": "Haotian Pang, Han Liu, Robert J Vanderbei, Tuo Zhao",
        "author": "Haotian Pang; Han Liu; Robert J Vanderbei; Tuo Zhao",
        "abstract": "High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we investiage a broad class of sparse learning approaches formulated as linear programs parametrized by a {\\em regularization factor}, and solve them by the parametric simplex method (PSM). PSM offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, sparse support vector machine for sparse linear classification, and sparse differential network estimation. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.",
        "bibtex": "@inproceedings{NIPS2017_fa7cdfad,\n author = {Pang, Haotian and Liu, Han and Vanderbei, Robert J and Zhao, Tuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parametric Simplex Method for Sparse Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Reviews.html",
        "metareview": "",
        "pdf_size": 506748,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5387540279077878458&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Princeton University; Princeton University; Tencent AI Lab + Northwestern University; Georgia Tech",
        "aff_domain": "princeton.edu;princeton.edu;tencent.com;isye.gatech.edu",
        "email": "princeton.edu;princeton.edu;tencent.com;isye.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html",
        "aff_unique_index": "0;0;1+2;3",
        "aff_unique_norm": "Princeton University;Tencent;Northwestern University;Georgia Institute of Technology",
        "aff_unique_dep": ";Tencent AI Lab;;",
        "aff_unique_url": "https://www.princeton.edu;https://ai.tencent.com;https://www.northwestern.edu;https://www.gatech.edu",
        "aff_unique_abbr": "Princeton;Tencent AI Lab;NU;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9097",
        "id": "9097",
        "author_site": "Jie Shen, Ping Li",
        "author": "Jie Shen; Ping Li",
        "abstract": "In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary \"s\"-sparse signal within O(s kappa log kappa) iterations where \"kappa\" is an appropriate condition number. Specifying the PHT operator, we obtain the best known result for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT compared to other popular recovery methods.",
        "bibtex": "@inproceedings{NIPS2017_4a2ddf14,\n author = {Shen, Jie and Li, Ping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4a2ddf148c5a9c42151a529e8cbdcc06-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4a2ddf148c5a9c42151a529e8cbdcc06-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4a2ddf148c5a9c42151a529e8cbdcc06-Reviews.html",
        "metareview": "",
        "pdf_size": 218217,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15908950077668641061&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, School of Arts and Sciences, Rutgers University, New Jersey, USA; Department of Statistics and Biostatistics, Department of Computer Science, Rutgers University, New Jersey, USA",
        "aff_domain": "rutgers.edu;stat.rutgers.edu",
        "email": "rutgers.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4a2ddf148c5a9c42151a529e8cbdcc06-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science, School of Arts and Sciences, Rutgers University, New Jersey, USA;Department of Statistics and Biostatistics, Department of Computer Science, Rutgers University, New Jersey, USA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Permutation-based Causal Inference Algorithms with Interventions",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9355",
        "id": "9355",
        "author_site": "Yuhao Wang, Liam Solus, Karren Yang, Caroline Uhler",
        "author": "Yuhao Wang; Liam Solus; Karren Yang; Caroline Uhler",
        "abstract": "Learning directed acyclic graphs using both observational and interventional data is now a fundamentally important problem due to recent technological developments in genomics that generate such single-cell gene expression data at a very large scale. In order to utilize this data for learning gene regulatory networks, efficient and reliable causal inference algorithms are needed that can make use of both observational and interventional data. In this paper, we present two algorithms of this type and prove that both are consistent under the faithfulness assumption. These algorithms are interventional adaptations of the Greedy SP algorithm and are the first algorithms using both observational and interventional data with consistency guarantees. Moreover, these algorithms have the advantage that they are nonparametric, which makes them useful also for analyzing non-Gaussian data. In this paper, we present these two algorithms and their consistency guarantees, and we analyze their performance on simulated data, protein signaling data, and single-cell gene expression data.",
        "bibtex": "@inproceedings{NIPS2017_275d7fb2,\n author = {Wang, Yuhao and Solus, Liam and Yang, Karren and Uhler, Caroline},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Permutation-based Causal Inference Algorithms with Interventions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/275d7fb2fd45098ad5c3ece2ed4a2824-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/275d7fb2fd45098ad5c3ece2ed4a2824-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/275d7fb2fd45098ad5c3ece2ed4a2824-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/275d7fb2fd45098ad5c3ece2ed4a2824-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/275d7fb2fd45098ad5c3ece2ed4a2824-Reviews.html",
        "metareview": "",
        "pdf_size": 641359,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=568612584245296667&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Laboratory for Information and Decision Systems and Institute for Data, Systems and Society, Massachusetts Institute of Technology; Department of Mathematics, KTH Royal Institute of Technology; Institute for Data, Systems and Society and Broad Institute of MIT and Harvard, Massachusetts Institute of Technology; Laboratory for Information and Decision Systems and Institute for Data, Systems and Society, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;kth.se;mit.edu;mit.edu",
        "email": "mit.edu;kth.se;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/275d7fb2fd45098ad5c3ece2ed4a2824-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Laboratory for Information and Decision Systems and Institute for Data, Systems and Society, Massachusetts Institute of Technology;Department of Mathematics, KTH Royal Institute of Technology;Institute for Data, Systems and Society and Broad Institute of MIT and Harvard, Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Perturbative Black Box Variational Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9284",
        "id": "9284",
        "author_site": "Robert Bamler, Cheng Zhang, Manfred Opper, Stephan Mandt",
        "author": "Robert Bamler; Cheng Zhang; Manfred Opper; Stephan Mandt",
        "abstract": "Black box variational inference (BBVI) with reparameterization gradients triggered the exploration of divergence measures other than the Kullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we view BBVI with generalized divergences as a form of estimating the marginal likelihood via biased importance sampling. The choice of divergence determines a bias-variance trade-off between the tightness of a bound on the marginal likelihood (low bias) and the variance of its gradient estimators. Drawing on variational perturbation theory of statistical physics, we use these insights to construct a family of new variational bounds. Enumerated by an odd integer order $K$, this family captures the standard KL bound for $K=1$, and converges to the exact marginal likelihood as $K\\to\\infty$. Compared to alpha-divergences, our reparameterization gradients have a lower variance. We show in experiments on Gaussian Processes and Variational Autoencoders that the new bounds are more mass covering, and that the resulting posterior covariances are closer to the true posterior and lead to higher likelihoods on held-out data.",
        "bibtex": "@inproceedings{NIPS2017_b75bd27b,\n author = {Bamler, Robert and Zhang, Cheng and Opper, Manfred and Mandt, Stephan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Perturbative Black Box Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b75bd27b5a48a1b48987a18d831f6336-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b75bd27b5a48a1b48987a18d831f6336-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b75bd27b5a48a1b48987a18d831f6336-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b75bd27b5a48a1b48987a18d831f6336-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b75bd27b5a48a1b48987a18d831f6336-Reviews.html",
        "metareview": "",
        "pdf_size": 1492060,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12441400581379182245&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b75bd27b5a48a1b48987a18d831f6336-Abstract.html"
    },
    {
        "title": "Phase Transitions in the Pooled Data Problem",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8834",
        "id": "8834",
        "author_site": "Jonathan Scarlett, Volkan Cevher",
        "author": "Jonathan Scarlett; Volkan Cevher",
        "abstract": "In this paper, we study the {\\em pooled data} problem of identifying the labels associated with a large collection of items, based on a sequence of pooled tests revealing the counts of each label within the pool.  In the noiseless setting, we identify an exact asymptotic threshold on the required number of tests with optimal decoding, and prove a {\\em phase transition} between complete success and complete failure.  In addition, we present a novel {\\em noisy} variation of the problem, and provide an information-theoretic framework for characterizing the required number of tests for general random noise models.  Our results reveal that noise can make the problem considerably more difficult, with strict increases in the scaling laws even at low noise levels.  Finally, we demonstrate similar behavior in an {\\em approximate recovery} setting, where a given number of errors is allowed in the decoded labels.",
        "bibtex": "@inproceedings{NIPS2017_17000029,\n author = {Scarlett, Jonathan and Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Phase Transitions in the Pooled Data Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1700002963a49da13542e0726b7bb758-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1700002963a49da13542e0726b7bb758-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1700002963a49da13542e0726b7bb758-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1700002963a49da13542e0726b7bb758-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1700002963a49da13542e0726b7bb758-Reviews.html",
        "metareview": "",
        "pdf_size": 495790,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=964048443737062129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Laboratory for Information and Inference Systems (LIONS) + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL); Laboratory for Information and Inference Systems (LIONS) + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)",
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1700002963a49da13542e0726b7bb758-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Laboratory for Information and Inference Systems;\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": "Information and Inference Systems;",
        "aff_unique_url": ";https://www.epfl.ch",
        "aff_unique_abbr": "LIONS;EPFL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Switzerland"
    },
    {
        "title": "PixelGAN Autoencoders",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8986",
        "id": "8986",
        "author_site": "Alireza Makhzani, Brendan J Frey",
        "author": "Alireza Makhzani; Brendan J. Frey",
        "abstract": "In this paper, we describe the \"PixelGAN autoencoder\", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.",
        "bibtex": "@inproceedings{NIPS2017_7e7e69ea,\n author = {Makhzani, Alireza and Frey, Brendan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PixelGAN Autoencoders},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7e7e69ea3384874304911625ac34321c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7e7e69ea3384874304911625ac34321c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7e7e69ea3384874304911625ac34321c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7e7e69ea3384874304911625ac34321c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7e7e69ea3384874304911625ac34321c-Reviews.html",
        "metareview": "",
        "pdf_size": 4953616,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=285212309354182083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Toronto; University of Toronto",
        "aff_domain": "psi.toronto.edu;psi.toronto.edu",
        "email": "psi.toronto.edu;psi.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7e7e69ea3384874304911625ac34321c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Pixels to Graphs by Associative Embedding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9005",
        "id": "9005",
        "author_site": "Alejandro Newell, Jia Deng",
        "author": "Alejandro Newell; Jia Deng",
        "abstract": "Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph definition. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and demonstrate state-of-the-art performance on the challenging task of scene graph generation.",
        "bibtex": "@inproceedings{NIPS2017_84438b7a,\n author = {Newell, Alejandro and Deng, Jia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Pixels to Graphs by Associative Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/84438b7aae55a0638073ef798e50b4ef-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/84438b7aae55a0638073ef798e50b4ef-Reviews.html",
        "metareview": "",
        "pdf_size": 5354138,
        "gs_citation": 269,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16469794244318812521&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science and Engineering, University of Michigan, Ann Arbor; Computer Science and Engineering, University of Michigan, Ann Arbor",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/84438b7aae55a0638073ef798e50b4ef-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Plan, Attend, Generate: Planning for Sequence-to-Sequence Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9322",
        "id": "9322",
        "author_site": "Caglar Gulcehre, Francis Dutil, Adam Trischler, Yoshua Bengio",
        "author": "Caglar Gulcehre; Francis Dutil; Adam Trischler; Yoshua Bengio",
        "abstract": "We investigate the integration of a planning mechanism into sequence-to-sequence models using attention. We develop a model which can plan ahead in the future when it computes its alignments between input and output sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the recently proposed strategic attentive reader and writer (STRAW) model for Reinforcement Learning. Our proposed model is end-to-end trainable using primarily differentiable operations. We show that it outperforms a strong baseline on character-level translation tasks from WMT'15, the algorithmic task of finding Eulerian circuits of graphs, and question generation from the text. Our analysis demonstrates that the model computes qualitatively intuitive alignments, converges faster than the baselines, and achieves superior performance with fewer parameters.",
        "bibtex": "@inproceedings{NIPS2017_b030afbb,\n author = {Gulcehre, Caglar and Dutil, Francis and Trischler, Adam and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Plan, Attend, Generate: Planning for Sequence-to-Sequence Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b030afbb3a8af8fb0759241c97466ee4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b030afbb3a8af8fb0759241c97466ee4-Reviews.html",
        "metareview": "",
        "pdf_size": 796421,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=922022945805507783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Montreal (MILA); University of Montreal (MILA); Microsoft Research Maluuba; University of Montreal (MILA)",
        "aff_domain": "gmail.com;gmail.com;microsoft.com;gmail.com",
        "email": "gmail.com;gmail.com;microsoft.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b030afbb3a8af8fb0759241c97466ee4-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Montreal (MILA);Microsoft Research Maluuba",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9404",
        "id": "9404",
        "author_site": "Maximilian Nickel, Douwe Kiela",
        "author": "Maximillian Nickel; Douwe Kiela",
        "abstract": "Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\u00e9 ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\u00e9 embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.",
        "bibtex": "@inproceedings{NIPS2017_59dfa2df,\n author = {Nickel, Maximillian and Kiela, Douwe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Poincar\\'{e} Embeddings for Learning Hierarchical Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Reviews.html",
        "metareview": "",
        "pdf_size": 1365887,
        "gs_citation": 1702,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6233393442090324202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com",
        "email": "fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Facebook",
        "aff_unique_dep": "Facebook AI Research",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9286",
        "id": "9286",
        "author_site": "Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas Guibas",
        "author": "Charles Ruizhongtai Qi; Li Yi; Hao Su; Leonidas Guibas",
        "abstract": "Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
        "bibtex": "@inproceedings{NIPS2017_d8bf84be,\n author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Reviews.html",
        "metareview": "",
        "pdf_size": 3758839,
        "gs_citation": 14360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12360107934915291237&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html"
    },
    {
        "title": "Policy Gradient With Value Function Approximation For Collective Multiagent Planning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9211",
        "id": "9211",
        "author_site": "Duc Thien Nguyen, Akshat Kumar, Hoong Chuin Lau",
        "author": "Duc Thien Nguyen; Akshat Kumar; Hoong Chuin Lau",
        "abstract": "Decentralized (PO)MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity, recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDec-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing CDec-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this, we show how a particular decomposition of the approximate action-value function over agents leads to effective updates, and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches.",
        "bibtex": "@inproceedings{NIPS2017_c2ba1bc5,\n author = {Nguyen, Duc Thien and Kumar, Akshat and Lau, Hoong Chuin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Policy Gradient With Value Function Approximation For Collective Multiagent Planning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c2ba1bc54b239208cb37b901c0d3b363-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c2ba1bc54b239208cb37b901c0d3b363-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c2ba1bc54b239208cb37b901c0d3b363-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c2ba1bc54b239208cb37b901c0d3b363-Reviews.html",
        "metareview": "",
        "pdf_size": 602796,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3800543031239137492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html"
    },
    {
        "title": "Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9219",
        "id": "9219",
        "author_site": "Qian Yu, Mohammad Maddah-Ali, Salman Avestimehr",
        "author": "Qian Yu; Mohammad Maddah-Ali; Salman Avestimehr",
        "abstract": "We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to optimally deal with straggling workers. The proposed strategy, named as \\emph{polynomial codes}, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. This is the first code that achieves the optimal utilization of redundancy for tolerating stragglers or failures in distributed matrix multiplication. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently.   Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics including computation latency and communication load.  Moreover, we extend this code to distributed convolution and show its order-wise optimality.",
        "bibtex": "@inproceedings{NIPS2017_e6c2dc3d,\n author = {Yu, Qian and Maddah-Ali, Mohammad and Avestimehr, Salman},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6c2dc3dee4a51dcec3a876aa2339a78-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6c2dc3dee4a51dcec3a876aa2339a78-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e6c2dc3dee4a51dcec3a876aa2339a78-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6c2dc3dee4a51dcec3a876aa2339a78-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6c2dc3dee4a51dcec3a876aa2339a78-Reviews.html",
        "metareview": "",
        "pdf_size": 951973,
        "gs_citation": 548,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11236355776396683403&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; Nokia Bell Labs, Holmdel, NJ, USA; Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6c2dc3dee4a51dcec3a876aa2339a78-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Southern California;Nokia Bell Labs, Holmdel, NJ, USA",
        "aff_unique_dep": "Department of Electrical Engineering;",
        "aff_unique_url": "https://www.usc.edu;",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Polynomial time algorithms for dual volume sampling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9280",
        "id": "9280",
        "author_site": "Chengtao Li, Stefanie Jegelka, Suvrit Sra",
        "author": "Chengtao Li; Stefanie Jegelka; Suvrit Sra",
        "abstract": "We study dual volume sampling, a method for selecting k columns from an n*m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume spanned by the rows of the induced submatrix. This method was proposed by Avron and Boutsidis (2013), who showed it to be a promising method for column subset selection and its multiple applications. However, its wider adoption has been hampered by the lack of polynomial time sampling algorithms. We remove this hindrance by developing an exact (randomized) polynomial time sampling algorithm as well as its derandomization. Thereafter, we study dual volume sampling via the theory of real stable polynomials and prove that its distribution satisfies the \u201cStrong Rayleigh\u201d property. This result has numerous consequences, including a provably fast-mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners. This sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well.",
        "bibtex": "@inproceedings{NIPS2017_18bb68e2,\n author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Polynomial time algorithms for dual volume sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Reviews.html",
        "metareview": "",
        "pdf_size": 489847,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=295103658939388332&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "MIT; MIT; MIT",
        "aff_domain": "mit.edu;csail.mit.edu;mit.edu",
        "email": "mit.edu;csail.mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/18bb68e2b38e4a8ce7cf4f6b2625768c-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Population Matching Discrepancy and Applications in Deep Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9397",
        "id": "9397",
        "author_site": "Jianfei Chen, Chongxuan LI, Yizhong Ru, Jun Zhu",
        "author": "Jianfei Chen; Chongxuan LI; Yizhong Ru; Jun Zhu",
        "abstract": "A differentiable estimation of the distance between two distributions based on samples is important for many deep learning tasks. One such estimation  is maximum mean discrepancy (MMD). However, MMD suffers from its sensitive kernel bandwidth hyper-parameter, weak gradients, and large mini-batch size when used as a training objective. In this paper, we propose population matching discrepancy (PMD) for estimating the distribution distance based on samples, as well as an algorithm to learn the parameters of the distributions using PMD as an objective. PMD is defined as the minimum weight matching of sample populations from each distribution, and we prove that PMD is a strongly consistent estimator of the first Wasserstein metric. We apply PMD to two deep learning tasks, domain adaptation and generative modeling. Empirical results demonstrate that PMD overcomes the aforementioned drawbacks of MMD, and outperforms MMD on both tasks in terms of the performance as well as the convergence speed.",
        "bibtex": "@inproceedings{NIPS2017_2b45e8d6,\n author = {Chen, Jianfei and LI, Chongxuan and Ru, Yizhong and Zhu, Jun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Population Matching Discrepancy and Applications in Deep Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2b45e8d6abf59038a975faeeb6dc0782-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2b45e8d6abf59038a975faeeb6dc0782-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2b45e8d6abf59038a975faeeb6dc0782-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2b45e8d6abf59038a975faeeb6dc0782-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2b45e8d6abf59038a975faeeb6dc0782-Reviews.html",
        "metareview": "",
        "pdf_size": 313617,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9447491471685586952&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab for Intell. Tech. & Sys., Tsinghua University, Beijing, 100084, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab for Intell. Tech. & Sys., Tsinghua University, Beijing, 100084, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab for Intell. Tech. & Sys., Tsinghua University, Beijing, 100084, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab for Intell. Tech. & Sys., Tsinghua University, Beijing, 100084, China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2b45e8d6abf59038a975faeeb6dc0782-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab for Intell. Tech. & Sys., Tsinghua University, Beijing, 100084, China",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Pose Guided Person Image Generation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8837",
        "id": "8837",
        "author_site": "Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool",
        "author": "Liqian Ma; Xu Jia; Qianru Sun; Bernt Schiele; Tinne Tuytelaars; Luc Van Gool",
        "abstract": "This paper proposes the novel Pose Guided Person Generation Network (PG$^2$) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG$^2$ utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128$\\times$64 re-identification images and 256$\\times$256 fashion photos show that our model generates high-quality person images with convincing details.",
        "bibtex": "@inproceedings{NIPS2017_34ed066d,\n author = {Ma, Liqian and Jia, Xu and Sun, Qianru and Schiele, Bernt and Tuytelaars, Tinne and Van Gool, Luc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Pose Guided Person Image Generation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/34ed066df378efacc9b924ec161e7639-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/34ed066df378efacc9b924ec161e7639-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/34ed066df378efacc9b924ec161e7639-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/34ed066df378efacc9b924ec161e7639-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/34ed066df378efacc9b924ec161e7639-Reviews.html",
        "metareview": "",
        "pdf_size": 1431981,
        "gs_citation": 1080,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4891149081490254273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/34ed066df378efacc9b924ec161e7639-Abstract.html"
    },
    {
        "title": "Position-based Multiple-play Bandit Problem with Unknown Position Bias",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9276",
        "id": "9276",
        "author_site": "Junpei Komiyama, Junya Honda, Akiko Takeda",
        "author": "Junpei Komiyama; Junya Honda; Akiko Takeda",
        "abstract": "Motivated by online advertising, we study a multiple-play multi-armed bandit problem with position bias that involves several slots and the latter slots yield fewer rewards. We characterize the hardness of the problem by deriving an asymptotic regret bound. We propose the Permutation Minimum Empirical Divergence (PMED) algorithm and derive its asymptotically optimal regret bound. Because of the uncertainty of the position bias, the optimal algorithm for such a problem requires non-convex optimizations that are different from usual partial monitoring and semi-bandit problems. We propose a cutting-plane method and related bi-convex relaxation for these optimizations by using auxiliary variables.",
        "bibtex": "@inproceedings{NIPS2017_c57168a9,\n author = {Komiyama, Junpei and Honda, Junya and Takeda, Akiko},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Position-based Multiple-play Bandit Problem with Unknown Position Bias},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c57168a952f5d46724cf35dfc3d48a7f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c57168a952f5d46724cf35dfc3d48a7f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c57168a952f5d46724cf35dfc3d48a7f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c57168a952f5d46724cf35dfc3d48a7f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c57168a952f5d46724cf35dfc3d48a7f-Reviews.html",
        "metareview": "",
        "pdf_size": 542432,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4826477757937357592&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "The University of Tokyo; The University of Tokyo / RIKEN; The Institute of Statistical Mathematics / RIKEN",
        "aff_domain": "komiyama.info;stat.t.u-tokyo.ac.jp;ism.ac.jp",
        "email": "komiyama.info;stat.t.u-tokyo.ac.jp;ism.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c57168a952f5d46724cf35dfc3d48a7f-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Tokyo;The University of Tokyo;The Institute of Statistical Mathematics / RIKEN",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.u-tokyo.ac.jp;",
        "aff_unique_abbr": "UTokyo;UTokyo;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan;"
    },
    {
        "title": "Positive-Unlabeled Learning with Non-Negative Risk Estimator",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8958",
        "id": "8958",
        "author_site": "Ryuichi Kiryo, Gang Niu, Marthinus C du Plessis, Masashi Sugiyama",
        "author": "Ryuichi Kiryo; Gang Niu; Marthinus C du Plessis; Masashi Sugiyama",
        "abstract": "From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning, in which the state of the art is unbiased PU learning. However, if its model is very flexible, empirical risks on training data will go negative, and we will suffer from serious overfitting. In this paper, we propose a non-negative risk estimator for PU learning: when getting minimized, it is more robust against overfitting, and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover, we analyze the bias, consistency, and mean-squared-error reduction of the proposed risk estimator, and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.",
        "bibtex": "@inproceedings{NIPS2017_7cce53cf,\n author = {Kiryo, Ryuichi and Niu, Gang and du Plessis, Marthinus C and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Positive-Unlabeled Learning with Non-Negative Risk Estimator},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7cce53cf90577442771720a370c3c723-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7cce53cf90577442771720a370c3c723-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7cce53cf90577442771720a370c3c723-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7cce53cf90577442771720a370c3c723-Reviews.html",
        "metareview": "",
        "pdf_size": 687392,
        "gs_citation": 612,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11887707764419291839&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "The University of Tokyo; RIKEN; The University of Tokyo; RIKEN",
        "aff_domain": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp; ;ms.k.u-tokyo.ac.jp",
        "email": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp; ;ms.k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7cce53cf90577442771720a370c3c723-Abstract.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Tokyo;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "UTokyo;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Posterior sampling for reinforcement learning: worst-case regret bounds",
        "author": "Shipra Agrawal, Randy Jia",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10031",
        "id": "10031"
    },
    {
        "title": "Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8973",
        "id": "8973",
        "author_site": "Luigi Acerbi, Wei Ji",
        "author": "Luigi Acerbi; Wei Ji Ma",
        "abstract": "Computational models in fields such as computational neuroscience  are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning.  Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.",
        "bibtex": "@inproceedings{NIPS2017_df0aab05,\n author = {Acerbi, Luigi and Ma, Wei Ji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Reviews.html",
        "metareview": "",
        "pdf_size": 740803,
        "gs_citation": 327,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7209174494000095753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Neural Science, New York University + D\u00e9partement des neurosciences fondamentales, Universit\u00e9 de Gen\u00e8ve; Center for Neural Science & Dept. of Psychology, New York University",
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/df0aab058ce179e4f7ab135ed4e641a9-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Center for Neural Science, New York University;D\u00e9partement des neurosciences fondamentales, Universit\u00e9 de Gen\u00e8ve;Center for Neural Science & Dept. of Psychology, New York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Practical Data-Dependent Metric Compression with Provable Guarantees",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9048",
        "id": "9048",
        "author_site": "Piotr Indyk, Ilya Razenshteyn, Tal Wagner",
        "author": "Piotr Indyk; Ilya Razenshteyn; Tal Wagner",
        "abstract": "We introduce a new distance-preserving compact representation of multi-dimensional point-sets. Given n points in a d-dimensional space where each coordinate is represented using B bits (i.e., dB bits per point), it produces  a representation of size O( d log(d B/epsilon) +log n) bits per point from which one can approximate the distances up to a factor of 1 + epsilon. Our algorithm almost matches the recent bound of Indyk et al, 2017} while being much simpler. We compare our algorithm to Product Quantization (PQ) (Jegou et al, 2011) a state of the art heuristic metric compression method. We evaluate both algorithms on several data sets: SIFT, MNIST, New York City taxi time series and a synthetic one-dimensional data set embedded in a high-dimensional space. Our algorithm produces representations that are comparable to or better than those produced by PQ, while having provable guarantees on its performance.",
        "bibtex": "@inproceedings{NIPS2017_49b8b4f9,\n author = {Indyk, Piotr and Razenshteyn, Ilya and Wagner, Tal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Data-Dependent Metric Compression with Provable Guarantees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/49b8b4f95f02e055801da3b4f58e28b7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/49b8b4f95f02e055801da3b4f58e28b7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/49b8b4f95f02e055801da3b4f58e28b7-Reviews.html",
        "metareview": "",
        "pdf_size": 600761,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17645011235606130033&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/49b8b4f95f02e055801da3b4f58e28b7-Abstract.html"
    },
    {
        "title": "Practical Hash Functions for Similarity Estimation and Dimensionality Reduction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9430",
        "id": "9430",
        "author_site": "S\u00f8ren Dahlgaard, Mathias Knudsen, Mikkel Thorup",
        "author": "S\u00f8ren Dahlgaard; Mathias Knudsen; Mikkel Thorup",
        "abstract": "Hashing is a basic tool for dimensionality reduction employed in several aspects of machine learning. However, the perfomance analysis is often carried out under the abstract assumption that a truly random unit cost hash function is used, without concern for which concrete hash function is employed. The concrete hash function may work fine on sufficiently random input. The question is if it can be trusted in the real world when faced with more structured input.  In this paper we focus on two prominent applications of hashing, namely similarity estimation with the one permutation hashing (OPH) scheme of Li et al. [NIPS'12] and feature hashing (FH) of Weinberger et al. [ICML'09], both of which have found numerous applications, i.e. in approximate near-neighbour search with LSH and large-scale classification with SVM.      We consider the recent mixed tabulation hash function of Dahlgaard et al. [FOCS'15] which was proved theoretically to perform like a truly random hash function in many applications, including the above OPH. Here we first show improved concentration bounds for FH with truly random hashing and then argue that mixed tabulation performs similar when the input vectors are sparse. Our main contribution, however, is an experimental comparison of different hashing schemes when used inside FH, OPH, and LSH.  We find that mixed tabulation hashing is almost as fast as the classic multiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work well on sufficiently random data, but we demonstrate that in the above applications, it can lead to bias and poor concentration on both real-world and synthetic data. We also compare with the very popular MurmurHash3, which has no proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to truly random hashing in our experiments. However, mixed tabulation was 40% faster than MurmurHash3, and it has the proven guarantee of good performance on all possible input making it more reliable.",
        "bibtex": "@inproceedings{NIPS2017_62dad6e2,\n author = {Dahlgaard, S\\o ren and Knudsen, Mathias and Thorup, Mikkel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Hash Functions for Similarity Estimation and Dimensionality Reduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/62dad6e273d32235ae02b7d321578ee8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/62dad6e273d32235ae02b7d321578ee8-Reviews.html",
        "metareview": "",
        "pdf_size": 456092,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2383980242610059240&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Copenhagen / SupWiz; University of Copenhagen / SupWiz; University of Copenhagen",
        "aff_domain": "supwiz.com;supwiz.com;di.ku.dk",
        "email": "supwiz.com;supwiz.com;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/62dad6e273d32235ae02b7d321578ee8-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Copenhagen / SupWiz;University of Copenhagen",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ku.dk",
        "aff_unique_abbr": ";UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Denmark"
    },
    {
        "title": "Practical Locally Private Heavy Hitters",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9016",
        "id": "9016",
        "author_site": "Raef Bassily, Kobbi Nissim, Uri Stemmer, Abhradeep Guha Thakurta",
        "author": "Raef Bassily; Kobbi Nissim; Uri Stemmer; Abhradeep Guha Thakurta",
        "abstract": "We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error -- TreeHist and Bitstogram. In both algorithms, server running time is $\\tilde O(n)$ and user running time is $\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $\\tilde O(n^{5/2})$ server time and $\\tilde O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for the use of such algorithms in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.",
        "bibtex": "@inproceedings{NIPS2017_3d779cae,\n author = {Bassily, Raef and Nissim, Kobbi and Stemmer, Uri and Guha Thakurta, Abhradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Locally Private Heavy Hitters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3d779cae2d46cf6a8a99a35ba4167977-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3d779cae2d46cf6a8a99a35ba4167977-Reviews.html",
        "metareview": "",
        "pdf_size": 389210,
        "gs_citation": 281,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1895862300640100749&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science & Engineering, The Ohio State University; Department of Computer Science, Georgetown University; Center for Research on Computation and Society (CRCS), Harvard University; Department of Computer Science, University of California Santa Cruz",
        "aff_domain": "osu.edu;georgetown.edu;uri.co.il;ucsc.edu",
        "email": "osu.edu;georgetown.edu;uri.co.il;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3d779cae2d46cf6a8a99a35ba4167977-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "The Ohio State University;Georgetown University;Center for Research on Computation and Society (CRCS), Harvard University;Department of Computer Science, University of California Santa Cruz",
        "aff_unique_dep": "Department of Computer Science & Engineering;Department of Computer Science;;",
        "aff_unique_url": "https://www.osu.edu;https://www.georgetown.edu;;",
        "aff_unique_abbr": "OSU;GU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8882",
        "id": "8882",
        "author_site": "Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S Yu",
        "author": "Yunbo Wang; Mingsheng Long; Jianmin Wang; Zhifeng Gao; Philip S Yu",
        "abstract": "The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical frames, where spatial appearances and temporal variations are two crucial structures. This paper models these structures by presenting a predictive recurrent neural network (PredRNN). This architecture is enlightened by the idea that spatiotemporal predictive learning should memorize both spatial appearances and temporal variations in a unified memory pool. Concretely, memory states are no longer constrained inside each LSTM unit. Instead, they are allowed to zigzag in two directions: across stacked RNN layers vertically and through all RNN states horizontally. The core of this network is a new Spatiotemporal LSTM (ST-LSTM) unit that extracts and memorizes spatial and temporal  representations simultaneously. PredRNN achieves the state-of-the-art prediction performance on three video prediction datasets and is a more general framework, that can be easily extended to other predictive learning tasks by integrating with other architectures.",
        "bibtex": "@inproceedings{NIPS2017_e5f6ad6c,\n author = {Wang, Yunbo and Long, Mingsheng and Wang, Jianmin and Gao, Zhifeng and Yu, Philip S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Reviews.html",
        "metareview": "",
        "pdf_size": 5009735,
        "gs_citation": 1099,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4329054855101528165&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Software Tsinghua University; School of Software Tsinghua University; School of Software Tsinghua University; School of Software Tsinghua University; School of Software Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;mails.tsinghua.edu.cn;uic.edu",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;mails.tsinghua.edu.cn;uic.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e5f6ad6ce374177eef023bf5d0c018b6-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "School of Software Tsinghua University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9047",
        "id": "9047",
        "author_site": "Wengong Jin, Connor Coley, Regina Barzilay, Tommi Jaakkola",
        "author": "Wengong Jin; Connor Coley; Regina Barzilay; Tommi Jaakkola",
        "abstract": "The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products.  The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.",
        "bibtex": "@inproceedings{NIPS2017_ced556cd,\n author = {Jin, Wengong and Coley, Connor and Barzilay, Regina and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ced556cd9f9c0c8315cfbe0744a3baf0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ced556cd9f9c0c8315cfbe0744a3baf0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ced556cd9f9c0c8315cfbe0744a3baf0-Reviews.html",
        "metareview": "",
        "pdf_size": 1962880,
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9471682499228718652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Artificial Intelligence Lab, MIT; Department of Chemical Engineering, MIT; Computer Science and Artificial Intelligence Lab, MIT; Computer Science and Artificial Intelligence Lab, MIT",
        "aff_domain": "csail.mit.edu;mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;mit.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ced556cd9f9c0c8315cfbe0744a3baf0-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Department of Chemical Engineering, MIT",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Lab;",
        "aff_unique_url": "https://www.csail.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Predicting Scene Parsing and Motion Dynamics in the Future",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9458",
        "id": "9458",
        "author_site": "Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng Chen, Zequn Jie, Jiashi Feng, Shuicheng Yan",
        "author": "Xiaojie Jin; Huaxin Xiao; Xiaohui Shen; Jimei Yang; Zhe Lin; Yunpeng Chen; Zequn Jie; Jiashi Feng; Shuicheng Yan",
        "abstract": "It is important for intelligent systems, e.g. autonomous vehicles and robotics to anticipate the future in order to plan early and make decisions accordingly. Predicting the future scene parsing and motion dynamics helps the agents better understand the visual environment better as the former provides dense semantic segmentations, i.e. what objects will be present and where they will appear, while the latter provides dense motion information, i.e. how the objects move in the future. In this paper, we propose a novel model to predict the scene parsing and motion dynamics in unobserved future video frames simultaneously. Using history information (preceding frames and corresponding scene parsing results) as input, our model is able to predict the scene parsing and motion for arbitrary time steps ahead. More importantly, our model is superior compared to other methods that predict parsing and motion separately, as the complementary relationship between the two tasks are fully utilized in our model through joint learning. To our best knowledge, this is the first attempt in jointly predicting scene parsing and motion dynamics in the future frames. On the large-scale Cityscapes dataset, it is demonstrated that our model produces significantly better parsing and motion prediction results compared to well established baselines. In addition, we also show our model can be used to predict the steering angle of the vehicles, which further verifies the ability of our model to learn underlying latent parameters.",
        "bibtex": "@inproceedings{NIPS2017_73fed7fd,\n author = {Jin, Xiaojie and Xiao, Huaxin and Shen, Xiaohui and Yang, Jimei and Lin, Zhe and Chen, Yunpeng and Jie, Zequn and Feng, Jiashi and Yan, Shuicheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Scene Parsing and Motion Dynamics in the Future},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/73fed7fd472e502d8908794430511f4d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/73fed7fd472e502d8908794430511f4d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/73fed7fd472e502d8908794430511f4d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/73fed7fd472e502d8908794430511f4d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/73fed7fd472e502d8908794430511f4d-Reviews.html",
        "metareview": "",
        "pdf_size": 1079796,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8789886126582489258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/73fed7fd472e502d8908794430511f4d-Abstract.html"
    },
    {
        "title": "Predicting User Activity Level In Point Processes With Mass Transport Equation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8955",
        "id": "8955",
        "author_site": "Yichen Wang, Xiaojing Ye, Hongyuan Zha, Le Song",
        "author": "Yichen Wang; Xiaojing Ye; Hongyuan Zha; Le Song",
        "abstract": "Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an unbiased estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to state-of-arts.",
        "bibtex": "@inproceedings{NIPS2017_a34bacf8,\n author = {Wang, Yichen and Ye, Xiaojing and Zha, Hongyuan and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting User Activity Level In Point Processes With Mass Transport Equation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a34bacf839b923770b2c360eefa26748-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a34bacf839b923770b2c360eefa26748-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a34bacf839b923770b2c360eefa26748-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a34bacf839b923770b2c360eefa26748-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a34bacf839b923770b2c360eefa26748-Reviews.html",
        "metareview": "",
        "pdf_size": 925125,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11091661881162994213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "College of Computing, Georgia Institute of Technology; School of Mathematics, Georgia State University; College of Computing, Georgia Institute of Technology + Ant Financial; College of Computing, Georgia Institute of Technology + Ant Financial",
        "aff_domain": "gatech.edu;gsu.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;gsu.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a34bacf839b923770b2c360eefa26748-Abstract.html",
        "aff_unique_index": "0;1;0+2;0+2",
        "aff_unique_norm": "Georgia Institute of Technology;School of Mathematics, Georgia State University;Ant Financial",
        "aff_unique_dep": "College of Computing;;",
        "aff_unique_url": "https://www.gatech.edu;;https://www.antgroup.com",
        "aff_unique_abbr": "Georgia Tech;;Ant Financial",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;0+2;0+2",
        "aff_country_unique": "United States;;China"
    },
    {
        "title": "Predictive State Recurrent Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9377",
        "id": "9377",
        "author_site": "Carlton Downey, Ahmed Hefny, Byron Boots, Geoffrey Gordon, Boyue Li",
        "author": "Carlton Downey; Ahmed Hefny; Byron Boots; Geoffrey J. Gordon; Boyue Li",
        "abstract": "We present a new model, Predictive State Recurrent Neural Networks (PSRNNs), for filtering and prediction in dynamical systems. PSRNNs draw on insights from both Recurrent Neural Networks (RNNs) and Predictive State Representations (PSRs), and inherit advantages from both types of models. Like many successful RNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer functions to combine information from multiple sources. We show that such bilinear functions arise naturally from state updates in Bayes filters like PSRs, in which observations can be viewed as gating belief states. We also show that PSRNNs can be learned effectively by combining Backpropogation Through Time (BPTT) with an initialization  derived from a statistically consistent learning algorithm for PSRs called two-stage regression (2SR). Finally, we show that PSRNNs can be  factorized using tensor decomposition, reducing model size and suggesting interesting connections to existing multiplicative architectures such as LSTMs and GRUs. We apply PSRNNs to 4 datasets, and show that we outperform several popular alternative approaches to modeling dynamical systems in all cases.",
        "bibtex": "@inproceedings{NIPS2017_2bb0502c,\n author = {Downey, Carlton and Hefny, Ahmed and Boots, Byron and Gordon, Geoffrey J and Li, Boyue},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predictive State Recurrent Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2bb0502c80b7432eee4c5847a5fd077b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2bb0502c80b7432eee4c5847a5fd077b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2bb0502c80b7432eee4c5847a5fd077b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2bb0502c80b7432eee4c5847a5fd077b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2bb0502c80b7432eee4c5847a5fd077b-Reviews.html",
        "metareview": "",
        "pdf_size": 818345,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10957869049170310093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Georgia Tech; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cc.gatech.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cc.gatech.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2bb0502c80b7432eee4c5847a5fd077b-Abstract.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.gatech.edu",
        "aff_unique_abbr": "CMU;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Predictive-State Decoders: Encoding the Future into Recurrent Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8910",
        "id": "8910",
        "author_site": "Arun Venkatraman, Nicholas Rhinehart, Wen Sun, Lerrel Pinto, Martial Hebert, Byron Boots, Kris Kitani, J. Bagnell",
        "author": "Arun Venkatraman; Nicholas Rhinehart; Wen Sun; Lerrel Pinto; Martial Hebert; Byron Boots; Kris Kitani; J. Bagnell",
        "abstract": "Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representation that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statistics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with Predictive-State Decoders (PSDs), which add supervision to the network's internal state representation to target predicting future observations. PSDs are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains:  probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data.",
        "bibtex": "@inproceedings{NIPS2017_61b4a64b,\n author = {Venkatraman, Arun and Rhinehart, Nicholas and Sun, Wen and Pinto, Lerrel and Hebert, Martial and Boots, Byron and Kitani, Kris and Bagnell, J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predictive-State Decoders: Encoding the Future into Recurrent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/61b4a64be663682e8cb037d9719ad8cd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/61b4a64be663682e8cb037d9719ad8cd-Reviews.html",
        "metareview": "",
        "pdf_size": 6678680,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9991312733852191744&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA; The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA; The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA; The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA; The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA; School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA; The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA; The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu; ; ; ; ; ; ",
        "email": "cs.cmu.edu;cs.cmu.edu; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;1;0;0",
        "aff_unique_norm": "The Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA;Georgia Institute of Technology",
        "aff_unique_dep": ";School of Interactive Computing",
        "aff_unique_url": ";https://www.gatech.edu",
        "aff_unique_abbr": ";Georgia Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Premise Selection for Theorem Proving by Deep Graph Embedding",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9064",
        "id": "9064",
        "author_site": "Mingzhe Wang, Yihe Tang, Jian Wang, Jia Deng",
        "author": "Mingzhe Wang; Yihe Tang; Jian Wang; Jia Deng",
        "abstract": "We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%.",
        "bibtex": "@inproceedings{NIPS2017_18d10dc6,\n author = {Wang, Mingzhe and Tang, Yihe and Wang, Jian and Deng, Jia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Premise Selection for Theorem Proving by Deep Graph Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/18d10dc6e666eab6de9215ae5b3d54df-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/18d10dc6e666eab6de9215ae5b3d54df-Reviews.html",
        "metareview": "",
        "pdf_size": 1731243,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7579291535270469187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html"
    },
    {
        "title": "Preventing Gradient Explosions in Gated Recurrent Units",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8840",
        "id": "8840",
        "author_site": "Sekitoshi Kanai, Yasuhiro Fujiwara, Sotetsu Iwamura",
        "author": "Sekitoshi Kanai; Yasuhiro Fujiwara; Sotetsu Iwamura",
        "abstract": "A gated recurrent unit (GRU) is a successful recurrent neural network architecture for time-series data. The GRU is typically trained using a gradient-based method, which is subject to the exploding gradient problem in which the gradient increases significantly. This problem is caused by an abrupt change in the dynamics of the GRU due to a small variation in the parameters. In this paper, we find a condition under which the dynamics of the GRU changes drastically and propose a learning method to address the exploding gradient problem. Our method constrains the dynamics of the GRU so that it does not drastically change. We evaluated our method in experiments on language modeling and polyphonic music modeling. Our experiments showed that our method can prevent the exploding gradient problem and improve modeling accuracy.",
        "bibtex": "@inproceedings{NIPS2017_f2fc9902,\n author = {Kanai, Sekitoshi and Fujiwara, Yasuhiro and Iwamura, Sotetsu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Preventing Gradient Explosions in Gated Recurrent Units},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f2fc990265c712c49d51a18a32b39f0c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f2fc990265c712c49d51a18a32b39f0c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f2fc990265c712c49d51a18a32b39f0c-Reviews.html",
        "metareview": "",
        "pdf_size": 352542,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16526936410517893799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html"
    },
    {
        "title": "Principles of Riemannian Geometry  in Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9066",
        "id": "9066",
        "author_site": "Michael Hauser, Asok Ray",
        "author": "Michael Hauser; Asok Ray",
        "abstract": "This study deals with neural networks in the sense of geometric transformations acting on the coordinate representation of the underlying data manifold which the data is sampled from. It forms part of an attempt to construct a formalized general theory of neural networks in the setting of Riemannian geometry. From this perspective, the following theoretical results are developed and proven for feedforward networks. First it is shown that residual neural networks are finite difference approximations to dynamical systems of first order differential equations, as opposed to ordinary networks that are static. This implies that the network is learning systems of differential equations governing the coordinate transformations that represent the data. Second it is shown that a closed form solution of the metric tensor on the underlying data manifold can be found by backpropagating the coordinate representations learned by the neural network itself. This is formulated in a formal abstract sense as a sequence of Lie group actions on the metric fibre space in the principal and associated bundles on the data manifold. Toy experiments were run to confirm parts of the proposed theory, as well as to provide intuitions as to how neural networks operate on data.",
        "bibtex": "@inproceedings{NIPS2017_0ebcc77d,\n author = {Hauser, Michael and Ray, Asok},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Principles of Riemannian Geometry  in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0ebcc77dc72360d0eb8e9504c78d38bd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0ebcc77dc72360d0eb8e9504c78d38bd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0ebcc77dc72360d0eb8e9504c78d38bd-Reviews.html",
        "metareview": "",
        "pdf_size": 19520677,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15690707017033951438&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Mechanical Engineering, Pennsylvania State University; Department of Mechanical Engineering, Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu",
        "email": "psu.edu;psu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Mechanical Engineering, Pennsylvania State University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8809",
        "id": "8809",
        "author_site": "Chris Oates, Steven Niederer, Angela Lee, Fran\u00e7ois-Xavier Briol, Mark Girolami",
        "author": "Chris Oates; Steven Niederer; Angela Lee; Fran\u00e7ois-Xavier Briol; Mark Girolami",
        "abstract": "This paper studies the numerical computation of integrals, representing estimates or predictions, over the output $f(x)$ of a computational model with respect to a distribution $p(\\mathrm{d}x)$ over uncertain inputs $x$ to the model. For the functional cardiac models that motivate this work, neither $f$ nor $p$ possess a closed-form expression and evaluation of either requires $\\approx$ 100 CPU hours, precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem, with a joint model for both the a priori unknown function $f$ and the a priori unknown distribution $p$. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account, in a statistically principled manner, for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment.",
        "bibtex": "@inproceedings{NIPS2017_98dce83d,\n author = {Oates, Chris and Niederer, Steven and Lee, Angela and Briol, Fran\\c{c}ois-Xavier and Girolami, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/98dce83da57b0395e163467c9dae521b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/98dce83da57b0395e163467c9dae521b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/98dce83da57b0395e163467c9dae521b-Reviews.html",
        "metareview": "",
        "pdf_size": 846628,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6862985338751713809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Newcastle University; King\u2019s College London; King\u2019s College London; University of Warwick; Imperial College London + Alan Turing Institute",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/98dce83da57b0395e163467c9dae521b-Abstract.html",
        "aff_unique_index": "0;1;1;2;3+4",
        "aff_unique_norm": "Newcastle University;King's College London;University of Warwick;Imperial College London;Alan Turing Institute",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ncl.ac.uk;https://www.kcl.ac.uk;https://www.warwick.ac.uk;https://www.imperial.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "NU;KCL;Warwick;ICL;ATI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Probabilistic Rule Realization and Selection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8947",
        "id": "8947",
        "author_site": "Haizi Yu, Tianxi Li, Lav Varshney",
        "author": "Haizi Yu; Tianxi Li; Lav R. Varshney",
        "abstract": "Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through \\emph{rules}: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis).",
        "bibtex": "@inproceedings{NIPS2017_7b13b220,\n author = {Yu, Haizi and Li, Tianxi and Varshney, Lav R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Rule Realization and Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7b13b2203029ed80337f27127a9f1d28-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7b13b2203029ed80337f27127a9f1d28-Reviews.html",
        "metareview": "",
        "pdf_size": 748186,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1171933578433088249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Statistics, University of Michigan; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;umich.edu;illinois.edu",
        "email": "illinois.edu;umich.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;University of Michigan",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics",
        "aff_unique_url": "https://illinois.edu;https://www.umich.edu",
        "aff_unique_abbr": "UIUC;UM",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Urbana-Champaign;Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Process-constrained batch Bayesian optimisation",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9125",
        "id": "9125",
        "author_site": "Pratibha Vellanki, Santu Rana, Sunil Gupta, David Rubin, Alessandra Sutti, Thomas Dorin, Murray Height, Paul Sanders, Svetha Venkatesh",
        "author": "Pratibha Vellanki; Santu Rana; Sunil Gupta; David Rubin; Alessandra Sutti; Thomas Dorin; Murray Height; Paul Sanders; Svetha Venkatesh",
        "abstract": "Abstract Prevailing batch Bayesian optimisation methods allow all control variables to be freely altered at each iteration. Real-world experiments, however, often have physical limitations making it time-consuming to alter all settings for each recommendation in a batch. This gives rise to a unique problem in BO: in a recommended batch, a set of variables that are expensive to experimentally change need to be fixed, while the remaining control variables can be varied. We formulate this as a process-constrained batch Bayesian optimisation problem. We propose two algorithms, pc-BO(basic) and pc-BO(nested). pc-BO(basic) is simpler but lacks convergence guarantee. In contrast pc-BO(nested) is slightly more complex, but admits convergence analysis. We show that the regret of pc-BO(nested) is sublinear. We demonstrate the performance of both pc-BO(basic) and pc-BO(nested) by optimising benchmark test functions, tuning hyper-parameters of the SVM classifier, optimising the heat-treatment process for an Al-Sc alloy to achieve target hardness, and optimising the short polymer fibre production process.",
        "bibtex": "@inproceedings{NIPS2017_1f71e393,\n author = {Vellanki, Pratibha and Rana, Santu and Gupta, Sunil and Rubin, David and Sutti, Alessandra and Dorin, Thomas and Height, Murray and Sanders, Paul and Venkatesh, Svetha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Process-constrained batch Bayesian optimisation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1f71e393b3809197ed66df836fe833e5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1f71e393b3809197ed66df836fe833e5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1f71e393b3809197ed66df836fe833e5-Reviews.html",
        "metareview": "",
        "pdf_size": 957962,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15840608874202132775&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Centre for Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia; Centre for Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia; Centre for Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia; Institute for Frontier Materials, GTP Research, Deakin University, Geelong, Australia; Institute for Frontier Materials, GTP Research, Deakin University, Geelong, Australia; Institute for Frontier Materials, GTP Research, Deakin University, Geelong, Australia; Institute for Frontier Materials, GTP Research, Deakin University, Geelong, Australia; Materials Science and Engineering, Michigan Technological University, USA; Centre for Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;mtu.edu;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au;mtu.edu;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1f71e393b3809197ed66df836fe833e5-Abstract.html",
        "aff_unique_index": "0;0;0;1;1;1;1;2;0",
        "aff_unique_norm": "Centre for Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia;Institute for Frontier Materials, GTP Research, Deakin University, Geelong, Australia;Materials Science and Engineering, Michigan Technological University, USA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Protein Interface Prediction using Graph Convolutional Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9422",
        "id": "9422",
        "author_site": "Alex Fout, Jonathon Byrd, Basir Shariat, Asa Ben-Hur",
        "author": "Alex Fout; Jonathon Byrd; Basir Shariat; Asa Ben-Hur",
        "abstract": "We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.",
        "bibtex": "@inproceedings{NIPS2017_f5077839,\n author = {Fout, Alex and Byrd, Jonathon and Shariat, Basir and Ben-Hur, Asa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Protein Interface Prediction using Graph Convolutional Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Reviews.html",
        "metareview": "",
        "pdf_size": 857856,
        "gs_citation": 1122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9144930129395134845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Colorado State University; Department of Computer Science, Colorado State University; Department of Computer Science, Colorado State University; Department of Computer Science, Colorado State University",
        "aff_domain": "colostate.edu;colostate.edu;cs.colostate.edu;cs.colostate.edu",
        "email": "colostate.edu;colostate.edu;cs.colostate.edu;cs.colostate.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Colorado State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colostate.edu",
        "aff_unique_abbr": "CSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Prototypical Networks for Few-shot Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9188",
        "id": "9188",
        "author_site": "Jake Snell, Kevin Swersky, Richard Zemel",
        "author": "Jake Snell; Kevin Swersky; Richard Zemel",
        "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",
        "bibtex": "@inproceedings{NIPS2017_cb8da676,\n author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Prototypical Networks for Few-shot Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Reviews.html",
        "metareview": "",
        "pdf_size": 536399,
        "gs_citation": 11001,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8721743270682962846&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html"
    },
    {
        "title": "Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9274",
        "id": "9274",
        "author_site": "Jianshu Chen, Chong Wang, Lin Xiao, Ji He, Lihong Li, Li Deng",
        "author": "Jianshu Chen; Chong Wang; Lin Xiao; Ji He; Lihong Li; Li Deng",
        "abstract": "In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm.  We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games.",
        "bibtex": "@inproceedings{NIPS2017_c0e90532,\n author = {Chen, Jianshu and Wang, Chong and Xiao, Lin and He, Ji and Li, Lihong and Deng, Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c0e90532fb42ac6de18e25e95db73047-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c0e90532fb42ac6de18e25e95db73047-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c0e90532fb42ac6de18e25e95db73047-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c0e90532fb42ac6de18e25e95db73047-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c0e90532fb42ac6de18e25e95db73047-Reviews.html",
        "metareview": "",
        "pdf_size": 2874890,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4336036448381669653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c0e90532fb42ac6de18e25e95db73047-Abstract.html"
    },
    {
        "title": "QMDP-Net: Deep Learning for Planning under Partial Observability",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9246",
        "id": "9246",
        "author_site": "Peter Karkus, David Hsu, Wee Sun Lee",
        "author": "Peter Karkus; David Hsu; Wee Sun Lee",
        "abstract": "This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and \u201ctransfer\u201d to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.",
        "bibtex": "@inproceedings{NIPS2017_e9412ee5,\n author = {Karkus, Peter and Hsu, David and Lee, Wee Sun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {QMDP-Net: Deep Learning for Planning under Partial Observability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e9412ee564384b987d086df32d4ce6b7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e9412ee564384b987d086df32d4ce6b7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e9412ee564384b987d086df32d4ce6b7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e9412ee564384b987d086df32d4ce6b7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e9412ee564384b987d086df32d4ce6b7-Reviews.html",
        "metareview": "",
        "pdf_size": 1556346,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15035143445263078910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "NUS Graduate School for Integrative Sciences and Engineering + School of Computing; NUS Graduate School for Integrative Sciences and Engineering + School of Computing; School of Computing",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e9412ee564384b987d086df32d4ce6b7-Abstract.html",
        "aff_unique_index": "0+1;0+1;1",
        "aff_unique_norm": "National University of Singapore;School of Computing",
        "aff_unique_dep": "Graduate School for Integrative Sciences and Engineering;Computing",
        "aff_unique_url": "https://www.nus.edu.sg;",
        "aff_unique_abbr": "NUS;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore;"
    },
    {
        "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8961",
        "id": "8961",
        "author_site": "Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic",
        "author": "Dan Alistarh; Demjan Grubic; Jerry Li; Ryota Tomioka; Milan Vojnovic",
        "abstract": "Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to its excellent scalability properties. A fundamental barrier when parallelizing SGD is the high bandwidth cost of communicating gradient updates between nodes; consequently, several lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always guarantee convergence, and it is not clear whether they can be improved.  In this paper, we propose Quantized SGD (QSGD), a family of compression schemes for gradient updates which provides convergence guarantees. QSGD allows the user to smoothly trade off \\emph{communication bandwidth} and \\emph{convergence time}: nodes can adjust the number of bits sent per iteration, at the cost of possibly higher variance. We show that this trade-off is inherent, in the sense that improving it past some threshold would violate  information-theoretic lower bounds. QSGD guarantees convergence for convex and non-convex objectives,  under asynchrony, and can be extended to stochastic variance-reduced techniques.   When applied to  training deep neural networks for image classification and  automated speech recognition, QSGD leads to significant reductions in  end-to-end training time. For example, on 16GPUs, we can train the ResNet152  network to full accuracy on ImageNet 1.8x faster than the full-precision  variant.",
        "bibtex": "@inproceedings{NIPS2017_6c340f25,\n author = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Reviews.html",
        "metareview": "",
        "pdf_size": 582129,
        "gs_citation": 2092,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14481730973283095659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "IST Austria + ETH Zurich; ETH Zurich + Google; MIT; Microsoft Research; London School of Economics",
        "aff_domain": "ist.ac.at;gmail.com;mit.edu;microsoft.com;lse.ac.uk",
        "email": "ist.ac.at;gmail.com;mit.edu;microsoft.com;lse.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6c340f25839e6acdc73414517203f5f0-Abstract.html",
        "aff_unique_index": "0+1;1+2;3;4;5",
        "aff_unique_norm": "Institute of Science and Technology Austria;ETH Zurich;Google;Massachusetts Institute of Technology;Microsoft Corporation;London School of Economics",
        "aff_unique_dep": ";;;;Microsoft Research;",
        "aff_unique_url": "https://www.ist.ac.at;https://www.ethz.ch;https://www.google.com;https://web.mit.edu;https://www.microsoft.com/en-us/research;https://www.lse.ac.uk",
        "aff_unique_abbr": "IST Austria;ETHZ;Google;MIT;MSR;LSE",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+1;1+2;2;2;3",
        "aff_country_unique": "Austria;Switzerland;United States;United Kingdom"
    },
    {
        "title": "Quantifying how much sensory information in a neural code is relevant for behavior",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9151",
        "id": "9151",
        "author_site": "Giuseppe Pica, Eugenio Piasini, Houman Safaai, Caroline Runyan, Christopher Harvey, Mathew Diamond, Christoph Kayser, Tommaso Fellin, Stefano Panzeri",
        "author": "Giuseppe Pica; Eugenio Piasini; Houman Safaai; Caroline Runyan; Christopher Harvey; Mathew Diamond; Christoph Kayser; Tommaso Fellin; Stefano Panzeri",
        "abstract": "Determining how much of the sensory information carried by a neural code contributes to behavioral performance is key to understand sensory function and neural information flow. However, there are as yet no analytical tools to compute this information that lies at the intersection between sensory coding and behavioral readout. Here we develop a novel measure, termed the information-theoretic intersection information $\\III(S;R;C)$, that quantifies how much of the sensory information carried by a neural response $R$ is  used for behavior during perceptual discrimination tasks. Building on the Partial Information Decomposition framework, we define $\\III(S;R;C)$ as the part of the mutual information between the stimulus $S$ and the response $R$ that also informs the consequent behavioral choice $C$. We compute $\\III(S;R;C)$ in the analysis of two experimental cortical datasets, to show how this measure can be used to compare quantitatively the contributions of spike timing and spike rates to task performance, and to identify brain areas or neural populations that specifically transform sensory information into choice.",
        "bibtex": "@inproceedings{NIPS2017_a9813e95,\n author = {Pica, Giuseppe and Piasini, Eugenio and Safaai, Houman and Runyan, Caroline and Harvey, Christopher and Diamond, Mathew and Kayser, Christoph and Fellin, Tommaso and Panzeri, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Quantifying how much sensory information in a neural code is relevant for behavior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Reviews.html",
        "metareview": "",
        "pdf_size": 2642961,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13721489779270574322&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Neural Computation Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy+Neural Coding Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy; Neural Computation Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy; Neural Computation Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy+Department of Neurobiology, Harvard Medical School, Boston, MA 02115, USA; Department of Neurobiology, Harvard Medical School, Boston, MA 02115, USA+Department of Neuroscience, University of Pittsburgh, Center for the Neural Basis of Cognition, Pittsburgh, USA; Tactile Perception and Learning Laboratory, International School for Advanced Studies (SISSA), Trieste, Italy; Neural Coding Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy+Optical Approaches to Brain Function Laboratory, Istituto Italiano di Tecnologia, Genova 16163, Italy; Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, UK+Department of Cognitive Neuroscience, Faculty of Biology, Bielefeld University, Universit\u00e4tsstr. 25, 33615 Bielefeld, Germany; Department of Neurobiology, Harvard Medical School, Boston, MA 02115, USA; Neural Computation Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy+Neural Coding Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy",
        "aff_domain": "iit.it;iit.it;hms.harvard.edu;pitt.edu;sissa.it;iit.it;uni-bielefeld.de;hms.harvard.edu;iit.it",
        "email": "iit.it;iit.it;hms.harvard.edu;pitt.edu;sissa.it;iit.it;uni-bielefeld.de;hms.harvard.edu;iit.it",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a9813e9550fee3110373c21fa012eee7-Abstract.html",
        "aff_unique_index": "0+1;0;0+2;2+3;4;1+5;6+7;2;0+1",
        "aff_unique_norm": "Neural Computation Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy;Neural Coding Laboratory, Center for Neuroscience and Cognitive Systems@UniTn, Istituto Italiano di Tecnologia, Rovereto (TN) 38068, Italy;Department of Neurobiology, Harvard Medical School, Boston, MA 02115, USA;Department of Neuroscience, University of Pittsburgh, Center for the Neural Basis of Cognition, Pittsburgh, USA;Tactile Perception and Learning Laboratory, International School for Advanced Studies (SISSA), Trieste, Italy;Optical Approaches to Brain Function Laboratory, Istituto Italiano di Tecnologia, Genova 16163, Italy;Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, UK;Department of Cognitive Neuroscience, Faculty of Biology, Bielefeld University, Universit\u00e4tsstr. 25, 33615 Bielefeld, Germany",
        "aff_unique_dep": ";;;;;;;",
        "aff_unique_url": ";;;;;;;",
        "aff_unique_abbr": ";;;;;;;",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;;;;",
        "aff_country_unique": ""
    },
    {
        "title": "Query Complexity of Clustering with Side Information",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9245",
        "id": "9245",
        "author_site": "Arya Mazumdar, Barna Saha",
        "author": "Arya Mazumdar; Barna Saha",
        "abstract": "Suppose, we are given a set of $n$ elements to be clustered into $k$ (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, ``do two elements $u$ and $v$ belong to the same cluster?''. The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some  function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution $f_+$ when the underlying pair of elements belong to the same cluster, and from some $f_-$ otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from $\\Theta(nk)$ (no similarity matrix) to $O(\\frac{k^2\\log{n}}{\\cH^2(f_+\\|f_-)})$ where $\\cH^2$ denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an $O(\\log{n})$ factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of $k, f_+$ and $f_-$, and only depend logarithmically with $n$.",
        "bibtex": "@inproceedings{NIPS2017_03e7ef47,\n author = {Mazumdar, Arya and Saha, Barna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Query Complexity of Clustering with Side Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/03e7ef47cee6fa4ae7567394b99912b7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/03e7ef47cee6fa4ae7567394b99912b7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/03e7ef47cee6fa4ae7567394b99912b7-Reviews.html",
        "metareview": "",
        "pdf_size": 593928,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5016677318784907150&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/03e7ef47cee6fa4ae7567394b99912b7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "College of Information and Computer Sciences",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Question Asking as Program Generation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8898",
        "id": "8898",
        "author_site": "Anselm Rothe, Brenden Lake, Todd Gureckis",
        "author": "Anselm Rothe; Brenden M Lake; Todd Gureckis",
        "abstract": "A hallmark of human intelligence is the ability to ask rich, creative, and revealing questions. Here we introduce a cognitive model capable of constructing human-like questions. Our approach treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. We evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game. We find that our model predicts what questions people will ask, and can creatively produce novel questions that were not present in the training set. In addition, we compare a number of model variants, finding that both question informativeness and complexity are important for producing human-like questions.",
        "bibtex": "@inproceedings{NIPS2017_24681928,\n author = {Rothe, Anselm and Lake, Brenden M and Gureckis, Todd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Question Asking as Program Generation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/24681928425f5a9133504de568f5f6df-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/24681928425f5a9133504de568f5f6df-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/24681928425f5a9133504de568f5f6df-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/24681928425f5a9133504de568f5f6df-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/24681928425f5a9133504de568f5f6df-Reviews.html",
        "metareview": "",
        "pdf_size": 2028987,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11072487698497718182&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Psychology; Department of Psychology+Center for Data Science; Department of Psychology",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/24681928425f5a9133504de568f5f6df-Abstract.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University Affiliation Not Specified;Center for Data Science",
        "aff_unique_dep": "Department of Psychology;Data Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9049",
        "id": "9049",
        "author_site": "George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, Jascha Sohl-Dickstein",
        "author": "George Tucker; Andriy Mnih; Chris J Maddison; John Lawson; Jascha Sohl-Dickstein",
        "abstract": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work \\citep{jang2016categorical, maddison2016concrete} has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
        "bibtex": "@inproceedings{NIPS2017_ebd6d2f5,\n author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Reviews.html",
        "metareview": "",
        "pdf_size": 1092373,
        "gs_citation": 359,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12991533425178900008&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Google Brain; DeepMind; University of Oxford + DeepMind; Google Brain; Google Brain",
        "aff_domain": "google.com;google.com;stats.ox.ac.uk;google.com;google.com",
        "email": "google.com;google.com;stats.ox.ac.uk;google.com;google.com",
        "github": "github.com/tensorflow/models/tree/master/research/rebar",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Abstract.html",
        "aff_unique_index": "0;1;2+1;0;0",
        "aff_unique_norm": "Google;DeepMind;University of Oxford",
        "aff_unique_dep": "Google Brain;;",
        "aff_unique_url": "https://brain.google.com;https://deepmind.com;https://www.ox.ac.uk",
        "aff_unique_abbr": "Google Brain;DeepMind;Oxford",
        "aff_campus_unique_index": "0;;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;1;1+1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Random Permutation Online Isotonic Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9198",
        "id": "9198",
        "author_site": "Wojciech Kotlowski, Wouter Koolen, Alan Malek",
        "author": "Wojciech Kotlowski; Wouter M. Koolen; Alan Malek",
        "abstract": "We revisit isotonic regression on linear orders, the problem of fitting monotonic functions to best explain the data, in an online setting. It was previously shown that online isotonic regression is unlearnable in a fully adversarial model, which lead to its study in the fixed design model. Here, we instead develop the more practical random permutation model. We show that the regret is bounded above by the excess leave-one-out loss for which we develop efficient algorithms and matching lower bounds. We also analyze the class of simple and popular forward algorithms and recommend where to look for  algorithms for online isotonic regression on partial orders.",
        "bibtex": "@inproceedings{NIPS2017_cd3afef9,\n author = {Kotlowski, Wojciech and Koolen, Wouter M and Malek, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Permutation Online Isotonic Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/cd3afef9b8b89558cd56638c3631868a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cd3afef9b8b89558cd56638c3631868a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cd3afef9b8b89558cd56638c3631868a-Reviews.html",
        "metareview": "",
        "pdf_size": 318704,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16054953436637852016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Pozna \u00b4n University of Technology, Poland; Centrum Wiskunde & Informatica, Amsterdam, The Netherlands; MIT, Cambridge, MA",
        "aff_domain": "cs.put.poznan.pl;cwi.nl;mit.edu",
        "email": "cs.put.poznan.pl;cwi.nl;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cd3afef9b8b89558cd56638c3631868a-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Pozna\u0144 University of Technology;Centrum Wiskunde & Informatica;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.put.poznan.pl/;https://www.cwi.nl;https://web.mit.edu",
        "aff_unique_abbr": "PUT;CWI;MIT",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Amsterdam;Cambridge",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Poland;Netherlands;United States"
    },
    {
        "title": "Random Projection Filter Bank for Time Series Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9425",
        "id": "9425",
        "author_site": "Amir-massoud Farahmand, Sepideh Pourazarm, Daniel Nikovski",
        "author": "Amir-massoud Farahmand; Sepideh Pourazarm; Daniel Nikovski",
        "abstract": "We propose Random Projection Filter Bank (RPFB) as a generic and simple approach to extract features from time series data. RPFB is a set of randomly generated stable autoregressive filters that are convolved with the input time series to generate the features. These features can be used by any conventional machine learning algorithm for solving tasks such as time series prediction, classification with time series data, etc. Different filters in RPFB extract different aspects of the time series, and together they provide a reasonably good summary of the time series. RPFB is easy to implement, fast to compute, and parallelizable. We provide an error upper bound indicating that RPFB provides a reasonable approximation to a class of dynamical systems. The empirical results in a series of synthetic and real-world problems show that RPFB is an effective method to extract features from time series.",
        "bibtex": "@inproceedings{NIPS2017_ca3ec598,\n author = {Farahmand, Amir-massoud and Pourazarm, Sepideh and Nikovski, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Projection Filter Bank for Time Series Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Reviews.html",
        "metareview": "",
        "pdf_size": 783164,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10038138207513958232&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Mitsubishi Electric Research Laboratories (MERL); Mitsubishi Electric Research Laboratories (MERL); Mitsubishi Electric Research Laboratories (MERL)",
        "aff_domain": "merl.com;bu.edu;merl.com",
        "email": "merl.com;bu.edu;merl.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.merl.com",
        "aff_unique_abbr": "MERL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Ranking Data with Continuous Labels through Oriented Recursive Partitions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9237",
        "id": "9237",
        "author_site": "St\u00e9phan Cl\u00e9men\u00e7on, Mastane Achab",
        "author": "St\u00e9phan Cl\u00e9men\u00e7on; Mastane Achab",
        "abstract": "We formulate a supervised learning problem, referred to as continuous ranking, where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space X and the goal is to order all possible observations x in X by means of a scoring function s : X \u2192 R so that s(X) and Y tend to increase or decrease together with highest probability. This problem generalizes bi/multi-partite ranking to a certain extent and the task of finding optimal scoring functions s(x) can be naturally cast as optimization of a dedicated functional cri- terion, called the IROC curve here, or as maximization of the Kendall \u03c4 related to the pair (s(X), Y ). From the theoretical side, we describe the optimal elements of this problem and provide statistical guarantees for empirical Kendall \u03c4 maximiza- tion under appropriate conditions for the class of scoring function candidates. We also propose a recursive statistical learning algorithm tailored to empirical IROC curve optimization and producing a piecewise constant scoring function that is fully described by an oriented binary tree. Preliminary numerical experiments highlight the difference in nature between regression and continuous ranking and provide strong empirical evidence of the performance of empirical optimizers of the criteria proposed.",
        "bibtex": "@inproceedings{NIPS2017_97416ac0,\n author = {Cl\\'{e}men\\c{c}on, St\\'{e}phan and Achab, Mastane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ranking Data with Continuous Labels through Oriented Recursive Partitions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/97416ac0f58056947e2eb5d5d253d4f2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/97416ac0f58056947e2eb5d5d253d4f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/97416ac0f58056947e2eb5d5d253d4f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/97416ac0f58056947e2eb5d5d253d4f2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/97416ac0f58056947e2eb5d5d253d4f2-Reviews.html",
        "metareview": "",
        "pdf_size": 378105,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13885265557222196362&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "LTCI, T\u00e9l\u00e9com ParisTech, Universit\u00e9 Paris-Saclay; LTCI, T\u00e9l\u00e9com ParisTech, Universit\u00e9 Paris-Saclay",
        "aff_domain": "telecom-paristech.fr;telecom-paristech.fr",
        "email": "telecom-paristech.fr;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/97416ac0f58056947e2eb5d5d253d4f2-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "T\u00e9l\u00e9com ParisTech",
        "aff_unique_dep": "LTCI",
        "aff_unique_url": "https://www.telecom-paris.fr",
        "aff_unique_abbr": "T\u00e9l\u00e9com ParisTech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Real Time Image Saliency for Black Box Classifiers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9463",
        "id": "9463",
        "author_site": "Piotr Dabkowski, Yarin Gal",
        "author": "Piotr Dabkowski; Yarin Gal",
        "abstract": "In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.",
        "bibtex": "@inproceedings{NIPS2017_0060ef47,\n author = {Dabkowski, Piotr and Gal, Yarin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Real Time Image Saliency for Black Box Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0060ef47b12160b9198302ebdb144dcf-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0060ef47b12160b9198302ebdb144dcf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0060ef47b12160b9198302ebdb144dcf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0060ef47b12160b9198302ebdb144dcf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0060ef47b12160b9198302ebdb144dcf-Reviews.html",
        "metareview": "",
        "pdf_size": 1751591,
        "gs_citation": 809,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5464580632985166934&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Cambridge; University of Cambridge+Alan Turing Institute, London",
        "aff_domain": "cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of Cambridge;Alan Turing Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "Cambridge;ATI",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Cambridge;London",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Real-Time Bidding with Side Information",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9292",
        "id": "9292",
        "author_site": "arthur flajolet, Patrick Jaillet",
        "author": "arthur flajolet; Patrick Jaillet",
        "abstract": "We consider the problem of repeated bidding in online advertising auctions when some side information (e.g. browser cookies) is available ahead of submitting a bid in the form of a $d$-dimensional vector. The goal for the advertiser is to maximize the total utility (e.g. the total number of clicks) derived from displaying ads given that a limited budget $B$ is allocated for a given time horizon $T$. Optimizing the bids is modeled as a contextual Multi-Armed Bandit (MAB) problem with a knapsack constraint and a continuum of arms. We develop UCB-type algorithms that combine two streams of literature: the confidence-set approach to linear contextual MABs and the probabilistic bisection search method for stochastic root-finding. Under mild assumptions on the underlying unknown distribution, we establish distribution-independent regret bounds of order $\\tilde{O}(d \\cdot \\sqrt{T})$ when either $B = \\infty$ or when $B$ scales linearly with $T$.",
        "bibtex": "@inproceedings{NIPS2017_0bed45bd,\n author = {flajolet, arthur and Jaillet, Patrick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Real-Time Bidding with Side Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0bed45bd5774ffddc95ffe500024f628-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0bed45bd5774ffddc95ffe500024f628-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0bed45bd5774ffddc95ffe500024f628-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0bed45bd5774ffddc95ffe500024f628-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0bed45bd5774ffddc95ffe500024f628-Reviews.html",
        "metareview": "",
        "pdf_size": 286459,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7343811482322231914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "MIT, ORC; MIT, EECS, LIDS, ORC",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0bed45bd5774ffddc95ffe500024f628-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "MIT, ORC;MIT, EECS, LIDS, ORC",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Reconstruct & Crush Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9232",
        "id": "9232",
        "author_site": "Erinc Merdivan, Mohammad Reza Loghmani, Matthieu Geist",
        "author": "Erinc Merdivan; Mohammad Reza Loghmani; Matthieu Geist",
        "abstract": "This article introduces an energy-based model that is adversarial regarding data: it minimizes the energy for a given data distribution (the positive samples) while maximizing the energy for another given data distribution (the negative or unlabeled samples). The model is especially instantiated with autoencoders where the energy, represented by the reconstruction error, provides a general distance measure for unknown data. The resulting neural network thus learns to reconstruct data from the first distribution while crushing data from the second distribution. This solution can handle different problems such as Positive and Unlabeled (PU) learning or covariate shift, especially with imbalanced data. Using autoencoders allows handling a large variety of data, such as images, text or even dialogues. Our experiments show the flexibility of the proposed approach in dealing with different types of data in different settings: images with CIFAR-10 and CIFAR-100 (not-in-training setting), text with Amazon reviews (PU learning) and dialogues with Facebook bAbI (next response classification and dialogue completion).",
        "bibtex": "@inproceedings{NIPS2017_269d837a,\n author = {Merdivan, Erinc and Loghmani, Mohammad Reza and Geist, Matthieu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reconstruct \\&amp; Crush Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/269d837afada308dd4aeab28ca2d57e4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/269d837afada308dd4aeab28ca2d57e4-Reviews.html",
        "metareview": "",
        "pdf_size": 1795526,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13013511485512679533&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "AIT Austrian Institute of Technology GmbH, Vienna, Austria+LORIA (Univ. Lorraine & CNRS), CentraleSup\u00e9lec, Univ. Paris-Saclay, 57070 Metz, France; Vision4Robotics lab, ACIN, TU Wien, Vienna, Austria; Universit\u00e9 de Lorraine & CNRS, LIEC, UMR 7360, Metz, F-57070 France",
        "aff_domain": "ait.ac.at;acin.tuwien.ac.at;univ-lorraine.fr",
        "email": "ait.ac.at;acin.tuwien.ac.at;univ-lorraine.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/269d837afada308dd4aeab28ca2d57e4-Abstract.html",
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "Austrian Institute of Technology;LORIA (Univ. Lorraine & CNRS), CentraleSup\u00e9lec, Univ. Paris-Saclay, 57070 Metz, France;Vision4Robotics lab, ACIN, TU Wien, Vienna, Austria;Universit\u00e9 de Lorraine & CNRS, LIEC, UMR 7360, Metz, F-57070 France",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ait.ac.at;;;",
        "aff_unique_abbr": "AIT;;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Vienna;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria;"
    },
    {
        "title": "Reconstructing perceived faces from brain activations with deep adversarial neural decoding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9204",
        "id": "9204",
        "author_site": "Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk, Umut G\u00fc\u00e7l\u00fc, Katja Seeliger, Sander Bosch, Rob van Lier, Marcel A. J. van Gerven",
        "author": "Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk; Umut G\u00fc\u00e7l\u00fc; Katja Seeliger; Sander Bosch; Rob van Lier; Marcel A. J. van Gerven",
        "abstract": "Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.",
        "bibtex": "@inproceedings{NIPS2017_efdf562c,\n author = {G\\\"{u}\\c{c}l\\\"{u}t\\\"{u}rk, Ya\\u{g}mur and G\\\"{u}\\c{c}l\\\"{u}, Umut and Seeliger, Katja and Bosch, Sander and van Lier, Rob and van Gerven, Marcel A. J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reconstructing perceived faces from brain activations with deep adversarial neural decoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Reviews.html",
        "metareview": "",
        "pdf_size": 7146810,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5280414994986256484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Radboud University, Donders Institute for Brain, Cognition and Behaviour; Radboud University, Donders Institute for Brain, Cognition and Behaviour; Radboud University, Donders Institute for Brain, Cognition and Behaviour; Radboud University, Donders Institute for Brain, Cognition and Behaviour; Radboud University, Donders Institute for Brain, Cognition and Behaviour; Radboud University, Donders Institute for Brain, Cognition and Behaviour",
        "aff_domain": "donders.ru.nl;donders.ru.nl; ; ; ; ",
        "email": "donders.ru.nl;donders.ru.nl; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/efdf562ce2fb0ad460fd8e9d33e57f57-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Radboud University, Donders Institute for Brain, Cognition and Behaviour",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Recurrent Ladder Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9373",
        "id": "9373",
        "author_site": "Isabeau Pr\u00e9mont-Schwarz, Alexander Ilin, Tele Hao, Antti Rasmus, Rinu Boney, Harri Valpola",
        "author": "Isabeau Pr\u00e9mont-Schwarz; Alexander Ilin; Tele Hao; Antti Rasmus; Rinu Boney; Harri Valpola",
        "abstract": "We propose a recurrent extension of the Ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.",
        "bibtex": "@inproceedings{NIPS2017_0a5c79b1,\n author = {Pr\\'{e}mont-Schwarz, Isabeau and Ilin, Alexander and Hao, Tele and Rasmus, Antti and Boney, Rinu and Valpola, Harri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recurrent Ladder Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0a5c79b1eaf15445da252ada718857e9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0a5c79b1eaf15445da252ada718857e9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0a5c79b1eaf15445da252ada718857e9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0a5c79b1eaf15445da252ada718857e9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0a5c79b1eaf15445da252ada718857e9-Reviews.html",
        "metareview": "",
        "pdf_size": 2775026,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=144007230704984123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "The Curious AI Company; The Curious AI Company; The Curious AI Company; The Curious AI Company; The Curious AI Company; The Curious AI Company",
        "aff_domain": "cai.fi;cai.fi;cai.fi;cai.fi;cai.fi;cai.fi",
        "email": "cai.fi;cai.fi;cai.fi;cai.fi;cai.fi;cai.fi",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0a5c79b1eaf15445da252ada718857e9-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "The Curious AI Company",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Recursive Sampling for the Nystrom Method",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9165",
        "id": "9165",
        "author_site": "Cameron Musco, Christopher Musco",
        "author": "Cameron Musco; Christopher Musco",
        "abstract": "We give the first algorithm for kernel Nystrom approximation that runs in linear time in the number of training points and is provably accurate for all kernel matrices, without dependence on regularity or incoherence conditions. The algorithm projects the kernel onto a set of s landmark points sampled by their ridge leverage scores, requiring just O(ns) kernel evaluations and O(ns^2) additional runtime. While leverage score sampling has long been known to give strong theoretical guarantees for Nystrom approximation, by employing a fast recursive sampling scheme, our algorithm is the first to make the approach scalable. Empirically we show that it finds more accurate kernel approximations in less time than popular techniques such as classic Nystrom approximation and the random Fourier features method.",
        "bibtex": "@inproceedings{NIPS2017_a03fa308,\n author = {Musco, Cameron and Musco, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recursive Sampling for the Nystrom Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a03fa30821986dff10fc66647c84c9c3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a03fa30821986dff10fc66647c84c9c3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a03fa30821986dff10fc66647c84c9c3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a03fa30821986dff10fc66647c84c9c3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a03fa30821986dff10fc66647c84c9c3-Reviews.html",
        "metareview": "",
        "pdf_size": 842767,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6777089599296226373&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "MIT EECS; MIT EECS",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a03fa30821986dff10fc66647c84c9c3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Electrical Engineering & Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Recycling Privileged Learning and Distribution Matching for Fairness",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8863",
        "id": "8863",
        "author_site": "Novi Quadrianto, Viktoriia Sharmanska",
        "author": "Novi Quadrianto; Viktoriia Sharmanska",
        "abstract": "Equipping machine learning models with ethical and legal constraints is a serious issue; without this, the future of machine learning is at risk. This paper takes a step forward in this direction and focuses on ensuring machine learning models deliver fair decisions. In legal scholarships, the notion of fairness itself is evolving and multi-faceted. We set an overarching goal to develop a unified machine learning framework that is able to handle any definitions of fairness, their combinations, and also new definitions that might be stipulated in the future. To achieve our goal, we recycle two well-established machine learning techniques, privileged learning and distribution matching, and harmonize them for satisfying multi-faceted fairness definitions. We consider protected characteristics such as race and gender as privileged information that is available at training but not at test time; this accelerates model training and delivers fairness through unawareness. Further, we cast demographic parity, equalized odds, and equality of opportunity as a classical two-sample problem of conditional distributions, which can be solved in a general form by using distance measures in Hilbert Space. We show several existing models are special cases of ours. Finally, we advocate returning the Pareto frontier of multi-objective minimization of error and unfairness in predictions. This will facilitate decision makers to select an operating point and to be accountable for it.",
        "bibtex": "@inproceedings{NIPS2017_250cf8b5,\n author = {Quadrianto, Novi and Sharmanska, Viktoriia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recycling Privileged Learning and Distribution Matching for Fairness},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Reviews.html",
        "metareview": "",
        "pdf_size": 830402,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9074746001373666477&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Predictive Analytics Lab (PAL), University of Sussex, Brighton, United Kingdom + National Research University Higher School of Economics, Moscow, Russia; Department of Computing, Imperial College London, London, United Kingdom",
        "aff_domain": "sussex.ac.uk;gmail.com",
        "email": "sussex.ac.uk;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "University of Sussex;National Research University Higher School of Economics;Imperial College London",
        "aff_unique_dep": "Predictive Analytics Lab (PAL);;Department of Computing",
        "aff_unique_url": "https://www.sussex.ac.uk;https://www.hse.ru;https://www.imperial.ac.uk",
        "aff_unique_abbr": ";HSE;Imperial College",
        "aff_campus_unique_index": "0+1;2",
        "aff_campus_unique": "Brighton;Moscow;London",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "United Kingdom;Russia"
    },
    {
        "title": "Reducing Reparameterization Gradient Variance",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9153",
        "id": "9153",
        "author_site": "Andrew Miller, Nick Foti, Alexander D'Amour, Ryan Adams",
        "author": "Andrew Miller; Nicholas Foti; Alexander D'Amour; Ryan P. Adams",
        "abstract": "Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the ``reparameterization trick,'' represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to generate more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on a non-conjugate hierarchical model and a Bayesian neural net where our method attained orders of magnitude (20-2{,}000$\\times$) reduction in gradient variance resulting in faster and more stable optimization.",
        "bibtex": "@inproceedings{NIPS2017_325995af,\n author = {Miller, Andrew and Foti, Nick and D\\textquotesingle Amour, Alexander and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reducing Reparameterization Gradient Variance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Reviews.html",
        "metareview": "",
        "pdf_size": 580560,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4368698317020690745&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Harvard University; University of Washington; UC Berkeley; Google Brain + Princeton University",
        "aff_domain": "seas.harvard.edu;uw.edu;berkeley.edu;princeton.edu",
        "email": "seas.harvard.edu;uw.edu;berkeley.edu;princeton.edu",
        "github": "http://andymiller.github.io/",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html",
        "aff_unique_index": "0;1;2;3+4",
        "aff_unique_norm": "Harvard University;University of Washington;University of California, Berkeley;Google;Princeton University",
        "aff_unique_dep": ";;;Google Brain;",
        "aff_unique_url": "https://www.harvard.edu;https://www.washington.edu;https://www.berkeley.edu;https://brain.google.com;https://www.princeton.edu",
        "aff_unique_abbr": "Harvard;UW;UC Berkeley;Google Brain;Princeton",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Mountain View",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Regret Analysis for Continuous Dueling Bandit",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8940",
        "id": "8940",
        "author": "Wataru Kumagai",
        "abstract": "The dueling bandit is a learning framework where the feedback information in the learning process is restricted to noisy comparison between a pair of actions. In this paper, we address a dueling bandit problem based on a cost function over a continuous space.   We propose a stochastic mirror descent algorithm  and show that  the algorithm achieves an $O(\\sqrt{T\\log T})$-regret bound under strong convexity and smoothness assumptions for the cost function. Then, we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function.  Moreover, considering a lower bound in convex optimization, it is turned out that our algorithm achieves the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor.",
        "bibtex": "@inproceedings{NIPS2017_58e4d44e,\n author = {Kumagai, Wataru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regret Analysis for Continuous Dueling Bandit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Reviews.html",
        "metareview": "",
        "pdf_size": 121089,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9383344269187202857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Center for Advanced Intelligence Project, RIKEN",
        "aff_domain": "riken.jp",
        "email": "riken.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/58e4d44e550d0f7ee0a23d6b02d9b0db-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "RIKEN",
        "aff_unique_dep": "Center for Advanced Intelligence Project",
        "aff_unique_url": "https://www.riken.jp",
        "aff_unique_abbr": "RIKEN",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Regret Minimization in MDPs with Options without Prior Knowledge",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9101",
        "id": "9101",
        "author_site": "Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, Emma Brunskill",
        "author": "Ronan Fruit; Matteo Pirotta; Alessandro Lazaric; Emma Brunskill",
        "abstract": "The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.",
        "bibtex": "@inproceedings{NIPS2017_90599c8f,\n author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regret Minimization in MDPs with Options without Prior Knowledge},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/90599c8fdd2f6e7a03ad173e2f535751-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/90599c8fdd2f6e7a03ad173e2f535751-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/90599c8fdd2f6e7a03ad173e2f535751-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/90599c8fdd2f6e7a03ad173e2f535751-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/90599c8fdd2f6e7a03ad173e2f535751-Reviews.html",
        "metareview": "",
        "pdf_size": 748960,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6521699062701623972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Sequel Team - Inria Lille; Sequel Team - Inria Lille; Sequel Team - Inria Lille; Stanford University",
        "aff_domain": "inria.fr;inria.fr;inria.fr;cs.stanford.edu",
        "email": "inria.fr;inria.fr;inria.fr;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Sequel Team - Inria Lille;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.stanford.edu",
        "aff_unique_abbr": ";Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Regularized Modal Regression with Applications in Cognitive Impairment Prediction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8936",
        "id": "8936",
        "author_site": "Xiaoqian Wang, Hong Chen, Weidong Cai, Dinggang Shen, Heng Huang",
        "author": "Xiaoqian Wang; Hong Chen; Weidong Cai; Dinggang Shen; Heng Huang",
        "abstract": "Linear regression models have been successfully used to function estimation and model selection in high-dimensional data analysis. However, most existing methods are built on least squares with the mean square error (MSE) criterion, which are sensitive to outliers and their performance may be degraded for heavy-tailed noise. In this paper, we go beyond this criterion by investigating the regularized modal regression from a statistical learning viewpoint. A new regularized modal regression model is proposed for estimation and variable selection, which is robust to outliers, heavy-tailed noise, and skewed noise. On the theoretical side, we establish the approximation estimate for learning the conditional mode function, the sparsity analysis for variable selection, and the robustness characterization. On the application side, we applied our model to successfully improve the cognitive impairment prediction using the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) cohort data.",
        "bibtex": "@inproceedings{NIPS2017_bea5955b,\n author = {Wang, Xiaoqian and Chen, Hong and Cai, Weidong and Shen, Dinggang and Huang, Heng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularized Modal Regression with Applications in Cognitive Impairment Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/bea5955b308361a1b07bc55042e25e54-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bea5955b308361a1b07bc55042e25e54-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bea5955b308361a1b07bc55042e25e54-Reviews.html",
        "metareview": "",
        "pdf_size": 614956,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=308482692349187665&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical and Computer Engineering, University of Pittsburgh, USA; Department of Electrical and Computer Engineering, University of Pittsburgh, USA; School of Information Technologies, University of Sydney, Australia; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, USA; Department of Electrical and Computer Engineering, University of Pittsburgh, USA",
        "aff_domain": "gmail.com;mail.hzau.edu.cn;sydney.edu.au;med.unc.edu;pitt.edu",
        "email": "gmail.com;mail.hzau.edu.cn;sydney.edu.au;med.unc.edu;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bea5955b308361a1b07bc55042e25e54-Abstract.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Pittsburgh;University of Sydney;University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;School of Information Technologies;Department of Radiology and BRIC",
        "aff_unique_url": "https://www.pitt.edu;https://www.sydney.edu.au;https://www.unc.edu",
        "aff_unique_abbr": "Pitt;USYD;UNC Chapel Hill",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Sydney;Chapel Hill",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Australia"
    },
    {
        "title": "Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9287",
        "id": "9287",
        "author_site": "Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han",
        "author": "Hyeonwoo Noh; Tackgeun You; Jonghwan Mun; Bohyung Han",
        "abstract": "Overfitting is one of the most critical challenges in deep neural networks, and there are various types of regularization methods to improve generalization performance. Injecting noises to hidden units during training, e.g., dropout, is known as a successful regularizer, but it is still not clear enough why such training techniques work well in practice and how we can maximize their benefit in the presence of two conflicting objectives---optimizing to true data distribution and preventing overfitting by regularization. This paper addresses the above issues by 1) interpreting that the conventional training methods with regularization by noise injection optimize the lower bound of the true objective and 2) proposing a technique to achieve a tighter lower bound using multiple noise samples per training example  in a stochastic gradient descent iteration. We demonstrate the effectiveness of our idea in several computer vision applications.",
        "bibtex": "@inproceedings{NIPS2017_217e342f,\n author = {Noh, Hyeonwoo and You, Tackgeun and Mun, Jonghwan and Han, Bohyung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/217e342fc01668b10cb1188d40d3370e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/217e342fc01668b10cb1188d40d3370e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/217e342fc01668b10cb1188d40d3370e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/217e342fc01668b10cb1188d40d3370e-Reviews.html",
        "metareview": "",
        "pdf_size": 381423,
        "gs_citation": 261,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11037005186625001703&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea",
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/217e342fc01668b10cb1188d40d3370e-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Reinforcement Learning under Model Mismatch",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9089",
        "id": "9089",
        "author_site": "Aurko Roy, Huan Xu, Sebastian Pokutta",
        "author": "Aurko Roy; Huan Xu; Sebastian Pokutta",
        "abstract": "We study reinforcement learning under model misspecification,   where we do not have access to the true environment but only to a   reasonably close approximation to it. We address this problem by   extending the framework of robust MDPs to the model-free   Reinforcement Learning setting, where we do not have access to the   model parameters, but can only sample states from it.  We define   robust versions of Q-learning, Sarsa, and   TD-learning and prove convergence to an approximately optimal   robust policy and approximate value function respectively.  We scale   up the robust algorithms to large MDPs via function approximation   and prove convergence under two different settings. We prove   convergence of robust approximate policy iteration and robust   approximate value iteration for linear architectures (under mild   assumptions). We also define a   robust loss function, the mean squared robust projected     Bellman error and give stochastic gradient descent algorithms   that are guaranteed to converge to a local minimum.",
        "bibtex": "@inproceedings{NIPS2017_84c6494d,\n author = {Roy, Aurko and Xu, Huan and Pokutta, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reinforcement Learning under Model Mismatch},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/84c6494d30851c63a55cdb8cb047fadd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/84c6494d30851c63a55cdb8cb047fadd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/84c6494d30851c63a55cdb8cb047fadd-Reviews.html",
        "metareview": "",
        "pdf_size": 408971,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11860135618693169477&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Google; ISyE, Georgia Institute of Technology; ISyE, Georgia Institute of Technology",
        "aff_domain": "google.com;isye.gatech.edu;isye.gatech.edu",
        "email": "google.com;isye.gatech.edu;isye.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/84c6494d30851c63a55cdb8cb047fadd-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Google;Georgia Institute of Technology",
        "aff_unique_dep": ";Industrial and Systems Engineering",
        "aff_unique_url": "https://www.google.com;https://www.gatech.edu",
        "aff_unique_abbr": "Google;Georgia Tech",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Mountain View;Atlanta",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Reliable Decision Support using Counterfactual Models",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8960",
        "id": "8960",
        "author_site": "Peter Schulam, Suchi Saria",
        "author": "Peter Schulam; Suchi Saria",
        "abstract": "Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and \u201cwhat if?\u201d reasoning for individualized treatment planning.",
        "bibtex": "@inproceedings{NIPS2017_299a23a2,\n author = {Schulam, Peter and Saria, Suchi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reliable Decision Support using Counterfactual Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/299a23a2291e2126b91d54f3601ec162-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/299a23a2291e2126b91d54f3601ec162-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/299a23a2291e2126b91d54f3601ec162-Reviews.html",
        "metareview": "",
        "pdf_size": 683970,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6163565691659888638&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Johns Hopkins University; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "cs.jhu.edu;cs.jhu.edu",
        "email": "cs.jhu.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/299a23a2291e2126b91d54f3601ec162-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Renyi Differential Privacy Mechanisms for Posterior Sampling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9304",
        "id": "9304",
        "author_site": "Joseph Geumlek, Shuang Song, Kamalika Chaudhuri",
        "author": "Joseph Geumlek; Shuang Song; Kamalika Chaudhuri",
        "abstract": "With the newly proposed privacy definition of R\u00e9nyi Differential Privacy (RDP) in (Mironov, 2017), we re-examine the inherent privacy of releasing a single sample from a posterior distribution. We exploit the impact of the prior distribution in mitigating the influence of individual data points. In particular, we focus on sampling from an exponential family and specific generalized linear models, such as logistic regression. We propose novel RDP mechanisms as well as offering a new RDP analysis for an existing method in order to add value to the RDP framework. Each method is capable of achieving arbitrary RDP privacy guarantees, and we offer experimental results of their efficacy.",
        "bibtex": "@inproceedings{NIPS2017_56584778,\n author = {Geumlek, Joseph and Song, Shuang and Chaudhuri, Kamalika},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Renyi Differential Privacy Mechanisms for Posterior Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/56584778d5a8ab88d6393cc4cd11e090-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/56584778d5a8ab88d6393cc4cd11e090-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/56584778d5a8ab88d6393cc4cd11e090-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/56584778d5a8ab88d6393cc4cd11e090-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/56584778d5a8ab88d6393cc4cd11e090-Reviews.html",
        "metareview": "",
        "pdf_size": 592336,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5498788825601960455&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;eng.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;eng.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/56584778d5a8ab88d6393cc4cd11e090-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Repeated Inverse Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8971",
        "id": "8971",
        "author_site": "Kareem Amin, Nan Jiang, Satinder Singh",
        "author": "Kareem Amin; Nan Jiang; Satinder Singh",
        "abstract": "We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.",
        "bibtex": "@inproceedings{NIPS2017_8ce6790c,\n author = {Amin, Kareem and Jiang, Nan and Singh, Satinder},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Repeated Inverse Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Reviews.html",
        "metareview": "",
        "pdf_size": 349535,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5117000304972063202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Google Research, New York, NY 10011; Computer Science & Engineering, University of Michigan, Ann Arbor, MI 48104; Computer Science & Engineering, University of Michigan, Ann Arbor, MI 48104",
        "aff_domain": "google.com;umich.edu;umich.edu",
        "email": "google.com;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8ce6790cc6a94e65f17f908f462fae85-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Google Research, New York, NY 10011;Computer Science & Engineering, University of Michigan, Ann Arbor, MI 48104",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9255",
        "id": "9255",
        "author_site": "Jeffrey Pennington, Samuel Schoenholz, Surya Ganguli",
        "author": "Jeffrey Pennington; Samuel Schoenholz; Surya Ganguli",
        "abstract": "It is well known that weight initialization in deep networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is O(1) is essential for avoiding exponentially vanishing or exploding gradients. Moreover, in deep linear networks, ensuring that all singular values of the Jacobian are concentrated near 1 can yield a dramatic additional speed-up in learning; this is a property known as dynamical isometry. However, it is unclear how to achieve dynamical isometry in nonlinear deep networks.  We address this question by employing powerful tools from free probability theory to analytically compute the {\\it entire} singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.",
        "bibtex": "@inproceedings{NIPS2017_d9fc0cdb,\n author = {Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Reviews.html",
        "metareview": "",
        "pdf_size": 4704051,
        "gs_citation": 335,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17360808380623156141&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Google Brain; Google Brain; Applied Physics, Stanford University + Google Brain",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d9fc0cdb67638d50f411432d0d41d0ba-Abstract.html",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Google;Stanford University",
        "aff_unique_dep": "Google Brain;Department of Applied Physics",
        "aff_unique_url": "https://brain.google.com;https://www.stanford.edu",
        "aff_unique_abbr": "Google Brain;Stanford",
        "aff_campus_unique_index": "0;0;1+0",
        "aff_campus_unique": "Mountain View;Stanford",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Revenue Optimization with Approximate Bid Predictions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8975",
        "id": "8975",
        "author_site": "Andres Munoz Medina, Sergei Vassilvitskii",
        "author": "Andres Munoz; Sergei Vassilvitskii",
        "abstract": "In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types, and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community.  We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.",
        "bibtex": "@inproceedings{NIPS2017_884d7996,\n author = {Munoz, Andres and Vassilvitskii, Sergei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Revenue Optimization with Approximate Bid Predictions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/884d79963bd8bc0ae9b13a1aa71add73-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/884d79963bd8bc0ae9b13a1aa71add73-Reviews.html",
        "metareview": "",
        "pdf_size": 283057,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13105147933636316356&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Google Research; Google Research",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/884d79963bd8bc0ae9b13a1aa71add73-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8981",
        "id": "8981",
        "author": "Lixin Fan",
        "abstract": "We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the \u201cnormalized\u201d bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.",
        "bibtex": "@inproceedings{NIPS2017_9a3d4583,\n author = {Fan, Lixin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9a3d458322d70046f63dfd8b0153ece4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9a3d458322d70046f63dfd8b0153ece4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9a3d458322d70046f63dfd8b0153ece4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9a3d458322d70046f63dfd8b0153ece4-Reviews.html",
        "metareview": "",
        "pdf_size": 3644765,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5136914533885467040&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Nokia Technologies, Tampere, Finland",
        "aff_domain": "nokia.com",
        "email": "nokia.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9a3d458322d70046f63dfd8b0153ece4-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Nokia Technologies, Tampere, Finland",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "title": "Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8899",
        "id": "8899",
        "author_site": "Songbai Yan, Chicheng Zhang",
        "author": "Songbai Yan; Chicheng Zhang",
        "abstract": "It has been a long-standing problem to efficiently learn a halfspace using as few labels as possible in the presence of noise. In this work, we propose an efficient Perceptron-based algorithm for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. Under the bounded noise condition~\\cite{MN06}, where each label is flipped with probability at most $\\eta < \\frac 1 2$, our algorithm achieves a near-optimal label complexity of $\\tilde{O}\\left(\\frac{d}{(1-2\\eta)^2}\\ln\\frac{1}{\\epsilon}\\right)$ in time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon(1-2\\eta)^3}\\right)$. Under the adversarial noise condition~\\cite{ABL14, KLS09, KKMS08}, where at most a $\\tilde \\Omega(\\epsilon)$ fraction of labels can be flipped, our algorithm achieves a near-optimal label complexity of $\\tilde{O}\\left(d\\ln\\frac{1}{\\epsilon}\\right)$ in time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon}\\right)$. Furthermore, we show that our active learning algorithm can be converted to an efficient passive learning algorithm that has near-optimal sample complexities with respect to $\\epsilon$ and $d$.",
        "bibtex": "@inproceedings{NIPS2017_556f3919,\n author = {Yan, Songbai and Zhang, Chicheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/556f391937dfd4398cbac35e050a2177-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/556f391937dfd4398cbac35e050a2177-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/556f391937dfd4398cbac35e050a2177-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/556f391937dfd4398cbac35e050a2177-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/556f391937dfd4398cbac35e050a2177-Reviews.html",
        "metareview": "",
        "pdf_size": 369444,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16197320038973979581&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "UC San Diego; Microsoft Research + UC San Diego",
        "aff_domain": "ucsd.edu;microsoft.com",
        "email": "ucsd.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/556f391937dfd4398cbac35e050a2177-Abstract.html",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "University of California, San Diego;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.ucsd.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UCSD;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Riemannian approach to batch normalization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9298",
        "id": "9298",
        "author_site": "Minhyung Cho, Jaehyung Lee",
        "author": "Minhyung Cho; Jaehyung Lee",
        "abstract": "Batch normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets.",
        "bibtex": "@inproceedings{NIPS2017_3a0844ce,\n author = {Cho, Minhyung and Lee, Jaehyung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Riemannian approach to batch normalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3a0844cee4fcf57de0c71e9ad3035478-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3a0844cee4fcf57de0c71e9ad3035478-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3a0844cee4fcf57de0c71e9ad3035478-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3a0844cee4fcf57de0c71e9ad3035478-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3a0844cee4fcf57de0c71e9ad3035478-Reviews.html",
        "metareview": "",
        "pdf_size": 619582,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11607270692404649857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Applied Research Korea; Gracenote Inc.",
        "aff_domain": "gmail.com;kaist.ac.kr",
        "email": "gmail.com;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Applied Research Korea;Gracenote Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9041",
        "id": "9041",
        "author_site": "Alyson Fletcher, Mojtaba Sahraee-Ardakan, Sundeep Rangan, Philip Schniter",
        "author": "Alyson K. Fletcher; Mojtaba Sahraee-Ardakan; Sundeep Rangan; Philip Schniter",
        "abstract": "The problem of estimating a random vector x from noisy linear measurements y=Ax+w with unknown parameters on the distributions of x and w, which must also be learned, arises in a wide range of statistical learning and linear inverse problems.  We show that a computationally simple iterative message-passing algorithm can provably obtain asymptotically consistent estimates in a certain high-dimensional large-system limit (LSL) under very general parameterizations.  Previous message passing techniques have required i.i.d. sub-Gaussian A matrices and often fail when the matrix is ill-conditioned. The proposed algorithm, called adaptive vector approximate message passing (Adaptive VAMP) with auto-tuning, applies to all right-rotationally random A.  Importantly, this class includes matrices with arbitrarily bad conditioning.  We show that the parameter estimates and mean squared error (MSE) of x in each iteration converge to deterministic limits that can be precisely predicted by a simple set of state evolution (SE) equations.  In addition, a simple testable condition is provided in which the MSE matches the Bayes-optimal value predicted by the replica method.  The paper thus provides a computationally simple method with provable guarantees of optimality and consistency over a large class of linear inverse problems.",
        "bibtex": "@inproceedings{NIPS2017_4e8412ad,\n author = {Fletcher, Alyson K and Sahraee-Ardakan, Mojtaba and Rangan, Sundeep and Schniter, Philip},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4e8412ad48562e3c9934f45c3e144d48-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4e8412ad48562e3c9934f45c3e144d48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4e8412ad48562e3c9934f45c3e144d48-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4e8412ad48562e3c9934f45c3e144d48-Reviews.html",
        "metareview": "",
        "pdf_size": 392079,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5234644863534619933&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. Statistics, UC Los Angeles; Dept. EE, UC Los Angeles; Dept. ECE, NYU; Dept. ECE, The Ohio State Univ.",
        "aff_domain": "ucla.edu;ucla.edu;nyu.edu;ece.osu.edu",
        "email": "ucla.edu;ucla.edu;nyu.edu;ece.osu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4e8412ad48562e3c9934f45c3e144d48-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Dept. Statistics, UC Los Angeles;Dept. EE, UC Los Angeles;New York University;Dept. ECE, The Ohio State Univ.",
        "aff_unique_dep": ";;Department of Electrical and Computer Engineering;",
        "aff_unique_url": ";;https://www.nyu.edu;",
        "aff_unique_abbr": ";;NYU;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Robust Conditional Probabilities",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9406",
        "id": "9406",
        "author_site": "Yoav Wald, Amir Globerson",
        "author": "Yoav Wald; Amir Globerson",
        "abstract": "Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label $Y$ given an input $X$ corresponds to maximizing the conditional probability of $Y$ given $X$. A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions.  Here we propose a framework for reasoning about conditional probabilities without assuming anything about the underlying distributions, except knowledge of their second order marginals, which can be estimated from data. We show how this setting leads to guaranteed bounds on conditional probabilities, which can be calculated efficiently in a variety of settings, including structured-prediction. Finally, we apply them to semi-supervised deep learning, obtaining results competitive with variational autoencoders.",
        "bibtex": "@inproceedings{NIPS2017_3eb414bf,\n author = {Wald, Yoav and Globerson, Amir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Conditional Probabilities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3eb414bf1c2a66a09c185d60553417b8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3eb414bf1c2a66a09c185d60553417b8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3eb414bf1c2a66a09c185d60553417b8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3eb414bf1c2a66a09c185d60553417b8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3eb414bf1c2a66a09c185d60553417b8-Reviews.html",
        "metareview": "",
        "pdf_size": 406026,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8460102110597702012&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Engineering, Hebrew University; The Balvatnik School of Computer Science, Tel-Aviv University",
        "aff_domain": "mail.huji.ac.il;mail.tau.ac.il",
        "email": "mail.huji.ac.il;mail.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3eb414bf1c2a66a09c185d60553417b8-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Hebrew University;The Balvatnik School of Computer Science, Tel-Aviv University",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "http://www.huji.ac.il;",
        "aff_unique_abbr": "HUJI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel;"
    },
    {
        "title": "Robust Estimation of Neural Signals in Calcium Imaging",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9075",
        "id": "9075",
        "author_site": "Hakan Inan, Murat Erdogdu, Mark Schnitzer",
        "author": "Hakan Inan; Murat A Erdogdu; Mark Schnitzer",
        "abstract": "Calcium imaging is a prominent technology in neuroscience research which allows for simultaneous recording of large numbers of neurons in awake animals. Automated extraction of neurons and their temporal activity from imaging datasets is an important step in the path to producing neuroscience results. However, nearly all imaging datasets contain gross contaminating sources which could originate from the technology used, or the underlying biological tissue. Although past work has considered the effects of contamination under limited circumstances, there has not been a general framework treating contamination and its effects on the statistical estimation of calcium signals. In this work, we proceed in a new direction and propose to extract cells and their activity using robust statistical estimation. Using the theory of M-estimation, we derive a minimax optimal robust loss,  and also find a simple and practical optimization routine for this loss with provably fast convergence. We use our proposed robust loss in a matrix factorization framework to extract the neurons and their temporal activity in calcium imaging datasets. We demonstrate the superiority of our robust estimation approach over existing methods on both simulated and real datasets.",
        "bibtex": "@inproceedings{NIPS2017_e449b931,\n author = {Inan, Hakan and Erdogdu, Murat A and Schnitzer, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Estimation of Neural Signals in Calcium Imaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e449b9317dad920c0dd5ad0a2a2d5e49-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e449b9317dad920c0dd5ad0a2a2d5e49-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e449b9317dad920c0dd5ad0a2a2d5e49-Reviews.html",
        "metareview": "",
        "pdf_size": 1251559,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13398759951676281430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Microsoft Research + Vector Institute; Stanford University + Howard Hughes Medical Institute",
        "aff_domain": "stanford.edu;cs.toronto.edu;stanford.edu",
        "email": "stanford.edu;cs.toronto.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e449b9317dad920c0dd5ad0a2a2d5e49-Abstract.html",
        "aff_unique_index": "0;1+2;0+3",
        "aff_unique_norm": "Stanford University;Microsoft Corporation;Vector Institute;Howard Hughes Medical Institute",
        "aff_unique_dep": ";Microsoft Research;;",
        "aff_unique_url": "https://www.stanford.edu;https://www.microsoft.com/en-us/research;https://vectorinstitute.ai/;https://www.hhmi.org",
        "aff_unique_abbr": "Stanford;MSR;Vector Institute;HHMI",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0+1;0+0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8874",
        "id": "8874",
        "author_site": "Jeremiah Liu, Brent Coull",
        "author": "Jeremiah Liu; Brent Coull",
        "abstract": "This work constructs a hypothesis test for detecting whether an data-generating function $h: \\real^p \\rightarrow \\real$ belongs to a specific reproducing kernel Hilbert space $\\mathcal{H}_0$, where the structure of $\\mathcal{H}_0$ is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test  under different data-generating functions and estimation strategies for the null model. Our results revealed interesting connection between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test), and also highlighted unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.",
        "bibtex": "@inproceedings{NIPS2017_2bb232c0,\n author = {Liu, Jeremiah and Coull, Brent},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Reviews.html",
        "metareview": "",
        "pdf_size": 387573,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=440149474648732835&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Biostatistics, Harvard University; Department of Biostatistics, Harvard University",
        "aff_domain": "mail.harvard.edu;hsph.harvard.edu",
        "email": "mail.harvard.edu;hsph.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2bb232c0b13c774965ef8558f0fbd615-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "Department of Biostatistics",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust Imitation of Diverse Behaviors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9307",
        "id": "9307",
        "author_site": "Ziyu Wang, Josh Merel, Scott Reed, Nando de Freitas, Gregory Wayne, Nicolas Heess",
        "author": "Ziyu Wang; Josh S Merel; Scott E Reed; Nando de Freitas; Gregory Wayne; Nicolas Heess",
        "abstract": "Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.",
        "bibtex": "@inproceedings{NIPS2017_044a23ca,\n author = {Wang, Ziyu and Merel, Josh S and Reed, Scott E and de Freitas, Nando and Wayne, Gregory and Heess, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Imitation of Diverse Behaviors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/044a23cadb567653eb51d4eb40acaa88-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/044a23cadb567653eb51d4eb40acaa88-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/044a23cadb567653eb51d4eb40acaa88-Reviews.html",
        "metareview": "",
        "pdf_size": 1240545,
        "gs_citation": 266,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2888579514670289560&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/044a23cadb567653eb51d4eb40acaa88-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Robust Optimization for Non-Convex Objectives",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9247",
        "id": "9247",
        "author_site": "Robert S Chen, Brendan Lucier, Yaron Singer, Vasilis Syrgkanis",
        "author": "Robert S. Chen; Brendan Lucier; Yaron Singer; Vasilis Syrgkanis",
        "abstract": "We consider robust optimization problems, where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to stochastic optimization: given an oracle that returns $\\alpha$-approximate solutions for distributions over objectives, we compute a distribution over solutions that is $\\alpha$-approximate in the worst case.  We show that derandomizing this solution is NP-hard in general, but can be done for a broad class of statistical learning tasks.  We apply our results to robust neural network training and submodular optimization.  We evaluate our approach experimentally on corrupted character classification and robust influence maximization in networks.",
        "bibtex": "@inproceedings{NIPS2017_10c66082,\n author = {Chen, Robert S. and Lucier, Brendan and Singer, Yaron and Syrgkanis, Vasilis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Optimization for Non-Convex Objectives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/10c66082c124f8afe3df4886f5e516e0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/10c66082c124f8afe3df4886f5e516e0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/10c66082c124f8afe3df4886f5e516e0-Reviews.html",
        "metareview": "",
        "pdf_size": 1209131,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6396865949853990429&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/10c66082c124f8afe3df4886f5e516e0-Abstract.html"
    },
    {
        "title": "Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9396",
        "id": "9396",
        "author_site": "Taylor Killian, Samuel Daulton, Finale Doshi-Velez, George Konidaris",
        "author": "Taylor W Killian; Samuel Daulton; George Konidaris; Finale Doshi-Velez",
        "abstract": "We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings.  Our new framework correctly models the joint uncertainty in the latent parameters and the state space.  We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference.  Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.",
        "bibtex": "@inproceedings{NIPS2017_2227d753,\n author = {Killian, Taylor W and Daulton, Samuel and Konidaris, George and Doshi-Velez, Finale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2227d753dc18505031869d44673728e2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2227d753dc18505031869d44673728e2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2227d753dc18505031869d44673728e2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2227d753dc18505031869d44673728e2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2227d753dc18505031869d44673728e2-Reviews.html",
        "metareview": "",
        "pdf_size": 2108633,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6997382761578063659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff": "Harvard University; Harvard University + Facebook; Brown University; Harvard University",
        "aff_domain": "g.harvard.edu;g.harvard.edu;cs.brown.edu;seas.harvard.edu",
        "email": "g.harvard.edu;g.harvard.edu;cs.brown.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2227d753dc18505031869d44673728e2-Abstract.html",
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "Harvard University;Facebook, Inc.;Brown University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.harvard.edu;https://www.facebook.com;https://www.brown.edu",
        "aff_unique_abbr": "Harvard;FB;Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Rotting Bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9092",
        "id": "9092",
        "author_site": "Nir Levine, Yacov Crammer, Shie Mannor",
        "author": "Nir Levine; Koby Crammer; Shie Mannor",
        "abstract": "The Multi-Armed Bandits (MAB) framework highlights the trade-off between acquiring new knowledge (Exploration) and leveraging available knowledge (Exploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising, content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.",
        "bibtex": "@inproceedings{NIPS2017_97d98119,\n author = {Levine, Nir and Crammer, Koby and Mannor, Shie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rotting Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Reviews.html",
        "metareview": "",
        "pdf_size": 573507,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11010825475273703392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Electrical Engineering Department, The Technion, Haifa 32000, Israel; Electrical Engineering Department, The Technion, Haifa 32000, Israel; Electrical Engineering Department, The Technion, Haifa 32000, Israel",
        "aff_domain": "gmail.com;ee.technion.ac.il;ee.technion.ac.il",
        "email": "gmail.com;ee.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/97d98119037c5b8a9663cb21fb8ebf47-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Electrical Engineering Department, The Technion, Haifa 32000, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Runtime Neural Pruning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9006",
        "id": "9006",
        "author_site": "Ji Lin, Yongming Rao, Jiwen Lu, Jie Zhou",
        "author": "Ji Lin; Yongming Rao; Jiwen Lu; Jie Zhou",
        "abstract": "In this paper, we propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. Unlike existing neural pruning methods which produce a fixed pruned model for deployment, our method preserves the full ability of the original network and conducts pruning according to the input image and current feature maps adaptively. The pruning is performed in a bottom-up, layer-by-layer manner, which we model as a Markov decision process and use reinforcement learning for training. The agent judges the importance of each convolutional kernel and conducts channel-wise pruning conditioned on different samples, where the network is pruned more when the image is easier for the task. Since the ability of network is fully preserved, the balance point is easily adjustable according to the available resources. Our method can be applied to off-the-shelf network structures and reach a better tradeoff between speed and accuracy, especially with a large pruning rate.",
        "bibtex": "@inproceedings{NIPS2017_a51fb975,\n author = {Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Runtime Neural Pruning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a51fb975227d6640e4fe47854476d133-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a51fb975227d6640e4fe47854476d133-Reviews.html",
        "metareview": "",
        "pdf_size": 378099,
        "gs_citation": 637,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16204348063310583286&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;gmail.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;gmail.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a51fb975227d6640e4fe47854476d133-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Automation",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "SGD Learns the Conjugate Kernel Class of the Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9029",
        "id": "9029",
        "author": "Amit Daniely",
        "abstract": "We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space of the network, as defined in Daniely, Frostig and Singer. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two.  As corollaries, it follows that for neural networks of any depth between 2 and log(n), SGD is guaranteed to learn, in polynomial time, constant degree polynomials with polynomially bounded coefficients. Likewise, it follows  that SGD on large enough networks can learn any continuous function (not in polynomial time), complementing classical expressivity results.",
        "bibtex": "@inproceedings{NIPS2017_489d0396,\n author = {Daniely, Amit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SGD Learns the Conjugate Kernel Class of the Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/489d0396e6826eb0c1e611d82ca8b215-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/489d0396e6826eb0c1e611d82ca8b215-Reviews.html",
        "metareview": "",
        "pdf_size": 433963,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10420302499043576567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/489d0396e6826eb0c1e611d82ca8b215-Abstract.html"
    },
    {
        "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9379",
        "id": "9379",
        "author_site": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
        "author": "Maithra Raghu; Justin Gilmer; Jason Yosinski; Jascha Sohl-Dickstein",
        "abstract": "We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods).  We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.",
        "bibtex": "@inproceedings{NIPS2017_dc6a7e65,\n author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Reviews.html",
        "metareview": "",
        "pdf_size": 703243,
        "gs_citation": 803,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8725324079138198219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Google Brain + Cornell University; Google Brain; Uber AI Labs; Google Brain",
        "aff_domain": "gmail.com;google.com;uber.com;google.com",
        "email": "gmail.com;google.com;uber.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html",
        "aff_unique_index": "0+1;0;2;0",
        "aff_unique_norm": "Google;Cornell University;Uber",
        "aff_unique_dep": "Google Brain;;Uber AI Labs",
        "aff_unique_url": "https://brain.google.com;https://www.cornell.edu;https://www.uber.com",
        "aff_unique_abbr": "Google Brain;Cornell;Uber AI Labs",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9321",
        "id": "9321",
        "author_site": "Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo, Wonyong Sung",
        "author": "Kyuhong Shim; Minjae Lee; Iksoo Choi; Yoonho Boo; Wonyong Sung",
        "abstract": "We propose a fast approximation method of a softmax function with a very large vocabulary using singular value decomposition (SVD). SVD-softmax targets fast and accurate probability estimation of the topmost probable words during inference of neural network language models. The proposed method transforms the weight matrix used in the calculation of the output vector by using SVD. The approximate probability of each word can be estimated with only a small part of the weight matrix by using a few large singular values and the corresponding elements for most of the words. We applied the technique to language modeling and neural machine translation and present a guideline for good approximation. The algorithm requires only approximately 20\\% of arithmetic operations for an 800K vocabulary case and shows more than a three-fold speedup on a GPU.",
        "bibtex": "@inproceedings{NIPS2017_4e2a6330,\n author = {Shim, Kyuhong and Lee, Minjae and Choi, Iksoo and Boo, Yoonho and Sung, Wonyong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4e2a6330465c8ffcaa696a5a16639176-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4e2a6330465c8ffcaa696a5a16639176-Reviews.html",
        "metareview": "",
        "pdf_size": 430234,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11145642462990947448&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea",
        "aff_domain": "snu.ac.kr;dsp.snu.ac.kr;dsp.snu.ac.kr;dsp.snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;dsp.snu.ac.kr;dsp.snu.ac.kr;dsp.snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4e2a6330465c8ffcaa696a5a16639176-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Safe Adaptive Importance Sampling",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9217",
        "id": "9217",
        "author_site": "Sebastian Stich, Anant Raj, Martin Jaggi",
        "author": "Sebastian U Stich; Anant Raj; Martin Jaggi",
        "abstract": "Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants -- using importance values defined by the complete gradient information which changes during optimization -- enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is  (i) provably the \\emph{best sampling} with respect to the given bounds,  (ii) always better than uniform sampling and fixed importance sampling and  (iii) can efficiently be computed -- in many applications  at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.",
        "bibtex": "@inproceedings{NIPS2017_1177967c,\n author = {Stich, Sebastian U and Raj, Anant and Jaggi, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Safe Adaptive Importance Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1177967c7957072da3dc1db4ceb30e7a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1177967c7957072da3dc1db4ceb30e7a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1177967c7957072da3dc1db4ceb30e7a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1177967c7957072da3dc1db4ceb30e7a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1177967c7957072da3dc1db4ceb30e7a-Reviews.html",
        "metareview": "",
        "pdf_size": 1376707,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9508583986347410455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "EPFL; Max Planck Institute for Intelligent Systems; EPFL",
        "aff_domain": "epfl.ch;tuebingen.mpg.de;epfl.ch",
        "email": "epfl.ch;tuebingen.mpg.de;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1177967c7957072da3dc1db4ceb30e7a-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";Intelligent Systems",
        "aff_unique_url": "https://www.epfl.ch;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "EPFL;MPI-IS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "title": "Safe Model-based Reinforcement Learning with Stability Guarantees",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8885",
        "id": "8885",
        "author_site": "Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, Andreas Krause",
        "author": "Felix Berkenkamp; Matteo Turchetta; Angela Schoellig; Andreas Krause",
        "abstract": "Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.",
        "bibtex": "@inproceedings{NIPS2017_766ebcd5,\n author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Safe Model-based Reinforcement Learning with Stability Guarantees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Reviews.html",
        "metareview": "",
        "pdf_size": 993022,
        "gs_citation": 1132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7457682645751123115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Institute for Aerospace Studies, University of Toronto; Department of Computer Science, ETH Zurich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;utias.utoronto.ca;ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;utias.utoronto.ca;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/766ebcd59621e305170616ba3d3dac32-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Zurich;Institute for Aerospace Studies, University of Toronto",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ethz.ch;",
        "aff_unique_abbr": "ETHZ;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "title": "Safe and Nested Subgame Solving for Imperfect-Information Games",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8864",
        "id": "8864",
        "author_site": "Noam Brown, Tuomas Sandholm",
        "author": "Noam Brown; Tuomas Sandholm",
        "abstract": "In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it in individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.",
        "bibtex": "@inproceedings{NIPS2017_7fe1f8ab,\n author = {Brown, Noam and Sandholm, Tuomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Safe and Nested Subgame Solving for Imperfect-Information Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7fe1f8abaad094e0b5cb1b01d712f708-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7fe1f8abaad094e0b5cb1b01d712f708-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7fe1f8abaad094e0b5cb1b01d712f708-Reviews.html",
        "metareview": "",
        "pdf_size": 678663,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11410538906864613060&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, Carnegie Mellon University; Computer Science Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7fe1f8abaad094e0b5cb1b01d712f708-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9244",
        "id": "9244",
        "author_site": "Zahra Ghodsi, Tianyu Gu, Siddharth Garg",
        "author": "Zahra Ghodsi; Tianyu Gu; Siddharth Garg",
        "abstract": "Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task.\u00a0 However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on three- and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition tasks (75.22%).",
        "bibtex": "@inproceedings{NIPS2017_6048ff4e,\n author = {Ghodsi, Zahra and Gu, Tianyu and Garg, Siddharth},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6048ff4e8cb07aa60b6777b6f7384d52-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6048ff4e8cb07aa60b6777b6f7384d52-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6048ff4e8cb07aa60b6777b6f7384d52-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6048ff4e8cb07aa60b6777b6f7384d52-Reviews.html",
        "metareview": "",
        "pdf_size": 432748,
        "gs_citation": 237,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3862726711307992593&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "New York University; New York University; New York University",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6048ff4e8cb07aa60b6777b6f7384d52-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Saliency-based Sequential Image Attention with Multiset Prediction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9293",
        "id": "9293",
        "author_site": "Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang",
        "author": "Sean Welleck; Jialin Mao; Kyunghyun Cho; Zheng Zhang",
        "abstract": "Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.",
        "bibtex": "@inproceedings{NIPS2017_028ee724,\n author = {Welleck, Sean and Mao, Jialin and Cho, Kyunghyun and Zhang, Zheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Saliency-based Sequential Image Attention with Multiset Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/028ee724157b05d04e7bdcf237d12e60-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/028ee724157b05d04e7bdcf237d12e60-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/028ee724157b05d04e7bdcf237d12e60-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/028ee724157b05d04e7bdcf237d12e60-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/028ee724157b05d04e7bdcf237d12e60-Reviews.html",
        "metareview": "",
        "pdf_size": 1238411,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9537786167979177103&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "New York University; New York University; New York University; New York University",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/028ee724157b05d04e7bdcf237d12e60-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9256",
        "id": "9256",
        "author_site": "Maria-Florina Balcan, Hongyang Zhang",
        "author": "Maria-Florina F Balcan; Hongyang Zhang",
        "abstract": "We provide new results for noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$ distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown about the geometry of this class of distributions and their applications in the context of learning. The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of $s$-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and the disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passive learning of intersections of halfspaces. Our analysis of geometric properties of $s$-concave distributions might be of independent interest to optimization more broadly.",
        "bibtex": "@inproceedings{NIPS2017_3f998e71,\n author = {Balcan, Maria-Florina F and Zhang, Hongyang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3f998e713a6e02287c374fd26835d87e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3f998e713a6e02287c374fd26835d87e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3f998e713a6e02287c374fd26835d87e-Reviews.html",
        "metareview": "",
        "pdf_size": 378503,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7237238309104935525&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Department, Carnegie Mellon University, USA; Machine Learning Department, Carnegie Mellon University, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3f998e713a6e02287c374fd26835d87e-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scalable Demand-Aware Recommendation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9028",
        "id": "9028",
        "author_site": "Jinfeng Yi, Cho-Jui Hsieh, Kush Varshney, Lijun Zhang, Yao Li",
        "author": "Jinfeng Yi; Cho-Jui Hsieh; Kush R Varshney; Lijun Zhang; Yao Li",
        "abstract": "Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to rating data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world datasets.",
        "bibtex": "@inproceedings{NIPS2017_9aa42b31,\n author = {Yi, Jinfeng and Hsieh, Cho-Jui and Varshney, Kush R and Zhang, Lijun and Li, Yao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Demand-Aware Recommendation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9aa42b31882ec039965f3c4923ce901b-Reviews.html",
        "metareview": "",
        "pdf_size": 461867,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9798287461665466217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; University of California, Davis, CA, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA + Tencent AI Lab, Bellevue, WA, USA; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; University of California, Davis, CA, USA",
        "aff_domain": "gmail.com;ucdavis.edu;us.ibm.com;lamda.nju.edu.cn;ucdavis.edu",
        "email": "gmail.com;ucdavis.edu;us.ibm.com;lamda.nju.edu.cn;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html",
        "aff_unique_index": "0;1;0+2;3;1",
        "aff_unique_norm": "IBM Thomas J. Watson Research Center;University of California, Davis;Tencent;Nanjing University",
        "aff_unique_dep": ";;AI Lab;National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://www.ucdavis.edu;https://ai.tencent.com;http://www.nju.edu.cn",
        "aff_unique_abbr": "IBM Watson;UC Davis;Tencent AI Lab;Nanjing U",
        "aff_campus_unique_index": "0;1;0+2;3;1",
        "aff_campus_unique": "Yorktown Heights;Davis;Bellevue;Nanjing",
        "aff_country_unique_index": "0;0;0+0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Scalable Generalized Linear Bandits: Online Computation and Hashing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8808",
        "id": "8808",
        "author_site": "Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, Rebecca Willett",
        "author": "Kwang-Sung Jun; Aniruddha Bhargava; Robert Nowak; Rebecca Willett",
        "abstract": "Generalized Linear Bandits (GLBs), a natural extension of the stochastic linear bandits, has been popular and successful in recent years.  However, existing GLBs scale poorly with the number of rounds and the number of arms, limiting their utility in practice.  This paper proposes new, scalable solutions to the GLB problem in two respects.  First, unlike existing GLBs, whose per-time-step space and time complexity grow at least linearly with time $t$, we propose a new algorithm that performs online computations to enjoy a constant space and time complexity.  At its heart is a novel Generalized Linear extension of the Online-to-confidence-set Conversion (GLOC method) that takes \\emph{any} online learning algorithm and turns it into a GLB algorithm.  As a special case, we apply GLOC to the online Newton step algorithm, which results in a low-regret GLB algorithm with much lower time and memory complexity than prior work.  Second, for the case where the number $N$ of arms is very large, we propose new algorithms in which each next arm is selected via an inner product search.  Such methods can be implemented via hashing algorithms (i.e., ``hash-amenable'') and result in a time complexity sublinear in $N$.  While a Thompson sampling extension of GLOC is hash-amenable, its regret bound for $d$-dimensional arm sets scales with $d^{3/2}$, whereas GLOC's regret bound scales with $d$.  Towards closing this gap, we propose a new hash-amenable algorithm whose regret bound scales with $d^{5/4}$.  Finally, we propose a fast approximate hash-key computation (inner product) with a better accuracy than the state-of-the-art, which can be of independent interest.  We conclude the paper with preliminary experimental results confirming the merits of our methods.",
        "bibtex": "@inproceedings{NIPS2017_28dd2c79,\n author = {Jun, Kwang-Sung and Bhargava, Aniruddha and Nowak, Robert and Willett, Rebecca},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Generalized Linear Bandits: Online Computation and Hashing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Reviews.html",
        "metareview": "",
        "pdf_size": 835272,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12740343788711001112&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "UW-Madison; UW-Madison; UW-Madison; UW-Madison",
        "aff_domain": "discovery.wisc.edu;wisc.edu;wisc.edu;discovery.wisc.edu",
        "email": "discovery.wisc.edu;wisc.edu;wisc.edu;discovery.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/28dd2c7955ce926456240b2ff0100bde-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scalable Levy Process Priors for Spectral Kernel Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9175",
        "id": "9175",
        "author_site": "Phillip Jang, Andrew Loeb, Matthew Davidow, Andrew Wilson",
        "author": "Phillip A Jang; Andrew Loeb; Matthew Davidow; Andrew G Wilson",
        "abstract": "Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a Levy process. The resulting distribution has support for all stationary covariances---including the popular RBF, periodic, and Matern kernels---combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the Levy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for O(n) training and O(1) predictions. We perform extrapolations having reasonable uncertainty estimates on several benchmarks, show that the proposed model can recover flexible ground truth covariances and that it is robust to errors in initialization.",
        "bibtex": "@inproceedings{NIPS2017_02b1be0d,\n author = {Jang, Phillip A and Loeb, Andrew and Davidow, Matthew and Wilson, Andrew G},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Levy Process Priors for Spectral Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/02b1be0d48924c327124732726097157-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/02b1be0d48924c327124732726097157-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/02b1be0d48924c327124732726097157-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/02b1be0d48924c327124732726097157-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/02b1be0d48924c327124732726097157-Reviews.html",
        "metareview": "",
        "pdf_size": 978374,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=908917168391710013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/02b1be0d48924c327124732726097157-Abstract.html"
    },
    {
        "title": "Scalable Log Determinants for Gaussian Process Kernel Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9403",
        "id": "9403",
        "author_site": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Wilson",
        "author": "Kun Dong; David Eriksson; Hannes Nickisch; David Bindel; Andrew G Wilson",
        "abstract": "For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an n by n positive definite matrix, and its derivatives---leading to prohibitive O(n^3) computations.  We propose novel O(n) approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra.  We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.",
        "bibtex": "@inproceedings{NIPS2017_976abf49,\n author = {Dong, Kun and Eriksson, David and Nickisch, Hannes and Bindel, David and Wilson, Andrew G},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Log Determinants for Gaussian Process Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/976abf49974d4686f87192efa0513ae0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/976abf49974d4686f87192efa0513ae0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/976abf49974d4686f87192efa0513ae0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/976abf49974d4686f87192efa0513ae0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/976abf49974d4686f87192efa0513ae0-Reviews.html",
        "metareview": "",
        "pdf_size": 499275,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2512721237224520817&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Cornell University; Cornell University; Phillips Research Hamburg; Cornell University; Cornell University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/976abf49974d4686f87192efa0513ae0-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Cornell University;Phillips Research Hamburg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cornell.edu;",
        "aff_unique_abbr": "Cornell;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Scalable Model Selection for Belief Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9238",
        "id": "9238",
        "author_site": "Zhao Song, Yusuke Muraoka, Ryohei Fujimaki, Lawrence Carin",
        "author": "Zhao Song; Yusuke Muraoka; Ryohei Fujimaki; Lawrence Carin",
        "abstract": "We propose a scalable algorithm for model selection in sigmoid belief networks (SBNs), based on the factorized asymptotic Bayesian (FAB) framework. We derive the corresponding generalized factorized information criterion (gFIC) for the SBN, which is proven to be statistically consistent with the marginal log-likelihood. To capture the dependencies within hidden variables in SBNs, a recognition network is employed to model the variational distribution. The resulting algorithm, which we call FABIA, can simultaneously execute both model selection and inference by maximizing the lower bound of gFIC. On both synthetic and real data, our experiments suggest that FABIA, when compared to state-of-the-art algorithms for learning SBNs, $(i)$ produces a more concise model, thus enabling faster testing; $(ii)$ improves predictive performance; $(iii)$ accelerates convergence; and $(iv)$ prevents overfitting.",
        "bibtex": "@inproceedings{NIPS2017_a6ea8471,\n author = {Song, Zhao and Muraoka, Yusuke and Fujimaki, Ryohei and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Model Selection for Belief Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a6ea8471c120fe8cc35a2954c9b9c595-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a6ea8471c120fe8cc35a2954c9b9c595-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a6ea8471c120fe8cc35a2954c9b9c595-Reviews.html",
        "metareview": "",
        "pdf_size": 489863,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12709418153775393736&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a6ea8471c120fe8cc35a2954c9b9c595-Abstract.html"
    },
    {
        "title": "Scalable Planning with Tensorflow for Hybrid Nonlinear Domains",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9398",
        "id": "9398",
        "author_site": "Ga Wu, Buser Say, Scott Sanner",
        "author": "Ga Wu; Buser Say; Scott Sanner",
        "abstract": "Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces?  To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains.  Furthermore, we remark that Tensorflow is highly scalable, converging to a strong plan on a large-scale concurrent domain with a total of 576,000 continuous action parameters distributed over a horizon of 96 time steps and 100 parallel instances in only 4 minutes.  We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradient problems. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optimized toolkits like Tensorflow.",
        "bibtex": "@inproceedings{NIPS2017_98b17f06,\n author = {Wu, Ga and Say, Buser and Sanner, Scott},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Planning with Tensorflow for Hybrid Nonlinear Domains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/98b17f068d5d9b7668e19fb8ae470841-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/98b17f068d5d9b7668e19fb8ae470841-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/98b17f068d5d9b7668e19fb8ae470841-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/98b17f068d5d9b7668e19fb8ae470841-Reviews.html",
        "metareview": "",
        "pdf_size": 513853,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11209755908046062436&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mechanical & Industrial Engineering, University of Toronto, Canada; Department of Mechanical & Industrial Engineering, University of Toronto, Canada; Department of Mechanical & Industrial Engineering, University of Toronto, Canada",
        "aff_domain": "mie.utoronto.ca;mie.utoronto.ca;mie.utoronto.ca",
        "email": "mie.utoronto.ca;mie.utoronto.ca;mie.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/98b17f068d5d9b7668e19fb8ae470841-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Mechanical & Industrial Engineering",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Scalable Variational Inference for Dynamical Systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9257",
        "id": "9257",
        "author_site": "Nico S Gorbach, Stefan Bauer, Joachim M Buhmann",
        "author": "Nico S Gorbach; Stefan Bauer; Joachim M Buhmann",
        "abstract": "Gradient matching is a promising tool for learning parameters and state dynamics of ordinary differential equations. It is a grid free inference approach, which, for fully observable systems is at times competitive with numerical integration. However, for many real-world applications, only sparse observations are available or even unobserved variables are included in the model description. In these cases most gradient matching methods are difficult to apply or simply do not provide satisfactory results. That is why, despite the high computational cost,  numerical integration is still the gold standard in many applications. Using an existing gradient matching approach, we propose a scalable variational inference framework which can infer states and parameters simultaneously, offers computational speedups, improved accuracy and works well even under model misspecifications in a partially observable system.",
        "bibtex": "@inproceedings{NIPS2017_e71e5cd1,\n author = {Gorbach, Nico S and Bauer, Stefan and Buhmann, Joachim M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Variational Inference for Dynamical Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e71e5cd119bbc5797164fb0cd7fd94a4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e71e5cd119bbc5797164fb0cd7fd94a4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e71e5cd119bbc5797164fb0cd7fd94a4-Reviews.html",
        "metareview": "",
        "pdf_size": 540741,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1128267559528379323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Computer Science, ETH Zurich; Dept. of Computer Science, ETH Zurich; Dept. of Computer Science, ETH Zurich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e71e5cd119bbc5797164fb0cd7fd94a4-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Dept. of Computer Science, ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9303",
        "id": "9303",
        "author_site": "Yuhuai Wu, Elman Mansimov, Roger Grosse, Shun Liao, Jimmy Ba",
        "author": "Yuhuai Wu; Elman Mansimov; Roger B Grosse; Shun Liao; Jimmy Ba",
        "abstract": "In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also the method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines",
        "bibtex": "@inproceedings{NIPS2017_36144052,\n author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/361440528766bbaaaa1901845cf4152b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/361440528766bbaaaa1901845cf4152b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/361440528766bbaaaa1901845cf4152b-Reviews.html",
        "metareview": "",
        "pdf_size": 5399173,
        "gs_citation": 887,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7666242084036831847&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Toronto+Vector Institute; New York University; University of Toronto+Vector Institute; University of Toronto+Vector Institute; University of Toronto+Vector Institute",
        "aff_domain": "cs.toronto.edu;cs.nyu.edu;cs.toronto.edu;cs.toronto.edu;psi.utoronto.ca",
        "email": "cs.toronto.edu;cs.nyu.edu;cs.toronto.edu;cs.toronto.edu;psi.utoronto.ca",
        "github": "https://github.com/openai/baselines",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html",
        "aff_unique_index": "0+1;2;0+1;0+1;0+1",
        "aff_unique_norm": "University of Toronto;Vector Institute;New York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://vectorinstitute.ai/;https://www.nyu.edu",
        "aff_unique_abbr": "U of T;Vector Institute;NYU",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;0+0;0+0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Scene Physics Acquisition via Visual De-animation",
        "author": "Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, Josh Tenenbaum",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10133",
        "id": "10133"
    },
    {
        "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8893",
        "id": "8893",
        "author_site": "Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, Klaus-Robert M\u00fcller",
        "author": "Kristof Sch\u00fctt; Pieter-Jan Kindermans; Huziel Enoc Sauceda Felix; Stefan Chmiela; Alexandre Tkatchenko; Klaus-Robert M\u00fcller",
        "abstract": "Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.",
        "bibtex": "@inproceedings{NIPS2017_303ed4c6,\n author = {Sch\\\"{u}tt, Kristof and Kindermans, Pieter-Jan and Sauceda Felix, Huziel Enoc and Chmiela, Stefan and Tkatchenko, Alexandre and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SchNet: A continuous-filter convolutional neural network for modeling quantum interactions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Reviews.html",
        "metareview": "",
        "pdf_size": 9717497,
        "gs_citation": 1597,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15908722681579728959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Group, Technische Universit\u00e4t Berlin, Germany; Machine Learning Group, Technische Universit\u00e4t Berlin, Germany; Theory Department, Fritz-Haber-Institut der Max-Planck-Gesellschaft, Berlin, Germany; Machine Learning Group, Technische Universit\u00e4t Berlin, Germany; Physics and Materials Science Research Unit, University of Luxembourg, Luxembourg; Machine Learning Group, Technische Universit\u00e4t Berlin, Germany+Max-Planck-Institut f\u00fcr Informatik, Saarbr\u00fccken, Germany+Dept. of Brain and Cognitive Engineering, Korea University, Seoul, South Korea",
        "aff_domain": "tu-berlin.de; ; ; ; ;tu-berlin.de",
        "email": "tu-berlin.de; ; ; ; ;tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html",
        "aff_unique_index": "0;0;1;0;2;0+3+4",
        "aff_unique_norm": "Machine Learning Group, Technische Universit\u00e4t Berlin, Germany;Theory Department, Fritz-Haber-Institut der Max-Planck-Gesellschaft, Berlin, Germany;Physics and Materials Science Research Unit, University of Luxembourg, Luxembourg;Max-Planck-Institut f\u00fcr Informatik, Saarbr\u00fccken, Germany;Dept. of Brain and Cognitive Engineering, Korea University, Seoul, South Korea",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";;;;",
        "aff_unique_abbr": ";;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Selective Classification for Deep Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9264",
        "id": "9264",
        "author_site": "Yonatan Geifman, Ran El-Yaniv",
        "author": "Yonatan Geifman; Ran El-Yaniv",
        "abstract": "Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, with almost 60% test coverage.",
        "bibtex": "@inproceedings{NIPS2017_4a8423d5,\n author = {Geifman, Yonatan and El-Yaniv, Ran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selective Classification for Deep Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Reviews.html",
        "metareview": "",
        "pdf_size": 414761,
        "gs_citation": 731,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3396165114315951142&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, Technion \u2013 Israel Institute of Technology; Computer Science Department, Technion \u2013 Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Self-Normalizing Neural Networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8891",
        "id": "8891",
        "author_site": "G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter",
        "author": "G\u00fcnter Klambauer; Thomas Unterthiner; Andreas Mayr; Sepp Hochreiter",
        "abstract": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep.",
        "bibtex": "@inproceedings{NIPS2017_5d44ee6f,\n author = {Klambauer, G\\\"{u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Self-Normalizing Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Reviews.html",
        "metareview": "",
        "pdf_size": 521791,
        "gs_citation": 3787,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3659160383490046744&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html"
    },
    {
        "title": "Self-Supervised Intrinsic Image Decomposition",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9366",
        "id": "9366",
        "author_site": "Michael Janner, Jiajun Wu, Tejas Kulkarni, Ilker Yildirim, Josh Tenenbaum",
        "author": "Michael Janner; Jiajun Wu; Tejas D Kulkarni; Ilker Yildirim; Josh Tenenbaum",
        "abstract": "Intrinsic decomposition from a single image is a highly challenging task, due to its inherent ambiguity and the scarcity of training data. In contrast to traditional fully supervised learning approaches, in this paper we propose learning intrinsic image decomposition by explaining the input image. Our model, the Rendered Intrinsics Network (RIN), joins together an image decomposition pipeline, which predicts reflectance, shape, and lighting conditions given a single image, with a recombination function, a learned shading model used to recompose the original input based off of intrinsic image predictions. Our network can then use unsupervised reconstruction error as an additional signal to improve its intermediate representations. This allows large-scale unlabeled data to be useful during training, and also enables transferring learned knowledge to images of unseen object categories, lighting conditions, and shapes. Extensive experiments demonstrate that our method performs well on both intrinsic image decomposition and knowledge transfer.",
        "bibtex": "@inproceedings{NIPS2017_c8862fc1,\n author = {Janner, Michael and Wu, Jiajun and Kulkarni, Tejas D and Yildirim, Ilker and Tenenbaum, Josh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Self-Supervised Intrinsic Image Decomposition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c8862fc1a32725712838863fb1a260b9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c8862fc1a32725712838863fb1a260b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c8862fc1a32725712838863fb1a260b9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c8862fc1a32725712838863fb1a260b9-Reviews.html",
        "metareview": "",
        "pdf_size": 6196026,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6325491824519610311&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "MIT; MIT; DeepMind; MIT; MIT",
        "aff_domain": "mit.edu;mit.edu;gmail.com;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;gmail.com;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c8862fc1a32725712838863fb1a260b9-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;DeepMind",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://deepmind.com",
        "aff_unique_abbr": "MIT;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Self-supervised Learning of Motion Capture",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9299",
        "id": "9299",
        "author_site": "Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, Katerina Fragkiadaki",
        "author": "Hsiao-Yu Tung; Hsiao-Wei Tung; Ersin Yumer; Katerina Fragkiadaki",
        "abstract": "Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.",
        "bibtex": "@inproceedings{NIPS2017_ab452534,\n author = {Tung, Hsiao-Yu and Tung, Hsiao-Wei and Yumer, Ersin and Fragkiadaki, Katerina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Self-supervised Learning of Motion Capture},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Reviews.html",
        "metareview": "",
        "pdf_size": 5491747,
        "gs_citation": 369,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11316472994665815439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University, Machine Learning Department; University of Pittsburgh, Department of Electrical and Computer Engineering; Adobe Research; Carnegie Mellon University, Machine Learning Department",
        "aff_domain": "cs.cmu.edu;pitt.edu;adobe.com;cs.cmu.edu",
        "email": "cs.cmu.edu;pitt.edu;adobe.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ab452534c5ce28c4fbb0e102d4a4fb2e-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Pittsburgh, Department of Electrical and Computer Engineering;Adobe",
        "aff_unique_dep": "Machine Learning Department;;Adobe Research",
        "aff_unique_url": "https://www.cmu.edu;;https://research.adobe.com",
        "aff_unique_abbr": "CMU;;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8832",
        "id": "8832",
        "author_site": "Wei-Sheng Lai, Jia-Bin Huang, Ming-Hsuan Yang",
        "author": "Wei-Sheng Lai; Jia-Bin Huang; Ming-Hsuan Yang",
        "abstract": "Convolutional neural networks (CNNs) have recently been applied to the optical flow estimation problem. As training the CNNs requires sufficiently large ground truth training data, existing approaches resort to synthetic, unrealistic datasets. On the other hand, unsupervised methods are capable of leveraging real-world videos for training where the ground truth flow fields are not available. These methods, however, rely on the fundamental assumptions of brightness constancy and spatial smoothness priors which do not hold near motion boundaries. In this paper, we propose to exploit unlabeled videos for semi-supervised learning of optical flow with a Generative Adversarial Network. Our key insight is that the adversarial loss can capture the structural patterns of flow warp errors without making explicit assumptions. Extensive experiments on benchmark datasets demonstrate that the proposed semi-supervised algorithm performs favorably against purely supervised and semi-supervised learning schemes.",
        "bibtex": "@inproceedings{NIPS2017_16a5cdae,\n author = {Lai, Wei-Sheng and Huang, Jia-Bin and Yang, Ming-Hsuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/16a5cdae362b8d27a1d8f8c7b78b4330-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/16a5cdae362b8d27a1d8f8c7b78b4330-Reviews.html",
        "metareview": "",
        "pdf_size": 4524302,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3720235825031326696&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Merced; Virginia Tech; Nvidia Research",
        "aff_domain": "ucmerced.edu;vt.edu;ucmerced.edu",
        "email": "ucmerced.edu;vt.edu;ucmerced.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of California, Merced;Virginia Tech;NVIDIA Corporation",
        "aff_unique_dep": ";;NVIDIA Research",
        "aff_unique_url": "https://www.ucmerced.edu;https://www.vt.edu;https://www.nvidia.com/research",
        "aff_unique_abbr": "UC Merced;VT;NVIDIA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9328",
        "id": "9328",
        "author_site": "Abhishek Kumar, Prasanna Sattigeri, Tom Fletcher",
        "author": "Abhishek Kumar; Prasanna Sattigeri; Tom Fletcher",
        "abstract": "Semi-supervised learning methods using Generative adversarial networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder)  which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.",
        "bibtex": "@inproceedings{NIPS2017_d3d80b65,\n author = {Kumar, Abhishek and Sattigeri, Prasanna and Fletcher, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d3d80b656929a5bc0fa34381bf42fbdd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d3d80b656929a5bc0fa34381bf42fbdd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d3d80b656929a5bc0fa34381bf42fbdd-Reviews.html",
        "metareview": "",
        "pdf_size": 3062505,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14034708826973057196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "IBM Research AI; IBM Research AI; University of Utah",
        "aff_domain": "us.ibm.com;us.ibm.com;sci.utah.edu",
        "email": "us.ibm.com;us.ibm.com;sci.utah.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "IBM Research;University of Utah",
        "aff_unique_dep": "AI;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.utah.edu",
        "aff_unique_abbr": "IBM;Utah",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9418",
        "id": "9418",
        "author_site": "Arya Mazumdar, Soumyabrata Pal",
        "author": "Arya Mazumdar; Soumyabrata Pal",
        "abstract": "Source coding is the canonical problem of data compression in information theory. In a  locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semisupervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers  to the queries. The queries cannot involve more than two (or a fixed constant number $\\Delta$ of) elements. Now the labeling of all the elements (or clustering) must be performed based on the (noisy) query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find  lower bounds on the number of queries required in variety of scenarios. We are also able to show fundamental limitations of pairwise `same cluster' queries - and propose pairwise AND queries, that provably performs better in many situations.",
        "bibtex": "@inproceedings{NIPS2017_2131f8ec,\n author = {Mazumdar, Arya and Pal, Soumyabrata},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2131f8ecf18db66a758f718dc729e00e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2131f8ecf18db66a758f718dc729e00e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2131f8ecf18db66a758f718dc729e00e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2131f8ecf18db66a758f718dc729e00e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2131f8ecf18db66a758f718dc729e00e-Reviews.html",
        "metareview": "",
        "pdf_size": 617728,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17868798935594362582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "College of Information & Computer Sciences, University of Massachusetts Amherst; College of Information & Computer Sciences, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;umass.edu",
        "email": "cs.umass.edu;umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2131f8ecf18db66a758f718dc729e00e-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "College of Information & Computer Sciences, University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Shallow Updates for Deep Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9098",
        "id": "9098",
        "author_site": "Nir Levine, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, Shie Mannor",
        "author": "Nir Levine; Tom Zahavy; Daniel J Mankowitz; Aviv Tamar; Shie Mannor",
        "abstract": "Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy.  Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.",
        "bibtex": "@inproceedings{NIPS2017_393c55ae,\n author = {Levine, Nir and Zahavy, Tom and Mankowitz, Daniel J and Tamar, Aviv and Mannor, Shie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shallow Updates for Deep Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/393c55aea738548df743a186d15f3bef-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/393c55aea738548df743a186d15f3bef-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/393c55aea738548df743a186d15f3bef-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/393c55aea738548df743a186d15f3bef-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/393c55aea738548df743a186d15f3bef-Reviews.html",
        "metareview": "",
        "pdf_size": 862445,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9905649853859679030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Electrical Engineering, The Technion - Israel Institute of Technology; Dept. of Electrical Engineering, The Technion - Israel Institute of Technology; Dept. of Electrical Engineering, The Technion - Israel Institute of Technology; Dept. of Electrical Engineering and Computer Sciences, UC Berkeley; Dept. of Electrical Engineering, The Technion - Israel Institute of Technology",
        "aff_domain": "gmail.com;campus.technion.ac.il;tx.technion.ac.il;berkeley.edu;ee.technion.ac.il",
        "email": "gmail.com;campus.technion.ac.il;tx.technion.ac.il;berkeley.edu;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/393c55aea738548df743a186d15f3bef-Abstract.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Dept. of Electrical Engineering, The Technion - Israel Institute of Technology;Dept. of Electrical Engineering and Computer Sciences, UC Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Shape and Material from Sound",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8920",
        "id": "8920",
        "author_site": "Zhoutong Zhang, Qiujia Li, Zhengjia Huang, Jiajun Wu, Josh Tenenbaum, Bill Freeman",
        "author": "Zhoutong Zhang; Qiujia Li; Zhengjia Huang; Jiajun Wu; Josh Tenenbaum; Bill Freeman",
        "abstract": "Hearing an object falling onto the ground, humans can recover rich information including its rough shape, material, and falling height. In this paper, we build machines to approximate such competency. We first mimic human knowledge of the physical world by building an efficient, physics-based simulation engine. Then, we present an analysis-by-synthesis approach to infer properties of the falling object. We further accelerate the process by learning a mapping from a sound wave to object properties, and using the predicted values to initialize the inference. This mapping can be viewed as an approximation of human commonsense learned from past experience. Our model performs well on both synthetic audio clips and real recordings without requiring any annotated data. We conduct behavior studies to compare human responses with ours on estimating object shape, material, and falling height from sound. Our model achieves near-human performance.",
        "bibtex": "@inproceedings{NIPS2017_f4552671,\n author = {Zhang, Zhoutong and Li, Qiujia and Huang, Zhengjia and Wu, Jiajun and Tenenbaum, Josh and Freeman, Bill},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shape and Material from Sound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f4552671f8909587cf485ea990207f3b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f4552671f8909587cf485ea990207f3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f4552671f8909587cf485ea990207f3b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f4552671f8909587cf485ea990207f3b-Reviews.html",
        "metareview": "",
        "pdf_size": 2903776,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4231315035144759404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "MIT; University of Cambridge; ShanghaiTech University; MIT; MIT; MIT+Google Research",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f4552671f8909587cf485ea990207f3b-Abstract.html",
        "aff_unique_index": "0;1;2;0;0;0+3",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Cambridge;ShanghaiTech University;Google",
        "aff_unique_dep": ";;;Google Research",
        "aff_unique_url": "https://web.mit.edu;https://www.cam.ac.uk;https://www.shanghaitech.edu.cn;https://research.google",
        "aff_unique_abbr": "MIT;Cambridge;ShanghaiTech;Google Research",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Cambridge;Mountain View",
        "aff_country_unique_index": "0;1;2;0;0;0+0",
        "aff_country_unique": "United States;United Kingdom;China"
    },
    {
        "title": "Sharpness, Restart and Acceleration",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8905",
        "id": "8905",
        "author_site": "Vincent Roulet, Alexandre d'Aspremont",
        "author": "Vincent Roulet; Alexandre d'Aspremont",
        "abstract": "The {\\L}ojasiewicz inequality shows that H\\\"olderian error bounds on the minimum of convex optimization problems hold almost generically. Here, we clarify results of \\citet{Nemi85} who show that H\\\"olderian error bounds directly controls the performance of restart schemes. The constants quantifying error bounds are of course unobservable, but we show that optimal restart strategies are robust, and searching for the best scheme only increases the complexity by a logarithmic factor compared to the optimal bound. Overall then, restart schemes generically accelerate accelerated methods.",
        "bibtex": "@inproceedings{NIPS2017_2ca65f58,\n author = {Roulet, Vincent and d\\textquotesingle Aspremont, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sharpness, Restart and Acceleration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Reviews.html",
        "metareview": "",
        "pdf_size": 347723,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8139104650086779980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "INRIA, ENS; CNRS, ENS",
        "aff_domain": "inria.fr;ens.fr",
        "email": "inria.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "INRIA, ENS;CNRS, ENS",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9410",
        "id": "9410",
        "author_site": "Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",
        "author": "Balaji Lakshminarayanan; Alexander Pritzel; Charles Blundell",
        "abstract": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem.  Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian)  NNs.  We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.",
        "bibtex": "@inproceedings{NIPS2017_9ef2ed4b,\n author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Reviews.html",
        "metareview": "",
        "pdf_size": 1185123,
        "gs_citation": 7516,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15810892316109997085&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html"
    },
    {
        "title": "Simple strategies for recovering inner products from coarsely quantized random projections",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9234",
        "id": "9234",
        "author_site": "Ping Li, Martin Slawski",
        "author": "Ping Li; Martin Slawski",
        "abstract": "Random projections have been increasingly adopted for a diverse set of tasks in machine learning involving dimensionality reduction. One specific line of research on this topic has investigated the use of quantization subsequent to projection with the aim of additional data compression. Motivated by applications in nearest neighbor search and linear learning, we revisit the problem of recovering inner products (respectively cosine similarities) in such setting. We show that even under coarse scalar quantization with 3 to 5 bits per projection, the loss in accuracy tends to range from",
        "bibtex": "@inproceedings{NIPS2017_ea159dc9,\n author = {Li, Ping and Slawski, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simple strategies for recovering inner products from coarsely quantized random projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ea159dc9788ffac311592613b7f71fbb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ea159dc9788ffac311592613b7f71fbb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ea159dc9788ffac311592613b7f71fbb-Reviews.html",
        "metareview": "",
        "pdf_size": 1056482,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17616307035469939899&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Baidu Research + Rutgers University; Department of Statistics, George Mason University",
        "aff_domain": "gmail.com;gmu.edu",
        "email": "gmail.com;gmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ea159dc9788ffac311592613b7f71fbb-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Baidu;Rutgers University;George Mason University",
        "aff_unique_dep": "Baidu Research;;Department of Statistics",
        "aff_unique_url": "https://research.baidu.com;https://www.rutgers.edu;https://www.gmu.edu",
        "aff_unique_abbr": "Baidu;Rutgers;GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9358",
        "id": "9358",
        "author_site": "Ahmet Alacaoglu, Quoc Tran Dinh, Olivier Fercoq, Volkan Cevher",
        "author": "Ahmet Alacaoglu; Quoc Tran Dinh; Olivier Fercoq; Volkan Cevher",
        "abstract": "We propose a new randomized coordinate descent method for a convex optimization template  with broad applications. Our analysis relies on a novel combination of four ideas applied to the primal-dual gap function: smoothing, acceleration, homotopy, and coordinate descent with non-uniform sampling. As a result, our method features the first convergence rate guarantees among the coordinate descent methods, that are the best-known under a variety of common structure assumptions on the template. We provide numerical evidence to support the theoretical results with a comparison to state-of-the-art algorithms.",
        "bibtex": "@inproceedings{NIPS2017_71887f62,\n author = {Alacaoglu, Ahmet and Tran Dinh, Quoc and Fercoq, Olivier and Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/71887f62f073a78511cbac56f8cab53f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/71887f62f073a78511cbac56f8cab53f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/71887f62f073a78511cbac56f8cab53f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/71887f62f073a78511cbac56f8cab53f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/71887f62f073a78511cbac56f8cab53f-Reviews.html",
        "metareview": "",
        "pdf_size": 413820,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2700047345658099792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland; Department of Statistics and Operations Research, UNC-Chapel Hill, NC, USA; LTCI, T\u00e9l\u00e9com ParisTech, Universit\u00e9 Paris-Saclay, Paris, France; Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;email.unc.edu;telecom-paristech.fr",
        "email": "epfl.ch;epfl.ch;email.unc.edu;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/71887f62f073a78511cbac56f8cab53f-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland;Department of Statistics and Operations Research, UNC-Chapel Hill, NC, USA;LTCI, T\u00e9l\u00e9com ParisTech, Universit\u00e9 Paris-Saclay, Paris, France",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Sobolev Training for Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9207",
        "id": "9207",
        "author_site": "Wojciech Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, Razvan Pascanu",
        "author": "Wojciech M. Czarnecki; Simon Osindero; Max Jaderberg; Grzegorz Swirszcz; Razvan Pascanu",
        "abstract": "At the heart of deep learning we aim to use neural networks as function approximators -  training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input -- for example when the ground truth function is itself a neural network such as in network compression or distillation.  Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function\u2019s outputs but also the function\u2019s derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients.  In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.",
        "bibtex": "@inproceedings{NIPS2017_758a0661,\n author = {Czarnecki, Wojciech M. and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sobolev Training for Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/758a06618c69880a6cee5314ee42d52f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/758a06618c69880a6cee5314ee42d52f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/758a06618c69880a6cee5314ee42d52f-Reviews.html",
        "metareview": "",
        "pdf_size": 2544826,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4464290023318242521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/758a06618c69880a6cee5314ee42d52f-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8907",
        "id": "8907",
        "author_site": "Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, Luc V Gool",
        "author": "Eirikur Agustsson; Fabian Mentzer; Michael Tschannen; Lukas Cavigelli; Radu Timofte; Luca Benini; Luc V. Gool",
        "abstract": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy.  Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training.  We showcase this method for two challenging applications: Image compression and neural network compression.  While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.",
        "bibtex": "@inproceedings{NIPS2017_86b122d4,\n author = {Agustsson, Eirikur and Mentzer, Fabian and Tschannen, Michael and Cavigelli, Lukas and Timofte, Radu and Benini, Luca and Gool, Luc V},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/86b122d4358357d834a87ce618a55de0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/86b122d4358357d834a87ce618a55de0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/86b122d4358357d834a87ce618a55de0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/86b122d4358357d834a87ce618a55de0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/86b122d4358357d834a87ce618a55de0-Reviews.html",
        "metareview": "",
        "pdf_size": 1373616,
        "gs_citation": 605,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1693533982215094430&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich + Merantix; ETH Zurich; KU Leuven + ETH Zurich",
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;nari.ee.ethz.ch;iis.ee.ethz.ch;vision.ee.ethz.ch;iis.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;nari.ee.ethz.ch;iis.ee.ethz.ch;vision.ee.ethz.ch;iis.ee.ethz.ch;vision.ee.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/86b122d4358357d834a87ce618a55de0-Abstract.html",
        "aff_unique_index": "0;0;0;0;0+1;0;2+0",
        "aff_unique_norm": "ETH Zurich;Merantix;Katholieke Universiteit Leuven",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;;KU Leuven",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;2+0",
        "aff_country_unique": "Switzerland;;Belgium"
    },
    {
        "title": "Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9423",
        "id": "9423",
        "author_site": "Michael Eickenberg, Georgios Exarchakis, Matthew Hirn, Stephane Mallat",
        "author": "Michael Eickenberg; Georgios Exarchakis; Matthew Hirn; Stephane Mallat",
        "abstract": "We introduce a solid harmonic wavelet scattering representation, invariant  to rigid motion and stable to deformations, for regression and classification  of 2D and 3D signals. Solid harmonic wavelets are computed by multiplying solid  harmonic functions with Gaussian windows dilated at different scales. Invariant  scattering coefficients are obtained by cascading such wavelet transforms with  the complex modulus nonlinearity. We study an application of solid harmonic  scattering invariants to the estimation of quantum molecular energies, which  are also invariant to rigid motion and stable with respect to deformations. A multilinear regression  over scattering invariants provides close to state of the art results over  small and large databases of organic molecules.",
        "bibtex": "@inproceedings{NIPS2017_72b38622,\n author = {Eickenberg, Michael and Exarchakis, Georgios and Hirn, Matthew and Mallat, Stephane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/72b386224056bf940cd5b01341f65e9d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/72b386224056bf940cd5b01341f65e9d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/72b386224056bf940cd5b01341f65e9d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/72b386224056bf940cd5b01341f65e9d-Reviews.html",
        "metareview": "",
        "pdf_size": 479656,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16765095912387685423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of computer science, Ecole normale sup\u00e9rieure, PSL Research University, 75005 Paris, France; Department of computer science, Ecole normale sup\u00e9rieure, PSL Research University, 75005 Paris, France; Department of Computational Mathematics, Science and Engineering + Department of Mathematics, Michigan State University, East Lansing, MI 48824, USA; Coll\u00e8ge de France, Ecole Normale Sup\u00e9rieure, PSL Research University, 75005 Paris, France",
        "aff_domain": "nsup.org;ens.fr;msu.edu; ",
        "email": "nsup.org;ens.fr;msu.edu; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/72b386224056bf940cd5b01341f65e9d-Abstract.html",
        "aff_unique_index": "0;0;1+2;3",
        "aff_unique_norm": "Department of computer science, Ecole normale sup\u00e9rieure, PSL Research University, 75005 Paris, France;Department of Computational Mathematics, Science and Engineering;Department of Mathematics, Michigan State University, East Lansing, MI 48824, USA;Coll\u00e8ge de France, Ecole Normale Sup\u00e9rieure, PSL Research University, 75005 Paris, France",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Solving Most Systems of Random Quadratic Equations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8976",
        "id": "8976",
        "author_site": "Gang Wang, Georgios Giannakis, Yousef Saad, Jie Chen",
        "author": "Gang Wang; Georgios Giannakis; Yousef Saad; Jie Chen",
        "abstract": "This paper deals with finding an $n$-dimensional solution $\\bm{x}$ to a system of quadratic equations $y_i=|\\langle\\bm{a}_i,\\bm{x}\\rangle|^2$, $1\\le i \\le m$, which in general is known to be NP-hard. We put forth a novel procedure, that starts with a \\emph{weighted maximal correlation initialization} obtainable with a few power iterations, followed by successive refinements based on \\emph{iteratively reweighted gradient-type iterations}. The novel techniques distinguish themselves from prior works by the inclusion of a fresh (re)weighting regularization. For certain random measurement models, the proposed procedure returns the true solution $\\bm{x}$ with high probability in time proportional to reading the data $\\{(\\bm{a}_i;y_i)\\}_{1\\le i \\le m}$, provided that the number $m$ of equations is some constant $c>0$ times the number $n$ of unknowns, that is, $m\\ge cn$. Empirically, the upshots of this contribution are: i) perfect signal recovery in the high-dimensional regime given only an \\emph{information-theoretic limit number} of equations; and, ii) (near-)optimal statistical accuracy in the presence of additive noise. Extensive numerical tests using both synthetic data and real images corroborate its improved signal recovery performance and computational efficiency relative to state-of-the-art approaches.",
        "bibtex": "@inproceedings{NIPS2017_36a1694b,\n author = {Wang, Gang and Giannakis, Georgios and Saad, Yousef and Chen, Jie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Solving Most Systems of Random Quadratic Equations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/36a1694bce9815b7e38a9dad05ad42e0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/36a1694bce9815b7e38a9dad05ad42e0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/36a1694bce9815b7e38a9dad05ad42e0-Reviews.html",
        "metareview": "",
        "pdf_size": 10093995,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2181364019085529910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Key Lab of Intell. Contr. and Decision of Complex Syst., Beijing Inst. of Technology + Digital Tech. Center & Dept. of Electrical and Computer Eng., Univ. of Minnesota; Digital Tech. Center & Dept. of Electrical and Computer Eng., Univ. of Minnesota + Department of Computer Science and Engineering, Univ. of Minnesota; Department of Computer Science and Engineering, Univ. of Minnesota; Key Lab of Intell. Contr. and Decision of Complex Syst., Beijing Inst. of Technology",
        "aff_domain": "umn.edu;umn.edu;umn.edu;bit.edu.cn",
        "email": "umn.edu;umn.edu;umn.edu;bit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/36a1694bce9815b7e38a9dad05ad42e0-Abstract.html",
        "aff_unique_index": "0+1;1+2;2;0",
        "aff_unique_norm": "Key Lab of Intell. Contr. and Decision of Complex Syst., Beijing Inst. of Technology;Digital Tech. Center & Dept. of Electrical and Computer Eng., Univ. of Minnesota;Department of Computer Science and Engineering, Univ. of Minnesota",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "title": "Sparse Approximate Conic Hulls",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9040",
        "id": "9040",
        "author_site": "Greg Van Buskirk, Benjamin Raichel, Nicholas Ruozzi",
        "author": "Greg Van Buskirk; Benjamin Raichel; Nicholas Ruozzi",
        "abstract": "We consider the problem of computing a restricted nonnegative matrix factorization (NMF) of an m\\times n matrix X.  Specifically, we seek a factorization X\\approx BC, where the k columns of B are a subset of those from X and C\\in\\Re_{\\geq 0}^{k\\times n}.  Equivalently, given the matrix X, consider the problem of finding a small subset, S, of the columns of X such that the conic hull of S \\eps-approximates the conic hull of the columns of X, i.e., the distance of every column of X to the conic hull of the columns of S should be at most an \\eps-fraction of the angular diameter of X.   If k is the size of the smallest \\eps-approximation, then we produce an O(k/\\eps^{2/3}) sized O(\\eps^{1/3})-approximation, yielding the first provable, polynomial time \\eps-approximation for this class of NMF problems, where also desirably the approximation is independent of n and m. Furthermore, we prove an approximate conic Carath\u00e9odory theorem, a general sparsity result, that shows that any column of X can be \\eps-approximated with an O(1/\\eps^2) sparse combination from S.   Our results are facilitated by a reduction to the problem of approximating convex hulls,  and we prove that both the convex and conic hull variants are d-sum-hard, resolving an open problem.  Finally, we provide experimental results for the convex and conic algorithms on a variety of feature selection tasks.",
        "bibtex": "@inproceedings{NIPS2017_6d9cb7de,\n author = {Van Buskirk, Greg and Raichel, Benjamin and Ruozzi, Nicholas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Approximate Conic Hulls},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Reviews.html",
        "metareview": "",
        "pdf_size": 450921,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5871929580890991945&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Texas at Dallas; Department of Computer Science, University of Texas at Dallas; Department of Computer Science, University of Texas at Dallas",
        "aff_domain": "utdallas.edu;utdallas.edu;utdallas.edu",
        "email": "utdallas.edu;utdallas.edu;utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6d9cb7de5e8ac30bd5e8734bc96a35c1-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utdallas.edu",
        "aff_unique_abbr": "UT Dallas",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Dallas",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sparse Embedded $k$-Means Clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9116",
        "id": "9116",
        "author_site": "Weiwei Liu, Xiaobo Shen, Ivor Tsang",
        "author": "Weiwei Liu; Xiaobo Shen; Ivor Tsang",
        "abstract": "The $k$-means clustering algorithm is a ubiquitous tool in data mining and machine learning that shows promising performance. However, its high computational cost has hindered its applications in broad domains. Researchers have successfully addressed these obstacles with dimensionality reduction methods. Recently, [1] develop a state-of-the-art random projection (RP) method for faster $k$-means clustering. Their method delivers many improvements over other dimensionality reduction methods. For example, compared to the advanced singular value decomposition based feature extraction approach,  [1] reduce the running time by a factor of $\\min \\{n,d\\}\\epsilon^2 log(d)/k$ for data matrix $X \\in \\mathbb{R}^{n\\times d} $ with $n$ data points and $d$ features, while losing only a factor of one in approximation accuracy. Unfortunately, they still require $\\mathcal{O}(\\frac{ndk}{\\epsilon^2log(d)})$ for matrix multiplication and this cost will be prohibitive for large values of $n$ and $d$. To break this bottleneck, we carefully build a sparse embedded $k$-means clustering algorithm which requires $\\mathcal{O}(nnz(X))$ ($nnz(X)$ denotes the number of non-zeros in $X$) for fast matrix multiplication. Moreover, our proposed algorithm improves on [1]'s results for approximation accuracy by a factor of one. Our empirical studies corroborate our theoretical findings, and demonstrate that our approach is able to significantly accelerate $k$-means clustering, while achieving satisfactory clustering performance.",
        "bibtex": "@inproceedings{NIPS2017_3214a6d8,\n author = {Liu, Weiwei and Shen, Xiaobo and Tsang, Ivor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Embedded k-Means Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3214a6d842cc69597f9edf26df552e43-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3214a6d842cc69597f9edf26df552e43-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3214a6d842cc69597f9edf26df552e43-Reviews.html",
        "metareview": "",
        "pdf_size": 375537,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10472778429349160301&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, The University of New South Wales + Centre for Arti\ufb01cial Intelligence, University of Technology Sydney; School of Computer Science and Engineering, Nanyang Technological University; Centre for Arti\ufb01cial Intelligence, University of Technology Sydney",
        "aff_domain": "gmail.com;gmail.com;uts.edu.au",
        "email": "gmail.com;gmail.com;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3214a6d842cc69597f9edf26df552e43-Abstract.html",
        "aff_unique_index": "0+1;2;1",
        "aff_unique_norm": "The University of New South Wales;University of Technology Sydney;Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Engineering;Centre for Arti\ufb01cial Intelligence;School of Computer Science and Engineering",
        "aff_unique_url": "https://www.unsw.edu.au;https://www.uts.edu.au;https://www.ntu.edu.sg",
        "aff_unique_abbr": "UNSW;UTS;NTU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Sydney",
        "aff_country_unique_index": "0+0;1;0",
        "aff_country_unique": "Australia;Singapore"
    },
    {
        "title": "Sparse convolutional coding for neuronal assembly detection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9150",
        "id": "9150",
        "author_site": "Sven Peter, Elke Kirschbaum, Martin Both, Lee Campbell, Brandon Harvey, Conor Heins, Daniel Durstewitz, Ferran Diego, Fred Hamprecht",
        "author": "Sven Peter; Elke Kirschbaum; Martin Both; Lee Campbell; Brandon Harvey; Conor Heins; Daniel Durstewitz; Ferran Diego; Fred A. Hamprecht",
        "abstract": "Cell assemblies, originally proposed by Donald Hebb (1949), are subsets of neurons firing in a temporally coordinated way that gives rise to repeated motifs supposed to underly neural representations and information processing. Although Hebb's original proposal dates back many decades, the detection of assemblies and their role in coding is still an open and current research topic, partly because simultaneous recordings from large populations of neurons became feasible only relatively recently. Most current and easy-to-apply computational techniques focus on the identification of strictly synchronously spiking neurons. In this paper we propose a new algorithm, based on sparse convolutional coding, for detecting recurrent motifs of arbitrary structure up to a given length. Testing of our algorithm on synthetically generated datasets shows that it outperforms established methods and accurately identifies the temporal structure of embedded assemblies, even when these contain overlapping neurons or when strong background noise is present. Moreover, exploratory analysis of experimental datasets from hippocampal slices and cortical neuron cultures have provided promising results.",
        "bibtex": "@inproceedings{NIPS2017_aebf7782,\n author = {Peter, Sven and Kirschbaum, Elke and Both, Martin and Campbell, Lee and Harvey, Brandon and Heins, Conor and Durstewitz, Daniel and Diego, Ferran and Hamprecht, Fred A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse convolutional coding for neuronal assembly detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/aebf7782a3d445f43cf30ee2c0d84dee-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/aebf7782a3d445f43cf30ee2c0d84dee-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/aebf7782a3d445f43cf30ee2c0d84dee-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/aebf7782a3d445f43cf30ee2c0d84dee-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/aebf7782a3d445f43cf30ee2c0d84dee-Reviews.html",
        "metareview": "",
        "pdf_size": 820355,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1201528336295877248&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Interdisciplinary Center for Scienti\ufb01c Computing (IWR), Heidelberg, Germany; Interdisciplinary Center for Scienti\ufb01c Computing (IWR), Heidelberg, Germany; Institute of Physiology and Pathophysiology, Heidelberg, Germany; National Institute on Drug Abuse, Baltimore, USA; National Institute on Drug Abuse, Baltimore, USA; National Institute on Drug Abuse, Baltimore, USA+Max Planck Institute for Dynamics and Self-Organization, G\u00f6ttingen, Germany; Dept. Theoretical Neuroscience, Central Institute of Mental Health, Mannheim, Germany; Robert Bosch GmbH, Hildesheim, Germany+Interdisciplinary Center for Scienti\ufb01c Computing (IWR), Heidelberg, Germany; Interdisciplinary Center for Scienti\ufb01c Computing (IWR), Heidelberg, Germany",
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;physiologie.uni-heidelberg.de;nih.gov;mail.nih.gov;ds.mpg.de;zi-mannheim.de;de.bosch.com;iwr.uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;physiologie.uni-heidelberg.de;nih.gov;mail.nih.gov;ds.mpg.de;zi-mannheim.de;de.bosch.com;iwr.uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/aebf7782a3d445f43cf30ee2c0d84dee-Abstract.html",
        "aff_unique_index": "0;0;1;2;2;2+3;4;5+0;0",
        "aff_unique_norm": "Interdisciplinary Center for Scienti\ufb01c Computing (IWR), Heidelberg, Germany;Institute of Physiology and Pathophysiology, Heidelberg, Germany;National Institute on Drug Abuse, Baltimore, USA;Max Planck Institute for Dynamics and Self-Organization, G\u00f6ttingen, Germany;Dept. Theoretical Neuroscience, Central Institute of Mental Health, Mannheim, Germany;Robert Bosch GmbH",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": ";;;;;https://www.bosch.com",
        "aff_unique_abbr": ";;;;;Bosch",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";1",
        "aff_country_unique": ";Germany"
    },
    {
        "title": "Spectral Mixture Kernels for Multi-Output Gaussian Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9436",
        "id": "9436",
        "author_site": "Gabriel Parra, Felipe Tobar",
        "author": "Gabriel Parra; Felipe Tobar",
        "abstract": "Early approaches to multiple-output Gaussian processes (MOGPs) relied on linear combinations of independent, latent, single-output Gaussian processes (GPs). This resulted in cross-covariance functions with limited parametric interpretation, thus conflicting with the ability  of single-output GPs to understand lengthscales, frequencies and magnitudes to name a few. On the contrary, current approaches to MOGP are able to better interpret the relationship between different channels by directly modelling the cross-covariances as a spectral mixture kernel with a phase shift. We extend this rationale and propose a parametric family of complex-valued cross-spectral densities and then build on Cram\u00e9r's Theorem (the multivariate version of Bochner's Theorem) to provide a principled approach to design multivariate covariance functions. The so-constructed kernels are able to model delays among channels in addition to phase differences and are thus more expressive than previous methods, while also providing full parametric interpretation of the relationship across channels. The proposed method is first validated on synthetic data and then compared to existing MOGP methods on two real-world examples.",
        "bibtex": "@inproceedings{NIPS2017_333cb763,\n author = {Parra, Gabriel and Tobar, Felipe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Mixture Kernels for Multi-Output Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/333cb763facc6ce398ff83845f224d62-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/333cb763facc6ce398ff83845f224d62-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/333cb763facc6ce398ff83845f224d62-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/333cb763facc6ce398ff83845f224d62-Reviews.html",
        "metareview": "",
        "pdf_size": 886708,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3001667877125768625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Mathematical Engineering, Universidad de Chile; Center for Mathematical Modeling, Universidad de Chile",
        "aff_domain": "dim.uchile.cl;dim.uchile.cl",
        "email": "dim.uchile.cl;dim.uchile.cl",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/333cb763facc6ce398ff83845f224d62-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Mathematical Engineering, Universidad de Chile;Center for Mathematical Modeling, Universidad de Chile",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Spectrally-normalized margin bounds for neural networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9395",
        "id": "9395",
        "author_site": "Peter Bartlett, Dylan J Foster, Matus Telgarsky",
        "author": "Peter L Bartlett; Dylan J Foster; Matus J Telgarsky",
        "abstract": "This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized \"spectral complexity\": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the MNIST and CIFAR10 datasets, with both original and random labels;  the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.",
        "bibtex": "@inproceedings{NIPS2017_b22b257a,\n author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectrally-normalized margin bounds for neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Reviews.html",
        "metareview": "",
        "pdf_size": 878091,
        "gs_citation": 1489,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16170177050810544440&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of California, Berkeley + Queensland University of Technology; Cornell University; University of Illinois, Urbana-Champaign",
        "aff_domain": "berkeley.edu;cornell.edu;illinois.edu",
        "email": "berkeley.edu;cornell.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b22b257ad0519d4500539da3c8bcf4dd-Abstract.html",
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "University of California, Berkeley;Queensland University of Technology;Cornell University;University of Illinois",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.qut.edu.au;https://www.cornell.edu;https://illinois.edu",
        "aff_unique_abbr": "UC Berkeley;QUT;Cornell;UIUC",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Berkeley;;Urbana-Champaign",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "United States;Australia"
    },
    {
        "title": "Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8982",
        "id": "8982",
        "author_site": "Pan Xu, Jian Ma, Quanquan Gu",
        "author": "Pan Xu; Jian Ma; Quanquan Gu",
        "abstract": "We study the estimation of the latent variable Gaussian graphical model (LVGGM), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix.  In order to speed up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization and an efficient alternating gradient descent algorithm with hard thresholding to solve it. Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. In addition, we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision. Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory.",
        "bibtex": "@inproceedings{NIPS2017_ae5e3ce4,\n author = {Xu, Pan and Ma, Jian and Gu, Quanquan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ae5e3ce40e0404a45ecacaaf05e5f735-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ae5e3ce40e0404a45ecacaaf05e5f735-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ae5e3ce40e0404a45ecacaaf05e5f735-Reviews.html",
        "metareview": "",
        "pdf_size": 697752,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13734135881749570182&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Virginia; School of Computer Science, Carnegie Mellon University; Department of Computer Science, University of Virginia",
        "aff_domain": "virginia.edu;cs.cmu.edu;virginia.edu",
        "email": "virginia.edu;cs.cmu.edu;virginia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Virginia;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.virginia.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UVA;CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spherical convolutions and their application in molecular modelling",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9127",
        "id": "9127",
        "author_site": "Wouter Boomsma, Jes Frellsen",
        "author": "Wouter Boomsma; Jes Frellsen",
        "abstract": "Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.",
        "bibtex": "@inproceedings{NIPS2017_1113d7a7,\n author = {Boomsma, Wouter and Frellsen, Jes},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spherical convolutions and their application in molecular modelling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Reviews.html",
        "metareview": "",
        "pdf_size": 14551910,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14076731846138299601&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Copenhagen; Department of Computer Science, IT University of Copenhagen",
        "aff_domain": "di.ku.dk;itu.dk",
        "email": "di.ku.dk;itu.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Copenhagen;IT University of Copenhagen",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.ku.dk;https://itu.dk",
        "aff_unique_abbr": "UCPH;ITU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "title": "Stabilizing Training of Generative Adversarial Networks through Regularization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8990",
        "id": "8990",
        "author_site": "Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann",
        "author": "Kevin Roth; Aurelien Lucchi; Sebastian Nowozin; Thomas Hofmann",
        "abstract": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
        "bibtex": "@inproceedings{NIPS2017_7bccfde7,\n author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stabilizing Training of Generative Adversarial Networks through Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Reviews.html",
        "metareview": "",
        "pdf_size": 10689237,
        "gs_citation": 554,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11974368860899381464&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich; Microsoft Research, Cambridge, UK; Department of Computer Science, ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;microsoft.com;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;microsoft.com;inf.ethz.ch",
        "github": "https://github.com/rothk/Stabilizing_GANs",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7bccfde7714a1ebadf06c5f4cea752c1-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Z\u00fcrich;Microsoft Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ethz.ch;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "ETHZ;MSR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "title": "State Aware Imitation Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9076",
        "id": "9076",
        "author_site": "Yannick Schroecker, Charles Isbell",
        "author": "Yannick Schroecker; Charles L Isbell",
        "abstract": "Imitation learning is the study of learning how to act given a set of demonstrations provided by a human expert. It is intuitively apparent that learning to take optimal actions is a simpler undertaking in situations that are similar to the ones shown by the teacher. However, imitation learning approaches do not tend to use this insight directly. In this paper, we introduce State Aware Imitation Learning (SAIL), an imitation learning algorithm that allows an agent to learn how to remain in states where it can confidently take the correct action and how to recover if it is lead astray. Key to this algorithm is a gradient learned using a temporal difference update rule which leads the agent to prefer states similar to the demonstrated states. We show that estimating a linear approximation of this gradient yields similar theoretical guarantees to online temporal difference learning approaches and empirically show that SAIL can effectively be used for imitation learning in continuous domains with non-linear function approximators used for both the policy representation and the gradient estimate.",
        "bibtex": "@inproceedings{NIPS2017_08e6bea8,\n author = {Schroecker, Yannick and Isbell, Charles L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {State Aware Imitation Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/08e6bea8e90ba87af3c9554d94db6579-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/08e6bea8e90ba87af3c9554d94db6579-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/08e6bea8e90ba87af3c9554d94db6579-Reviews.html",
        "metareview": "",
        "pdf_size": 438227,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8900249840175968929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/08e6bea8e90ba87af3c9554d94db6579-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "College of Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Statistical Cost Sharing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9393",
        "id": "9393",
        "author_site": "Eric Balkanski, Umar Syed, Sergei Vassilvitskii",
        "author": "Eric Balkanski; Umar Syed; Sergei Vassilvitskii",
        "abstract": "We study the cost sharing problem for cooperative games in situations where the cost function C is not available via oracle queries, but must instead be learned from samples drawn from a distribution, represented as tuples (S, C(S)), for different subsets S of players. We formalize this approach, which we call statistical cost sharing, and consider the computation of the core and the Shapley value.  Expanding on the work by Balcan et al, we give precise sample complexity bounds for computing cost shares that satisfy the core property with high probability for any function with a non-empty core. For the Shapley value, which has never been studied in this setting, we show that for submodular cost functions with curvature bounded curvature kappa it can be approximated from samples from the uniform distribution to a sqrt{1 - kappa} factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value and that these can  be approximated arbitrarily well from samples from any distribution and for any function.",
        "bibtex": "@inproceedings{NIPS2017_32b3ee02,\n author = {Balkanski, Eric and Syed, Umar and Vassilvitskii, Sergei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Cost Sharing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/32b3ee0272954b956a7d1f86f76afa21-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/32b3ee0272954b956a7d1f86f76afa21-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/32b3ee0272954b956a7d1f86f76afa21-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/32b3ee0272954b956a7d1f86f76afa21-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/32b3ee0272954b956a7d1f86f76afa21-Reviews.html",
        "metareview": "",
        "pdf_size": 395012,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5346153230204255644&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Harvard University; Google NYC; Google NYC",
        "aff_domain": "g.harvard.edu;google.com;google.com",
        "email": "g.harvard.edu;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/32b3ee0272954b956a7d1f86f76afa21-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Harvard University;Google NYC",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.harvard.edu;",
        "aff_unique_abbr": "Harvard;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Stein Variational Gradient Descent as Gradient Flow",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9096",
        "id": "9096",
        "author": "Qiang Liu",
        "abstract": "Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a  gradient-based update constructed to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient flow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator.",
        "bibtex": "@inproceedings{NIPS2017_17ed8abe,\n author = {Liu, Qiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stein Variational Gradient Descent as Gradient Flow},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/17ed8abedc255908be746d245e50263a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/17ed8abedc255908be746d245e50263a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/17ed8abedc255908be746d245e50263a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/17ed8abedc255908be746d245e50263a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/17ed8abedc255908be746d245e50263a-Reviews.html",
        "metareview": "",
        "pdf_size": 289951,
        "gs_citation": 344,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11291251331453142697&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Dartmouth College",
        "aff_domain": "dartmouth.edu",
        "email": "dartmouth.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/17ed8abedc255908be746d245e50263a-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Dartmouth College",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://dartmouth.edu",
        "aff_unique_abbr": "Dartmouth",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9459",
        "id": "9459",
        "author_site": "Geoffrey Roeder, Yuhuai Wu, David Duvenaud",
        "author": "Geoffrey Roeder; Yuhuai Wu; David K. Duvenaud",
        "abstract": "We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.",
        "bibtex": "@inproceedings{NIPS2017_e91068ff,\n author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Reviews.html",
        "metareview": "",
        "pdf_size": 583948,
        "gs_citation": 252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9098640777107985598&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Toronto; University of Toronto; University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e91068fff3d7fa1594dfdf3b4308433a-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Stochastic Approximation for Canonical Correlation Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9254",
        "id": "9254",
        "author_site": "Raman Arora, Teodor Vanislavov Marinov, Poorya Mianjy, Nati Srebro",
        "author": "Raman Arora; Teodor Vanislavov Marinov; Poorya Mianjy; Nati Srebro",
        "abstract": "We propose novel first-order stochastic approximation algorithms for canonical correlation analysis (CCA). Algorithms presented are instances of inexact matrix stochastic gradient (MSG) and inexact matrix exponentiated gradient (MEG), and achieve $\\epsilon$-suboptimality in the population objective in $\\operatorname{poly}(\\frac{1}{\\epsilon})$ iterations. We also consider practical variants of the proposed algorithms and compare them with other methods for CCA both theoretically and empirically.",
        "bibtex": "@inproceedings{NIPS2017_c30fb4dc,\n author = {Arora, Raman and Marinov, Teodor Vanislavov and Mianjy, Poorya and Srebro, Nati},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Approximation for Canonical Correlation Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c30fb4dc55d801fc7473840b5b161dfa-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c30fb4dc55d801fc7473840b5b161dfa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c30fb4dc55d801fc7473840b5b161dfa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c30fb4dc55d801fc7473840b5b161dfa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c30fb4dc55d801fc7473840b5b161dfa-Reviews.html",
        "metareview": "",
        "pdf_size": 682285,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11782199183468065161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computer Science, Johns Hopkins University; Dept. of Computer Science, Johns Hopkins University; Dept. of Computer Science, Johns Hopkins University; TTI-Chicago",
        "aff_domain": "cs.jhu.edu;jhu.edu;jhu.edu;ttic.edu",
        "email": "cs.jhu.edu;jhu.edu;jhu.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c30fb4dc55d801fc7473840b5b161dfa-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Johns Hopkins University;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Dept. of Computer Science;",
        "aff_unique_url": "https://www.jhu.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "JHU;TTI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Stochastic Mirror Descent in Variationally Coherent Optimization Problems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9470",
        "id": "9470",
        "author_site": "Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, Peter W Glynn",
        "author": "Zhengyuan Zhou; Panayotis Mertikopoulos; Nicholas Bambos; Stephen Boyd; Peter W. Glynn",
        "abstract": "In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasiconvex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem\u2019s solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.",
        "bibtex": "@inproceedings{NIPS2017_e6ba70fc,\n author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Boyd, Stephen and Glynn, Peter W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Mirror Descent in Variationally Coherent Optimization Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6ba70fc093b4ce912d769ede1ceeba8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6ba70fc093b4ce912d769ede1ceeba8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e6ba70fc093b4ce912d769ede1ceeba8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6ba70fc093b4ce912d769ede1ceeba8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6ba70fc093b4ce912d769ede1ceeba8-Reviews.html",
        "metareview": "",
        "pdf_size": 2256985,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14931532468795980165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Stanford University; Univ. Grenoble Alpes, CNRS, Inria, LIG; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;imag.fr;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;imag.fr;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6ba70fc093b4ce912d769ede1ceeba8-Abstract.html",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Stanford University;Universit\u00e9 Grenoble Alpes",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.univ-grenoble-alpes.fr",
        "aff_unique_abbr": "Stanford;UGA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8953",
        "id": "8953",
        "author_site": "Alberto Bietti, Julien Mairal",
        "author": "Alberto Bietti; Julien Mairal",
        "abstract": "Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. Unfortunately, these techniques are unable to deal with stochastic perturbations of input data, induced for example by data augmentation. In such cases, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for these settings when the objective is composite and strongly convex. The convergence rate outperforms SGD with a typically much smaller constant factor, which depends on the variance of gradient estimates only due to perturbations on a single example.",
        "bibtex": "@inproceedings{NIPS2017_82b8a343,\n author = {Bietti, Alberto and Mairal, Julien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/82b8a3434904411a9fdc43ca87cee70c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/82b8a3434904411a9fdc43ca87cee70c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/82b8a3434904411a9fdc43ca87cee70c-Reviews.html",
        "metareview": "",
        "pdf_size": 890378,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7797413739712726072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Inria*; Inria*",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/82b8a3434904411a9fdc43ca87cee70c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Inria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Stochastic Submodular Maximization: The Case of Coverage Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9452",
        "id": "9452",
        "author_site": "Mohammad Karimi, Mario Lucic, Hamed Hassani, Andreas Krause",
        "author": "Mohammad Karimi; Mario Lucic; Hamed Hassani; Andreas Krause",
        "abstract": "Stochastic optimization of continuous objectives is at the heart of modern machine learning. However, many important problems are of discrete nature and often involve submodular objectives. We seek to unleash the power of stochastic continuous optimization, namely stochastic gradient descent and its variants, to such discrete problems. We first introduce the problem of stochastic submodular optimization, where one needs to optimize a submodular objective which is given as an expectation. Our model captures situations where the discrete objective arises as an empirical risk (e.g., in the case of exemplar-based clustering), or is given as an explicit stochastic model (e.g., in the case of influence maximization in social networks). By exploiting that common extensions act linearly on the class of submodular functions, we employ projected stochastic gradient ascent and its variants in the continuous domain, and perform rounding to obtain discrete solutions. We focus on the rich and widely used family of weighted coverage functions. We show that our approach yields solutions that are guaranteed to match the optimal approximation guarantees, while reducing the computational cost by several orders of magnitude, as we demonstrate empirically.",
        "bibtex": "@inproceedings{NIPS2017_c2f32522,\n author = {Karimi, Mohammad and Lucic, Mario and Hassani, Hamed and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Submodular Maximization: The Case of Coverage Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c2f32522a84d5e6357e6abac087f1b0b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c2f32522a84d5e6357e6abac087f1b0b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c2f32522a84d5e6357e6abac087f1b0b-Reviews.html",
        "metareview": "",
        "pdf_size": 1484733,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17029717477502049690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Electrical and Systems Engineering, University of Pennsylvania; Department of Computer Science, ETH Zurich",
        "aff_domain": "ethz.ch;inf.ethz.ch;seas.upenn.edu;ethz.ch",
        "email": "ethz.ch;inf.ethz.ch;seas.upenn.edu;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c2f32522a84d5e6357e6abac087f1b0b-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Zurich;University of Pennsylvania",
        "aff_unique_dep": "Department of Computer Science;Department of Electrical and Systems Engineering",
        "aff_unique_url": "https://www.ethz.ch;https://www.upenn.edu",
        "aff_unique_abbr": "ETHZ;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Stochastic and Adversarial Online Learning without Hyperparameters",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9282",
        "id": "9282",
        "author_site": "Ashok Cutkosky, Kwabena A Boahen",
        "author": "Ashok Cutkosky; Kwabena A. Boahen",
        "abstract": "Most online optimization algorithms focus on one of two things: performing well in adversarial settings by adapting to unknown data parameters (such as Lipschitz constants), typically achieving $O(\\sqrt{T})$ regret, or performing well in stochastic settings where they can leverage some structure in the losses (such as strong convexity), typically achieving $O(\\log(T))$ regret. Algorithms that focus on the former problem hitherto achieved $O(\\sqrt{T})$ in the stochastic setting rather than $O(\\log(T))$. Here we introduce an online optimization algorithm that achieves $O(\\log^4(T))$ regret in a wide class of stochastic settings while gracefully degrading to the optimal $O(\\sqrt{T})$ regret in adversarial settings (up to logarithmic factors). Our algorithm does not require any prior knowledge about the data or tuning of parameters to achieve superior performance.",
        "bibtex": "@inproceedings{NIPS2017_6aed000a,\n author = {Cutkosky, Ashok and Boahen, Kwabena A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic and Adversarial Online Learning without Hyperparameters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Reviews.html",
        "metareview": "",
        "pdf_size": 272466,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3329494846031999193&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Stanford University; Department of Bioengineering, Stanford University",
        "aff_domain": "cs.stanford.edu;stanford.edu",
        "email": "cs.stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Straggler Mitigation in Distributed Optimization Through Data Encoding",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9318",
        "id": "9318",
        "author_site": "Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin",
        "author": "Can Karakus; Yifan Sun; Suhas Diggavi; Wotao Yin",
        "abstract": "Slow running or straggler tasks can significantly reduce computation speed in distributed computation. Recently, coding-theory-inspired approaches have been applied to mitigate the effect of straggling, through embedding redundancy in certain linear computational steps of the optimization algorithm, thus completing the computation without waiting for the stragglers. In this paper, we propose an alternate approach where we embed the redundancy directly in the data itself, and allow the computation to proceed completely oblivious to encoding. We propose several encoding schemes, and demonstrate that popular batch algorithms, such as gradient descent and L-BFGS, applied in a coding-oblivious manner, deterministically achieve sample path linear convergence to an approximate solution of the original problem, using an arbitrarily varying subset of the nodes at each iteration. Moreover, this approximation can be controlled by the amount of redundancy and the number of nodes used in each iteration. We provide experimental results demonstrating the advantage of the approach over uncoded and data replication strategies.",
        "bibtex": "@inproceedings{NIPS2017_663772ea,\n author = {Karakus, Can and Sun, Yifan and Diggavi, Suhas and Yin, Wotao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Straggler Mitigation in Distributed Optimization Through Data Encoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Reviews.html",
        "metareview": "",
        "pdf_size": 670932,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14887223700693942308&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "UCLA; Technicolor Research; UCLA; UCLA",
        "aff_domain": "ucla.edu;technicolor.com;ucla.edu;math.ucla.edu",
        "email": "ucla.edu;technicolor.com;ucla.edu;math.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, Los Angeles;Technicolor",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.ucla.edu;https://www.technicolor.com/en",
        "aff_unique_abbr": "UCLA;Technicolor",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9233",
        "id": "9233",
        "author_site": "Slobodan Mitrovic, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub M Tarnawski, Volkan Cevher",
        "author": "Slobodan Mitrovic; Ilija Bogunovic; Ashkan Norouzi-Fard; Jakub M Tarnawski; Volkan Cevher",
        "abstract": "We study the classical problem of maximizing a monotone submodular function subject to a cardinality constraint k, with two additional twists: (i) elements arrive in a streaming fashion, and (ii) m items from the algorithm\u2019s memory are removed after the stream is finished. We develop a robust submodular algorithm STAR-T. It is based on a novel partitioning structure and an exponentially decreasing thresholding rule. STAR-T makes one pass over the data and retains a short but robust summary. We show that after the removal of any m elements from the obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the remaining elements achieves a constant-factor approximation guarantee. In two different data summarization tasks, we demonstrate that it matches or outperforms existing greedy and streaming methods, even if they are allowed the benefit of knowing the removed subset in advance.",
        "bibtex": "@inproceedings{NIPS2017_3baa271b,\n author = {Mitrovic, Slobodan and Bogunovic, Ilija and Norouzi-Fard, Ashkan and Tarnawski, Jakub M and Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3baa271bc35fe054c86928f7016e8ae6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3baa271bc35fe054c86928f7016e8ae6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3baa271bc35fe054c86928f7016e8ae6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3baa271bc35fe054c86928f7016e8ae6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3baa271bc35fe054c86928f7016e8ae6-Reviews.html",
        "metareview": "",
        "pdf_size": 1640397,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6283509143330468604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "EPFL; EPFL; EPFL; EPFL; EPFL",
        "aff_domain": "ep\ufb02.ch;ep\ufb02.ch;ep\ufb02.ch;ep\ufb02.ch;ep\ufb02.ch",
        "email": "ep\ufb02.ch;ep\ufb02.ch;ep\ufb02.ch;ep\ufb02.ch;ep\ufb02.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3baa271bc35fe054c86928f7016e8ae6-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Streaming Sparse Gaussian Process Approximations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9114",
        "id": "9114",
        "author_site": "Thang Bui, Cuong Nguyen, Richard Turner",
        "author": "Thang D Bui; Cuong Nguyen; Richard E Turner",
        "abstract": "Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing  methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.",
        "bibtex": "@inproceedings{NIPS2017_f31b2046,\n author = {Bui, Thang D and Nguyen, Cuong and Turner, Richard E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Streaming Sparse Gaussian Process Approximations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f31b20466ae89669f9741e047487eb37-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f31b20466ae89669f9741e047487eb37-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f31b20466ae89669f9741e047487eb37-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f31b20466ae89669f9741e047487eb37-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f31b20466ae89669f9741e047487eb37-Reviews.html",
        "metareview": "",
        "pdf_size": 745385,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4864142280297531741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Engineering, University of Cambridge, UK; Department of Engineering, University of Cambridge, UK; Department of Engineering, University of Cambridge, UK",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f31b20466ae89669f9741e047487eb37-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Streaming Weak Submodularity: Interpreting Neural Networks on the Fly",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9185",
        "id": "9185",
        "author_site": "Ethan Elenberg, Alex Dimakis, Moran Feldman, Amin Karbasi",
        "author": "Ethan Elenberg; Alexandros G Dimakis; Moran Feldman; Amin Karbasi",
        "abstract": "In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].",
        "bibtex": "@inproceedings{NIPS2017_c182f930,\n author = {Elenberg, Ethan and Dimakis, Alexandros G and Feldman, Moran and Karbasi, Amin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Streaming Weak Submodularity: Interpreting Neural Networks on the Fly},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c182f930a06317057d31c73bb2fedd4f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c182f930a06317057d31c73bb2fedd4f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c182f930a06317057d31c73bb2fedd4f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c182f930a06317057d31c73bb2fedd4f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c182f930a06317057d31c73bb2fedd4f-Reviews.html",
        "metareview": "",
        "pdf_size": 2677549,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6803052023567746900&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Mathematics and Computer Science, Open University of Israel; Department of Electrical Engineering + Department of Computer Science, Yale University",
        "aff_domain": "utexas.edu;austin.utexas.edu;openu.ac.il;yale.edu",
        "email": "utexas.edu;austin.utexas.edu;openu.ac.il;yale.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c182f930a06317057d31c73bb2fedd4f-Abstract.html",
        "aff_unique_index": "0;0;1;2+3",
        "aff_unique_norm": "The University of Texas at Austin;Department of Mathematics and Computer Science, Open University of Israel;Institution not specified;Yale University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;;Department of Electrical Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu;;;https://www.yale.edu",
        "aff_unique_abbr": "UT Austin;;;Yale",
        "aff_campus_unique_index": "0;0;",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9445",
        "id": "9445",
        "author_site": "Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov",
        "author": "Kirill Neklyudov; Dmitry Molchanov; Arsenii Ashukha; Dmitry P Vetrov",
        "abstract": "Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves gener- alization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural net- works and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is com- puted in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.",
        "bibtex": "@inproceedings{NIPS2017_dab49080,\n author = {Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Bayesian Pruning via Log-Normal Multiplicative Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Reviews.html",
        "metareview": "",
        "pdf_size": 590570,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=397771198683206613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "National Research University Higher School of Economics+Yandex; National Research University Higher School of Economics+Skolkovo Institute of Science and Technology; National Research University Higher School of Economics+Yandex; National Research University Higher School of Economics+Yandex",
        "aff_domain": "gmail.com;hse.ru;hse.ru;hse.ru",
        "email": "gmail.com;hse.ru;hse.ru;hse.ru",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dab49080d80c724aad5ebf158d63df41-Abstract.html",
        "aff_unique_index": "0+1;0+2;0+1;0+1",
        "aff_unique_norm": "National Research University Higher School of Economics;Yandex;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://hse.ru;https://yandex.com;https://www.skoltech.ru",
        "aff_unique_abbr": "HSE;Yandex;Skoltech",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Russia"
    },
    {
        "title": "Structured Embedding Models for Grouped Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8822",
        "id": "8822",
        "author_site": "Maja Rudolph, Francisco Ruiz, Susan Athey, David Blei",
        "author": "Maja Rudolph; Francisco Ruiz; Susan Athey; David Blei",
        "abstract": "Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how SEFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.",
        "bibtex": "@inproceedings{NIPS2017_bd686fd6,\n author = {Rudolph, Maja and Ruiz, Francisco and Athey, Susan and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Embedding Models for Grouped Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bd686fd640be98efaae0091fa301e613-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bd686fd640be98efaae0091fa301e613-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bd686fd640be98efaae0091fa301e613-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bd686fd640be98efaae0091fa301e613-Reviews.html",
        "metareview": "",
        "pdf_size": 499918,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2222908665550488010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Columbia Univ.; Univ. of Cambridge + Columbia Univ.; Stanford Univ.; Columbia Univ.",
        "aff_domain": "cs.columbia.edu; ; ; ",
        "email": "cs.columbia.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bd686fd640be98efaae0091fa301e613-Abstract.html",
        "aff_unique_index": "0;1+0;2;0",
        "aff_unique_norm": "Columbia Univ.;Univ. of Cambridge;Stanford Univ.",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Structured Generative Adversarial Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9171",
        "id": "9171",
        "author_site": "Zhijie Deng, Hao Zhang, Xiaodan Liang, Luona Yang, Shizhen Xu, Jun Zhu, Eric Xing",
        "author": "Zhijie Deng; Hao Zhang; Xiaodan Liang; Luona Yang; Shizhen Xu; Jun Zhu; Eric P Xing",
        "abstract": "We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.",
        "bibtex": "@inproceedings{NIPS2017_c3535feb,\n author = {Deng, Zhijie and Zhang, Hao and Liang, Xiaodan and Yang, Luona and Xu, Shizhen and Zhu, Jun and Xing, Eric P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Generative Adversarial Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/c3535febaff29fcb7c0d20cbe94391c7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/c3535febaff29fcb7c0d20cbe94391c7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/c3535febaff29fcb7c0d20cbe94391c7-Reviews.html",
        "metareview": "",
        "pdf_size": 2044001,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4671210543197172043&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Tsinghua University; Carnegie Mellon University+Petuum Inc.; Carnegie Mellon University; Carnegie Mellon University; Tsinghua University+Carnegie Mellon University; Tsinghua University; Petuum Inc.",
        "aff_domain": "mails.tsinghua.edu.cn;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;cs.cmu.edu",
        "email": "mails.tsinghua.edu.cn;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/c3535febaff29fcb7c0d20cbe94391c7-Abstract.html",
        "aff_unique_index": "0;1+2;1;1;0+1;0;2",
        "aff_unique_norm": "Tsinghua University;Carnegie Mellon University;Petuum Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cmu.edu;https://www.petuum.com",
        "aff_unique_abbr": "THU;CMU;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1;1;1;0+1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Style Transfer from Non-Parallel Text by Cross-Alignment",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9450",
        "id": "9450",
        "author_site": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola",
        "author": "Tianxiao Shen; Tao Lei; Regina Barzilay; Tommi Jaakkola",
        "abstract": "This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.",
        "bibtex": "@inproceedings{NIPS2017_2d2c8394,\n author = {Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Style Transfer from Non-Parallel Text by Cross-Alignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2d2c8394e31101a261abf1784302bf75-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2d2c8394e31101a261abf1784302bf75-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2d2c8394e31101a261abf1784302bf75-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2d2c8394e31101a261abf1784302bf75-Reviews.html",
        "metareview": "",
        "pdf_size": 350602,
        "gs_citation": 944,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14976647505606347245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "MIT CSAIL; ASAPP Inc.; MIT CSAIL; MIT CSAIL",
        "aff_domain": "csail.mit.edu;asapp.com;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;asapp.com;csail.mit.edu;csail.mit.edu",
        "github": "https://github.com/shentianxiao/language-style-transfer",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2d2c8394e31101a261abf1784302bf75-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;ASAPP Inc.",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.asapp.com",
        "aff_unique_abbr": "MIT CSAIL;ASAPP",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Style Transfer from Non-parallel Text by Cross-Alignment",
        "author": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10118",
        "id": "10118"
    },
    {
        "title": "Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8956",
        "id": "8956",
        "author_site": "Noga Alon, Moshe Babaioff, Yannai A. Gonczarowski, Yishay Mansour, Shay Moran, Amir Yehudayoff",
        "author": "Noga Alon; Moshe Babaioff; Yannai A. Gonczarowski; Yishay Mansour; Shay Moran; Amir Yehudayoff",
        "abstract": "In this work we derive a variant of the classic Glivenko-Cantelli Theorem, which asserts uniform convergence of the empirical Cumulative Distribution Function (CDF) to the CDF of the underlying distribution. Our variant allows for tighter convergence bounds for extreme values of the CDF.  We apply our bound in the context of revenue learning, which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues, assuming a bound on the k'th moment of the valuations, for any (possibly fractional) k > 1.  For uniform convergence in the limit, we give a complete characterization and a zero-one law: if the first moment of the valuations is finite, then uniform convergence almost surely occurs; conversely, if the first moment is infinite, then uniform convergence almost never occurs.",
        "bibtex": "@inproceedings{NIPS2017_eddb904a,\n author = {Alon, Noga and Babaioff, Moshe and Gonczarowski, Yannai A. and Mansour, Yishay and Moran, Shay and Yehudayoff, Amir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/eddb904a6db773755d2857aacadb1cb0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/eddb904a6db773755d2857aacadb1cb0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/eddb904a6db773755d2857aacadb1cb0-Reviews.html",
        "metareview": "",
        "pdf_size": 309539,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17736722162917949258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Tel Aviv University, Israel + Microsoft Research; Microsoft Research; The Hebrew University of Jerusalem, Israel + Microsoft Research; Tel Aviv University, Israel + Google Research, Israel; Institute for Advanced Study, Princeton; Technion \u2014 IIT, Israel",
        "aff_domain": "tau.ac.il;microsoft.com;gonch.name;tau.ac.il;gmail.com;gmail.com",
        "email": "tau.ac.il;microsoft.com;gonch.name;tau.ac.il;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/eddb904a6db773755d2857aacadb1cb0-Abstract.html",
        "aff_unique_index": "0+1;1;2+1;0+3;4;5",
        "aff_unique_norm": "Tel Aviv University;Microsoft Corporation;The Hebrew University of Jerusalem;Google;Institute for Advanced Study;Technion \u2014 IIT, Israel",
        "aff_unique_dep": ";Microsoft Research;;Google Research;;",
        "aff_unique_url": "https://www.tau.ac.il;https://www.microsoft.com/en-us/research;https://www.huji.ac.il;https://research.google;https://ias.edu;",
        "aff_unique_abbr": "TAU;MSR;HUJI;Google;IAS;",
        "aff_campus_unique_index": ";;1;2",
        "aff_campus_unique": ";Israel;Princeton",
        "aff_country_unique_index": "0+1;1;0+1;0+0;1",
        "aff_country_unique": "Israel;United States;"
    },
    {
        "title": "Subset Selection and Summarization in Sequential Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8897",
        "id": "8897",
        "author_site": "Ehsan Elhamifar, M. Clara De Paolis Kaluza",
        "author": "Ehsan Elhamifar; M. Clara De Paolis Kaluza",
        "abstract": "Subset selection, which is the task of finding a small subset of representative items from a large ground set, finds numerous applications in different areas. Sequential data, including time-series and ordered data, contain important structural relationships among items, imposed by underlying dynamic models of data, that should play a vital role in the selection of representatives. However, nearly all existing subset selection techniques ignore underlying dynamics of data and treat items independently, leading to incompatible sets of representatives. In this paper, we develop a new framework for sequential subset selection that finds a set of representatives compatible with the dynamic models of data. To do so, we equip items with transition dynamic models and pose the problem as an integer binary optimization over assignments of sequential items to representatives, that leads to high encoding, diversity and transition potentials. Our formulation generalizes the well-known facility location objective to deal with sequential data, incorporating transition dynamics among facilities. As the proposed formulation is non-convex, we derive a max-sum message passing algorithm to solve the problem efficiently. Experiments on synthetic and real data, including instructional video summarization, show that our sequential subset selection framework not only achieves better encoding and diversity than the state of the art, but also successfully incorporates dynamics of data, leading to compatible representatives.",
        "bibtex": "@inproceedings{NIPS2017_8fecb208,\n author = {Elhamifar, Ehsan and De Paolis Kaluza, M. Clara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subset Selection and Summarization in Sequential Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8fecb20817b3847419bb3de39a609afe-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8fecb20817b3847419bb3de39a609afe-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8fecb20817b3847419bb3de39a609afe-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8fecb20817b3847419bb3de39a609afe-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8fecb20817b3847419bb3de39a609afe-Reviews.html",
        "metareview": "",
        "pdf_size": 1187963,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9332977185428718407&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer and Information Science College, Northeastern University, Boston, MA 02115; Computer and Information Science College, Northeastern University, Boston, MA 02115",
        "aff_domain": "ccs.neu.edu;ccs.neu.edu",
        "email": "ccs.neu.edu;ccs.neu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8fecb20817b3847419bb3de39a609afe-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer and Information Science College, Northeastern University, Boston, MA 02115",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Subset Selection under Noise",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9139",
        "id": "9139",
        "author_site": "Chao Qian, Jing-Cheng Shi, Yang Yu, Ke Tang, Zhi-Hua Zhou",
        "author": "Chao Qian; Jing-Cheng Shi; Yang Yu; Ke Tang; Zhi-Hua Zhou",
        "abstract": "The problem of selecting the best $k$-element subset from a universe is involved in many applications. While previous studies assumed a noise-free environment or a noisy monotone submodular objective function, this paper considers a more realistic and general situation where the evaluation of a subset is a noisy monotone function (not necessarily submodular), with both multiplicative and additive noises. To understand the impact of the noise, we firstly show the approximation ratio of the greedy algorithm and POSS, two powerful algorithms for noise-free subset selection, in the noisy environments. We then propose to incorporate a noise-aware strategy into POSS, resulting in the new PONSS algorithm. We prove that PONSS can achieve a better approximation ratio under some assumption such as i.i.d. noise distribution. The empirical results on influence maximization and sparse regression problems show the superior performance of PONSS.",
        "bibtex": "@inproceedings{NIPS2017_d7a84628,\n author = {Qian, Chao and Shi, Jing-Cheng and Yu, Yang and Tang, Ke and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subset Selection under Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d7a84628c025d30f7b2c52c958767e76-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d7a84628c025d30f7b2c52c958767e76-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d7a84628c025d30f7b2c52c958767e76-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d7a84628c025d30f7b2c52c958767e76-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d7a84628c025d30f7b2c52c958767e76-Reviews.html",
        "metareview": "",
        "pdf_size": 1386986,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15984400577609886539&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Anhui Province Key Lab of Big Data Analysis and Application, USTC, China; National Key Lab for Novel Software Technology, Nanjing University, China; National Key Lab for Novel Software Technology, Nanjing University, China; Anhui Province Key Lab of Big Data Analysis and Application, USTC, China + Shenzhen Key Lab of Computational Intelligence, SUSTech, China; National Key Lab for Novel Software Technology, Nanjing University, China",
        "aff_domain": "ustc.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn;sustc.edu.cn;lamda.nju.edu.cn",
        "email": "ustc.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn;sustc.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d7a84628c025d30f7b2c52c958767e76-Abstract.html",
        "aff_unique_index": "0;1;1;0+2;1",
        "aff_unique_norm": "Anhui Province Key Lab of Big Data Analysis and Application, USTC, China;Nanjing University;Shenzhen Key Lab of Computational Intelligence, SUSTech, China",
        "aff_unique_dep": ";National Key Lab for Novel Software Technology;",
        "aff_unique_url": ";http://www.nju.edu.cn;",
        "aff_unique_abbr": ";Nanjing U;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;;1",
        "aff_country_unique": ";China"
    },
    {
        "title": "Subspace Clustering via Tangent Cones",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9442",
        "id": "9442",
        "author_site": "Amin Jalali, Rebecca Willett",
        "author": "Amin Jalali; Rebecca Willett",
        "abstract": "Given samples lying on any of a number of subspaces, subspace clustering is the task of grouping the samples based on the their corresponding subspaces. Many subspace clustering methods operate by assigning a measure of affinity to each pair of points and feeding these affinities into a graph clustering algorithm. This paper proposes a new paradigm for subspace clustering that computes affinities based on the corresponding conic geometry. The proposed conic subspace clustering (CSC) approach considers the convex hull of a collection of normalized data points and the corresponding tangent cones. The union of subspaces underlying the data imposes a strong association between the tangent cone at a sample $x$ and the original subspace containing $x$. In addition to describing this novel geometric perspective, this paper provides a practical algorithm for subspace clustering that leverages this perspective, where a tangent cone membership test is used to estimate the affinities. This algorithm is accompanied with deterministic and stochastic guarantees on the properties of the learned affinity matrix, on the true and false positive rates and spread, which directly translate into the overall clustering accuracy.",
        "bibtex": "@inproceedings{NIPS2017_dbb42293,\n author = {Jalali, Amin and Willett, Rebecca},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subspace Clustering via Tangent Cones},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dbb422937d7ff56e049d61da730b3e11-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dbb422937d7ff56e049d61da730b3e11-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dbb422937d7ff56e049d61da730b3e11-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dbb422937d7ff56e049d61da730b3e11-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dbb422937d7ff56e049d61da730b3e11-Reviews.html",
        "metareview": "",
        "pdf_size": 10250803,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2067058578333197270&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Wisconsin Institute for Discovery, University of Wisconsin; Department of Electrical and Computer Engineering, University of Wisconsin",
        "aff_domain": "wisc.edu;discovery.wisc.edu",
        "email": "wisc.edu;discovery.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dbb422937d7ff56e049d61da730b3e11-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Wisconsin Institute for Discovery, University of Wisconsin;Department of Electrical and Computer Engineering, University of Wisconsin",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Successor Features for Transfer in Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9186",
        "id": "9186",
        "author_site": "Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom Schaul, David Silver, Hado van Hasselt",
        "author": "Andre Barreto; Will Dabney; Remi Munos; Jonathan J Hunt; Tom Schaul; Hado P van Hasselt; David Silver",
        "abstract": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: \"successor features\", a value function representation that decouples the dynamics of the environment from the rewards, and \"generalized policy improvement\", a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.",
        "bibtex": "@inproceedings{NIPS2017_350db081,\n author = {Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Successor Features for Transfer in Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/350db081a661525235354dd3e19b8c05-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/350db081a661525235354dd3e19b8c05-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/350db081a661525235354dd3e19b8c05-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/350db081a661525235354dd3e19b8c05-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/350db081a661525235354dd3e19b8c05-Reviews.html",
        "metareview": "",
        "pdf_size": 1435397,
        "gs_citation": 732,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6393259981776898534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/350db081a661525235354dd3e19b8c05-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Targeting EEG/LFP Synchrony with Neural Nets",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9239",
        "id": "9239",
        "author_site": "Yitong Li, michael Murias, samantha Major, geraldine Dawson, Kafui Dzirasa, Lawrence Carin, David Carlson",
        "author": "Yitong Li; michael Murias; samantha Major; geraldine Dawson; Kafui Dzirasa; Lawrence Carin; David E Carlson",
        "abstract": "We consider the analysis of Electroencephalography (EEG) and Local Field Potential (LFP) datasets, which are \u201cbig\u201d in terms of the size of recorded data but rarely have sufficient labels required to train complex models (e.g., conventional deep learning methods).  Furthermore, in many scientific applications, the goal is to be able to understand the underlying features related to the classification, which prohibits the blind application of deep networks. This motivates the development of a new model based on {\\em parameterized} convolutional filters guided by previous neuroscience research; the filters learn relevant frequency bands while targeting synchrony, which are frequency-specific power and phase correlations between electrodes. This results in a highly expressive convolutional neural network with only a few hundred parameters, applicable to smaller datasets.  The proposed approach is demonstrated to yield competitive (often state-of-the-art) predictive performance during our empirical tests while yielding interpretable features.  Furthermore, a Gaussian process adapter is developed to combine analysis over distinct electrode layouts, allowing the joint processing of multiple datasets to address overfitting and improve generalizability.  Finally, it is demonstrated that the proposed framework effectively tracks neural dynamics on children in a clinical trial on Autism Spectrum Disorder.",
        "bibtex": "@inproceedings{NIPS2017_7993e112,\n author = {Li, Yitong and Murias, michael and Major, samantha and Dawson, geraldine and Dzirasa, Kafui and Carin, Lawrence and Carlson, David E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Targeting EEG/LFP Synchrony with Neural Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7993e11204b215b27694b6f139e34ce8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7993e11204b215b27694b6f139e34ce8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7993e11204b215b27694b6f139e34ce8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7993e11204b215b27694b6f139e34ce8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7993e11204b215b27694b6f139e34ce8-Reviews.html",
        "metareview": "",
        "pdf_size": 810414,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16035983333844150979&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7993e11204b215b27694b6f139e34ce8-Abstract.html"
    },
    {
        "title": "Task-based End-to-end Model Learning in Stochastic Optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9323",
        "id": "9323",
        "author_site": "Priya Donti, J. Zico Kolter, Brandon Amos",
        "author": "Priya Donti; Brandon Amos; J. Zico Kolter",
        "abstract": "With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.",
        "bibtex": "@inproceedings{NIPS2017_3fc2c60b,\n author = {Donti, Priya and Amos, Brandon and Kolter, J. Zico},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Task-based End-to-end Model Learning in Stochastic Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3fc2c60b5782f641f76bcefc39fb2392-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3fc2c60b5782f641f76bcefc39fb2392-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3fc2c60b5782f641f76bcefc39fb2392-Reviews.html",
        "metareview": "",
        "pdf_size": 901226,
        "gs_citation": 458,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11880055461098651537&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science + Dept. of Engr. & Public Policy, Carnegie Mellon University; Dept. of Computer Science, Carnegie Mellon University; Dept. of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html",
        "aff_unique_index": "0+1;2;2",
        "aff_unique_norm": "University Affiliation Not Specified;Dept. of Engr. & Public Policy, Carnegie Mellon University;Dept. of Computer Science, Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Teaching Machines to Describe Images with Natural Language Feedback",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9283",
        "id": "9283",
        "author_site": "Huan Ling, Sanja Fidler",
        "author": "huan ling; Sanja Fidler",
        "abstract": "Robots will eventually be part of every household.  It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. A descriptive sentence can provide a stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.  We propose a phrase-based captioning model trained with policy gradients, and design a critic that provides reward to the learner by conditioning on the human-provided feedback. We show  that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.",
        "bibtex": "@inproceedings{NIPS2017_8e68c3c7,\n author = {ling, huan and Fidler, Sanja},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Teaching Machines to Describe Images with Natural Language Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8e68c3c7bf14ad0bcaba52babfa470bd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8e68c3c7bf14ad0bcaba52babfa470bd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8e68c3c7bf14ad0bcaba52babfa470bd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8e68c3c7bf14ad0bcaba52babfa470bd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8e68c3c7bf14ad0bcaba52babfa470bd-Reviews.html",
        "metareview": "",
        "pdf_size": 1136683,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8055553653707786114&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Toronto1 + Vector Institute2; University of Toronto1 + Vector Institute2",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8e68c3c7bf14ad0bcaba52babfa470bd-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Toronto;Vector Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://vectorinstitute.ai/",
        "aff_unique_abbr": "U of T;Vector Institute",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9206",
        "id": "9206",
        "author_site": "Prateep Bhattacharjee, Sukhendu Das",
        "author": "Prateep Bhattacharjee; Sukhendu Das",
        "abstract": "Predicting the future from a sequence of video frames has been recently a sought after yet challenging task in the field of computer vision and machine learning. Although there have been efforts for tracking using motion trajectories and flow features, the complex problem of generating unseen frames has not been studied extensively. In this paper, we deal with this problem using convolutional models within a multi-stage Generative Adversarial Networks (GAN) framework. The proposed method uses two stages of GANs to generate a crisp and clear set of future frames. Although GANs have been used in the past for predicting the future, none of the works consider the relation between subsequent frames in the temporal dimension. Our main contribution lies in formulating two objective functions based on the Normalized Cross Correlation (NCC) and the Pairwise Contrastive Divergence (PCD) for solving this problem. This method, coupled with the traditional L1 loss, has been experimented with three real-world video datasets, viz. Sports-1M, UCF-101 and the KITTI. Performance analysis reveals superior results over the recent state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2017_b166b57d,\n author = {Bhattacharjee, Prateep and Das, Sukhendu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b166b57d195370cd41f80dd29ed523d9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b166b57d195370cd41f80dd29ed523d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b166b57d195370cd41f80dd29ed523d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b166b57d195370cd41f80dd29ed523d9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b166b57d195370cd41f80dd29ed523d9-Reviews.html",
        "metareview": "",
        "pdf_size": 2079160,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12728801572679411474&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Visualization and Perception Laboratory, Department of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India; Visualization and Perception Laboratory, Department of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India",
        "aff_domain": "cse.iitm.ac.in;iitm.ac.in",
        "email": "cse.iitm.ac.in;iitm.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b166b57d195370cd41f80dd29ed523d9-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Visualization and Perception Laboratory, Department of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Tensor Biclustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8923",
        "id": "8923",
        "author_site": "Soheil Feizi, Hamid Javadi, David Tse",
        "author": "Soheil Feizi; Hamid Javadi; David Tse",
        "abstract": "Consider a dataset where data is collected on multiple features of multiple individuals over multiple times. This type of data can be represented as a three dimensional individual/feature/time tensor and has become increasingly prominent in various areas of science. The tensor biclustering problem computes a subset of individuals and a subset of features whose signal trajectories over time lie in a low-dimensional subspace, modeling similarity among the signal trajectories while allowing different scalings across different individuals or different features. We study the information-theoretic limit of this problem under a generative model. Moreover, we propose an efficient spectral algorithm to solve the tensor biclustering problem and analyze its achievability bound in an asymptotic regime. Finally, we show the efficiency of our proposed method in several synthetic and real datasets.",
        "bibtex": "@inproceedings{NIPS2017_ede7e2b6,\n author = {Feizi, Soheil and Javadi, Hamid and Tse, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tensor Biclustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Reviews.html",
        "metareview": "",
        "pdf_size": 3769831,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5637264363449881172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ede7e2b6d13a41ddf9f4bdef84fdc737-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Tensor encoding and decomposition of brain connectomes with application to tractography evaluation",
        "author": "Cesar F Caiafa, Olaf Sporns, Andrew Saykin, Franco Pestilli",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/spotlight/10141",
        "id": "10141"
    },
    {
        "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8942",
        "id": "8942",
        "author_site": "Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li",
        "author": "Wei Wen; Cong Xu; Feng Yan; Chunpeng Wu; Yandan Wang; Yiran Chen; Hai Li",
        "abstract": "High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available.",
        "bibtex": "@inproceedings{NIPS2017_89fcd07f,\n author = {Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/89fcd07f20b6785b92134bd6c1d0fa42-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/89fcd07f20b6785b92134bd6c1d0fa42-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/89fcd07f20b6785b92134bd6c1d0fa42-Reviews.html",
        "metareview": "",
        "pdf_size": 726980,
        "gs_citation": 1233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10878522154628533487&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Duke University; Hewlett Packard Labs; University of Nevada \u2013 Reno; Duke University; University of Pittsburgh; Duke University; Duke University",
        "aff_domain": "duke.edu;hpe.com;unr.edu;duke.edu;pitt.edu;duke.edu;duke.edu",
        "email": "duke.edu;hpe.com;unr.edu;duke.edu;pitt.edu;duke.edu;duke.edu",
        "github": "https://github.com/wenwei202/terngrad",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html",
        "aff_unique_index": "0;1;2;0;3;0;0",
        "aff_unique_norm": "Duke University;Hewlett Packard Labs;University of Nevada \u2013 Reno;University of Pittsburgh",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.duke.edu;https://www.hpl.hp.com;;https://www.pitt.edu",
        "aff_unique_abbr": "Duke;HPL;;Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Test of Time Award",
        "author": "ali rahimi, Benjamin Recht",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/oral/10828",
        "id": "10828"
    },
    {
        "title": "Testing and Learning on Distributions with Symmetric Noise Invariance",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8926",
        "id": "8926",
        "author_site": "Ho Chung Law, Christopher Yau, Dino Sejdinovic",
        "author": "Ho Chung Law; Christopher Yau; Dino Sejdinovic",
        "abstract": "Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rarely that all possible differences between samples are of interest -- discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise.",
        "bibtex": "@inproceedings{NIPS2017_67d16d00,\n author = {Law, Ho Chung and Yau, Christopher and Sejdinovic, Dino},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Testing and Learning on Distributions with Symmetric Noise Invariance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/67d16d00201083a2b118dd5128dd6f59-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/67d16d00201083a2b118dd5128dd6f59-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/67d16d00201083a2b118dd5128dd6f59-Reviews.html",
        "metareview": "",
        "pdf_size": 471247,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9075551890882084322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics, University Of Oxford; Centre for Computational Biology, University of Birmingham; Department of Statistics, University Of Oxford",
        "aff_domain": "stats.ox.ac.uk;bham.ac.uk;stats.ox.ac.uk",
        "email": "stats.ox.ac.uk;bham.ac.uk;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department of Statistics, University Of Oxford;Centre for Computational Biology, University of Birmingham",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "The Expressive Power of Neural Networks: A View from the Width",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9394",
        "id": "9394",
        "author_site": "Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, Liwei Wang",
        "author": "Zhou Lu; Hongming Pu; Feicheng Wang; Zhiqiang Hu; Liwei Wang",
        "abstract": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.",
        "bibtex": "@inproceedings{NIPS2017_32cbf687,\n author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Expressive Power of Neural Networks: A View from the Width},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Reviews.html",
        "metareview": "",
        "pdf_size": 2916583,
        "gs_citation": 1371,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11989002681809936407&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematics, Peking University + Center for Data Science, Peking University, Beijing Institute of Big Data Research; Department of Mathematics, Peking University; Department of Mathematics, Peking University + Center for Data Science, Peking University, Beijing Institute of Big Data Research; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University + Center for Data Science, Peking University, Beijing Institute of Big Data Research",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;cis.pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;cis.pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html",
        "aff_unique_index": "0+1;0;0+1;1;1+1",
        "aff_unique_norm": "Department of Mathematics, Peking University;Peking University",
        "aff_unique_dep": ";Center for Data Science",
        "aff_unique_url": ";http://www.pku.edu.cn",
        "aff_unique_abbr": ";PKU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "1;1;1;1+1",
        "aff_country_unique": ";China"
    },
    {
        "title": "The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9222",
        "id": "9222",
        "author_site": "Arun Suggala, Mladen Kolar, Pradeep Ravikumar",
        "author": "Arun Suggala; Mladen Kolar; Pradeep K Ravikumar",
        "abstract": "Non-parametric multivariate density estimation faces strong statistical and computational bottlenecks, and the more practical approaches impose near-parametric assumptions on the form of the density functions. In this paper, we leverage recent developments to propose a class of non-parametric models which have very attractive computational and statistical properties. Our approach relies on the simple function space assumption that the conditional distribution of each variable conditioned on the other variables has a non-parametric exponential family form.",
        "bibtex": "@inproceedings{NIPS2017_fd69dbe2,\n author = {Suggala, Arun and Kolar, Mladen and Ravikumar, Pradeep K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/fd69dbe29f156a7ef876a40a94f65599-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/fd69dbe29f156a7ef876a40a94f65599-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/fd69dbe29f156a7ef876a40a94f65599-Reviews.html",
        "metareview": "",
        "pdf_size": 383654,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9604231113490643968&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University; University of Chicago; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;chicagobooth.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;chicagobooth.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/fd69dbe29f156a7ef876a40a94f65599-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.uchicago.edu",
        "aff_unique_abbr": "CMU;UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Importance of Communities for Learning to Influence",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9359",
        "id": "9359",
        "author_site": "Eric Balkanski, Nicole Immorlica, Yaron Singer",
        "author": "Eric Balkanski; Nicole Immorlica; Yaron Singer",
        "abstract": "We consider the canonical problem of influence maximization in social networks. Since the seminal work of Kempe, Kleinberg, and Tardos there have been two, largely disjoint efforts on this problem. The first studies the problem associated with learning the generative model that produces cascades, and the second focuses on the algorithmic challenge of identifying a set of influencers, assuming the generative model is known. Recent results on learning and optimization imply that in general, if the generative model is not known but rather learned from training data, no algorithm for influence maximization can yield a constant factor approximation guarantee using polynomially-many samples, drawn from any distribution.  In this paper we describe a simple algorithm for maximizing influence from training data. The main idea behind the algorithm is to leverage the strong community structure of social networks and identify a set of individuals who are influentials but whose communities have little overlap. Although in general, the approximation guarantee of such an algorithm is unbounded, we show that this algorithm performs well experimentally. To analyze its performance, we prove this algorithm obtains a constant factor approximation guarantee on graphs generated through the stochastic block model, traditionally used to model networks with community structure.",
        "bibtex": "@inproceedings{NIPS2017_a36e841c,\n author = {Balkanski, Eric and Immorlica, Nicole and Singer, Yaron},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Importance of Communities for Learning to Influence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a36e841c5230a79c2102036d2e259848-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a36e841c5230a79c2102036d2e259848-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a36e841c5230a79c2102036d2e259848-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a36e841c5230a79c2102036d2e259848-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a36e841c5230a79c2102036d2e259848-Reviews.html",
        "metareview": "",
        "pdf_size": 1343817,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16128141484141956014&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Harvard University; Microsoft Research; Harvard University",
        "aff_domain": "g.harvard.edu;microsoft.com;seas.harvard.edu",
        "email": "g.harvard.edu;microsoft.com;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a36e841c5230a79c2102036d2e259848-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Harvard University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.harvard.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Harvard;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9195",
        "id": "9195",
        "author_site": "Ashia C Wilson, Becca Roelofs, Mitchell Stern, Nati Srebro, Benjamin Recht",
        "author": "Ashia C Wilson; Rebecca Roelofs; Mitchell Stern; Nati Srebro; Benjamin Recht",
        "abstract": "Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks.  Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD).  We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half.  We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.",
        "bibtex": "@inproceedings{NIPS2017_81b3833e,\n author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Reviews.html",
        "metareview": "",
        "pdf_size": 920795,
        "gs_citation": 1427,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14847173290255149613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; Toyota Technological Institute at Chicago; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;ttic.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;ttic.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UC Berkeley;TTI Chicago",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Berkeley;Chicago",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9443",
        "id": "9443",
        "author_site": "Hongyuan Mei, Jason Eisner",
        "author": "Hongyuan Mei; Jason M Eisner",
        "abstract": "Many events occur in the world. Some event types are stochastically excited or inhibited\u2014in the sense of having their probabilities elevated or decreased\u2014by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.",
        "bibtex": "@inproceedings{NIPS2017_6463c884,\n author = {Mei, Hongyuan and Eisner, Jason M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6463c88460bd63bbe256e495c63aa40b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6463c88460bd63bbe256e495c63aa40b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6463c88460bd63bbe256e495c63aa40b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6463c88460bd63bbe256e495c63aa40b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6463c88460bd63bbe256e495c63aa40b-Reviews.html",
        "metareview": "",
        "pdf_size": 554050,
        "gs_citation": 812,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1403516690866775840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6463c88460bd63bbe256e495c63aa40b-Abstract.html"
    },
    {
        "title": "The Numerics of GANs",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8972",
        "id": "8972",
        "author_site": "Lars Mescheder, Sebastian Nowozin, Andreas Geiger",
        "author": "Lars Mescheder; Sebastian Nowozin; Andreas Geiger",
        "abstract": "In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.",
        "bibtex": "@inproceedings{NIPS2017_4588e674,\n author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Numerics of GANs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/4588e674d3f0faf985047d4c3f13ed0d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4588e674d3f0faf985047d4c3f13ed0d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4588e674d3f0faf985047d4c3f13ed0d-Reviews.html",
        "metareview": "",
        "pdf_size": 1206569,
        "gs_citation": 546,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10067526666278916016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Autonomous Vision Group, MPI T\u00fcbingen; Machine Intelligence and Perception Group, Microsoft Research; Autonomous Vision Group, MPI T\u00fcbingen",
        "aff_domain": "tuebingen.mpg.de;microsoft.com;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;microsoft.com;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Machine Intelligence and Perception Group, Microsoft Research",
        "aff_unique_dep": "Autonomous Vision Group;",
        "aff_unique_url": "https://www.mpi-bcb.de;",
        "aff_unique_abbr": "MPI T\u00fcbingen;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "title": "The Reversible Residual Network: Backpropagation Without Storing Activations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9009",
        "id": "9009",
        "author_site": "Aidan Gomez, Mengye Ren, Raquel Urtasun, Roger Grosse",
        "author": "Aidan N Gomez; Mengye Ren; Raquel Urtasun; Roger B Grosse",
        "abstract": "Residual Networks (ResNets) have demonstrated significant improvement over traditional Convolutional Neural Networks (CNNs) on image classification, increasing in performance as networks grow both deeper and wider.  However, memory consumption becomes a bottleneck as one needs to store all the intermediate activations for calculating gradients using backpropagation. In this work, we present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backprop. We demonstrate the effectiveness of RevNets on CIFAR and ImageNet, establishing nearly identical performance to equally-sized ResNets, with activation storage requirements independent of depth.",
        "bibtex": "@inproceedings{NIPS2017_f9be311e,\n author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Reversible Residual Network: Backpropagation Without Storing Activations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Reviews.html",
        "metareview": "",
        "pdf_size": 482512,
        "gs_citation": 670,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2439303773648944118&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of Toronto1 + Vector Institute for Arti\ufb01cial Intelligence2 + Uber Advanced Technologies Group3; University of Toronto1 + Vector Institute for Arti\ufb01cial Intelligence2 + Uber Advanced Technologies Group3; University of Toronto1 + Vector Institute for Arti\ufb01cial Intelligence2 + Uber Advanced Technologies Group3; University of Toronto1 + Vector Institute for Arti\ufb01cial Intelligence2",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "https://github.com/renmengye/revnet-public",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html",
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1",
        "aff_unique_norm": "University of Toronto;Vector Institute for Arti\ufb01cial Intelligence2;Uber Advanced Technologies Group3",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;;",
        "aff_unique_abbr": "U of T;;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "title": "The Scaling Limit of High-Dimensional Online Independent Component Analysis",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9432",
        "id": "9432",
        "author_site": "Chuang Wang, Yue Lu",
        "author": "Chuang Wang; Yue Lu",
        "abstract": "We analyze the dynamics of an online algorithm for independent component analysis in the high-dimensional scaling limit. As the ambient dimension tends to infinity, and with proper time scaling, we show that the time-varying joint empirical measure of the target feature vector and the estimates provided by the algorithm will converge weakly to a deterministic measured-valued process that can be characterized as the unique solution of a nonlinear PDE. Numerical solutions of this PDE, which involves two spatial variables and one time variable, can be efficiently obtained. These solutions provide detailed information about the performance of the ICA algorithm, as many practical performance metrics are functionals of the joint empirical measures. Numerical simulations show that our asymptotic analysis is accurate even for moderate dimensions. In addition to providing a tool for understanding the performance of the algorithm, our PDE analysis also provides useful insight. In particular, in the high-dimensional limit, the original coupled dynamics associated with the algorithm will be asymptotically \u201cdecoupled\u201d, with each coordinate independently solving a 1-D effective minimization problem via stochastic gradient descent. Exploiting this insight to design new algorithms for achieving optimal trade-offs between computational and statistical efficiency may prove an interesting line of future research.",
        "bibtex": "@inproceedings{NIPS2017_3cfbdf46,\n author = {Wang, Chuang and Lu, Yue},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Scaling Limit of High-Dimensional Online Independent Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3cfbdf468f0a03187f6cee51a25e5e9a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3cfbdf468f0a03187f6cee51a25e5e9a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3cfbdf468f0a03187f6cee51a25e5e9a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3cfbdf468f0a03187f6cee51a25e5e9a-Reviews.html",
        "metareview": "",
        "pdf_size": 558353,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1274614364226949463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3cfbdf468f0a03187f6cee51a25e5e9a-Abstract.html"
    },
    {
        "title": "The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8819",
        "id": "8819",
        "author_site": "Krzysztof Choromanski, Mark Rowland, Adrian Weller",
        "author": "Krzysztof M Choromanski; Mark Rowland; Adrian Weller",
        "abstract": "We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel, we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits, and empirical results which suggest that the approach is helpful in a wider range of applications.",
        "bibtex": "@inproceedings{NIPS2017_bf822969,\n author = {Choromanski, Krzysztof M and Rowland, Mark and Weller, Adrian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/bf8229696f7a3bb4700cfddef19fa23f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bf8229696f7a3bb4700cfddef19fa23f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bf8229696f7a3bb4700cfddef19fa23f-Reviews.html",
        "metareview": "",
        "pdf_size": 1259228,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10087756030114797990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Google Brain Robotics; University of Cambridge; University of Cambridge + Alan Turing Institute",
        "aff_domain": "google.com;cam.ac.uk;cam.ac.uk",
        "email": "google.com;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bf8229696f7a3bb4700cfddef19fa23f-Abstract.html",
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "Google;University of Cambridge;Alan Turing Institute",
        "aff_unique_dep": "Google Brain Robotics;;",
        "aff_unique_url": "https://ai.google;https://www.cam.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "Google Brain Robotics;Cambridge;ATI",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Mountain View;Cambridge;",
        "aff_country_unique_index": "0;1;1+1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "The power of absolute discounting: all-dimensional distribution estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9434",
        "id": "9434",
        "author_site": "Moein Falahatgar, Mesrob Ohannessian, Alon Orlitsky, Venkatadheeraj Pichapati",
        "author": "Moein Falahatgar; Mesrob I Ohannessian; Alon Orlitsky; Venkatadheeraj Pichapati",
        "abstract": "Categorical models are a natural fit for many problems. When learning the distribution of categories from samples, high-dimensionality may dilute the data. Minimax optimality is too pessimistic to remedy this issue. A serendipitously discovered estimator, absolute discounting, corrects empirical frequencies by subtracting a constant from observed categories, which it then redistributes among the unobserved. It outperforms classical estimators empirically, and has been used extensively in natural language modeling. In this paper, we rigorously explain the prowess of this estimator using less pessimistic notions. We show  that (1) absolute discounting recovers classical minimax KL-risk rates, (2) it is \\emph{adaptive} to an effective dimension rather than the true dimension, (3) it is strongly related to the Good-Turing estimator and inherits its \\emph{competitive} properties. We use power-law distributions as the cornerstone of these results. We validate the theory via synthetic data and an application to the Global Terrorism Database.",
        "bibtex": "@inproceedings{NIPS2017_331316d4,\n author = {Falahatgar, Moein and Ohannessian, Mesrob I and Orlitsky, Alon and Pichapati, Venkatadheeraj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The power of absolute discounting: all-dimensional distribution estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/331316d4efb44682092a006307b9ae3a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/331316d4efb44682092a006307b9ae3a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/331316d4efb44682092a006307b9ae3a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/331316d4efb44682092a006307b9ae3a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/331316d4efb44682092a006307b9ae3a-Reviews.html",
        "metareview": "",
        "pdf_size": 416204,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8501751566033251349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "UCSD; TTIC; UCSD; UCSD",
        "aff_domain": "ucsd.edu;gmail.com;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;gmail.com;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/331316d4efb44682092a006307b9ae3a-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, San Diego;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://ucsd.edu;https://www.ttic.edu",
        "aff_unique_abbr": "UCSD;TTIC",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "San Diego;Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Thinking Fast and Slow with Deep Learning and Tree Search",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9311",
        "id": "9311",
        "author_site": "Thomas Anthony, Zheng Tian, David Barber",
        "author": "Thomas Anthony; Zheng Tian; David Barber",
        "abstract": "Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex1.0, the most recent Olympiad Champion player to be publicly released.",
        "bibtex": "@inproceedings{NIPS2017_d8e1344e,\n author = {Anthony, Thomas and Tian, Zheng and Barber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Thinking Fast and Slow with Deep Learning and Tree Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Reviews.html",
        "metareview": "",
        "pdf_size": 946093,
        "gs_citation": 479,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15054045725784990239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University College London; University College London; University College London + Alan Turing Institute",
        "aff_domain": "ucl.ac.uk; ; ",
        "email": "ucl.ac.uk; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University College London;Alan Turing Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "UCL;ATI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9248",
        "id": "9248",
        "author_site": "Christian Borgs, Jennifer Chayes, Christina Lee, Devavrat Shah",
        "author": "Christian Borgs; Jennifer Chayes; Christina E. Lee; Devavrat Shah",
        "abstract": "The sparse matrix estimation problem consists of estimating the distribution of an $n\\times n$  matrix $Y$, from a sparsely observed single instance of this matrix where the entries of $Y$ are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filtering-style algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to $0$ at the rate of $O(d^2 (pn)^{-2/5})$ as long as $\\omega(d^5 n)$ random entries from a total of $n^2$ entries of $Y$ are observed (uniformly sampled), $\\E[Y]$ has rank $d$, and the entries of $Y$ have bounded support. The maximum squared error across all entries converges to $0$ with high probability as long as we observe a little more, $\\Omega(d^5 n \\ln^5(n))$ entries. Our results are the best known sample complexity results in this generality.",
        "bibtex": "@inproceedings{NIPS2017_7cc23420,\n author = {Borgs, Christian and Chayes, Jennifer and Lee, Christina E. and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7cc234202e98d2722580858573fd0817-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7cc234202e98d2722580858573fd0817-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7cc234202e98d2722580858573fd0817-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7cc234202e98d2722580858573fd0817-Reviews.html",
        "metareview": "",
        "pdf_size": 297472,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17600484680269763861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Research New England; Microsoft Research New England; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "microsoft.com;microsoft.com;mit.edu;mit.edu",
        "email": "microsoft.com;microsoft.com;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7cc234202e98d2722580858573fd0817-Abstract.html",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Microsoft Research;Massachusetts Institute of Technology",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-new-england;https://web.mit.edu",
        "aff_unique_abbr": "MSR NE;MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New England;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Time-dependent spatially varying graphical models, with application to brain fMRI data analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9356",
        "id": "9356",
        "author_site": "Kristjan Greenewald, Seyoung Park, Shuheng Zhou, Alexander Giessing",
        "author": "Kristjan Greenewald; Seyoung Park; Shuheng Zhou; Alexander Giessing",
        "abstract": "In this work, we present an additive model for space-time data that splits the data into a temporally correlated component and a spatially correlated component. We model the spatially correlated portion using a time-varying Gaussian graphical model. Under assumptions on the smoothness of changes in covariance matrices,  we derive strong single sample convergence results, confirming our ability to estimate meaningful graphical structures as they evolve over time. We apply our methodology to the discovery of time-varying spatial structures in human brain fMRI signals.",
        "bibtex": "@inproceedings{NIPS2017_769675d7,\n author = {Greenewald, Kristjan and Park, Seyoung and Zhou, Shuheng and Giessing, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Time-dependent spatially varying graphical models, with application to brain fMRI data analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/769675d7c11f336ae6573e7e533570ec-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/769675d7c11f336ae6573e7e533570ec-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/769675d7c11f336ae6573e7e533570ec-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/769675d7c11f336ae6573e7e533570ec-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/769675d7c11f336ae6573e7e533570ec-Reviews.html",
        "metareview": "",
        "pdf_size": 393359,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4969282219463036977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/769675d7c11f336ae6573e7e533570ec-Abstract.html"
    },
    {
        "title": "Tomography of the London Underground: a Scalable Model for Origin-Destination Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9091",
        "id": "9091",
        "author_site": "Nicol\u00f2 Colombo, Ricardo Silva, Soong Moon Kang",
        "author": "Nicol\u00f2 Colombo; Ricardo Silva; Soong Moon Kang",
        "abstract": "The paper addresses the classical network tomography problem of inferring local traffic given origin-destination observations. Focussing on large complex public transportation systems, we build a scalable model that exploits input-output information to estimate the unobserved link/station loads and the users path preferences. Based on the reconstruction of the users' travel time distribution, the model is flexible enough to capture possible different path-choice strategies and correlations between users travelling on similar paths at similar times. The corresponding likelihood function is intractable for medium or large-scale networks and we propose two distinct strategies, namely the exact maximum-likelihood inference of an approximate but tractable model and the variational inference of the original intractable model. As an application of our approach, we consider the emblematic case of the London Underground network, where a tap-in/tap-out system tracks the start/exit time and location of all journeys in a day. A set of synthetic simulations and real data provided by Transport For London are used to validate and test the model on the predictions of observable and unobservable quantities.",
        "bibtex": "@inproceedings{NIPS2017_b3b43aee,\n author = {Colombo, Nicol\\`{o} and Silva, Ricardo and Kang, Soong Moon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tomography of the London Underground: a Scalable Model for Origin-Destination Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b3b43aeeacb258365cc69cdaf42a68af-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b3b43aeeacb258365cc69cdaf42a68af-Reviews.html",
        "metareview": "",
        "pdf_size": 1311656,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1391606713518898765&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistical Science, University College London; The Alan Turing Institute + Department of Statistical Science, University College London; School of Management, University College London",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "email": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html",
        "aff_unique_index": "0;1+0;2",
        "aff_unique_norm": "University College London;The Alan Turing Institute;School of Management, University College London",
        "aff_unique_dep": "Department of Statistical Science;;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.turing.ac.uk;",
        "aff_unique_abbr": "UCL;ATI;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "title": "Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9042",
        "id": "9042",
        "author_site": "Chengxu Zhuang, Jonas Kubilius, Mitra JZ Hartmann, Daniel Yamins",
        "author": "Chengxu Zhuang; Jonas Kubilius; Mitra JZ Hartmann; Daniel L Yamins",
        "abstract": "In large part, rodents \u201csee\u201d the world through their whiskers, a powerful tactile sense enabled by a series of brain areas that form the whisker-trigeminal system. Raw sensory data arrives in the form of mechanical input to the exquisitely sensitive, actively-controllable whisker array, and is processed through a sequence of neural circuits, eventually arriving in cortical regions that communicate with decision making and memory areas. Although a long history of experimental studies has characterized many aspects of these processing stages, the computational operations of the whisker-trigeminal system remain largely unknown. In the present work, we take a goal-driven deep neural network (DNN) approach to modeling these computations. First, we construct a biophysically-realistic model of the rat whisker array. We then generate a large dataset of whisker sweeps across a wide variety of 3D objects in highly-varying poses, angles, and speeds. Next, we train DNNs from several distinct architectural families to solve a shape recognition task in this dataset. Each architectural family represents a structurally-distinct hypothesis for processing in the whisker-trigeminal system, corresponding to different ways in which spatial and temporal information can be integrated. We find that most networks perform poorly on the challenging shape recognition task, but that specific architectures from several families can achieve reasonable performance levels. Finally, we show that Representational Dissimilarity Matrices (RDMs), a tool for comparing population codes between neural systems, can separate these higher performing networks with data of a type that could plausibly be collected in a neurophysiological or imaging experiment. Our results are a proof-of-concept that DNN models of the whisker-trigeminal system are potentially within reach.",
        "bibtex": "@inproceedings{NIPS2017_ab541d87,\n author = {Zhuang, Chengxu and Kubilius, Jonas and Hartmann, Mitra JZ and Yamins, Daniel L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ab541d874c7bc19ab77642849e02b89f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ab541d874c7bc19ab77642849e02b89f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ab541d874c7bc19ab77642849e02b89f-Reviews.html",
        "metareview": "",
        "pdf_size": 8468296,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4407957713684452231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology, Stanford University; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology + Brain and Cognition, KU Leuven; Departments of Biomedical Engineering and Mechanical Engineering, Northwestern University; Departments of Psychology and Computer Science, Stanford Neurosciences Institute, Stanford University",
        "aff_domain": "stanford.edu;mit.edu;northwestern.edu;stanford.edu",
        "email": "stanford.edu;mit.edu;northwestern.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ab541d874c7bc19ab77642849e02b89f-Abstract.html",
        "aff_unique_index": "0;1+2;3;4",
        "aff_unique_norm": "Stanford University;Massachusetts Institute of Technology;Brain and Cognition, KU Leuven;Departments of Biomedical Engineering and Mechanical Engineering, Northwestern University;Departments of Psychology and Computer Science, Stanford Neurosciences Institute, Stanford University",
        "aff_unique_dep": "Department of Psychology;Department of Brain and Cognitive Sciences;;;",
        "aff_unique_url": "https://www.stanford.edu;https://web.mit.edu;;;",
        "aff_unique_abbr": "Stanford;MIT;;;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Stanford;Cambridge;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Toward Multimodal Image-to-Image Translation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8843",
        "id": "8843",
        "author_site": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei Efros, Oliver Wang, Eli Shechtman",
        "author": "Jun-Yan Zhu; Richard Zhang; Deepak Pathak; Trevor Darrell; Alexei A Efros; Oliver Wang; Eli Shechtman",
        "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
        "bibtex": "@inproceedings{NIPS2017_819f46e5,\n author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Toward Multimodal Image-to-Image Translation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/819f46e52c25763a55cc642422644317-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/819f46e52c25763a55cc642422644317-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/819f46e52c25763a55cc642422644317-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/819f46e52c25763a55cc642422644317-Reviews.html",
        "metareview": "",
        "pdf_size": 3298747,
        "gs_citation": 2040,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4719212061533508568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; Adobe Research; Adobe Research",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/819f46e52c25763a55cc642422644317-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;1;1",
        "aff_unique_norm": "University of California, Berkeley;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.berkeley.edu;https://research.adobe.com",
        "aff_unique_abbr": "UC Berkeley;Adobe",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9334",
        "id": "9334",
        "author": "Arash Vahdat",
        "abstract": "Collecting large training datasets, annotated with high-quality labels, is costly and time-consuming. This paper proposes a novel framework for training deep convolutional neural networks from noisy labeled datasets that can be obtained cheaply. The problem is formulated using an undirected graphical model that represents the relationship between noisy and clean labels, trained in a semi-supervised setting. In our formulation, the inference over latent clean labels is tractable and is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.",
        "bibtex": "@inproceedings{NIPS2017_e6af401c,\n author = {Vahdat, Arash},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/e6af401c28c1790eaef7d55c92ab6ab6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/e6af401c28c1790eaef7d55c92ab6ab6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/e6af401c28c1790eaef7d55c92ab6ab6-Reviews.html",
        "metareview": "",
        "pdf_size": 347543,
        "gs_citation": 373,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10407058703831739029&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "D-Wave Systems Inc.",
        "aff_domain": "dwavesys.com",
        "email": "dwavesys.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "D-Wave Systems Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.dwavesys.com",
        "aff_unique_abbr": "D-Wave",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Towards Accurate Binary Convolutional Neural Network",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8831",
        "id": "8831",
        "author_site": "Xiaofan Lin, Cong Zhao, Wei Pan",
        "author": "Xiaofan Lin; Cong Zhao; Wei Pan",
        "abstract": "We introduce a novel scheme to train binary convolutional neural networks (CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.",
        "bibtex": "@inproceedings{NIPS2017_b1a59b31,\n author = {Lin, Xiaofan and Zhao, Cong and Pan, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Towards Accurate Binary Convolutional Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/b1a59b315fc9a3002ce38bbe070ec3f5-Reviews.html",
        "metareview": "",
        "pdf_size": 530915,
        "gs_citation": 859,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9345699458847974423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "DJI Innovations Inc, Shenzhen, China; DJI Innovations Inc, Shenzhen, China; DJI Innovations Inc, Shenzhen, China",
        "aff_domain": "dji.com;dji.com;dji.com",
        "email": "dji.com;dji.com;dji.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/b1a59b315fc9a3002ce38bbe070ec3f5-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "DJI Innovations Inc, Shenzhen, China",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Towards Generalization and Simplicity in Continuous Control",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9424",
        "id": "9424",
        "author_site": "Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, Sham Kakade",
        "author": "Aravind Rajeswaran; Kendall Lowrey; Emanuel V. Todorov; Sham M. Kakade",
        "abstract": "The remarkable successes of deep learning in speech recognition and computer vision have motivated efforts to adapt similar techniques to other problem domains, including reinforcement learning (RL). Consequently, RL methods have produced rich motor behaviors on simulated robot tasks, with their success largely attributed to the use of multi-layer neural networks. This work is among the first to carefully study what might be responsible for these recent advancements. Our main result calls this emerging narrative into question by showing that much simpler architectures -- based on linear and RBF parameterizations  -- achieve comparable performance to state of the art results.  We not only study different policy representations with regard to performance measures at hand, but also towards robustness to external perturbations. We again find that the learned neural network policies --- under the standard training scenarios ---  are no more robust than linear (or RBF) policies; in fact, all three are remarkably brittle. Finally, we then directly modify the training scenarios in order to favor more robust policies, and we again do not find a compelling case to favor multi-layer architectures.  Overall, this study suggests that multi-layer architectures should not be the default choice, unless a side-by-side comparison to simpler architectures shows otherwise. More generally, we hope that these results lead to more interest in carefully studying the architectural choices, and associated trade-offs, for training generalizable and robust policies.",
        "bibtex": "@inproceedings{NIPS2017_9ddb9dd5,\n author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel V. and Kakade, Sham M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Towards Generalization and Simplicity in Continuous Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Reviews.html",
        "metareview": "",
        "pdf_size": 816299,
        "gs_citation": 370,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7854491148327945577&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "University of Washington Seattle; University of Washington Seattle; University of Washington Seattle; University of Washington Seattle",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "https://sites.google.com/view/simple-pol",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9ddb9dd5d8aee9a76bf217a2a3c54833-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Tractability in Structured Probability Spaces",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9131",
        "id": "9131",
        "author_site": "Arthur Choi, Yujia Shen, Adnan Darwiche",
        "author": "Arthur Choi; Yujia Shen; Adnan Darwiche",
        "abstract": "Recently, the Probabilistic Sentential Decision Diagram (PSDD) has been proposed as a framework for systematically inducing and learning distributions over structured objects, including combinatorial objects such as permutations and rankings, paths and matchings on a graph, etc. In this paper, we study the scalability of such models in the context of representing and learning distributions over routes on a map. In particular, we introduce the notion of a hierarchical route distribution and show how they can be leveraged to construct tractable PSDDs over route distributions, allowing them to scale to larger maps. We illustrate the utility of our model empirically, in a route prediction task, showing how accuracy can be increased significantly compared to Markov models.",
        "bibtex": "@inproceedings{NIPS2017_deb54ffb,\n author = {Choi, Arthur and Shen, Yujia and Darwiche, Adnan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tractability in Structured Probability Spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/deb54ffb41e085fd7f69a75b6359c989-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/deb54ffb41e085fd7f69a75b6359c989-Reviews.html",
        "metareview": "",
        "pdf_size": 561219,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9336877379971101222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Los Angeles, CA 90095; University of California, Los Angeles, CA 90095; University of California, Los Angeles, CA 90095",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/deb54ffb41e085fd7f69a75b6359c989-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8963",
        "id": "8963",
        "author_site": "Elad Hoffer, Itay Hubara, Daniel Soudry",
        "author": "Elad Hoffer; Itay Hubara; Daniel Soudry",
        "abstract": "Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance -  known as the \"generalization gap\" phenomenon. Identifying the origin of this gap and closing it had remained an open problem.  Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a \"random walk on a random landscape\" statistical model which is known to exhibit similar \"ultra-slow\" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the \"generalization gap\" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named \"Ghost Batch Normalization\" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.",
        "bibtex": "@inproceedings{NIPS2017_a5e0ff62,\n author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Reviews.html",
        "metareview": "",
        "pdf_size": 710946,
        "gs_citation": 1055,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11429742229244735024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Technion - Israel Institute of Technology, Haifa, Israel; Technion - Israel Institute of Technology, Haifa, Israel; Technion - Israel Institute of Technology, Haifa, Israel",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/a5e0ff62be0b08456fc7f1e88812af3d-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Training Deep Networks without Learning Rates Through Coin Betting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9004",
        "id": "9004",
        "author_site": "Francesco Orabona, Tatiana Tommasi",
        "author": "Francesco Orabona; Tatiana Tommasi",
        "abstract": "Deep learning methods achieve state-of-the-art performance in many application scenarios.  Yet, these methods require a significant amount of hyperparameters tuning in order to achieve  the best results. In particular, tuning the learning rates in the stochastic optimization  process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that  does not require any learning rate setting. Contrary to previous methods, we do not adapt the  learning rates nor we make use of the assumed curvature of the objective function. Instead,  we reduce the optimization process to a game of betting on a coin and propose a learning rate free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows  the advantage of our algorithm over popular stochastic gradient algorithms.",
        "bibtex": "@inproceedings{NIPS2017_7c82fab8,\n author = {Orabona, Francesco and Tommasi, Tatiana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Training Deep Networks without Learning Rates Through Coin Betting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7c82fab8c8f89124e2ce92984e04fb40-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/7c82fab8c8f89124e2ce92984e04fb40-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/7c82fab8c8f89124e2ce92984e04fb40-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/7c82fab8c8f89124e2ce92984e04fb40-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/7c82fab8c8f89124e2ce92984e04fb40-Reviews.html",
        "metareview": "",
        "pdf_size": 664686,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13198884174909111726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Stony Brook University, Stony Brook, NY; Department of Computer, Control, and Management Engineering, Sapienza, Rome University, Italy",
        "aff_domain": "orabona.com;dis.uniroma1.it",
        "email": "orabona.com;dis.uniroma1.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/7c82fab8c8f89124e2ce92984e04fb40-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science, Stony Brook University, Stony Brook, NY;Department of Computer, Control, and Management Engineering, Sapienza, Rome University, Italy",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Training Quantized Nets: A Deeper Understanding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9354",
        "id": "9354",
        "author_site": "Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein",
        "author": "Hao Li; Soham De; Zheng Xu; Christoph Studer; Hanan Samet; Tom Goldstein",
        "abstract": "Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption.  Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint.  We first explore accuracy guarantees for training methods under convexity assumptions.  We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.",
        "bibtex": "@inproceedings{NIPS2017_1c303b0e,\n author = {Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Training Quantized Nets: A Deeper Understanding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Reviews.html",
        "metareview": "",
        "pdf_size": 1341182,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16361204502930415283&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Maryland, College Park; Department of Computer Science, University of Maryland, College Park; Department of Computer Science, University of Maryland, College Park; School of Electrical and Computer Engineering, Cornell University; Department of Computer Science, University of Maryland, College Park; Department of Computer Science, University of Maryland, College Park",
        "aff_domain": "cs.umd.edu;cs.umd.edu;cs.umd.edu;cornell.edu;cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu;cs.umd.edu;cornell.edu;cs.umd.edu;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1c303b0eed3133200cf715285011b4e4-Abstract.html",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Maryland, College Park;Cornell University",
        "aff_unique_dep": "Department of Computer Science;School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.umd.edu;https://www.cornell.edu",
        "aff_unique_abbr": "UMD;Cornell",
        "aff_campus_unique_index": "0;0;0;1;0;0",
        "aff_campus_unique": "College Park;Ithaca",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9230",
        "id": "9230",
        "author_site": "Ingmar Kanitscheider, Ila Fiete",
        "author": "Ingmar Kanitscheider; Ila Fiete",
        "abstract": "Self-localization during navigation with noisy sensors in an ambiguous world is computationally challenging, yet animals and humans excel at it. In robotics, {\\em Simultaneous Location and Mapping} (SLAM) algorithms solve this problem through joint sequential probabilistic inference of their own coordinates and those of external spatial landmarks. We generate the first neural solution to the SLAM problem by training recurrent LSTM networks to perform a set of hard 2D navigation tasks that require generalization to completely novel trajectories and environments. Our goal is to make sense of how the diverse phenomenology in the brain's spatial navigation circuits is related to their function. We show that the hidden unit representations exhibit several key properties of hippocampal place cells, including stable tuning curves that remap between environments. Our result is also a proof of concept for end-to-end-learning of a SLAM algorithm using recurrent networks, and a demonstration of why this approach may have some advantages for robotic SLAM.",
        "bibtex": "@inproceedings{NIPS2017_5f146156,\n author = {Kanitscheider, Ingmar and Fiete, Ila},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5f14615696649541a025d3d0f8e0447f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5f14615696649541a025d3d0f8e0447f-Reviews.html",
        "metareview": "",
        "pdf_size": 8038123,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16580922756101546373&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Neuroscience, The University of Texas, Austin, TX 78712; Department of Neuroscience, The University of Texas, Austin, TX 78712",
        "aff_domain": "mail.clm.utexas.edu;mail.clm.utexas.edu",
        "email": "mail.clm.utexas.edu;mail.clm.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5f14615696649541a025d3d0f8e0447f-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Neuroscience, The University of Texas, Austin, TX 78712",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Translation Synchronization via Truncated Least Squares",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8937",
        "id": "8937",
        "author_site": "Xiangru Huang, Zhenxiao Liang, Chandrajit Bajaj, Qixing Huang",
        "author": "Xiangru Huang; Zhenxiao Liang; Chandrajit Bajaj; Qixing Huang",
        "abstract": "In this paper, we introduce a robust algorithm, \\textsl{TranSync}, for the 1D translation synchronization problem, in which the aim is to recover the global coordinates of a set of nodes from noisy measurements of relative coordinates along an observation graph. The basic idea of TranSync is to apply truncated least squares, where the solution at each step is used to gradually prune out noisy measurements. We analyze TranSync under both deterministic and randomized  noisy models, demonstrating its robustness and stability. Experimental results on synthetic and real datasets show that TranSync is superior to state-of-the-art convex formulations in terms of both efficiency and accuracy.",
        "bibtex": "@inproceedings{NIPS2017_9f53d83e,\n author = {Huang, Xiangru and Liang, Zhenxiao and Bajaj, Chandrajit and Huang, Qixing},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Translation Synchronization via Truncated Least Squares},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/9f53d83ec0691550f7d2507d57f4f5a2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/9f53d83ec0691550f7d2507d57f4f5a2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/9f53d83ec0691550f7d2507d57f4f5a2-Reviews.html",
        "metareview": "",
        "pdf_size": 702025,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1647886820299219158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The University of Texas at Austin; Tsinghua University; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;mails.tsinghua.edu.cn;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;mails.tsinghua.edu.cn;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Texas at Austin;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UT Austin;THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Triangle Generative Adversarial Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9300",
        "id": "9300",
        "author_site": "Zhe Gan, Liqun Chen, Weiyao Wang, Yuchen Pu, Yizhe Zhang, Hao Liu, Chunyuan Li, Lawrence Carin",
        "author": "Zhe Gan; Liqun Chen; Weiyao Wang; Yuchen Pu; Yizhe Zhang; Hao Liu; Chunyuan Li; Lawrence Carin",
        "abstract": "A Triangle Generative Adversarial Network ($\\Delta$-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. $\\Delta$-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution.  In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.",
        "bibtex": "@inproceedings{NIPS2017_bbeb0c1b,\n author = {Gan, Zhe and Chen, Liqun and Wang, Weiyao and Pu, Yuchen and Zhang, Yizhe and Liu, Hao and Li, Chunyuan and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Triangle Generative Adversarial Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/bbeb0c1b1fd44e392c7ce2fdbd137e87-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/bbeb0c1b1fd44e392c7ce2fdbd137e87-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/bbeb0c1b1fd44e392c7ce2fdbd137e87-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/bbeb0c1b1fd44e392c7ce2fdbd137e87-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/bbeb0c1b1fd44e392c7ce2fdbd137e87-Reviews.html",
        "metareview": "",
        "pdf_size": 5375027,
        "gs_citation": 168,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12657863605119984031&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Duke University; Duke University; Duke University; Duke University; Duke University; Duke University; Duke University; Duke University",
        "aff_domain": "duke.edu; ; ; ; ; ; ; ",
        "email": "duke.edu; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/bbeb0c1b1fd44e392c7ce2fdbd137e87-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Trimmed Density Ratio Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9229",
        "id": "9229",
        "author_site": "Song Liu, Akiko Takeda, Taiji Suzuki, Kenji Fukumizu",
        "author": "Song Liu; Akiko Takeda; Taiji Suzuki; Kenji Fukumizu",
        "abstract": "Density ratio estimation is a vital tool in both machine learning and statistical community. However, due to the unbounded nature of density ratio, the estimation proceudre can be vulnerable to corrupted data points, which often pushes the estimated ratio toward infinity. In this paper, we present a robust estimator which automatically identifies and trims outliers. The proposed estimator has a convex formulation, and the global optimum can be obtained via subgradient descent. We analyze the parameter estimation error of this estimator under high-dimensional settings. Experiments are conducted to verify the effectiveness of the estimator.",
        "bibtex": "@inproceedings{NIPS2017_ea204361,\n author = {Liu, Song and Takeda, Akiko and Suzuki, Taiji and Fukumizu, Kenji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Trimmed Density Ratio Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ea204361fe7f024b130143eb3e189a18-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ea204361fe7f024b130143eb3e189a18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ea204361fe7f024b130143eb3e189a18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ea204361fe7f024b130143eb3e189a18-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ea204361fe7f024b130143eb3e189a18-Reviews.html",
        "metareview": "",
        "pdf_size": 3063258,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=44657049516755576&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Bristol + The Institute of Statistical Mathematics; The Institute of Statistical Mathematics + AIP, RIKEN; University of Tokyo + Sakigake (PRESTO), JST + AIP, RIKEN; The Institute of Statistical Mathematics",
        "aff_domain": "bristol.ac.uk;ism.ac.jp;mist.i.u-tokyo.ac.jp;ism.ac.jp",
        "email": "bristol.ac.uk;ism.ac.jp;mist.i.u-tokyo.ac.jp;ism.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ea204361fe7f024b130143eb3e189a18-Abstract.html",
        "aff_unique_index": "0+1;1+2;3+4+2;1",
        "aff_unique_norm": "University of Bristol;The Institute of Statistical Mathematics;AIP, RIKEN;University of Tokyo;Sakigake (PRESTO), JST",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.bristol.ac.uk;https://www.ism.ac.jp;;https://www.u-tokyo.ac.jp;",
        "aff_unique_abbr": "Bristol;ISM;;UTokyo;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1",
        "aff_country_unique": "United Kingdom;Japan;"
    },
    {
        "title": "Triple Generative Adversarial Nets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9189",
        "id": "9189",
        "author_site": "Chongxuan LI, Kun Xu, Jun Zhu, Bo Zhang",
        "author": "Chongxuan LI; Taufik Xu; Jun Zhu; Bo Zhang",
        "abstract": "Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.",
        "bibtex": "@inproceedings{NIPS2017_86e78499,\n author = {LI, Chongxuan and Xu, Taufik and Zhu, Jun and Zhang, Bo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Triple Generative Adversarial Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/86e78499eeb33fb9cac16b7555b50767-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/86e78499eeb33fb9cac16b7555b50767-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/86e78499eeb33fb9cac16b7555b50767-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/86e78499eeb33fb9cac16b7555b50767-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/86e78499eeb33fb9cac16b7555b50767-Reviews.html",
        "metareview": "",
        "pdf_size": 4070151,
        "gs_citation": 581,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=147373412659881126&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Center for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Center for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Center for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Center for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/86e78499eeb33fb9cac16b7555b50767-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Center for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Unbiased estimates for linear regression via volume sampling",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9093",
        "id": "9093",
        "author_site": "Michal Derezinski, Manfred K. Warmuth",
        "author": "Michal Derezinski; Manfred K. Warmuth",
        "abstract": "Given a full rank matrix X with more columns than rows consider the task of estimating the pseudo inverse $X^+$ based  on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\\top}$.  Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of $X$. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from $X$. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all  column labels.   We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.",
        "bibtex": "@inproceedings{NIPS2017_54e36c5f,\n author = {Derezinski, Michal and Warmuth, Manfred K. K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unbiased estimates for linear regression via volume sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/54e36c5ff5f6a1802925ca009f3ebb68-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/54e36c5ff5f6a1802925ca009f3ebb68-Reviews.html",
        "metareview": "",
        "pdf_size": 371517,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3059912764119815196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of California Santa Cruz; Department of Computer Science, University of California Santa Cruz",
        "aff_domain": "ucsc.edu;ucsc.edu",
        "email": "ucsc.edu;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, University of California Santa Cruz",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Unbounded cache model for online language modeling with open vocabulary",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9376",
        "id": "9376",
        "author_site": "Edouard Grave, Moustapha Cisse, Armand Joulin",
        "author": "Edouard Grave; Moustapha M Cisse; Armand Joulin",
        "abstract": "Recently, continuous cache models were proposed as extensions to recurrent neural network language models, to adapt their predictions to local changes in the data distribution. These models only capture the local context, of up to a few thousands tokens. In this paper, we propose an extension of continuous cache models, which can scale to larger contexts. In particular, we use a large scale non-parametric memory component that stores all the hidden activations seen in the past. We leverage recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. We conduct extensive experiments showing that our approach significantly improves the perplexity of pre-trained language models on new distributions, and can scale efficiently to much larger contexts than previously proposed local cache models.",
        "bibtex": "@inproceedings{NIPS2017_f44ee263,\n author = {Grave, Edouard and Cisse, Moustapha M and Joulin, Armand},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unbounded cache model for online language modeling with open vocabulary},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f44ee263952e65b3610b8ba51229d1f9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f44ee263952e65b3610b8ba51229d1f9-Reviews.html",
        "metareview": "",
        "pdf_size": 304990,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13060058683014636439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f44ee263952e65b3610b8ba51229d1f9-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Facebook",
        "aff_unique_dep": "Facebook AI Research",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9213",
        "id": "9213",
        "author_site": "Cesar F Caiafa, Olaf Sporns, Andrew Saykin, Franco Pestilli",
        "author": "Cesar F. Caiafa; Olaf Sporns; Andrew Saykin; Franco Pestilli",
        "abstract": "Recently, linear formulations and convex optimization methods have been proposed to predict diffusion-weighted Magnetic Resonance Imaging (dMRI) data given estimates of brain connections generated using tractography algorithms. The size of the linear models comprising such methods grows with both dMRI data and connectome resolution, and can become very large when applied to modern data. In this paper, we introduce a method to encode dMRI signals and large connectomes, i.e., those that range from hundreds of thousands to millions of fascicles (bundles of neuronal axons), by using a sparse tensor decomposition. We show that this tensor decomposition accurately approximates the Linear Fascicle Evaluation (LiFE) model, one of the recently developed linear models. We provide a theoretical analysis of the accuracy of the sparse decomposed model, LiFESD, and demonstrate that it can reduce the size of the model significantly. Also, we develop algorithms to implement the optimisation solver using the tensor representation in an efficient way.",
        "bibtex": "@inproceedings{NIPS2017_ccbd8ca9,\n author = {Caiafa, Cesar F and Sporns, Olaf and Saykin, Andrew and Pestilli, Franco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ccbd8ca962b80445df1f7f38c57759f0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ccbd8ca962b80445df1f7f38c57759f0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ccbd8ca962b80445df1f7f38c57759f0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ccbd8ca962b80445df1f7f38c57759f0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ccbd8ca962b80445df1f7f38c57759f0-Reviews.html",
        "metareview": "",
        "pdf_size": 6252923,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=843550150927025509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Psychological and Brain Sciences, Indiana University (47405) Bloomington, IN, USA + IAR - CCT La Plata, CONICET / CIC-PBA (1894) V. Elisa, ARGENTINA; Department of Psychological and Brain Sciences, Indiana University (47405) Bloomington, IN, USA; Department of Radiology - Indiana University School of Medicine. (46202) Indianapolis, IN, USA; Department of Psychological and Brain Sciences, Indiana University (47405) Bloomington, IN, USA",
        "aff_domain": "gmail.com;indiana.edu;iupui.edu;indiana.edu",
        "email": "gmail.com;indiana.edu;iupui.edu;indiana.edu",
        "github": "",
        "project": "http://web.fi.uba.ar/~ccaiafa/Cesar.html; http://www.brain-life.org/plab/",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ccbd8ca962b80445df1f7f38c57759f0-Abstract.html",
        "aff_unique_index": "0+1;0;2;0",
        "aff_unique_norm": "Department of Psychological and Brain Sciences, Indiana University (47405) Bloomington, IN, USA;IAR - CCT La Plata, CONICET / CIC-PBA (1894) V. Elisa, ARGENTINA;Department of Radiology - Indiana University School of Medicine. (46202) Indianapolis, IN, USA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9345",
        "id": "9345",
        "author_site": "Christoph Dann, Tor Lattimore, Emma Brunskill",
        "author": "Christoph Dann; Tor Lattimore; Emma Brunskill",
        "abstract": "Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.",
        "bibtex": "@inproceedings{NIPS2017_17d8da81,\n author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/17d8da815fa21c57af9829fb0a869602-Reviews.html",
        "metareview": "",
        "pdf_size": 740662,
        "gs_citation": 357,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1992654125570741957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Machine Learning Department, Carnegie-Mellon University; DeepMind, London; Computer Science Department, Stanford University",
        "aff_domain": "cdann.net;gmail.com;cs.stanford.edu",
        "email": "cdann.net;gmail.com;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/17d8da815fa21c57af9829fb0a869602-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Machine Learning Department, Carnegie-Mellon University;DeepMind;Stanford University",
        "aff_unique_dep": ";;Computer Science Department",
        "aff_unique_url": ";https://deepmind.com;https://www.stanford.edu",
        "aff_unique_abbr": ";DeepMind;Stanford",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";London;Stanford",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";United Kingdom;United States"
    },
    {
        "title": "Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8901",
        "id": "8901",
        "author_site": "Kristofer Bouchard, Alejandro Bujan, Farbod Roosta-Khorasani, Shashanka Ubaru, Mr. Prabhat, Antoine Snijders, Jian-Hua Mao, Edward Chang, Michael W Mahoney, Sharmodeep Bhattacharya",
        "author": "Kristofer Bouchard; Alejandro Bujan; Fred Roosta; Shashanka Ubaru; Mr. Prabhat; Antoine Snijders; Jian-Hua Mao; Edward Chang; Michael W. Mahoney; Sharmodeep Bhattacharya",
        "abstract": "The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications, e.g., neuroscience, genetics, systems biology, etc. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce the Union of Intersections (UoI) method, a flexible, modular, and scalable framework for enhanced model selection and estimation. The method performs model selection and model estimation through intersection and union operations, respectively. We show that UoI can satisfy the bi-criteria of low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm ($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as the accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the $UoI_{L1Logistic}$ and $UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields.",
        "bibtex": "@inproceedings{NIPS2017_788d9869,\n author = {Bouchard, Kristofer and Bujan, Alejandro and Roosta, Fred and Ubaru, Shashanka and Prabhat, Mr. and Snijders, Antoine and Mao, Jian-Hua and Chang, Edward and Mahoney, Michael W and Bhattacharya, Sharmodeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/788d986905533aba051261497ecffcbb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/788d986905533aba051261497ecffcbb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/788d986905533aba051261497ecffcbb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/788d986905533aba051261497ecffcbb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/788d986905533aba051261497ecffcbb-Reviews.html",
        "metareview": "",
        "pdf_size": 3200040,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5929793565112525450&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;;;;;;;",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 10,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/788d986905533aba051261497ecffcbb-Abstract.html"
    },
    {
        "title": "Universal Style Transfer via Feature Transforms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8835",
        "id": "8835",
        "author_site": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang",
        "author": "Yijun Li; Chen Fang; Jimei Yang; Zhaowen Wang; Xin Lu; Ming-Hsuan Yang",
        "abstract": "Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures by simple feature coloring.",
        "bibtex": "@inproceedings{NIPS2017_49182f81,\n author = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Universal Style Transfer via Feature Transforms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/49182f81e6a13cf5eaa496d51fea6406-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/49182f81e6a13cf5eaa496d51fea6406-Reviews.html",
        "metareview": "",
        "pdf_size": 12877833,
        "gs_citation": 1242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7001062204457348357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "UC Merced; Adobe Research; Adobe Research; Adobe Research; Adobe Research; UC Merced + NVIDIA Research",
        "aff_domain": "ucmerced.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucmerced.edu",
        "email": "ucmerced.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucmerced.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html",
        "aff_unique_index": "0;1;1;1;1;0+2",
        "aff_unique_norm": "University of California, Merced;Adobe;NVIDIA Corporation",
        "aff_unique_dep": ";Adobe Research;NVIDIA Research",
        "aff_unique_url": "https://www.ucmerced.edu;https://research.adobe.com;https://www.nvidia.com/research",
        "aff_unique_abbr": "UCM;Adobe;NVIDIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Universal consistency and minimax rates for online Mondrian Forests",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9158",
        "id": "9158",
        "author_site": "Jaouad Mourtada, St\u00e9phane Ga\u00efffas, Erwan Scornet",
        "author": "Jaouad Mourtada; St\u00e9phane Ga\u00efffas; Erwan Scornet",
        "abstract": "We establish the consistency of an algorithm of Mondrian Forests~\\cite{lakshminarayanan2014mondrianforests,lakshminarayanan2016mondrianuncertainty}, a randomized classification algorithm that can be implemented online.  First, we amend the original Mondrian Forest algorithm proposed in~\\cite{lakshminarayanan2014mondrianforests}, that considers a \\emph{fixed} lifetime parameter.  Indeed, the fact that this parameter is fixed actually hinders statistical consistency of the original procedure.  Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters $\\lambda_n$, and uses an alternative updating rule, allowing to work also in an online fashion.   Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results~\\cite{arlot2014purf_bias} to an \\emph{arbitrary dimension}.",
        "bibtex": "@inproceedings{NIPS2017_f80ff32e,\n author = {Mourtada, Jaouad and Ga\\\"{\\i}ffas, St\\'{e}phane and Scornet, Erwan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Universal consistency and minimax rates for online Mondrian Forests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Reviews.html",
        "metareview": "",
        "pdf_size": 445775,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18166920663638409906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Centre de Math\u00e9matiques Appliqu\u00e9es, \u00c9cole Polytechnique, Palaiseau, France; Centre de Math\u00e9matiques Appliqu\u00e9es, \u00c9cole Polytechnique, Palaiseau, France; Centre de Math\u00e9matiques Appliqu\u00e9es, \u00c9cole Polytechnique, Palaiseau, France",
        "aff_domain": "polytechnique.edu;polytechnique.edu;polytechnique.edu",
        "email": "polytechnique.edu;polytechnique.edu;polytechnique.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f80ff32e08a25270b5f252ce39522f72-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Centre de Math\u00e9matiques Appliqu\u00e9es, \u00c9cole Polytechnique, Palaiseau, France",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Unsupervised Image-to-Image Translation Networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8865",
        "id": "8865",
        "author_site": "Ming-Yu Liu, Thomas Breuel, Jan Kautz",
        "author": "Ming-Yu Liu; Thomas Breuel; Jan Kautz",
        "abstract": "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.",
        "bibtex": "@inproceedings{NIPS2017_dc6a6489,\n author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Image-to-Image Translation Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Reviews.html",
        "metareview": "",
        "pdf_size": 7433433,
        "gs_citation": 3485,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14169741715291172305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "NVIDIA; NVIDIA; NVIDIA",
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com",
        "email": "nvidia.com;nvidia.com;nvidia.com",
        "github": "https://github.com/mingyuliutw/unit",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/dc6a6489640ca02b0d42dabeb8e46bb7-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NVIDIA Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nvidia.com",
        "aff_unique_abbr": "NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Learning of Disentangled Representations from Video",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9477",
        "id": "9477",
        "author_site": "Emily Denton, vighnesh Birodkar",
        "author": "Emily L Denton; vighnesh Birodkar",
        "abstract": "We present a new model DRNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying  component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluating our approach on a range of synthetic and real videos. For the latter, we demonstrate the ability to coherently generate up to several hundred steps into the future.",
        "bibtex": "@inproceedings{NIPS2017_2d2ca7ee,\n author = {Denton, Emily L and Birodkar, vighnesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Learning of Disentangled Representations from Video},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2d2ca7eedf739ef4c3800713ec482e1a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2d2ca7eedf739ef4c3800713ec482e1a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2d2ca7eedf739ef4c3800713ec482e1a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2d2ca7eedf739ef4c3800713ec482e1a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2d2ca7eedf739ef4c3800713ec482e1a-Reviews.html",
        "metareview": "",
        "pdf_size": 17299348,
        "gs_citation": 869,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9768655920777432327&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, New York University; Department of Computer Science, New York University",
        "aff_domain": "cs.nyu.edu;nyu.edu",
        "email": "cs.nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2d2ca7eedf739ef4c3800713ec482e1a-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8977",
        "id": "8977",
        "author_site": "Wei-Ning Hsu, Yu Zhang, James Glass",
        "author": "Wei-Ning Hsu; Yu Zhang; James Glass",
        "abstract": "We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.",
        "bibtex": "@inproceedings{NIPS2017_0a0a0c8a,\n author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Reviews.html",
        "metareview": "",
        "pdf_size": 3415002,
        "gs_citation": 442,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8296656324579565267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html"
    },
    {
        "title": "Unsupervised Sequence Classification using Sequential Output Statistics",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9138",
        "id": "9138",
        "author_site": "Yu Liu, Jianshu Chen, Li Deng",
        "author": "Yu Liu; Jianshu Chen; Li Deng",
        "abstract": "We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.",
        "bibtex": "@inproceedings{NIPS2017_f1981e4b,\n author = {Liu, Yu and Chen, Jianshu and Deng, Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Sequence Classification using Sequential Output Statistics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f1981e4bd8a0d6d8462016d2fc6276b3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f1981e4bd8a0d6d8462016d2fc6276b3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f1981e4bd8a0d6d8462016d2fc6276b3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f1981e4bd8a0d6d8462016d2fc6276b3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f1981e4bd8a0d6d8462016d2fc6276b3-Reviews.html",
        "metareview": "",
        "pdf_size": 1636148,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3662081628253858302&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research, Redmond, WA 98052, USA\u2020; Microsoft Research, Redmond, WA 98052, USA\u21e4; Citadel LLC, Seattle/Chicago, USA\u2020",
        "aff_domain": "citadel.com;microsoft.com;citadel.com",
        "email": "citadel.com;microsoft.com;citadel.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f1981e4bd8a0d6d8462016d2fc6276b3-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft Research, Redmond, WA 98052, USA\u2020;Microsoft Research, Redmond, WA 98052, USA\u21e4;Citadel LLC, Seattle/Chicago, USA\u2020",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Unsupervised Transformation Learning via Convex Relaxations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9454",
        "id": "9454",
        "author_site": "Tatsunori Hashimoto, Percy Liang, John Duchi",
        "author": "Tatsunori B Hashimoto; Percy Liang; John C. Duchi",
        "abstract": "Our goal is to extract meaningful transformations from raw images, such as varying the thickness of lines in handwriting or the lighting in a portrait. We propose an unsupervised approach to learn such transformations by attempting to reconstruct an image from a linear combination of transformations of its nearest neighbors.  On handwritten digits and celebrity portraits, we show that even with linear transformations, our method generates visually high-quality modified images.  Moreover, since our method is semiparametric and does not model the data distribution, the learned transformations extrapolate off the training data and can be applied to new types of images.",
        "bibtex": "@inproceedings{NIPS2017_86a1793f,\n author = {Hashimoto, Tatsunori B and Liang, Percy S and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Transformation Learning via Convex Relaxations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/86a1793f65aeef4aeef4b479fc9b2bca-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/86a1793f65aeef4aeef4b479fc9b2bca-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/86a1793f65aeef4aeef4b479fc9b2bca-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/86a1793f65aeef4aeef4b479fc9b2bca-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/86a1793f65aeef4aeef4b479fc9b2bca-Reviews.html",
        "metareview": "",
        "pdf_size": 3256452,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9545045200435532045&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/86a1793f65aeef4aeef4b479fc9b2bca-Abstract.html"
    },
    {
        "title": "Unsupervised learning of object frames by dense equivariant image labelling",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8879",
        "id": "8879",
        "author_site": "James Thewlis, Hakan Bilen, Andrea Vedaldi",
        "author": "James Thewlis; Hakan Bilen; Andrea Vedaldi",
        "abstract": "One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.",
        "bibtex": "@inproceedings{NIPS2017_cbcb58ac,\n author = {Thewlis, James and Bilen, Hakan and Vedaldi, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised learning of object frames by dense equivariant image labelling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/cbcb58ac2e496207586df2854b17995f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/cbcb58ac2e496207586df2854b17995f-Reviews.html",
        "metareview": "",
        "pdf_size": 5611704,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4014184974974685278&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Visual Geometry Group, University of Oxford; School of Informatics, University of Edinburgh; Visual Geometry Group, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;ed.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;ed.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/cbcb58ac2e496207586df2854b17995f-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Oxford;University of Edinburgh",
        "aff_unique_dep": "Visual Geometry Group;School of Informatics",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.ed.ac.uk",
        "aff_unique_abbr": "Oxford;Edinburgh",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Oxford;Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Uprooting and Rerooting Higher-Order Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8818",
        "id": "8818",
        "author_site": "Mark Rowland, Adrian Weller",
        "author": "Mark Rowland; Adrian Weller",
        "abstract": "The idea of uprooting and rerooting graphical models was introduced specifically for binary pairwise models by Weller (2016) as a way to transform a model to any of a whole equivalence class of related models, such that inference on any one model yields inference results for all others. This is very helpful since inference, or relevant bounds, may be much easier to obtain or more accurate for some model in the class. Here we introduce methods to extend the approach to models with higher-order potentials and develop theoretical insights. In particular, we show that the triplet-consistent polytope TRI is unique in being `universally rooted'. We demonstrate  empirically that rerooting can significantly improve accuracy of methods of inference for higher-order models at negligible computational cost.",
        "bibtex": "@inproceedings{NIPS2017_1ff8a7b5,\n author = {Rowland, Mark and Weller, Adrian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Uprooting and Rerooting Higher-Order Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Reviews.html",
        "metareview": "",
        "pdf_size": 630686,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15037653658086707512&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Cambridge; University of Cambridge + Alan Turing Institute",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of Cambridge;Alan Turing Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "Cambridge;ATI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9036",
        "id": "9036",
        "author_site": "Zhaohan Guo, Philip S. Thomas, Emma Brunskill",
        "author": "Zhaohan Guo; Philip S. Thomas; Emma Brunskill",
        "abstract": "Evaluating a policy by deploying it in the real world can be risky and costly. Off-policy policy evaluation (OPE) algorithms use historical data collected from running a previous policy to evaluate a new policy, which provides a means for evaluating a policy without requiring it to ever be deployed. Importance sampling is a popular OPE method because it is robust to partial observability and works with continuous states and actions. However, the amount of historical data required by importance sampling can scale exponentially with the horizon of the problem: the number of sequential decisions that are made. We propose using policies over temporally extended actions, called options, and show that combining these policies with importance sampling can significantly improve performance for long-horizon problems. In addition, we can take advantage of special cases that arise due to options-based policies to further improve the performance of importance sampling. We further generalize these special cases to a general covariance testing rule that can be used to decide which weights to drop in an IS estimate, and derive a new IS algorithm called Incremental Importance Sampling that can provide significantly more accurate estimates for a broad class of domains.",
        "bibtex": "@inproceedings{NIPS2017_6786f3c6,\n author = {Guo, Zhaohan and Thomas, Philip S. and Brunskill, Emma},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6786f3c62fbf9021694f6e51cc07fe3c-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/6786f3c62fbf9021694f6e51cc07fe3c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/6786f3c62fbf9021694f6e51cc07fe3c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/6786f3c62fbf9021694f6e51cc07fe3c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/6786f3c62fbf9021694f6e51cc07fe3c-Reviews.html",
        "metareview": "",
        "pdf_size": 482206,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2149180900860237156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University; University of Massachusetts Amherst; Stanford University",
        "aff_domain": "cs.cmu.edu;cs.umass.edu;cs.stanford.edu",
        "email": "cs.cmu.edu;cs.umass.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/6786f3c62fbf9021694f6e51cc07fe3c-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Massachusetts Amherst;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.umass.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;UMass Amherst;Stanford",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Amherst;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "VAE Learning via Stein Variational Gradient Descent",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9203",
        "id": "9203",
        "author_site": "Yuchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, Lawrence Carin",
        "author": "Yuchen Pu; Zhe Gan; Ricardo Henao; Chunyuan Li; Shaobo Han; Lawrence Carin",
        "abstract": "A new method for learning variational autoencoders (VAEs) is developed, based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.",
        "bibtex": "@inproceedings{NIPS2017_443dec30,\n author = {Pu, Yuchen and Gan, Zhe and Henao, Ricardo and Li, Chunyuan and Han, Shaobo and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {VAE Learning via Stein Variational Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/443dec3062d0286986e21dc0631734c9-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/443dec3062d0286986e21dc0631734c9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/443dec3062d0286986e21dc0631734c9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/443dec3062d0286986e21dc0631734c9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/443dec3062d0286986e21dc0631734c9-Reviews.html",
        "metareview": "",
        "pdf_size": 746720,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10858329545757287872&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/443dec3062d0286986e21dc0631734c9-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "VAIN: Attentional Multi-agent Predictive Modeling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9056",
        "id": "9056",
        "author": "Yedid Hoshen",
        "abstract": "Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.",
        "bibtex": "@inproceedings{NIPS2017_748ba69d,\n author = {Hoshen, Yedid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {VAIN: Attentional Multi-agent Predictive Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/748ba69d3e8d1af87f84fee909eef339-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/748ba69d3e8d1af87f84fee909eef339-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/748ba69d3e8d1af87f84fee909eef339-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/748ba69d3e8d1af87f84fee909eef339-Reviews.html",
        "metareview": "",
        "pdf_size": 408806,
        "gs_citation": 319,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=466897156843231983&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Facebook AI Research, NYC",
        "aff_domain": "fb.com",
        "email": "fb.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/748ba69d3e8d1af87f84fee909eef339-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Facebook AI Research, NYC",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "title": "VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9115",
        "id": "9115",
        "author_site": "Akash Srivastava, Lazar Valkov, Chris Russell, Michael Gutmann, Charles Sutton",
        "author": "Akash Srivastava; Lazar Valkov; Chris Russell; Michael U. Gutmann; Charles Sutton",
        "abstract": "Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images.  But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution.  To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise.  Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise.  In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption.  On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.",
        "bibtex": "@inproceedings{NIPS2017_44a2e080,\n author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Reviews.html",
        "metareview": "",
        "pdf_size": 883987,
        "gs_citation": 899,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4824955249203309779&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; The Alan Turing Institute, London; School of Informatics, University of Edinburgh; School of Informatics & The Alan Turing Institute, University of Edinburgh",
        "aff_domain": "ed.ac.uk;sms.ed.ac.uk;turing.ac.uk;ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;sms.ed.ac.uk;turing.ac.uk;ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "University of Edinburgh;The Alan Turing Institute, London;School of Informatics & The Alan Turing Institute, University of Edinburgh",
        "aff_unique_dep": "School of Informatics;;",
        "aff_unique_url": "https://www.ed.ac.uk;;",
        "aff_unique_abbr": "Edinburgh;;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "title": "Value Prediction Network",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9383",
        "id": "9383",
        "author_site": "Junhyuk Oh, Satinder Singh, Honglak Lee",
        "author": "Junhyuk Oh; Satinder Singh; Honglak Lee",
        "abstract": "This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.",
        "bibtex": "@inproceedings{NIPS2017_ffbd6cbb,\n author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Value Prediction Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Reviews.html",
        "metareview": "",
        "pdf_size": 999718,
        "gs_citation": 430,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17526159437756263895&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/ffbd6cbb019a1413183c8d08f2929307-Abstract.html"
    },
    {
        "title": "Variable Importance Using Decision Trees",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8839",
        "id": "8839",
        "author_site": "Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S Talwalkar",
        "author": "Jalil Kazemitabar; Arash Amini; Adam Bloniarz; Ameet S Talwalkar",
        "abstract": "Decision trees and random forests are well established models that not only offer good predictive performance, but also provide rich feature importance information. While practitioners often employ variable importance methods that rely on this impurity-based information, these methods remain poorly characterized from a theoretical perspective. We provide novel insights into the performance of these methods by deriving finite sample performance guarantees in a high-dimensional setting under various modeling assumptions.  We further demonstrate the effectiveness of these impurity-based methods via an extensive set of simulations.",
        "bibtex": "@inproceedings{NIPS2017_5737c6ec,\n author = {Kazemitabar, Jalil and Amini, Arash and Bloniarz, Adam and Talwalkar, Ameet S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variable Importance Using Decision Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Reviews.html",
        "metareview": "",
        "pdf_size": 646486,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1116797662358135255&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "UCLA; UCLA; UC Berkeley*; CMU",
        "aff_domain": "ucla.edu;ucla.edu;stat.berkeley.edu;cmu.edu",
        "email": "ucla.edu;ucla.edu;stat.berkeley.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of California, Los Angeles;University of California, Berkeley;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucla.edu;https://www.berkeley.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UCLA;UC Berkeley;CMU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Los Angeles;Berkeley;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Variance-based Regularization with Convex Objectives",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9082",
        "id": "9082",
        "author_site": "Hongseok Namkoong, John Duchi",
        "author": "Hongseok Namkoong; John C. Duchi",
        "abstract": "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.",
        "bibtex": "@inproceedings{NIPS2017_5a142a55,\n author = {Namkoong, Hongseok and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variance-based Regularization with Convex Objectives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/5a142a55461d5fef016acfb927fee0bd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/5a142a55461d5fef016acfb927fee0bd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/5a142a55461d5fef016acfb927fee0bd-Reviews.html",
        "metareview": "",
        "pdf_size": 738724,
        "gs_citation": 408,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18398012294308713831&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Variational Inference for Gaussian Process Models with Linear Complexity",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9294",
        "id": "9294",
        "author_site": "Ching-An Cheng, Byron Boots",
        "author": "Ching-An Cheng; Byron Boots",
        "abstract": "Large-scale Gaussian process inference has long faced practical challenges due to time and space complexity that is superlinear in dataset size. While sparse variational Gaussian process models are capable of learning from large-scale data, standard strategies for sparsifying the model can prevent the approximation of complex functions. In this work, we propose a novel variational Gaussian process model that decouples the representation of mean and covariance functions in reproducing kernel Hilbert space. We show that this new parametrization generalizes previous models. Furthermore, it yields a variational inference problem that can be solved by stochastic gradient ascent with time and space complexity that is only linear in the number of mean function parameters, regardless of the choice of kernels, likelihoods, and inducing points. This strategy makes the adoption of large-scale expressive Gaussian process models possible. We run several experiments on regression tasks and show that this decoupled approach greatly outperforms previous sparse variational Gaussian process inference procedures.",
        "bibtex": "@inproceedings{NIPS2017_f8da71e5,\n author = {Cheng, Ching-An and Boots, Byron},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Inference for Gaussian Process Models with Linear Complexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Reviews.html",
        "metareview": "",
        "pdf_size": 978048,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11174343637866639512&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute for Robotics and Intelligent Machines, Georgia Institute of Technology; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f8da71e562ff44a2bc7edf3578c593da-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "Institute for Robotics and Intelligent Machines",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Variational Inference via $\\chi$ Upper Bound Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9059",
        "id": "9059",
        "author_site": "Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David Blei",
        "author": "Adji Bousso Dieng; Dustin Tran; Rajesh Ranganath; John Paisley; David Blei",
        "abstract": "Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions $q$ and finds the closest member to the exact posterior $p$. Closeness is usually measured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence from $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we term the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance.",
        "bibtex": "@inproceedings{NIPS2017_35464c84,\n author = {Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Inference via \\textbackslash chi Upper Bound Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/35464c848f410e55a13bb9d78e7fddd0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/35464c848f410e55a13bb9d78e7fddd0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/35464c848f410e55a13bb9d78e7fddd0-Reviews.html",
        "metareview": "",
        "pdf_size": 2245793,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14901563675443305978&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Columbia University; Columbia University; Princeton University; Columbia University; Columbia University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/35464c848f410e55a13bb9d78e7fddd0-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Columbia University;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Columbia;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Variational Laws of Visual Attention for Dynamic Scenes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9164",
        "id": "9164",
        "author_site": "Dario Zanca, Marco Gori",
        "author": "Dario Zanca; Marco Gori",
        "abstract": "Computational models of visual attention are at the crossroad of disciplines like cognitive science, computational neuroscience, and computer vision. This paper proposes a model of attentional scanpath that is based on the principle that there are foundational laws that drive the emergence of visual attention. We devise variational laws of the eye-movement that rely on a generalized view of the Least Action Principle in physics. The potential energy  captures details  as well as peripheral visual features, while the kinetic energy corresponds with the classic interpretation in analytic mechanics. In addition, the Lagrangian contains a brightness invariance term, which characterizes significantly the scanpath trajectories. We obtain differential equations of visual attention as the stationary point of the generalized action, and we propose an algorithm to estimate the model parameters.  Finally, we report experimental results to validate the model in tasks of saliency detection.",
        "bibtex": "@inproceedings{NIPS2017_194cf6c2,\n author = {Zanca, Dario and Gori, Marco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Laws of Visual Attention for Dynamic Scenes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/194cf6c2de8e00c05fcf16c498adc7bf-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/194cf6c2de8e00c05fcf16c498adc7bf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/194cf6c2de8e00c05fcf16c498adc7bf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/194cf6c2de8e00c05fcf16c498adc7bf-Reviews.html",
        "metareview": "",
        "pdf_size": 248847,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6555482399142072726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "DINFO, University of Florence + DIISM, University of Siena; DIISM, University of Siena",
        "aff_domain": "unifi.it;diism.unisi.it",
        "email": "unifi.it;diism.unisi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/194cf6c2de8e00c05fcf16c498adc7bf-Abstract.html",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "University of Florence;DIISM, University of Siena",
        "aff_unique_dep": "DINFO;",
        "aff_unique_url": "https://www.unifi.it;",
        "aff_unique_abbr": "UNIFI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Italy;"
    },
    {
        "title": "Variational Memory Addressing in Generative Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9173",
        "id": "9173",
        "author_site": "J\u00f6rg Bornschein, Andriy Mnih, Daniel Zoran, Danilo Jimenez Rezende",
        "author": "J\u00f6rg Bornschein; Andriy Mnih; Daniel Zoran; Danilo Jimenez Rezende",
        "abstract": "Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory.",
        "bibtex": "@inproceedings{NIPS2017_3937230d,\n author = {Bornschein, J\\\"{o}rg and Mnih, Andriy and Zoran, Daniel and Jimenez Rezende, Danilo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Memory Addressing in Generative Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3937230de3c8041e4da6ac3246a888e8-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/3937230de3c8041e4da6ac3246a888e8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/3937230de3c8041e4da6ac3246a888e8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/3937230de3c8041e4da6ac3246a888e8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/3937230de3c8041e4da6ac3246a888e8-Reviews.html",
        "metareview": "",
        "pdf_size": 910130,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5447075617754084282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/3937230de3c8041e4da6ac3246a888e8-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9218",
        "id": "9218",
        "author_site": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio",
        "author": "Anirudh Goyal ALIAS PARTH GOYAL; Nan Rosemary Ke; Surya Ganguli; Yoshua Bengio",
        "abstract": "We propose a novel method to {\\it directly} learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems.   The proposed training objective, which we derive via principled variational methods, encourages the transition operator to \"walk back\" (prefer to revert its steps) in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution. Source Code:http://github.com/anirudh9119/walkback_nips17",
        "bibtex": "@inproceedings{NIPS2017_46a558d9,\n author = {ALIAS PARTH GOYAL, Anirudh Goyal and Ke, Nan Rosemary and Ganguli, Surya and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/46a558d97954d0692411c861cf78ef79-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/46a558d97954d0692411c861cf78ef79-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/46a558d97954d0692411c861cf78ef79-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/46a558d97954d0692411c861cf78ef79-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/46a558d97954d0692411c861cf78ef79-Reviews.html",
        "metareview": "",
        "pdf_size": 386989,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11187763082524566489&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "MILA, Universit\u00e9 de Montr\u00e9al; MILA, \u00c9cole Polytechnique de Montr\u00e9al; Stanford University; MILA, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "gmail.com;gmail.com;stanford.edu;gmail.com",
        "email": "gmail.com;gmail.com;stanford.edu;gmail.com",
        "github": "http://github.com/anirudh9119/walkback_nips17",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/46a558d97954d0692411c861cf78ef79-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;MILA, \u00c9cole Polytechnique de Montr\u00e9al;Stanford University",
        "aff_unique_dep": "MILA;;",
        "aff_unique_url": "https://www.umontreal.ca;;https://www.stanford.edu",
        "aff_unique_abbr": "UdeM;;Stanford",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Montr\u00e9al;;Stanford",
        "aff_country_unique_index": "0;2;0",
        "aff_country_unique": "Canada;;United States"
    },
    {
        "title": "Visual Interaction Networks: Learning a Physics Simulator from Video",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9231",
        "id": "9231",
        "author_site": "Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, Andrea Tacchetti",
        "author": "Nicholas Watters; Daniel Zoran; Theophane Weber; Peter Battaglia; Razvan Pascanu; Andrea Tacchetti",
        "abstract": "From just a glance, humans can make rich predictions about the future of a wide range of physical systems.  On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains or require information about the underlying state. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.",
        "bibtex": "@inproceedings{NIPS2017_8cbd005a,\n author = {Watters, Nicholas and Zoran, Daniel and Weber, Theophane and Battaglia, Peter and Pascanu, Razvan and Tacchetti, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Visual Interaction Networks: Learning a Physics Simulator from Video},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Reviews.html",
        "metareview": "",
        "pdf_size": 5671272,
        "gs_citation": 444,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5649908015913062760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "DeepMind; DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/8cbd005a556ccd4211ce43f309bc0eac-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "DeepMind",
        "aff_unique_dep": "",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Visual Reference Resolution using Attention Memory for Visual Dialog",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9154",
        "id": "9154",
        "author_site": "Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal",
        "author": "Paul Hongsuck Seo; Andreas Lehrmann; Bohyung Han; Leonid Sigal",
        "abstract": "Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention ({\\em a.k.a. visual grounding}) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves previous attention, taking into account recency, that is most relevant for the current question, in order to resolve potentially ambiguous reference(s). The model then merges the retrieved attention with the tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by ~16 % points) in the situation where the visual reference resolution plays an important role. Moreover, the proposed model presents superior performance (~2 % points improvement) in the Visual Dialog dataset, despite having significantly fewer parameters than the baselines.",
        "bibtex": "@inproceedings{NIPS2017_654ad60e,\n author = {Seo, Paul Hongsuck and Lehrmann, Andreas and Han, Bohyung and Sigal, Leonid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Visual Reference Resolution using Attention Memory for Visual Dialog},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/654ad60ebd1ae29cedc37da04b6b0672-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/654ad60ebd1ae29cedc37da04b6b0672-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/654ad60ebd1ae29cedc37da04b6b0672-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/654ad60ebd1ae29cedc37da04b6b0672-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/654ad60ebd1ae29cedc37da04b6b0672-Reviews.html",
        "metareview": "",
        "pdf_size": 908348,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5025704664190171133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "POSTECH; Disney Research; POSTECH; Disney Research",
        "aff_domain": "postech.ac.kr;disneyresearch.com;postech.ac.kr;disneyresearch.com",
        "email": "postech.ac.kr;disneyresearch.com;postech.ac.kr;disneyresearch.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/654ad60ebd1ae29cedc37da04b6b0672-Abstract.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Pohang University of Science and Technology;Disney Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.postech.ac.kr;https://research.disney.com",
        "aff_unique_abbr": "POSTECH;Disney Research",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pohang;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Wasserstein Learning of Deep Generative Point Process Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9109",
        "id": "9109",
        "author_site": "Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Xiaokang Yang, Le Song, Hongyuan Zha",
        "author": "Shuai Xiao; Mehrdad Farajtabar; Xiaojing Ye; Junchi Yan; Le Song; Hongyuan Zha",
        "abstract": "Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.",
        "bibtex": "@inproceedings{NIPS2017_f45a1078,\n author = {Xiao, Shuai and Farajtabar, Mehrdad and Ye, Xiaojing and Yan, Junchi and Song, Le and Zha, Hongyuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Wasserstein Learning of Deep Generative Point Process Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f45a1078feb35de77d26b3f7a52ef502-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/f45a1078feb35de77d26b3f7a52ef502-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/f45a1078feb35de77d26b3f7a52ef502-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/f45a1078feb35de77d26b3f7a52ef502-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/f45a1078feb35de77d26b3f7a52ef502-Reviews.html",
        "metareview": "",
        "pdf_size": 1202235,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10193999566191256083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Shanghai Jiao Tong University; College of Computing, Georgia Institute of Technology; School of Mathematics, Georgia State University; IBM Research \u2013 China; College of Computing, Georgia Institute of Technology + Ant Financial; College of Computing, Georgia Institute of Technology",
        "aff_domain": "sjtu.edu.cn;gatech.edu;gsu.edu;cn.ibm.com;cc.gatech.edu;cc.gatech.edu",
        "email": "sjtu.edu.cn;gatech.edu;gsu.edu;cn.ibm.com;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/f45a1078feb35de77d26b3f7a52ef502-Abstract.html",
        "aff_unique_index": "0;1;2;3;1+4;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Georgia Institute of Technology;School of Mathematics, Georgia State University;IBM Research;Ant Financial",
        "aff_unique_dep": ";College of Computing;;China;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.gatech.edu;;https://www.ibm.com/research/global/china;https://www.antgroup.com",
        "aff_unique_abbr": "SJTU;Georgia Tech;;IBM;Ant Financial",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "0;1;0;1+0;1",
        "aff_country_unique": "China;United States;"
    },
    {
        "title": "Welfare Guarantees from Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9159",
        "id": "9159",
        "author_site": "Darrell Hoy, Denis Nekipelov, Vasilis Syrgkanis",
        "author": "Darrell Hoy; Denis Nekipelov; Vasilis Syrgkanis",
        "abstract": "Analysis of efficiency of outcomes in game theoretic settings has been a main item of study at the intersection of economics and computer science. The notion of the price of anarchy takes a worst-case stance to efficiency analysis, considering instance independent guarantees of efficiency. We propose a data-dependent analog of the price of anarchy that refines this worst-case assuming access to samples of strategic behavior. We focus on auction settings, where the latter is non-trivial due to the private information held by participants. Our approach to bounding the efficiency from data is robust to statistical errors and mis-specification. Unlike traditional econometrics, which seek to learn the private information of players from observed behavior and then analyze properties of the outcome, we directly quantify the inefficiency without going through the private information. We apply our approach to datasets from a sponsored search auction system and find empirical results that are a significant improvement over bounds from worst-case analysis.",
        "bibtex": "@inproceedings{NIPS2017_d3a7f48c,\n author = {Hoy, Darrell and Nekipelov, Denis and Syrgkanis, Vasilis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Welfare Guarantees from Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d3a7f48c12e697d50c8a7ae7684644ef-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/d3a7f48c12e697d50c8a7ae7684644ef-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/d3a7f48c12e697d50c8a7ae7684644ef-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/d3a7f48c12e697d50c8a7ae7684644ef-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/d3a7f48c12e697d50c8a7ae7684644ef-Reviews.html",
        "metareview": "",
        "pdf_size": 379862,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6109544060180199813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland; University of Virginia; Microsoft Research",
        "aff_domain": "gmail.com;virginia.edu;microsoft.com",
        "email": "gmail.com;virginia.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/d3a7f48c12e697d50c8a7ae7684644ef-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Maryland;University of Virginia;Microsoft Corporation",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www/umd.edu;https://www.virginia.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UMD;UVA;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9332",
        "id": "9332",
        "author_site": "Alex Kendall, Yarin Gal",
        "author": "Alex Kendall; Yarin Gal",
        "abstract": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",
        "bibtex": "@inproceedings{NIPS2017_2650d608,\n author = {Kendall, Alex and Gal, Yarin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Reviews.html",
        "metareview": "",
        "pdf_size": 2488839,
        "gs_citation": 6455,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6063636260641358531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge; University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9466",
        "id": "9466",
        "author_site": "Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo A Parrilo, Nuri Vanli",
        "author": "Mert Gurbuzbalaban; Asuman Ozdaglar; Pablo A Parrilo; Nuri Vanli",
        "abstract": "The coordinate descent (CD) method is a classical optimization algorithm that has seen a revival of interest because of its competitive performance in machine learning applications. A number of recent papers provided convergence rate estimates for their deterministic (cyclic) and randomized variants that differ in the selection of update coordinates. These estimates suggest randomized coordinate descent (RCD) performs better than cyclic coordinate descent (CCD), although numerical experiments do not provide clear justification for this comparison. In this paper, we provide examples and more generally problem classes for which CCD (or CD with any deterministic order) is faster than RCD in terms of asymptotic worst-case convergence. Furthermore, we provide lower and upper bounds on the amount of improvement on the rate of CCD relative to RCD, which depends on the deterministic order used. We also provide a characterization of the best deterministic order (that leads to the maximum improvement in convergence rate) in terms of the combinatorial properties of the Hessian matrix of the objective function.",
        "bibtex": "@inproceedings{NIPS2017_0e7c7d6c,\n author = {Gurbuzbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A and Vanli, Nuri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Reviews.html",
        "metareview": "",
        "pdf_size": 937169,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10518692102055860673&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Rutgers University; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "rutgers.edu;mit.edu;mit.edu;mit.edu",
        "email": "rutgers.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/0e7c7d6c41c76b9ee6445ae01cc0181d-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Rutgers University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rutgers.edu;https://web.mit.edu",
        "aff_unique_abbr": "Rutgers;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9411",
        "id": "9411",
        "author_site": "Chris Russell, Matt Kusner, Joshua Loftus, Ricardo Silva",
        "author": "Chris Russell; Matt J Kusner; Joshua Loftus; Ricardo Silva",
        "abstract": "Machine learning is now being used to make crucial decisions about people's lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal \"world\" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.",
        "bibtex": "@inproceedings{NIPS2017_1271a702,\n author = {Russell, Chris and Kusner, Matt J and Loftus, Joshua and Silva, Ricardo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Reviews.html",
        "metareview": "",
        "pdf_size": 663683,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17502408284461811749&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "The Alan Turing Institute + University of Surrey; The Alan Turing Institute + University of Warwick; New York University; The Alan Turing Institute + University College London",
        "aff_domain": "turing.ac.uk;turing.ac.uk;nyu.edu;stats.ucl.ac.uk",
        "email": "turing.ac.uk;turing.ac.uk;nyu.edu;stats.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1271a7029c9df08643b631b02cf9e116-Abstract.html",
        "aff_unique_index": "0+1;0+2;3;0+4",
        "aff_unique_norm": "The Alan Turing Institute;University of Surrey;University of Warwick;New York University;University College London",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.turing.ac.uk;https://www.surrey.ac.uk;https://www.warwick.ac.uk;https://www.nyu.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "ATI;Surrey;Warwick;NYU;UCL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1;0+0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8798",
        "id": "8798",
        "author_site": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber",
        "author": "Zhen He; Shaobing Gao; Liang Xiao; Daxue Liu; Hangen He; David Barber",
        "abstract": "Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.",
        "bibtex": "@inproceedings{NIPS2017_70efdf2e,\n author = {He, Zhen and Gao, Shaobing and Xiao, Liang and Liu, Daxue and He, Hangen and Barber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/70efdf2ec9b086079795c442636b55fb-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/70efdf2ec9b086079795c442636b55fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/70efdf2ec9b086079795c442636b55fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/70efdf2ec9b086079795c442636b55fb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/70efdf2ec9b086079795c442636b55fb-Reviews.html",
        "metareview": "",
        "pdf_size": 3066552,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13702292894678017907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University College London + National University of Defense Technology; Sichuan University; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; University College London + Alan Turing Institute",
        "aff_domain": "gmail.com;scu.edu.cn; ; ; ; ",
        "email": "gmail.com;scu.edu.cn; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/70efdf2ec9b086079795c442636b55fb-Abstract.html",
        "aff_unique_index": "0+1;2;1;1;1;0+3",
        "aff_unique_norm": "University College London;National University of Defense Technology;Sichuan University;Alan Turing Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ucl.ac.uk;http://www.nudt.edu.cn/;https://www.scu.edu.cn;https://www.turing.ac.uk",
        "aff_unique_abbr": "UCL;NUDT;SCU;ATI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1;0+0",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "title": "Working hard to know your neighbor's margins: Local descriptor learning loss",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9259",
        "id": "9259",
        "author_site": "Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas",
        "author": "Anastasiia Mishchuk; Dmytro Mishkin; Filip Radenovic; Jiri Matas",
        "abstract": "We introduce a loss for metric learning, which is inspired by the Lowe's matching criterion for SIFT. We show that the proposed loss, that maximizes the distance between the closest positive and closest negative example in the batch, is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks.",
        "bibtex": "@inproceedings{NIPS2017_831caa1b,\n author = {Mishchuk, Anastasiia and Mishkin, Dmytro and Radenovic, Filip and Matas, Jiri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Working hard to know your neighbor\\textquotesingle s margins: Local descriptor learning loss},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/831caa1b600f852b7844499430ecac17-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/831caa1b600f852b7844499430ecac17-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/831caa1b600f852b7844499430ecac17-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/831caa1b600f852b7844499430ecac17-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/831caa1b600f852b7844499430ecac17-Reviews.html",
        "metareview": "",
        "pdf_size": 960412,
        "gs_citation": 876,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3890579834109030721&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Szkocka Research Group, Ukraine; Visual Recognition Group, CTU in Prague; Visual Recognition Group, CTU in Prague; Visual Recognition Group, CTU in Prague",
        "aff_domain": "gmail.com;cmp.felk.cvut.cz;cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "email": "gmail.com;cmp.felk.cvut.cz;cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/831caa1b600f852b7844499430ecac17-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Szkocka Research Group, Ukraine;Visual Recognition Group, CTU in Prague",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "YASS: Yet Another Spike Sorter",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9181",
        "id": "9181",
        "author_site": "Jin Hyung Lee, David Carlson, Hooshmand Shokri Razaghi, Weichi Yao, Georges A Goetz, Espen Hagen, Eleanor Batty, E.J. Chichilnisky, Gaute T. Einevoll, Liam Paninski",
        "author": "Jin Hyung Lee; David E Carlson; Hooshmand Shokri Razaghi; Weichi Yao; Georges A Goetz; Espen Hagen; Eleanor Batty; E. J. Chichilnisky; Gaute T. Einevoll; Liam Paninski",
        "abstract": "Spike sorting is a critical first step in extracting neural signals from large-scale electrophysiological data.  This manuscript describes an efficient, reliable pipeline for spike sorting on dense multi-electrode arrays (MEAs), where neural signals appear across many electrodes and spike sorting currently represents a major computational bottleneck. We present several new techniques that make dense MEA spike sorting more robust and scalable. Our pipeline is based on an efficient multi-stage ''triage-then-cluster-then-pursuit'' approach that initially extracts only clean, high-quality waveforms from the electrophysiological time series by temporarily skipping noisy or ''collided'' events (representing two neurons firing synchronously). This is accomplished by developing a neural network detection method followed by efficient outlier triaging. The clean waveforms are then used to infer the set of neural spike waveform templates through nonparametric Bayesian clustering. Our clustering approach adapts a ''coreset'' approach for data reduction and uses efficient inference methods in a Dirichlet process mixture model framework to dramatically improve the scalability and reliability of the entire pipeline. The ''triaged'' waveforms are then finally recovered with matching-pursuit deconvolution techniques. The proposed methods improve on the state-of-the-art in terms of accuracy and stability on both real and biophysically-realistic simulated MEA data. Furthermore, the proposed pipeline is efficient, learning templates and clustering faster than real-time for a 500-electrode dataset, largely on a single CPU core.",
        "bibtex": "@inproceedings{NIPS2017_19431027,\n author = {Lee, Jin Hyung and Carlson, David E and Shokri Razaghi, Hooshmand and Yao, Weichi and Goetz, Georges A and Hagen, Espen and Batty, Eleanor and Chichilnisky, E.J. and Einevoll, Gaute T. and Paninski, Liam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {YASS: Yet Another Spike Sorter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1943102704f8f8f3302c2b730728e023-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/1943102704f8f8f3302c2b730728e023-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/1943102704f8f8f3302c2b730728e023-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/1943102704f8f8f3302c2b730728e023-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/1943102704f8f8f3302c2b730728e023-Reviews.html",
        "metareview": "",
        "pdf_size": 1729497,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6624261122241586116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Columbia University; Duke University; Columbia University; Columbia University; Stanford University; University of Oslo; Columbia University; Stanford University; Norwegian University of Life Sciences; Columbia University",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 10,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/1943102704f8f8f3302c2b730728e023-Abstract.html",
        "aff_unique_index": "0;1;0;0;2;3;0;2;4;0",
        "aff_unique_norm": "Columbia University;Duke University;Stanford University;University of Oslo;Norwegian University of Life Sciences",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.columbia.edu;https://www.duke.edu;https://www.stanford.edu;https://www.uio.no;https://www.nmbu.no",
        "aff_unique_abbr": "Columbia;Duke;Stanford;UiO;NMBU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0;1;0;0;1;0",
        "aff_country_unique": "United States;Norway"
    },
    {
        "title": "Z-Forcing: Training Stochastic Recurrent Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9439",
        "id": "9439",
        "author_site": "Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre C\u00f4t\u00e9, Nan Rosemary Ke, Yoshua Bengio",
        "author": "Anirudh Goyal ALIAS PARTH GOYAL; Alessandro Sordoni; Marc-Alexandre C\u00f4t\u00e9; Nan Rosemary Ke; Yoshua Bengio",
        "abstract": "Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortised variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.",
        "bibtex": "@inproceedings{NIPS2017_900c563b,\n author = {ALIAS PARTH GOYAL, Anirudh Goyal and Sordoni, Alessandro and C\\^{o}t\\'{e}, Marc-Alexandre and Ke, Nan Rosemary and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Z-Forcing: Training Stochastic Recurrent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/900c563bfd2c48c16701acca83ad858a-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/900c563bfd2c48c16701acca83ad858a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/900c563bfd2c48c16701acca83ad858a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/900c563bfd2c48c16701acca83ad858a-Reviews.html",
        "metareview": "",
        "pdf_size": 338684,
        "gs_citation": 240,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13517121338688320232&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "MILA, Universit\u00e9 de Montr\u00e9al; Microsoft Maluuba; Microsoft Maluuba; MILA, Polytechnique Montr\u00e9al; MILA, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/900c563bfd2c48c16701acca83ad858a-Abstract.html",
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Microsoft Maluuba;MILA, Polytechnique Montr\u00e9al",
        "aff_unique_dep": "MILA;;",
        "aff_unique_url": "https://www.umontreal.ca;;",
        "aff_unique_abbr": "UdeM;;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "title": "Zap Q-Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/9011",
        "id": "9011",
        "author_site": "Adithya M Devraj, Sean Meyn",
        "author": "Adithya M Devraj; Sean Meyn",
        "abstract": "The  Zap Q-learning algorithm introduced in this paper is an improvement of Watkins' original algorithm and recent competitors in several respects. It is a matrix-gain algorithm designed so that its asymptotic variance is optimal. Moreover, an ODE analysis suggests that the transient behavior is a close match to a deterministic Newton-Raphson implementation. This is made possible by a  two time-scale update equation for the matrix gain sequence. The analysis suggests that the approach will lead to stable and efficient computation even for non-ideal parameterized settings. Numerical experiments confirm the quick convergence, even in such non-ideal cases.",
        "bibtex": "@inproceedings{NIPS2017_4671aeaf,\n author = {Devraj, Adithya M and Meyn, Sean},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Zap Q-Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/4671aeaf49c792689533b00664a5c3ef-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/4671aeaf49c792689533b00664a5c3ef-Reviews.html",
        "metareview": "",
        "pdf_size": 5062009,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2898018990159470434&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32608; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32608",
        "aff_domain": "ufl.edu;ece.ufl.edu",
        "email": "ufl.edu;ece.ufl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/4671aeaf49c792689533b00664a5c3ef-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32608",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "f-GANs in an Information Geometric Nutshell",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8842",
        "id": "8842",
        "author_site": "Richard Nock, Zac Cranko, Aditya K Menon, Lizhen Qu, Robert Williamson",
        "author": "Richard Nock; Zac Cranko; Aditya K Menon; Lizhen Qu; Robert C. Williamson",
        "abstract": "Nowozin \\textit{et al} showed last year how to extend the GAN \\textit{principle} to all $f$-divergences. The approach is elegant but falls short of a full description of the supervised game, and says little about the key player, the generator: for example, what does the generator actually converge to if solving the GAN game means convergence in some space of parameters? How does that provide hints on the generator's design and compare to the flourishing but almost exclusively experimental literature on the subject? In this paper, we unveil a broad class of distributions for which such convergence happens --- namely, deformed exponential families, a wide superset of exponential families ---. We show that current deep architectures are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep architectures and their concinnity in the $f$-GAN game. This result holds given a sufficient condition on \\textit{activation functions} ---  which turns out to be satisfied by popular choices. The key to our results is a variational generalization of an old theorem that relates the KL divergence between regular exponential families and divergences between their natural parameters. We complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of GAN architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses' link function in the discriminator.",
        "bibtex": "@inproceedings{NIPS2017_2f2b2656,\n author = {Nock, Richard and Cranko, Zac and Menon, Aditya K and Qu, Lizhen and Williamson, Robert C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {f-GANs in an Information Geometric Nutshell},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/2f2b265625d76a6704b08093c652fd79-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/2f2b265625d76a6704b08093c652fd79-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/2f2b265625d76a6704b08093c652fd79-Reviews.html",
        "metareview": "",
        "pdf_size": 587269,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5193599389479427447&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Data61+the Australian National University+the University of Sydney; Data61+the Australian National University; Data61+the Australian National University; Data61+the Australian National University; Data61+the Australian National University",
        "aff_domain": "data61.csiro.au;data61.csiro.au;data61.csiro.au;data61.csiro.au;data61.csiro.au",
        "email": "data61.csiro.au;data61.csiro.au;data61.csiro.au;data61.csiro.au;data61.csiro.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html",
        "aff_unique_index": "0+1+2;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Data61;the Australian National University;the University of Sydney",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://data61.csiro.au;;",
        "aff_unique_abbr": "Data61;;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia;"
    },
    {
        "title": "k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2017/poster/8825",
        "id": "8825",
        "author_site": "Cong Han Lim, Stephen Wright",
        "author": "Cong Han Lim; Stephen Wright",
        "abstract": "The k-support and OWL norms generalize the l1 norm, providing better prediction accuracy and better handling of correlated variables. We study the norms obtained from extending the k-support norm and OWL norms to the setting in which there are overlapping groups. The resulting norms are in general NP-hard to compute, but they are tractable for certain collections of groups. To demonstrate this fact, we  develop a dynamic program for the problem of projecting onto the set of vectors supported by a fixed number of groups. Our dynamic program utilizes tree decompositions and its complexity scales with the treewidth. This program can be converted to an extended formulation which, for the associated group structure, models the k-group support norms and an overlapping group variant of the ordered weighted l1 norm. Numerical results demonstrate the efficacy of the new penalties.",
        "bibtex": "@inproceedings{NIPS2017_13fe9d84,\n author = {Lim, Cong Han and Wright, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf},\n volume = {30},\n year = {2017}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2017/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2017/file/13fe9d84310e77f13a6d184dbf1232f3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2017/file/13fe9d84310e77f13a6d184dbf1232f3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2017/file/13fe9d84310e77f13a6d184dbf1232f3-Reviews.html",
        "metareview": "",
        "pdf_size": 488866,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6112440938144229321&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Wisconsin-Madison; University of Wisconsin-Madison",
        "aff_domain": "wisc.edu;cs.wisc.edu",
        "email": "wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2017/hash/13fe9d84310e77f13a6d184dbf1232f3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    }
]