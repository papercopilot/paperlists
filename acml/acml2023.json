[
    {
        "title": "A Corrected Expected Improvement Acquisition Function Under Noisy Observations",
        "site": "https://proceedings.mlr.press/v222/zhou24a.html",
        "author": "Han Zhou; Xingchen Ma; Matthew B Blaschko",
        "abstract": "Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless settings. We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise. Our empirical results demonstrate that our proposed acquisition function can outperform EI in the presence of noisy observations on benchmark functions for black-box optimization, as well as on parameter search for neural network model compression.",
        "bibtex": "@InProceedings{pmlr-v222-zhou24a,\n  title = \t {A Corrected Expected Improvement Acquisition Function Under Noisy Observations},\n  author =       {Zhou, Han and Ma, Xingchen and Blaschko, Matthew B},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1747--1762},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhou24a/zhou24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhou24a.html},\n  abstract = \t {Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless settings. We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise. Our empirical results demonstrate that our proposed acquisition function can outperform EI in the presence of noisy observations on benchmark functions for black-box optimization, as well as on parameter search for neural network model compression.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhou24a/zhou24a.pdf",
        "supp": "",
        "pdf_size": 3587118,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6090193387695291895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. ESAT, Center for Processing Speech and Images, KU Leuven, Belgium; Amazon Web Services + Dept. ESAT, Center for Processing Speech and Images, KU Leuven, Belgium; Dept. ESAT, Center for Processing Speech and Images, KU Leuven, Belgium",
        "aff_domain": "esat.kuleuven.be;amazon.de;esat.kuleuven.be",
        "email": "esat.kuleuven.be;amazon.de;esat.kuleuven.be",
        "github": "https://github.com/han678/correctedNoisyEI",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "KU Leuven;Amazon Web Services",
        "aff_unique_dep": "Center for Processing Speech and Images;",
        "aff_unique_url": "https://www.kuleuven.be;https://aws.amazon.com",
        "aff_unique_abbr": "KU Leuven;AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Belgium;United States"
    },
    {
        "title": "A Mixed-Precision Quantization Method without Accuracy Degradation Using Semilayers",
        "site": "https://proceedings.mlr.press/v222/matsumoto24a.html",
        "author": "Kengo Matsumoto; Tomoya Matsuda; Atsuki Inoue; Hiroshi Kawaguchi; Yasufumi Sakai",
        "abstract": "Reducing the memory usage and computational complexity of high-performance deep neural networks while minimizing degradation of accuracy is a key issue in implementing these models on edge devices. To address this issue, partial quantization methods have been proposed to partially reduce the weight parameters of neural network models. However, the accuracy of existing methods degrades rapidly with increasing compression ratio. Although retraining can compensate for this issue to some extent, it is computationally very expensive. In this study, we propose a mixed-precision quantization algorithm without retraining or degradation in accuracy. In the proposed method, first, the difference between values after and before quantization losses of each channel in the layers of the pretrained model is calculated for all channels. Next, the layers are divided into two groups called semilayers according to whether the loss difference is positive or negative. The priorities for quantization in the semilayers are determined based on the Kulback-Leibler divergence derived from the probability distribution of the softmax output after and before quantization. The same process is repeated as a mixed-precision quantization while gradually decreasing the bitwidth, for example, with 8-, 6-, and 4-bit quantizations, and so forth. The results of an experimental evaluation show that the proposed method successfully compressed a ResNet-18 model by 81.44%, a ResNet-34 model by 84.25%, and a ResNet-50 model by 80.39% on image classification tasks using the ImageNet dataset, and a ResNet-18 model by 80.56% on image classification tasks using the CIFAR-10 dataset, with no degradation of the inference accuracy of the pretrained models.",
        "bibtex": "@InProceedings{pmlr-v222-matsumoto24a,\n  title = \t {A Mixed-Precision Quantization Method without Accuracy Degradation Using Semilayers},\n  author =       {Matsumoto, Kengo and Matsuda, Tomoya and Inoue, Atsuki and Kawaguchi, Hiroshi and Sakai, Yasufumi},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {882--894},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/matsumoto24a/matsumoto24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/matsumoto24a.html},\n  abstract = \t {Reducing the memory usage and computational complexity of high-performance deep neural networks while minimizing degradation of accuracy is a key issue in implementing these models on edge devices. To address this issue, partial quantization methods have been proposed to partially reduce the weight parameters of neural network models. However, the accuracy of existing methods degrades rapidly with increasing compression ratio. Although retraining can compensate for this issue to some extent, it is computationally very expensive. In this study, we propose a mixed-precision quantization algorithm without retraining or degradation in accuracy. In the proposed method, first, the difference between values after and before quantization losses of each channel in the layers of the pretrained model is calculated for all channels. Next, the layers are divided into two groups called semilayers according to whether the loss difference is positive or negative. The priorities for quantization in the semilayers are determined based on the Kulback-Leibler divergence derived from the probability distribution of the softmax output after and before quantization. The same process is repeated as a mixed-precision quantization while gradually decreasing the bitwidth, for example, with 8-, 6-, and 4-bit quantizations, and so forth. The results of an experimental evaluation show that the proposed method successfully compressed a ResNet-18 model by 81.44%, a ResNet-34 model by 84.25%, and a ResNet-50 model by 80.39% on image classification tasks using the ImageNet dataset, and a ResNet-18 model by 80.56% on image classification tasks using the CIFAR-10 dataset, with no degradation of the inference accuracy of the pretrained models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/matsumoto24a/matsumoto24a.pdf",
        "supp": "",
        "pdf_size": 1065577,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16068558623053018426&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Graduate School of Science, Technology and Innovation, Kobe University; Graduate School of Science, Technology and Innovation, Kobe University; Graduate School of Science, Technology and Innovation, Kobe University; Graduate School of Science, Technology and Innovation, Kobe University; Fujitsu Research, Fujitsu Limited",
        "aff_domain": "CS28.CS.KOBE-U.AC.JP;CS28.CS.KOBE-U.AC.JP;GODZILLA.KOBE-U.AC.JP;GODZILLA.KOBE-U.AC.JP;FUJITSU.COM",
        "email": "CS28.CS.KOBE-U.AC.JP;CS28.CS.KOBE-U.AC.JP;GODZILLA.KOBE-U.AC.JP;GODZILLA.KOBE-U.AC.JP;FUJITSU.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Kobe University;Fujitsu Limited",
        "aff_unique_dep": "Graduate School of Science, Technology and Innovation;Fujitsu Research",
        "aff_unique_url": "https://www.kobe-u.ac.jp;https://www.fujitsu.com/",
        "aff_unique_abbr": "Kobe U;Fujitsu",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kobe;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "A Multi-Surrogate Assisted Salp Swarm Feature Selection Algorithm with Multi-Population Adaptive Generation Strategy for Classification",
        "site": "https://proceedings.mlr.press/v222/yu24a.html",
        "author": "Zikang Yu; Hongbin Dong; Tianyu Guo; Bingxu Zhao",
        "abstract": "The salp swarm algorithm(SSA) has been successfully used to solve the feature selection problem due to its fast convergence and simple structure. However, existing SSA-based methods still suffer from the issue of low classification accuracy due to the problem of getting trapped in local optima. Therefore, this paper proposes a novel feature selection method for classification based on SSA, which can continuously generate new sub-populations to improve the search environment of the main population. Specifically, a flip-prohibition(F-P) operator is first proposed to help the main population, which may currently fall into a local optimum, find a new and more promising region. A multi-surrogate technique is suggested to evaluate the region to determine the position of sub-populations, which can reduce the high computational cost. In addition, a population initialization method is developed according to the importance of features and the dimensionality of the dataset. Finally, a communication mechanism is presented to enable different sub-populations to learn from each other. By comparing the proposed method with other 6 feature selection methods on 16 datasets, we demonstrate that the proposed method has better classification ability and can select a smaller feature subset in most cases.",
        "bibtex": "@InProceedings{pmlr-v222-yu24a,\n  title = \t {A Multi-Surrogate Assisted Salp Swarm Feature Selection Algorithm with Multi-Population Adaptive Generation Strategy for Classification},\n  author =       {Yu, Zikang and Dong, Hongbin and Guo, Tianyu and Zhao, Bingxu},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1590--1605},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yu24a/yu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yu24a.html},\n  abstract = \t {The salp swarm algorithm(SSA) has been successfully used to solve the feature selection problem due to its fast convergence and simple structure. However, existing SSA-based methods still suffer from the issue of low classification accuracy due to the problem of getting trapped in local optima. Therefore, this paper proposes a novel feature selection method for classification based on SSA, which can continuously generate new sub-populations to improve the search environment of the main population. Specifically, a flip-prohibition(F-P) operator is first proposed to help the main population, which may currently fall into a local optimum, find a new and more promising region. A multi-surrogate technique is suggested to evaluate the region to determine the position of sub-populations, which can reduce the high computational cost. In addition, a population initialization method is developed according to the importance of features and the dimensionality of the dataset. Finally, a communication mechanism is presented to enable different sub-populations to learn from each other. By comparing the proposed method with other 6 feature selection methods on 16 datasets, we demonstrate that the proposed method has better classification ability and can select a smaller feature subset in most cases.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yu24a/yu24a.pdf",
        "supp": "",
        "pdf_size": 1750920,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17647371701252691381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Technology, Harbin Engineering University, China; Department of Computer Science and Technology, Harbin Engineering University, China; Department of Computer Science and Technology, Harbin Engineering University, China; University of Victoria",
        "aff_domain": "hrbeu.edu.cn;hrbeu.edu.cn;hrbeu.edu.cn;hrbeu.edu.cn",
        "email": "hrbeu.edu.cn;hrbeu.edu.cn;hrbeu.edu.cn;hrbeu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Harbin Engineering University;University of Victoria",
        "aff_unique_dep": "Department of Computer Science and Technology;",
        "aff_unique_url": ";https://www.uvic.ca",
        "aff_unique_abbr": ";UVic",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "title": "A New Perspective On the Expressive Equivalence Between Graph Convolution and Attention Models",
        "site": "https://proceedings.mlr.press/v222/shi24a.html",
        "author": "Dai Shi; Zhiqi Shao; Andi Han; Yi Guo; Gao Junbin",
        "abstract": "Graph neural networks (GNNs) have demonstrated impressive achievements in diverse graph tasks, and research on their expressive power has experienced significant growth in recent years. The well-known Weisfeiler and Lehman (WL) isomorphism test has been widely used to assess GNNs\u2019 ability to distinguish graph structures. However, despite being considered less expressive than other GNNs in graph-level tasks based on the WL test, two prominent GNN models, namely graph convolution networks (GCN) and attention-based graph networks (GAT), still exhibit strong performance in node-level classification tasks. In this paper, we present a comprehensive analysis of their expressive power using a novel evaluation metric: the number of linear regions. We demonstrate that by enhancing GCN with refined graph Ricci curvature, our proposed high-rank graph convolution network (HRGCN) can match or even surpass the prediction advantage of attention models. Thus, the two models exhibit equivalent node-level expressive powers. This fresh perspective highlights the evaluation of GNNs\u2019 expressive power in node-level classifications rather than solely at the graph level. Experimental results showcase that the proposed HRGCN model outperforms the state-of-the-art in various classification and prediction tasks.",
        "bibtex": "@InProceedings{pmlr-v222-shi24a,\n  title = \t {A New Perspective On the Expressive Equivalence Between Graph Convolution and Attention Models},\n  author =       {Shi, Dai and Shao, Zhiqi and Han, Andi and Guo, Yi and Junbin, Gao},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1199--1214},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/shi24a/shi24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/shi24a.html},\n  abstract = \t {Graph neural networks (GNNs) have demonstrated impressive achievements in diverse graph tasks, and research on their expressive power has experienced significant growth in recent years. The well-known Weisfeiler and Lehman (WL) isomorphism test has been widely used to assess GNNs\u2019 ability to distinguish graph structures. However, despite being considered less expressive than other GNNs in graph-level tasks based on the WL test, two prominent GNN models, namely graph convolution networks (GCN) and attention-based graph networks (GAT), still exhibit strong performance in node-level classification tasks. In this paper, we present a comprehensive analysis of their expressive power using a novel evaluation metric: the number of linear regions. We demonstrate that by enhancing GCN with refined graph Ricci curvature, our proposed high-rank graph convolution network (HRGCN) can match or even surpass the prediction advantage of attention models. Thus, the two models exhibit equivalent node-level expressive powers. This fresh perspective highlights the evaluation of GNNs\u2019 expressive power in node-level classifications rather than solely at the graph level. Experimental results showcase that the proposed HRGCN model outperforms the state-of-the-art in various classification and prediction tasks.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/shi24a/shi24a.pdf",
        "supp": "",
        "pdf_size": 513434,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6904565423792978543&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Western Sydney University; University of Sydney; University of Sydney; Western Sydney University; University of Sydney",
        "aff_domain": "student.westernsydney.edu.au;sydney.edu.au;sydney.edu.au;westernsydney.edu.au;sydney.edu.au",
        "email": "student.westernsydney.edu.au;sydney.edu.au;sydney.edu.au;westernsydney.edu.au;sydney.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "Western Sydney University;University of Sydney",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.westernsydney.edu.au;https://www.sydney.edu.au",
        "aff_unique_abbr": "WSU;USYD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis",
        "site": "https://proceedings.mlr.press/v222/wu24a.html",
        "author": "Dongming Wu; Lulu Wen; Chao Chen; Zhaoshu Shi",
        "abstract": "Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyzes the emotional polarity of the evaluation aspects. Generally, the emotional polarity of an aspect exists in the corresponding opinion expression, whose diversity has great impact on model\u2019s performance. To mitigate this problem, we propose a novel and simple counterfactual data augmentation method to generate opinion expressions with reversed sentiment polarity. In particular, the integrated gradients are calculated to locate and mask the opinion expression. Then, a prompt combined with the reverse expression polarity is added to the original text, and a Pre-trained language model (PLM), T5, is finally was employed to predict the masks. The experimental results shows the proposed counterfactual data augmentation method performs better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant, and MAMS.",
        "bibtex": "@InProceedings{pmlr-v222-wu24a,\n  title = \t {A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis},\n  author =       {Wu, Dongming and Wen, Lulu and Chen, Chao and Shi, Zhaoshu},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1479--1493},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/wu24a/wu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/wu24a.html},\n  abstract = \t {Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyzes the emotional polarity of the evaluation aspects. Generally, the emotional polarity of an aspect exists in the corresponding opinion expression, whose diversity has great impact on model\u2019s performance. To mitigate this problem, we propose a novel and simple counterfactual data augmentation method to generate opinion expressions with reversed sentiment polarity. In particular, the integrated gradients are calculated to locate and mask the opinion expression. Then, a prompt combined with the reverse expression polarity is added to the original text, and a Pre-trained language model (PLM), T5, is finally was employed to predict the masks. The experimental results shows the proposed counterfactual data augmentation method performs better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant, and MAMS.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/wu24a/wu24a.pdf",
        "supp": "",
        "pdf_size": 4971700,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11560040276241768647&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China; College of Computer Science and Technology, ZhejiangUniversity, Hangzhou, China + Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China; Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China; Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China",
        "aff_domain": "myhexin.com;zju.edu.cn;myhexin.com;myhexin.com",
        "email": "myhexin.com;zju.edu.cn;myhexin.com;myhexin.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Hithink RoyalFlush Information Network Co., Ltd.;Zhejiang University",
        "aff_unique_dep": ";College of Computer Science and Technology",
        "aff_unique_url": ";http://www.zju.edu.cn",
        "aff_unique_abbr": ";ZJU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hangzhou",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Partially Observable Monte Carlo Planning Algorithm Based on Path Modification",
        "site": "https://proceedings.mlr.press/v222/wang24c.html",
        "author": "Qingya Wang; Feng Liu; Bin Luo",
        "abstract": "Balancing exploration and exploitation has long been recognized as an important theme in the online planning algorithms for POMDP problems. Explorative actions on one hand prevent the planning from falling into the suboptimal dilemma, while hindering the convergence of the planning procedure on the other hand. Therefore, it is meaningful to maintain the exploration as well as taking a step forward towards exploitation. Note that there is a deviation between the action selection criteria in the planning procedure and in the execution procedure, which inspires us to build a bridge between these two criteria to accelerate the convergence. A Partially Observable Monte Carlo Planning algorithm based on Path Modification (POMCP-PM) is presented in the paper, which modifies the backtracing paths by considering the two criteria simultaneously when updating the values of parent nodes. The algorithm is general as the Upper Confidence Bound Apply to Tree (UCT) algorithm used to select actions can be easily replaced by other criteria. Experimental results demonstrate that POMCP-PM outperforms POMCP with varying numbers of simulations on several scenarios with different scales.",
        "bibtex": "@InProceedings{pmlr-v222-wang24c,\n  title = \t {A Partially Observable {M}onte {C}arlo Planning Algorithm Based on Path Modification},\n  author =       {Wang, Qingya and Liu, Feng and Luo, Bin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1449--1462},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/wang24c/wang24c.pdf},\n  url = \t {https://proceedings.mlr.press/v222/wang24c.html},\n  abstract = \t {Balancing exploration and exploitation has long been recognized as an important theme in the online planning algorithms for POMDP problems. Explorative actions on one hand prevent the planning from falling into the suboptimal dilemma, while hindering the convergence of the planning procedure on the other hand. Therefore, it is meaningful to maintain the exploration as well as taking a step forward towards exploitation. Note that there is a deviation between the action selection criteria in the planning procedure and in the execution procedure, which inspires us to build a bridge between these two criteria to accelerate the convergence. A Partially Observable Monte Carlo Planning algorithm based on Path Modification (POMCP-PM) is presented in the paper, which modifies the backtracing paths by considering the two criteria simultaneously when updating the values of parent nodes. The algorithm is general as the Upper Confidence Bound Apply to Tree (UCT) algorithm used to select actions can be easily replaced by other criteria. Experimental results demonstrate that POMCP-PM outperforms POMCP with varying numbers of simulations on several scenarios with different scales.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/wang24c/wang24c.pdf",
        "supp": "",
        "pdf_size": 745261,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GsycvywCaLIJ:scholar.google.com/&scioq=A+Partially+Observable+Monte+Carlo+Planning+Algorithm+Based+on+Path+Modification&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "National Key Laboratory for Novel Software Technology+Software Institute, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology+Software Institute, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology+Software Institute, Nanjing University, Nanjing, China",
        "aff_domain": "163.com;nju.edu.cn;nju.edu.cn",
        "email": "163.com;nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "National Key Laboratory for Novel Software Technology;Nanjing University",
        "aff_unique_dep": ";Software Institute",
        "aff_unique_url": ";http://www.nju.edu.cn",
        "aff_unique_abbr": ";NJU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Nanjing",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Pragmatic Look at Deep Imitation Learning",
        "site": "https://proceedings.mlr.press/v222/arulkumaran24a.html",
        "author": "Kai Arulkumaran; Dan Ogawa Lillrank",
        "abstract": "The introduction of the generative adversarial imitation learning (GAIL) algorithm has spurred the development of scalable imitation learning approaches using deep neural networks. Many of the algorithms that followed used a similar procedure, combining on-policy actor-critic algorithms with inverse reinforcement learning. More recently there have been an even larger breadth of approaches, most of which use off-policy algorithms. However, with the breadth of algorithms, everything from datasets to base reinforcement learning algorithms to evaluation settings can vary, making it difficult to fairly compare them. In this work we re-implement 6 different IL algorithms, updating 3 of them to be off-policy, base them on a common off-policy algorithm (SAC), and evaluate them on a widely-used expert trajectory dataset (D4RL) for the most common benchmark (MuJoCo). After giving all algorithms the same hyperparameter optimisation budget, we compare their results for a range of expert trajectories. In summary, GAIL, with all of its improvements, consistently performs well across a range of sample sizes, AdRIL is a simple contender that performs well with one important hyperparameter to tune, and behavioural cloning remains a strong baseline when data is more plentiful.",
        "bibtex": "@InProceedings{pmlr-v222-arulkumaran24a,\n  title = \t {A Pragmatic Look at Deep Imitation Learning},\n  author =       {Arulkumaran, Kai and Ogawa Lillrank, Dan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {58--73},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/arulkumaran24a/arulkumaran24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/arulkumaran24a.html},\n  abstract = \t {The introduction of the generative adversarial imitation learning (GAIL) algorithm has spurred the development of scalable imitation learning approaches using deep neural networks. Many of the algorithms that followed used a similar procedure, combining on-policy actor-critic algorithms with inverse reinforcement learning. More recently there have been an even larger breadth of approaches, most of which use off-policy algorithms. However, with the breadth of algorithms, everything from datasets to base reinforcement learning algorithms to evaluation settings can vary, making it difficult to fairly compare them. In this work we re-implement 6 different IL algorithms, updating 3 of them to be off-policy, base them on a common off-policy algorithm (SAC), and evaluate them on a widely-used expert trajectory dataset (D4RL) for the most common benchmark (MuJoCo). After giving all algorithms the same hyperparameter optimisation budget, we compare their results for a range of expert trajectories. In summary, GAIL, with all of its improvements, consistently performs well across a range of sample sizes, AdRIL is a simple contender that performs well with one important hyperparameter to tune, and behavioural cloning remains a strong baseline when data is more plentiful.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/arulkumaran24a/arulkumaran24a.pdf",
        "supp": "",
        "pdf_size": 495493,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13070768988868442702&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Araya Inc., Tokyo, Japan; Araya Inc., Tokyo, Japan",
        "aff_domain": "araya.org;araya.org",
        "email": "araya.org;araya.org",
        "github": "https://github.com/Kaixhin/imitation-learning",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Araya Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "A Simple and General Binarization Method for Image Restoration Neural Networks",
        "site": "https://proceedings.mlr.press/v222/wang24b.html",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NWfTCrgBkvcJ:scholar.google.com/&scioq=A+Simple+and+General+Binarization+Method+for+Image+Restoration+Neural+Networks&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "author": "",
        "aff": "",
        "aff_domain": "araya.org;araya.org",
        "email": "araya.org;araya.org",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "ASAP: Attention-Based State Space Abstraction for Policy Summarization",
        "site": "https://proceedings.mlr.press/v222/bekkemoen24a.html",
        "author": "Yanzhe Bekkemoen; Helge Langseth",
        "abstract": "Deep reinforcement learning (RL) has shown remarkable performance, but end-users do not understand how the system solves tasks due to the black-box nature of neural networks. Many methods from explainable machine learning have been adapted to RL. However, they do not focus on the unique challenges of explaining actions\u2019 short-term and long-term consequences. This work introduces a new perspective to understanding RL policies by clustering states into abstract states utilizing attention maps, giving a bird\u2019s-eye view of the policy\u2019s behavior. We learn the attention maps iteratively together with the clustering of states by masking the input features to estimate their importance. In contrast to previous works that have uninterpretable abstract states and/or clustering objectives using state values that are non-human intuitive, we only leverage attention maps in the clustering. The policy only indirectly affects the clustering via attention maps. This allows us to give global explanations from the view of feature attention, a quantity a human can relate to given interpretable features. The experiments demonstrate that our method provides faithful abstractions by capturing state semantics, policy behavior, and feature attention. Furthermore, we show that our attention maps can mask state features without affecting policy performance.",
        "bibtex": "@InProceedings{pmlr-v222-bekkemoen24a,\n  title = \t {{ASAP}: {A}ttention-Based State Space Abstraction for Policy Summarization},\n  author =       {Bekkemoen, Yanzhe and Langseth, Helge},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {137--152},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/bekkemoen24a/bekkemoen24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/bekkemoen24a.html},\n  abstract = \t {Deep reinforcement learning (RL) has shown remarkable performance, but end-users do not understand how the system solves tasks due to the black-box nature of neural networks. Many methods from explainable machine learning have been adapted to RL. However, they do not focus on the unique challenges of explaining actions\u2019 short-term and long-term consequences. This work introduces a new perspective to understanding RL policies by clustering states into abstract states utilizing attention maps, giving a bird\u2019s-eye view of the policy\u2019s behavior. We learn the attention maps iteratively together with the clustering of states by masking the input features to estimate their importance. In contrast to previous works that have uninterpretable abstract states and/or clustering objectives using state values that are non-human intuitive, we only leverage attention maps in the clustering. The policy only indirectly affects the clustering via attention maps. This allows us to give global explanations from the view of feature attention, a quantity a human can relate to given interpretable features. The experiments demonstrate that our method provides faithful abstractions by capturing state semantics, policy behavior, and feature attention. Furthermore, we show that our attention maps can mask state features without affecting policy performance.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/bekkemoen24a/bekkemoen24a.pdf",
        "supp": "",
        "pdf_size": 1078361,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15921897414093522218&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway",
        "aff_domain": "ntnu.no;ntnu.no",
        "email": "ntnu.no;ntnu.no",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Norwegian University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ntnu.no",
        "aff_unique_abbr": "NTNU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Trondheim",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Norway"
    },
    {
        "title": "Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee",
        "site": "https://proceedings.mlr.press/v222/ngo24a.html",
        "author": "Giang Ngo; Dang Nguyen; Dat Phan-Trong; Sunil Gupta",
        "abstract": "A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v222-ngo24a,\n  title = \t {Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee},\n  author =       {Ngo, Giang and Nguyen, Dang and Phan-Trong, Dat and Gupta, Sunil},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {943--958},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ngo24a/ngo24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ngo24a.html},\n  abstract = \t {A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ngo24a/ngo24a.pdf",
        "supp": "",
        "pdf_size": 783888,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12834075869876983122&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Applied Artificial Intelligence Institute (A2I2), Geelong, Victoria, Australia; Applied Artificial Intelligence Institute (A2I2), Geelong, Victoria, Australia; Applied Artificial Intelligence Institute (A2I2), Geelong, Victoria, Australia; Applied Artificial Intelligence Institute (A2I2), Geelong, Victoria, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Applied Artificial Intelligence Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "A2I2",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Geelong",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Ada$^2$NPT: An Adaptive Nearest Proxies Triplet Loss for Attribute-Aware Face Recognition with Adaptively Compacted Feature Learning",
        "site": "https://proceedings.mlr.press/v222/ju24a.html",
        "author": "Lei Ju; Zhanhua Feng; Muhammad Awais; Josef Kittler",
        "abstract": "Attribute-aware face recognition has gained increasing attention in recent years due to its potential to improve the robustness of face recognition systems. However, this may raise concerns about potential biases and privacy issues. To alleviate this, some studies involve complex designs to obtain independent ID and attribute features and fuse them based on the application scenario (for better accuracy or fairness). In this paper, we obviate their complex design and demonstrate that the Nearest neighbours Proxy Triplet (NPT) loss has an intrinsic capability for feature disentanglement. To further enhance the effectiveness of NPT, we propose a novel margin-based loss, namely Adaptive-rank NPT, which naturally separates the identity and attribute features. While a margin-based loss ensures inter-class separability, it imposes no constraints on intra-class compactness. The samples that meet the inter-class margin will not contribute to network training. To mitigate this issue, we propose an adaptive distance measurement to promote the compactness of the learned features, resulting in the final Ada$^2$NPT loss. The experimental results obtained on several benchmarks demonstrate the superiority and merits of the proposed loss function over the state-of-the-art losses in terms of accuracy and fairness.",
        "bibtex": "@InProceedings{pmlr-v222-ju24a,\n  title = \t {{Ada$^2$NPT}: {A}n Adaptive Nearest Proxies Triplet Loss for Attribute-Aware Face Recognition with Adaptively Compacted Feature Learning},\n  author =       {Ju, Lei and Feng, Zhanhua and Awais, Muhammad and Kittler, Josef},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {614--629},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ju24a/ju24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ju24a.html},\n  abstract = \t {Attribute-aware face recognition has gained increasing attention in recent years due to its potential to improve the robustness of face recognition systems. However, this may raise concerns about potential biases and privacy issues. To alleviate this, some studies involve complex designs to obtain independent ID and attribute features and fuse them based on the application scenario (for better accuracy or fairness). In this paper, we obviate their complex design and demonstrate that the Nearest neighbours Proxy Triplet (NPT) loss has an intrinsic capability for feature disentanglement. To further enhance the effectiveness of NPT, we propose a novel margin-based loss, namely Adaptive-rank NPT, which naturally separates the identity and attribute features. While a margin-based loss ensures inter-class separability, it imposes no constraints on intra-class compactness. The samples that meet the inter-class margin will not contribute to network training. To mitigate this issue, we propose an adaptive distance measurement to promote the compactness of the learned features, resulting in the final Ada$^2$NPT loss. The experimental results obtained on several benchmarks demonstrate the superiority and merits of the proposed loss function over the state-of-the-art losses in terms of accuracy and fairness.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ju24a/ju24a.pdf",
        "supp": "",
        "pdf_size": 1916844,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:QQRlGE_4jtgJ:scholar.google.com/&scioq=Ada%24%5E2%24NPT:+An+Adaptive+Nearest+Proxies+Triplet+Loss+for+Attribute-Aware+Face+Recognition+with+Adaptively+Compacted+Feature+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Electronic Engineering, University of Surrey, UK; School of Computer Science and Electronic Engineering, University of Surrey, UK; School of Computer Science and Electronic Engineering, University of Surrey, UK; School of Computer Science and Electronic Engineering, University of Surrey, UK",
        "aff_domain": "surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;surrey.ac.uk",
        "email": "surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;surrey.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "School of Computer Science and Electronic Engineering",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "Surrey",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Adaptive Riemannian stochastic gradient descent and reparameterization for Gaussian mixture model fitting",
        "site": "https://proceedings.mlr.press/v222/ji24a.html",
        "author": "Chunlin Ji; Yuhao Fu; Ping He",
        "abstract": "Recent advances in manifold optimization for the Gaussian mixture model (GMM) have gained increasing interest. In this work, instead of directly addressing the manifold optimization on covariance matrices of GMM, we consider the GMM fitting as an optimization of the density function over a statistical manifold and seek the natural gradient to speed up the optimization process. We present an upper bound for the Kullback\u2013Leibler (KL) divergence between two GMMs and obtain simple closed-form expressions for the natural gradients. With the natural gradients, we then apply the Riemannian stochastic gradient descent (RSGD) algorithm to optimize covariance matrices on a symmetric and positive definite (SPD) matrix manifold. We further propose a Riemannian Adam (RAdam) algorithm that extends the momentum method and adaptive learning in the Euclidean space to the SPD manifold space. Extensive simulations show that the proposed algorithms scale well to high-dimensional large-scale datasets and outperform expectation maximization (EM) algorithms in fitted log-likelihood.",
        "bibtex": "@InProceedings{pmlr-v222-ji24a,\n  title = \t {Adaptive {R}iemannian stochastic gradient descent and reparameterization for {G}aussian mixture model fitting},\n  author =       {Ji, Chunlin and Fu, Yuhao and He, Ping},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {534--549},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ji24a/ji24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ji24a.html},\n  abstract = \t {Recent advances in manifold optimization for the Gaussian mixture model (GMM) have gained increasing interest. In this work, instead of directly addressing the manifold optimization on covariance matrices of GMM, we consider the GMM fitting as an optimization of the density function over a statistical manifold and seek the natural gradient to speed up the optimization process. We present an upper bound for the Kullback\u2013Leibler (KL) divergence between two GMMs and obtain simple closed-form expressions for the natural gradients. With the natural gradients, we then apply the Riemannian stochastic gradient descent (RSGD) algorithm to optimize covariance matrices on a symmetric and positive definite (SPD) matrix manifold. We further propose a Riemannian Adam (RAdam) algorithm that extends the momentum method and adaptive learning in the Euclidean space to the SPD manifold space. Extensive simulations show that the proposed algorithms scale well to high-dimensional large-scale datasets and outperform expectation maximization (EM) algorithms in fitted log-likelihood.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ji24a/ji24a.pdf",
        "supp": "",
        "pdf_size": 796802,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3122100829141446613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Kuang-Chi Institute of Advanced Technology, Shenzhen, China; Kuang-Chi Institute of Advanced Technology, Shenzhen, China+Origin Artificial Intelligence Technology Co., Shenzhen, China; HeGuangLiangZi Tech., Shenzhen, China",
        "aff_domain": "kuang-chi.org;kuang-chi.com;ssr.fund",
        "email": "kuang-chi.org;kuang-chi.com;ssr.fund",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;2",
        "aff_unique_norm": "Kuang-Chi Institute of Advanced Technology;Origin Artificial Intelligence Technology Co.;HeGuangLiangZi Tech",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.kuangchi.org/;;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Advancing Deep Metric Learning With Adversarial Robustness",
        "site": "https://proceedings.mlr.press/v222/singh24a.html",
        "author": "Inderjeet Singh; Kazuya Kakizaki; Toshinori Araki",
        "abstract": "Deep Metric Learning (DML) is a prominent subfield of machine learning with extensive practical applications in learning visual similarities. However, DML systems are vulnerable to input distributions during inference that differ from the training data, such as adversarial examples (AXs). In this paper, we introduce MDProp, a framework that enhances the clean data performance and adversarial robustness of DML models by generating novel Multi-Targeted AXs and Unadversarial Examples, in addition to conventional single-targeted AXs, in the feature space. To handle the input distribution shift caused by the generated novel input distributions, MDProp scales the separate batch normalization layer strategy. Our comprehensive experimental analysis demonstrates that MDProp outperforms current state-of-the-art convolutional neural networks by up to 2.95% in terms of R@1 scores for clean data, while simultaneously improving adversarial robustness by up to 2.12 times. Additionally, MDProp achieves state-of-the-art results in data-scarce setting while utilizing only half of the training data. Implementation is available at \\url{https://github.com/intherejeet/MDProp}.",
        "bibtex": "@InProceedings{pmlr-v222-singh24a,\n  title = \t {Advancing Deep Metric Learning With Adversarial Robustness},\n  author =       {Singh, Inderjeet and Kakizaki, Kazuya and Araki, Toshinori},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1231--1246},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/singh24a/singh24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/singh24a.html},\n  abstract = \t {Deep Metric Learning (DML) is a prominent subfield of machine learning with extensive practical applications in learning visual similarities. However, DML systems are vulnerable to input distributions during inference that differ from the training data, such as adversarial examples (AXs). In this paper, we introduce MDProp, a framework that enhances the clean data performance and adversarial robustness of DML models by generating novel Multi-Targeted AXs and Unadversarial Examples, in addition to conventional single-targeted AXs, in the feature space. To handle the input distribution shift caused by the generated novel input distributions, MDProp scales the separate batch normalization layer strategy. Our comprehensive experimental analysis demonstrates that MDProp outperforms current state-of-the-art convolutional neural networks by up to 2.95% in terms of R@1 scores for clean data, while simultaneously improving adversarial robustness by up to 2.12 times. Additionally, MDProp achieves state-of-the-art results in data-scarce setting while utilizing only half of the training data. Implementation is available at \\url{https://github.com/intherejeet/MDProp}.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/singh24a/singh24a.pdf",
        "supp": "",
        "pdf_size": 9284921,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rPnwFp4G4KwJ:scholar.google.com/&scioq=Advancing+Deep+Metric+Learning+With+Adversarial+Robustness&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "NEC Corporation, Kawasaki, Kanagawa, Japan + Fujitsu Research of Europe Ltd, Slough, UK; NEC Corporation, Kawasaki, Kanagawa, Japan; NEC Corporation, Kawasaki, Kanagawa, Japan",
        "aff_domain": "nec.com;nec.com;nec.com",
        "email": "nec.com;nec.com;nec.com",
        "github": "https://github.com/intherejeet/MDProp",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "NEC Corporation;Fujitsu Research of Europe Ltd",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nec.com;https://www.fujitsu.com/uk/",
        "aff_unique_abbr": "NEC;FRE",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kawasaki;",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "Japan;United Kingdom"
    },
    {
        "title": "Asian Conference on Machine Learning: Preface",
        "site": "https://proceedings.mlr.press/v222/yanikoglu24a.html",
        "author": "Berrin Yan\u0131ko\u011flu; Wray Buntine",
        "abstract": "Preface to ACML 2023.",
        "bibtex": "@InProceedings{pmlr-v222-yanikoglu24a,\n  title = \t {Asian Conference on Machine Learning: Preface},\n  author =       {Yan{\\i}ko\\u{g}lu, Berrin and Buntine, Wray},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {i--ii},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yanikoglu24a/yanikoglu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yanikoglu24a.html},\n  abstract = \t {Preface to ACML 2023.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yanikoglu24a/yanikoglu24a.pdf",
        "supp": "",
        "pdf_size": 83666,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:l6k7FqW5IuIJ:scholar.google.com/&scioq=Asian+Conference+on+Machine+Learning:+Preface&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Attributed Graph Subspace Clustering with Graph-Boosting",
        "site": "https://proceedings.mlr.press/v222/li24c.html",
        "author": "Wang Li; En Zhu; Siwei Wang; Xifeng Guo",
        "abstract": "Attributed graph clustering groups nodes into disjoint categories with graph convolutional networks and has exhibited promising performance in various applications. However, there are two issues preventing the performance from being improved  further. First, the relationships between distant nodes are generally overlooked due to the sparsity of graphs. Second, the graph convolutional networks with few layers are sensitive to noises. To address these issues, we propose Attributed Graph Subspace clustering with Graph-Boosting (AGSGB). Specifically, to deal with the first issue, an auxiliary graph is built from the feature matrix to establish the distant relationships. And to address the second issue, a subspace clustering module, famous for its robustness to noise, is introduced. Based on the auxiliary graph and the subspace clustering module, a graph enhance module and a graph refine module are constructed, together with the graph autoencoder constituting the final clustering model. By using the given graph and the refined graph built by the graph refine module, a dual guidance supervisor is designed to train the clustering model. Finally, the clustering result can be obtained by the subspace clustering module. Extensive experimental results on five public benchmark datasets validate the effectiveness of the proposed method.",
        "bibtex": "@InProceedings{pmlr-v222-li24c,\n  title = \t {Attributed Graph Subspace Clustering with Graph-Boosting},\n  author =       {Li, Wang and Zhu, En and Wang, Siwei and Guo, Xifeng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {723--738},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/li24c/li24c.pdf},\n  url = \t {https://proceedings.mlr.press/v222/li24c.html},\n  abstract = \t {Attributed graph clustering groups nodes into disjoint categories with graph convolutional networks and has exhibited promising performance in various applications. However, there are two issues preventing the performance from being improved  further. First, the relationships between distant nodes are generally overlooked due to the sparsity of graphs. Second, the graph convolutional networks with few layers are sensitive to noises. To address these issues, we propose Attributed Graph Subspace clustering with Graph-Boosting (AGSGB). Specifically, to deal with the first issue, an auxiliary graph is built from the feature matrix to establish the distant relationships. And to address the second issue, a subspace clustering module, famous for its robustness to noise, is introduced. Based on the auxiliary graph and the subspace clustering module, a graph enhance module and a graph refine module are constructed, together with the graph autoencoder constituting the final clustering model. By using the given graph and the refined graph built by the graph refine module, a dual guidance supervisor is designed to train the clustering model. Finally, the clustering result can be obtained by the subspace clustering module. Extensive experimental results on five public benchmark datasets validate the effectiveness of the proposed method.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/li24c/li24c.pdf",
        "supp": "",
        "pdf_size": 1617425,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10800010246103300460&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "College of Computer, National University of Defense Technology, Changsha 410073, China; College of Computer, National University of Defense Technology, Changsha 410073, China; College of Computer, National University of Defense Technology, Changsha 410073, China; School of Cyberspace Science, Dongguan University of Technology, Guangdong, 523808, China",
        "aff_domain": "163.com;nudt.edu.cn;nudt.edu.cn;163.com",
        "email": "163.com;nudt.edu.cn;nudt.edu.cn;163.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "National University of Defense Technology;Dongguan University of Technology",
        "aff_unique_dep": "College of Computer;School of Cyberspace Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Changsha;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings",
        "site": "https://proceedings.mlr.press/v222/scafarto24a.html",
        "author": "Gregory Scafarto; Madalina Ciortan; Simon Tihon; Quentin Ferre",
        "abstract": "Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation-learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on subsequent downstream tasks.",
        "bibtex": "@InProceedings{pmlr-v222-scafarto24a,\n  title = \t {{Augment to Interpret}: {U}nsupervised and Inherently Interpretable Graph Embeddings},\n  author =       {Scafarto, Gregory and Ciortan, Madalina and Tihon, Simon and Ferre, Quentin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1183--1198},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/scafarto24a/scafarto24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/scafarto24a.html},\n  abstract = \t {Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation-learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on subsequent downstream tasks.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/scafarto24a/scafarto24a.pdf",
        "supp": "",
        "pdf_size": 2126325,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6064884533103488920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Euranova, Brussels, Belgium; Euranova, Brussels, Belgium; Euranova, Brussels, Belgium; Euranova, Brussels, Belgium",
        "aff_domain": "euranova.eu;euranova.eu;euranova.eu;euranova.eu",
        "email": "euranova.eu;euranova.eu;euranova.eu;euranova.eu",
        "github": "https://github.com/euranova/Augment_to_Interpret",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Euranova",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Brussels",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "title": "Automatic Segmentation of Aortic and Mitral Valves for Heart Surgical Planning of Hypertrophic Obstructive Cardiomyopathy",
        "site": "https://proceedings.mlr.press/v222/zheng24a.html",
        "author": "Limin Zheng; Hongyu Chen; Lu Qing; Jian Zhuang; Bo Meng; Xiaowei Xu",
        "abstract": "Hypertrophic obstructive cardiomyopathy (HOCM) is a leading cause of sudden cardiac death in young people. Septal myectomy surgery has been recognized as the gold standard for non-pharmacological therapy of HOCM, in which aortic and mitral valves are critical regions for surgical planning.Currently, manual segmentation of aortic and mitral valves is widely performed in clinical practice to construct 3D models used for HOCM surgical planning. Such a process, however, is time-consuming and costly. In this paper, we integrate anatomical prior knowledge into deep learning for automatic segmentation of aortic and mitral valves.In particular, a two-stage method is proposed: we first obtain the region of interest (RoI) from a CT image, where heart segmentation is then performed. The spatial relationship between heart substructures is utilized to identify a valve region that contains the aortic and mitral valves. Unlike typical two-stage methods, we feed the refined segmentation of the left ventricle, left atrium, and aorta as additional input for the valve segmentation. By incorporating this anatomical prior knowledge, deep neural networks (DNNs) can leverage the surrounding anatomical structures to improve valve segmentation. We collected a dataset of 27 CT images from patients with a medical history of septal myectomy surgery.Experimental results show that our method achieves an average Dice score of 71.2% and an improvement of 4.2% over existing methods.",
        "bibtex": "@InProceedings{pmlr-v222-zheng24a,\n  title = \t {Automatic Segmentation of Aortic and Mitral Valves for Heart Surgical Planning of Hypertrophic Obstructive Cardiomyopathy},\n  author =       {Zheng, Limin and Chen, Hongyu and Qing, Lu and Zhuang, Jian and Meng, Bo and Xu, Xiaowei},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1715--1730},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zheng24a/zheng24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zheng24a.html},\n  abstract = \t {Hypertrophic obstructive cardiomyopathy (HOCM) is a leading cause of sudden cardiac death in young people. Septal myectomy surgery has been recognized as the gold standard for non-pharmacological therapy of HOCM, in which aortic and mitral valves are critical regions for surgical planning.Currently, manual segmentation of aortic and mitral valves is widely performed in clinical practice to construct 3D models used for HOCM surgical planning. Such a process, however, is time-consuming and costly. In this paper, we integrate anatomical prior knowledge into deep learning for automatic segmentation of aortic and mitral valves.In particular, a two-stage method is proposed: we first obtain the region of interest (RoI) from a CT image, where heart segmentation is then performed. The spatial relationship between heart substructures is utilized to identify a valve region that contains the aortic and mitral valves. Unlike typical two-stage methods, we feed the refined segmentation of the left ventricle, left atrium, and aorta as additional input for the valve segmentation. By incorporating this anatomical prior knowledge, deep neural networks (DNNs) can leverage the surrounding anatomical structures to improve valve segmentation. We collected a dataset of 27 CT images from patients with a medical history of septal myectomy surgery.Experimental results show that our method achieves an average Dice score of 71.2% and an improvement of 4.2% over existing methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zheng24a/zheng24a.pdf",
        "supp": "",
        "pdf_size": 6140550,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13507544691567712529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Shenzhen University; Shenzhen University; Shenzhen University; Guangdong Provincial People\u2019s Hospital; Guangdong Provincial People\u2019s Hospital; Guangdong Provincial People\u2019s Hospital",
        "aff_domain": "email.szu.edu.cn;email.szu.edu.cn;szu.edu.cn;gmail.com;163.com;foxmail.com",
        "email": "email.szu.edu.cn;email.szu.edu.cn;szu.edu.cn;gmail.com;163.com;foxmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;1",
        "aff_unique_norm": "Shenzhen University;Guangdong Provincial People\u2019s Hospital",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.szu.edu.cn;",
        "aff_unique_abbr": "SZU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v222/cagatan24a.html",
        "author": "Omer Veysel Cagatan; Baris Akgun",
        "abstract": "This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.",
        "bibtex": "@InProceedings{pmlr-v222-cagatan24a,\n  title = \t {{BarlowRL}: {B}arlow Twins for Data-Efficient Reinforcement Learning},\n  author =       {Cagatan, Omer Veysel and Akgun, Baris},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {201--216},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/cagatan24a/cagatan24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/cagatan24a.html},\n  abstract = \t {This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/cagatan24a/cagatan24a.pdf",
        "supp": "",
        "pdf_size": 409535,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1424007486048460241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Koc University, Turkey; Koc University, Turkey",
        "aff_domain": "ku.edu.tr;ku.edu.tr",
        "email": "ku.edu.tr;ku.edu.tr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Koc University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.koc.edu.tr",
        "aff_unique_abbr": "Koc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Turkey"
    },
    {
        "title": "Better Loss Landscape Visualization for Deep Neural Networks with Trajectory Information",
        "site": "https://proceedings.mlr.press/v222/ding24a.html",
        "author": "Ruiqi Ding; Tao Li; Xiaolin Huang",
        "abstract": "The loss landscape of neural networks is a valuable perspective for studying the trainability, generalization, and robustness of networks, and hence its visualization has been extensively studied. Essentially, visualization methods project the parameter space into a low-dimensional subspace, resulting in a substantial loss of network parameter information. The key is to identify the direction of loss reduction in the visualized loss landscape. However, the existing methods generally focus on one simple point, make it challenging to properly capture the main properties of the landscape. An obvious and important problem is that regardless of whether the center point is the convergence or not, the current methods may depict it a local optimal point in the visualization. To address this issue, we propose a visualization method that relies on the whole training process not a single solution, better reflecting the actual training loss.",
        "bibtex": "@InProceedings{pmlr-v222-ding24a,\n  title = \t {Better Loss Landscape Visualization for Deep Neural Networks with Trajectory Information},\n  author =       {Ding, Ruiqi and Li, Tao and Huang, Xiaolin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {311--326},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ding24a/ding24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ding24a.html},\n  abstract = \t {The loss landscape of neural networks is a valuable perspective for studying the trainability, generalization, and robustness of networks, and hence its visualization has been extensively studied. Essentially, visualization methods project the parameter space into a low-dimensional subspace, resulting in a substantial loss of network parameter information. The key is to identify the direction of loss reduction in the visualized loss landscape. However, the existing methods generally focus on one simple point, make it challenging to properly capture the main properties of the landscape. An obvious and important problem is that regardless of whether the center point is the convergence or not, the current methods may depict it a local optimal point in the visualization. To address this issue, we propose a visualization method that relies on the whole training process not a single solution, better reflecting the actual training loss.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ding24a/ding24a.pdf",
        "supp": "",
        "pdf_size": 2306111,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18387663562690120055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Automation",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Can Infinitely Wide Deep Nets Help Small-data Multi-label Learning?",
        "site": "https://proceedings.mlr.press/v222/wu24b.html",
        "author": "Guoqiang Wu; Jun Zhu",
        "abstract": "In Multi-label Learning (MLL), kernel methods and deep neural networks (DNNs) are two typical families of approaches. Recent theory discovers an interesting connection between infinitely wide DNNs and neural tangent kernel (NTK) based methods. Further, recent work has shown the promising performance of NTK-based methods in \\emph{small-data single-labeled tasks}. Then, a natural question arises: can infinitely wide DNNs help small-data multi-label learning? To answer this question, in this paper, we present to utilize infinitely wide DNNs for the MLL task. Specifically, we propose an NTK-based kernel method for MLL, which aims to minimize Hamming and ranking loss simultaneously. Moreover, to efficiently train the model, we use the Nystr{\u00f6}m method, which has rarely been used in MLL. Further, we give rigorous theoretical analyses on learning guarantees of the proposed algorithm w.r.t. these two measures. Finally, empirical results on small-scale datasets illustrate its superior performance along with efficiency over several related baselines.",
        "bibtex": "@InProceedings{pmlr-v222-wu24b,\n  title = \t {Can Infinitely Wide Deep Nets Help Small-data Multi-label Learning?},\n  author =       {Wu, Guoqiang and Zhu, Jun},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1494--1509},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/wu24b/wu24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/wu24b.html},\n  abstract = \t {In Multi-label Learning (MLL), kernel methods and deep neural networks (DNNs) are two typical families of approaches. Recent theory discovers an interesting connection between infinitely wide DNNs and neural tangent kernel (NTK) based methods. Further, recent work has shown the promising performance of NTK-based methods in \\emph{small-data single-labeled tasks}. Then, a natural question arises: can infinitely wide DNNs help small-data multi-label learning? To answer this question, in this paper, we present to utilize infinitely wide DNNs for the MLL task. Specifically, we propose an NTK-based kernel method for MLL, which aims to minimize Hamming and ranking loss simultaneously. Moreover, to efficiently train the model, we use the Nystr{\u00f6}m method, which has rarely been used in MLL. Further, we give rigorous theoretical analyses on learning guarantees of the proposed algorithm w.r.t. these two measures. Finally, empirical results on small-scale datasets illustrate its superior performance along with efficiency over several related baselines.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/wu24b/wu24b.pdf",
        "supp": "",
        "pdf_size": 426049,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mofSe1ICOoUJ:scholar.google.com/&scioq=Can+Infinitely+Wide+Deep+Nets+Help+Small-data+Multi-label+Learning%3F&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Software, Shandong University; Department of Computer Science and Technology, BNRist Center, THU-Bosch Joint ML Center+State Key Laboratory of Intelligent Technology and Systems, Tsinghua University",
        "aff_domain": "sdu.edu.cn;tsinghua.edu.cn",
        "email": "sdu.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+1",
        "aff_unique_norm": "Shandong University;Tsinghua University",
        "aff_unique_dep": "School of Software;Department of Computer Science and Technology",
        "aff_unique_url": "http://www.sdu.edu.cn;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": ";THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Cell Variational Information Bottleneck Network",
        "site": "https://proceedings.mlr.press/v222/zhai24a.html",
        "author": "Zhonghua Zhai",
        "abstract": "In this work, we propose \u201cCell Variational Information Bottleneck Network (cellVIB)\u201d, a convolutional neural network using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method. Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty. As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB. In each VIB cell, the feedforward process learns an independent mean term and a standard deviation term, and predicts the Gaussian distribution based on them. The feedback process is based on reparameterization trick for effective training. This work performs an extensive analysis on MNIST dataset to verify the effectiveness of each VIB cells mentioned above, and provides an insightful analysis on how the VIB cells affect mutual information. Experiments conducted on CIFAR-10 also prove that our network is robust against noisy labels during training and against corrupted images during testing. Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model. Finally, in a more complex representation learning task, face recognition, our network structure has also achieved very competitive results.",
        "bibtex": "@InProceedings{pmlr-v222-zhai24a,\n  title = \t {Cell Variational Information Bottleneck Network},\n  author =       {Zhai, Zhonghua},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1606--1621},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhai24a/zhai24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhai24a.html},\n  abstract = \t {In this work, we propose \u201cCell Variational Information Bottleneck Network (cellVIB)\u201d, a convolutional neural network using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method. Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty. As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB. In each VIB cell, the feedforward process learns an independent mean term and a standard deviation term, and predicts the Gaussian distribution based on them. The feedback process is based on reparameterization trick for effective training. This work performs an extensive analysis on MNIST dataset to verify the effectiveness of each VIB cells mentioned above, and provides an insightful analysis on how the VIB cells affect mutual information. Experiments conducted on CIFAR-10 also prove that our network is robust against noisy labels during training and against corrupted images during testing. Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model. Finally, in a more complex representation learning task, face recognition, our network structure has also achieved very competitive results.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhai24a/zhai24a.pdf",
        "supp": "",
        "pdf_size": 552280,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TMgBRIlJyUQJ:scholar.google.com/&scioq=Cell+Variational+Information+Bottleneck+Network&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Hikvision Research Institute",
        "aff_domain": "hikvision.com",
        "email": "hikvision.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Hikvision Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hikvision.com/cn/",
        "aff_unique_abbr": "Hikvision",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "title": "Cross-Domain Relation Adaptation",
        "site": "https://proceedings.mlr.press/v222/kessler24a.html",
        "author": "Ido Kessler; Omri Lifshitz; Sagie Benaim; Lior Wolf",
        "abstract": "We consider the challenge of establishing relationships between samples in distinct domains, A and B, using supervised data that captures the intrinsic relationships within each domain. In other words, we present a semi-supervised setting in which there are no labeled mixed-domain pairs of samples. Our method is derived based on a generalization bound and incorporates supervised terms for each domain, a domain confusion term on the learned features, and a consistency term for domain-specific relationships when considering mixed-domain sample pairs. Our findings showcase the efficacy of our approach in two disparate domains: (i) Predicting protein-protein interactions between viruses and hosts by modeling genetic sequences. (ii) Forecasting link connections within citation graphs using graph neural networks.",
        "bibtex": "@InProceedings{pmlr-v222-kessler24a,\n  title = \t {Cross-Domain Relation Adaptation},\n  author =       {Kessler, Ido and Lifshitz, Omri and Benaim, Sagie and Wolf, Lior},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {630--645},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/kessler24a/kessler24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/kessler24a.html},\n  abstract = \t {We consider the challenge of establishing relationships between samples in distinct domains, A and B, using supervised data that captures the intrinsic relationships within each domain. In other words, we present a semi-supervised setting in which there are no labeled mixed-domain pairs of samples. Our method is derived based on a generalization bound and incorporates supervised terms for each domain, a domain confusion term on the learned features, and a consistency term for domain-specific relationships when considering mixed-domain sample pairs. Our findings showcase the efficacy of our approach in two disparate domains: (i) Predicting protein-protein interactions between viruses and hosts by modeling genetic sequences. (ii) Forecasting link connections within citation graphs using graph neural networks.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/kessler24a/kessler24a.pdf",
        "supp": "",
        "pdf_size": 674624,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:A0i41-lIBV8J:scholar.google.com/&scioq=Cross-Domain+Relation+Adaptation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Tel Aviv University; School of Computer Science, Tel Aviv University; Department of Computer Science, Hebrew University; School of Computer Science, Tel Aviv University",
        "aff_domain": "gmail.com;gmail.com;mail.huji.ac.il;cs.tau.ac.il",
        "email": "gmail.com;gmail.com;mail.huji.ac.il;cs.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Tel Aviv University;Hebrew University",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il;http://www.huji.ac.il",
        "aff_unique_abbr": "TAU;HUJI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "DENL: Diverse Ensemble and Noisy Logits for Improved Robustness of Neural Networks",
        "site": "https://proceedings.mlr.press/v222/yazdani24a.html",
        "author": "Mina Yazdani; Hamed Karimi; Reza Samavi",
        "abstract": "Neural Networks (NN) are increasingly used for image classification in medical, transportation, and security devices. However, recent studies have revealed neural networks\u2019 vulnerability against adversarial examples generated by adding small perturbations to images. These malicious samples are imperceptible by human eyes, but can give rise to misclassification by NN models. Defensive distillation is a defence mechanism in which the NN\u2019s output probabilities are scaled to a user-defined range and used as labels to train a new model less sensitive to input perturbations. Despite initial success, defensive distillation was defeated by state-of-the-art attacks. A proposed countermeasure was to add noise in the inference time to hamper the adversarial attack which also decreased the model accuracy. In this paper, we address this limitation by proposing a two-phase training methodology to defend against adversarial attacks. In the first phase, we train architecturally diversified models individually using the cross-entropy loss function. In the second phase, we train the ensemble using a diversity-promoting loss function. Our experimental results show that our training methodology and noise addition in the inference time improved our ensemble\u2019s resistance against adversarial attacks, while maintaining reasonable accuracy, compared to the state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v222-yazdani24a,\n  title = \t {{DENL}: {D}iverse Ensemble and Noisy Logits for Improved Robustness of Neural Networks},\n  author =       {Yazdani, Mina and Karimi, Hamed and Samavi, Reza},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1574--1589},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yazdani24a/yazdani24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yazdani24a.html},\n  abstract = \t {Neural Networks (NN) are increasingly used for image classification in medical, transportation, and security devices. However, recent studies have revealed neural networks\u2019 vulnerability against adversarial examples generated by adding small perturbations to images. These malicious samples are imperceptible by human eyes, but can give rise to misclassification by NN models. Defensive distillation is a defence mechanism in which the NN\u2019s output probabilities are scaled to a user-defined range and used as labels to train a new model less sensitive to input perturbations. Despite initial success, defensive distillation was defeated by state-of-the-art attacks. A proposed countermeasure was to add noise in the inference time to hamper the adversarial attack which also decreased the model accuracy. In this paper, we address this limitation by proposing a two-phase training methodology to defend against adversarial attacks. In the first phase, we train architecturally diversified models individually using the cross-entropy loss function. In the second phase, we train the ensemble using a diversity-promoting loss function. Our experimental results show that our training methodology and noise addition in the inference time improved our ensemble\u2019s resistance against adversarial attacks, while maintaining reasonable accuracy, compared to the state-of-the-art methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yazdani24a/yazdani24a.pdf",
        "supp": "",
        "pdf_size": 428910,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11270363617595259222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Electrical, Computer, and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON, Canada+Vector Institute, Toronto, ON, Canada; Department of Electrical, Computer, and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON, Canada+Vector Institute, Toronto, ON, Canada; Department of Electrical, Computer, and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON, Canada+Vector Institute, Toronto, ON, Canada",
        "aff_domain": "torontomu.ca;torontomu.ca;torontomu.ca",
        "email": "torontomu.ca;torontomu.ca;torontomu.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Toronto Metropolitan University;Vector Institute",
        "aff_unique_dep": "Department of Electrical, Computer, and Biomedical Engineering;",
        "aff_unique_url": "https://www.tmuh.ca;https://vectorinstitute.ai/",
        "aff_unique_abbr": "TMU;Vector Institute",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Decouple then Combine: A Simple and Effective Framework for Fraud Transaction Detection",
        "site": "https://proceedings.mlr.press/v222/tang24a.html",
        "author": "Pengwei Tang; Huayi Tang; Wenhan Wang; Hanjing Su; Yong Liu",
        "abstract": "With the popularity of electronic mobile and online payment, the demand for detecting financial fraudulent transactions is increasing. Although numerous efforts are devoted to tackling this problem, there are still two key challenges that are not well resolved, \\emph{i.e.}, the class imbalance ratio of test samples are extremely larger than that of training samples and amount of detected fraudulent transactions do not be considered. In this paper, we propose a simple and effective framework composed of majority and minority branches to address the above issues. The input samples of majority and minority branches come from vanilla and re-adjusted distribution, respectively. Parameters of each branch are optimized individually, by which the representation learning for majority and minority samples are decoupled. Besides, an extra loss re-weighted by amount is added in the majority branch to improve the recall amount of detected fraudulent transactions. Theoretical results show that under the proposed framework, minimizing the empirical risk is guaranteed to achieve small generalization risk on more imbalanced data with high probability. Experiments on real-world datasets from Tencent Wechat payments demonstrate that our framework achieves superior performance than competitive methods in terms of both number and money of detected fraudulent transactions.",
        "bibtex": "@InProceedings{pmlr-v222-tang24a,\n  title = \t {{Decouple then Combine}: {A} Simple and Effective Framework for Fraud Transaction Detection},\n  author =       {Tang, Pengwei and Tang, Huayi and Wang, Wenhan and Su, Hanjing and Liu, Yong},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1353--1368},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/tang24a/tang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/tang24a.html},\n  abstract = \t {With the popularity of electronic mobile and online payment, the demand for detecting financial fraudulent transactions is increasing. Although numerous efforts are devoted to tackling this problem, there are still two key challenges that are not well resolved, \\emph{i.e.}, the class imbalance ratio of test samples are extremely larger than that of training samples and amount of detected fraudulent transactions do not be considered. In this paper, we propose a simple and effective framework composed of majority and minority branches to address the above issues. The input samples of majority and minority branches come from vanilla and re-adjusted distribution, respectively. Parameters of each branch are optimized individually, by which the representation learning for majority and minority samples are decoupled. Besides, an extra loss re-weighted by amount is added in the majority branch to improve the recall amount of detected fraudulent transactions. Theoretical results show that under the proposed framework, minimizing the empirical risk is guaranteed to achieve small generalization risk on more imbalanced data with high probability. Experiments on real-world datasets from Tencent Wechat payments demonstrate that our framework achieves superior performance than competitive methods in terms of both number and money of detected fraudulent transactions.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/tang24a/tang24a.pdf",
        "supp": "",
        "pdf_size": 489265,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:U7KMc4Qg6qIJ:scholar.google.com/&scioq=Decouple+then+Combine:+A+Simple+and+Effective+Framework+for+Fraud+Transaction+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Renmin University of China; Renmin University of China; Tencent Inc.; Tencent Inc.; Renmin University of China",
        "aff_domain": "163.com;gmail.com;tencent.com;gmail.com;ruc.edu.cn",
        "email": "163.com;gmail.com;tencent.com;gmail.com;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Renmin University of China;Tencent",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "RUC;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Kernel Regression with Finite Learnable Kernels",
        "site": "https://proceedings.mlr.press/v222/ji24b.html",
        "author": "Chunlin Ji; Yuhao Fu",
        "abstract": "In this work, we study kernel regression that integrates with a modern deep neural network (DNN). The DNN projects the input into an embedding space, meanwhile a set of representative points is constructed in this embedding space. We build the regression using a finite set of kernels defined on the embedding space, where the DNN weights, the regression coefficients, and kernel hyperparameters are all learnable. We extend the model by introducing a location attention strategy and the multiple kernel technique. We provide effective ways to obtain representative points. The proposed model can be trained with an end-to-end learning algorithm with simple implementation. Simulation studies show that the proposed deep kernel regression is well scalable to large datasets and comparable to or superior to recent deep kernel models in various regression problems.",
        "bibtex": "@InProceedings{pmlr-v222-ji24b,\n  title = \t {Deep Kernel Regression with Finite Learnable Kernels},\n  author =       {Ji, Chunlin and Fu, Yuhao},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {550--565},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ji24b/ji24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ji24b.html},\n  abstract = \t {In this work, we study kernel regression that integrates with a modern deep neural network (DNN). The DNN projects the input into an embedding space, meanwhile a set of representative points is constructed in this embedding space. We build the regression using a finite set of kernels defined on the embedding space, where the DNN weights, the regression coefficients, and kernel hyperparameters are all learnable. We extend the model by introducing a location attention strategy and the multiple kernel technique. We provide effective ways to obtain representative points. The proposed model can be trained with an end-to-end learning algorithm with simple implementation. Simulation studies show that the proposed deep kernel regression is well scalable to large datasets and comparable to or superior to recent deep kernel models in various regression problems.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ji24b/ji24b.pdf",
        "supp": "",
        "pdf_size": 690059,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=925740446098814155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Kuang-Chi Institute of Advanced Technology, Shenzhen, China; Kuang-Chi Institute of Advanced Technology, Shenzhen, China + Origin Artificial Intelligence Technology Co., Shenzhen, China",
        "aff_domain": "kuang-chi.org;kuang-chi.com",
        "email": "kuang-chi.org;kuang-chi.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Kuang-Chi Institute of Advanced Technology;Origin Artificial Intelligence Technology Co.",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.kuangchi.org/;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Reinforcement Learning for Two-sided Online Bipartite Matching in Collaborative Order Picking",
        "site": "https://proceedings.mlr.press/v222/begnardi24a.html",
        "author": "Luca Begnardi; Hendrik Baier; Willem van Jaarsveld; Yingqian Zhang",
        "abstract": "As a growing number of warehouse operators are moving from human-only to Collaborative human-robot Order Picking solutions, more efficient picker routing policies are needed, since the complexity of coordinating multiple actors in the system increases significantly. The objective of these policies is to match human pickers and robot carriers to fulfill picking tasks, optimizing pick-rate and total tardiness of the orders. In this paper, we propose to formulate the order picking routing problem as a more general combinatorial optimization problem known as Two-sided Online Bipartite Matching. We present an end-to-end Deep Reinforcement Learning approach to optimize a combination of pick-rate and order tardiness, and to deal with the uncertainty of real-world warehouse environments. To extract and exploit spatial information from the environment, we devise three different Graph Neural Network architectures and empirically evaluate them on several scenarios of growing complexity in a simulation environment we developed. We show that all proposed methods significantly outperform greedy and more sophisticated heuristics, as well as non-GNN-based DRL approaches. Moreover, our methods exhibit good transferability properties, even when scaling up test problem instances to more than forty times the size of the ones the models were trained on. Code is available at: \\url{https://github.com/lbegnardi/DRL-TOBM-CPR}.",
        "bibtex": "@InProceedings{pmlr-v222-begnardi24a,\n  title = \t {Deep Reinforcement Learning for Two-sided Online Bipartite Matching in Collaborative Order Picking},\n  author =       {Begnardi, Luca and Baier, Hendrik and van Jaarsveld, Willem and Zhang, Yingqian},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {121--136},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/begnardi24a/begnardi24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/begnardi24a.html},\n  abstract = \t {As a growing number of warehouse operators are moving from human-only to Collaborative human-robot Order Picking solutions, more efficient picker routing policies are needed, since the complexity of coordinating multiple actors in the system increases significantly. The objective of these policies is to match human pickers and robot carriers to fulfill picking tasks, optimizing pick-rate and total tardiness of the orders. In this paper, we propose to formulate the order picking routing problem as a more general combinatorial optimization problem known as Two-sided Online Bipartite Matching. We present an end-to-end Deep Reinforcement Learning approach to optimize a combination of pick-rate and order tardiness, and to deal with the uncertainty of real-world warehouse environments. To extract and exploit spatial information from the environment, we devise three different Graph Neural Network architectures and empirically evaluate them on several scenarios of growing complexity in a simulation environment we developed. We show that all proposed methods significantly outperform greedy and more sophisticated heuristics, as well as non-GNN-based DRL approaches. Moreover, our methods exhibit good transferability properties, even when scaling up test problem instances to more than forty times the size of the ones the models were trained on. Code is available at: \\url{https://github.com/lbegnardi/DRL-TOBM-CPR}.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/begnardi24a/begnardi24a.pdf",
        "supp": "",
        "pdf_size": 641249,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16934238127946499916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Eindhoven University of Technology, The Netherlands; Eindhoven University of Technology, The Netherlands; Eindhoven University of Technology, The Netherlands; Eindhoven University of Technology, The Netherlands",
        "aff_domain": "tue.nl;tue.nl;tue.nl;tue.nl",
        "email": "tue.nl;tue.nl;tue.nl;tue.nl",
        "github": "https://github.com/ai-for-decision-making-tue/DRL-TOBM-CPR",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Eindhoven University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tue.nl",
        "aff_unique_abbr": "TU/e",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "title": "Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain",
        "site": "https://proceedings.mlr.press/v222/dutta24a.html",
        "author": "Parag Dutta; Kawin Mayilvaghanan; Pratyaksha Sinha; Ambedkar Dukkipati",
        "abstract": "Temporal Point Processes (TPP) play an important role in predicting or forecasting events. Although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. For instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. Similarly people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Na\u00efve approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. Our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. We demonstrate the effectiveness of our approach through extensive experiments on multiple datasets, showing that our model outperforms existing methods in terms of prediction metrics and computational efficiency. To the best of our knowledge, this is the first work that solves the problem of predicting event set intensities in the continuous time domain by using TPPs.",
        "bibtex": "@InProceedings{pmlr-v222-dutta24a,\n  title = \t {Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain},\n  author =       {Dutta, Parag and Mayilvaghanan, Kawin and Sinha, Pratyaksha and Dukkipati, Ambedkar},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {343--358},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/dutta24a/dutta24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/dutta24a.html},\n  abstract = \t {Temporal Point Processes (TPP) play an important role in predicting or forecasting events. Although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. For instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. Similarly people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Na\u00efve approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. Our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. We demonstrate the effectiveness of our approach through extensive experiments on multiple datasets, showing that our model outperforms existing methods in terms of prediction metrics and computational efficiency. To the best of our knowledge, this is the first work that solves the problem of predicting event set intensities in the continuous time domain by using TPPs.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/dutta24a/dutta24a.pdf",
        "supp": "",
        "pdf_size": 841718,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sRaQsCXcunYJ:scholar.google.com/&scioq=Deep+Representation+Learning+for+Prediction+of+Temporal+Event+Sets+in+the+Continuous+Time+Domain&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science (IISc), Bangalore, KA, IN - 560012; Department of Computer Science and Automation, Indian Institute of Science (IISc), Bangalore, KA, IN - 560012; Department of Computer Science and Automation, Indian Institute of Science (IISc), Bangalore, KA, IN - 560012; Department of Computer Science and Automation, Indian Institute of Science (IISc), Bangalore, KA, IN - 560012",
        "aff_domain": "IISC.AC.IN;IISC.AC.IN;IISC.AC.IN;IISC.AC.IN",
        "email": "IISC.AC.IN;IISC.AC.IN;IISC.AC.IN;IISC.AC.IN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Deep Traffic Benchmark: Aerial Perception and Driven Behavior Dataset",
        "site": "https://proceedings.mlr.press/v222/zhang24d.html",
        "author": "Guoxing Zhang; Qiuping Li; Yiming Liu; Zhanpeng Wang; Yuanqi Chen; Wenrui Cai; Weiye Zhang; Bingting Guo; Zhi Zeng; Jiasong Zhu",
        "abstract": "Predicting human driving behavior has always been an important area of autonomous driving research. Existing data on autonomous driving in this area is limited in both perspective and duration. For example, vehicles may block each other on the road, although data from vehicles behind them is useful for research. In addition, driving in this area is constrained by the road environment, and the host vehicle cannot observe the designated area for an extended period of time. To investigate the potential relationship between human driving behavior and traffic conditions, we provide a drone-collected video dataset, Deep Traffic, that includes: (1) aerial footage from a vertical perspective, (2) image and annotation capture for training vehicle destination detection and semantic segmentation model, (3) high-definition map data of the captured area, (4) development scripts for various features. Deep Traffic is the largest and most comprehensive dataset to date, covering both urban and high-speed areas. We believe that this benchmark dataset will greatly facilitate the development of drones to monitor traffic flow and study human driver behavior, and that the capacity of the traffic system is of great importance. All datasets and pre-training results can be downloaded from github project.",
        "bibtex": "@InProceedings{pmlr-v222-zhang24d,\n  title = \t {Deep Traffic Benchmark: {A}erial Perception and Driven Behavior Dataset},\n  author =       {Zhang, Guoxing and Li, Qiuping and Liu, Yiming and Wang, Zhanpeng and Chen, Yuanqi and Cai, Wenrui and Zhang, Weiye and Guo, Bingting and Zeng, Zhi and Zhu, Jiasong},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1670--1682},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhang24d/zhang24d.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhang24d.html},\n  abstract = \t {Predicting human driving behavior has always been an important area of autonomous driving research. Existing data on autonomous driving in this area is limited in both perspective and duration. For example, vehicles may block each other on the road, although data from vehicles behind them is useful for research. In addition, driving in this area is constrained by the road environment, and the host vehicle cannot observe the designated area for an extended period of time. To investigate the potential relationship between human driving behavior and traffic conditions, we provide a drone-collected video dataset, Deep Traffic, that includes: (1) aerial footage from a vertical perspective, (2) image and annotation capture for training vehicle destination detection and semantic segmentation model, (3) high-definition map data of the captured area, (4) development scripts for various features. Deep Traffic is the largest and most comprehensive dataset to date, covering both urban and high-speed areas. We believe that this benchmark dataset will greatly facilitate the development of drones to monitor traffic flow and study human driver behavior, and that the capacity of the traffic system is of great importance. All datasets and pre-training results can be downloaded from github project.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhang24d/zhang24d.pdf",
        "supp": "",
        "pdf_size": 4738197,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rL_bQOsKXoYJ:scholar.google.com/&scioq=Deep+Traffic+Benchmark:+Aerial+Perception+and+Driven+Behavior+Dataset&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China; Shenzhen University, Shenzhen, Guangdong, China",
        "aff_domain": "email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;szu.edu.cn",
        "email": "email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;email.szu.edu.cn;szu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Shenzhen University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.szu.edu.cn",
        "aff_unique_abbr": "SZU",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Uniformly Distributed Centers on a Hypersphere for Open Set Recognition",
        "site": "https://proceedings.mlr.press/v222/cevikalp24a.html",
        "author": "Hakan Cevikalp; Hasan Serhan Yavuz; Hasan Saribas",
        "abstract": "This study introduces a new approach for open set recognition, wherein we propose a novel method utilizing uniformly distributed centers on a hypersphere. Each class in the proposed method is represented by a center, and these centers and features of the deep learning architecture are jointly learned from the training data in an end-to-end fashion. We ensure that the centers lie on the boundary of a hypersphere whose center is positioned at the origin. The class-specific samples are compelled by the proposed loss function to be closer to their respective centers. In open set recognition scenarios, an additional loss term is employed to separate the background samples from the known class centers. The assignment of test samples to classes is based on the Euclidean distances calculated from the learned class centers. Experimental results show that the proposed method yields the state-of-the-art accuracies on open set recognition datasets.",
        "bibtex": "@InProceedings{pmlr-v222-cevikalp24a,\n  title = \t {Deep Uniformly Distributed Centers on a Hypersphere for Open Set Recognition},\n  author =       {Cevikalp, Hakan and Yavuz, Hasan Serhan and Saribas, Hasan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {217--230},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/cevikalp24a/cevikalp24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/cevikalp24a.html},\n  abstract = \t {This study introduces a new approach for open set recognition, wherein we propose a novel method utilizing uniformly distributed centers on a hypersphere. Each class in the proposed method is represented by a center, and these centers and features of the deep learning architecture are jointly learned from the training data in an end-to-end fashion. We ensure that the centers lie on the boundary of a hypersphere whose center is positioned at the origin. The class-specific samples are compelled by the proposed loss function to be closer to their respective centers. In open set recognition scenarios, an additional loss term is employed to separate the background samples from the known class centers. The assignment of test samples to classes is based on the Euclidean distances calculated from the learned class centers. Experimental results show that the proposed method yields the state-of-the-art accuracies on open set recognition datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/cevikalp24a/cevikalp24a.pdf",
        "supp": "",
        "pdf_size": 484218,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9472235767122328587&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Eskisehir Osmangazi University, Machine Learning and Computer Vision Laboratory, Eskisehir, Turkey; Eskisehir Osmangazi University, Machine Learning and Computer Vision Laboratory, Eskisehir, Turkey; Huawei Turkey R&D Center, Istanbul, Turkey",
        "aff_domain": "ogu.edu.tr;ogu.edu.tr;huawei.com",
        "email": "ogu.edu.tr;ogu.edu.tr;huawei.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Eskisehir Osmangazi University;Huawei",
        "aff_unique_dep": "Machine Learning and Computer Vision Laboratory;R&D Center",
        "aff_unique_url": ";https://www.huawei.com/tr",
        "aff_unique_abbr": ";Huawei Turkey",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Eskisehir;Istanbul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Turkey"
    },
    {
        "title": "Degree-based stratification of nodes in Graph Neural Networks",
        "site": "https://proceedings.mlr.press/v222/ali24a.html",
        "author": "Ameen Ali; Lior Wolf; Hakan Cevikalp",
        "abstract": "Despite much research, Graph Neural Networks (GNNs) still do not display the favorable scaling properties of other deep neural networks such as Convolutional Neural Networks and Transformers. Previous work has identified issues such as oversmoothing of the latent representation and have suggested solutions such as skip connections and sophisticated normalization schemes. Here, we propose a different approach that is based on a stratification of the graph nodes. We provide motivation that the nodes in a graph can be stratified into those with a low degree and those with a high degree and that the two groups are likely to behave differently. Based on this motivation, we modify the Graph Neural Network (GNN) architecture so that the weight matrices are learned, separately, for the nodes in each group. This simple-to-implement modification seems to improve performance across datasets and GNN methods. To verify that this increase in performance is not only due to the added capacity, we also perform the same modification for random splits of the nodes, which does not lead to any improvement.",
        "bibtex": "@InProceedings{pmlr-v222-ali24a,\n  title = \t {Degree-based stratification of nodes in Graph Neural Networks},\n  author =       {Ali, Ameen and Wolf, Lior and Cevikalp, Hakan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {15--27},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ali24a/ali24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ali24a.html},\n  abstract = \t {Despite much research, Graph Neural Networks (GNNs) still do not display the favorable scaling properties of other deep neural networks such as Convolutional Neural Networks and Transformers. Previous work has identified issues such as oversmoothing of the latent representation and have suggested solutions such as skip connections and sophisticated normalization schemes. Here, we propose a different approach that is based on a stratification of the graph nodes. We provide motivation that the nodes in a graph can be stratified into those with a low degree and those with a high degree and that the two groups are likely to behave differently. Based on this motivation, we modify the Graph Neural Network (GNN) architecture so that the weight matrices are learned, separately, for the nodes in each group. This simple-to-implement modification seems to improve performance across datasets and GNN methods. To verify that this increase in performance is not only due to the added capacity, we also perform the same modification for random splits of the nodes, which does not lead to any improvement.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ali24a/ali24a.pdf",
        "supp": "",
        "pdf_size": 344734,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0g2BH_1wfroJ:scholar.google.com/&scioq=Degree-based+stratification+of+nodes+in+Graph+Neural+Networks&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Tel Aviv, Israel; School of Computer Science, Tel Aviv, Israel; MLCV Laboratory, Eskisehir Osmangazi University, Eskisehir, Turkey",
        "aff_domain": "mail.tau.ac.il;mail.tau.ac.il;gmail.com",
        "email": "mail.tau.ac.il;mail.tau.ac.il;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Tel Aviv University;Eskisehir Osmangazi University",
        "aff_unique_dep": "School of Computer Science;MLCV Laboratory",
        "aff_unique_url": "https://www.tau.ac.il;",
        "aff_unique_abbr": "TAU;",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Tel Aviv;Eskisehir",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;Turkey"
    },
    {
        "title": "Detecting and Repairing Deviated Outputs of Compressed Models",
        "site": "https://proceedings.mlr.press/v222/li24b.html",
        "author": "Yichen Li; Qi Pang; Dongwei Xiao; Zhibo Liu; Shuai Wang",
        "abstract": "With the rapid development of deep learning and its pervasive usage on various low-power and resource-constrained devices, model compression methods are increasingly used to reduce the model size and computation cost. Despite the overall high test accuracy of the compressed models, our observation shows that an original model and its compressed version (e.g., via quantization) can have deviated prediction outputs on the same inputs. These behavior deviations on compressed models are undesirable, given that the compressed models may be used in reliability-critical scenarios such as automated manufacturing and robotics systems. Inspired by software engineering practices, this paper proposes CompD, a differential testing (DT)-based framework for detecting and repairing prediction deviations on compressed models and their plaintext versions. CompD treats original/compressed models as \u201cblack-box,\u201d thus offering an efficient method orthogonal to specific different compression schemes. Furthermore, CompD can leverage deviation-triggering inputs to finetune the compressed models, largely \u201crepairing\u201d their defects. Evaluations show that CompD can effectively test and repair common models compressed by different schemes.",
        "bibtex": "@InProceedings{pmlr-v222-li24b,\n  title = \t {Detecting and Repairing Deviated Outputs of Compressed Models},\n  author =       {Li, Yichen and Pang, Qi and Xiao, Dongwei and Liu, Zhibo and Wang, Shuai},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {707--722},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/li24b/li24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/li24b.html},\n  abstract = \t {With the rapid development of deep learning and its pervasive usage on various low-power and resource-constrained devices, model compression methods are increasingly used to reduce the model size and computation cost. Despite the overall high test accuracy of the compressed models, our observation shows that an original model and its compressed version (e.g., via quantization) can have deviated prediction outputs on the same inputs. These behavior deviations on compressed models are undesirable, given that the compressed models may be used in reliability-critical scenarios such as automated manufacturing and robotics systems. Inspired by software engineering practices, this paper proposes CompD, a differential testing (DT)-based framework for detecting and repairing prediction deviations on compressed models and their plaintext versions. CompD treats original/compressed models as \u201cblack-box,\u201d thus offering an efficient method orthogonal to specific different compression schemes. Furthermore, CompD can leverage deviation-triggering inputs to finetune the compressed models, largely \u201crepairing\u201d their defects. Evaluations show that CompD can effectively test and repair common models compressed by different schemes.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/li24b/li24b.pdf",
        "supp": "",
        "pdf_size": 463273,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hFr0Tp96d-4J:scholar.google.com/&scioq=Detecting+and+Repairing+Deviated+Outputs+of+Compressed+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "The Hong Kong University of Science and Technology; Carnegie Mellon University; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "aff_domain": "cse.ust.hk;andrew.cmu.edu;cse.ust.hk;cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;andrew.cmu.edu;cse.ust.hk;cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://www.cmu.edu",
        "aff_unique_abbr": "HKUST;CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Diffusion-based Visual Representation Learning for Medical Question Answering",
        "site": "https://proceedings.mlr.press/v222/bian24a.html",
        "author": "Dexin Bian; Xiaoru Wang; Meifang Li",
        "abstract": "Medical visual question answering (Med-VQA) aims to correctly answer the medical question based on the given image. One of the major challenges is the scarcity of large professional labeled datasets for training, which poses obstacles to feature extraction, especially for medical images. To overcome it, we propose a method to learn transferable visual representation based on conditional denoising diffusion probabilistic model(conditional DDPM).Specifically, we collate a large amount of unlabeled radiological images and train a conditional DDPM with the paradigm of auto-encoder to obtain a model which can extract high-level semantic information from medical images.The pre-trained model can be used as a well initialized visual feature extractor and can be easily adapt to any Med-VQA systems. We build our Med-VQA system follow the state-of-the-art Med-VQA architecture and replace the visual extractor with our pre-trained model.Our proposal method outperforms the state-of-the-art Med-VQA method on VQA-RAD and achieves comparable result on SLAKE.",
        "bibtex": "@InProceedings{pmlr-v222-bian24a,\n  title = \t {Diffusion-based Visual Representation Learning for Medical Question Answering},\n  author =       {Bian, Dexin and Wang, Xiaoru and Li, Meifang},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {169--184},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/bian24a/bian24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/bian24a.html},\n  abstract = \t {Medical visual question answering (Med-VQA) aims to correctly answer the medical question based on the given image. One of the major challenges is the scarcity of large professional labeled datasets for training, which poses obstacles to feature extraction, especially for medical images. To overcome it, we propose a method to learn transferable visual representation based on conditional denoising diffusion probabilistic model(conditional DDPM).Specifically, we collate a large amount of unlabeled radiological images and train a conditional DDPM with the paradigm of auto-encoder to obtain a model which can extract high-level semantic information from medical images.The pre-trained model can be used as a well initialized visual feature extractor and can be easily adapt to any Med-VQA systems. We build our Med-VQA system follow the state-of-the-art Med-VQA architecture and replace the visual extractor with our pre-trained model.Our proposal method outperforms the state-of-the-art Med-VQA method on VQA-RAD and achieves comparable result on SLAKE.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/bian24a/bian24a.pdf",
        "supp": "",
        "pdf_size": 1616805,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8159899656745290940&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/lluviosac/Diffusion-Based-MedVQA",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Distilling Influences to Mitigate Prediction Churn in Graph Neural Networks",
        "site": "https://proceedings.mlr.press/v222/roth24a.html",
        "author": "Andreas Roth; Thomas Liebig",
        "abstract": "Models with similar performances exhibit significant disagreement in the predictions of individual samples, referred to as prediction churn. Our work explores this phenomenon in graph neural networks by investigating differences between models differing only in their initializations in their utilized features for predictions. We propose a novel metric called Influence Difference (ID) to quantify the variation in reasons used by nodes across models by comparing their influence distribution. Additionally, we consider the differences between nodes with a stable and an unstable prediction, positing that both equally utilize different reasons and thus provide a meaningful gradient signal to closely match two models even when the predictions for nodes are similar. Based on our analysis, we propose to minimize this ID in Knowledge Distillation, a domain where a new model should closely match an established one. As an efficient approximation, we introduce DropDistillation (DD) that matches the output for a graph perturbed by edge deletions. Our empirical evaluation of six benchmark datasets for node classification validates the differences in utilized features. DD outperforms previous methods regarding prediction stability and overall performance in all considered Knowledge Distillation experiments.",
        "bibtex": "@InProceedings{pmlr-v222-roth24a,\n  title = \t {Distilling Influences to Mitigate Prediction Churn in Graph Neural Networks},\n  author =       {Roth, Andreas and Liebig, Thomas},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1151--1166},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/roth24a/roth24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/roth24a.html},\n  abstract = \t {Models with similar performances exhibit significant disagreement in the predictions of individual samples, referred to as prediction churn. Our work explores this phenomenon in graph neural networks by investigating differences between models differing only in their initializations in their utilized features for predictions. We propose a novel metric called Influence Difference (ID) to quantify the variation in reasons used by nodes across models by comparing their influence distribution. Additionally, we consider the differences between nodes with a stable and an unstable prediction, positing that both equally utilize different reasons and thus provide a meaningful gradient signal to closely match two models even when the predictions for nodes are similar. Based on our analysis, we propose to minimize this ID in Knowledge Distillation, a domain where a new model should closely match an established one. As an efficient approximation, we introduce DropDistillation (DD) that matches the output for a graph perturbed by edge deletions. Our empirical evaluation of six benchmark datasets for node classification validates the differences in utilized features. DD outperforms previous methods regarding prediction stability and overall performance in all considered Knowledge Distillation experiments.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/roth24a/roth24a.pdf",
        "supp": "",
        "pdf_size": 366960,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_JuogkkV9wQJ:scholar.google.com/&scioq=Distilling+Influences+to+Mitigate+Prediction+Churn+in+Graph+Neural+Networks&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "TU Dortmund University, Dortmund, Germany; TU Dortmund University, Dortmund, Germany + Lamarr Institute for Machine Learning and Artificial Intelligence, Germany",
        "aff_domain": "tu-dortmund.de;tu-dortmund.de",
        "email": "tu-dortmund.de;tu-dortmund.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "TU Dortmund University;Lamarr Institute for Machine Learning and Artificial Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-dortmund.de;",
        "aff_unique_abbr": "TUDO;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dortmund;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Domain Generalization with Interpolation Robustness",
        "site": "https://proceedings.mlr.press/v222/palakkadavath24a.html",
        "author": "Ragja Palakkadavath; Thanh Nguyen-Tang; Hung Le; Svetha Venkatesh; Sunil Gupta",
        "abstract": "Domain generalization (DG) uses multiple source (training) domains to learn a model that generalizes well to unseen domains. Existing approaches to DG need more scrutiny over (i) the ability to imagine data beyond the source domains and (ii) the ability to cope with the scarcity of training data. To address these shortcomings, we propose a novel framework - \\emph{interpolation robustness}, where we view each training domain as a point on a domain manifold and learn class-specific representations that are domain invariant across all interpolations between domains. We use this representation to propose a generic domain generalization approach that can be seamlessly combined with many state-of-the-art methods in DG. Through extensive experiments, we show that our approach can enhance the performance of several methods in the conventional and the limited training data setting.",
        "bibtex": "@InProceedings{pmlr-v222-palakkadavath24a,\n  title = \t {Domain Generalization with Interpolation Robustness},\n  author =       {Palakkadavath, Ragja and Nguyen-Tang, Thanh and Le, Hung and Venkatesh, Svetha and Gupta, Sunil},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1039--1054},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/palakkadavath24a/palakkadavath24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/palakkadavath24a.html},\n  abstract = \t {Domain generalization (DG) uses multiple source (training) domains to learn a model that generalizes well to unseen domains. Existing approaches to DG need more scrutiny over (i) the ability to imagine data beyond the source domains and (ii) the ability to cope with the scarcity of training data. To address these shortcomings, we propose a novel framework - \\emph{interpolation robustness}, where we view each training domain as a point on a domain manifold and learn class-specific representations that are domain invariant across all interpolations between domains. We use this representation to propose a generic domain generalization approach that can be seamlessly combined with many state-of-the-art methods in DG. Through extensive experiments, we show that our approach can enhance the performance of several methods in the conventional and the limited training data setting.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/palakkadavath24a/palakkadavath24a.pdf",
        "supp": "",
        "pdf_size": 2101680,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15433499348443027066&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Applied Artificial Intelligence Institute, Deakin University; Whiting School of Engineering, Johns Hopkins University + Applied Artificial Intelligence Institute, Deakin University; Applied Artificial Intelligence Institute, Deakin University; Applied Artificial Intelligence Institute, Deakin University; Applied Artificial Intelligence Institute, Deakin University",
        "aff_domain": "deakin.edu.au;cs.jhu.edu;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;cs.jhu.edu;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0;0",
        "aff_unique_norm": "Deakin University;Johns Hopkins University",
        "aff_unique_dep": "Applied Artificial Intelligence Institute;Whiting School of Engineering",
        "aff_unique_url": "https://www.deakin.edu.au;https://www.jhu.edu",
        "aff_unique_abbr": ";JHU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Baltimore",
        "aff_country_unique_index": "0;1+0;0;0;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "title": "Dynamic Offset Metric on Heterogeneous Information Networks for Cold-start Recommendation",
        "site": "https://proceedings.mlr.press/v222/liu24a.html",
        "author": "Mingshi Liu; Xiaoru Wang; Zhihong Yu; Fu Li",
        "abstract": "The cold-start problem poses a significant challenge in recommendation systems, particularly when interaction data is scarce. While meta-learning has shown promise in few-shot classification, its application to cold-start recommendations has mostly involved simple transplantations of generic approaches. The effectiveness of metric learning, a powerful meta-learning method, is hindered by differences in problem definition when applied to rating prediction. Heterogeneous information networks (HINs), as high-order graph structures, can capture valuable semantic information even in data-starved conditions. Efficient utilization of HINs can alleviate the cold-start dilemma. However, in the cold-start domain, there is a lack of dynamic node-level and semantic-level feature fusion schemes, resulting in the underutilization of complex information. This study addresses these issues by combining metric learning and HINs, proposing OMHIN (Dynamic Offset Metric approach to Heterogeneous Information Networks). Our approach transforms a direct similarity metric into an indirect metric to enhance model robustness. By flexibly applying one-dimensional convolution, OMHIN effectively integrates rich information from HINs while minimizing noise introduction. Experimental results on two datasets demonstrate that OMHIN achieves state-of-the-art performance in various scenarios, particularly in complex and challenging situations. It is especially suitable for sequence cold-start recommendations.",
        "bibtex": "@InProceedings{pmlr-v222-liu24a,\n  title = \t {Dynamic Offset Metric on Heterogeneous Information Networks for Cold-start Recommendation},\n  author =       {Liu, Mingshi and Wang, Xiaoru and Yu, Zhihong and Li, Fu},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {787--802},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/liu24a/liu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/liu24a.html},\n  abstract = \t {The cold-start problem poses a significant challenge in recommendation systems, particularly when interaction data is scarce. While meta-learning has shown promise in few-shot classification, its application to cold-start recommendations has mostly involved simple transplantations of generic approaches. The effectiveness of metric learning, a powerful meta-learning method, is hindered by differences in problem definition when applied to rating prediction. Heterogeneous information networks (HINs), as high-order graph structures, can capture valuable semantic information even in data-starved conditions. Efficient utilization of HINs can alleviate the cold-start dilemma. However, in the cold-start domain, there is a lack of dynamic node-level and semantic-level feature fusion schemes, resulting in the underutilization of complex information. This study addresses these issues by combining metric learning and HINs, proposing OMHIN (Dynamic Offset Metric approach to Heterogeneous Information Networks). Our approach transforms a direct similarity metric into an indirect metric to enhance model robustness. By flexibly applying one-dimensional convolution, OMHIN effectively integrates rich information from HINs while minimizing noise introduction. Experimental results on two datasets demonstrate that OMHIN achieves state-of-the-art performance in various scenarios, particularly in complex and challenging situations. It is especially suitable for sequence cold-start recommendations.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/liu24a/liu24a.pdf",
        "supp": "",
        "pdf_size": 514063,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16607816624846192488&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Intel China research center; Portland States University",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;intel.com;pdx.edu",
        "email": "bupt.edu.cn;bupt.edu.cn;intel.com;pdx.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Intel Corporation;Portland State University",
        "aff_unique_dep": ";Research Center;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.intel.cn;https://www.pdx.edu",
        "aff_unique_abbr": "BUPT;Intel;PSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Early Diagnosis of Alzheimer through Swin-Transformer-Based Deep Learning Framework using Sparse Diffusion Measures",
        "site": "https://proceedings.mlr.press/v222/tiwari24a.html",
        "author": "Abhishek Tiwari; Ananya Singhal; Saurabh J. Shigwan; Rajeev Kumar Singh",
        "abstract": "Alzheimer disease is one of the most common neuro-degenerative diseases, with an estimated 6.2 million cases in the United States. This research article investigates the potential of Transformer-based deep learning techniques to accelerate the processing of diffusion tensor imaging (DTI) measures and improve the early diagnosis of Alzheimer disease (AD) using sparse data. Diffusion Weighted Imaging (DWI) is a time-consuming process, with each diffusion direction taking between 2-5 minutes, and at least 40 diffusion directions are needed for routine clinical diagnosis, which needs scanning duration exceeding 3 hours for each patient. By leveraging the attention mechanism, our proposed model generates quantitative measures of fractional anisotropy (FA), axial diffusivity (AxD), and mean diffusivity (MD) using 5 and 21 diffusion directions, making it useful for clinical diagnosis through reduced scanning time of more than half. Our experimental results on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed model outperforms the traditional linear least square method, achieving accurate quantitative measurement of FA, AxD, and MD scores for early diagnosis of AD patients from healthy controls using sparse diffusion directions. Our analysis highlights the potential of Swin-Transformer attention-based deep learning framework to improve the early diagnosis and treatment of Alzheimer\u2019s disease.",
        "bibtex": "@InProceedings{pmlr-v222-tiwari24a,\n  title = \t {Early Diagnosis of Alzheimer through Swin-Transformer-Based Deep Learning Framework using Sparse Diffusion Measures},\n  author =       {Tiwari, Abhishek and Singhal, Ananya and Shigwan, Saurabh J. and Singh, Rajeev Kumar},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1369--1384},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/tiwari24a/tiwari24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/tiwari24a.html},\n  abstract = \t {Alzheimer disease is one of the most common neuro-degenerative diseases, with an estimated 6.2 million cases in the United States. This research article investigates the potential of Transformer-based deep learning techniques to accelerate the processing of diffusion tensor imaging (DTI) measures and improve the early diagnosis of Alzheimer disease (AD) using sparse data. Diffusion Weighted Imaging (DWI) is a time-consuming process, with each diffusion direction taking between 2-5 minutes, and at least 40 diffusion directions are needed for routine clinical diagnosis, which needs scanning duration exceeding 3 hours for each patient. By leveraging the attention mechanism, our proposed model generates quantitative measures of fractional anisotropy (FA), axial diffusivity (AxD), and mean diffusivity (MD) using 5 and 21 diffusion directions, making it useful for clinical diagnosis through reduced scanning time of more than half. Our experimental results on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed model outperforms the traditional linear least square method, achieving accurate quantitative measurement of FA, AxD, and MD scores for early diagnosis of AD patients from healthy controls using sparse diffusion directions. Our analysis highlights the potential of Swin-Transformer attention-based deep learning framework to improve the early diagnosis and treatment of Alzheimer\u2019s disease.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/tiwari24a/tiwari24a.pdf",
        "supp": "",
        "pdf_size": 5281377,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3479669317688400020&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Shiv Nadar Institution of Eminence, Delhi NCR, India; Shiv Nadar Institution of Eminence, Delhi NCR, India; Shiv Nadar Institution of Eminence, Delhi NCR, India; Shiv Nadar Institution of Eminence, Delhi NCR, India",
        "aff_domain": "snu.edu.in;snu.edu.in;snu.edu.in;snu.edu.in",
        "email": "snu.edu.in;snu.edu.in;snu.edu.in;snu.edu.in",
        "github": "https://github.com/AbhishekTiwari101/ACML2023-Early-Diagnosis-of-Alzheimer-via-Deep-Learning; https://github.com/reachananya/Early-diagnosis-of-Alzheimer-via-DL",
        "project": "adni.loni.usc.edu",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Shiv Nadar Institution of Eminence",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.snu.edu.in",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Delhi NCR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Edit-A-Video: Single Video Editing with Object-Aware Consistency",
        "site": "https://proceedings.mlr.press/v222/shin24a.html",
        "author": "Chaehun Shin; Heeseung Kim; Che Hyun Lee; Sang-gil Lee; Sungroh Yoon",
        "abstract": "With advancements in text-to-image (TTI) models, text-to-video (TTV) models have recently been introduced. Motivated by approaches on TTV models adapting from diffusion-based TTI models, we suggest the text-guided video editing framework given only a pretrained TTI model and a single",
        "bibtex": "@InProceedings{pmlr-v222-shin24a,\n  title = \t {{Edit-A-Video}: {S}ingle Video Editing with Object-Aware Consistency},\n  author =       {Shin, Chaehun and Kim, Heeseung and Lee, Che Hyun and Lee, Sang-gil and Yoon, Sungroh},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1215--1230},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/shin24a/shin24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/shin24a.html},\n  abstract = \t {With advancements in text-to-image (TTI) models, text-to-video (TTV) models have recently been introduced. Motivated by approaches on TTV models adapting from diffusion-based TTI models, we suggest the text-guided video editing framework given only a pretrained TTI model and a single",
        "pdf": "https://proceedings.mlr.press/v222/shin24a/shin24a.pdf",
        "supp": "",
        "pdf_size": 6972002,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8565510783367461378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://editavideo.github.io",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Efficient Medical Images Text Detection with Vision-Language Pre-training Approach",
        "site": "https://proceedings.mlr.press/v222/li24e.html",
        "author": "Tianyang Li; Jinxu Bai; Qingzhu Wang; Hanwen Xu",
        "abstract": "Text detection in medical images is a critical task, essential for automating the extraction of valuable information from diverse healthcare documents. Conventional text detection methods, predominantly based on segmentation, encounter substantial challenges when confronted with text-rich images, extreme aspect ratios, and multi-oriented text. In response to these complexities, this paper introduces an innovative text detection system aimed at enhancing its efficacy. Our proposed system comprises two fundamental components: the Efficient Feature Enhancement Module (EFEM) and the Multi-Scale Feature Fusion Module (MSFM), both serving as integral elements of the segmentation head. The EFEM incorporates a spatial attention mechanism to improve segmentation performance by introducing multi-level information. The MSFM merges features from the EFEM at different depths and scales to generate final segmentation features. In conjunction with our segmentation methodology, our post-processing module employs a differentiable binarization technique, facilitating adaptive threshold adjustment to enhance text detection precision. To further bolster accuracy and robustness, we introduce the integration of a vision-language pre-training model. Through extensive pretraining on large-scale visual language understanding tasks, this model amasses a wealth of rich visual and semantic representations. When seamlessly integrated with the segmentation module, the pretraining model effectively leverages its potent representation capabilities. Our proposed model undergoes rigorous evaluation on medical text image datasets, consistently demonstrating exceptional performance. Benchmark experiments reaffirm its efficacy.",
        "bibtex": "@InProceedings{pmlr-v222-li24e,\n  title = \t {Efficient Medical Images Text Detection with Vision-Language Pre-training Approach},\n  author =       {Li, Tianyang and Bai, Jinxu and Wang, Qingzhu and Xu, Hanwen},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {755--770},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/li24e/li24e.pdf},\n  url = \t {https://proceedings.mlr.press/v222/li24e.html},\n  abstract = \t {Text detection in medical images is a critical task, essential for automating the extraction of valuable information from diverse healthcare documents. Conventional text detection methods, predominantly based on segmentation, encounter substantial challenges when confronted with text-rich images, extreme aspect ratios, and multi-oriented text. In response to these complexities, this paper introduces an innovative text detection system aimed at enhancing its efficacy. Our proposed system comprises two fundamental components: the Efficient Feature Enhancement Module (EFEM) and the Multi-Scale Feature Fusion Module (MSFM), both serving as integral elements of the segmentation head. The EFEM incorporates a spatial attention mechanism to improve segmentation performance by introducing multi-level information. The MSFM merges features from the EFEM at different depths and scales to generate final segmentation features. In conjunction with our segmentation methodology, our post-processing module employs a differentiable binarization technique, facilitating adaptive threshold adjustment to enhance text detection precision. To further bolster accuracy and robustness, we introduce the integration of a vision-language pre-training model. Through extensive pretraining on large-scale visual language understanding tasks, this model amasses a wealth of rich visual and semantic representations. When seamlessly integrated with the segmentation module, the pretraining model effectively leverages its potent representation capabilities. Our proposed model undergoes rigorous evaluation on medical text image datasets, consistently demonstrating exceptional performance. Benchmark experiments reaffirm its efficacy.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/li24e/li24e.pdf",
        "supp": "",
        "pdf_size": 2089899,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZYfslOPfhjAJ:scholar.google.com/&scioq=Efficient+Medical+Images+Text+Detection+with+Vision-Language+Pre-training+Approach&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Computer Science, Northeast Electric Power University, Jilin, China+Jiangxi New Energy Technology Institute, Jiangxi, China; Computer Science, Northeast Electric Power University, Jilin, China; Computer Science, Northeast Electric Power University, Jilin, China; Computer Science, Northeast Electric Power University, Jilin, China",
        "aff_domain": "neepu.edu.cn;qq.com;qq.com;qq.com",
        "email": "neepu.edu.cn;qq.com;qq.com;qq.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Northeast Electric Power University;Jiangxi New Energy Technology Institute",
        "aff_unique_dep": "Computer Science;",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Jilin;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Empirical Study of Federated Unlearning: Efficiency and Effectiveness",
        "site": "https://proceedings.mlr.press/v222/nguyen24a.html",
        "author": "Thai-Hung Nguyen; Hong-Phuc Vu; Dung Thuy Nguyen; Tuan Minh Nguyen; Khoa D Doan; Kok-Seng Wong",
        "abstract": "The right to be forgotten (RTBF) is a concept that pertains to an individual\u2019s right to request the removal or deletion of their personal information when it is no longer necessary, relevant, or accurate for the purposes for which it was initially collected. Machine Learning (ML) models often rely on large, diverse datasets for optimal performance. Hence, when an individual exercises the RTBF, it can impact the ML model\u2019s performance and accuracy. In the context of Federated Learning (FL), where a server trains a model across multiple decentralized devices without moving data away from clients, implementing the RTBF in FL presents some unique challenges compared to traditional ML approaches. For instance, the decentralized nature makes it challenging to identify and remove specific user data from the model. Although various unlearning methods have been proposed in the literature, they have not been well investigated from the efficiency perspective. To fill this gap, this paper presents an empirical study to investigate the impacts of various unlearning methods. Our experiments are designed in diverse scenarios involving multiple communication and unlearning rounds using three datasets, MNIST, CIFAR-10, and CIFAR-100. We utilize backdoor attack and Cosine Similarity to assess the effectiveness of each unlearning method. The findings and insights from this research can be integrated into FL systems to enhance their overall performance and effectiveness. Our research codes are available on GitHub at \\url{https://github.com/sail-research/fed-unlearn}.",
        "bibtex": "@InProceedings{pmlr-v222-nguyen24a,\n  title = \t {Empirical Study of Federated Unlearning: {E}fficiency and Effectiveness},\n  author =       {Nguyen, Thai-Hung and Vu, Hong-Phuc and Nguyen, Dung Thuy and Nguyen, Tuan Minh and Doan, Khoa D and Wong, Kok-Seng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {959--974},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/nguyen24a/nguyen24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/nguyen24a.html},\n  abstract = \t {The right to be forgotten (RTBF) is a concept that pertains to an individual\u2019s right to request the removal or deletion of their personal information when it is no longer necessary, relevant, or accurate for the purposes for which it was initially collected. Machine Learning (ML) models often rely on large, diverse datasets for optimal performance. Hence, when an individual exercises the RTBF, it can impact the ML model\u2019s performance and accuracy. In the context of Federated Learning (FL), where a server trains a model across multiple decentralized devices without moving data away from clients, implementing the RTBF in FL presents some unique challenges compared to traditional ML approaches. For instance, the decentralized nature makes it challenging to identify and remove specific user data from the model. Although various unlearning methods have been proposed in the literature, they have not been well investigated from the efficiency perspective. To fill this gap, this paper presents an empirical study to investigate the impacts of various unlearning methods. Our experiments are designed in diverse scenarios involving multiple communication and unlearning rounds using three datasets, MNIST, CIFAR-10, and CIFAR-100. We utilize backdoor attack and Cosine Similarity to assess the effectiveness of each unlearning method. The findings and insights from this research can be integrated into FL systems to enhance their overall performance and effectiveness. Our research codes are available on GitHub at \\url{https://github.com/sail-research/fed-unlearn}.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/nguyen24a/nguyen24a.pdf",
        "supp": "",
        "pdf_size": 511248,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17205335165182690895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "College of Engineering & Computer Science, VinUniversity, Hanoi, Vietnam; College of Engineering & Computer Science, VinUniversity, Hanoi, Vietnam; VinUni-Illinois Smart Health Center, VinUniversity, Hanoi, Vietnam; College of Engineering & Computer Science, VinUniversity, Hanoi, Vietnam; College of Engineering & Computer Science, VinUniversity, Hanoi, Vietnam; VinUni-Illinois Smart Health Center, VinUniversity, Hanoi, Vietnam+College of Engineering & Computer Science, VinUniversity, Hanoi, Vietnam",
        "aff_domain": "vinuni.edu.vn;vinuni.edu.vn;vinuni.edu.vn;vinuni.edu.vn;gmail.com;vinuni.edu.vn",
        "email": "vinuni.edu.vn;vinuni.edu.vn;vinuni.edu.vn;vinuni.edu.vn;gmail.com;vinuni.edu.vn",
        "github": "https://github.com/sail-research/fed-unlearn",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0+0",
        "aff_unique_norm": "VinUniversity",
        "aff_unique_dep": "College of Engineering & Computer Science",
        "aff_unique_url": "https://vinuni.edu.vn",
        "aff_unique_abbr": "VinUni",
        "aff_campus_unique_index": "0;0;0;0;0;0+0",
        "aff_campus_unique": "Hanoi",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "Vietnam"
    },
    {
        "title": "Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training",
        "site": "https://proceedings.mlr.press/v222/deng24a.html",
        "author": "Zihao Deng; Benjamin Ghaemmaghami; Ashish Kumar Singh; Benjamin Cho; Leo Orshansky; Mattan Erez; Michael Orshansky",
        "abstract": "Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged. Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors by showing that MLET creates an adaptive update mechanism modulated by the singular vectors of embeddings. When tested on multiple state-of-the-art recommendation models for click-through rate (CTR) prediction tasks, MLET consistently produces better models, especially for rare items. At constant model quality, MLET allows embedding dimension, and model size, reduction by up to 16x, and 5.8x on average, across the models.",
        "bibtex": "@InProceedings{pmlr-v222-deng24a,\n  title = \t {Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training},\n  author =       {Deng, Zihao and Ghaemmaghami, Benjamin and Singh, Ashish Kumar and Cho, Benjamin and Orshansky, Leo and Erez, Mattan and Orshansky, Michael},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {263--278},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/deng24a/deng24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/deng24a.html},\n  abstract = \t {Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged. Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors by showing that MLET creates an adaptive update mechanism modulated by the singular vectors of embeddings. When tested on multiple state-of-the-art recommendation models for click-through rate (CTR) prediction tasks, MLET consistently produces better models, especially for rare items. At constant model quality, MLET allows embedding dimension, and model size, reduction by up to 16x, and 5.8x on average, across the models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/deng24a/deng24a.pdf",
        "supp": "",
        "pdf_size": 2854610,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bzIsLHzqqugJ:scholar.google.com/&scioq=Enhancing+Cross-Category+Learning+in+Recommendation+Systems+with+Multi-Layer+Embedding+Training&hl=en&as_sdt=0,14",
        "gs_version_total": 4,
        "aff": "The University of Texas at Austin; The University of Texas at Austin; E2open; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;gmail.com;utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;gmail.com;utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin;E2open",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.e2open.com",
        "aff_unique_abbr": "UT Austin;E2open",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Enhancing Model Generalization of Cervical Fluid-Based Cell Detection through Causal Feature Extraction:A Novel Method",
        "site": "https://proceedings.mlr.press/v222/pan24a.html",
        "author": "Qiao Pan; Bin Yang; Dehua Chen; Mei Wang",
        "abstract": "Cervical cancer is the most common gynecologic malignancy, and in clinical practice, cervical cancer is best treated if it is detected at an early stage. Thinprep Cytologic Test (TCT) is the best early detection method for cervical cancer as determined by the WHO. As the coverage of early detection of cervical cancer increases, the number of samples in hospitals increases annually, and the pressure on the pathologists to read the cytological images increases, which easily leads to an increase in the rate of misdiagnosis and missed diagnosis. Therefore, automatic detection of abnormal cells in cervical cytology images of cervical fluid using deep learning techniques has become a hot research topic today. However, existing deep learning models for cell detection often collect a single data source from a medical institution for construction. Different medical institutions have different equipment and staining methods, and the accuracy, magnification, and staining results of the images obtained will be different. As a result, the application performance of the model in different medical institution data is not good, and there is a problem of domain shift. To address these problems, this paper proposes a method for cervical fluid-based cell detection based on causal feature extraction. The method is based on the one-stage detection model RetinaNet, and incorporates causal autoencoder to learn the invariant causal feature representation from data. It reduces the impact of task-irrelevant feature representations, reduces the variability of feature distributions in different datasets, and effectively solves the domain shift problem. The addition of deformable convolution and attention mechanism enhances the feature extraction capability for foreground categories with variable shapes in cervical fluid-based pathology images. This reduces the impact of possible strong correlation between background features and goal cells, and reduces the interference of the foreground categories by fading and lack of brightness in the staining. The generalization ability of the model is improved, which makes the model better applicable to different medical institutions. The experimental results show that the method in this paper not only improves the accuracy of the model detection, but also verifies its good generalization effect on different datasets.",
        "bibtex": "@InProceedings{pmlr-v222-pan24a,\n  title = \t {Enhancing Model Generalization of Cervical Fluid-Based Cell Detection through Causal Feature Extraction:{A} Novel Method},\n  author =       {Pan, Qiao and Yang, Bin and Chen, Dehua and Wang, Mei},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1055--1070},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/pan24a/pan24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/pan24a.html},\n  abstract = \t {Cervical cancer is the most common gynecologic malignancy, and in clinical practice, cervical cancer is best treated if it is detected at an early stage. Thinprep Cytologic Test (TCT) is the best early detection method for cervical cancer as determined by the WHO. As the coverage of early detection of cervical cancer increases, the number of samples in hospitals increases annually, and the pressure on the pathologists to read the cytological images increases, which easily leads to an increase in the rate of misdiagnosis and missed diagnosis. Therefore, automatic detection of abnormal cells in cervical cytology images of cervical fluid using deep learning techniques has become a hot research topic today. However, existing deep learning models for cell detection often collect a single data source from a medical institution for construction. Different medical institutions have different equipment and staining methods, and the accuracy, magnification, and staining results of the images obtained will be different. As a result, the application performance of the model in different medical institution data is not good, and there is a problem of domain shift. To address these problems, this paper proposes a method for cervical fluid-based cell detection based on causal feature extraction. The method is based on the one-stage detection model RetinaNet, and incorporates causal autoencoder to learn the invariant causal feature representation from data. It reduces the impact of task-irrelevant feature representations, reduces the variability of feature distributions in different datasets, and effectively solves the domain shift problem. The addition of deformable convolution and attention mechanism enhances the feature extraction capability for foreground categories with variable shapes in cervical fluid-based pathology images. This reduces the impact of possible strong correlation between background features and goal cells, and reduces the interference of the foreground categories by fading and lack of brightness in the staining. The generalization ability of the model is improved, which makes the model better applicable to different medical institutions. The experimental results show that the method in this paper not only improves the accuracy of the model detection, but also verifies its good generalization effect on different datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/pan24a/pan24a.pdf",
        "supp": "",
        "pdf_size": 1922750,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14508433923381085829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China",
        "aff_domain": "dhu.edu.cn;126.com;dhu.edu.cn;dhu.edu.cn",
        "email": "dhu.edu.cn;126.com;dhu.edu.cn;dhu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Donghua University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://www.dhu.edu.cn",
        "aff_unique_abbr": "Donghua",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Equilibrium Point Learning",
        "site": "https://proceedings.mlr.press/v222/baik24a.html",
        "author": "Dowoo Baik; Ji Won Yoon",
        "abstract": "We present a novel approach, Equilibrium Point Learning (EPL), for training the deep equilibrium model (DEQ). In this method, the equilibrium point of the DEQ serves as the learnable parameters. Notably, the DEQ parameters encapsulate the learning algorithm itself and remain fixed. Consequently, by exploring the parameter space, we can discover a more efficient learning algorithm without relying on conventional techniques such as backpropagation or Q-learning. In this paper, we adopt an evolutionary approach inspired by biological neurons to evolve the DEQ model parameters. Initially, we examine the physical dynamics of neurons at the molecular level and translate them into a dynamical system representation. Subsequently, we formulate a deep implicit layer that is mathematically proven to possess an equilibrium point. The energy function of the implicit layer is defined using a quadratic form augmented with entropy and momentum terms. Given the resemblance between the dynamics of the deep implicit layer and the principles of physics and chemistry, it can effectively capture the biomodel of systems biology and the neural model of spiking neural networks (SNNs). This equivalence enables us to define the implicit layer of the DEQ, allowing for seamless integration with existing artificial neural networks (ANNs). Finally, we employ HyperNEAT to evolve the parameters of the dynamical system. Through our experiments, we observe a consistent improvement in learning efficiency, with each successive generation exhibiting a 0.2% increase in learning speed per generation.",
        "bibtex": "@InProceedings{pmlr-v222-baik24a,\n  title = \t {Equilibrium Point Learning},\n  author =       {Baik, Dowoo and Yoon, Ji Won},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {74--89},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/baik24a/baik24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/baik24a.html},\n  abstract = \t {We present a novel approach, Equilibrium Point Learning (EPL), for training the deep equilibrium model (DEQ). In this method, the equilibrium point of the DEQ serves as the learnable parameters. Notably, the DEQ parameters encapsulate the learning algorithm itself and remain fixed. Consequently, by exploring the parameter space, we can discover a more efficient learning algorithm without relying on conventional techniques such as backpropagation or Q-learning. In this paper, we adopt an evolutionary approach inspired by biological neurons to evolve the DEQ model parameters. Initially, we examine the physical dynamics of neurons at the molecular level and translate them into a dynamical system representation. Subsequently, we formulate a deep implicit layer that is mathematically proven to possess an equilibrium point. The energy function of the implicit layer is defined using a quadratic form augmented with entropy and momentum terms. Given the resemblance between the dynamics of the deep implicit layer and the principles of physics and chemistry, it can effectively capture the biomodel of systems biology and the neural model of spiking neural networks (SNNs). This equivalence enables us to define the implicit layer of the DEQ, allowing for seamless integration with existing artificial neural networks (ANNs). Finally, we employ HyperNEAT to evolve the parameters of the dynamical system. Through our experiments, we observe a consistent improvement in learning efficiency, with each successive generation exhibiting a 0.2% increase in learning speed per generation.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/baik24a/baik24a.pdf",
        "supp": "",
        "pdf_size": 2758099,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Wfmz99Y6nHoJ:scholar.google.com/&scioq=Equilibrium+Point+Learning&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Naval Academy; Korea University",
        "aff_domain": "gmail.com;korea.ac.kr",
        "email": "gmail.com;korea.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "United States Naval Academy;Korea University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usna.edu;https://www.korea.ac.kr",
        "aff_unique_abbr": "USNA;KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "Estimation of Counterfactual Interventions under Uncertainties",
        "site": "https://proceedings.mlr.press/v222/weilbach24a.html",
        "author": "Juliane Weilbach; Sebastian Gerwinn; Melih Kandemir; Martin Fraenzle",
        "abstract": "Counterfactual analysis is intuitively performed by humans on a daily basis eg. \u201dWhat should I have done differently to get the loan approved?\u201d. Such counterfactual questions also steer the formulation of scientific hypotheses. More formally it provides insights about potential improvements of a system by inferring the effects of hypothetical interventions into a past observation of the system\u2019s behaviour which plays a prominent role in a variety of industrial applications. Due to the hypothetical nature of such analysis, counterfactual distributions are inherently ambiguous. This ambiguity is particularly challenging in continuous settings in which a continuum of explanations exist for the same observation. In this paper, we address this problem by following a hierarchical Bayesian approach which explicitly models such uncertainty. In particular, we derive counterfactual distributions for a Bayesian Warped Gaussian Process thereby allowing for non-Gaussian distributions and non-additive noise. We illustrate the properties of our approach on a synthetic and on a semi-synthetic example and show its performance when used within an algorithmic recourse downstream task.",
        "bibtex": "@InProceedings{pmlr-v222-weilbach24a,\n  title = \t {Estimation of Counterfactual Interventions under Uncertainties},\n  author =       {Weilbach, Juliane and Gerwinn, Sebastian and Kandemir, Melih and Fraenzle, Martin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1463--1478},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/weilbach24a/weilbach24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/weilbach24a.html},\n  abstract = \t {Counterfactual analysis is intuitively performed by humans on a daily basis eg. \u201dWhat should I have done differently to get the loan approved?\u201d. Such counterfactual questions also steer the formulation of scientific hypotheses. More formally it provides insights about potential improvements of a system by inferring the effects of hypothetical interventions into a past observation of the system\u2019s behaviour which plays a prominent role in a variety of industrial applications. Due to the hypothetical nature of such analysis, counterfactual distributions are inherently ambiguous. This ambiguity is particularly challenging in continuous settings in which a continuum of explanations exist for the same observation. In this paper, we address this problem by following a hierarchical Bayesian approach which explicitly models such uncertainty. In particular, we derive counterfactual distributions for a Bayesian Warped Gaussian Process thereby allowing for non-Gaussian distributions and non-additive noise. We illustrate the properties of our approach on a synthetic and on a semi-synthetic example and show its performance when used within an algorithmic recourse downstream task.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/weilbach24a/weilbach24a.pdf",
        "supp": "",
        "pdf_size": 3743856,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7159682501477480882&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Bosch Center for Artificial Intelligence; Bosch Center for Artificial Intelligence; University of Southern Denmark; Carl von Ossietzky University of Oldenburg",
        "aff_domain": "de.bosch.com;de.bosch.com;imada.sdu.dk;informatik.uni-oldenburg.de",
        "email": "de.bosch.com;de.bosch.com;imada.sdu.dk;informatik.uni-oldenburg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Bosch Center for Artificial Intelligence;University of Southern Denmark;Carl von Ossietzky University of Oldenburg",
        "aff_unique_dep": "Center for Artificial Intelligence;;",
        "aff_unique_url": "https://www.bosch-ai.com;https://www.sdu.dk;https://www.uol.de",
        "aff_unique_abbr": "BCAI;SDU;UvO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Germany;Denmark"
    },
    {
        "title": "Evolutionary Neural Architecture Search for Multivariate Time Series Forecasting",
        "site": "https://proceedings.mlr.press/v222/liang24a.html",
        "author": "Zixuan Liang; Yanan Sun",
        "abstract": "Multivariate time series forecasting is of great importance in a diverse range of domains. In recent years, a variety of spatial-temporal graph neural networks (STGNNs) have been proposed to address this task and achieved promising results. However, these networks are typically handcrafted and require extensive human expertise. Additionally, the temporal and spatial dependencies hidden within different scenarios vary, making it difficult for them to adapt to different scenarios. In this paper, we propose an evolutionary neural architecture search framework, entitled EMTSF, for automated STGNN design. Specifically, we employ fine-grained neural architecture search into both the spatial convolution module and the temporal convolution module. For the spatial convolution search space, various feature filtering and neighbor aggregation operations are employed to find the most suitable message-passing mechanism for modeling the spatial dependencies. For the temporal convolution search space, gated temporal convolutions with different kernel sizes are utilized to best capture temporal dependencies with various ranges. The spatial convolution module and temporal convolution module are jointly optimized with the proposed evolutionary search algorithm to heuristically identify the optimal STGNN architecture. Extensive experiments on four commonly used benchmark datasets show EMTSF achieves promising performance compared with the state-of-the-art methods, which demonstrates the effectiveness of the proposed framework.",
        "bibtex": "@InProceedings{pmlr-v222-liang24a,\n  title = \t {Evolutionary Neural Architecture Search for Multivariate Time Series Forecasting},\n  author =       {Liang, Zixuan and Sun, Yanan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {771--786},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/liang24a/liang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/liang24a.html},\n  abstract = \t {Multivariate time series forecasting is of great importance in a diverse range of domains. In recent years, a variety of spatial-temporal graph neural networks (STGNNs) have been proposed to address this task and achieved promising results. However, these networks are typically handcrafted and require extensive human expertise. Additionally, the temporal and spatial dependencies hidden within different scenarios vary, making it difficult for them to adapt to different scenarios. In this paper, we propose an evolutionary neural architecture search framework, entitled EMTSF, for automated STGNN design. Specifically, we employ fine-grained neural architecture search into both the spatial convolution module and the temporal convolution module. For the spatial convolution search space, various feature filtering and neighbor aggregation operations are employed to find the most suitable message-passing mechanism for modeling the spatial dependencies. For the temporal convolution search space, gated temporal convolutions with different kernel sizes are utilized to best capture temporal dependencies with various ranges. The spatial convolution module and temporal convolution module are jointly optimized with the proposed evolutionary search algorithm to heuristically identify the optimal STGNN architecture. Extensive experiments on four commonly used benchmark datasets show EMTSF achieves promising performance compared with the state-of-the-art methods, which demonstrates the effectiveness of the proposed framework.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/liang24a/liang24a.pdf",
        "supp": "",
        "pdf_size": 828880,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8066365971222629570&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science, Sichuan University; College of Computer Science, Sichuan University",
        "aff_domain": "stu.scu.edu.cn;scu.edu.cn",
        "email": "stu.scu.edu.cn;scu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sichuan University",
        "aff_unique_dep": "College of Computer Science",
        "aff_unique_url": "https://www.scu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Facto-CNN: Memory-Efficient CNN Training with Low-rank Tensor Factorization and Lossy Tensor Compression",
        "site": "https://proceedings.mlr.press/v222/lee24b.html",
        "author": "Seungtae Lee; Jonghwan Ko; Seokin Hong",
        "abstract": "Convolutional neural networks (CNNs) are becoming deeper and wider to achieve higher accuracy and lower loss, significantly expanding the computational resources. Especially, training CNN models extensively consumes memory mainly due to storing intermediate feature maps generated in the forward-propagation for calculating the gradient in the backpropagation. The memory usage of the CNN model training escalates with the increase in batch size and the complexity of the model. Therefore, a lightweight training method is essential, especially when the computational resources are limited. In this paper, we propose a CNN training mechanism called Facto-CNN, leveraging low-rank tensor factorization and lossy tensor compression to reduce the memory usage required in training the CNN models. Facto-CNN factorizes the weight tensors of convolutional and fully-connected layers and then only updates one of the factorized tensors for each layer, dramatically reducing the feature map size stored in the memory. To further reduce memory consumption, Facto-CNN compresses the feature maps with a simple lossy compression technique that exploits the value similarity in the feature maps. Our experimental evaluation demonstrates that Facto-CNN reduces the memory usage for storing the feature maps by 68-93% with a trivial accuracy degradation when training the CNN models.",
        "bibtex": "@InProceedings{pmlr-v222-lee24b,\n  title = \t {{Facto-CNN}: {M}emory-Efficient {CNN} Training with Low-rank Tensor Factorization and Lossy Tensor Compression},\n  author =       {Lee, Seungtae and Ko, Jonghwan and Hong, Seokin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {662--677},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/lee24b/lee24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/lee24b.html},\n  abstract = \t {Convolutional neural networks (CNNs) are becoming deeper and wider to achieve higher accuracy and lower loss, significantly expanding the computational resources. Especially, training CNN models extensively consumes memory mainly due to storing intermediate feature maps generated in the forward-propagation for calculating the gradient in the backpropagation. The memory usage of the CNN model training escalates with the increase in batch size and the complexity of the model. Therefore, a lightweight training method is essential, especially when the computational resources are limited. In this paper, we propose a CNN training mechanism called Facto-CNN, leveraging low-rank tensor factorization and lossy tensor compression to reduce the memory usage required in training the CNN models. Facto-CNN factorizes the weight tensors of convolutional and fully-connected layers and then only updates one of the factorized tensors for each layer, dramatically reducing the feature map size stored in the memory. To further reduce memory consumption, Facto-CNN compresses the feature maps with a simple lossy compression technique that exploits the value similarity in the feature maps. Our experimental evaluation demonstrates that Facto-CNN reduces the memory usage for storing the feature maps by 68-93% with a trivial accuracy degradation when training the CNN models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/lee24b/lee24b.pdf",
        "supp": "",
        "pdf_size": 1948056,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18325975134231053000&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Sungkyunkwan University, Suwon, Republic of Korea; Sungkyunkwan University, Suwon, Republic of Korea; Sungkyunkwan University, Suwon, Republic of Korea",
        "aff_domain": "g.skku.edu;skku.edu;skku.edu",
        "email": "g.skku.edu;skku.edu;skku.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sungkyunkwan University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.sungkyunkwan.edu",
        "aff_unique_abbr": "SKKU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Suwon",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Faster Target Encirclement with Utilization of Obstacles via Multi-Agent Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v222/zheng24b.html",
        "author": "Yuxi Zheng; Yongjun Zhang; Chenran Zhao; Huanhuan Yang; Tongyue Li; Qianying Ouyang; Ying Chen",
        "abstract": "Multi-agent encirclement refers to controlling multiple agents to restrict the movement of a target and surround it with a specific formation. However, two challenges remain: encirclement in obstacle scenarios and encirclement of a faster target. In obstacle scenarios, we propose the utilization of obstacles for facilitating encirclement and introduce the concept of contributing angle to quantify the contribution of agents and obstacles, which enables agents to effectively utilize obstacles while mitigating the credit assignment problem. To address the challenge of encircling a faster target, we propose a two-stage encirclement method inspired by lions\u2019 hunting strategy, effectively preventing target escape. We design the reward function based on the contributing angle and the lion encirclement method, integrating it with the Multi-Agent Deep Deterministic Policy Gradient (MADDPG). The simulation results demonstrate that our method can utilize obstacles to complete encirclement and has a higher success rate. In some conditions with insufficient numbers of agents, our methods can still accomplish the task. Ablation experiments are conducted to verify the effectiveness of the contributing angle and the lion encirclement method respectively.",
        "bibtex": "@InProceedings{pmlr-v222-zheng24b,\n  title = \t {Faster Target Encirclement with Utilization of Obstacles via Multi-Agent Reinforcement Learning},\n  author =       {Zheng, Yuxi and Zhang, Yongjun and Zhao, Chenran and Yang, Huanhuan and Li, Tongyue and Ouyang, Qianying and Chen, Ying},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1731--1746},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zheng24b/zheng24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zheng24b.html},\n  abstract = \t {Multi-agent encirclement refers to controlling multiple agents to restrict the movement of a target and surround it with a specific formation. However, two challenges remain: encirclement in obstacle scenarios and encirclement of a faster target. In obstacle scenarios, we propose the utilization of obstacles for facilitating encirclement and introduce the concept of contributing angle to quantify the contribution of agents and obstacles, which enables agents to effectively utilize obstacles while mitigating the credit assignment problem. To address the challenge of encircling a faster target, we propose a two-stage encirclement method inspired by lions\u2019 hunting strategy, effectively preventing target escape. We design the reward function based on the contributing angle and the lion encirclement method, integrating it with the Multi-Agent Deep Deterministic Policy Gradient (MADDPG). The simulation results demonstrate that our method can utilize obstacles to complete encirclement and has a higher success rate. In some conditions with insufficient numbers of agents, our methods can still accomplish the task. Ablation experiments are conducted to verify the effectiveness of the contributing angle and the lion encirclement method respectively.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zheng24b/zheng24b.pdf",
        "supp": "",
        "pdf_size": 10724025,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LAOlQ9lQ5L4J:scholar.google.com/&scioq=Faster+Target+Encirclement+with+Utilization+of+Obstacles+via+Multi-Agent+Reinforcement+Learning&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "FasterVoxelPose+: Fast and Accurate Voxel-based 3D Human Pose Estimation by Depth-wise Projection Decay",
        "site": "https://proceedings.mlr.press/v222/zhuang24a.html",
        "author": "Zonghuang Zhuang; Yue Zhou",
        "abstract": "In terms of multi-person multi-view 3D pose estimation, voxel-based methods gain promising accuracy by directly manipulating features in 3D space. Since their high computational cost prevents them from practical applications, Faster VoxelPose was proposed to address this complication by re-projecting the 3D feature volume onto coordinate planes, which greatly improved the efficiency of the model. However, it suffers from an obvious performance drop, especially when there are fewer cameras. In this paper, we propose a more accurate real-time 3D pose estimation method, FasterVoxelPose+, to address the above problem. We have made two improvements to the previous methods. First, we propose a novel method for constructing voxel feature volume called Depth-wise Projection Decay (DPD). It introduces extra depth information to the projection to alleviate depth ambiguity. Second, we design an Encoder-Decoder Network for processing the re-projected voxel features to further push up the performance of the model. Our method obtains 17.42mm MPJPE on Panoptic with real-time speed and can be easily used in other voxel-based models.",
        "bibtex": "@InProceedings{pmlr-v222-zhuang24a,\n  title = \t {{FasterVoxelPose+}: {F}ast and Accurate Voxel-based {3D} Human Pose Estimation by Depth-wise Projection Decay},\n  author =       {Zhuang, Zonghuang and Zhou, Yue},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1763--1778},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhuang24a/zhuang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhuang24a.html},\n  abstract = \t {In terms of multi-person multi-view 3D pose estimation, voxel-based methods gain promising accuracy by directly manipulating features in 3D space. Since their high computational cost prevents them from practical applications, Faster VoxelPose was proposed to address this complication by re-projecting the 3D feature volume onto coordinate planes, which greatly improved the efficiency of the model. However, it suffers from an obvious performance drop, especially when there are fewer cameras. In this paper, we propose a more accurate real-time 3D pose estimation method, FasterVoxelPose+, to address the above problem. We have made two improvements to the previous methods. First, we propose a novel method for constructing voxel feature volume called Depth-wise Projection Decay (DPD). It introduces extra depth information to the projection to alleviate depth ambiguity. Second, we design an Encoder-Decoder Network for processing the re-projected voxel features to further push up the performance of the model. Our method obtains 17.42mm MPJPE on Panoptic with real-time speed and can be easily used in other voxel-based models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhuang24a/zhuang24a.pdf",
        "supp": "",
        "pdf_size": 1402278,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10103784732706850005&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Automation, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; Department of Automation, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Automation, School of Electronic Information and Electrical Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Federated Learning with Uncertainty via Distilled Predictive Distributions",
        "site": "https://proceedings.mlr.press/v222/bhatt24a.html",
        "author": "Shrey Bhatt; Aishwarya Gupta; Piyush Rai",
        "abstract": "Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require sending the whole posterior distribution of the parameters from each client to the server but only the PPD in the distilled form as a deep neural network. In addition, when making predictions at test time, it does not require computationally expensive Monte-Carlo averaging over the posterior distribution because our approach always maintains the PPD in form a single deep neural network. Moreover, our approach does not make any restrictive assumptions, such as the form of the clients\u2019 posterior distributions, or of their PPDs. We evaluate our approach on classification in federated setting, as well as active learning and OOD detection in federated settings, on which our approach outperforms various existing federated learning baselines.",
        "bibtex": "@InProceedings{pmlr-v222-bhatt24a,\n  title = \t {Federated Learning with Uncertainty via Distilled Predictive Distributions},\n  author =       {Bhatt, Shrey and Gupta, Aishwarya and Rai, Piyush},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {153--168},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/bhatt24a/bhatt24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/bhatt24a.html},\n  abstract = \t {Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require sending the whole posterior distribution of the parameters from each client to the server but only the PPD in the distilled form as a deep neural network. In addition, when making predictions at test time, it does not require computationally expensive Monte-Carlo averaging over the posterior distribution because our approach always maintains the PPD in form a single deep neural network. Moreover, our approach does not make any restrictive assumptions, such as the form of the clients\u2019 posterior distributions, or of their PPDs. We evaluate our approach on classification in federated setting, as well as active learning and OOD detection in federated settings, on which our approach outperforms various existing federated learning baselines.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/bhatt24a/bhatt24a.pdf",
        "supp": "",
        "pdf_size": 408440,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2048040319752805746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Eightfold AI; IIT Kanpur; IIT Kanpur",
        "aff_domain": "gmail.com;cse.iitk.ac.in;cse.iitk.ac.in",
        "email": "gmail.com;cse.iitk.ac.in;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Eightfold AI;Indian Institute of Technology Kanpur",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.eightfold.ai;https://www.iitk.ac.in",
        "aff_unique_abbr": "Eightfold AI;IITK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Kanpur",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Folded Hamiltonian Monte Carlo for Bayesian Generative Adversarial Networks",
        "site": "https://proceedings.mlr.press/v222/pourshahrokhi24a.html",
        "author": "Narges Pourshahrokhi; Yunpeng Li; Samaneh Kouchaki; Payam Barnaghi",
        "abstract": "Probabilistic modelling on Generative Adversarial Networks (GANs) within the Bayesian framework has shown success in estimating the complex distribution in literature. In this paper, we develop a Bayesian formulation for unsupervised and semi-supervised GAN learning. Specifically, we propose Folded Hamiltonian Monte Carlo (F-HMC) methods within this framework to learn the distributions over the parameters of the generators and discriminators. We show that the F-HMC efficiently approximates multi-modal and high dimensional data when combined with Bayesian GANs. Its composition improves run time and test error in generating diverse samples. Experimental results with high-dimensional synthetic multi-modal data and natural image benchmarks, including CIFAR-10, SVHN and ImageNet, show that F-HMC outperforms the state-of-the-art methods in terms of test error, run times per epoch, inception score and Frechet Inception Distance scores.",
        "bibtex": "@InProceedings{pmlr-v222-pourshahrokhi24a,\n  title = \t {{F}olded {H}amiltonian {M}onte {C}arlo for {B}ayesian {G}enerative {A}dversarial {N}etworks},\n  author =       {Pourshahrokhi, Narges and Li, Yunpeng and Kouchaki, Samaneh and Barnaghi, Payam},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1103--1118},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/pourshahrokhi24a/pourshahrokhi24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/pourshahrokhi24a.html},\n  abstract = \t {Probabilistic modelling on Generative Adversarial Networks (GANs) within the Bayesian framework has shown success in estimating the complex distribution in literature. In this paper, we develop a Bayesian formulation for unsupervised and semi-supervised GAN learning. Specifically, we propose Folded Hamiltonian Monte Carlo (F-HMC) methods within this framework to learn the distributions over the parameters of the generators and discriminators. We show that the F-HMC efficiently approximates multi-modal and high dimensional data when combined with Bayesian GANs. Its composition improves run time and test error in generating diverse samples. Experimental results with high-dimensional synthetic multi-modal data and natural image benchmarks, including CIFAR-10, SVHN and ImageNet, show that F-HMC outperforms the state-of-the-art methods in terms of test error, run times per epoch, inception score and Frechet Inception Distance scores.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/pourshahrokhi24a/pourshahrokhi24a.pdf",
        "supp": "",
        "pdf_size": 1854792,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4300378884013872120&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Electronic Engineering, University of Surrey, UK; School of Computer Science and Electronic Engineering, University of Surrey, UK; School of Computer Science and Electronic Engineering, University of Surrey, UK+UK Dementia Research Institute, Care Research and Technology Centre; Imperial College London, Department of Brain Sciences, UK+UK Dementia Research Institute, Care Research and Technology Centre",
        "aff_domain": "surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;imperial.ac.uk",
        "email": "surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;imperial.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;2+1",
        "aff_unique_norm": "University of Surrey;UK Dementia Research Institute;Imperial College London",
        "aff_unique_dep": "School of Computer Science and Electronic Engineering;Care Research and Technology Centre;Department of Brain Sciences",
        "aff_unique_url": "https://www.surrey.ac.uk;https://www.dementiaresearchuk.org;https://www.imperial.ac.uk",
        "aff_unique_abbr": "Surrey;UK DRI;ICL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Free Energy of Bayesian Convolutional Neural Network with Skip Connection",
        "site": "https://proceedings.mlr.press/v222/nagayasu24a.html",
        "author": "Shuya Nagayasu; Sumio Watanabe",
        "abstract": "Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters has not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. Bayesian Free Energy is the negative log marginal likelihood which is equivalent to Stochastic Complexity or Minimum Description Length (MDL) used for evaluating model complexity. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.",
        "bibtex": "@InProceedings{pmlr-v222-nagayasu24a,\n  title = \t {Free Energy of {B}ayesian Convolutional Neural Network with Skip Connection},\n  author =       {Nagayasu, Shuya and Watanabe, Sumio},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {927--942},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/nagayasu24a/nagayasu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/nagayasu24a.html},\n  abstract = \t {Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters has not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. Bayesian Free Energy is the negative log marginal likelihood which is equivalent to Stochastic Complexity or Minimum Description Length (MDL) used for evaluating model complexity. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/nagayasu24a/nagayasu24a.pdf",
        "supp": "",
        "pdf_size": 2163463,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10328073338524273004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Mathematical and Computing Science, Tokyo Institute of Technology; Department of Mathematical and Computing Science, Tokyo Institute of Technology",
        "aff_domain": "m.titech.ac.jp;m.titech.ac.jp",
        "email": "m.titech.ac.jp;m.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Department of Mathematical and Computing Science",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Frequency-dependent Image Reconstruction Error for Micro Defect Detection",
        "site": "https://proceedings.mlr.press/v222/nomura24a.html",
        "author": "Yuhei Nomura; Hirotaka Hachiya",
        "abstract": "Micro defects, such as casting pores in industrial products, have been detected by human visual inspection using X-ray CT images and image processing tools. Automatic detection of micro defects is challenging for anomaly detection methods using image reconstruction errors and nearest neighbor distances because these metrics are dominated by low-frequency information and are insensitive to minor defects. Although recent methods achieve high anomaly detection performances, their detection abilities are insufficient for micro defects. To overcome these problems, we propose to extend a state-of-the-art anomaly detection method by introducing frequency-dependent losses to capture reconstruction errors appearing around micro defects and frequency-dependent data augmentation to improve the sensitivity against the errors. We demonstrate the effectiveness of the proposed method through experiments with MVTec AD dataset especially on the detection of micro defects.",
        "bibtex": "@InProceedings{pmlr-v222-nomura24a,\n  title = \t {Frequency-dependent Image Reconstruction Error for Micro Defect Detection},\n  author =       {Nomura, Yuhei and Hachiya, Hirotaka},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1007--1022},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/nomura24a/nomura24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/nomura24a.html},\n  abstract = \t {Micro defects, such as casting pores in industrial products, have been detected by human visual inspection using X-ray CT images and image processing tools. Automatic detection of micro defects is challenging for anomaly detection methods using image reconstruction errors and nearest neighbor distances because these metrics are dominated by low-frequency information and are insensitive to minor defects. Although recent methods achieve high anomaly detection performances, their detection abilities are insufficient for micro defects. To overcome these problems, we propose to extend a state-of-the-art anomaly detection method by introducing frequency-dependent losses to capture reconstruction errors appearing around micro defects and frequency-dependent data augmentation to improve the sensitivity against the errors. We demonstrate the effectiveness of the proposed method through experiments with MVTec AD dataset especially on the detection of micro defects.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/nomura24a/nomura24a.pdf",
        "supp": "",
        "pdf_size": 2898618,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sJ40qfnrLlMJ:scholar.google.com/&scioq=Frequency-dependent+Image+Reconstruction+Error+for+Micro+Defect+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Industrial Technology Center of Wakayama Prefecture, Japan+Graduate School of Systems Engineering, Wakayama University, Japan; Graduate School of Systems Engineering, Wakayama University, Japan",
        "aff_domain": "wakayama-kg.jp;wakayama-u.ac.jp",
        "email": "wakayama-kg.jp;wakayama-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Industrial Technology Center of Wakayama Prefecture;Wakayama University",
        "aff_unique_dep": ";Graduate School of Systems Engineering",
        "aff_unique_url": ";https://www.wakayama-u.ac.jp",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation",
        "site": "https://proceedings.mlr.press/v222/li24a.html",
        "author": "Zongyi Li; Hongbing Lyu; Jun Wang",
        "abstract": "In recent years, U-Net and its variants have been widely used in pathology image segmentation tasks. One of the key designs of U-Net is the use of skip connections between the encoder and decoder, which helps to recover detailed information after upsampling. While most variations of U-Net adopt the original skip connection design, there is semantic gap between the encoder and decoder that can negatively impact model performance. Therefore, it is important to reduce this semantic gap before conducting skip connection. To address this issue, we propose a new segmentation network called FusionU-Net, which is based on U-Net structure and incorporates a fusion module to exchange information between different skip connections to reduce semantic gaps. Unlike the other fusion modules in existing networks, ours is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue our fusion module is more effective than the designs of existing networks, and it could be easily embedded into other networks to further enhance the model performance. Our code is available at: https://github.com/Zongyi-Lee/FusionU-Net",
        "bibtex": "@InProceedings{pmlr-v222-li24a,\n  title = \t {{FusionU-Net}: {U-Net} with Enhanced Skip Connection for Pathology Image Segmentation},\n  author =       {Li, Zongyi and Lyu, Hongbing and Wang, Jun},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {694--706},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/li24a/li24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/li24a.html},\n  abstract = \t { In recent years, U-Net and its variants have been widely used in pathology image segmentation tasks. One of the key designs of U-Net is the use of skip connections between the encoder and decoder, which helps to recover detailed information after upsampling. While most variations of U-Net adopt the original skip connection design, there is semantic gap between the encoder and decoder that can negatively impact model performance. Therefore, it is important to reduce this semantic gap before conducting skip connection. To address this issue, we propose a new segmentation network called FusionU-Net, which is based on U-Net structure and incorporates a fusion module to exchange information between different skip connections to reduce semantic gaps. Unlike the other fusion modules in existing networks, ours is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue our fusion module is more effective than the designs of existing networks, and it could be easily embedded into other networks to further enhance the model performance. Our code is available at: https://github.com/Zongyi-Lee/FusionU-Net}\n}",
        "pdf": "https://proceedings.mlr.press/v222/li24a/li24a.pdf",
        "supp": "",
        "pdf_size": 5792754,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3763818991870144763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Zhejiang University; Zhejiang University; Hangzhou City University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;163.com",
        "email": "zju.edu.cn;zju.edu.cn;163.com",
        "github": "https://github.com/Zongyi-Lee/FusionU-Net",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Zhejiang University;Hangzhou City University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zju.edu.cn;http://www.hghedu.com",
        "aff_unique_abbr": "ZJU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "GWQ: Group-Wise Quantization Framework for Neural Networks",
        "site": "https://proceedings.mlr.press/v222/yang24a.html",
        "author": "Jiaming Yang; Chenwei Tang; Caiyang Yu; Jiancheng Lv",
        "abstract": "As the most commonly used quantization techniques for deep neural networks, the int-only quantization methods use scale factor to linearly approximate the weights or activation of each layer. However, when passing activation data between layers, such int-only quantization methods require extra Scale Factor Conversion (SFC) operations, resulting in computational overhead. In this paper, we propose a Group-Wise Quantization framework, called GWQ, to reduce computational consumption during the activation data pass process by allowing multiple layers share one scale factor in SFC operations. Specifically, in the GWQ framework, we propose two algorithms for network layers grouping and model training. For the grouping of network layers, we propose a grouping algorithm based on the similarity of data numerical distribution. Then, the network layers divided into the same group will be quantified using the same common scale factor to reduce the computational consumption. Considering the additional performance loss caused by sharing scale factors among multiple layers, we propose a training algorithm to optimize these shared scale factors and model parameters, by designing a learnable power-of-two scaling parameter for each layer. Extensive experiments demonstrate that the proposed GWQ framework is able to effectively reduce the computational burden during inference, while maintaining model performance with negligible impact.",
        "bibtex": "@InProceedings{pmlr-v222-yang24a,\n  title = \t {{GWQ}: {G}roup-Wise Quantization Framework for Neural Networks},\n  author =       {Yang, Jiaming and Tang, Chenwei and Yu, Caiyang and Lv, Jiancheng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1526--1541},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yang24a/yang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yang24a.html},\n  abstract = \t {As the most commonly used quantization techniques for deep neural networks, the int-only quantization methods use scale factor to linearly approximate the weights or activation of each layer. However, when passing activation data between layers, such int-only quantization methods require extra Scale Factor Conversion (SFC) operations, resulting in computational overhead. In this paper, we propose a Group-Wise Quantization framework, called GWQ, to reduce computational consumption during the activation data pass process by allowing multiple layers share one scale factor in SFC operations. Specifically, in the GWQ framework, we propose two algorithms for network layers grouping and model training. For the grouping of network layers, we propose a grouping algorithm based on the similarity of data numerical distribution. Then, the network layers divided into the same group will be quantified using the same common scale factor to reduce the computational consumption. Considering the additional performance loss caused by sharing scale factors among multiple layers, we propose a training algorithm to optimize these shared scale factors and model parameters, by designing a learnable power-of-two scaling parameter for each layer. Extensive experiments demonstrate that the proposed GWQ framework is able to effectively reduce the computational burden during inference, while maintaining model performance with negligible impact.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yang24a/yang24a.pdf",
        "supp": "",
        "pdf_size": 584948,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9680740001478503019&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science, Sichuan University, Chengdu 610065, P. R. China+Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P. R. China; College of Computer Science, Sichuan University, Chengdu 610065, P. R. China+Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P. R. China; College of Computer Science, Sichuan University, Chengdu 610065, P. R. China; College of Computer Science, Sichuan University, Chengdu 610065, P. R. China+Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P. R. China",
        "aff_domain": "stu.scu.edu.cn;scu.edu.cn;gmail.com;scu.edu.cn",
        "email": "stu.scu.edu.cn;scu.edu.cn;gmail.com;scu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0;0+1",
        "aff_unique_norm": "Sichuan University;Engineering Research Center of Machine Learning and Industry Intelligence",
        "aff_unique_dep": "College of Computer Science;Ministry of Education",
        "aff_unique_url": "http://www.scu.edu.cn;",
        "aff_unique_abbr": "SCU;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chengdu;",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples",
        "site": "https://proceedings.mlr.press/v222/yamaguchi24a.html",
        "author": "Shin\u2019ya Yamaguchi",
        "abstract": "Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: \\textit{Can we train SSL models without real unlabeled datasets?} Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are to identify synthetic samples that emulate unlabeled samples from generative foundation models and to train classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthetic unlabeled samples. For (i), we propose a meta-learning objective that optimizes latent variables to generate samples that resemble real labeled samples and minimize the validation loss. For (ii), we propose a simple unsupervised loss function that regularizes the feature extractors of classifiers to maximize the performance improvement obtained from synthetic samples. We confirm that our method outperforms baselines using generative foundation models on SSL. We also demonstrate that our methods outperform SSL using real unlabeled datasets in scenarios with extremely small amounts of labeled datasets. This suggests that synthetic samples have the potential to provide improvement gains more efficiently than real unlabeled data.",
        "bibtex": "@InProceedings{pmlr-v222-yamaguchi24a,\n  title = \t {Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples},\n  author =       {Yamaguchi, Shin'ya},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1510--1525},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yamaguchi24a/yamaguchi24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yamaguchi24a.html},\n  abstract = \t {Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: \\textit{Can we train SSL models without real unlabeled datasets?} Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are to identify synthetic samples that emulate unlabeled samples from generative foundation models and to train classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthetic unlabeled samples. For (i), we propose a meta-learning objective that optimizes latent variables to generate samples that resemble real labeled samples and minimize the validation loss. For (ii), we propose a simple unsupervised loss function that regularizes the feature extractors of classifiers to maximize the performance improvement obtained from synthetic samples. We confirm that our method outperforms baselines using generative foundation models on SSL. We also demonstrate that our methods outperform SSL using real unlabeled datasets in scenarios with extremely small amounts of labeled datasets. This suggests that synthetic samples have the potential to provide improvement gains more efficiently than real unlabeled data.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yamaguchi24a/yamaguchi24a.pdf",
        "supp": "",
        "pdf_size": 2389730,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8078316957176020342&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "NTT / Kyoto University",
        "aff_domain": "ntt.com",
        "email": "ntt.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Graph Contrastive Learning with Group Whitening",
        "site": "https://proceedings.mlr.press/v222/zhang24a.html",
        "author": "Chunhui Zhang; Rui Miao",
        "abstract": "Graph neural networks (GNNs) have demonstrated their great power in learning graph-structured data. Due to the limitations of expensive labeled data, contrastive learning has been applied in graph domain. We propose GWGCL, a graph contrastive learning method based on feature group whitening to achieve two key properties of contrastive learning: alignment and uniformity. GWGCL achieves the alignment by ensuring consistency between positive samples. There is no need for negative samples to participate, but rather to achieve the uniformity between samples through whitening. Because whitening has the effect of feature divergence, it avoids the collapse of all sample representations to a single point, which is called dimensional collapse. Moreover, GWGCL can achieve better results and higher efficiency without the need for asymmetric networks, projection layers, stopping gradients and complex loss function. Through extensive experiments, GWGCL performs competitively on node classification and graph classification tasks across ten common graph datasets.The code is in: https://github.com/MR9812/GWGCL.",
        "bibtex": "@InProceedings{pmlr-v222-zhang24a,\n  title = \t {Graph Contrastive Learning with Group Whitening},\n  author =       {Zhang, Chunhui and Miao, Rui},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1622--1637},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhang24a/zhang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhang24a.html},\n  abstract = \t {Graph neural networks (GNNs) have demonstrated their great power in learning graph-structured data. Due to the limitations of expensive labeled data, contrastive learning has been applied in graph domain. We propose GWGCL, a graph contrastive learning method based on feature group whitening to achieve two key properties of contrastive learning: alignment and uniformity. GWGCL achieves the alignment by ensuring consistency between positive samples. There is no need for negative samples to participate, but rather to achieve the uniformity between samples through whitening. Because whitening has the effect of feature divergence, it avoids the collapse of all sample representations to a single point, which is called dimensional collapse. Moreover, GWGCL can achieve better results and higher efficiency without the need for asymmetric networks, projection layers, stopping gradients and complex loss function. Through extensive experiments, GWGCL performs competitively on node classification and graph classification tasks across ten common graph datasets.The code is in: https://github.com/MR9812/GWGCL.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhang24a/zhang24a.pdf",
        "supp": "",
        "pdf_size": 527810,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12259184065945335897&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "North China Institute of Computing Technology, Beijing, China; School of Artificial Intelligence, Jilin University, Changchun, China",
        "aff_domain": "163.com;mails.jlu.edu.cn",
        "email": "163.com;mails.jlu.edu.cn",
        "github": "https://github.com/MR9812/GWGCL",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "North China Institute of Computing Technology;Jilin University",
        "aff_unique_dep": ";School of Artificial Intelligence",
        "aff_unique_url": ";http://www.jlu.edu.cn",
        "aff_unique_abbr": ";JLU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Beijing;Changchun",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Graph Structure Learning via Lottery Hypothesis at Scale",
        "site": "https://proceedings.mlr.press/v222/yuxin24a.html",
        "author": "Wang Yuxin; Hu Xiannian; Xie Jiaqing; Yin Zhangyue; Zhou Yunhua; Qiu Xipeng; Huang Xuanjing",
        "abstract": "Graph Neural Networks (GNNs) are commonly applied to analyze real-world graph-structured data. However, GNNs are sensitive to the given graph structure, which cast importance on graph structure learning to find optimal graph structures and representations. Previous methods have been restricted from large graphs due to high computational complexity. Lottery ticket hypothesis suggests that there exists a subnetwork that has comparable or better performance with proto-networks, which has been transferred to suit for pruning GNNs recently. There are few studies that address lottery ticket hypothesis\u2019s performance on defense in graphs. In this paper, we propose a scalable graph structure learning method leveraging lottery (ticket) hypothesis : GSL-LH. Our experiments show that GSL-LH can outperform its backbone model without attack and show better robustness against attack, achieving state-of-the-art performances in regular-size graphs compared to other graph structure learning methods without feature augmentation. In large graphs, GSL-LH can have comparable results with state-of-the-art defense methods other than graph structure learning, while bringing some insights into explanation of robustness.",
        "bibtex": "@InProceedings{pmlr-v222-yuxin24a,\n  title = \t {Graph Structure Learning via Lottery Hypothesis at Scale},\n  author =       {Yuxin, Wang and Xiannian, Hu and Jiaqing, Xie and Zhangyue, Yin and Yunhua, Zhou and Xipeng, Qiu and Xuanjing, Huang},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1401--1416},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yuxin24a/yuxin24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yuxin24a.html},\n  abstract = \t {Graph Neural Networks (GNNs) are commonly applied to analyze real-world graph-structured data. However, GNNs are sensitive to the given graph structure, which cast importance on graph structure learning to find optimal graph structures and representations. Previous methods have been restricted from large graphs due to high computational complexity. Lottery ticket hypothesis suggests that there exists a subnetwork that has comparable or better performance with proto-networks, which has been transferred to suit for pruning GNNs recently. There are few studies that address lottery ticket hypothesis\u2019s performance on defense in graphs. In this paper, we propose a scalable graph structure learning method leveraging lottery (ticket) hypothesis : GSL-LH. Our experiments show that GSL-LH can outperform its backbone model without attack and show better robustness against attack, achieving state-of-the-art performances in regular-size graphs compared to other graph structure learning methods without feature augmentation. In large graphs, GSL-LH can have comparable results with state-of-the-art defense methods other than graph structure learning, while bringing some insights into explanation of robustness.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yuxin24a/yuxin24a.pdf",
        "supp": "",
        "pdf_size": 973653,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8699735341169329552&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Fudan University+Institute of Modern Languages and Linguistics, Fudan University; School of Computer Science, Fudan University; Department of Computer Science, ETH Zurich; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University+Institute of Modern Languages and Linguistics, Fudan University; School of Computer Science, Fudan University+Shanghai Key Laboratory of Intelligent Information Processing, Fudan University",
        "aff_domain": "m.fudan.edu.cn;m.fudan.edu.cn;ethz.ch;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn;m.fudan.edu.cn;ethz.ch;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "https://github.com/jiaqingxie/GSL-LH",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0;1;0;0;0+0;0+0",
        "aff_unique_norm": "Fudan University;ETH Zurich",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.ethz.ch",
        "aff_unique_abbr": "Fudan;ETHZ",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0;1;0;0;0+0;0+0",
        "aff_country_unique": "China;Switzerland"
    },
    {
        "title": "Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices",
        "site": "https://proceedings.mlr.press/v222/ghebriout24a.html",
        "author": "Mohamed Imed Eddine Ghebriout; Halima Bouzidi; Smail Niar; Hamza Ouarnoughi",
        "abstract": "The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate multiscale information from diverse data sources. MM-NNs extract and fuse features from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose \\textit{Harmonic-NAS}, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. \\textit{Harmonic-NAS} involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of \\textit{Harmonic-NAS} over state-of-the-art approaches achieving up to $\\sim \\textbf{10.9%} accuracy improvement, $\\sim$\\textbf{1.91x} latency reduction, and $\\sim$\\textbf{2.14x} energy efficiency gain.",
        "bibtex": "@InProceedings{pmlr-v222-ghebriout24a,\n  title = \t {{Harmonic-NAS}: {H}ardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices},\n  author =       {Ghebriout, Mohamed Imed Eddine and Bouzidi, Halima and Niar, Smail and Ouarnoughi, Hamza},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {374--389},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ghebriout24a/ghebriout24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ghebriout24a.html},\n  abstract = \t {The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate multiscale information from diverse data sources. MM-NNs extract and fuse features from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose \\textit{Harmonic-NAS}, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. \\textit{Harmonic-NAS} involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of \\textit{Harmonic-NAS} over state-of-the-art approaches achieving up to $\\sim \\textbf{10.9%} accuracy improvement, $\\sim$\\textbf{1.91x} latency reduction, and $\\sim$\\textbf{2.14x} energy efficiency gain.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ghebriout24a/ghebriout24a.pdf",
        "supp": "",
        "pdf_size": 2278422,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15761144127664349247&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Higher National School of Computer Science - ESI ex INI; Universit\u00e9 Polytechnique Hauts-de-France, LAMIH/CNRS; Universit\u00e9 Polytechnique Hauts-de-France, LAMIH/CNRS; Universit\u00e9 Polytechnique Hauts-de-France, LAMIH/CNRS, INSA Hauts-de-France",
        "aff_domain": "esi.dz;uphf.fr;uphf.fr;uphf.fr",
        "email": "esi.dz;uphf.fr;uphf.fr;uphf.fr",
        "github": "https://github.com/Mohamed-Imed-Eddine/Harmonic-NAS",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Higher National School of Computer Science;Universit\u00e9 Polytechnique Hauts-de-France",
        "aff_unique_dep": "Computer Science;LAMIH/CNRS",
        "aff_unique_url": ";https://www.uphf.fr",
        "aff_unique_abbr": "ESI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Algeria;France"
    },
    {
        "title": "How GAN Generators can Invert Networks in Real-Time",
        "site": "https://proceedings.mlr.press/v222/herdt24a.html",
        "author": "Rudolf Herdt; Maximilian Schmidt; Daniel Otero Baguer; Jean Le\u2019Clerc Arrastia; Peter Maa\u00df",
        "abstract": "In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, making this approach promising for practical applications.",
        "bibtex": "@InProceedings{pmlr-v222-herdt24a,\n  title = \t {How {GAN} Generators can Invert Networks in Real-Time},\n  author =       {Herdt, Rudolf and Schmidt, Maximilian and Otero Baguer, Daniel and Le'Clerc Arrastia, Jean and Maa{\\ss}, Peter},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {422--437},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/herdt24a/herdt24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/herdt24a.html},\n  abstract = \t {In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, making this approach promising for practical applications.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/herdt24a/herdt24a.pdf",
        "supp": "",
        "pdf_size": 11720060,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10635982061583143235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Center of Industrial Mathematics, University of Bremen, Germany; Center of Industrial Mathematics, University of Bremen, Germany; Center of Industrial Mathematics, University of Bremen, Germany; Center of Industrial Mathematics, University of Bremen, Germany; Center of Industrial Mathematics, University of Bremen, Germany",
        "aff_domain": "uni-bremen.de;uni-bremen.de;uni-bremen.de;uni-bremen.de;uni-bremen.de",
        "email": "uni-bremen.de;uni-bremen.de;uni-bremen.de;uni-bremen.de;uni-bremen.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Bremen",
        "aff_unique_dep": "Center of Industrial Mathematics",
        "aff_unique_url": "https://www.uni-bremen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Bremen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Hybrid Convolution Method for Graph Classification Using Hierarchical Topology Feature",
        "site": "https://proceedings.mlr.press/v222/sun24c.html",
        "author": "Jiangfeng Sun; Xinyue Lin; Fangyu Hao; Meina Song",
        "abstract": "Graph classification is a crucial task in the field of graph learning with numerous practical applications. Typically, the first step is to construct vertex features by the statistical information of the graph. Existing graph neural networks often adopt the one-hot degree encoding strategy to construct vertex features. Then, these features are fed into a linear layer, which outputs a low-dimensional real vector serving as the initial vertex representation for the graph model. However, the conventional approach of constructing vertex features may not be optimal. Intuitively, the method of constructing vertex features can have significant impact on the effectiveness of model. Hence, the construction of informative vertex features from the graph and the design of an efficient graph model to process these features pose great challenges. In this paper, we propose a novel method for constructing hierarchical topology vertex features and designing a hybrid convolution method to handle these features. Experimental results on public graph datasets of Social Networks, Small Molecules, and Bioinformatics demonstrate the superior performance of our method compared to baselines.",
        "bibtex": "@InProceedings{pmlr-v222-sun24c,\n  title = \t {Hybrid Convolution Method for Graph Classification Using Hierarchical Topology Feature},\n  author =       {Sun, Jiangfeng and Lin, Xinyue and Hao, Fangyu and Song, Meina},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1308--1320},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/sun24c/sun24c.pdf},\n  url = \t {https://proceedings.mlr.press/v222/sun24c.html},\n  abstract = \t {Graph classification is a crucial task in the field of graph learning with numerous practical applications. Typically, the first step is to construct vertex features by the statistical information of the graph. Existing graph neural networks often adopt the one-hot degree encoding strategy to construct vertex features. Then, these features are fed into a linear layer, which outputs a low-dimensional real vector serving as the initial vertex representation for the graph model. However, the conventional approach of constructing vertex features may not be optimal. Intuitively, the method of constructing vertex features can have significant impact on the effectiveness of model. Hence, the construction of informative vertex features from the graph and the design of an efficient graph model to process these features pose great challenges. In this paper, we propose a novel method for constructing hierarchical topology vertex features and designing a hybrid convolution method to handle these features. Experimental results on public graph datasets of Social Networks, Small Molecules, and Bioinformatics demonstrate the superior performance of our method compared to baselines.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/sun24c/sun24c.pdf",
        "supp": "",
        "pdf_size": 2215190,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:l8SnyauEI5oJ:scholar.google.com/&scioq=Hybrid+Convolution+Method+for+Graph+Classification+Using+Hierarchical+Topology+Feature&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "bupt.edu.cn;foxmail.com;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;foxmail.com;bupt.edu.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Hyper-Label-Graph: Modeling Branch-Level Dependencies of Labels for Hierarchical Multi-Label Text Classification",
        "site": "https://proceedings.mlr.press/v222/deng24b.html",
        "author": "Wenmin Deng; Jing Zhang; Peng Zhang; Yitong Yao; Hui Gao; Yurui Zhang",
        "abstract": "In the task of Hierarchical Multi-label Text Classification (HTMC), there exist multiple multivariate relations between labels, particularly the semantic dependencies within label branches of the hierarchy. However, existing methods struggle to fully exploit these potential multivariate dependencies since they can only model binary relationships at best. In this paper, we address this limitation by focusing on leveraging semantic dependencies among labels within branches and propose a Hyper-Label-Graph Model (HLGM). Specifically, we first construct a label hypergraph based on the taxonomy hierarchy and utilize a hypergraph attention mechanism to learn branch-level multivariate dependencies among labels. Furthermore, the model employs a label-text fusion module to generate label-level text representations, facilitating the comprehensive integration of semantic features between text and labels. Additionally, we introduce a hierarchical triplet loss to enhance the ability to distinguish labels within the hyperedge structure. We validate the effectiveness of the proposed model on three benchmark datasets, and the experimental results demonstrate that HLGM outperforms competitive GNN-based baselines.",
        "bibtex": "@InProceedings{pmlr-v222-deng24b,\n  title = \t {{Hyper-Label-Graph}: {M}odeling Branch-Level Dependencies of Labels for Hierarchical Multi-Label Text Classification},\n  author =       {Deng, Wenmin and Zhang, Jing and Zhang, Peng and Yao, Yitong and Gao, Hui and Zhang, Yurui},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {279--294},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/deng24b/deng24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/deng24b.html},\n  abstract = \t {In the task of Hierarchical Multi-label Text Classification (HTMC), there exist multiple multivariate relations between labels, particularly the semantic dependencies within label branches of the hierarchy. However, existing methods struggle to fully exploit these potential multivariate dependencies since they can only model binary relationships at best. In this paper, we address this limitation by focusing on leveraging semantic dependencies among labels within branches and propose a Hyper-Label-Graph Model (HLGM). Specifically, we first construct a label hypergraph based on the taxonomy hierarchy and utilize a hypergraph attention mechanism to learn branch-level multivariate dependencies among labels. Furthermore, the model employs a label-text fusion module to generate label-level text representations, facilitating the comprehensive integration of semantic features between text and labels. Additionally, we introduce a hierarchical triplet loss to enhance the ability to distinguish labels within the hyperedge structure. We validate the effectiveness of the proposed model on three benchmark datasets, and the experimental results demonstrate that HLGM outperforms competitive GNN-based baselines.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/deng24b/deng24b.pdf",
        "supp": "",
        "pdf_size": 1422774,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3928498889856990201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise",
        "site": "https://proceedings.mlr.press/v222/zhang24b.html",
        "author": "Zhenkai Zhang; Krista A. Ehinger; Tom Drummond",
        "abstract": "This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\\sqrt{\\bar{\\alpha}}=\\cos(\\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\\mathbf{x}_0$) and noise ($\\mathbf{\\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achieves faster generation, with the ability to converge on high-quality images more quickly, and higher quality of the generated images, as measured by metrics such as Fr{\u00e9}chet Inception Distance (FID), spatial Fr{\u00e9}chet Inception Distance (sFID), precision, and recall.",
        "bibtex": "@InProceedings{pmlr-v222-zhang24b,\n  title = \t {Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise},\n  author =       {Zhang, Zhenkai and Ehinger, Krista A. and Drummond, Tom},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1638--1653},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhang24b/zhang24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhang24b.html},\n  abstract = \t {This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\\sqrt{\\bar{\\alpha}}=\\cos(\\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\\mathbf{x}_0$) and noise ($\\mathbf{\\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achieves faster generation, with the ability to converge on high-quality images more quickly, and higher quality of the generated images, as measured by metrics such as Fr{\u00e9}chet Inception Distance (FID), spatial Fr{\u00e9}chet Inception Distance (sFID), precision, and recall.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhang24b/zhang24b.pdf",
        "supp": "",
        "pdf_size": 16681407,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2607974745793555485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Melbourne",
        "aff_unique_dep": "School of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Intractability of Learning the Discrete Logarithm with Gradient-Based Methods",
        "site": "https://proceedings.mlr.press/v222/takhanov24a.html",
        "author": "Rustem Takhanov; Maxat Tezekbayev; Artur Pak; Arman Bolatov; Zhibek Kadyrsizova; Zhenisbek Assylbekov",
        "abstract": "The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm\u2019s base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained. Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm\u2019s parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the limitations of gradient-based learning, demonstrating the decreasing success rate in predicting the parity bit as the group order increases.",
        "bibtex": "@InProceedings{pmlr-v222-takhanov24a,\n  title = \t {Intractability of Learning the Discrete Logarithm with Gradient-Based Methods},\n  author =       {Takhanov, Rustem and Tezekbayev, Maxat and Pak, Artur and Bolatov, Arman and Kadyrsizova, Zhibek and Assylbekov, Zhenisbek},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1321--1336},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/takhanov24a/takhanov24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/takhanov24a.html},\n  abstract = \t {The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm\u2019s base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained. Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm\u2019s parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the limitations of gradient-based learning, demonstrating the decreasing success rate in predicting the parity bit as the group order increases.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/takhanov24a/takhanov24a.pdf",
        "supp": "",
        "pdf_size": 1451474,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3691165009559240700&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Mathematics, Nazarbayev University, Astana, Kazakhstan; Department of Mathematics, Nazarbayev University, Astana, Kazakhstan; Department of Mathematics, Nazarbayev University, Astana, Kazakhstan; Department of Computer Science, Nazarbayev University, Astana, Kazakhstan; Department of Mathematics, Nazarbayev University, Astana, Kazakhstan; Department of Mathematical Sciences, Purdue University Fort Wayne, Fort Wayne, IN, USA",
        "aff_domain": "nu.edu.kz;nu.edu.kz;nu.edu.kz;nu.edu.kz;nu.edu.kz;pfw.edu",
        "email": "nu.edu.kz;nu.edu.kz;nu.edu.kz;nu.edu.kz;nu.edu.kz;pfw.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Nazarbayev University;Purdue University Fort Wayne",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematical Sciences",
        "aff_unique_url": "https://www.nu.edu.kz;https://www.purdue.edu",
        "aff_unique_abbr": "NU;Purdue",
        "aff_campus_unique_index": "0;0;0;0;0;1",
        "aff_campus_unique": "Astana;Fort Wayne",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "Kazakhstan;United States"
    },
    {
        "title": "K-Truss Based Temporal Graph Convolutional Network for Dynamic Graphs",
        "site": "https://proceedings.mlr.press/v222/li24d.html",
        "author": "Hongxi Li; Zuxuan Zhang; Dengzhe Liang; Yuncheng Jiang",
        "abstract": "Learning latent representations of nodes in graphs is important for many real-world applications, such as recommender systems, traffic prediction and fraud detection. Most of the existing research on graph representation learning has focused on static graphs. However, many real-world graphs are dynamic and their structures change over time, which makes learning dynamic node representations challenging. We propose a novel k-truss based temporal graph convolutional network named TTGCN to learn potential node representations on dynamic graphs. Specifically, TTGCN utilizes a novel truss-based graph convolutional layer named TrussGCN to capture the topology and hierarchical structure information of graphs, and combines it with a temporal evolution module to capture complex temporal dependencies. We conduct link prediction experiments on five different dynamic graph datasets. Experimental results demonstrate the superiority of TTGCN for dynamic graph embedding, as it consistently outperforms several state-of-the-art baselines in the link prediction task. In addition, our ablation experiments demonstrate the effectiveness of adopting TrussGCN in a dynamic graph embedding method.",
        "bibtex": "@InProceedings{pmlr-v222-li24d,\n  title = \t {K-Truss Based Temporal Graph Convolutional Network for Dynamic Graphs},\n  author =       {Li, Hongxi and Zhang, Zuxuan and Liang, Dengzhe and Jiang, Yuncheng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {739--754},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/li24d/li24d.pdf},\n  url = \t {https://proceedings.mlr.press/v222/li24d.html},\n  abstract = \t {Learning latent representations of nodes in graphs is important for many real-world applications, such as recommender systems, traffic prediction and fraud detection. Most of the existing research on graph representation learning has focused on static graphs. However, many real-world graphs are dynamic and their structures change over time, which makes learning dynamic node representations challenging. We propose a novel k-truss based temporal graph convolutional network named TTGCN to learn potential node representations on dynamic graphs. Specifically, TTGCN utilizes a novel truss-based graph convolutional layer named TrussGCN to capture the topology and hierarchical structure information of graphs, and combines it with a temporal evolution module to capture complex temporal dependencies. We conduct link prediction experiments on five different dynamic graph datasets. Experimental results demonstrate the superiority of TTGCN for dynamic graph embedding, as it consistently outperforms several state-of-the-art baselines in the link prediction task. In addition, our ablation experiments demonstrate the effectiveness of adopting TrussGCN in a dynamic graph embedding method.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/li24d/li24d.pdf",
        "supp": "",
        "pdf_size": 482356,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3259302344849366493&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, South China Normal University, Foshan, 528225, China; School of Artificial Intelligence, South China Normal University, Foshan, 528225, China; School of Artificial Intelligence, South China Normal University, Foshan, 528225, China; School of Artificial Intelligence, South China Normal University, Foshan, 528225, China+School of Computer Science, South China Normal University, Guangzhou, 510631, China",
        "aff_domain": "m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn",
        "email": "m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+0",
        "aff_unique_norm": "South China Normal University",
        "aff_unique_dep": "School of Artificial Intelligence",
        "aff_unique_url": "http://www.scnu.edu.cn",
        "aff_unique_abbr": "SCNU",
        "aff_campus_unique_index": "0;0;0;0+1",
        "aff_campus_unique": "Foshan;Guangzhou",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "KURL: A Knowledge-Guided Reinforcement Learning Model for Active Object Tracking",
        "site": "https://proceedings.mlr.press/v222/liu24c.html",
        "author": "Xin Liu; Jie Tan; Xiaoguang Ren; Weiya Ren; Huadong Dai",
        "abstract": "Recent studies have shown that active object tracking algorithms based on deep reinforcement learning have the difficulty of model training while achieving favorable tracking outcomes. In addition, current active object tracking methods are not suitable for air-to-ground object tracking scenarios in high-altitude environments, such as air search and rescue. Therefore, we proposed a Knowledge-gUided Reinforcement learning (KURL) model for active object tracking, which includes two embedded knowledge-guided models (i.e., the state recognition model and the world model), together with a reinforcement learning module. The state recognition model utilizes the correlation between the observed states and image quality (as measured by object recognition probability) as prior knowledge to guide reinforcement learning algorithm to improve the observed image quality. The reinforcement learning module actively controls the Pan-Tilt-Zoom (PTZ) camera to achieve stable tracking. Additionally, a world model is proposed to replace the traditional Unreal Engine (UE) simulator for model training, which significantly enhancing the training efficiency (about ten times). The results indicate that the KURL model can significantly enhance the image quality, stability and robustness of tracking, compared with other methods in similar tasks.",
        "bibtex": "@InProceedings{pmlr-v222-liu24c,\n  title = \t {{KURL}: {A} Knowledge-Guided Reinforcement Learning Model for Active Object Tracking},\n  author =       {Liu, Xin and Tan, Jie and Ren, Xiaoguang and Ren, Weiya and Dai, Huadong},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {818--833},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/liu24c/liu24c.pdf},\n  url = \t {https://proceedings.mlr.press/v222/liu24c.html},\n  abstract = \t {Recent studies have shown that active object tracking algorithms based on deep reinforcement learning have the difficulty of model training while achieving favorable tracking outcomes. In addition, current active object tracking methods are not suitable for air-to-ground object tracking scenarios in high-altitude environments, such as air search and rescue. Therefore, we proposed a Knowledge-gUided Reinforcement learning (KURL) model for active object tracking, which includes two embedded knowledge-guided models (i.e., the state recognition model and the world model), together with a reinforcement learning module. The state recognition model utilizes the correlation between the observed states and image quality (as measured by object recognition probability) as prior knowledge to guide reinforcement learning algorithm to improve the observed image quality. The reinforcement learning module actively controls the Pan-Tilt-Zoom (PTZ) camera to achieve stable tracking. Additionally, a world model is proposed to replace the traditional Unreal Engine (UE) simulator for model training, which significantly enhancing the training efficiency (about ten times). The results indicate that the KURL model can significantly enhance the image quality, stability and robustness of tracking, compared with other methods in similar tasks.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/liu24c/liu24c.pdf",
        "supp": "",
        "pdf_size": 1693427,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10468635674649702957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Intelligent Game and Decision Lab (IGDL), Beijing, China; Intelligent Game and Decision Lab (IGDL), Beijing, China; Intelligent Game and Decision Lab (IGDL), Beijing, China; Intelligent Game and Decision Lab (IGDL), Beijing, China; Intelligent Game and Decision Lab (IGDL), Beijing, China",
        "aff_domain": "nudt.edu.cn;outlook.com;126.com;gmail.com;vip.163.com",
        "email": "nudt.edu.cn;outlook.com;126.com;gmail.com;vip.163.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Intelligent Game and Decision Lab",
        "aff_unique_dep": "Game and Decision",
        "aff_unique_url": "",
        "aff_unique_abbr": "IGDL",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning to Terminate in Object Navigation",
        "site": "https://proceedings.mlr.press/v222/song24a.html",
        "author": "Yuhang Song; Anh Nguyen; Chun-Yi Lee",
        "abstract": "This paper tackles the critical challenge of object navigation in autonomous navigation systems, particularly focusing on the problem of target approach and episode termination in environments with long optimal episode length in Deep Reinforcement Learning (DRL) based methods. While effective in environment exploration and object localization, conventional DRL methods often struggle with optimal path planning and termination recognition due to a lack of depth information. To overcome these limitations, we propose a novel approach, namely the Depth-Inference Termination Agent (DITA), which incorporates a supervised model called the Judge Model to implicitly infer object-wise depth and decide termination jointly with reinforcement learning. We train our judge model along with reinforcement learning in parallel and supervise the former efficiently by reward signal. Our evaluation shows the method is demonstrating superior performance, we achieve a 9.3% gain on success rate than our baseline method across all room types and gain 51.2% improvements on long episodes environment while maintaining slightly better Success Weighted by Path Length (SPL). Code and resources, visualization are available at: \\url{https://github.com/HuskyKingdom/DITA_acml2023}",
        "bibtex": "@InProceedings{pmlr-v222-song24a,\n  title = \t {Learning to Terminate in Object Navigation},\n  author =       {Song, Yuhang and Nguyen, Anh and Lee, Chun-Yi},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1247--1262},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/song24a/song24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/song24a.html},\n  abstract = \t {This paper tackles the critical challenge of object navigation in autonomous navigation systems, particularly focusing on the problem of target approach and episode termination in environments with long optimal episode length in Deep Reinforcement Learning (DRL) based methods. While effective in environment exploration and object localization, conventional DRL methods often struggle with optimal path planning and termination recognition due to a lack of depth information. To overcome these limitations, we propose a novel approach, namely the Depth-Inference Termination Agent (DITA), which incorporates a supervised model called the Judge Model to implicitly infer object-wise depth and decide termination jointly with reinforcement learning. We train our judge model along with reinforcement learning in parallel and supervise the former efficiently by reward signal. Our evaluation shows the method is demonstrating superior performance, we achieve a 9.3% gain on success rate than our baseline method across all room types and gain 51.2% improvements on long episodes environment while maintaining slightly better Success Weighted by Path Length (SPL). Code and resources, visualization are available at: \\url{https://github.com/HuskyKingdom/DITA_acml2023}}\n}",
        "pdf": "https://proceedings.mlr.press/v222/song24a/song24a.pdf",
        "supp": "",
        "pdf_size": 15702334,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12137678665294190066&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Liverpool + Department of Computer Science, National Tsing Hua University; Department of Computer Science, University of Liverpool; Department of Computer Science, National Tsing Hua University",
        "aff_domain": "liverpool.ac.uk;liverpool.ac.uk;cs.nthu.edu.tw",
        "email": "liverpool.ac.uk;liverpool.ac.uk;cs.nthu.edu.tw",
        "github": "https://github.com/HuskyKingdom/DITA_acml2023",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "University of Liverpool;National Tsing Hua University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.liverpool.ac.uk;https://www.nthu.edu.tw",
        "aff_unique_abbr": "Liv Uni;NTHU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "title": "Logarithmic regret in communicating MDPs: Leveraging known dynamics with bandits",
        "site": "https://proceedings.mlr.press/v222/saber24a.html",
        "author": "Hassan Saber; Fabien Pesquerel; Odalric-Ambrym Maillard; Mohammad Sadegh Talebi",
        "abstract": "We study regret minimization in an average-reward and communicating Markov Decision Process (MDP) with known dynamics, but unknown reward function. Although learning in such MDPs is a priori easier than in fully unknown ones, they are still largely challenging as they include as special cases large classes of problems such as combinatorial semi-bandits. Leveraging the knowledge on transition function in regret minimization, in a statistically efficient way, appears largely unexplored. As it is conjectured that achieving exact optimality in generic MDPs is NP-hard, even with known transitions, we focus on a computationally efficient relaxation, at the cost of achieving order-optimal logarithmic regret instead of exact optimality. We contribute to filling this gap by introducing a novel algorithm based on the popular Indexed Minimum Empirical Divergence strategy for bandits. A key component of the proposed algorithm is a carefully designed stopping criterion leveraging the recurrent classes induced by stationary policies. We derive a non-asymptotic, problem-dependent, and logarithmic regret bound for this algorithm, which relies on a novel regret decomposition leveraging the structure. We further provide an efficient implementation and experiments illustrating its promising empirical performance.",
        "bibtex": "@InProceedings{pmlr-v222-saber24a,\n  title = \t {Logarithmic regret in communicating {MDPs}: {L}everaging known dynamics with bandits},\n  author =       {Saber, Hassan and Pesquerel, Fabien and Maillard, Odalric-Ambrym and Talebi, Mohammad Sadegh},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1167--1182},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/saber24a/saber24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/saber24a.html},\n  abstract = \t {We study regret minimization in an average-reward and communicating Markov Decision Process (MDP) with known dynamics, but unknown reward function. Although learning in such MDPs is a priori easier than in fully unknown ones, they are still largely challenging as they include as special cases large classes of problems such as combinatorial semi-bandits. Leveraging the knowledge on transition function in regret minimization, in a statistically efficient way, appears largely unexplored. As it is conjectured that achieving exact optimality in generic MDPs is NP-hard, even with known transitions, we focus on a computationally efficient relaxation, at the cost of achieving order-optimal logarithmic regret instead of exact optimality. We contribute to filling this gap by introducing a novel algorithm based on the popular Indexed Minimum Empirical Divergence strategy for bandits. A key component of the proposed algorithm is a carefully designed stopping criterion leveraging the recurrent classes induced by stationary policies. We derive a non-asymptotic, problem-dependent, and logarithmic regret bound for this algorithm, which relies on a novel regret decomposition leveraging the structure. We further provide an efficient implementation and experiments illustrating its promising empirical performance.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/saber24a/saber24a.pdf",
        "supp": "",
        "pdf_size": 903834,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1380252861444242761&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 \u2013 CRIStAL, F-59000, Lille, France; Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 \u2013 CRIStAL, F-59000, Lille, France; Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 \u2013 CRIStAL, F-59000, Lille, France; Universitetsparken 1, DK-2100 Copenhagen \u00d8, Denmark",
        "aff_domain": "inria.fr;inria.fr;inria.fr;di.ku.dk",
        "email": "inria.fr;inria.fr;inria.fr;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Lille;University of Copenhagen",
        "aff_unique_dep": "UMR 9189 \u2013 CRIStAL;",
        "aff_unique_url": "https://www.univ-lille.fr;https://www.ku.dk",
        "aff_unique_abbr": "Univ. Lille;UCPH",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Lille;Copenhagen",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "France;Denmark"
    },
    {
        "title": "Long-Range Graph U-Nets: Node and Edge Clustering Pooling Model For Stroke Classification in Online Handwritten Documents",
        "site": "https://proceedings.mlr.press/v222/yao24a.html",
        "author": "Muwu Yao; Shuang She; Jinrong Li; Jianmin Lin; Ming Yang; Hongxing Peng",
        "abstract": "Stroke classification is a crucial step for applications with online handwritten input. It is a challenging task due to the variations in writing style, complex structure, long contextual semantic dependence of written content and etc. In this work, we propose a method called Long-Range Graph U-Nets, which involves using a novel node and edge clustering graph pooling layer in the encoder block and a multi-level feature fusion strategy. Such operations guide the model to leverage both temporal and spatial contextual information, establish long-range semantic dependencies, and effectively reduce redundant information caused by local instances of the same category. Extensive experiments conducted on publicly available online handwritten document datasets, demonstrate that our proposed method outperforms previous methods by a significant margin, particularly in the List category, and achieves state-of-the-art performance.",
        "bibtex": "@InProceedings{pmlr-v222-yao24a,\n  title = \t {{Long-Range Graph U-Nets}: {N}ode and Edge Clustering Pooling Model For Stroke Classification in Online Handwritten Documents},\n  author =       {Yao, Muwu and She, Shuang and Li, Jinrong and Lin, Jianmin and Yang, Ming and Peng, Hongxing},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1542--1557},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yao24a/yao24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yao24a.html},\n  abstract = \t {Stroke classification is a crucial step for applications with online handwritten input. It is a challenging task due to the variations in writing style, complex structure, long contextual semantic dependence of written content and etc. In this work, we propose a method called Long-Range Graph U-Nets, which involves using a novel node and edge clustering graph pooling layer in the encoder block and a multi-level feature fusion strategy. Such operations guide the model to leverage both temporal and spatial contextual information, establish long-range semantic dependencies, and effectively reduce redundant information caused by local instances of the same category. Extensive experiments conducted on publicly available online handwritten document datasets, demonstrate that our proposed method outperforms previous methods by a significant margin, particularly in the List category, and achieves state-of-the-art performance.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yao24a/yao24a.pdf",
        "supp": "",
        "pdf_size": 1937153,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:P7eBldwrTsQJ:scholar.google.com/&scioq=Long-Range+Graph+U-Nets:+Node+and+Edge+Clustering+Pooling+Model+For+Stroke+Classification+in+Online+Handwritten+Documents&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "College of Mathematics and Informatics, South China Agricultural University; CVTE Research; CVTE Research; CVTE Research; CVTE Research; College of Mathematics and Informatics, South China Agricultural University",
        "aff_domain": "scau.edu.cn;cvte.com;cvte.com;cvte.com;cvte.com;scau.edu.cn",
        "email": "scau.edu.cn;cvte.com;cvte.com;cvte.com;cvte.com;scau.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "South China Agricultural University;CVTE Research",
        "aff_unique_dep": "College of Mathematics and Informatics;",
        "aff_unique_url": "http://www.scau.edu.cn;https://www.cvte.com/",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Lost and Found: How Self-Supervised Learning Helps GPS Coordinates Find Their Way",
        "site": "https://proceedings.mlr.press/v222/bougie24a.html",
        "author": "Nicolas Bougie; Daria Vazhenina; Narimasa Watanabe",
        "abstract": "GPS coordinates are a fundamental aspect of location-based applications, yet prior methods for representing them do not fully capture the intricate relationships between different locations. In this paper, we propose a novel map-based approach to embedding GPS coordinates using self-supervised learning. Unlike most prior studies that directly embed GPS coordinates to a latent space, we leverage a map-based approach, allowing embeddings to capture geographical and economic features. Namely, we use a student-teacher architecture, where a student network is trained to mimic the outputs of the teacher, using two different augmented versions of the same input. To capture the rich underlying semantics of GPS coordinates, we further leverage auxiliary tasks including \\textit{geo} prediction, high-level reconstruction, and intermediate clustering. The intermediate clustering loss facilitates learning features at different levels of granularity, while the high-level reconstruction loss encourages \u201clocal-to-global\u201d correspondences. We evaluate our method on a large-scale dataset of GPS coordinates and demonstrate that it outperforms several baseline methods in terms of the quality of the learned embeddings. Moreover, we show the usefulness of our embeddings in various downstream tasks, such as predicting land price, land cover type, or water quality indice.",
        "bibtex": "@InProceedings{pmlr-v222-bougie24a,\n  title = \t {{Lost and Found}: {H}ow Self-Supervised Learning Helps {GPS} Coordinates Find Their Way},\n  author =       {Bougie, Nicolas and Vazhenina, Daria and Watanabe, Narimasa},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {185--200},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/bougie24a/bougie24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/bougie24a.html},\n  abstract = \t {GPS coordinates are a fundamental aspect of location-based applications, yet prior methods for representing them do not fully capture the intricate relationships between different locations. In this paper, we propose a novel map-based approach to embedding GPS coordinates using self-supervised learning. Unlike most prior studies that directly embed GPS coordinates to a latent space, we leverage a map-based approach, allowing embeddings to capture geographical and economic features. Namely, we use a student-teacher architecture, where a student network is trained to mimic the outputs of the teacher, using two different augmented versions of the same input. To capture the rich underlying semantics of GPS coordinates, we further leverage auxiliary tasks including \\textit{geo} prediction, high-level reconstruction, and intermediate clustering. The intermediate clustering loss facilitates learning features at different levels of granularity, while the high-level reconstruction loss encourages \u201clocal-to-global\u201d correspondences. We evaluate our method on a large-scale dataset of GPS coordinates and demonstrate that it outperforms several baseline methods in terms of the quality of the learned embeddings. Moreover, we show the usefulness of our embeddings in various downstream tasks, such as predicting land price, land cover type, or water quality indice.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/bougie24a/bougie24a.pdf",
        "supp": "",
        "pdf_size": 2385259,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5818166323126508503&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Woven by Toyota, Tokyo, Japan; Woven by Toyota, Tokyo, Japan; Woven by Toyota, Tokyo, Japan",
        "aff_domain": "WOVEN.TOYOTA;WOVEN.TOYOTA;WOVEN.TOYOTA",
        "email": "WOVEN.TOYOTA;WOVEN.TOYOTA;WOVEN.TOYOTA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Toyota",
        "aff_unique_dep": "Woven",
        "aff_unique_url": "https://www.toyota-global.com",
        "aff_unique_abbr": "Toyota",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Maximization of Minimum Weighted Hamming Distance between Set Pairs",
        "site": "https://proceedings.mlr.press/v222/matsuoka24a.html",
        "author": "Tatsuya Matsuoka; Shinji Ito",
        "abstract": "\\emph{Finding diverse solutions} to combinatorial optimization problems is beneficial for a deeper understanding of complicated real-world problems and for simpler and more practical mathematical modeling. For this purpose, it is desirable that every solution is far away from one another and that solutions can be found in time not depending polynomially on the size of the family of feasible solutions. In this paper, we investigate the problem of finding diverse sets in the sense of maximizing the \\emph{minimum} of \\emph{weighted Hamming distance} between \\emph{set pairs}. Under a particular assumption, we provide an algorithm that gives diverse sets of almost $\\mu /2$-approximation in expectation in the sense of maximization of the minimum of the expected value, where $\\mu \\in [0, 1]$ is a parameter on a subroutine. We further give a hardness result that any approximation ratio better than $2/3$ is impossible in polynomial time under the assumption of $\\mathrm{P} \\neq \\mathrm{NP}$.",
        "bibtex": "@InProceedings{pmlr-v222-matsuoka24a,\n  title = \t {Maximization of Minimum Weighted Hamming Distance between Set Pairs},\n  author =       {Matsuoka, Tatsuya and Ito, Shinji},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {895--910},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/matsuoka24a/matsuoka24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/matsuoka24a.html},\n  abstract = \t {\\emph{Finding diverse solutions} to combinatorial optimization problems is beneficial for a deeper understanding of complicated real-world problems and for simpler and more practical mathematical modeling. For this purpose, it is desirable that every solution is far away from one another and that solutions can be found in time not depending polynomially on the size of the family of feasible solutions. In this paper, we investigate the problem of finding diverse sets in the sense of maximizing the \\emph{minimum} of \\emph{weighted Hamming distance} between \\emph{set pairs}. Under a particular assumption, we provide an algorithm that gives diverse sets of almost $\\mu /2$-approximation in expectation in the sense of maximization of the minimum of the expected value, where $\\mu \\in [0, 1]$ is a parameter on a subroutine. We further give a hardness result that any approximation ratio better than $2/3$ is impossible in polynomial time under the assumption of $\\mathrm{P} \\neq \\mathrm{NP}$.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/matsuoka24a/matsuoka24a.pdf",
        "supp": "",
        "pdf_size": 131967,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3894228078623761963&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "NEC Corporation; NEC Corporation",
        "aff_domain": "nec.com;nec.com",
        "email": "nec.com;nec.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec.com",
        "aff_unique_abbr": "NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Mem-Rec: Memory Efficient Recommendation System using Alternative Representation",
        "site": "https://proceedings.mlr.press/v222/jha24a.html",
        "author": "Gopi Krishna Jha; Anthony Thomas; Nilesh Jain; Sameh Gobriel; Tajana Rosing; Ravi Iyer",
        "abstract": "Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial performance burden. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages Bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements. In comparison with state-of-the-art techniques MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by $2900\\times$ and performs up to $3.4\\times$ faster embeddings while achieving the same AUC as that of the full uncompressed model.",
        "bibtex": "@InProceedings{pmlr-v222-jha24a,\n  title = \t {{Mem-Rec}: {M}emory Efficient Recommendation System using Alternative Representation},\n  author =       {Jha, Gopi Krishna and Thomas, Anthony and Jain, Nilesh and Gobriel, Sameh and Rosing, Tajana and Iyer, Ravi},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {518--533},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/jha24a/jha24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/jha24a.html},\n  abstract = \t {Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial performance burden. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages Bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements. In comparison with state-of-the-art techniques MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by $2900\\times$ and performs up to $3.4\\times$ faster embeddings while achieving the same AUC as that of the full uncompressed model.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/jha24a/jha24a.pdf",
        "supp": "",
        "pdf_size": 852489,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10008139530955029197&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Intel Labs; University of California, San Diego; Intel Labs; Intel Labs; University of California, San Diego; Intel Labs",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;1;0",
        "aff_unique_norm": "Intel Corporation;University of California, San Diego",
        "aff_unique_dep": "Intel Labs;",
        "aff_unique_url": "https://www.intel.com;https://www.ucsd.edu",
        "aff_unique_abbr": "Intel;UCSD",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Meta-forests: Domain generalization on random forests with meta-learning",
        "site": "https://proceedings.mlr.press/v222/sun24b.html",
        "author": "Yuyang Sun; Panagiotis Kosmas",
        "abstract": "Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called \"meta-forests\", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.",
        "bibtex": "@InProceedings{pmlr-v222-sun24b,\n  title = \t {{Meta-forests}: {D}omain generalization on random forests with meta-learning},\n  author =       {Sun, Yuyang and Kosmas, Panagiotis},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1292--1307},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/sun24b/sun24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/sun24b.html},\n  abstract = \t {Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called \"meta-forests\", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/sun24b/sun24b.pdf",
        "supp": "",
        "pdf_size": 393321,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1904082392511892344&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Engineering, King\u2019s College London, UK; Department of Engineering, King\u2019s College London, UK",
        "aff_domain": "kcl.ac.uk;kcl.ac.uk",
        "email": "kcl.ac.uk;kcl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "King\u2019s College London",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.kcl.ac.uk",
        "aff_unique_abbr": "KCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Mitigating Bias: Enhancing Image Classification by Improving Model Explanations",
        "site": "https://proceedings.mlr.press/v222/ahmadi24a.html",
        "author": "Raha Ahmadi; Mohammad Javad Rajabi; Mohammad Khalooie; Mohammad Sabokrou",
        "abstract": "Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model\u2019s attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient attention to the foreground. We investigate various strategies, including modifying the loss function or incorporating additional architectural components, to enable the classifier to effectively capture the primary concept within an image. Additionally, we explore the impact of different foreground attention mechanisms on model performance and provide insights into their effectiveness. Through extensive experimentation on benchmark datasets, we demonstrate the efficacy of our proposed approach in improving the classification accuracy of image classifiers. Our findings highlight the importance of foreground attention in enhancing model understanding and representation of the main concepts within images. The results of this study contribute to advancing the field of image classification and provide valuable insights for developing more robust and accurate deep-learning models.",
        "bibtex": "@InProceedings{pmlr-v222-ahmadi24a,\n  title = \t {{Mitigating Bias}: {E}nhancing Image Classification by Improving Model Explanations},\n  author =       {Ahmadi, Raha and Rajabi, Mohammad Javad and Khalooie, Mohammad and Sabokrou, Mohammad},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1--14},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ahmadi24a/ahmadi24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ahmadi24a.html},\n  abstract = \t {Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model\u2019s attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient attention to the foreground. We investigate various strategies, including modifying the loss function or incorporating additional architectural components, to enable the classifier to effectively capture the primary concept within an image. Additionally, we explore the impact of different foreground attention mechanisms on model performance and provide insights into their effectiveness. Through extensive experimentation on benchmark datasets, we demonstrate the efficacy of our proposed approach in improving the classification accuracy of image classifiers. Our findings highlight the importance of foreground attention in enhancing model understanding and representation of the main concepts within images. The results of this study contribute to advancing the field of image classification and provide valuable insights for developing more robust and accurate deep-learning models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ahmadi24a/ahmadi24a.pdf",
        "supp": "",
        "pdf_size": 11297080,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6876984485894487231&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Amirkabir University of Technology; Amirkabir University of Technology; Amirkabir University of Technology; Okinawa Institute of Science and Technology+Institute for Research in Fundamental Sciences(IPM)",
        "aff_domain": "aut.ac.ir;aut.ac.ir;aut.ac.ir;oist.jp",
        "email": "aut.ac.ir;aut.ac.ir;aut.ac.ir;oist.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1+2",
        "aff_unique_norm": "Amirkabir University of Technology;Okinawa Institute of Science and Technology;Institute for Research in Fundamental Sciences",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.aut.ac.ir;https://www.oist.jp;http://ipm.ir",
        "aff_unique_abbr": "AUT;OIST;IPM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1+0",
        "aff_country_unique": "Iran;Japan"
    },
    {
        "title": "Multi-behavior Session-based Recommendation via Graph Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v222/qin24a.html",
        "author": "Shuo Qin; Feng Lin; Lingxiao Xu; Bowen Deng; Siwen Li; Fangcheng Yang",
        "abstract": "Multi-behavior session-based recommendation (MBSBR) is a critical task in e-commerce and online advertising. By modeling these multiple behaviors, models can better capture the user intent and make more effective recommendations. However, existing models face the challenge of incompletely differentiating between different behavior types, which hinders their ability to fully capture the different tendencies exhibited by each behavior. In addition, most existing multi-behavior methods focus only on predicting a single target behavior and fail to achieve a unified model for predicting the next user-item interaction across multiple behavior types. To address these limitations, we introduce reinforcement learning to the multi-behavior session-based recommendation task and propose a novel approach called the multi-behavior graph reinforcement learning network (MB-GRL). Specifically, we use a graph neural network to encode item transition information from the session graph. Then, we use an attention network to obtain a session representation and generate recommendations based on it. At the same time, we also apply Deep Q-Network (DQN) as a regularizer to improve the recommendation performance for certain behavior types. Experiments on various public benchmark datasets show that MB-GRL outperforms other models for multi-behavior session-based recommendation.",
        "bibtex": "@InProceedings{pmlr-v222-qin24a,\n  title = \t {Multi-behavior Session-based Recommendation via Graph Reinforcement Learning},\n  author =       {Qin, Shuo and Lin, Feng and Xu, Lingxiao and Deng, Bowen and Li, Siwen and Yang, Fangcheng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1119--1134},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/qin24a/qin24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/qin24a.html},\n  abstract = \t {Multi-behavior session-based recommendation (MBSBR) is a critical task in e-commerce and online advertising. By modeling these multiple behaviors, models can better capture the user intent and make more effective recommendations. However, existing models face the challenge of incompletely differentiating between different behavior types, which hinders their ability to fully capture the different tendencies exhibited by each behavior. In addition, most existing multi-behavior methods focus only on predicting a single target behavior and fail to achieve a unified model for predicting the next user-item interaction across multiple behavior types. To address these limitations, we introduce reinforcement learning to the multi-behavior session-based recommendation task and propose a novel approach called the multi-behavior graph reinforcement learning network (MB-GRL). Specifically, we use a graph neural network to encode item transition information from the session graph. Then, we use an attention network to obtain a session representation and generate recommendations based on it. At the same time, we also apply Deep Q-Network (DQN) as a regularizer to improve the recommendation performance for certain behavior types. Experiments on various public benchmark datasets show that MB-GRL outperforms other models for multi-behavior session-based recommendation.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/qin24a/qin24a.pdf",
        "supp": "",
        "pdf_size": 565763,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0lM2akiloi0J:scholar.google.com/&scioq=Multi-behavior+Session-based+Recommendation+via+Graph+Reinforcement+Learning&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Sichuan Normal University; Sichuan Normal University; Sichuan Normal University; Sichuan Normal University; Sichuan Normal University; Sichuan Normal University",
        "aff_domain": "stu.sicnu.edu.cn;sicnu.edu.cn;stu.sicnu.edu.cn;stu.sicnu.edu.cn;stu.sicnu.edu.cn;stu.sicnu.edu.cn",
        "email": "stu.sicnu.edu.cn;sicnu.edu.cn;stu.sicnu.edu.cn;stu.sicnu.edu.cn;stu.sicnu.edu.cn;stu.sicnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Sichuan Normal University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.sctu.edu.cn",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-objective Adaptive Dynamics Attention Model to Solve Multi-objective Vehicle Routing Problem",
        "site": "https://proceedings.mlr.press/v222/luo24a.html",
        "author": "Guang Luo; Jianping Luo",
        "abstract": "Multi-objective combinatorial optimization problems (MOCOP) are commonly encountered in everyday life. However, finding the optimal solution through traditional exact and heuristic algorithms can be time-consuming due to its NP-hard nature. Fortunately, deep reinforcement learning (DRL) has shown promise in solving complex combinatorial optimization problems (COP). In this paper, we introduce a new Multi-objective Adaptive Dynamics Attention Model (MOADAM) that aims to better approximate the whole Pareto set. We modify the encoder and decoder of the model to better utilize dynamic information, and we also design a new weight sampling method to improve the model\u2019s performance for extreme solutions. Our experimental results demonstrate that our proposed model outperforms the current state-of-the-art algorithm in terms of solution quality on multi-objective vehicle routing problems with capacity constraints (MOCVRP).",
        "bibtex": "@InProceedings{pmlr-v222-luo24a,\n  title = \t {Multi-objective Adaptive Dynamics Attention Model to Solve Multi-objective Vehicle Routing Problem},\n  author =       {Luo, Guang and Luo, Jianping},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {834--849},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/luo24a/luo24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/luo24a.html},\n  abstract = \t {Multi-objective combinatorial optimization problems (MOCOP) are commonly encountered in everyday life. However, finding the optimal solution through traditional exact and heuristic algorithms can be time-consuming due to its NP-hard nature. Fortunately, deep reinforcement learning (DRL) has shown promise in solving complex combinatorial optimization problems (COP). In this paper, we introduce a new Multi-objective Adaptive Dynamics Attention Model (MOADAM) that aims to better approximate the whole Pareto set. We modify the encoder and decoder of the model to better utilize dynamic information, and we also design a new weight sampling method to improve the model\u2019s performance for extreme solutions. Our experimental results demonstrate that our proposed model outperforms the current state-of-the-art algorithm in terms of solution quality on multi-objective vehicle routing problems with capacity constraints (MOCVRP).}\n}",
        "pdf": "https://proceedings.mlr.press/v222/luo24a/luo24a.pdf",
        "supp": "",
        "pdf_size": 1842136,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16153401903840162768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Guangdong Key Laboratory of Intelligent Information Processing, College of Electronic and Information Engineering, Shenzhen University, China; Guangdong Key Laboratory of Intelligent Information Processing, College of Electronic and Information Engineering, Shenzhen University, China",
        "aff_domain": "email.szu.edu.cn;szu.edu.cn",
        "email": "email.szu.edu.cn;szu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shenzhen University",
        "aff_unique_dep": "College of Electronic and Information Engineering",
        "aff_unique_url": "https://www.szu.edu.cn",
        "aff_unique_abbr": "SZU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Optimal Nonlinearities Improve Generalization Performance of Random Features",
        "site": "https://proceedings.mlr.press/v222/demir24a.html",
        "author": "Samet Demir; Zafer Do\u011fan",
        "abstract": "Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the \"parameters\" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than widely-used nonlinear functions such as ReLU. Furthermore, we illustrate that the proposed nonlinearities also mitigate the so-called double descent phenomenon, which is known as the non-monotonic generalization performance regarding the sample size and the model size.",
        "bibtex": "@InProceedings{pmlr-v222-demir24a,\n  title = \t {Optimal Nonlinearities Improve Generalization Performance of Random Features},\n  author =       {Demir, Samet and Do\\u{g}an, Zafer},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {247--262},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/demir24a/demir24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/demir24a.html},\n  abstract = \t {Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the \"parameters\" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than widely-used nonlinear functions such as ReLU. Furthermore, we illustrate that the proposed nonlinearities also mitigate the so-called double descent phenomenon, which is known as the non-monotonic generalization performance regarding the sample size and the model size.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/demir24a/demir24a.pdf",
        "supp": "",
        "pdf_size": 1748309,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9685144915085103771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Machine Learning and Information Processing Group, KUIS AI Center, Ko\u00e7 University, Turkey; Machine Learning and Information Processing Group, KUIS AI Center, Ko\u00e7 University, Turkey+Electrical and Electronics Engineering, Ko\u00e7 University, Turkey",
        "aff_domain": "ku.edu.tr;ku.edu.tr",
        "email": "ku.edu.tr;ku.edu.tr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0",
        "aff_unique_norm": "Ko\u00e7 University",
        "aff_unique_dep": "Machine Learning and Information Processing Group",
        "aff_unique_url": "https://www.ku.edu.tr",
        "aff_unique_abbr": "Ko\u00e7",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Turkey"
    },
    {
        "title": "Outlier Robust Adversarial Training",
        "site": "https://proceedings.mlr.press/v222/hu24a.html",
        "author": "Shu Hu; Zhenhuan Yang; Xin Wang; Yiming Ying; Siwei Lyu",
        "abstract": "Supervised learning models are challenged by the intrinsic complexities of training data such as outliers and minority subpopulations and intentional attacks at inference time with adversarial samples. While traditional robust learning methods and the recent adversarial training approaches are designed to handle each of the two challenges, to date, no work has been done to develop models that are robust with regard to the low-quality training data and the potential adversarial attack at inference time simultaneously. It is for this reason that we introduce \\underline{O}utlier \\underline{R}obust \\underline{A}dversarial \\underline{T}raining (ORAT) in this work. ORAT is based on a bi-level optimization formulation of adversarial training with a robust rank-based loss function. Theoretically, we show that the learning objective of ORAT satisfies the $\\mathcal{H}$-consistency in binary classification, which establishes it as a proper surrogate to adversarial 0/1 loss. Furthermore, we analyze its generalization ability and provide uniform convergence rates in high probability. ORAT can be optimized with a simple algorithm. Experimental evaluations on three benchmark datasets demonstrate the effectiveness and robustness of ORAT in handling outliers and adversarial attacks. Our code is available at \\url{https://github.com/discovershu/ORAT}.",
        "bibtex": "@InProceedings{pmlr-v222-hu24a,\n  title = \t {Outlier Robust Adversarial Training},\n  author =       {Hu, Shu and Yang, Zhenhuan and Wang, Xin and Ying, Yiming and Lyu, Siwei},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {454--469},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/hu24a/hu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/hu24a.html},\n  abstract = \t {Supervised learning models are challenged by the intrinsic complexities of training data such as outliers and minority subpopulations and intentional attacks at inference time with adversarial samples. While traditional robust learning methods and the recent adversarial training approaches are designed to handle each of the two challenges, to date, no work has been done to develop models that are robust with regard to the low-quality training data and the potential adversarial attack at inference time simultaneously. It is for this reason that we introduce \\underline{O}utlier \\underline{R}obust \\underline{A}dversarial \\underline{T}raining (ORAT) in this work. ORAT is based on a bi-level optimization formulation of adversarial training with a robust rank-based loss function. Theoretically, we show that the learning objective of ORAT satisfies the $\\mathcal{H}$-consistency in binary classification, which establishes it as a proper surrogate to adversarial 0/1 loss. Furthermore, we analyze its generalization ability and provide uniform convergence rates in high probability. ORAT can be optimized with a simple algorithm. Experimental evaluations on three benchmark datasets demonstrate the effectiveness and robustness of ORAT in handling outliers and adversarial attacks. Our code is available at \\url{https://github.com/discovershu/ORAT}.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/hu24a/hu24a.pdf",
        "supp": "",
        "pdf_size": 1459152,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=866946738225240122&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer and Information Technology, Purdue School of Engineering and Technology, Indiana University\u2013Purdue University Indianapolis, Indianapolis, IN, 46202. USA; Etsy, Inc, Brooklyn, New York, USA; Department of Epidemiology and Biostatistics, School of Public Health, University at Albany, State University of New York, Albany, NY 12222, USA; Department of Mathematics and Statistics, University at Albany, State University of New York, Albany, NY 12222, USA; Department of Computer Science and Engineering, University at Buffalo, State University of New York, Buffalo, NY 14260-2500, USA",
        "aff_domain": "purdue.edu;hotmail.com;albany.edu;albany.edu;buffalo.edu",
        "email": "purdue.edu;hotmail.com;albany.edu;albany.edu;buffalo.edu",
        "github": "https://github.com/discovershu/ORAT",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;3",
        "aff_unique_norm": "Indiana University\u2013Purdue University Indianapolis;Etsy, Inc;University at Albany, State University of New York;University at Buffalo, State University of New York",
        "aff_unique_dep": "Department of Computer and Information Technology;;Department of Epidemiology and Biostatistics;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iupui.edu;https://www.etsy.com;https://www.albany.edu;https://www.buffalo.edu",
        "aff_unique_abbr": "IUPUI;Etsy;UAlbany;UB",
        "aff_campus_unique_index": "0;1;2;2;3",
        "aff_campus_unique": "Indianapolis;Brooklyn;Albany;Buffalo",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Overcoming catastrophic forgetting with classifier expander",
        "site": "https://proceedings.mlr.press/v222/liu24b.html",
        "author": "Xinchen Liu; Hongbo Wang; Yingjian Tian; Linyao Xie",
        "abstract": "It is essential for models to gradually adapt to the world\u2019s increasing complexity, and we can use models more effectively if they keep up with the times. However, the technique of continuous learning (CL) has an issue with catastrophic forgetting, and effective continuous learning methods can only be attained by effectively limiting forgetting and learning new tasks. In this study, we offer the Classifier Expander(CE) method, which combines the regularization-based and replay-based approaches. By undergoing two stages of training, it fulfills the aforementioned standards. The training content for the new task is limited to the portion of the network relevant to that task in the first stage, which uses the replay approach to reduce the forgetting problem. This strategy minimizes disruption to the old task while facilitating efficient learning of the new one. Utilizing all of the data available, the second stage retrains the network and sufficiently trains the classifier to balance the learning performance of the old and new tasks. Our method regularly outperforms previous CL methods on the CIFAR-100 and CUB-200 datasets, obtaining an average improvement of 2.94% on the class-incremental learning and 1.16% on the task-incremental learning compared to the best method currently available. Our code is available at https://github.com/EmbraceTomorrow/CE.",
        "bibtex": "@InProceedings{pmlr-v222-liu24b,\n  title = \t {Overcoming catastrophic forgetting with classifier expander},\n  author =       {Liu, Xinchen and Wang, Hongbo and Tian, Yingjian and Xie, Linyao},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {803--817},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/liu24b/liu24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/liu24b.html},\n  abstract = \t {It is essential for models to gradually adapt to the world\u2019s increasing complexity, and we can use models more effectively if they keep up with the times. However, the technique of continuous learning (CL) has an issue with catastrophic forgetting, and effective continuous learning methods can only be attained by effectively limiting forgetting and learning new tasks. In this study, we offer the Classifier Expander(CE) method, which combines the regularization-based and replay-based approaches. By undergoing two stages of training, it fulfills the aforementioned standards. The training content for the new task is limited to the portion of the network relevant to that task in the first stage, which uses the replay approach to reduce the forgetting problem. This strategy minimizes disruption to the old task while facilitating efficient learning of the new one. Utilizing all of the data available, the second stage retrains the network and sufficiently trains the classifier to balance the learning performance of the old and new tasks. Our method regularly outperforms previous CL methods on the CIFAR-100 and CUB-200 datasets, obtaining an average improvement of 2.94% on the class-incremental learning and 1.16% on the task-incremental learning compared to the best method currently available. Our code is available at https://github.com/EmbraceTomorrow/CE.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/liu24b/liu24b.pdf",
        "supp": "",
        "pdf_size": 2112968,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6mi71ciSOoEJ:scholar.google.com/&scioq=Overcoming+catastrophic+forgetting+with+classifier+expander&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/EmbraceTomorrow/CE",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy",
        "site": "https://proceedings.mlr.press/v222/sun24a.html",
        "author": "Ke Sun; Bing Yu; Zhouchen Lin; Zhanxing Zhu",
        "abstract": "Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \\textbf{Patch-level Neighborhood Interpolation\u00a0(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \\textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. The second derived \\textbf{Pani MixUp} method extends the MixUp, and achieves superiority over MixUp and competitive performance over state-of-the-art variants of MixUp method with a significant advantage in computational efficiency. Extensive experiments have verified the effectiveness of our Pani approach in both supervised and semi-supervised settings.",
        "bibtex": "@InProceedings{pmlr-v222-sun24a,\n  title = \t {Patch-level Neighborhood Interpolation: {A} General and Effective Graph-based Regularization Strategy},\n  author =       {Sun, Ke and Yu, Bing and Lin, Zhouchen and Zhu, Zhanxing},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1276--1291},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/sun24a/sun24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/sun24a.html},\n  abstract = \t {Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \\textbf{Patch-level Neighborhood Interpolation\u00a0(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \\textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. The second derived \\textbf{Pani MixUp} method extends the MixUp, and achieves superiority over MixUp and competitive performance over state-of-the-art variants of MixUp method with a significant advantage in computational efficiency. Extensive experiments have verified the effectiveness of our Pani approach in both supervised and semi-supervised settings.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/sun24a/sun24a.pdf",
        "supp": "",
        "pdf_size": 1181628,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8173276351083573417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Data Science, Peking University; School of Mathematical Sciences, Peking University; National Key Lab of General AI, School of Intelligence Science and Technology, Peking University + Institute for Artificial Intelligence, Peking University + Peng Cheng Laboratory; School of Mathematical Sciences, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+0+1;0",
        "aff_unique_norm": "Peking University;Peng Cheng Laboratory",
        "aff_unique_dep": "Center for Data Science;",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.pcl.ac.cn",
        "aff_unique_abbr": "PKU;PCL",
        "aff_campus_unique_index": "0;0;;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Pedestrian Cross Forecasting with Hybrid Feature Fusion",
        "site": "https://proceedings.mlr.press/v222/dong24a.html",
        "author": "Meng Dong",
        "abstract": "Forecasting the crossing intention of pedestrians is an essential task for the safe driving of Autonomous Vehicles (AVs) in the real world. Pedestrians\u2019 behaviors are usually influenced by their surroundings in traffic scenes. Recent works based on vision-based neural networks extract key information from images to perform prediction. However, in the driving environment, there exists much critical information, such as the social and scene interaction in the driving area, the location and distance between the ego car and target pedestrian, and the motion of all targets. How properly exploring and utilizing the above implicit interactions will promote the development of Autonomous Vehicles. In this chapter, two novel attributes, the pedestrian\u2019s location on the road or sidewalk, and the relative distance from the target pedestrian to the ego-car, which are derived from the semantic map and depth map combined with bounding boxes, are introduced. A hybrid prediction network based on multi-modal is proposed to capture the interactions between all the features and predict pedestrian crossing intention. Evaluated by two public pedestrian crossing datasets, PIE and JAAD, the proposed hybrid framework outperforms the state-of-the-art by about an accuracy of 3%.",
        "bibtex": "@InProceedings{pmlr-v222-dong24a,\n  title = \t {Pedestrian Cross Forecasting with Hybrid Feature Fusion},\n  author =       {Dong, Meng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {327--342},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/dong24a/dong24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/dong24a.html},\n  abstract = \t {Forecasting the crossing intention of pedestrians is an essential task for the safe driving of Autonomous Vehicles (AVs) in the real world. Pedestrians\u2019 behaviors are usually influenced by their surroundings in traffic scenes. Recent works based on vision-based neural networks extract key information from images to perform prediction. However, in the driving environment, there exists much critical information, such as the social and scene interaction in the driving area, the location and distance between the ego car and target pedestrian, and the motion of all targets. How properly exploring and utilizing the above implicit interactions will promote the development of Autonomous Vehicles. In this chapter, two novel attributes, the pedestrian\u2019s location on the road or sidewalk, and the relative distance from the target pedestrian to the ego-car, which are derived from the semantic map and depth map combined with bounding boxes, are introduced. A hybrid prediction network based on multi-modal is proposed to capture the interactions between all the features and predict pedestrian crossing intention. Evaluated by two public pedestrian crossing datasets, PIE and JAAD, the proposed hybrid framework outperforms the state-of-the-art by about an accuracy of 3%.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/dong24a/dong24a.pdf",
        "supp": "",
        "pdf_size": 1818791,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10876575285205119309&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Probing Traffic Trend Forecasting via Spatial-Temporal Aware Learning-Graph Attention",
        "site": "https://proceedings.mlr.press/v222/huang24b.html",
        "author": "Xinyuan Huang; Qianqian Ren",
        "abstract": "Traffic forecasting plays an extremely important role in many applications such as intelligent transportation and smart cities. However, due to the hidden and complex dynamic spatio-temporal correlations and heterogeneity, achieving high-precision traffic prediction is a challenging task. This paper proposes a new spatio-temporal aware learning graph neural network (STALGNN) for traffic prediction. First, a temporal-aware graph generation module is designed to exploit the spatial-temporal features that the spatial graph may not be able to present. Then, a spatio-temporal joint module is designed to more effectively capture local spatio-temporal correlations. Next, a multi-scale gated convolutions module is proposed to capture gloable dynamic spatio-temporal correlations. Furthermore, STALGNN further learns explicit  spatio-temporal correlations through integrated attention mechanisms and stacked graph convolutional networks to handle long-term prediction. Extensive experiments on several real traffic datasets show that the proposed method can achieve the superior performance compared with other baselines.",
        "bibtex": "@InProceedings{pmlr-v222-huang24b,\n  title = \t {Probing Traffic Trend Forecasting via Spatial-Temporal Aware Learning-Graph Attention},\n  author =       {Huang, Xinyuan and Ren, Qianqian},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {486--501},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/huang24b/huang24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/huang24b.html},\n  abstract = \t {Traffic forecasting plays an extremely important role in many applications such as intelligent transportation and smart cities. However, due to the hidden and complex dynamic spatio-temporal correlations and heterogeneity, achieving high-precision traffic prediction is a challenging task. This paper proposes a new spatio-temporal aware learning graph neural network (STALGNN) for traffic prediction. First, a temporal-aware graph generation module is designed to exploit the spatial-temporal features that the spatial graph may not be able to present. Then, a spatio-temporal joint module is designed to more effectively capture local spatio-temporal correlations. Next, a multi-scale gated convolutions module is proposed to capture gloable dynamic spatio-temporal correlations. Furthermore, STALGNN further learns explicit  spatio-temporal correlations through integrated attention mechanisms and stacked graph convolutional networks to handle long-term prediction. Extensive experiments on several real traffic datasets show that the proposed method can achieve the superior performance compared with other baselines.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/huang24b/huang24b.pdf",
        "supp": "",
        "pdf_size": 7771351,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GruLPXitqGEJ:scholar.google.com/&scioq=Probing+Traffic+Trend+Forecasting+via+Spatial-Temporal+Aware+Learning-Graph+Attention&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Heilongjiang University; Heilongjiang University",
        "aff_domain": "gmail.com;hlju.edu.cn",
        "email": "gmail.com;hlju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Heilongjiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.hljnu.edu.cn",
        "aff_unique_abbr": "HGHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning",
        "site": "https://proceedings.mlr.press/v222/baykal24a.html",
        "author": "Gulcin Baykal; Halil Faruk Karagoz; Taha Binhuraib; Gozde Unal",
        "abstract": "Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.",
        "bibtex": "@InProceedings{pmlr-v222-baykal24a,\n  title = \t {{ProtoDiffusion}: {C}lassifier-Free Diffusion Guidance with Prototype Learning},\n  author =       {Baykal, Gulcin and Karagoz, Halil Faruk and Binhuraib, Taha and Unal, Gozde},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {106--120},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/baykal24a/baykal24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/baykal24a.html},\n  abstract = \t {Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/baykal24a/baykal24a.pdf",
        "supp": "",
        "pdf_size": 6615281,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1546651658620595143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Istanbul Technical University, Computer Engineering Department, Istanbul, Turkey; Istanbul Technical University, Computer Engineering Department, Istanbul, Turkey; Novus Technologies, Boston, USA; Istanbul Technical University, AI and Data Engineering Department, Istanbul, Turkey",
        "aff_domain": "itu.edu.tr;gmail.com;gmail.com;itu.edu.tr",
        "email": "itu.edu.tr;gmail.com;gmail.com;itu.edu.tr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Istanbul Technical University;Novus Technologies",
        "aff_unique_dep": "Computer Engineering Department;",
        "aff_unique_url": "https://www.itu.edu.tr;",
        "aff_unique_abbr": "ITU;",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Istanbul;Boston",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Turkey;United States"
    },
    {
        "title": "Prototypical Model with Information-Theoretic Loss Functions for Generalized Zero-Shot Learning",
        "site": "https://proceedings.mlr.press/v222/ji24c.html",
        "author": "Chunlin Ji; Zhan Xiong; Meiying Zhang; Huiwen Yang; Feng Chen; Hanchun Shen",
        "abstract": "Generalized zero-shot learning (GZSL) is still a technical challenge of deep learning. To preserve the semantic relation between source and target classes when only trained with data from source classes, we address the quantification of the knowledge transfer from an information-theoretic viewpoint. We use the prototypical model and format the variables of concern as a probability vector. Taking advantage of the probability vector representation, information measurements can be effectively evaluated with simple closed forms. We propose two information-theoretic loss functions: a mutual information loss to bridge seen data and target classes; an uncertainty-aware entropy constraint loss to prevent overfitting when using seen data to learn the embedding of target classes. Simulation shows that, as a deterministic model, our proposed method obtains state-of-the-art results on GZSL benchmark datasets. We achieve 21% \u2212 64% improvements over the baseline model \u2013 deep calibration network (DCN) and demonstrate that a deterministic model can perform as well as generative ones. Furthermore, the proposed method is compatible with generative models and can noticeably improve their performance.",
        "bibtex": "@InProceedings{pmlr-v222-ji24c,\n  title = \t {Prototypical Model with Information-Theoretic Loss Functions for Generalized Zero-Shot Learning},\n  author =       {Ji, Chunlin and Xiong, Zhan and Zhang, Meiying and Yang, Huiwen and Chen, Feng and Shen, Hanchun},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {566--581},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ji24c/ji24c.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ji24c.html},\n  abstract = \t {Generalized zero-shot learning (GZSL) is still a technical challenge of deep learning. To preserve the semantic relation between source and target classes when only trained with data from source classes, we address the quantification of the knowledge transfer from an information-theoretic viewpoint. We use the prototypical model and format the variables of concern as a probability vector. Taking advantage of the probability vector representation, information measurements can be effectively evaluated with simple closed forms. We propose two information-theoretic loss functions: a mutual information loss to bridge seen data and target classes; an uncertainty-aware entropy constraint loss to prevent overfitting when using seen data to learn the embedding of target classes. Simulation shows that, as a deterministic model, our proposed method obtains state-of-the-art results on GZSL benchmark datasets. We achieve 21% \u2212 64% improvements over the baseline model \u2013 deep calibration network (DCN) and demonstrate that a deterministic model can perform as well as generative ones. Furthermore, the proposed method is compatible with generative models and can noticeably improve their performance.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ji24c/ji24c.pdf",
        "supp": "",
        "pdf_size": 544964,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Ax7rgf_H7tIJ:scholar.google.com/&scioq=Prototypical+Model+with+Information-Theoretic+Loss+Functions+for+Generalized+Zero-Shot+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Kuang-Chi Institute of Advanced Technology, Shenzhen, China; Kuang-Chi Institute of Advanced Technology, Shenzhen, China; Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen, China; University of California, Berkeley, CA, USA; Kuang-Chi Institute of Advanced Technology, Shenzhen, China; Kuang-Chi Institute of Advanced Technology, Shenzhen, China",
        "aff_domain": "kuang-chi.org;kuang-chi.com;sustech.edu.cn;berkeley.edu;kuang-chi.com;kuang-chi.com",
        "email": "kuang-chi.org;kuang-chi.com;sustech.edu.cn;berkeley.edu;kuang-chi.com;kuang-chi.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Kuang-Chi Institute of Advanced Technology;Southern University of Science and Technology;University of California, Berkeley",
        "aff_unique_dep": ";Research Institute of Trustworthy Autonomous Systems;",
        "aff_unique_url": "http://www.kuangchi.org/;https://www.sustech.edu.cn;https://www.berkeley.edu",
        "aff_unique_abbr": ";SUSTech;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;1;0;0",
        "aff_campus_unique": "Shenzhen;Berkeley",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation",
        "site": "https://proceedings.mlr.press/v222/jiang24a.html",
        "author": "Junqi Jiang; Jianglin Lan; Francesco Leofante; Antonio Rago; Francesca Toni",
        "abstract": "Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for closeness and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limitations in the literature. We formulate an iterative algorithm to compute provably robust CEs and prove its convergence, soundness and completeness. Through a comparative experiment involving six baselines, five of which target robustness, we show that PROPLACE achieves state-of-the-art performances against metrics on three evaluation aspects.",
        "bibtex": "@InProceedings{pmlr-v222-jiang24a,\n  title = \t {Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation},\n  author =       {Jiang, Junqi and Lan, Jianglin and Leofante, Francesco and Rago, Antonio and Toni, Francesca},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {582--597},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/jiang24a/jiang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/jiang24a.html},\n  abstract = \t {Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for closeness and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limitations in the literature. We formulate an iterative algorithm to compute provably robust CEs and prove its convergence, soundness and completeness. Through a comparative experiment involving six baselines, five of which target robustness, we show that PROPLACE achieves state-of-the-art performances against metrics on three evaluation aspects.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/jiang24a/jiang24a.pdf",
        "supp": "",
        "pdf_size": 349702,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4532691169095567302&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computing, Imperial College London; James Watt School of Engineering, University of Glasgow; Department of Computing, Imperial College London; Department of Computing, Imperial College London; Department of Computing, Imperial College London",
        "aff_domain": "imperial.ac.uk;glasgow.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;glasgow.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "github": "https://github.com/junqi-jiang/proplace",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Imperial College London;University of Glasgow",
        "aff_unique_dep": "Department of Computing;James Watt School of Engineering",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.gla.ac.uk",
        "aff_unique_abbr": "Imperial;UoG",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "London;Glasgow",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Q-Match: Self-Supervised Learning by Matching Distributions Induced by a Queue",
        "site": "https://proceedings.mlr.press/v222/mulc24a.html",
        "author": "Thomas Mulc; Debidatta Dwibedi",
        "abstract": "In semi-supervised learning, student-teacher distribution matching has been successful in improving performance of models using unlabeled data in conjunction with few labeled samples. In this paper, we aim to replicate that success in the self-supervised setup where we do not have access to any labeled data during pre-training. We introduce our algorithm, Q-Match, and show it is possible to induce the student-teacher distributions without any knowledge of downstream classes by using a queue of embeddings of samples from the unlabeled dataset. We focus our study on tabular datasets and show that Q-Match outperforms previous self-supervised learning techniques when measuring downstream classification performance. Furthermore, we show that our method is sample efficient\u2013in terms of both the labels required for downstream training and the amount of unlabeled data required for pre-training\u2013and scales well to the sizes of both the labeled and unlabeled data.",
        "bibtex": "@InProceedings{pmlr-v222-mulc24a,\n  title = \t {{Q-Match}: {S}elf-Supervised Learning by Matching Distributions Induced by a Queue},\n  author =       {Mulc, Thomas and Dwibedi, Debidatta},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {911--926},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/mulc24a/mulc24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/mulc24a.html},\n  abstract = \t {In semi-supervised learning, student-teacher distribution matching has been successful in improving performance of models using unlabeled data in conjunction with few labeled samples. In this paper, we aim to replicate that success in the self-supervised setup where we do not have access to any labeled data during pre-training. We introduce our algorithm, Q-Match, and show it is possible to induce the student-teacher distributions without any knowledge of downstream classes by using a queue of embeddings of samples from the unlabeled dataset. We focus our study on tabular datasets and show that Q-Match outperforms previous self-supervised learning techniques when measuring downstream classification performance. Furthermore, we show that our method is sample efficient\u2013in terms of both the labels required for downstream training and the amount of unlabeled data required for pre-training\u2013and scales well to the sizes of both the labeled and unlabeled data.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/mulc24a/mulc24a.pdf",
        "supp": "",
        "pdf_size": 796820,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rU8JchDEGhUJ:scholar.google.com/&scioq=Q-Match:+Self-Supervised+Learning+by+Matching+Distributions+Induced+by+a+Queue&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Google; Google Deepmind",
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;DeepMind",
        "aff_unique_dep": ";DeepMind",
        "aff_unique_url": "https://www.google.com;https://deepmind.com",
        "aff_unique_abbr": "Google;DeepMind",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Reinforcement Learning for Solving Stochastic Vehicle Routing Problem",
        "site": "https://proceedings.mlr.press/v222/iklassov24a.html",
        "author": "Zangir Iklassov; Ikboljon Sobirov; Ruben Solozabal; Martin Tak\u00e1\u010d",
        "abstract": "This study addresses a gap in the utilization of Reinforcement Learning (RL) and Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing Problem (SVRP) that involves the challenging task of optimizing vehicle routes under uncertain conditions. We propose a novel end-to-end framework that comprehensively addresses the key sources of stochasticity in SVRP and utilizes an RL agent with a simple yet effective architecture and a tailored training method. Through comparative analysis, our proposed model demonstrates superior performance compared to a widely adopted state-of-the-art metaheuristic, achieving a significant 3.43% reduction in travel costs. Furthermore, the model exhibits robustness across diverse SVRP settings, highlighting its adaptability and ability to learn optimal routing strategies in varying environments. The publicly available implementation of our framework serves as a valuable resource for future research endeavors aimed at advancing RL-based solutions for SVRP.",
        "bibtex": "@InProceedings{pmlr-v222-iklassov24a,\n  title = \t {Reinforcement Learning for Solving Stochastic Vehicle Routing Problem},\n  author =       {Iklassov, Zangir and Sobirov, Ikboljon and Solozabal, Ruben and Tak\\'{a}\\v{c}, Martin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {502--517},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/iklassov24a/iklassov24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/iklassov24a.html},\n  abstract = \t {This study addresses a gap in the utilization of Reinforcement Learning (RL) and Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing Problem (SVRP) that involves the challenging task of optimizing vehicle routes under uncertain conditions. We propose a novel end-to-end framework that comprehensively addresses the key sources of stochasticity in SVRP and utilizes an RL agent with a simple yet effective architecture and a tailored training method. Through comparative analysis, our proposed model demonstrates superior performance compared to a widely adopted state-of-the-art metaheuristic, achieving a significant 3.43% reduction in travel costs. Furthermore, the model exhibits robustness across diverse SVRP settings, highlighting its adaptability and ability to learn optimal routing strategies in varying environments. The publicly available implementation of our framework serves as a valuable resource for future research endeavors aimed at advancing RL-based solutions for SVRP.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/iklassov24a/iklassov24a.pdf",
        "supp": "",
        "pdf_size": 366183,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10795993029196445358&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "MBZUAI, UAE, Abu-Dhabi; MBZUAI, UAE, Abu-Dhabi; MBZUAI, UAE, Abu-Dhabi; MBZUAI, UAE, Abu-Dhabi",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Mohamed Bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mbziai.ac.ae",
        "aff_unique_abbr": "MBZUAI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Abu-Dhabi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Arab Emirates"
    },
    {
        "title": "Remote Wildfire Detection using Multispectral Satellite Imagery and Vision Transformers",
        "site": "https://proceedings.mlr.press/v222/rad24a.html",
        "author": "Ryan Rad",
        "abstract": "Wildfires pose a significant and recurring challenge in North America, impacting both human and natural environments. The size and severity of wildfires in the region have been increasing in recent years, making it a pressing concern for communities, ecosystems, and the economy. The accurate and timely detection of active wildfires in remote areas is crucial for effective wildfire management and mitigation efforts. In this research paper, we propose a robust approach for detecting active wildfires using multispectral satellite imagery by leveraging vision transformers and a vast repository of landsat-$8$ satellite data with a $30$m spatial resolution in North America. Our methodology involves experimenting with vision transformers and deep convolutional neural networks for wildfire detection in multispectral satellite images. We compare the capabilities of these two architecture families in detecting wildfires within the multispectral satellite imagery. Furthermore, we propose a novel u-shape vision transformer that effectively captures spatial dependencies and learns meaningful representations from multispectral images, enabling precise discrimination between wildfire and non-wildfire regions. To evaluate the performance of our approach, we conducted experiments on a comprehensive dataset of wildfire incidents. The results demonstrate the effectiveness of the proposed method in accurately detecting active wildfires with an \\textit{Dice Score or F$1$} of $%90.05$ and \\textit{Recall} of $%89.61$ . Overall, our research presents a promising approach for leveraging vision transformers for multispectral satellite imagery to detect remote wildfires.",
        "bibtex": "@InProceedings{pmlr-v222-rad24a,\n  title = \t {Remote Wildfire Detection using Multispectral Satellite Imagery and Vision Transformers},\n  author =       {Rad, Ryan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1135--1150},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/rad24a/rad24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/rad24a.html},\n  abstract = \t {Wildfires pose a significant and recurring challenge in North America, impacting both human and natural environments. The size and severity of wildfires in the region have been increasing in recent years, making it a pressing concern for communities, ecosystems, and the economy. The accurate and timely detection of active wildfires in remote areas is crucial for effective wildfire management and mitigation efforts. In this research paper, we propose a robust approach for detecting active wildfires using multispectral satellite imagery by leveraging vision transformers and a vast repository of landsat-$8$ satellite data with a $30$m spatial resolution in North America. Our methodology involves experimenting with vision transformers and deep convolutional neural networks for wildfire detection in multispectral satellite images. We compare the capabilities of these two architecture families in detecting wildfires within the multispectral satellite imagery. Furthermore, we propose a novel u-shape vision transformer that effectively captures spatial dependencies and learns meaningful representations from multispectral images, enabling precise discrimination between wildfire and non-wildfire regions. To evaluate the performance of our approach, we conducted experiments on a comprehensive dataset of wildfire incidents. The results demonstrate the effectiveness of the proposed method in accurately detecting active wildfires with an \\textit{Dice Score or F$1$} of $%90.05$ and \\textit{Recall} of $%89.61$ . Overall, our research presents a promising approach for leveraging vision transformers for multispectral satellite imagery to detect remote wildfires.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/rad24a/rad24a.pdf",
        "supp": "",
        "pdf_size": 7277028,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:jU2h5QtmBIMJ:scholar.google.com/&scioq=Remote+Wildfire+Detection+using+Multispectral+Satellite+Imagery+and+Vision+Transformers&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Khoury College of Computer Science, Northeastern University, Vancouver, BC, Canada",
        "aff_domain": "northeastern.edu",
        "email": "northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "Khoury College of Computer Science",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Revisiting Structured Dropout",
        "site": "https://proceedings.mlr.press/v222/zhao24a.html",
        "author": "Yiren Zhao; Oluwatomisin Dada; Robert Mullins; Xitong Gao",
        "abstract": "Large neural networks are often overparameterised and prone to overfitting, Dropout is a widely used regularization technique to combat overfitting and improve model generalization. However, unstructured Dropout is not always effective for specific network architectures and this has led to the formation of multiple structured Dropout approaches to improve model performance and, sometimes, reduce the computational resources required for inference. In this work, we revisit structured Dropout comparing different Dropout approaches on natural language processing and computer vision tasks for multiple state-of-the-art networks. Additionally, we devise an approach to structured Dropout we call \\textbf{\\emph{ProbDropBlock}} which drops contiguous blocks from feature maps with a probability given by the normalized feature salience values. We find that, with a simple scheduling strategy, the proposed approach to structured Dropout consistently improves model performance compared to baselines and other Dropout approaches on a diverse range of tasks and models. In particular, we show \\textbf{\\emph{ProbDropBlock}} improves RoBERTa finetuning on MNLI by $0.22%$, and training of ResNet50 on ImageNet by $0.28%$.",
        "bibtex": "@InProceedings{pmlr-v222-zhao24a,\n  title = \t {Revisiting Structured Dropout},\n  author =       {Zhao, Yiren and Dada, Oluwatomisin and Mullins, Robert and Gao, Xitong},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1699--1714},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhao24a/zhao24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhao24a.html},\n  abstract = \t {Large neural networks are often overparameterised and prone to overfitting, Dropout is a widely used regularization technique to combat overfitting and improve model generalization. However, unstructured Dropout is not always effective for specific network architectures and this has led to the formation of multiple structured Dropout approaches to improve model performance and, sometimes, reduce the computational resources required for inference. In this work, we revisit structured Dropout comparing different Dropout approaches on natural language processing and computer vision tasks for multiple state-of-the-art networks. Additionally, we devise an approach to structured Dropout we call \\textbf{\\emph{ProbDropBlock}} which drops contiguous blocks from feature maps with a probability given by the normalized feature salience values. We find that, with a simple scheduling strategy, the proposed approach to structured Dropout consistently improves model performance compared to baselines and other Dropout approaches on a diverse range of tasks and models. In particular, we show \\textbf{\\emph{ProbDropBlock}} improves RoBERTa finetuning on MNLI by $0.22%$, and training of ResNet50 on ImageNet by $0.28%$.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhao24a/zhao24a.pdf",
        "supp": "",
        "pdf_size": 392258,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7344619094915077763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Imperial College London; University of Cambridge; University of Cambridge; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "aff_domain": "ic.ac.uk;cam.ac.uk;cl.cam.ac.uk;siat.ac.cn",
        "email": "ic.ac.uk;cam.ac.uk;cl.cam.ac.uk;siat.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Imperial College London;University of Cambridge;Shenzhen Institute of Advanced Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.cam.ac.uk;http://www.siat.cas.cn",
        "aff_unique_abbr": "ICL;Cambridge;SIAT",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Cambridge;Shenzhen",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "title": "Robust Blind Watermarking Framework for Hybrid Networks Combining CNN and Transformer",
        "site": "https://proceedings.mlr.press/v222/wang24a.html",
        "author": "Baowei Wang; Ziwei Song; Yufeng Wu",
        "abstract": "As an essential means of copyright protection, the deep learning-based robust watermarking method is being studied extensively. Its framework consists of three main parts: the encoder, the noise layer and the decoder. But practically all of the schemes are directed at the encoder rather than the decoder. And the whole network is structured by shallow Convolutional Neural Networks (CNNs) for primary feature extraction, while CNNs capture local information and do not model non-local information in watermarked images well. To solve this problem, we consider the use of Transformer networks with a spatially self-attention mechanism. We propose to construct a novel decoder network by combining Transformer and CNNs, which can not only enriches local feature information but also enhances the ability to explore global representations. Meanwhile, to embed secret messages more perfectly, we design a multi-scale attentional feature fusion module to achieve an efficient aggregation of cover image features and secret message features, resulting in the encoded images with rich hybrid features. In addition, perceptual loss is introduced to better evaluate the visual quality of the watermarked images. Extensive experimental results show that our proposed method achieves better results in terms of imperceptibility and robustness compared with existing State-Of-The-Art (SOTA) methods.",
        "bibtex": "@InProceedings{pmlr-v222-wang24a,\n  title = \t {Robust Blind Watermarking Framework for Hybrid Networks Combining {CNN} and Transformer},\n  author =       {Wang, Baowei and Song, Ziwei and Wu, Yufeng},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1417--1432},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/wang24a/wang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/wang24a.html},\n  abstract = \t {As an essential means of copyright protection, the deep learning-based robust watermarking method is being studied extensively. Its framework consists of three main parts: the encoder, the noise layer and the decoder. But practically all of the schemes are directed at the encoder rather than the decoder. And the whole network is structured by shallow Convolutional Neural Networks (CNNs) for primary feature extraction, while CNNs capture local information and do not model non-local information in watermarked images well. To solve this problem, we consider the use of Transformer networks with a spatially self-attention mechanism. We propose to construct a novel decoder network by combining Transformer and CNNs, which can not only enriches local feature information but also enhances the ability to explore global representations. Meanwhile, to embed secret messages more perfectly, we design a multi-scale attentional feature fusion module to achieve an efficient aggregation of cover image features and secret message features, resulting in the encoded images with rich hybrid features. In addition, perceptual loss is introduced to better evaluate the visual quality of the watermarked images. Extensive experimental results show that our proposed method achieves better results in terms of imperceptibility and robustness compared with existing State-Of-The-Art (SOTA) methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/wang24a/wang24a.pdf",
        "supp": "",
        "pdf_size": 2584623,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:U6ZIgyP2vDIJ:scholar.google.com/&scioq=Robust+Blind+Watermarking+Framework+for+Hybrid+Networks+Combining+CNN+and+Transformer&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Nanjing University of Information Science and Technology, Nanjing, China; Nanjing University of Information Science and Technology, Nanjing, China; Nanjing University of Information Science and Technology, Nanjing, China",
        "aff_domain": "163.com;nuist.edu.cn;nuist.edu.cn",
        "email": "163.com;nuist.edu.cn;nuist.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanjing University of Information Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.nuist.edu.cn",
        "aff_unique_abbr": "NUIST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Robust Image Classification via Using Multiple Diversity Losses",
        "site": "https://proceedings.mlr.press/v222/fang24a.html",
        "author": "Yi Fang; Wen-Hao Zheng; Qihui Wang; Xiao-Xin Li",
        "abstract": "Many research works focus on the robustness of convolutional neural networks (CNNs) on image classification. Diversity loss has been demonstrated to be an effective method to boost robustness. However, the existing diversity losses did not fully consider the strong correlation between regional features when filters are locally activated. They focused on improving filter responses constraint with classification loss. However, diversity loss has deeper optimization space. We explore the combinations of different filter diversity losses and feature diversity losses. We enhance the orthogonality between pair-wise filters to make them more diverse and penalize irrelevance between regional response mappings. We make multiple combinations and propose several methods on improving orthogonality, which have different adaptations for different datasets and network models. We evaluate their effectiveness in experiment. Our combinations could improve the efficiency of robust image recognition.",
        "bibtex": "@InProceedings{pmlr-v222-fang24a,\n  title = \t {Robust Image Classification via Using Multiple Diversity Losses},\n  author =       {Fang, Yi and Zheng, Wen-Hao and Wang, Qihui and Li, Xiao-Xin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {359--373},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/fang24a/fang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/fang24a.html},\n  abstract = \t {Many research works focus on the robustness of convolutional neural networks (CNNs) on image classification. Diversity loss has been demonstrated to be an effective method to boost robustness. However, the existing diversity losses did not fully consider the strong correlation between regional features when filters are locally activated. They focused on improving filter responses constraint with classification loss. However, diversity loss has deeper optimization space. We explore the combinations of different filter diversity losses and feature diversity losses. We enhance the orthogonality between pair-wise filters to make them more diverse and penalize irrelevance between regional response mappings. We make multiple combinations and propose several methods on improving orthogonality, which have different adaptations for different datasets and network models. We evaluate their effectiveness in experiment. Our combinations could improve the efficiency of robust image recognition.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/fang24a/fang24a.pdf",
        "supp": "",
        "pdf_size": 10334211,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_bwT7DPXc0MJ:scholar.google.com/&scioq=Robust+Image+Classification+via+Using+Multiple+Diversity+Losses&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Zhejiang University of Technology; Zhejiang University of Technology; Qianjiang College, Hangzhou Normal University; Zhejiang University of Technology",
        "aff_domain": "zjut.edu.cn;163.com;hznu.edu.cn;zjut.edu.cn",
        "email": "zjut.edu.cn;163.com;hznu.edu.cn;zjut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Zhejiang University of Technology;Hangzhou Normal University",
        "aff_unique_dep": ";Qianjiang College",
        "aff_unique_url": "https://www.zjut.edu.cn;http://www.hgh.edu.cn",
        "aff_unique_abbr": "ZJUT;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hangzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "SANGEA: Scalable and Attributed Network Generation",
        "site": "https://proceedings.mlr.press/v222/lemaire24a.html",
        "author": "Valentin Lemaire; Youssef Achenchabe; Lucas Ody; Houssem Eddine Souid; Gianmarco Aversano; Nicolas Posocco; Sabri Skhiri",
        "abstract": "The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in $\\mathcal{O}(N^2)$, with $N$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these generated graphs achieve high utility on downstream tasks such as link prediction. Finally, we provide a privacy assessment of the generated graphs to show that, even though they have excellent utility, they also achieve reasonable privacy scores.",
        "bibtex": "@InProceedings{pmlr-v222-lemaire24a,\n  title = \t {{SANGEA}: {S}calable and Attributed Network Generation},\n  author =       {Lemaire, Valentin and Achenchabe, Youssef and Ody, Lucas and Souid, Houssem Eddine and Aversano, Gianmarco and Posocco, Nicolas and Skhiri, Sabri},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {678--693},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/lemaire24a/lemaire24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/lemaire24a.html},\n  abstract = \t {The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in $\\mathcal{O}(N^2)$, with $N$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these generated graphs achieve high utility on downstream tasks such as link prediction. Finally, we provide a privacy assessment of the generated graphs to show that, even though they have excellent utility, they also achieve reasonable privacy scores.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/lemaire24a/lemaire24a.pdf",
        "supp": "",
        "pdf_size": 1907147,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=794982576343561857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Euranova, Rue Emile Francqui 4, 1435 Mont-Saint-Guibert, Belgium; Euranova, 146 Rue Paradis, 13006 Marseille, France; Euranova, 6 Les Berges du Lac 3, Tunis 2015, Tunisia; Euranova, Rue Emile Francqui 4, 1435 Mont-Saint-Guibert, Belgium; Euranova, Rue Emile Francqui 4, 1435 Mont-Saint-Guibert, Belgium; Euranova, Rue Emile Francqui 4, 1435 Mont-Saint-Guibert, Belgium; Euranova, Rue Emile Francqui 4, 1435 Mont-Saint-Guibert, Belgium",
        "aff_domain": "euranova.eu;euranova.eu;euranova.eu;euranova.eu;euranova.eu;euranova.eu;euranova.eu",
        "email": "euranova.eu;euranova.eu;euranova.eu;euranova.eu;euranova.eu;euranova.eu;euranova.eu",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Euranova",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;0;0;0",
        "aff_country_unique": "Belgium;France;Tunisia"
    },
    {
        "title": "SART-Res-UNet: Fan Beam CT Image Reconstruction from Limited Projections using attention-enabled residual U-Net",
        "site": "https://proceedings.mlr.press/v222/jinka24a.html",
        "author": "Harika Jinka; Jyothsna Shaji; Sangeeth John; Sreeraj R Menon; Amalu Pradeep; Jayaraj P B; Pournami P N; Niyas Puzhakkal",
        "abstract": "CT scans significantly improve analytical competencies but uses X-Rays which will produce ionizing radiation that bring higher radiation to the living tissues. Thus, optimization of CT radiation dose has a significant concern to lower the health risks. Many manufacturers have done a greater contribution by developing technologies to reduce dosage by maintaining image quality by adding noise reduction filters, automatic exposure control, and using many iterative reconstruction algorithms. Image reconstruction algorithms play a vital role in maintaining or improving image quality in reduced-dose CT. The present research work combines the state-of-the-art reconstruction technique Simultaneous Algebraic Reconstruction Technique (SART) with a Residual U-Net network to generate images from limited number of sinograms. The proposed model is trained using sinograms corresponding head and neck and head CT images of 10 patients. The proposed model predicted superior diagnostic quality images with max PSNR of 70.23 and Structural Similarity Index Measure (SSIM) of 0.99. Thus the proposed model, SART-Res-Unet, ensures a very low radiation exposure to a patient during the repeated CT imaging sequence, which is an inevitable part of radiotherapy.",
        "bibtex": "@InProceedings{pmlr-v222-jinka24a,\n  title = \t {{SART-Res-UNet}: {F}an Beam {CT} Image Reconstruction from Limited Projections using attention-enabled residual {U-Net}},\n  author =       {Jinka, Harika and Shaji, Jyothsna and John, Sangeeth and Menon, Sreeraj R and Pradeep, Amalu and P B, Jayaraj and P N, Pournami and Puzhakkal, Niyas},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {598--613},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/jinka24a/jinka24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/jinka24a.html},\n  abstract = \t {CT scans significantly improve analytical competencies but uses X-Rays which will produce ionizing radiation that bring higher radiation to the living tissues. Thus, optimization of CT radiation dose has a significant concern to lower the health risks. Many manufacturers have done a greater contribution by developing technologies to reduce dosage by maintaining image quality by adding noise reduction filters, automatic exposure control, and using many iterative reconstruction algorithms. Image reconstruction algorithms play a vital role in maintaining or improving image quality in reduced-dose CT. The present research work combines the state-of-the-art reconstruction technique Simultaneous Algebraic Reconstruction Technique (SART) with a Residual U-Net network to generate images from limited number of sinograms. The proposed model is trained using sinograms corresponding head and neck and head CT images of 10 patients. The proposed model predicted superior diagnostic quality images with max PSNR of 70.23 and Structural Similarity Index Measure (SSIM) of 0.99. Thus the proposed model, SART-Res-Unet, ensures a very low radiation exposure to a patient during the repeated CT imaging sequence, which is an inevitable part of radiotherapy.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/jinka24a/jinka24a.pdf",
        "supp": "",
        "pdf_size": 1271595,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DRwGE1DkqnAJ:scholar.google.com/&scioq=SART-Res-UNet:+Fan+Beam+CT+Image+Reconstruction+from+Limited+Projections+using+attention-enabled+residual+U-Net&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; Department of Computer Science and Engineering, National Institute of Technology Calicut, Pin 673 601, Kerala, India; MVR Cancer Center and Research Institute, Calicut, Kerala, India",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;nitc.ac.in;nitc.ac.in;mvrccri.co",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;nitc.ac.in;nitc.ac.in;mvrccri.co",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;1",
        "aff_unique_norm": "National Institute of Technology Calicut;MVR Cancer Center and Research Institute",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.nitc.ac.in;",
        "aff_unique_abbr": "NITC;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Calicut",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Selective Nonparametric Regression via Testing",
        "site": "https://proceedings.mlr.press/v222/noskov24a.html",
        "author": "Fedor Noskov; Alexander Fishkov; Maxim Panov",
        "abstract": "Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.",
        "bibtex": "@InProceedings{pmlr-v222-noskov24a,\n  title = \t {Selective Nonparametric Regression via Testing},\n  author =       {Noskov, Fedor and Fishkov, Alexander and Panov, Maxim},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1023--1038},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/noskov24a/noskov24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/noskov24a.html},\n  abstract = \t {Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/noskov24a/noskov24a.pdf",
        "supp": "",
        "pdf_size": 701273,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mJOLUbklG4AJ:scholar.google.com/&scioq=Selective+Nonparametric+Regression+via+Testing&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "HSE University + Institute for Information Transmission Problems RAS + Moscow Institute of Science and Technology (MIPT), Moscow, Russia; Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russia; Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE",
        "aff_domain": "hse.ru;skoltech.ru;gmail.com",
        "email": "hse.ru;skoltech.ru;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3;4",
        "aff_unique_norm": "Higher School of Economics;Institute for Information Transmission Problems;Moscow Institute of Science and Technology;Skolkovo Institute of Science and Technology;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://hse.ru;http://www.iitp.ru;https://mipt.ru/en;https://www.skoltech.ru;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "HSE;IITP RAS;MIPT;Skoltech;MBZUAI",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Moscow;Abu Dhabi",
        "aff_country_unique_index": "0+0+0;0;1",
        "aff_country_unique": "Russia;United Arab Emirates"
    },
    {
        "title": "Self Weighted Multiplex Modularity Maximization for Multiview Clustering",
        "site": "https://proceedings.mlr.press/v222/henka24a.html",
        "author": "Noureddine Henka; Mohamad Assaad; Sami Tazi",
        "abstract": "In response to the challenge of representing data from multiple sources, researchers have proposed the use of multiplex graphs as a solution. Multiplex graphs are particularly useful for representing multi-view data, where each layer represents a specific type of interaction. Pillar community detection of multiplex graphs is a clustering application that computes groups of vertices across all layers. Modularity maximization is a popular technique for graph clustering, which has been generalized to multiplex graphs. However, this generalization did not consider the importance of each layer in pillar clustering. This paper presents a new technique called Self Weighted Multiplex Modularity (SWMM), which optimizes the weights associated with each layer and the partition that maximizes the multiplex modularity. The paper proposes two optimization methods, iterative and direct, and demonstrates the effectiveness and robustness of the technique in accurately retrieving clusters even when data is highly missing.",
        "bibtex": "@InProceedings{pmlr-v222-henka24a,\n  title = \t {Self Weighted Multiplex Modularity Maximization for Multiview Clustering},\n  author =       {Henka, Noureddine and Assaad, Mohamad and Tazi, Sami},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {406--421},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/henka24a/henka24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/henka24a.html},\n  abstract = \t {In response to the challenge of representing data from multiple sources, researchers have proposed the use of multiplex graphs as a solution. Multiplex graphs are particularly useful for representing multi-view data, where each layer represents a specific type of interaction. Pillar community detection of multiplex graphs is a clustering application that computes groups of vertices across all layers. Modularity maximization is a popular technique for graph clustering, which has been generalized to multiplex graphs. However, this generalization did not consider the importance of each layer in pillar clustering. This paper presents a new technique called Self Weighted Multiplex Modularity (SWMM), which optimizes the weights associated with each layer and the partition that maximizes the multiplex modularity. The paper proposes two optimization methods, iterative and direct, and demonstrates the effectiveness and robustness of the technique in accurately retrieving clusters even when data is highly missing.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/henka24a/henka24a.pdf",
        "supp": "",
        "pdf_size": 2127729,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:PX2ouSd6g0gJ:scholar.google.com/&scioq=Self+Weighted+Multiplex+Modularity+Maximization+for+Multiview+Clustering&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "L2S and R &D RTE; L2S, Gif-sur-Yvette, France; R&DRTE, La defense, France",
        "aff_domain": "rte-france.com;centralesupelec.fr;rte-france.com",
        "email": "rte-france.com;centralesupelec.fr;rte-france.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "L2S;Laboratoire des Signaux et Syst\u00e8mes;R&DRTE",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.l2s.ens-cachan.fr;",
        "aff_unique_abbr": ";L2S;",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Gif-sur-Yvette;La defense",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Self-supervised Example Difficulty Balancing for Local Descriptor Learning",
        "site": "https://proceedings.mlr.press/v222/zhang24c.html",
        "author": "Jiahan Zhang; Dayong Tian; Tianyang Wu; Yiqing Cao; Yaoqi Du; Yiwen Wei",
        "abstract": "In scenarios where there is an imbalance between positive and negative examples, hard example mining strategies have been shown to improve recognition performance by assisting models in distinguishing subtle differences between positive and negative examples. However, overly strict mining strategies may introduce false negative examples, while implementing the mining strategy can disrupt the difficulty distribution of examples in the real dataset and cause overfitting on difficult examples in the model. Therefore, in this paper, we explore how to balance the difficulty of mined examples in order to obtain and exploit high-quality negative examples, and try to solve the problem in terms of both loss function and training strategy. The proposed balance loss provides an effective discriminant for the quality of negative examples by incorporating a self-supervised approach into the loss function, employing dynamic gradient modulation to achieve finer adjustment for examples of different difficulties. The proposed annealing training strategy constrains the difficulty of negative examples drawn from mining and uses examples of decreasing difficulty to mitigate the overfitting issue of hard negative examples in training. Extensive experiments demonstrate that our new sparse descriptors outperform previously established state-of-the-art sparse descriptors.",
        "bibtex": "@InProceedings{pmlr-v222-zhang24c,\n  title = \t {Self-supervised Example Difficulty Balancing for Local Descriptor Learning},\n  author =       {Zhang, Jiahan and Tian, Dayong and Wu, Tianyang and Cao, Yiqing and Du, Yaoqi and Wei, Yiwen},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1654--1669},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhang24c/zhang24c.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhang24c.html},\n  abstract = \t {In scenarios where there is an imbalance between positive and negative examples, hard example mining strategies have been shown to improve recognition performance by assisting models in distinguishing subtle differences between positive and negative examples. However, overly strict mining strategies may introduce false negative examples, while implementing the mining strategy can disrupt the difficulty distribution of examples in the real dataset and cause overfitting on difficult examples in the model. Therefore, in this paper, we explore how to balance the difficulty of mined examples in order to obtain and exploit high-quality negative examples, and try to solve the problem in terms of both loss function and training strategy. The proposed balance loss provides an effective discriminant for the quality of negative examples by incorporating a self-supervised approach into the loss function, employing dynamic gradient modulation to achieve finer adjustment for examples of different difficulties. The proposed annealing training strategy constrains the difficulty of negative examples drawn from mining and uses examples of decreasing difficulty to mitigate the overfitting issue of hard negative examples in training. Extensive experiments demonstrate that our new sparse descriptors outperform previously established state-of-the-art sparse descriptors.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhang24c/zhang24c.pdf",
        "supp": "",
        "pdf_size": 1319195,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ycNPHleLzBgJ:scholar.google.com/&scioq=Self-supervised+Example+Difficulty+Balancing+for+Local+Descriptor+Learning&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Queen Mary University of London, UK; School of Electronics and Information, Northwestern Polytechnical University, China; School of Electronics and Information, Northwestern Polytechnical University, China; School of Electronics and Information, Northwestern Polytechnical University, China; School of Electronics and Information, Northwestern Polytechnical University, China; Xidian University, China",
        "aff_domain": "se19.qmul.ac.uk;nwpu.edu.cn;mail.nwpu.edu.cn;mail.nwpu.edu.cn;qq.com;xidian.edu.cn",
        "email": "se19.qmul.ac.uk;nwpu.edu.cn;mail.nwpu.edu.cn;mail.nwpu.edu.cn;qq.com;xidian.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;2",
        "aff_unique_norm": "Queen Mary University of London;Northwestern Polytechnical University;Xidian University",
        "aff_unique_dep": ";School of Electronics and Information;",
        "aff_unique_url": "https://www.qmul.ac.uk;http://www.nwpu.edu.cn;http://www.xidian.edu.cn/",
        "aff_unique_abbr": "QMUL;NWPU;Xidian",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "title": "Show Me How It\u2019s Done: The Role of Explanations in Fine-Tuning Language Models",
        "site": "https://proceedings.mlr.press/v222/ballout24a.html",
        "author": "Mohamad Ballout; Ulf Krumnack; Gunther Heidemann; Kai-Uwe K\u00fchnberger",
        "abstract": "Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model\u2019s parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.",
        "bibtex": "@InProceedings{pmlr-v222-ballout24a,\n  title = \t {{Show Me How It\u2019s Done}: {T}he Role of Explanations in Fine-Tuning Language Models},\n  author =       {Ballout, Mohamad and Krumnack, Ulf and Heidemann, Gunther and K\\\"{u}hnberger, Kai-Uwe},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {90--105},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ballout24a/ballout24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ballout24a.html},\n  abstract = \t {Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model\u2019s parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ballout24a/ballout24a.pdf",
        "supp": "",
        "pdf_size": 2082329,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4269212261736731608&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Cognitive Science, University of Osnabr\u00a8 uck, Osnabr\u00a8 uck, Germany; Institute of Cognitive Science, University of Osnabr\u00a8 uck, Osnabr\u00a8 uck, Germany; Institute of Cognitive Science, University of Osnabr\u00a8 uck, Osnabr\u00a8 uck, Germany; Institute of Cognitive Science, University of Osnabr\u00a8 uck, Osnabr\u00a8 uck, Germany",
        "aff_domain": "uos.de;uos.de;uos.de;uos.de",
        "email": "uos.de;uos.de;uos.de;uos.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Osnabr\u00fcck",
        "aff_unique_dep": "Institute of Cognitive Science",
        "aff_unique_url": "https://www.uni-osnabrueck.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Osnabr\u00fcck",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Simple and Efficient Vision Backbone Adapter for Image Semantic Segmentation",
        "site": "https://proceedings.mlr.press/v222/peng24a.html",
        "author": "Dingjie Peng; Wataru Kameyama",
        "abstract": "Utilizing a pretrained vision backbone to finetune a model for semantic segmentation is common practice in computer vision. However, there are few works intending to enlarge the semantic context learning capacity by incorporating a segmentation adapter into the backbone. Thus, in this paper, we present a simple but efficient segmentation adapter, termed as SegAdapter, which can be plugged into the pretrained vision backbone to improve the performance of existing models for image semantic segmentation. We summarize SegAdapter with three attractive advantages: 1) SegAdapter is a plug-and-play module demonstrating strong adaptability in CNN and Transformer based models such as ConvNext and Segformer, 2) SegAdapter applies a light-weight High-order Spatial Attention (HSA) to make use of intermediate features from the pretrained backbone which extends the model depth and produces auxiliary segmentation maps for model enhancement, 3) SegAdapter builds a powerful vision backbone by incorporating the semantic context into each stage which takes on some of the functions of the segmentation head. So, SegAdapter augmented model can be used in simple designed decode head to avoid heavy computational cost. By plugging multiple SegAdapter layers into different vision backbones, we construct a series of SegAdapter-based segmentation models. We show through the extensive experiments that SegAdapter can be used with mainstream backbones like CNN and Transformer to improve mIoU performance in a large margin while introducing minimal additional parameters and FLOPs.",
        "bibtex": "@InProceedings{pmlr-v222-peng24a,\n  title = \t {Simple and Efficient Vision Backbone Adapter for Image Semantic Segmentation},\n  author =       {Peng, Dingjie and Kameyama, Wataru},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1071--1086},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/peng24a/peng24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/peng24a.html},\n  abstract = \t {Utilizing a pretrained vision backbone to finetune a model for semantic segmentation is common practice in computer vision. However, there are few works intending to enlarge the semantic context learning capacity by incorporating a segmentation adapter into the backbone. Thus, in this paper, we present a simple but efficient segmentation adapter, termed as SegAdapter, which can be plugged into the pretrained vision backbone to improve the performance of existing models for image semantic segmentation. We summarize SegAdapter with three attractive advantages: 1) SegAdapter is a plug-and-play module demonstrating strong adaptability in CNN and Transformer based models such as ConvNext and Segformer, 2) SegAdapter applies a light-weight High-order Spatial Attention (HSA) to make use of intermediate features from the pretrained backbone which extends the model depth and produces auxiliary segmentation maps for model enhancement, 3) SegAdapter builds a powerful vision backbone by incorporating the semantic context into each stage which takes on some of the functions of the segmentation head. So, SegAdapter augmented model can be used in simple designed decode head to avoid heavy computational cost. By plugging multiple SegAdapter layers into different vision backbones, we construct a series of SegAdapter-based segmentation models. We show through the extensive experiments that SegAdapter can be used with mainstream backbones like CNN and Transformer to improve mIoU performance in a large margin while introducing minimal additional parameters and FLOPs.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/peng24a/peng24a.pdf",
        "supp": "",
        "pdf_size": 940139,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18424514737632255418&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Graduate School of Fund. Sci. and Eng., Waseda University, Tokyo, Japan; Faculty of Sci. and Eng., Waseda University, Tokyo, Japan",
        "aff_domain": "asagi.waseda.jp;waseda.jp",
        "email": "asagi.waseda.jp;waseda.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Waseda University",
        "aff_unique_dep": "Graduate School of Fundamental Science and Engineering",
        "aff_unique_url": "https://www.waseda.jp/top",
        "aff_unique_abbr": "Waseda",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Single Image Super-resolution Based On Non-subsampled Shearlet Transform",
        "site": "https://proceedings.mlr.press/v222/tan24a.html",
        "author": "Ming Tan; Liang Chen; Xuan Wu; Yi Wu",
        "abstract": "With the development of deep learning, breakthroughs in single image super-resolution have been achieved. However, most existing methods are limited to using only spatial domain information or only frequency domain information, and the rich information of the image in the frequency domain space is not fully utilized, so it is still difficult to recover satisfactory texture details. In this paper, we propose a method to fuse the frequency domain and spatial domain information. Our method uses a two-branch network to extract the spatial domain information and the frequency domain information separately and uses a fusion module to fuse the different information in the two domains. We also use the Non-Subsampled Shearlet Transform (NSST) to preserve the texture directionality well, and design two NSST-based directional texture enhancement modules, which are embedded in different parts of the network, to enhance the recovery of texture details in the image reconstruction process. Quantitative and qualitative experimental results show that the method outperforms advanced single-image super-resolution methods in recovering images.",
        "bibtex": "@InProceedings{pmlr-v222-tan24a,\n  title = \t {Single Image Super-resolution Based On Non-subsampled Shearlet Transform},\n  author =       {Tan, Ming and Chen, Liang and Wu, Xuan and Wu, Yi},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1337--1352},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/tan24a/tan24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/tan24a.html},\n  abstract = \t {With the development of deep learning, breakthroughs in single image super-resolution have been achieved. However, most existing methods are limited to using only spatial domain information or only frequency domain information, and the rich information of the image in the frequency domain space is not fully utilized, so it is still difficult to recover satisfactory texture details. In this paper, we propose a method to fuse the frequency domain and spatial domain information. Our method uses a two-branch network to extract the spatial domain information and the frequency domain information separately and uses a fusion module to fuse the different information in the two domains. We also use the Non-Subsampled Shearlet Transform (NSST) to preserve the texture directionality well, and design two NSST-based directional texture enhancement modules, which are embedded in different parts of the network, to enhance the recovery of texture details in the image reconstruction process. Quantitative and qualitative experimental results show that the method outperforms advanced single-image super-resolution methods in recovering images.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/tan24a/tan24a.pdf",
        "supp": "",
        "pdf_size": 9300133,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:D3joY71kcMQJ:scholar.google.com/&scioq=Single+Image+Super-resolution+Based+On+Non-subsampled+Shearlet+Transform&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Fujian Provincial Key Laboratory of Photonics Technology, Fujian Normal University, Fuzhou, China; Fujian Provincial Key Laboratory of Photonics Technology, Fujian Normal University, Fuzhou, China; Fujian Provincial Key Laboratory of Photonics Technology, Fujian Normal University, Fuzhou, China; Fujian Provincial Key Laboratory of Photonics Technology, Fujian Normal University, Fuzhou, China",
        "aff_domain": "qq.com;126.com;gmail.com;fjnu.edu.cn",
        "email": "qq.com;126.com;gmail.com;fjnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Fujian Normal University",
        "aff_unique_dep": "Provincial Key Laboratory of Photonics Technology",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Fuzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "State Value Generation with Prompt Learning and Self-Training for Low-Resource Dialogue State Tracking",
        "site": "https://proceedings.mlr.press/v222/gu24a.html",
        "author": "Ming Gu; Yan Yang; Chengcai Chen; Zhou Yu",
        "abstract": "Recently, low-resource dialogue state tracking (DST) has received increasing attention. First obtaining state values then based on values to generate slot types has made great progress in this task. However, obtaining state values is still an under-studied problem. Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either. To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation. Specifically, we propose to generate state values and use self-training to further improve state value generation. Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training. Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the data ratio settings of 5%, 10%, and 25% when limited to models under 100 billion parameters. Compared to models with more than 100 billion parameters, SVAG still reaches competitive results.",
        "bibtex": "@InProceedings{pmlr-v222-gu24a,\n  title = \t {State Value Generation with Prompt Learning and Self-Training for Low-Resource Dialogue State Tracking},\n  author =       {Gu, Ming and Yang, Yan and Chen, Chengcai and Yu, Zhou},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {390--405},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/gu24a/gu24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/gu24a.html},\n  abstract = \t {Recently, low-resource dialogue state tracking (DST) has received increasing attention. First obtaining state values then based on values to generate slot types has made great progress in this task. However, obtaining state values is still an under-studied problem. Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either. To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation. Specifically, we propose to generate state values and use self-training to further improve state value generation. Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training. Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the data ratio settings of 5%, 10%, and 25% when limited to models under 100 billion parameters. Compared to models with more than 100 billion parameters, SVAG still reaches competitive results.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/gu24a/gu24a.pdf",
        "supp": "",
        "pdf_size": 723982,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16058774470504895800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University; Xiaoi Research, Xiaoi Robot Technology Co., Ltd; Dialogue NLP Lab, Columbia University",
        "aff_domain": "stu.ecnu.edu.cn;cs.ecnu.edu.cn;xiaoi.com;columbia.edu",
        "email": "stu.ecnu.edu.cn;cs.ecnu.edu.cn;xiaoi.com;columbia.edu",
        "github": "https://github.com/SLEEPWALKERG/SVAG",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "East China Normal University;Xiaoi Robot Technology Co., Ltd;Columbia University",
        "aff_unique_dep": "School of Computer Science and Technology;Xiaoi Research;Dialogue NLP Lab",
        "aff_unique_url": "http://www.ecnu.edu.cn;http://www.xiaoi.com/;https://www.columbia.edu",
        "aff_unique_abbr": "ECNU;Xiaoi;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "TFAN: Temporal-Feature correlations Attention-based Network for Urban Air Quality Prediction using Data Fusion technology",
        "site": "https://proceedings.mlr.press/v222/ma24a.html",
        "author": "Siyuan Ma; Fan Zhang; Wanli Hou; Yarui Li; Wei Song",
        "abstract": "Air pollution raises a detrimental impact on human health and natural environment. Accurate prediction of air quality is crucial for effective pollution control and mitigation strategies. Numerous existing methods for analyzing the variation tendency of a specific air component primarily focus on its temporal and spatial information, neglecting the potential interactions between different attributes within the same time interval. In this paper, we propose a Temporal-Feature correlations Attention-based deep learning Network (TFAN), which incorporates data fusion technology. TFAN focuses on capturing temporal dependencies, feature correlations, and the potential relationship between temporal-feature through the Attention mechanism, and the data fusion method allows for a comprehensive consideration of multiple factors on prediction. Experimental results conducted using real-world data from Beijing City demonstrate that TFAN outperforms various baseline models in prediction accuracy for multiple pollutants by 10+%.",
        "bibtex": "@InProceedings{pmlr-v222-ma24a,\n  title = \t {{TFAN}: {T}emporal-Feature correlations Attention-based Network for Urban Air Quality Prediction using Data Fusion technology},\n  author =       {Ma, Siyuan and Zhang, Fan and Hou, Wanli and Li, Yarui and Song, Wei},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {850--865},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ma24a/ma24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ma24a.html},\n  abstract = \t {Air pollution raises a detrimental impact on human health and natural environment. Accurate prediction of air quality is crucial for effective pollution control and mitigation strategies. Numerous existing methods for analyzing the variation tendency of a specific air component primarily focus on its temporal and spatial information, neglecting the potential interactions between different attributes within the same time interval. In this paper, we propose a Temporal-Feature correlations Attention-based deep learning Network (TFAN), which incorporates data fusion technology. TFAN focuses on capturing temporal dependencies, feature correlations, and the potential relationship between temporal-feature through the Attention mechanism, and the data fusion method allows for a comprehensive consideration of multiple factors on prediction. Experimental results conducted using real-world data from Beijing City demonstrate that TFAN outperforms various baseline models in prediction accuracy for multiple pollutants by 10+%.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ma24a/ma24a.pdf",
        "supp": "",
        "pdf_size": 11530923,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-Hxjhmkts7sJ:scholar.google.com/&scioq=TFAN:+Temporal-Feature+correlations+Attention-based+Network+for+Urban+Air+Quality+Prediction+using+Data+Fusion+technology&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Henan Academy of Big Data, Zhengzhou University, Zhengzhou, China+School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China; Henan Academy of Big Data, Zhengzhou University, Zhengzhou, China+School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer, Zhongyuan University of Technology, Zhengzhou, China; Henan Academy of Big Data, Zhengzhou University, Zhengzhou, China",
        "aff_domain": "qq.com;ncwu.edu.cn;163.com;zut.edu.cn;zzu.edu.cn",
        "email": "qq.com;ncwu.edu.cn;163.com;zut.edu.cn;zzu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;1;0+0;2;0",
        "aff_unique_norm": "Zhengzhou University;North China University of Water Resources and Electric Power;Zhongyuan University of Technology",
        "aff_unique_dep": "Henan Academy of Big Data;;School of Computer",
        "aff_unique_url": "http://www.zzu.edu.cn;;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "0+0;0;0+0;0;0",
        "aff_campus_unique": "Zhengzhou",
        "aff_country_unique_index": "0+0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Ted: Tree delineation with reduced dimensions using entropy and deep learning",
        "site": "https://proceedings.mlr.press/v222/anjani24a.html",
        "author": "RN Anjani; CH Sarvani; K Kalyan Deep; P Aravinda Kumar; Sitiraju Srinivasa Rao",
        "abstract": "Trees play a very important role in maintaining the ecosystem. To automate the process of extracting trees from Remote Sensing data and counting thereby, we have used Cartosat-2S merged products @ 0.6m as input. Vegetation indices such as NDVI, and EVI can retrieve the Vegetation class which contains trees and their look-alikes (such as grass, fields, and shrubs) having the same spectral signatures \\em{i.e.}, high reflectance in NIR band, and high absorption in Red band. Extraction of only Trees is not possible with these indices. In this paper we present a novel method for tree delineation from its look-alikes using AIML, concentrating more on preparation of input datasets efficiently. Based on the Spectral Separability analysis, only Red and NIR bands from the satellite imagery are utilized in the proposed method to separate Vegetation from the background classes (such as water, bare soil, terrain, and built-up). In addition to the two bands from satellite imagery entropy layer is computed from the NIR band and utilized as the third band to delineate trees from their look-alikes. Deep Neural Networks have the capability of learning complex patterns that can separate trees and their look-alikes. However, the performance is boosted when the entropy layer is added to the input image. The proposed method showed better performance when utilizing 3 bands Red, NIR, and Entropy bands compared to 4 bands i.e. Red, Green, Blue, and NIR bands. The proposed method obtains a precision of 96%, a recall of 90%, and an F1-score of 93% even with a relatively smaller training dataset. The study is performed on the data collected from various locations of the Indian state Rajasthan.",
        "bibtex": "@InProceedings{pmlr-v222-anjani24a,\n  title = \t {{Ted}: {T}ree delineation with reduced dimensions using entropy and deep learning},\n  author =       {Anjani, RN and Sarvani, CH and Kalyan Deep, K and Aravinda Kumar, P and Srinivasa Rao, Sitiraju},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {44--57},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/anjani24a/anjani24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/anjani24a.html},\n  abstract = \t {Trees play a very important role in maintaining the ecosystem. To automate the process of extracting trees from Remote Sensing data and counting thereby, we have used Cartosat-2S merged products @ 0.6m as input. Vegetation indices such as NDVI, and EVI can retrieve the Vegetation class which contains trees and their look-alikes (such as grass, fields, and shrubs) having the same spectral signatures \\em{i.e.}, high reflectance in NIR band, and high absorption in Red band. Extraction of only Trees is not possible with these indices. In this paper we present a novel method for tree delineation from its look-alikes using AIML, concentrating more on preparation of input datasets efficiently. Based on the Spectral Separability analysis, only Red and NIR bands from the satellite imagery are utilized in the proposed method to separate Vegetation from the background classes (such as water, bare soil, terrain, and built-up). In addition to the two bands from satellite imagery entropy layer is computed from the NIR band and utilized as the third band to delineate trees from their look-alikes. Deep Neural Networks have the capability of learning complex patterns that can separate trees and their look-alikes. However, the performance is boosted when the entropy layer is added to the input image. The proposed method showed better performance when utilizing 3 bands Red, NIR, and Entropy bands compared to 4 bands i.e. Red, Green, Blue, and NIR bands. The proposed method obtains a precision of 96%, a recall of 90%, and an F1-score of 93% even with a relatively smaller training dataset. The study is performed on the data collected from various locations of the Indian state Rajasthan.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/anjani24a/anjani24a.pdf",
        "supp": "",
        "pdf_size": 18107383,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NTulAgDU_q4J:scholar.google.com/&scioq=Ted:+Tree+delineation+with+reduced+dimensions+using+entropy+and+deep+learning&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "National Remote Sensing Center, India; National Remote Sensing Center, India; National Remote Sensing Center, India; National Remote Sensing Center, India; National Remote Sensing Center, India",
        "aff_domain": "nrsc.gov.in;nrsc.gov.in;nrsc.gov.in;nrsc.gov.in;nrsc.gov.in",
        "email": "nrsc.gov.in;nrsc.gov.in;nrsc.gov.in;nrsc.gov.in;nrsc.gov.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National Remote Sensing Center",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Temporal RPN Learning for Weakly-Supervised Temporal Action Localization",
        "site": "https://proceedings.mlr.press/v222/huang24a.html",
        "author": "Jing Huang; Ming Kong; Luyuan Chen; Tian Liang; Qiang Zhu",
        "abstract": "Weakly-Supervised Temporal Action Localization (WSTAL) aims to train an action instance localization model from untrimmed videos with only video-level labels, similar to the Object Detection (OD) task. Existing Top-k MIL-based WSTAL methods cannot flexibly define the learning space, which limits the model\u2019s learning efficiency and performance. Faster R-CNN is a classic two-stage object detection architecture with an efficient Region Proposal Network. This paper successfully migrates the Faster R-CNN liked two-stage architecture to the WSTAL task: first to build a T-RPN and integrate it with the traditional WSTAL framework; and then to propose a pseudo label generation mechanism to enable the T-RPN learning without temporal annotations. Our new framework has achieved breakthrough performances on THUMOS-14 and ActivityNet-v1.2 datasets, and comprehensive ablation experiments have verified the effectiveness of the innovations. Code will be available at: \\href{https://github.com/ZJUHJ/TRPN}{https://github.com/ZJUHJ/TRPN}.",
        "bibtex": "@InProceedings{pmlr-v222-huang24a,\n  title = \t {Temporal RPN Learning for Weakly-Supervised Temporal Action Localization},\n  author =       {Huang, Jing and Kong, Ming and Chen, Luyuan and Liang, Tian and Zhu, Qiang},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {470--485},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/huang24a/huang24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/huang24a.html},\n  abstract = \t {Weakly-Supervised Temporal Action Localization (WSTAL) aims to train an action instance localization model from untrimmed videos with only video-level labels, similar to the Object Detection (OD) task. Existing Top-k MIL-based WSTAL methods cannot flexibly define the learning space, which limits the model\u2019s learning efficiency and performance. Faster R-CNN is a classic two-stage object detection architecture with an efficient Region Proposal Network. This paper successfully migrates the Faster R-CNN liked two-stage architecture to the WSTAL task: first to build a T-RPN and integrate it with the traditional WSTAL framework; and then to propose a pseudo label generation mechanism to enable the T-RPN learning without temporal annotations. Our new framework has achieved breakthrough performances on THUMOS-14 and ActivityNet-v1.2 datasets, and comprehensive ablation experiments have verified the effectiveness of the innovations. Code will be available at: \\href{https://github.com/ZJUHJ/TRPN}{https://github.com/ZJUHJ/TRPN}.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/huang24a/huang24a.pdf",
        "supp": "",
        "pdf_size": 1180532,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0taE1D6fBzIJ:scholar.google.com/&scioq=Temporal+RPN+Learning+for+Weakly-Supervised+Temporal+Action+Localization&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Zhejiang University, Hangzhou 310058, China; College of Computer Science and Technology, Zhejiang University, Hangzhou 310058, China+Hikvision Research Institute, Hangzhou 310051, China; Beijing Information Science and Technology University, Beijing 100101, China; Zhejiang University, Hangzhou 310058, China; College of Computer Science, Zhejiang University, Hangzhou 310058, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;bistu.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;bistu.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/ZJUHJ/TRPN",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;2;0;0",
        "aff_unique_norm": "Zhejiang University;Hikvision Research Institute;Beijing Information Science and Technology University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.hikvision.com/cn/;http://www.bistu.edu.cn/",
        "aff_unique_abbr": "ZJU;HRI;BISTU",
        "aff_campus_unique_index": "0;0+0;1;0;0",
        "aff_campus_unique": "Hangzhou;Beijing",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Temporal Shift - Multi-Objective Loss Function for Improved Anomaly Fall Detection",
        "site": "https://proceedings.mlr.press/v222/denkovski24a.html",
        "author": "Stefan Denkovski; Shehroz S Khan; Alex Mihailidis",
        "abstract": "Falls are a major cause of injuries and deaths among older adults worldwide. Accurate fall detection can help reduce potential injuries and additional health complications. Different types of video modalities can be used in a home setting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly detection frameworks using autoencoders and their variants can be used for fall detection due to the data imbalance that arises from the rarity and diversity of falls. However, the use of reconstruction error in autoencoders can limit the application of networks\u2019 structures that propagate information. In this paper, we propose a new multi-objective loss function called Temporal Shift, which aims to predict both future and reconstructed frames within a window of sequential frames. The proposed loss function is evaluated on a semi-naturalistic fall detection dataset containing multiple camera modalities. The autoencoders were trained on normal activities of daily living (ADL) performed by older adults and tested on ADLs and falls performed by young adults. Temporal shift shows significant improvement to a baseline 3D Convolutional autoencoder, an attention U-Net CAE, and a multi-modal neural network. The greatest improvement was observed in an attention U-Net model improving by $0.20$ AUC ROC for a single camera when compared to reconstruction alone. With significant improvement across different models, this approach has the potential to be widely adopted and improve anomaly detection capabilities in other settings besides fall detection.",
        "bibtex": "@InProceedings{pmlr-v222-denkovski24a,\n  title = \t {Temporal Shift - Multi-Objective Loss Function for Improved Anomaly Fall Detection},\n  author =       {Denkovski, Stefan and Khan, Shehroz S and Mihailidis, Alex},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {295--310},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/denkovski24a/denkovski24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/denkovski24a.html},\n  abstract = \t {Falls are a major cause of injuries and deaths among older adults worldwide. Accurate fall detection can help reduce potential injuries and additional health complications. Different types of video modalities can be used in a home setting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly detection frameworks using autoencoders and their variants can be used for fall detection due to the data imbalance that arises from the rarity and diversity of falls. However, the use of reconstruction error in autoencoders can limit the application of networks\u2019 structures that propagate information. In this paper, we propose a new multi-objective loss function called Temporal Shift, which aims to predict both future and reconstructed frames within a window of sequential frames. The proposed loss function is evaluated on a semi-naturalistic fall detection dataset containing multiple camera modalities. The autoencoders were trained on normal activities of daily living (ADL) performed by older adults and tested on ADLs and falls performed by young adults. Temporal shift shows significant improvement to a baseline 3D Convolutional autoencoder, an attention U-Net CAE, and a multi-modal neural network. The greatest improvement was observed in an attention U-Net model improving by $0.20$ AUC ROC for a single camera when compared to reconstruction alone. With significant improvement across different models, this approach has the potential to be widely adopted and improve anomaly detection capabilities in other settings besides fall detection.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/denkovski24a/denkovski24a.pdf",
        "supp": "",
        "pdf_size": 5611204,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8246657446018497857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "KITE Research Institute \u2013 University Health Network, Toronto, Canada; KITE Research Institute \u2013 University Health Network, Toronto, Canada; Institute of Biomedical Engineering, University of Toronto, Toronto, Canada",
        "aff_domain": "mail.utoronto.ca;uhn.ca;utoronto.ca",
        "email": "mail.utoronto.ca;uhn.ca;utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University Health Network;University of Toronto",
        "aff_unique_dep": "KITE Research Institute;Institute of Biomedical Engineering",
        "aff_unique_url": "https://www.uhn.ca;https://www.utoronto.ca",
        "aff_unique_abbr": "UHN;U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "The Fine Print on Tempered Posteriors",
        "site": "https://proceedings.mlr.press/v222/pitas24a.html",
        "author": "Konstantinos Pitas; Julyan Arbel",
        "abstract": "We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.",
        "bibtex": "@InProceedings{pmlr-v222-pitas24a,\n  title = \t {The Fine Print on Tempered Posteriors},\n  author =       {Pitas, Konstantinos and Arbel, Julyan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1087--1102},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/pitas24a/pitas24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/pitas24a.html},\n  abstract = \t {We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/pitas24a/pitas24a.pdf",
        "supp": "",
        "pdf_size": 601355,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6381336011827710712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France; Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universite Grenoble Alpes",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.univ-grenoble-alpes.fr",
        "aff_unique_abbr": "UGA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Grenoble",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "The Importance of Anti-Aliasing in Tiny Object Detection",
        "site": "https://proceedings.mlr.press/v222/ning24a.html",
        "author": "Jinlai Ning; Michael Spratling",
        "abstract": "Tiny object detection has gained considerable attention in the research community owing to the frequent occurrence of tiny objects in numerous critical real-world scenarios. However, convolutional neural networks (CNNs) used as the backbone for object detection architectures typically neglect Nyquist\u2019s sampling theorem during down-sampling operations, resulting in aliasing and degraded performance. This is likely to be a particular issue for tiny objects that occupy very few pixels and therefore have high spatial frequency features. This paper applied an existing approach WaveCNet for anti-aliasing to tiny object detection. WaveCNet addresses aliasing by replacing standard down-sampling processes in CNNs with Wavelet Pooling (WaveletPool) layers, effectively suppressing aliasing. We modify the original WaveCNet to apply WaveletPool in a consistent way in both pathways of the residual blocks in ResNets. Additionally, we also propose a bottom-heavy version of the backbone, which further improves the performance of tiny object detection while also reducing the required number of parameters by almost half. Experimental results on the TinyPerson, WiderFace, and DOTA datasets demonstrate the importance of anti-aliasing in tiny object detection and the effectiveness of the proposed method which achieves new state-of-the-art results on all three datasets. Codes and experiment results are released at \\url{https://github.com/freshn/Anti-aliasing-Tiny-Object-Detection.git}.",
        "bibtex": "@InProceedings{pmlr-v222-ning24a,\n  title = \t {The Importance of Anti-Aliasing in Tiny Object Detection},\n  author =       {Ning, Jinlai and Spratling, Michael},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {975--990},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ning24a/ning24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ning24a.html},\n  abstract = \t {Tiny object detection has gained considerable attention in the research community owing to the frequent occurrence of tiny objects in numerous critical real-world scenarios. However, convolutional neural networks (CNNs) used as the backbone for object detection architectures typically neglect Nyquist\u2019s sampling theorem during down-sampling operations, resulting in aliasing and degraded performance. This is likely to be a particular issue for tiny objects that occupy very few pixels and therefore have high spatial frequency features. This paper applied an existing approach WaveCNet for anti-aliasing to tiny object detection. WaveCNet addresses aliasing by replacing standard down-sampling processes in CNNs with Wavelet Pooling (WaveletPool) layers, effectively suppressing aliasing. We modify the original WaveCNet to apply WaveletPool in a consistent way in both pathways of the residual blocks in ResNets. Additionally, we also propose a bottom-heavy version of the backbone, which further improves the performance of tiny object detection while also reducing the required number of parameters by almost half. Experimental results on the TinyPerson, WiderFace, and DOTA datasets demonstrate the importance of anti-aliasing in tiny object detection and the effectiveness of the proposed method which achieves new state-of-the-art results on all three datasets. Codes and experiment results are released at \\url{https://github.com/freshn/Anti-aliasing-Tiny-Object-Detection.git}.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ning24a/ning24a.pdf",
        "supp": "",
        "pdf_size": 438006,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3154603305522707480&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "King\u2019s College London, Department of Informatics, London. UK.; King\u2019s College London, Department of Informatics, London. UK.",
        "aff_domain": "kcl.ac.uk;kcl.ac.uk",
        "email": "kcl.ac.uk;kcl.ac.uk",
        "github": "https://github.com/freshn/Anti-aliasing-Tiny-Object-Detection.git",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "King's College London",
        "aff_unique_dep": "Department of Informatics",
        "aff_unique_url": "https://www.kcl.ac.uk",
        "aff_unique_abbr": "KCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Thompson Exploration with Best Challenger Rule in Best Arm Identification",
        "site": "https://proceedings.mlr.press/v222/lee24a.html",
        "author": "Jongyeong Lee; Junya Honda; Masashi Sugiyama",
        "abstract": "This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K \\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to asymptotically optimal policies in terms of sample complexity while requiring less computation cost. In addition, we highlight the advantages of our policy by comparing it to the concept of $\\beta$-optimality, a relaxed notion of asymptotic optimality commonly considered in the analysis of a class of policies including the proposed one.",
        "bibtex": "@InProceedings{pmlr-v222-lee24a,\n  title = \t {Thompson Exploration with Best Challenger Rule in Best Arm Identification},\n  author =       {Lee, Jongyeong and Honda, Junya and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {646--661},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/lee24a/lee24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/lee24a.html},\n  abstract = \t {This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K \\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to asymptotically optimal policies in terms of sample complexity while requiring less computation cost. In addition, we highlight the advantages of our policy by comparing it to the concept of $\\beta$-optimality, a relaxed notion of asymptotic optimality commonly considered in the analysis of a class of policies including the proposed one.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/lee24a/lee24a.pdf",
        "supp": "",
        "pdf_size": 518200,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7974445392570268819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Tokyo + RIKEN AIP; Kyoto University + RIKEN AIP; RIKEN AIP + The University of Tokyo",
        "aff_domain": "MS.K.U-TOKYO.AC.JP;I.KYOTO-U.AC.JP;K.U-TOKYO.AC.JP",
        "email": "MS.K.U-TOKYO.AC.JP;I.KYOTO-U.AC.JP;K.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;1+0",
        "aff_unique_norm": "University of Tokyo;RIKEN;Kyoto University",
        "aff_unique_dep": ";Advanced Institute for Computational Science;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.aip.riken.jp;https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "UTokyo;RIKEN AIP;Kyoto U",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Towards Better Explanations for Object Detection",
        "site": "https://proceedings.mlr.press/v222/truong24a.html",
        "author": "Van Binh Truong; Truong Thanh Hung Nguyen; Vo Thanh Khang Nguyen; Quoc Khanh Nguyen; Quoc Hung Cao",
        "abstract": "Recent advances in Artificial Intelligence (AI) technology have promoted their use in almost every field. The growing complexity of deep neural networks (DNNs) makes it increasingly difficult and important to explain the inner workings and decisions of the network. However, most current techniques for explaining DNNs focus mainly on interpreting classification tasks. This paper proposes a method to explain the decision for any object detection model called D-CLOSE. To closely track the model\u2019s behavior, we used multiple levels of segmentation on the image and a process to combine them. We performed tests on the MS-COCO dataset with the YOLOX model, which shows that our method outperforms D-RISE and can give a better quality and less noise explanation.",
        "bibtex": "@InProceedings{pmlr-v222-truong24a,\n  title = \t {Towards Better Explanations for Object Detection},\n  author =       {Truong, Van Binh and Nguyen, Truong Thanh Hung and Nguyen, Vo Thanh Khang and Nguyen, Quoc Khanh and Cao, Quoc Hung},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1385--1400},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/truong24a/truong24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/truong24a.html},\n  abstract = \t {Recent advances in Artificial Intelligence (AI) technology have promoted their use in almost every field. The growing complexity of deep neural networks (DNNs) makes it increasingly difficult and important to explain the inner workings and decisions of the network. However, most current techniques for explaining DNNs focus mainly on interpreting classification tasks. This paper proposes a method to explain the decision for any object detection model called D-CLOSE. To closely track the model\u2019s behavior, we used multiple levels of segmentation on the image and a process to combine them. We performed tests on the MS-COCO dataset with the YOLOX model, which shows that our method outperforms D-RISE and can give a better quality and less noise explanation.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/truong24a/truong24a.pdf",
        "supp": "",
        "pdf_size": 14353815,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4102867260753405614&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Quy Nhon AI, FPT Software, Vietnam; Quy Nhon AI, FPT Software, Vietnam + Analytics Everywhere Lab, University of New Brunswick, Canada; Quy Nhon AI, FPT Software, Vietnam; Quy Nhon AI, FPT Software, Vietnam; Quy Nhon AI, FPT Software, Vietnam",
        "aff_domain": "fpt.com;fpt.com;fpt.com;fpt.com;fpt.com",
        "email": "fpt.com;fpt.com;fpt.com;fpt.com;fpt.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "FPT Software;University of New Brunswick",
        "aff_unique_dep": ";Analytics Everywhere Lab",
        "aff_unique_url": "https://fpt-software.com;https://www.unb.ca",
        "aff_unique_abbr": "FPT Software;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0",
        "aff_country_unique": "Vietnam;Canada"
    },
    {
        "title": "Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive Behavioral Costs in 3D Games",
        "site": "https://proceedings.mlr.press/v222/ho24a.html",
        "author": "Kuo-Hao Ho; Ping-Chun Hsieh; Chiu-Chou Lin; You-Ren Lou; Feng-Jian Wang; I-Chen Wu",
        "abstract": "In this paper, we propose a new approach called Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL) for training a human-like agent with competitive strength. While deep reinforcement learning agents have recently achieved superhuman performance in various video games, some of these unconstrained agents may exhibit actions, such as shaking and spinning, that are not typically observed in human behavior, resulting in peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL augments behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between the performance and the human-like behavior. Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, we demonstrate that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning. These findings underscore the effectiveness of our proposed approach in promoting more natural and human-like behavior during gameplay.",
        "bibtex": "@InProceedings{pmlr-v222-ho24a,\n  title = \t {{Towards Human-Like RL}: {T}aming Non-Naturalistic Behavior in Deep {RL} via Adaptive Behavioral Costs in {3D} Games},\n  author =       {Ho, Kuo-Hao and Hsieh, Ping-Chun and Lin, Chiu-Chou and Lou, You-Ren and Wang, Feng-Jian and Wu, I-Chen},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {438--453},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/ho24a/ho24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/ho24a.html},\n  abstract = \t {In this paper, we propose a new approach called Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL) for training a human-like agent with competitive strength. While deep reinforcement learning agents have recently achieved superhuman performance in various video games, some of these unconstrained agents may exhibit actions, such as shaking and spinning, that are not typically observed in human behavior, resulting in peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL augments behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between the performance and the human-like behavior. Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, we demonstrate that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning. These findings underscore the effectiveness of our proposed approach in promoting more natural and human-like behavior during gameplay.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/ho24a/ho24a.pdf",
        "supp": "",
        "pdf_size": 1406550,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14578229167803307626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, National Yang Ming Chiao Tung University; Department of Computer Science, National Yang Ming Chiao Tung University; ; ; Department of Computer Science, National Chiao Tung University; Department of Computer Science, National Chiao Tung University",
        "aff_domain": "nycu.edu.tw;nycu.edu.tw;outlook.com;gmail.com;cs.nctu.edu.tw;cs.nctu.edu.tw",
        "email": "nycu.edu.tw;nycu.edu.tw;outlook.com;gmail.com;cs.nctu.edu.tw;cs.nctu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "National Yang Ming Chiao Tung University;National Chiao Tung University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.nctu.edu.tw;https://www.nctu.edu.tw",
        "aff_unique_abbr": "NYCU;NCTU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency",
        "site": "https://proceedings.mlr.press/v222/yao24b.html",
        "author": "Yunpeng Yao; Man Wu; Zheng Chen; Renyuan Zhang",
        "abstract": "Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs.  Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spike firing threshold and activation). We evaluate the proposal for both convolution and recurrent models. Our experimental results indicate state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and TinyImageNet. Our framework achieves 72.41% and 72.31% top-1 accuracy with only 1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10X and 3X joule energy than a standard ANN and SNN, respectively, on CIFAR10, without additional time steps.",
        "bibtex": "@InProceedings{pmlr-v222-yao24b,\n  title = \t {Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency},\n  author =       {Yao, Yunpeng and Wu, Man and Chen, Zheng and Zhang, Renyuan},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1558--1573},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/yao24b/yao24b.pdf},\n  url = \t {https://proceedings.mlr.press/v222/yao24b.html},\n  abstract = \t {Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs.  Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spike firing threshold and activation). We evaluate the proposal for both convolution and recurrent models. Our experimental results indicate state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and TinyImageNet. Our framework achieves 72.41% and 72.31% top-1 accuracy with only 1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10X and 3X joule energy than a standard ANN and SNN, respectively, on CIFAR10, without additional time steps.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/yao24b/yao24b.pdf",
        "supp": "",
        "pdf_size": 3418233,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5352166859701507563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Information Science and Engineering, Shandong University, Shandong, China; Department of Information and Computer Science, Keio University, Kanagawa, Japan; SANKEN, Osaka University, Osaka, Japan; Division of Information Science, Nara Institute of Science and Technology, Nara, Japan",
        "aff_domain": "mail.sdu.edu.cn;am.ics.keio.ac.jp;sanken.osaka-u.ac.jp;is.naist.jp",
        "email": "mail.sdu.edu.cn;am.ics.keio.ac.jp;sanken.osaka-u.ac.jp;is.naist.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Shandong University;Keio University;Osaka University;Nara Institute of Science and Technology",
        "aff_unique_dep": "Information Science and Engineering;Department of Information and Computer Science;SANKEN;Division of Information Science",
        "aff_unique_url": "http://www.sdu.edu.cn;https://www.keio.ac.jp;https://www.osaka-u.ac.jp;https://www.nist.go.jp",
        "aff_unique_abbr": ";Keio;;NIST",
        "aff_campus_unique_index": "1;2;3",
        "aff_campus_unique": ";Kanagawa;Osaka;Nara",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;Japan"
    },
    {
        "title": "Transformed Gaussian Processes for Characterizing a Model\u2019s Discrepancy",
        "site": "https://proceedings.mlr.press/v222/nioche24a.html",
        "author": "Aur\u00e9lien Nioche; Ville Tanskanen; Marcelo Hartmann; Arto Klami",
        "abstract": "Mathematical models of observational phenomena are at the core of experimental sciences. By learning the parameters of such models from typically noisy observations, we can interpret and predict the phenomena under investigation. This process, however, assumes that the model itself is correct and that we are only uncertain of its parameters. In practice, this is rarely true, but rather the model is a simplification of the actual generative process. One proposed remedy is a post hoc investigation of how the model differs from reality, by explicitly modeling the discrepancy between the two. In this paper, we use transformed Gaussian processes as flexible models for this. Our formulation relaxes the assumption on the correctness of the model by assuming it is only correct in expectation, and it directly supports both additive and multiplicative corrections, treated separately in the literature, using suitable transformations. We demonstrate the approach in two example cases: modeling human growth (relation age-height) and modeling the risk attitude (relation reward-utility). The former provides a simple example, while the second case highlights the importance of the transformations in obtaining meaningful information about the discrepancy.",
        "bibtex": "@InProceedings{pmlr-v222-nioche24a,\n  title = \t {Transformed Gaussian Processes for Characterizing a Model\u2019s Discrepancy},\n  author =       {Nioche, Aur\\'elien and Tanskanen, Ville and Hartmann, Marcelo and Klami, Arto},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {991--1006},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/nioche24a/nioche24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/nioche24a.html},\n  abstract = \t {Mathematical models of observational phenomena are at the core of experimental sciences. By learning the parameters of such models from typically noisy observations, we can interpret and predict the phenomena under investigation. This process, however, assumes that the model itself is correct and that we are only uncertain of its parameters. In practice, this is rarely true, but rather the model is a simplification of the actual generative process. One proposed remedy is a post hoc investigation of how the model differs from reality, by explicitly modeling the discrepancy between the two. In this paper, we use transformed Gaussian processes as flexible models for this. Our formulation relaxes the assumption on the correctness of the model by assuming it is only correct in expectation, and it directly supports both additive and multiplicative corrections, treated separately in the literature, using suitable transformations. We demonstrate the approach in two example cases: modeling human growth (relation age-height) and modeling the risk attitude (relation reward-utility). The former provides a simple example, while the second case highlights the importance of the transformations in obtaining meaningful information about the discrepancy.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/nioche24a/nioche24a.pdf",
        "supp": "",
        "pdf_size": 709281,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0ysxbwW6cTkJ:scholar.google.com/&scioq=Transformed+Gaussian+Processes+for+Characterizing+a+Model%E2%80%99s+Discrepancy&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "School of Computing Science, University of Glasgow, Glasgow G12 8RZ, UK; Department of Computer Science, University of Helsinki, 00560 Helsinki, Finland; Department of Computer Science, University of Helsinki, 00560 Helsinki, Finland; Department of Computer Science, University of Helsinki, 00560 Helsinki, Finland",
        "aff_domain": "gmail.com;helsinki.fi;helsinki.fi;helsinki.fi",
        "email": "gmail.com;helsinki.fi;helsinki.fi;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Glasgow;University of Helsinki",
        "aff_unique_dep": "School of Computing Science;Department of Computer Science",
        "aff_unique_url": "https://www.gla.ac.uk;https://www.helsinki.fi",
        "aff_unique_abbr": "UofG;UH",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Glasgow;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United Kingdom;Finland"
    },
    {
        "title": "Understanding More Knowledge Makes the Transformer Perform Better in Document-level Relation Extraction",
        "site": "https://proceedings.mlr.press/v222/haotian24a.html",
        "author": "Chen Haotian; Chen Yijiang; Zhou Xiangdong",
        "abstract": "Relation extraction plays a vital role in knowledge graph construction. In contrast with the traditional relation extraction on a single sentence, extracting relations from multiple sentences as a whole will harvest more valuable and richer knowledge. Recently, the Transformer-based pre-trained language models (TPLMs) are widely adopted to tackle document-level relation extraction (DocRE). Graph-based methods, aiming to acquire knowledge between entities to form entity-level relation graphs, have facilitated the rapid development of DocRE by infusing their proposed models with the knowledge. However, beyond entity-level knowledge, we discover many other kinds of knowledge that can aid humans to extract relations. It remains unclear whether and in which way they can be adopted to improve the performance of the Transformer, which affects the maximum performance gain of Transformer-based methods. In this paper, we propose a novel weighted multi-channel Transformer (WMCT) to infuse unlimited kinds of knowledge into the vanilla Transformer. Based on WMCT, we also explore five kinds of knowledge to enhance both its reasoning ability and expressive power. Our extensive experimental results demonstrate that: (1) more knowledge makes the performance of the Transformer better and (2) more informative knowledge leads to more performance gain. We appeal to future Transformer-based work to consider exploring more informative knowledge to improve the performance of the Transformer.",
        "bibtex": "@InProceedings{pmlr-v222-haotian24a,\n  title = \t {Understanding More Knowledge Makes the Transformer Perform Better in Document-level Relation Extraction},\n  author =       {Haotian, Chen and Yijiang, Chen and Xiangdong, Zhou},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {231--246},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/haotian24a/haotian24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/haotian24a.html},\n  abstract = \t {Relation extraction plays a vital role in knowledge graph construction. In contrast with the traditional relation extraction on a single sentence, extracting relations from multiple sentences as a whole will harvest more valuable and richer knowledge. Recently, the Transformer-based pre-trained language models (TPLMs) are widely adopted to tackle document-level relation extraction (DocRE). Graph-based methods, aiming to acquire knowledge between entities to form entity-level relation graphs, have facilitated the rapid development of DocRE by infusing their proposed models with the knowledge. However, beyond entity-level knowledge, we discover many other kinds of knowledge that can aid humans to extract relations. It remains unclear whether and in which way they can be adopted to improve the performance of the Transformer, which affects the maximum performance gain of Transformer-based methods. In this paper, we propose a novel weighted multi-channel Transformer (WMCT) to infuse unlimited kinds of knowledge into the vanilla Transformer. Based on WMCT, we also explore five kinds of knowledge to enhance both its reasoning ability and expressive power. Our extensive experimental results demonstrate that: (1) more knowledge makes the performance of the Transformer better and (2) more informative knowledge leads to more performance gain. We appeal to future Transformer-based work to consider exploring more informative knowledge to improve the performance of the Transformer.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/haotian24a/haotian24a.pdf",
        "supp": "",
        "pdf_size": 949865,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16845971114606767565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Unleashing the Power of High-pass Filtering in Continuous Graph Neural Networks",
        "site": "https://proceedings.mlr.press/v222/zhang24e.html",
        "author": "Acong Zhang; Ping Li",
        "abstract": "Recent Continuous Graph Neural Networks (CGNNs) have attracted great attention due to its merits of infinite depth without oversmoothing. However, most of the existing CGNNs perform low-pass filtering in nature, as they are derived from discrete Laplacian-smoothing based graph neural networks (GNNs). While prior research has shown the promising results of high-pass filtering for node representation learning, particularly on heterophilous graphs, there remains a need to extend it to continuous domain and explore the synergy between two filtering channels. In this paper, by leveraging low-pass and high-pass filtering, we propose a novel dual-channel continuous graph neural network architecture to address this gap. In particular, we introduce a dimension masking method to coordinate the contribution of all low and high pass filtered feature dimensions to node classification. Our aim is to deepen the understanding of the link between high and low filters, unraveling their distinct roles in learning node representations. To evaluate the effectiveness of our framework, we conduct extensive experiments focusing on the node classification task of heterophilous graphs. Our results demonstrate the competitive performance of our approach, showcasing its robustness to oversmoothing.",
        "bibtex": "@InProceedings{pmlr-v222-zhang24e,\n  title = \t {Unleashing the Power of High-pass Filtering in Continuous Graph Neural Networks},\n  author =       {Zhang, Acong and Li, Ping},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1683--1698},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/zhang24e/zhang24e.pdf},\n  url = \t {https://proceedings.mlr.press/v222/zhang24e.html},\n  abstract = \t {Recent Continuous Graph Neural Networks (CGNNs) have attracted great attention due to its merits of infinite depth without oversmoothing. However, most of the existing CGNNs perform low-pass filtering in nature, as they are derived from discrete Laplacian-smoothing based graph neural networks (GNNs). While prior research has shown the promising results of high-pass filtering for node representation learning, particularly on heterophilous graphs, there remains a need to extend it to continuous domain and explore the synergy between two filtering channels. In this paper, by leveraging low-pass and high-pass filtering, we propose a novel dual-channel continuous graph neural network architecture to address this gap. In particular, we introduce a dimension masking method to coordinate the contribution of all low and high pass filtered feature dimensions to node classification. Our aim is to deepen the understanding of the link between high and low filters, unraveling their distinct roles in learning node representations. To evaluate the effectiveness of our framework, we conduct extensive experiments focusing on the node classification task of heterophilous graphs. Our results demonstrate the competitive performance of our approach, showcasing its robustness to oversmoothing.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/zhang24e/zhang24e.pdf",
        "supp": "",
        "pdf_size": 419263,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5582557511464121489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Southwest Petroleum University; School of Computer Science, Southwest Petroleum University",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Southwest Petroleum University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.swpu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer",
        "site": "https://proceedings.mlr.press/v222/sreekar24a.html",
        "author": "P Aditya Sreekar; Sahil Verma; Varun Madhavan; Abhishek Persad",
        "abstract": "Amazon ships billions of packages to its customers annually within the United States. Shipping cost of these packages are used on the day of shipping (day 0) to estimate profitability of sales. Downstream systems utilize these days 0 profitability estimates to make financial decisions, such as pricing strategies and delisting loss-making products. However, obtaining accurate shipping cost estimates on day 0 is complex for reasons like delay in carrier invoicing or fixed cost components getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead to bad decision, such as pricing items too low or high, or promoting the wrong product to the customers. Current solutions for estimating shipping costs on day 0 rely on tree-based models that require extensive manual engineering efforts. In this study, we propose a novel architecture called the Rate Card Transformer (RCT) that uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan. Unlike other transformer-based tabular models, RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment. For example, RCT can encode properties of all products in a package. Our results demonstrate that cost predictions made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover, the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models.",
        "bibtex": "@InProceedings{pmlr-v222-sreekar24a,\n  title = \t {Unveiling the Power of Self-Attention for Shipping Cost Prediction: {T}he Rate Card Transformer},\n  author =       {Sreekar, P Aditya and Verma, Sahil and Madhavan, Varun and Persad, Abhishek},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {1263--1275},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/sreekar24a/sreekar24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/sreekar24a.html},\n  abstract = \t {Amazon ships billions of packages to its customers annually within the United States. Shipping cost of these packages are used on the day of shipping (day 0) to estimate profitability of sales. Downstream systems utilize these days 0 profitability estimates to make financial decisions, such as pricing strategies and delisting loss-making products. However, obtaining accurate shipping cost estimates on day 0 is complex for reasons like delay in carrier invoicing or fixed cost components getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead to bad decision, such as pricing items too low or high, or promoting the wrong product to the customers. Current solutions for estimating shipping costs on day 0 rely on tree-based models that require extensive manual engineering efforts. In this study, we propose a novel architecture called the Rate Card Transformer (RCT) that uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan. Unlike other transformer-based tabular models, RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment. For example, RCT can encode properties of all products in a package. Our results demonstrate that cost predictions made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover, the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/sreekar24a/sreekar24a.pdf",
        "supp": "",
        "pdf_size": 458022,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:a6AufaLe4XIJ:scholar.google.com/&scioq=Unveiling+the+Power+of+Self-Attention+for+Shipping+Cost+Prediction:+The+Rate+Card+Transformer&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Amazon; Amazon; Indian Institute of Technology, Kharagpur + Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;iitkgp.ac.in;amazon.com",
        "email": "amazon.com;amazon.com;iitkgp.ac.in;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0;0",
        "aff_unique_norm": "Amazon.com, Inc.;Indian Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amazon.com;https://www.iitkgp.ac.in",
        "aff_unique_abbr": "Amazon;IIT Kharagpur",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Kharagpur",
        "aff_country_unique_index": "0;0;1+0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "VMLC: Statistical Process Control for Image Classification in Manufacturing",
        "site": "https://proceedings.mlr.press/v222/mascha24a.html",
        "author": "Philipp Mascha",
        "abstract": "Through ground-breaking advances in Machine Learning its real-world applications have become commonplace in many areas over the past decade. Deep and complex models are able to solve difficult tasks with super-human precision. But for manufacturing quality control, in theory a ideal match for these methods, the step from proof-of-concept towards live deployment is often not feasible. One major obstacle is the unreliability of Machine Learning predictions when confronted with data diverging from the known characteristics. While overall accuracy is high, wrong results may be returned with no indication of their uncertainty. In manufacturing, where scarce errors mean great damages, additional safety measures are required. In this work, I present Visual Machine Learning Control (VMLC), an approach developed upon a real world visual quality control system that operates in a high throughput manufacturing line. Instead of applying sole classification or anomaly detection, both is done in combination. A scalar metric derived from an Auto-Encoder reconstruction error measures the compliance of captured images with the training data the system is trained on. This metric is integrated into the widely used framework of industrial Statistical Process Control (SPC), significantly increasing robustness through meaningful control limits and enabling active learning. The system is evaluated on a large dataset of real-world industrial welding images.",
        "bibtex": "@InProceedings{pmlr-v222-mascha24a,\n  title = \t {{VMLC}: {S}tatistical Process Control for Image Classification in Manufacturing},\n  author =       {Mascha, Philipp},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {866--881},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/mascha24a/mascha24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/mascha24a.html},\n  abstract = \t {Through ground-breaking advances in Machine Learning its real-world applications have become commonplace in many areas over the past decade. Deep and complex models are able to solve difficult tasks with super-human precision. But for manufacturing quality control, in theory a ideal match for these methods, the step from proof-of-concept towards live deployment is often not feasible. One major obstacle is the unreliability of Machine Learning predictions when confronted with data diverging from the known characteristics. While overall accuracy is high, wrong results may be returned with no indication of their uncertainty. In manufacturing, where scarce errors mean great damages, additional safety measures are required. In this work, I present Visual Machine Learning Control (VMLC), an approach developed upon a real world visual quality control system that operates in a high throughput manufacturing line. Instead of applying sole classification or anomaly detection, both is done in combination. A scalar metric derived from an Auto-Encoder reconstruction error measures the compliance of captured images with the training data the system is trained on. This metric is integrated into the widely used framework of industrial Statistical Process Control (SPC), significantly increasing robustness through meaningful control limits and enabling active learning. The system is evaluated on a large dataset of real-world industrial welding images.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/mascha24a/mascha24a.pdf",
        "supp": "",
        "pdf_size": 2175384,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6578713621833578281&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Manufacturing IT and Automation, Osram Automotive, Schwabm\u00a8 unchen, Germany",
        "aff_domain": "ams-osram.com",
        "email": "ams-osram.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Osram Automotive",
        "aff_unique_dep": "Manufacturing IT and Automation",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory",
        "site": "https://proceedings.mlr.press/v222/alquabeh24a.html",
        "author": "Hilal AlQuabeh; Bhaskar Mukhoty; Bin Gu",
        "abstract": "Pairwise learning is essential in machine learning, especially for problems involving loss functions defined on pairs of training examples. Online gradient descent (OGD) algorithms have been proposed to handle online pairwise learning, where data arrives sequentially. However, the pairwise nature of the problem makes scalability challenging, as the gradient computation for a new sample involves all past samples. Recent advancements in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance. In this study, we propose a limited memory OGD algorithm that extends to kernel online pairwise learning while improving the sublinear regret. Specifically, we establish a clear connection between the variance of online gradients and the regret, and construct online gradients using the most recent stratified samples with a limited buffer of size of $s$ representing all past data, which have a complexity of $O(sT)$ and employs $O(\\sqrt{T}\\log{T})$ random Fourier features for kernel approximation. Importantly, our theoretical results demonstrate that the variance-reduced online gradients lead to an improved sublinear regret bound. The experiments on real-world datasets demonstrate the superiority of our algorithm over both kernelized and linear online pairwise learning algorithms.The code is available at https://github.com/halquabeh/ACML-2023-FPOGD-Code.git.",
        "bibtex": "@InProceedings{pmlr-v222-alquabeh24a,\n  title = \t {Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory},\n  author =       {AlQuabeh, Hilal and Mukhoty, Bhaskar and Gu, Bin},\n  booktitle = \t {Proceedings of the 15th Asian Conference on Machine Learning},\n  pages = \t {28--43},\n  year = \t {2024},\n  editor = \t {Yan\u0131ko\u011flu, Berrin and Buntine, Wray},\n  volume = \t {222},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {11--14 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v222/alquabeh24a/alquabeh24a.pdf},\n  url = \t {https://proceedings.mlr.press/v222/alquabeh24a.html},\n  abstract = \t {Pairwise learning is essential in machine learning, especially for problems involving loss functions defined on pairs of training examples. Online gradient descent (OGD) algorithms have been proposed to handle online pairwise learning, where data arrives sequentially. However, the pairwise nature of the problem makes scalability challenging, as the gradient computation for a new sample involves all past samples. Recent advancements in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance. In this study, we propose a limited memory OGD algorithm that extends to kernel online pairwise learning while improving the sublinear regret. Specifically, we establish a clear connection between the variance of online gradients and the regret, and construct online gradients using the most recent stratified samples with a limited buffer of size of $s$ representing all past data, which have a complexity of $O(sT)$ and employs $O(\\sqrt{T}\\log{T})$ random Fourier features for kernel approximation. Importantly, our theoretical results demonstrate that the variance-reduced online gradients lead to an improved sublinear regret bound. The experiments on real-world datasets demonstrate the superiority of our algorithm over both kernelized and linear online pairwise learning algorithms.The code is available at https://github.com/halquabeh/ACML-2023-FPOGD-Code.git.}\n}",
        "pdf": "https://proceedings.mlr.press/v222/alquabeh24a/alquabeh24a.pdf",
        "supp": "",
        "pdf_size": 696178,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=530207670637400495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; ; Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
        "aff_domain": "mbzuai.ac.ae;gmail.com;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;gmail.com;mbzuai.ac.ae",
        "github": "https://github.com/halquabeh/ACML-2023-FPOGD-Code.git",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.mbru.ac.ae",
        "aff_unique_abbr": "MBZUAI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Abu Dhabi",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Arab Emirates"
    }
]