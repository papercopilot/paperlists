[
    {
        "id": "2ba02d79f4",
        "title": "A Fast Hierarchical Alternating Least Squares Algorithm for Orthogonal Nonnegative Matrix Factorization",
        "site": "https://proceedings.mlr.press/v39/kimura14.html",
        "author": "Keigo Kimura; Yuzuru Tanaka; Mineichi Kudo",
        "abstract": "Nonnegative Matrix Factorization (NMF) is a popular technique in a variety of fields due to its component-based representation with physical interpretablity. NMF finds a nonnegative hidden structures as oblique bases and coefficients. Recently, Orthogonal NMF (ONMF), imposing an orthogonal constraint into NMF, has been gathering a great deal of attention. ONMF is more appropriate for the clustering task because the resultant constrained matrix consisting of the coefficients can be considered as an indicator matrix. All traditional ONMF algorithms are based on multiplicative update rules or project gradient descent method. However, these algorithms are slow in convergence compared with the state-of-the-art algorithms used for regular NMF. This is because they update a matrix in each iteration step. In this paper, therefore, we propose to update the current matrix column-wisely using Hierarchical Alternating Least Squares algorithm (HALS) that is typically used for NMF. The orthogonality and nonnegativity constraints are both utilized efficiently in the column-wise update procedure. Through theoretical analysis and experiments on six real-life datasets, it was shown that the proposed algorithm converges faster than the other conventional ONMF algorithms due to a smaller number of iterations, although the theoretical complexity is the same. It was also shown that the orthogonality is also attained in an earlier stage.",
        "bibtex": "@InProceedings{pmlr-v39-kimura14,\n  title = \t {A Fast Hierarchical Alternating Least Squares Algorithm for Orthogonal Nonnegative Matrix Factorization},\n  author = \t {Kimura, Keigo and Tanaka, Yuzuru and Kudo, Mineichi},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {129--141},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/kimura14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/kimura14.html},\n  abstract = \t {Nonnegative Matrix Factorization (NMF) is a popular technique in a variety of fields due to its component-based representation with physical interpretablity. NMF finds a nonnegative hidden structures as oblique bases and coefficients. Recently, Orthogonal NMF (ONMF), imposing an orthogonal constraint into NMF, has been gathering a great deal of attention. ONMF is more appropriate for the clustering task because the resultant constrained matrix consisting of the coefficients can be considered as an indicator matrix. All traditional ONMF algorithms are based on multiplicative update rules or project gradient descent method. However, these algorithms are slow in convergence compared with the state-of-the-art algorithms used for regular NMF. This is because they update a matrix in each iteration step. In this paper, therefore, we propose to update the current matrix column-wisely using Hierarchical Alternating Least Squares algorithm (HALS) that is typically used for NMF. The orthogonality and nonnegativity constraints are both utilized efficiently in the column-wise update procedure. Through theoretical analysis and experiments on six real-life datasets, it was shown that the proposed algorithm converges faster than the other conventional ONMF algorithms due to a smaller number of iterations, although the theoretical complexity is the same. It was also shown that the orthogonality is also attained in an earlier stage.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/kimura14.pdf",
        "supp": "",
        "pdf_size": 380799,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15387851584331569797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Graduate School of Information Science and Technology, Hokkaido University, Sapporo, 060-0814 Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, 060-0814 Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, 060-0814 Japan",
        "aff_domain": "main.ist.hokudai.ac.jp;meme.hokudai.ac.jp;main.ist.hokudai.ac.jp",
        "email": "main.ist.hokudai.ac.jp;meme.hokudai.ac.jp;main.ist.hokudai.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hokkaido University",
        "aff_unique_dep": "Graduate School of Information Science and Technology",
        "aff_unique_url": "https://www.hokudai.ac.jp",
        "aff_unique_abbr": "Hokkaido U.",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Sapporo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "cab77daf15",
        "title": "A UCB-Like Strategy of Collaborative Filtering",
        "site": "https://proceedings.mlr.press/v39/nakamura14.html",
        "author": "Atsuyoshi Nakamura",
        "abstract": "We consider a direct mail problem in which a system repeats the following process every day during some period: select a set of user-item pairs (u,i), send a recommendation mail of item i to user u for each selected pair (u,i), and receive a response from each user. We assume that each response can be obtained before the next process and through the response, the system can know the user\u2019s evaluation of the recommended item directly or indirectly. Each pair (u,i) can be selected at most once during the period. If the total number of selections is very small compared to the number of entries in the whole user-item matrix, what selection strategy should be used to maximize the total sum of users\u2019 evaluations during the period? We consider a UCB-like strategy for this problem, and show two methods using the strategy. The effectiveness of our methods are demonstrated by experiments using synthetic and real datasets.",
        "bibtex": "@InProceedings{pmlr-v39-nakamura14,\n  title = \t {A {UCB}-Like Strategy of Collaborative Filtering},\n  author = \t {Nakamura, Atsuyoshi},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {315--329},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/nakamura14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/nakamura14.html},\n  abstract = \t {We consider a direct mail problem in which a system repeats the following process every day during some period: select a set of user-item pairs (u,i), send a recommendation mail of item i to user u for each selected pair (u,i), and receive a response from each user. We assume that each response can be obtained before the next process and through the response, the system can know the user\u2019s evaluation of the recommended item directly or indirectly. Each pair (u,i) can be selected at most once during the period. If the total number of selections is very small compared to the number of entries in the whole user-item matrix, what selection strategy should be used to maximize the total sum of users\u2019 evaluations during the period? We consider a UCB-like strategy for this problem, and show two methods using the strategy. The effectiveness of our methods are demonstrated by experiments using synthetic and real datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/nakamura14.pdf",
        "supp": "",
        "pdf_size": 201112,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9739334703470241053&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Hokkaido University, Kita 14, Nishi 9, Kita-ku, Sapporo 060 -0814, Japan",
        "aff_domain": "main.ist.hokudai.ac.jp",
        "email": "main.ist.hokudai.ac.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Hokkaido University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hokudai.ac.jp",
        "aff_unique_abbr": "Hokkaido U",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Sapporo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b8d120cfe2",
        "title": "Bibliographic Analysis with the Citation Network Topic Model",
        "site": "https://proceedings.mlr.press/v39/lim14.html",
        "author": "Kar Wai Lim; Wray Buntine",
        "abstract": "Bibliographic analysis considers author\u2019s research areas, the citation network and paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents using a non-parametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. We propose a novel and efficient inference algorithm for the model to explore subsets of research publications from CiteSeerX. Our model demonstrates improved performance in both model fitting and clustering task comparing to several baselines.",
        "bibtex": "@InProceedings{pmlr-v39-lim14,\n  title = \t {Bibliographic Analysis with the Citation Network Topic Model},\n  author = \t {Lim, Kar Wai and Buntine, Wray},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {142--158},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/lim14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/lim14.html},\n  abstract = \t {Bibliographic analysis considers author\u2019s research areas, the citation network and paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents using a non-parametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. We propose a novel and efficient inference algorithm for the model to explore subsets of research publications from CiteSeerX. Our model demonstrates improved performance in both model fitting and clustering task comparing to several baselines.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/lim14.pdf",
        "supp": "",
        "pdf_size": 539460,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5184579664722464329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Australian National University, Canberra, Australia + NICTA, Canberra, Australia; Monash University, Clayton, Australia",
        "aff_domain": "anu.edu.au;monash.edu",
        "email": "anu.edu.au;monash.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Australian National University;NICTA;Monash University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.anu.edu.au;;https://www.monash.edu",
        "aff_unique_abbr": "ANU;;Monash",
        "aff_campus_unique_index": "0+0;1",
        "aff_campus_unique": "Canberra;Clayton",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "65521d136f",
        "title": "Dual online inference for latent Dirichlet allocation",
        "site": "https://proceedings.mlr.press/v39/than14.html",
        "author": "Khoat Than; Tung Doan",
        "abstract": "Latent Dirichlet allocation (LDA) provides an efficient tool to analyze very large text collections. In this paper, we discuss three novel contributions: (1) a proof for the tractability of the MAP estimation of topic mixtures under certain conditions that might fit well with practices, even though the problem is known to be intractable in the worst case; (2) a provably fast algorithm (OFW) for inferring topic mixtures; (3) a dual online algorithm (DOLDA) for learning LDA at a large scale. We show that OFW converges to some local optima, but under certain conditions it can converge to global optima. The discussion of OFW is very general and hence can be readily employed to accelerate the MAP estimation in a wide class of probabilistic models. From extensive experiments we find that DOLDA can achieve significantly better predictive performance and more interpretable topics, with lower runtime, than stochastic variational inference. Further, DOLDA enables us to easily analyze text streams or millions of documents.",
        "bibtex": "@InProceedings{pmlr-v39-than14,\n  title = \t {Dual online inference for latent {D}irichlet allocation},\n  author = \t {Than, Khoat and Doan, Tung},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {80--95},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/than14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/than14.html},\n  abstract = \t {Latent Dirichlet allocation (LDA) provides an efficient tool to analyze very large text collections. In this paper, we discuss three novel contributions: (1) a proof for the tractability of the MAP estimation of topic mixtures under certain conditions that might fit well with practices, even though the problem is known to be intractable in the worst case; (2) a provably fast algorithm (OFW) for inferring topic mixtures; (3) a dual online algorithm (DOLDA) for learning LDA at a large scale. We show that OFW converges to some local optima, but under certain conditions it can converge to global optima. The discussion of OFW is very general and hence can be readily employed to accelerate the MAP estimation in a wide class of probabilistic models. From extensive experiments we find that DOLDA can achieve significantly better predictive performance and more interpretable topics, with lower runtime, than stochastic variational inference. Further, DOLDA enables us to easily analyze text streams or millions of documents.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/than14.pdf",
        "supp": "",
        "pdf_size": 461455,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17037362118568439871&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Hanoi University of Science and Technology; Hanoi University of Science and Technology",
        "aff_domain": "soict.hust.edu.vn;bmw.yahoo.com",
        "email": "soict.hust.edu.vn;bmw.yahoo.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hanoi University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hust.edu.vn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hanoi",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "5c73420308",
        "title": "Efficient Sample Mining for Object Detection",
        "site": "https://proceedings.mlr.press/v39/canevet14a.html",
        "author": "Olivier Canevet; Francois Fleuret",
        "abstract": "Object detectors based on the sliding window technique are usually trained in two successive steps: first, an initial classifier is trained on a population of positive samples (i.e. images of the object to detect) and negative samples randomly extracted from scenes which do not contain the object to detect. Then, the scenes are scanned with that initial classifier to enrich the initial set with negative samples incorrectly classified as positive. This bootstrapping process provides the learning algorithm with \"hard\" samples, which help to improve the decision boundary. Little work has been done on how to efficiently enrich the training set. While the standard bootstrapping approach densely visits the scenes, we propose to evaluate which regions of scenes can be discarded without any further computation to concentrate the search on promising areas. We apply our method to two standard object detection settings, pedestrian and face detection, and show that it provides a multi-fold speed up.",
        "bibtex": "@InProceedings{pmlr-v39-canevet14a,\n  title = \t {Efficient Sample Mining for Object Detection},\n  author = \t {Canevet, Olivier and Fleuret, Francois},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {48--63},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/canevet14a.pdf},\n  url = \t {https://proceedings.mlr.press/v39/canevet14a.html},\n  abstract = \t {Object detectors based on the sliding window technique are usually trained in two successive steps: first, an initial classifier is trained on a population of positive samples (i.e. images of the object to detect) and negative samples randomly extracted from scenes which do not contain the object to detect. Then, the scenes are scanned with that initial classifier to enrich the initial set with negative samples incorrectly classified as positive. This bootstrapping process provides the learning algorithm with \"hard\" samples, which help to improve the decision boundary. Little work has been done on how to efficiently enrich the training set. While the standard bootstrapping approach densely visits the scenes, we propose to evaluate which regions of scenes can be discarded without any further computation to concentrate the search on promising areas. We apply our method to two standard object detection settings, pedestrian and face detection, and show that it provides a multi-fold speed up.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/canevet14a.pdf",
        "supp": "",
        "pdf_size": 1533806,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1287097385929075804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Vision and Learning group, Idiap Research Institute, Martigny, Switzerland + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland; Computer Vision and Learning group, Idiap Research Institute, Martigny, Switzerland + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland",
        "aff_domain": "idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Idiap Research Institute;EPFL",
        "aff_unique_dep": "Computer Vision and Learning group;",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch",
        "aff_unique_abbr": ";EPFL",
        "aff_campus_unique_index": "0+1;0+1",
        "aff_campus_unique": "Martigny;Lausanne",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "19bb738cda",
        "title": "Ensembles for Time Series Forecasting",
        "site": "https://proceedings.mlr.press/v39/oliveira14.html",
        "author": "Mariana Oliveira; Luis Torgo",
        "abstract": "This paper describes a new type of ensembles that aims at improving the predictive performance of these approaches in time series forecasting. Ensembles are recognised as one of the most successful approaches to prediction tasks. Previous theoretical studies of ensembles have shown that one of the key reasons for this performance is diversity among ensemble members. Several methods exist to generate diversity. The key idea of the work we are presenting here is to propose a new form of diversity generation that explores some specific properties of time series prediction tasks. Our hypothesis is that the resulting ensemble members will be better at addressing different dynamic regimes of time series data. Our large set of experiments confirms that the methods we have explored for generating diversity are able to improve the performance of the equivalent ensembles with standard diversity generation procedures.",
        "bibtex": "@InProceedings{pmlr-v39-oliveira14,\n  title = \t {Ensembles for Time Series Forecasting},\n  author = \t {Oliveira, Mariana and Torgo, Luis},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {360--370},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/oliveira14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/oliveira14.html},\n  abstract = \t {This paper describes a new type of ensembles that aims at improving the predictive performance of these approaches in time series forecasting. Ensembles are recognised as one of the most successful approaches to prediction tasks. Previous theoretical studies of ensembles have shown that one of the key reasons for this performance is diversity among ensemble members. Several methods exist to generate diversity. The key idea of the work we are presenting here is to propose a new form of diversity generation that explores some specific properties of time series prediction tasks. Our hypothesis is that the resulting ensemble members will be better at addressing different dynamic regimes of time series data. Our large set of experiments confirms that the methods we have explored for generating diversity are able to improve the performance of the equivalent ensembles with standard diversity generation procedures.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/oliveira14.pdf",
        "supp": "",
        "pdf_size": 274942,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12263206878760330564&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "LIAAD-INESC TEC / DCC-FCUP; LIAAD-INESC TEC / DCC-FCUP",
        "aff_domain": "inesctec.pt;dcc.fc.up.pt",
        "email": "inesctec.pt;dcc.fc.up.pt",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INESC TEC",
        "aff_unique_dep": "LIAAD",
        "aff_unique_url": "https://www.inesctec.pt",
        "aff_unique_abbr": "INESC TEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "372ec875a5",
        "title": "Interval Insensitive Loss for Ordinal Classification",
        "site": "https://proceedings.mlr.press/v39/antoniuk14.html",
        "author": "Kostiantyn Antoniuk; Vojtech Franc; Vaclav Hlavac",
        "abstract": "We address a problem of learning ordinal classifier from partially annotated examples. We introduce an interval-insensitive loss function to measure discrepancy between predictions of an ordinal classifier and a partial annotation provided in the form of intervals of admissible labels. The proposed interval-insensitive loss is an instance of loss functions previously used for learning of different classification models from partially annotated examples. We propose several convex surrogates of the interval-insensitive loss which can be efficiently optimized by existing solvers. Experiments on standard benchmarks and a real-life application show that learning ordinal classifiers from partially annotated examples is competitive to the so-far used methods learning from the complete annotation.",
        "bibtex": "@InProceedings{pmlr-v39-antoniuk14,\n  title = \t {Interval Insensitive Loss for Ordinal Classification},\n  author = \t {Antoniuk, Kostiantyn and Franc, Vojtech and Hlavac, Vaclav},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {189--204},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/antoniuk14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/antoniuk14.html},\n  abstract = \t {We address a problem of learning ordinal classifier from partially annotated examples. We introduce an interval-insensitive loss function to measure discrepancy between predictions of an ordinal classifier and a partial annotation provided in the form of intervals of admissible labels. The proposed interval-insensitive loss is an instance of loss functions previously used for learning of different classification models from partially annotated examples. We propose several convex surrogates of the interval-insensitive loss which can be efficiently optimized by existing solvers. Experiments on standard benchmarks and a real-life application show that learning ordinal classifiers from partially annotated examples is competitive to the so-far used methods learning from the complete annotation.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/antoniuk14.pdf",
        "supp": "",
        "pdf_size": 770665,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8228496424014762065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Machine Perception, Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Technick\u00b4a 2, 166 27 Prague 6 Czech Republic; Center for Machine Perception, Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Technick\u00b4a 2, 166 27 Prague 6 Czech Republic; Center for Machine Perception, Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Technick\u00b4a 2, 166 27 Prague 6 Czech Republic",
        "aff_domain": "cmp.felk.cvut.cz;cmp.felk.cvut.cz;fel.cvut.cz",
        "email": "cmp.felk.cvut.cz;cmp.felk.cvut.cz;fel.cvut.cz",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Czech Technical University in Prague",
        "aff_unique_dep": "Department of Cybernetics",
        "aff_unique_url": "https://www.cvut.cz",
        "aff_unique_abbr": "CTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Prague",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "b751ce93cd",
        "title": "Learning with Augmented Multi-Instance View",
        "site": "https://proceedings.mlr.press/v39/zhu14.html",
        "author": "Yue Zhu; Jianxin Wu; Yuan Jiang; Zhi-Hua Zhou",
        "abstract": "In this paper, we propose the Augmented Multi-Instance View (AMIV) framework to construct a better model by exploiting augmented information. For example, abstract screening tasks may be difficult because only abstract information is available, whereas the performance can be improved when the abstracts of references listed in the document can be exploited as augmented information. If each abstract is represented as an instance (i.e., a feature vector) x, then with the augmented information, it can be represented as an instance-bag pair (x;B), where B is a bag of instances (i.e., the abstracts of references). Note that if x has a label y, then we assume that there must exist at least one instance in the bag B having the label y. We regard x and B as two views, i.e., a single-instance view augmented with a multi-instance view, and propose the AMIV-lss approach by establishing a latent semantic subspace between the two views. The AMIV framework can be applied when the augmented information is presented as multi-instance bags and to the best of our knowledge, such a learning with augmented multi-instance view problem has not been touched before. Experimental results on twelve TechPaper datasets, five PubMed data sets and a WebPage data set validate the effectiveness of our AMIV-lss approach.",
        "bibtex": "@InProceedings{pmlr-v39-zhu14,\n  title = \t {Learning with Augmented Multi-Instance View},\n  author = \t {Zhu, Yue and Wu, Jianxin and Jiang, Yuan and Zhou, Zhi-Hua},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {234--249},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/zhu14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/zhu14.html},\n  abstract = \t {In this paper, we propose the Augmented Multi-Instance View (AMIV) framework to construct a better model by exploiting augmented information. For example, abstract screening tasks may be difficult because only abstract information is available, whereas the performance can be improved when the abstracts of references listed in the document can be exploited as augmented information. If each abstract is represented as an instance (i.e., a feature vector) x, then with the augmented information, it can be represented as an instance-bag pair (x;B), where B is a bag of instances (i.e., the abstracts of references). Note that if x has a label y, then we assume that there must exist at least one instance in the bag B having the label y. We regard x and B as two views, i.e., a single-instance view augmented with a multi-instance view, and propose the AMIV-lss approach by establishing a latent semantic subspace between the two views. The AMIV framework can be applied when the augmented information is presented as multi-instance bags and to the best of our knowledge, such a learning with augmented multi-instance view problem has not been touched before. Experimental results on twelve TechPaper datasets, five PubMed data sets and a WebPage data set validate the effectiveness of our AMIV-lss approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/zhu14.pdf",
        "supp": "",
        "pdf_size": 583485,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5094982720364378582&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023 + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 210023; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023 + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 210023; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023 + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 210023; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023 + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 210023",
        "aff_domain": "lamda.nju.edu.cn;nju.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;nju.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "http://www.nju.edu.cn;",
        "aff_unique_abbr": "Nanjing U;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "607d703657",
        "title": "Nonlinear Dimensionality Reduction of Data by Deep Distributed Random Samplings",
        "site": "https://proceedings.mlr.press/v39/zhang14.html",
        "author": "Xiao-Lei Zhang",
        "abstract": "Dimensionality reduction is a fundamental problem of machine learning, and has been intensively studied, where classification and clustering are two special cases of dimensionality reduction that reduce high-dimensional data to discrete points. Here we describe a simple multilayer network for dimensionality reduction that each layer of the network is a group of mutually independent k-centers clusterings. We find that the network can be trained successfully layer-by-layer by simply assigning the centers of each clustering by randomly sampled data points from the input. Our results show that the described simple method outperformed 7 well-known dimensionality reduction methods on both very small-scale biomedical data and large-scale image and document data, with less training time than multilayer neural networks on large-scale data.",
        "bibtex": "@InProceedings{pmlr-v39-zhang14,\n  title = \t {Nonlinear Dimensionality Reduction of Data by Deep Distributed Random Samplings},\n  author = \t {Zhang, Xiao-Lei},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {221--233},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/zhang14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/zhang14.html},\n  abstract = \t {Dimensionality reduction is a fundamental problem of machine learning, and has been intensively studied, where classification and clustering are two special cases of dimensionality reduction that reduce high-dimensional data to discrete points. Here we describe a simple multilayer network for dimensionality reduction that each layer of the network is a group of mutually independent k-centers clusterings. We find that the network can be trained successfully layer-by-layer by simply assigning the centers of each clustering by randomly sampled data points from the input. Our results show that the described simple method outperformed 7 well-known dimensionality reduction methods on both very small-scale biomedical data and large-scale image and document data, with less training time than multilayer neural networks on large-scale data.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/zhang14.pdf",
        "supp": "",
        "pdf_size": 747449,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5852573195808741333&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing, China, 100084",
        "aff_domain": "126.com",
        "email": "126.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "d0d65976a5",
        "title": "Online Passive Aggressive Active Learning and Its Applications",
        "site": "https://proceedings.mlr.press/v39/lu14.html",
        "author": "Jing Lu; Peilin Zhao; Steven Hoi",
        "abstract": "We investigate online active learning techniques for classification tasks in data stream mining applications. Unlike traditional learning approaches (either batch or online learning) that often require to request class label of each incoming instance, online active learning queries only a subset of informative incoming instances to update the classification model, which aims to maximize the classification performance using the minimal human labeling effort during the entire online stream data mining tasks. In this paper, we present a new family of algorithms for online active learning called Passive-Aggressive Active (PAA) learning algorithms by adapting the popular Passive-Aggressive algorithms in an online active learning setting. Unlike the conventional Perceptron-based approach that employs only the misclassified instances for updating the model, the proposed PAA learning algorithms not only use the misclassified instances to update the classifier, but also exploit those correctly classified examples yet with low prediction confidence. We theoretically analyze the mistakes bounds of the proposed algorithms and conduct extensive experiments to examine their empirical performance, in which the encouraging results show clear advantages of our algorithms over the baselines.",
        "bibtex": "@InProceedings{pmlr-v39-lu14,\n  title = \t {Online Passive Aggressive Active Learning and Its Applications},\n  author = \t {Lu, Jing and Zhao, Peilin and Hoi, Steven},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {266--282},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/lu14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/lu14.html},\n  abstract = \t {We investigate online active learning techniques for classification tasks in data stream mining applications. Unlike traditional learning approaches (either batch or online learning) that often require to request class label of each incoming instance, online active learning queries only a subset of informative incoming instances to update the classification model, which aims to maximize the classification performance using the minimal human labeling effort during the entire online stream data mining tasks. In this paper, we present a new family of algorithms for online active learning called Passive-Aggressive Active (PAA) learning algorithms by adapting the popular Passive-Aggressive algorithms in an online active learning setting. Unlike the conventional Perceptron-based approach that employs only the misclassified instances for updating the model, the proposed PAA learning algorithms not only use the misclassified instances to update the classifier, but also exploit those correctly classified examples yet with low prediction confidence. We theoretically analyze the mistakes bounds of the proposed algorithms and conduct extensive experiments to examine their empirical performance, in which the encouraging results show clear advantages of our algorithms over the baselines.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/lu14.pdf",
        "supp": "",
        "pdf_size": 308275,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3066626548784777579&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Information Systems, Singapore Management University, Singapore; Institute for Infocomm Research, A*STAR, Singapore; School of Information Systems, Singapore Management University, Singapore",
        "aff_domain": "phdis.smu.edu.sg;i2r.a-star.edu.sg;smu.edu.sg",
        "email": "phdis.smu.edu.sg;i2r.a-star.edu.sg;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Singapore Management University;Institute for Infocomm Research",
        "aff_unique_dep": "School of Information Systems;",
        "aff_unique_url": "https://www.smu.edu.sg;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "SMU;I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "8ac1cda1e2",
        "title": "Online matrix prediction for sparse loss matrices",
        "site": "https://proceedings.mlr.press/v39/moridomi14.html",
        "author": "Ken-ichiro Moridomi; Kohei Hatano; Eiji Takimoto; Koji Tsuda",
        "abstract": "We consider an online matrix prediction problem. The FTRL is a famous method to deal with online prediction task, which makes prediction by minimizing cumulative loss function and regularizer function. There are three popular regularizer functions for matrices, Frobenius norm, quantum relative entropy and log-determinant. We propose a FTRL based algorithm with log-determinant as regularizer and show regret bound of algorithm. Our main contribution is to show that log-determinant regularization is efficient when sparse loss function setting. We also show the optimal performance algorithm for online collaborative filtering problem with log-determinant regularization.",
        "bibtex": "@InProceedings{pmlr-v39-moridomi14,\n  title = \t {Online matrix prediction for sparse loss matrices},\n  author = \t {Moridomi, Ken-ichiro and Hatano, Kohei and Takimoto, Eiji and Tsuda, Koji},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {250--265},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/moridomi14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/moridomi14.html},\n  abstract = \t {We consider an online matrix prediction problem. The FTRL is a famous method to deal with online prediction task, which makes prediction by minimizing cumulative loss function and regularizer function. There are three popular regularizer functions for matrices, Frobenius norm, quantum relative entropy and log-determinant. We propose a FTRL based algorithm with log-determinant as regularizer and show regret bound of algorithm. Our main contribution is to show that log-determinant regularization is efficient when sparse loss function setting. We also show the optimal performance algorithm for online collaborative filtering problem with log-determinant regularization.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/moridomi14.pdf",
        "supp": "",
        "pdf_size": 371919,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12903699083259766531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Informatics, Kyushu University; Department of Informatics, Kyushu University; Department of Informatics, Kyushu University; Department of Computational Biology, Graduate School of Frontier Sciences, The University of Tokyo",
        "aff_domain": "inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;k.u-tokyo.ac.jp",
        "email": "inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "1",
        "aff_unique_norm": ";University of Tokyo",
        "aff_unique_dep": ";Department of Computational Biology",
        "aff_unique_url": ";https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": ";UTokyo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Japan"
    },
    {
        "id": "bec1aa5df6",
        "title": "Ordinal Random Fields for Recommender Systems",
        "site": "https://proceedings.mlr.press/v39/liu14.html",
        "author": "Shaowu Liu; Truyen Tran; Gang Li",
        "abstract": "Recommender Systems heavily rely on numerical preferences, whereas the importance of ordinal preferences has only been recognised in recent works of Ordinal Matrix Factorisation (OMF). Although the OMF can effectively exploit ordinal properties, it captures only the higher-order interactions among users and items, without considering the localised interactions properly. This paper employs Markov Random Fields (MRF) to investigate the localised interactions, and proposes a unified model called Ordinal Random Fields (ORF) to take advantages of both the representational power of the MRF and the ease of modelling ordinal preferences by the OMF. Experimental result on public datasets demonstrates that the proposed ORF model can capture both types of interactions, resulting in improved recommendation accuracy.",
        "bibtex": "@InProceedings{pmlr-v39-liu14,\n  title = \t {Ordinal Random Fields for Recommender Systems},\n  author = \t {Liu, Shaowu and Tran, Truyen and Li, Gang},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {283--298},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/liu14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/liu14.html},\n  abstract = \t {Recommender Systems heavily rely on numerical preferences, whereas the importance of ordinal preferences has only been recognised in recent works of Ordinal Matrix Factorisation (OMF). Although the OMF can effectively exploit ordinal properties, it captures only the higher-order interactions among users and items, without considering the localised interactions properly. This paper employs Markov Random Fields (MRF) to investigate the localised interactions, and proposes a unified model called Ordinal Random Fields (ORF) to take advantages of both the representational power of the MRF and the ease of modelling ordinal preferences by the OMF. Experimental result on public datasets demonstrates that the proposed ORF model can capture both types of interactions, resulting in improved recommendation accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/liu14.pdf",
        "supp": "",
        "pdf_size": 449856,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2449551651385217007&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7615a98d33",
        "title": "Polya-gamma augmentations for factor models",
        "site": "https://proceedings.mlr.press/v39/klami14.html",
        "author": "Arto Klami",
        "abstract": "Bayesian inference for latent factor models, such as principal component and canonical correlation analysis, is easy for Gaussian likelihoods. In particular, full conjugacy makes both Gibbs samplers and mean-field variational approximations straightforward. For other likelihood potentials one needs to either resort to more complex sampling schemes or to specifying dedicated forms of variational lower bounds. Recently, however, it was shown that for specific likelihoods related to the logistic function it is possible to augment the joint density with auxiliary variables following a Polya-Gamma distribution, leading to closed-form updates for binary and over-dispersed count models. In this paper we describe how Gibbs sampling and mean-field variational approximation for various latent factor models can be implemented for these cases, presenting easy-to-implement and efficient inference schemas.",
        "bibtex": "@InProceedings{pmlr-v39-klami14,\n  title = \t {{P}olya-gamma augmentations for factor models},\n  author = \t {Klami, Arto},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {112--128},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/klami14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/klami14.html},\n  abstract = \t {Bayesian inference for latent factor models, such as principal component and canonical correlation analysis, is easy for Gaussian likelihoods. In particular, full conjugacy makes both Gibbs samplers and mean-field variational approximations straightforward. For other likelihood potentials one needs to either resort to more complex sampling schemes or to specifying dedicated forms of variational lower bounds. Recently, however, it was shown that for specific likelihoods related to the logistic function it is possible to augment the joint density with auxiliary variables following a Polya-Gamma distribution, leading to closed-form updates for binary and over-dispersed count models. In this paper we describe how Gibbs sampling and mean-field variational approximation for various latent factor models can be implemented for these cases, presenting easy-to-implement and efficient inference schemas.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/klami14.pdf",
        "supp": "",
        "pdf_size": 325277,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5319263951717729565&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v39/preface.html",
        "author": "Dinh Phung; Hang Li",
        "abstract": "Preface to ACML 2014",
        "bibtex": "@InProceedings{pmlr-v39-preface,\n  title = \t {Preface},\n  author = \t {Phung, Dinh and Li, Hang},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {i--xiv},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/preface.pdf},\n  url = \t {https://proceedings.mlr.press/v39/preface.html},\n  abstract = \t {Preface to ACML 2014}\n}",
        "pdf": "http://proceedings.mlr.press/v39/preface.pdf",
        "supp": "",
        "pdf_size": 135745,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "02e1c42b7b",
        "title": "Pseudo-reward Algorithms for Contextual Bandits with Linear Payoff Functions",
        "site": "https://proceedings.mlr.press/v39/chou14.html",
        "author": "Ku-Chun Chou; Hsuan-Tien Lin; Chao-Kai Chiang; Chi-Jen Lu",
        "abstract": "We study the contextual bandit problem with linear payoff functions, which is a generalization of the traditional multi-armed bandit problem. In the contextual bandit problem, the learner needs to iteratively select an action based on an observed context, and receives a linear score on only the selected action as the reward feedback. Motivated by the observation that better performance is achievable if the other rewards on the non-selected actions can also be revealed to the learner, we propose a new framework that feeds the learner with pseudo-rewards, which are estimates of the rewards on the non-selected actions. We argue that the pseudo-rewards should better contain over-estimates of the true rewards, and propose a forgetting mechanism to decrease the negative influence of the over-estimation in the long run. Then, we couple the two key ideas above with the linear upper confidence bound (LinUCB) algorithm to design a novel algorithm called linear pseudo-reward upper confidence bound (LinPRUCB). We prove that LinPRUCB shares the same order of regret bound to LinUCB, while enjoying the practical observation of faster reward-gathering in the earlier iterations. Experiments on artificial and real-world data sets justify that LinPRUCB is competitive to and sometimes even better than LinUCB. Furthermore, we couple LinPRUCB with a special parameter to formalize a new algorithm that yields faster computation in updating the internal models while keeping the promising practical performance. The two properties match the real-world needs of the contextual bandit problem and make the new algorithm a favorable choice in practice.",
        "bibtex": "@InProceedings{pmlr-v39-chou14,\n  title = \t {Pseudo-reward Algorithms for Contextual Bandits with Linear Payoff Functions},\n  author = \t {Chou, Ku-Chun and Lin, Hsuan-Tien and Chiang, Chao-Kai and Lu, Chi-Jen},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {344--359},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/chou14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/chou14.html},\n  abstract = \t {We study the contextual bandit problem with linear payoff functions, which is a generalization of the traditional multi-armed bandit problem. In the contextual bandit problem, the learner needs to iteratively select an action based on an observed context, and receives a linear score on only the selected action as the reward feedback. Motivated by the observation that better performance is achievable if the other rewards on the non-selected actions can also be revealed to the learner, we propose a new framework that feeds the learner with pseudo-rewards, which are estimates of the rewards on the non-selected actions. We argue that the pseudo-rewards should better contain over-estimates of the true rewards, and propose a forgetting mechanism to decrease the negative influence of the over-estimation in the long run. Then, we couple the two key ideas above with the linear upper confidence bound (LinUCB) algorithm to design a novel algorithm called linear pseudo-reward upper confidence bound (LinPRUCB). We prove that LinPRUCB shares the same order of regret bound to LinUCB, while enjoying the practical observation of faster reward-gathering in the earlier iterations. Experiments on artificial and real-world data sets justify that LinPRUCB is competitive to and sometimes even better than LinUCB. Furthermore, we couple LinPRUCB with a special parameter to formalize a new algorithm that yields faster computation in updating the internal models while keeping the promising practical performance. The two properties match the real-world needs of the contextual bandit problem and make the new algorithm a favorable choice in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/chou14.pdf",
        "supp": "",
        "pdf_size": 405712,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3511705346654285200&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University; Department of Mathematics and Information Technology, University of Leoben; Department of Computer Science and Information Engineering, National Taiwan University; Institute of Information Science, Academia Sinica",
        "aff_domain": "csie.ntu.edu.tw;unileoben.ac.at;csie.ntu.edu.tw;iis.sinica.edu.tw",
        "email": "csie.ntu.edu.tw;unileoben.ac.at;csie.ntu.edu.tw;iis.sinica.edu.tw",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "National Taiwan University;University of Leoben;Academia Sinica",
        "aff_unique_dep": "Department of Computer Science and Information Engineering;Department of Mathematics and Information Technology;Institute of Information Science",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.unileoben.ac.at;https://www.sinica.edu.tw",
        "aff_unique_abbr": "NTU;;AS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;Austria"
    },
    {
        "id": "655106af44",
        "title": "Quasi Newton Temporal Difference Learning",
        "site": "https://proceedings.mlr.press/v39/givchi14.html",
        "author": "Arash Givchi; Maziar Palhang",
        "abstract": "Fast convergent and computationally inexpensive policy evaluation is an essential part of reinforcement learning algorithms based on policy iteration. Algorithms such as LSTD, LSPE, FPKF and NTD, have faster convergence rate but they are computationally slow. On the other hand, there are algorithms that are computationally fast but with slower convergence rate, among them are TD, RG, GTD2 and TDC. This paper presents a regularized Quasi Newton Temporal Difference learning algorithm which uses the second-order information while maintaining a fast convergence rate. In simple language, we combine the idea of TD learning with Quasi Newton algorithm SGD-QN. We explore the development of QNTD algorithm and discuss its convergence properties. We support our ideas with empirical results on 4 standard benchmarks in reinforcement learning literature with 2 small problems, Random Walk and Boyan chain and 2 bigger problems, cart-pole and linked-pole balancing. Empirical studies show that QNTD speeds up convergence and provides better accuracy in comparison to the conventional TD.",
        "bibtex": "@InProceedings{pmlr-v39-givchi14,\n  title = \t {Quasi Newton Temporal Difference Learning},\n  author = \t {Givchi, Arash and Palhang, Maziar},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {159--172},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/givchi14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/givchi14.html},\n  abstract = \t {Fast convergent and computationally inexpensive policy evaluation is an essential part of reinforcement learning algorithms based on policy iteration. Algorithms such as LSTD, LSPE, FPKF and NTD, have faster convergence rate but they are computationally slow. On the other hand, there are algorithms that are computationally fast but with slower convergence rate, among them are TD, RG, GTD2 and TDC. This paper presents a regularized Quasi Newton Temporal Difference learning algorithm which uses the second-order information while maintaining a fast convergence rate. In simple language, we combine the idea of TD learning with Quasi Newton algorithm SGD-QN. We explore the development of QNTD algorithm and discuss its convergence properties. We support our ideas with empirical results on 4 standard benchmarks in reinforcement learning literature with 2 small problems, Random Walk and Boyan chain and 2 bigger problems, cart-pole and linked-pole balancing. Empirical studies show that QNTD speeds up convergence and provides better accuracy in comparison to the conventional TD.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/givchi14.pdf",
        "supp": "",
        "pdf_size": 474750,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12654472078398523822&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Artificial Intelligence Laboratory, Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran; Artificial Intelligence Laboratory, Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran",
        "aff_domain": "ec.iut.ac.ir;cc.iut.ac.ir",
        "email": "ec.iut.ac.ir;cc.iut.ac.ir",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Isfahan University of Technology",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Isfahan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "f868e6b950",
        "title": "Reduction from Cost-Sensitive Multiclass Classification to One-versus-One Binary Classification",
        "site": "https://proceedings.mlr.press/v39/lin14.html",
        "author": "Hsuan-Tien Lin",
        "abstract": "Many real-world applications require varying costs for different types of mis-classification errors. Such a cost-sensitive classification setup can be very different from the regular classification one, especially in the multiclass case. Thus, traditional meta-algorithms for regular multiclass classification, such as the popular one-versus-one approach, may not always work well under the cost-sensitive classification setup. In this paper, we extend the one-versus-one approach to the field of cost-sensitive classification. The extension is derived using a rigorous mathematical tool called the cost-transformation technique, and takes the original one-versus-one as a special case. Experimental results demonstrate that the proposed approach can achieve better performance in many cost-sensitive classification scenarios when compared with the original one-versus-one as well as existing cost-sensitive classification algorithms.",
        "bibtex": "@InProceedings{pmlr-v39-lin14,\n  title = \t {Reduction from Cost-Sensitive Multiclass Classification to One-versus-One Binary Classification},\n  author = \t {Lin, Hsuan-Tien},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {371--386},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/lin14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/lin14.html},\n  abstract = \t {Many real-world applications require varying costs for different types of mis-classification errors. Such a cost-sensitive classification setup can be very different from the regular classification one, especially in the multiclass case. Thus, traditional meta-algorithms for regular multiclass classification, such as the popular one-versus-one approach, may not always work well under the cost-sensitive classification setup. In this paper, we extend the one-versus-one approach to the field of cost-sensitive classification. The extension is derived using a rigorous mathematical tool called the cost-transformation technique, and takes the original one-versus-one as a special case. Experimental results demonstrate that the proposed approach can achieve better performance in many cost-sensitive classification scenarios when compared with the original one-versus-one as well as existing cost-sensitive classification algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/lin14.pdf",
        "supp": "",
        "pdf_size": 436291,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9643735258606394652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University",
        "aff_domain": "csie.ntu.edu.tw",
        "email": "csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "263336b177",
        "title": "Reinforcement learning with value advice",
        "site": "https://proceedings.mlr.press/v39/daswani14.html",
        "author": "Mayank Daswani; Peter Sunehag; Marcus Hutter",
        "abstract": "The problem we consider in this paper is reinforcement learning with value advice. In this setting, the agent is given limited access to an oracle that can tell it the expected return (value) of any state-action pair with respect to the optimal policy. The agent must use this value to learn an explicit policy that performs well in the environment. We provide an algorithm called RLAdvice, based on the imitation learning algorithm DAgger. We illustrate the effectiveness of this method in the Arcade Learning Environment on three different games, using value estimates from UCT as advice.",
        "bibtex": "@InProceedings{pmlr-v39-daswani14,\n  title = \t {Reinforcement learning with value advice},\n  author = \t {Daswani, Mayank and Sunehag, Peter and Hutter, Marcus},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {299--314},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/daswani14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/daswani14.html},\n  abstract = \t {The problem we consider in this paper is reinforcement learning with value advice. In this setting, the agent is given limited access to an oracle that can tell it the expected return (value) of any state-action pair with respect to the optimal policy. The agent must use this value to learn an explicit policy that performs well in the environment. We provide an algorithm called RLAdvice, based on the imitation learning algorithm DAgger. We illustrate the effectiveness of this method in the Arcade Learning Environment on three different games, using value estimates from UCT as advice.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/daswani14.pdf",
        "supp": "",
        "pdf_size": 341010,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8515504817054910677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Research School of Computer Science, Australian National University, Canberra, ACT-2601, Australia; Research School of Computer Science, Australian National University, Canberra, ACT-2601, Australia; Research School of Computer Science, Australian National University, Canberra, ACT-2601, Australia",
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "Research School of Computer Science",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "b80afd35f4",
        "title": "Sample Distillation for Object Detection and Image Classification",
        "site": "https://proceedings.mlr.press/v39/canevet14b.html",
        "author": "Olivier Canevet; Leonidas Lefakis; Francois Fleuret",
        "abstract": "We propose a novel approach to efficiently select informative samples for large-scale learning. Instead of directly feeding a learning algorithm with a very large amount of samples, as it is usually done to reach state-of-the-art performance, we have developed a \"distillation\" procedure to recursively reduce the size of an initial training set using a criterion that ensures the maximization of the information content of the selected sub-set. We demonstrate the performance of this procedure for two different computer vision problems. First, we show that distillation can be used to improve the traditional bootstrapping approach to object detection. Second, we apply distillation to a classification problem with artificial distortions. We show that in both cases, using the result of a distillation process instead of a random sub-set taken uniformly in the original sample set improves performance significantly.",
        "bibtex": "@InProceedings{pmlr-v39-canevet14b,\n  title = \t {Sample Distillation for Object Detection and Image Classification},\n  author = \t {Canevet, Olivier and Lefakis, Leonidas and Fleuret, Francois},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {64--79},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/canevet14b.pdf},\n  url = \t {https://proceedings.mlr.press/v39/canevet14b.html},\n  abstract = \t {We propose a novel approach to efficiently select informative samples for large-scale learning. Instead of directly feeding a learning algorithm with a very large amount of samples, as it is usually done to reach state-of-the-art performance, we have developed a \"distillation\" procedure to recursively reduce the size of an initial training set using a criterion that ensures the maximization of the information content of the selected sub-set. We demonstrate the performance of this procedure for two different computer vision problems. First, we show that distillation can be used to improve the traditional bootstrapping approach to object detection. Second, we apply distillation to a classification problem with artificial distortions. We show that in both cases, using the result of a distillation process instead of a random sub-set taken uniformly in the original sample set improves performance significantly.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/canevet14b.pdf",
        "supp": "",
        "pdf_size": 416248,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4687637750359870439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Vision and Learning group, Idiap Research Institute, Martigny, Switzerland + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland; Computer Vision and Learning group, Idiap Research Institute, Martigny, Switzerland + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland; Computer Vision and Learning group, Idiap Research Institute, Martigny, Switzerland + \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland",
        "aff_domain": "idiap.ch;idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Idiap Research Institute;EPFL",
        "aff_unique_dep": "Computer Vision and Learning group;",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch",
        "aff_unique_abbr": ";EPFL",
        "aff_campus_unique_index": "0+1;0+1;0+1",
        "aff_campus_unique": "Martigny;Lausanne",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "92d0d27e0c",
        "title": "Sparse binary zero-sum games",
        "site": "https://proceedings.mlr.press/v39/auger14.html",
        "author": "David Auger; Jianlin Liu; Sylkvie Ruette; David Saint-Pierre; Oliver Teytaud",
        "abstract": "Solving zero-sum matrix games is polynomial, because it boils down to linear programming. The approximate solving is sublinear by randomized algorithms on machines with random access memory. Algorithms working separately and independently on columns and rows have been proposed, with the same performance; these versions are compliant with matrix games with stochastic reward. [1] has proposed a new version, empirically performing better on sparse problems, i.e. cases in which the Nash equilibrium has small support. In this paper, we propose a variant, similar to their work, also dedicated to sparse problems, with provably better bounds than existing methods. We then experiment the method on a card game.",
        "bibtex": "@InProceedings{pmlr-v39-auger14,\n  title = \t {Sparse binary zero-sum games},\n  author = \t {Auger, David and Liu, Jianlin and Ruette, Sylkvie and Saint-Pierre, David and Teytaud, Oliver},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {173--188},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/auger14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/auger14.html},\n  abstract = \t {Solving zero-sum matrix games is polynomial, because it boils down to linear programming. The approximate solving is sublinear by randomized algorithms on machines with random access memory. Algorithms working separately and independently on columns and rows have been proposed, with the same performance; these versions are compliant with matrix games with stochastic reward. [1] has proposed a new version, empirically performing better on sparse problems, i.e. cases in which the Nash equilibrium has small support. In this paper, we propose a variant, similar to their work, also dedicated to sparse problems, with provably better bounds than existing methods. We then experiment the method on a card game.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/auger14.pdf",
        "supp": "",
        "pdf_size": 373323,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2464127452717232323&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "AlCAAP, Laboratoire PRiSM, B\u02c6 at. Descartes, Universit\u00b4 e de Versailles Saint-Quentin-en-Yvelines, 45 avenue des \u00b4Etats-Unis, F-78035 Versailles Cedex, France; TAO, Lri, UMR CNRS 8623, Univ. Paris-Sud, F-91405 Orsay, France; Laboratoire de Math\u00b4 ematiques, CNRS UMR 8628, B\u02c6 at. 425, Univ. Paris-Sud, F-91405 Orsay, France; Monte\ufb01ore Institute, Universit\u00b4 e de Li` ege, Belgium; TAO, Lri, UMR CNRS 8623, Univ. Paris-Sud, F-91405 Orsay, France+OASE Lab, National Univ. of Tainan+AILab, National Dong Hwa Univ., Hualien, Taiwan",
        "aff_domain": "prism.uvsq.fr;inria.fr;math.u-psud.fr;gmail.com;inria.fr",
        "email": "prism.uvsq.fr;inria.fr;math.u-psud.fr;gmail.com;inria.fr",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;1+4+5",
        "aff_unique_norm": "Universit\u00b4 e de Versailles Saint-Quentin-en-Yvelines;University of Paris-Sud;Universite Paris-Sud;Universit\u00e9 de Li\u00e8ge;National University of Tainan;National Dong Hwa University",
        "aff_unique_dep": "Laboratoire PRiSM;TAO, Lri, UMR CNRS 8623;Laboratoire de Mathematiques;Monte\ufb01ore Institute;OASE Lab;AILab",
        "aff_unique_url": ";https://www.universite-paris-sud.fr;https://www.universite-paris-sud.fr;https://www.ulg.ac.be;https://www.nutn.edu.tw;https://www.ndhu.edu.tw",
        "aff_unique_abbr": ";Paris-Sud;Paris-Sud;ULi\u00e8ge;NUTN;NDHU",
        "aff_campus_unique_index": "0;1;1;1+3+3",
        "aff_campus_unique": "Versailles;Orsay;;Taiwan",
        "aff_country_unique_index": "0;0;0;1;0+2+2",
        "aff_country_unique": "France;Belgium;China"
    },
    {
        "id": "02f498fcf8",
        "title": "Sparsity on Statistical Simplexes and Diversity in Social Ranking",
        "site": "https://proceedings.mlr.press/v39/sun14.html",
        "author": "Ke Sun; Hisham Mohamed; Stephane Marchand-Maillet",
        "abstract": "Sparsity in \\Re^m has been widely explored in machine learning. We study sparsity on a statistical simplex consisting of all categorical distributions. This is different from the case in \\Re^m because such a simplex is a Riemannian manifold, a curved space. A learner with sparse constraints should be likely to fall to its low-dimensional boundaries. We present a novel analysis on the statistical simplex as a manifold with boundary. The main contribution is an explicit view of the learning dynamics in between high-dimensional models in the interior of the simplex and low-dimensional models on its boundaries. We prove the differentiability of the cost function, the natural gradient with respect to the Riemannian structure, and convexity around the singular regions. We uncover an interesting relationship with L_1 regularization. We apply the proposed technique to social network analysis. Given a directed graph, the task is to rank a subset of influencer nodes. Here, sparsity means that the top-ranked nodes should present diversity in the sense of minimizing influence overlap. We present a ranking algorithm based on the natural gradient. It can scale up to graph datasets with millions of nodes. On real large networks, the top-ranked nodes are the most informative among several commonly-used techniques.",
        "bibtex": "@InProceedings{pmlr-v39-sun14,\n  title = \t {Sparsity on Statistical Simplexes and Diversity in Social Ranking},\n  author = \t {Sun, Ke and Mohamed, Hisham and Marchand-Maillet, Stephane},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {16--31},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/sun14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/sun14.html},\n  abstract = \t {Sparsity in \\Re^m has been widely explored in machine learning. We study sparsity on a statistical simplex consisting of all categorical distributions. This is different from the case in \\Re^m because such a simplex is a Riemannian manifold, a curved space. A learner with sparse constraints should be likely to fall to its low-dimensional boundaries. We present a novel analysis on the statistical simplex as a manifold with boundary. The main contribution is an explicit view of the learning dynamics in between high-dimensional models in the interior of the simplex and low-dimensional models on its boundaries. We prove the differentiability of the cost function, the natural gradient with respect to the Riemannian structure, and convexity around the singular regions. We uncover an interesting relationship with L_1 regularization. We apply the proposed technique to social network analysis. Given a directed graph, the task is to rank a subset of influencer nodes. Here, sparsity means that the top-ranked nodes should present diversity in the sense of minimizing influence overlap. We present a ranking algorithm based on the natural gradient. It can scale up to graph datasets with millions of nodes. On real large networks, the top-ranked nodes are the most informative among several commonly-used techniques.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/sun14.pdf",
        "supp": "",
        "pdf_size": 591658,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7198189513070238504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva; Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva; Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva",
        "aff_domain": "unige.ch;unige.ch;unige.ch",
        "email": "unige.ch;unige.ch;unige.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Geneva",
        "aff_unique_dep": "Computer Vision and Multimedia Laboratory",
        "aff_unique_url": "https://www.unige.ch",
        "aff_unique_abbr": "UniGE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "815a9807ae",
        "title": "Structured Denoising Autoencoder for Fault Detection and Analysis",
        "site": "https://proceedings.mlr.press/v39/tagawa14.html",
        "author": "Takaaki Tagawa; Yukihiro Tadokoro; Takehisa Yairi",
        "abstract": "This paper proposes a new fault detection and analysis approach which can leverage incomplete prior information. Conventional data-driven approaches suffer from the problem of overfitting and result in high rates of false positives, and model-driven approaches suffer from a lack of specific information about complex systems. We overcome these problems by modifying the denoising autoencoder (DA), a data-driven method, to form a new approach, called the structured denoising autoencoder (SDA), which can utilize incomplete prior information. The SDA does not require specific information and can perform well without overfitting. In particular, an empirical analysis with synthetic data revealed that the SDA performs better than the DA even when there is partially incorrect or abstract information. An evaluation using real data from moving cars also showed that the SDA with incomplete knowledge outperformed conventional methods. Surprisingly, the SDA results were better even though the parameters of the conventional methods were tuned using faulty data, which are normally unknown. In addition, the SDA fault analysis was able to extract the true causes of the changes within the faulty data; the other methods were unable to do this. Thus, only our proposed method can explain why the faults occurred.",
        "bibtex": "@InProceedings{pmlr-v39-tagawa14,\n  title = \t {Structured Denoising Autoencoder for Fault Detection and Analysis},\n  author = \t {Tagawa, Takaaki and Tadokoro, Yukihiro and Yairi, Takehisa},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {96--111},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/tagawa14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/tagawa14.html},\n  abstract = \t {This paper proposes a new fault detection and analysis approach which can leverage incomplete prior information. Conventional data-driven approaches suffer from the problem of overfitting and result in high rates of false positives, and model-driven approaches suffer from a lack of specific information about complex systems. We overcome these problems by modifying the denoising autoencoder (DA), a data-driven method, to form a new approach, called the structured denoising autoencoder (SDA), which can utilize incomplete prior information. The SDA does not require specific information and can perform well without overfitting. In particular, an empirical analysis with synthetic data revealed that the SDA performs better than the DA even when there is partially incorrect or abstract information. An evaluation using real data from moving cars also showed that the SDA with incomplete knowledge outperformed conventional methods. Surprisingly, the SDA results were better even though the parameters of the conventional methods were tuned using faulty data, which are normally unknown. In addition, the SDA fault analysis was able to extract the true causes of the changes within the faulty data; the other methods were unable to do this. Thus, only our proposed method can explain why the faults occurred.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/tagawa14.pdf",
        "supp": "",
        "pdf_size": 1228852,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15350549033832143991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "TOYOTA CENTRAL R&D LABS., INC., Japan; TOYOTA CENTRAL R&D LABS., INC., Japan; RCAST, University of Tokyo, Japan",
        "aff_domain": "mosk.tytlabs.co.jp;mosk.tytlabs.co.jp;space.rcast.u-tokyo.ac.jp",
        "email": "mosk.tytlabs.co.jp;mosk.tytlabs.co.jp;space.rcast.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Toyota Central Research & Development Laboratories, Inc.;University of Tokyo",
        "aff_unique_dep": ";RCAST",
        "aff_unique_url": "https://www.toyota-global.com/company/research_development/;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Toyota CRDL;UTokyo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "96d4ede84b",
        "title": "Support vector machines with indefinite kernels",
        "site": "https://proceedings.mlr.press/v39/alabdulmohsin14.html",
        "author": "Ibrahim Alabdulmohsin; Xin Gao; Xiangliang Zhang Zhang",
        "abstract": "Training support vector machines (SVM) with indefinite kernels has recently attracted attention in the machine learning community. This is partly due to the fact that many similarity functions that arise in practice are not symmetric positive semidefinite, i.e. the Mercer condition is not satisfied, or the Mercer condition is difficult to verify. Previous work on training SVM with indefinite kernels has generally fallen into three categories: (1) positive semidefinite kernel approximation, (2) non-convex optimization, and (3) learning in Krein spaces. All approaches are not fully satisfactory. They have either introduced sources of inconsistency in handling training and test examples using kernel approximation, settled for approximate local minimum solutions using non-convex optimization, or produced non-sparse solutions. In this paper, we establish both theoretically and experimentally that the 1-norm SVM, proposed more than 10 years ago for embedded feature selection, is a better solution for extending SVM to indefinite kernels. More specifically, 1-norm SVM can be interpreted as a structural risk minimization method that seeks a decision boundary with large similarity margin in the original space. It uses a linear programming formulation that remains convex even if the kernel matrix is indefinite, and hence can always be solved quite efficiently. Also, it uses the indefinite similarity function (or distance) directly without any transformation, and, hence, it always treats both training and test examples consistently. Finally, it achieves the highest accuracy among all methods that train SVM with indefinite kernels with a statistically significant evidence while also retaining sparsity of the support vector set.",
        "bibtex": "@InProceedings{pmlr-v39-alabdulmohsin14,\n  title = \t {Support vector machines with indefinite kernels},\n  author = \t {Alabdulmohsin, Ibrahim and Gao, Xin and Zhang, Xiangliang Zhang},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {32--47},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/alabdulmohsin14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/alabdulmohsin14.html},\n  abstract = \t {Training support vector machines (SVM) with indefinite kernels has recently attracted attention in the machine learning community. This is partly due to the fact that many similarity functions that arise in practice are not symmetric positive semidefinite, i.e. the Mercer condition is not satisfied, or the Mercer condition is difficult to verify. Previous work on training SVM with indefinite kernels has generally fallen into three categories: (1) positive semidefinite kernel approximation, (2) non-convex optimization, and (3) learning in Krein spaces. All approaches are not fully satisfactory. They have either introduced sources of inconsistency in handling training and test examples using kernel approximation, settled for approximate local minimum solutions using non-convex optimization, or produced non-sparse solutions. In this paper, we establish both theoretically and experimentally that the 1-norm SVM, proposed more than 10 years ago for embedded feature selection, is a better solution for extending SVM to indefinite kernels. More specifically, 1-norm SVM can be interpreted as a structural risk minimization method that seeks a decision boundary with large similarity margin in the original space. It uses a linear programming formulation that remains convex even if the kernel matrix is indefinite, and hence can always be solved quite efficiently. Also, it uses the indefinite similarity function (or distance) directly without any transformation, and, hence, it always treats both training and test examples consistently. Finally, it achieves the highest accuracy among all methods that train SVM with indefinite kernels with a statistically significant evidence while also retaining sparsity of the support vector set.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/alabdulmohsin14.pdf",
        "supp": "",
        "pdf_size": 494568,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11512011956754455804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer, Electrical, and Mathematical Sciences & Engineering (CEMSE) Division, King Abdullah University of Science & Technology (KAUST), Thuwal 23955, Saudi Arabia; Computer, Electrical, and Mathematical Sciences & Engineering (CEMSE) Division, King Abdullah University of Science & Technology (KAUST), Thuwal 23955, Saudi Arabia; Computer, Electrical, and Mathematical Sciences & Engineering (CEMSE) Division, King Abdullah University of Science & Technology (KAUST), Thuwal 23955, Saudi Arabia",
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "email": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "King Abdullah University of Science & Technology",
        "aff_unique_dep": "CEMSE Division",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Thuwal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "id": "6d89ca451f",
        "title": "Theoretical Analyses on Ensemble and Multiple Kernel Regressors",
        "site": "https://proceedings.mlr.press/v39/tanaka14.html",
        "author": "Akira Tanaka; Ichigaku Takigawa; Hideyuki Imai; Mineichi Kudo",
        "abstract": "For the last few decades, a combination of different learning machines so-called ensemble learning, including learning with multiple kernels, has attracted much attention in the field of machine learning. Although its efficacy was revealed numerically in many works, its theoretical grounds are not investigated sufficiently. In this paper, we discuss regression problems with a class of kernels whose corresponding reproducing kernel Hilbert spaces have a common subspace with an invariant metric and prove that the ensemble kernel regressor (the mean of kernel regressors with those kernels) gives a better learning result than the multiple kernel regressor (the kernel regressor with the sum of those kernels) in terms of the generalization ability of a model space.",
        "bibtex": "@InProceedings{pmlr-v39-tanaka14,\n  title = \t {Theoretical Analyses on Ensemble and Multiple Kernel Regressors},\n  author = \t {Tanaka, Akira and Takigawa, Ichigaku and Imai, Hideyuki and Kudo, Mineichi},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {1--15},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/tanaka14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/tanaka14.html},\n  abstract = \t {For the last few decades, a combination of different learning machines so-called ensemble learning, including learning with multiple kernels, has attracted much attention in the field of machine learning. Although its efficacy was revealed numerically in many works, its theoretical grounds are not investigated sufficiently. In this paper, we discuss regression problems with a class of kernels whose corresponding reproducing kernel Hilbert spaces have a common subspace with an invariant metric and prove that the ensemble kernel regressor (the mean of kernel regressors with those kernels) gives a better learning result than the multiple kernel regressor (the kernel regressor with the sum of those kernels) in terms of the generalization ability of a model space.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/tanaka14.pdf",
        "supp": "",
        "pdf_size": 328606,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3846045874349055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "128b060812",
        "title": "Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo",
        "site": "https://proceedings.mlr.press/v39/xiong14.html",
        "author": "Hanchen Xiong; Sandor Szedmak; Justus Piater",
        "abstract": "Along with the emergence of algorithms such as persistent contrastive divergence (PCD), tempered transition and parallel tempering, the past decade has witnessed a revival of learning undirected graphical models (UGMs) with sampling-based approximations. In this paper, based upon the analogy between Robbins-Monro\u2019s stochastic approximation procedure and sequential Monte Carlo (SMC), we analyze the strengths and limitations of state-of-the-art learning algorithms from an SMC point of view. Moreover, we apply the rationale further in sampling at each iteration, and propose to learn UGMs using persistent sequential Monte Carlo (PSMC). The whole learning procedure is based on the samples from a long, persistent sequence of distributions which are actively constructed. Compared to the above-mentioned algorithms, one critical strength of PSMC- based learning is that it can explore the sampling space more effectively. In particular, it is robust when learning rates are large or model distributions are high-dimensional and thus multi-modal, which often causes other algorithms to deteriorate. We tested PSMC learning, also with other related methods, on carefully-designed experiments with both synthetic and real-word data, and our empirical results demonstrate that PSMC compares favorably with the state of the art.",
        "bibtex": "@InProceedings{pmlr-v39-xiong14,\n  title = \t {Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo},\n  author = \t {Xiong, Hanchen and Szedmak, Sandor and Piater, Justus},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {205--220},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/xiong14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/xiong14.html},\n  abstract = \t {Along with the emergence of algorithms such as persistent contrastive divergence (PCD), tempered transition and parallel tempering, the past decade has witnessed a revival of learning undirected graphical models (UGMs) with sampling-based approximations. In this paper, based upon the analogy between Robbins-Monro\u2019s stochastic approximation procedure and sequential Monte Carlo (SMC), we analyze the strengths and limitations of state-of-the-art learning algorithms from an SMC point of view. Moreover, we apply the rationale further in sampling at each iteration, and propose to learn UGMs using persistent sequential Monte Carlo (PSMC). The whole learning procedure is based on the samples from a long, persistent sequence of distributions which are actively constructed. Compared to the above-mentioned algorithms, one critical strength of PSMC- based learning is that it can explore the sampling space more effectively. In particular, it is robust when learning rates are large or model distributions are high-dimensional and thus multi-modal, which often causes other algorithms to deteriorate. We tested PSMC learning, also with other related methods, on carefully-designed experiments with both synthetic and real-word data, and our empirical results demonstrate that PSMC compares favorably with the state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/xiong14.pdf",
        "supp": "",
        "pdf_size": 608240,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hjciDj83pz0J:scholar.google.com/&scioq=Towards+Maximum+Likelihood:+Learning+Undirected+Graphical+Models+using+Persistent+Sequential+Monte+Carlo&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff": "Institute of Computer Science, University of Innsbruck; Institute of Computer Science, University of Innsbruck; Institute of Computer Science, University of Innsbruck",
        "aff_domain": "UIBK.AC.AT;UIBK.AC.AT;UIBK.AC.AT",
        "email": "UIBK.AC.AT;UIBK.AC.AT;UIBK.AC.AT",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Innsbruck",
        "aff_unique_dep": "Institute of Computer Science",
        "aff_unique_url": "https://www.uibk.ac.at",
        "aff_unique_abbr": "UIBK",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Innsbruck",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "a4cb05fed2",
        "title": "Variational Gaussian Inference for Bilinear Models of Count Data",
        "site": "https://proceedings.mlr.press/v39/ko14.html",
        "author": "Young-Jun Ko; Mohammad Khan",
        "abstract": "Bilinear models of count data with Poisson distribution are popular in applications such as matrix factorization for recommendation systems, modeling of receptive fields of sensory neurons, and modeling of neural-spike trains. Bayesian inference in such models remains challenging due to the product term of two Gaussian random vectors. In this paper, we propose new algorithms for such models based on variational Gaussian (VG) inference. We make two contributions. First, we show that the VG lower bound for these models, previously known to be intractable, is available in closed form under certain non-trivial constraints on the form of the posterior. Second, we show that the lower bound is bi- concave and can be efficiently optimized for mean-field approximations. We also show that bi-concavity generalizes to the larger family of log-concave likelihoods that subsume the Poisson distribution. We present new inference algorithms based on these results and demonstrate better performance on real-world problems at the cost of a modest increase in computation. Our contributions in this paper, therefore, provide more choices for Bayesian inference in terms of a speed-vs-accuracy tradeoff.",
        "bibtex": "@InProceedings{pmlr-v39-ko14,\n  title = \t {Variational Gaussian Inference for Bilinear Models of Count Data},\n  author = \t {Ko, Young-Jun and Khan, Mohammad},\n  booktitle = \t {Proceedings of the Sixth Asian Conference on Machine Learning},\n  pages = \t {330--343},\n  year = \t {2015},\n  editor = \t {Phung, Dinh and Li, Hang},\n  volume = \t {39},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Nha Trang City, Vietnam},\n  month = \t {26--28 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v39/ko14.pdf},\n  url = \t {https://proceedings.mlr.press/v39/ko14.html},\n  abstract = \t {Bilinear models of count data with Poisson distribution are popular in applications such as matrix factorization for recommendation systems, modeling of receptive fields of sensory neurons, and modeling of neural-spike trains. Bayesian inference in such models remains challenging due to the product term of two Gaussian random vectors. In this paper, we propose new algorithms for such models based on variational Gaussian (VG) inference. We make two contributions. First, we show that the VG lower bound for these models, previously known to be intractable, is available in closed form under certain non-trivial constraints on the form of the posterior. Second, we show that the lower bound is bi- concave and can be efficiently optimized for mean-field approximations. We also show that bi-concavity generalizes to the larger family of log-concave likelihoods that subsume the Poisson distribution. We present new inference algorithms based on these results and demonstrate better performance on real-world problems at the cost of a modest increase in computation. Our contributions in this paper, therefore, provide more choices for Bayesian inference in terms of a speed-vs-accuracy tradeoff.}\n}",
        "pdf": "http://proceedings.mlr.press/v39/ko14.pdf",
        "supp": "",
        "pdf_size": 385702,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5708453837632938573&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland; \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    }
]