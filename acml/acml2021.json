[
    {
        "title": "$K^2$-GNN: Multiple Users\u2019 Comments Integration with Probabilistic K-Hop Knowledge Graph Neural Networks",
        "site": "https://proceedings.mlr.press/v157/zhan21b.html",
        "author": "Huixin Zhan; Kun Zhang; Chenyi Hu; Victor Sheng",
        "abstract": "Integrating multiple comments into a concise statement for any online products or web services requires a non-trivial understanding of the input. Recently, graph neural networks (GNN) has been successfully applied to learn from highly-structured graph representations to mitigate the relationship between entities, such as co-references. However, current inter-sentence relation extraction cannot leverage discrete reasoning chains over multiple comments. To address this issue, in this paper, we propose a probabilistic $K$-hop knowledge graph (KKG) to extend existing knowledge graphs with inferred relations via discrete intra-sentence and inter-sentence reasoning chains. KKG associates each inferred relation with a confidence value through Bayesian inference. We further answer how a knowledge graph with inferred relations can help the multiple comments integration through integrating KKG with GNN ($\\text{K}^2$-GNN). Our extensive experimental results show that our $\\text{K}^2$-GNN outperforms all baseline graph models on multiple comments integration.",
        "bibtex": "@InProceedings{pmlr-v157-zhan21b,\n  title = \t {$K^2$-GNN: Multiple Users\u2019 Comments Integration with Probabilistic K-Hop Knowledge Graph Neural Networks},\n  author =       {Zhan, Huixin and Zhang, Kun and Hu, Chenyi and Sheng, Victor},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1477--1492},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhan21b/zhan21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhan21b.html},\n  abstract = \t {Integrating multiple comments into a concise statement for any online products or web services requires a non-trivial understanding of the input. Recently, graph neural networks (GNN) has been successfully applied to learn from highly-structured graph representations to mitigate the relationship between entities, such as co-references. However, current inter-sentence relation extraction cannot leverage discrete reasoning chains over multiple comments. To address this issue, in this paper, we propose a probabilistic $K$-hop knowledge graph (KKG) to extend existing knowledge graphs with inferred relations via discrete intra-sentence and inter-sentence reasoning chains. KKG associates each inferred relation with a confidence value through Bayesian inference. We further answer how a knowledge graph with inferred relations can help the multiple comments integration through integrating KKG with GNN ($\\text{K}^2$-GNN). Our extensive experimental results show that our $\\text{K}^2$-GNN outperforms all baseline graph models on multiple comments integration.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhan21b/zhan21b.pdf",
        "supp": "",
        "pdf_size": 1251171,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11688655222287917849&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Texas Tech University, Lubbock, TX 79409, USA; Department of Computer Science, Xavier University of Louisiana, New Orleans, LA 70125, USA; Department of Computer Science, University of Central Arkansas, Conway, AR 72035, USA; Department of Computer Science, Texas Tech University, Lubbock, TX 79409, USA",
        "aff_domain": "ttu.edu;xula.edu;uca.edu;ttu.edu",
        "email": "ttu.edu;xula.edu;uca.edu;ttu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Texas Tech University;Xavier University of Louisiana;University of Central Arkansas",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.ttu.edu;https://www.xula.edu;https://www.uca.edu",
        "aff_unique_abbr": "TTU;XULA;UCA",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Lubbock;New Orleans;Conway",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "$h$-DBSCAN: A simple fast DBSCAN algorithm for big data",
        "site": "https://proceedings.mlr.press/v157/weng21a.html",
        "author": "Shaoyuan Weng; Jin Gou; Zongwen Fan",
        "abstract": "DBSCAN is a classical clustering algorithm, which can identify different shapes and isolate noisy patterns from a dataset. Despite the above advantages, the bottleneck of DBSCAN is its computation time for high dimensional datasets. This work, thus, presents a simple and fast method to improve the efficiency of DBSCAN algorithm. We reduce the execution time in two aspects. The first one is to reduce the number of points presented to DBSCAN and the second one is to apply the HNSW technique instead of the linear search structure for improving its efficiency. The experimental results show that our proposed algorithm can greatly improve the clustering speed without losing or even obtaining better accuracy, especially for large-scale datasets.",
        "bibtex": "@InProceedings{pmlr-v157-weng21a,\n  title = \t {$h$-DBSCAN: A simple fast DBSCAN algorithm for big data},\n  author =       {Weng, Shaoyuan and Gou, Jin and Fan, Zongwen},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {81--96},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/weng21a/weng21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/weng21a.html},\n  abstract = \t {DBSCAN is a classical clustering algorithm, which can identify different shapes and isolate noisy patterns from a dataset. Despite the above advantages, the bottleneck of DBSCAN is its computation time for high dimensional datasets. This work, thus, presents a simple and fast method to improve the efficiency of DBSCAN algorithm. We reduce the execution time in two aspects. The first one is to reduce the number of points presented to DBSCAN and the second one is to apply the HNSW technique instead of the linear search structure for improving its efficiency. The experimental results show that our proposed algorithm can greatly improve the clustering speed without losing or even obtaining better accuracy, especially for large-scale datasets. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/weng21a/weng21a.pdf",
        "supp": "",
        "pdf_size": 1266155,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17645580841323784897&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science and Technology Huaqiao University Xiamen, China; College of Computer Science and Technology Huaqiao University Xiamen, China; College of Computer Science and Technology Huaqiao University Xiamen, China",
        "aff_domain": "stu.hqu.edu.cn;hqu.edu.cn;hqu.edu.cn",
        "email": "stu.hqu.edu.cn;hqu.edu.cn;hqu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huaqiao University",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "https://www.hqu.edu.cn",
        "aff_unique_abbr": "HQU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Xiamen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Causal Approach for Unfair Edge Prioritization and Discrimination Removal",
        "site": "https://proceedings.mlr.press/v157/pavan-ravishankar21a.html",
        "author": "Balaraman Ravindran Pavan Ravishankar Pranshu Malviya",
        "abstract": "In budget-constrained settings aimed at mitigating unfairness, like law enforcement, it is essential to prioritize the sources of unfairness before taking measures to mitigate them in the real world. Unlike previous works, which only serve as a caution against possible discrimination and de-bias data after data generation, this work provides a toolkit to mitigate unfairness during data generation, given by the Unfair Edge Prioritization algorithm, in addition to de-biasing data after generation, given by the Discrimination Removal algorithm. We assume that a non-parametric Markovian causal model representative of the data generation procedure is given. The edges emanating from the sensitive nodes in the causal graph, such as race, are assumed to be the sources of unfairness. We first quantify Edge Flow in any edge X \u2013> Y, which is the belief of observing a specific value of Y due to the influence of a specific value of X along X \u2013> Y. We then quantify Edge Unfairness by formulating a non-parametric model in terms of edge flows. We then prove that cumulative unfairness towards sensitive groups in a decision, like race in a bail decision, is non-existent when edge unfairness is absent. We prove this result for the non-trivial non-parametric model setting when the cumulative unfairness cannot be expressed in terms of edge unfairness. We then measure the Potential to mitigate the Cumulative Unfairness when edge unfairness is decreased. Based on these measurements, we propose the Unfair Edge Prioritization algorithm that can then be used by policymakers. We also propose the Discrimination Removal Procedure that de-biases a data distribution by eliminating optimization constraints that grow exponentially in the number of sensitive attributes and values taken by them. Extensive experiments validate the theorem and specifications used for quantifying the above measures.",
        "bibtex": "@InProceedings{pmlr-v157-pavan-ravishankar21a,\n  title = \t {A Causal Approach for Unfair Edge Prioritization and Discrimination Removal},\n  author =       {Pavan Ravishankar, Pranshu Malviya, Balaraman Ravindran},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {518--533},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/pavan-ravishankar21a/pavan-ravishankar21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/pavan-ravishankar21a.html},\n  abstract = \t {In budget-constrained settings aimed at mitigating unfairness, like law enforcement, it is essential to prioritize the sources of unfairness before taking measures to mitigate them in the real world. Unlike previous works, which only serve as a caution against possible discrimination and de-bias data after data generation, this work provides a toolkit to mitigate unfairness during data generation, given by the Unfair Edge Prioritization algorithm, in addition to de-biasing data after generation, given by the Discrimination Removal algorithm. We assume that a non-parametric Markovian causal model representative of the data generation procedure is given. The edges emanating from the sensitive nodes in the causal graph, such as race, are assumed to be the sources of unfairness. We first quantify Edge Flow in any edge X \u2013> Y, which is the belief of observing a specific value of Y due to the influence of a specific value of X along X \u2013> Y. We then quantify Edge Unfairness by formulating a non-parametric model in terms of edge flows. We then prove that cumulative unfairness towards sensitive groups in a decision, like race in a bail decision, is non-existent when edge unfairness is absent. We prove this result for the non-trivial non-parametric model setting when the cumulative unfairness cannot be expressed in terms of edge unfairness. We then measure the Potential to mitigate the Cumulative Unfairness when edge unfairness is decreased. Based on these measurements, we propose the Unfair Edge Prioritization algorithm that can then be used by policymakers. We also propose the Discrimination Removal Procedure that de-biases a data distribution by eliminating optimization constraints that grow exponentially in the number of sensitive attributes and values taken by them. Extensive experiments validate the theorem and specifications used for quantifying the above measures.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/pavan-ravishankar21a/pavan-ravishankar21a.pdf",
        "supp": "",
        "pdf_size": 1007255,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11921275117891823338&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "stu.hqu.edu.cn;hqu.edu.cn;hqu.edu.cn",
        "email": "stu.hqu.edu.cn;hqu.edu.cn;hqu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "A Mutual Information Regularization for Adversarial Training",
        "site": "https://proceedings.mlr.press/v157/atsague21a.html",
        "author": "Modeste Atsague; Olukorede Fakorede; Jin Tian",
        "abstract": "Recently, a number of  methods have been developed to alleviate the vulnerability of deep neural networks to adversarial examples, among which adversarial training and its variants have been demonstrated to be the most effective empirically. This paper aims to further improve the robustness of adversarial training against adversarial examples. We propose a new training method called mutual information and mean absolute error adversarial training (MIMAE-AT) in which the mutual information between the probabilistic predictions of the natural and the adversarial examples along with the mean absolute error between their logits are used as regularization terms to the standard adversarial training.We conduct  experiments and demonstrate that the proposed MIMAE-AT method improves the state-of-the-art on adversarial robustness.",
        "bibtex": "@InProceedings{pmlr-v157-atsague21a,\n  title = \t {A Mutual Information Regularization for Adversarial Training},\n  author =       {Atsague, Modeste and Fakorede, Olukorede and Tian, Jin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {188--203},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/atsague21a/atsague21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/atsague21a.html},\n  abstract = \t {Recently, a number of  methods have been developed to alleviate the vulnerability of deep neural networks to adversarial examples, among which adversarial training and its variants have been demonstrated to be the most effective empirically. This paper aims to further improve the robustness of adversarial training against adversarial examples. We propose a new training method called mutual information and mean absolute error adversarial training (MIMAE-AT) in which the mutual information between the probabilistic predictions of the natural and the adversarial examples along with the mean absolute error between their logits are used as regularization terms to the standard adversarial training.We conduct  experiments and demonstrate that the proposed MIMAE-AT method improves the state-of-the-art on adversarial robustness.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/atsague21a/atsague21a.pdf",
        "supp": "",
        "pdf_size": 244544,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8234370430743684029&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Iowa State University; Iowa State University; Iowa State University",
        "aff_domain": "iastate.edu;iastate.edu;iastate.edu",
        "email": "iastate.edu;iastate.edu;iastate.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Iowa State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iastate.edu",
        "aff_unique_abbr": "ISU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Partial Label Metric Learning Algorithm for Class Imbalanced Data",
        "site": "https://proceedings.mlr.press/v157/liu21f.html",
        "author": "Wenpeng Liu; Li Wang; Jie Chen; Yu Zhou; Ruirui Zheng; Jianjun He",
        "abstract": "The performance of machine learning algorithms depends on the distance metric, in addition to the model and loss function, etc. The partial label metric learning technique can improve the accuracy of partial label learning algorithms by using training data to learn a better distance metric, which has gradually attracted the attention of scholars in recent years. The essence of partial label learning is mainly to deal with multi-class classification problems, while class imbalance is a common phenomenon in these problems. The class imbalanced problem affects the prediction accuracy of minority class samples, but the current partial label metric learning algorithms rarely consider the problem. In this paper, we propose two partial label metric learning algorithms (PL-CCML-SFN and PL-CCML-LDD) that can solve the class imbalanced problem. The basic idea is to add a regularization term to the objective function of the PL-CCML model, which can induce each class to be uniformly distributed in the new metric space and thus play the role of balancing each class. The experimental results show that these two algorithms, compared with the existing partial label metric learning algorithms, have improved the overall performance on the class imbalanced data.",
        "bibtex": "@InProceedings{pmlr-v157-liu21f,\n  title = \t {A Partial Label Metric Learning Algorithm for Class Imbalanced Data},\n  author =       {Liu, Wenpeng and Wang, Li and Chen, Jie and Zhou, Yu and Zheng, Ruirui and He, Jianjun},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1413--1428},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/liu21f/liu21f.pdf},\n  url = \t {https://proceedings.mlr.press/v157/liu21f.html},\n  abstract = \t {The performance of machine learning algorithms depends on the distance metric, in addition to the model and loss function, etc. The partial label metric learning technique can improve the accuracy of partial label learning algorithms by using training data to learn a better distance metric, which has gradually attracted the attention of scholars in recent years. The essence of partial label learning is mainly to deal with multi-class classification problems, while class imbalance is a common phenomenon in these problems. The class imbalanced problem affects the prediction accuracy of minority class samples, but the current partial label metric learning algorithms rarely consider the problem. In this paper, we propose two partial label metric learning algorithms (PL-CCML-SFN and PL-CCML-LDD) that can solve the class imbalanced problem. The basic idea is to add a regularization term to the objective function of the PL-CCML model, which can induce each class to be uniformly distributed in the new metric space and thus play the role of balancing each class. The experimental results show that these two algorithms, compared with the existing partial label metric learning algorithms, have improved the overall performance on the class imbalanced data.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/liu21f/liu21f.pdf",
        "supp": "",
        "pdf_size": 298000,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15453947785587440055&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Information and Communication Engineering, Dalian Minzu University, Dalian 116600, Liaoning, China; College of Information and Communication Engineering, Dalian Minzu University, Dalian 116600, Liaoning, China; College of Information and Communication Engineering, Dalian Minzu University, Dalian 116600, Liaoning, China; College of Information and Communication Engineering, Dalian Minzu University, Dalian 116600, Liaoning, China; College of Information and Communication Engineering, Dalian Minzu University, Dalian 116600, Liaoning, China; College of Information and Communication Engineering, Dalian Minzu University, Dalian 116600, Liaoning, China",
        "aff_domain": "dlnu.edu.cn;qq.com;gmail.com;sina.com;dlnu.edu.cn;live.com",
        "email": "dlnu.edu.cn;qq.com;gmail.com;sina.com;dlnu.edu.cn;live.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Dalian Minzu University",
        "aff_unique_dep": "College of Information and Communication Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Dalian",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Two-Stage Training Framework with Feature-Label Matching Mechanism for Learning from Label Proportions",
        "site": "https://proceedings.mlr.press/v157/yang21b.html",
        "author": "Haoran Yang; Wanjing Zhang; Wai Lam",
        "abstract": "In this paper, we study a task called Learning from Label Proportions (LLP). LLP aims to learn an instance-level classifier given a number of bags and each bag is composed of several instances. The label of each instance is concealed and what we know is the proportion of each class in each bag. The lack of instance-level supervision information makes the model struggle for finding the right direction for optimization. In this paper, we solve this problem by developing a two-stage training framework. First, we facilitate contrastive learning to train a feature extractor in an unsupervised way. Second, we train a linear classifier with the parameter of the feature extractor fixed. This framework performs much better than most baselines but is still unsatisfactory when the bag size or the number of classes is large. Therefore, we further propose a Feature-Label Matching mechanism (FLMm). FLMm can provide a roughly right optimization direction for the classifier by assigning labels to a subset of instances selected in this bag with a high degree of confidence. Therefore, the classifier can more easily establish the correspondence between instances and labels in the second stage. Experimental results on two benchmark datasets, namely CIFAR10 and CIFAR100, show that our model is far superior than baseline models, for example, accuracy increases from 43.44% to 61.25% for bag size 128 on CIFAR100.",
        "bibtex": "@InProceedings{pmlr-v157-yang21b,\n  title = \t {A Two-Stage Training Framework with Feature-Label Matching Mechanism for Learning from Label Proportions},\n  author =       {Yang, Haoran and Zhang, Wanjing and Lam, Wai},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1461--1476},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/yang21b/yang21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/yang21b.html},\n  abstract = \t {In this paper, we study a task called Learning from Label Proportions (LLP). LLP aims to learn an instance-level classifier given a number of bags and each bag is composed of several instances. The label of each instance is concealed and what we know is the proportion of each class in each bag. The lack of instance-level supervision information makes the model struggle for finding the right direction for optimization. In this paper, we solve this problem by developing a two-stage training framework. First, we facilitate contrastive learning to train a feature extractor in an unsupervised way. Second, we train a linear classifier with the parameter of the feature extractor fixed. This framework performs much better than most baselines but is still unsatisfactory when the bag size or the number of classes is large. Therefore, we further propose a Feature-Label Matching mechanism (FLMm). FLMm can provide a roughly right optimization direction for the classifier by assigning labels to a subset of instances selected in this bag with a high degree of confidence. Therefore, the classifier can more easily establish the correspondence between instances and labels in the second stage. Experimental results on two benchmark datasets, namely CIFAR10 and CIFAR100, show that our model is far superior than baseline models, for example, accuracy increases from 43.44% to 61.25% for bag size 128 on CIFAR100.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/yang21b/yang21b.pdf",
        "supp": "",
        "pdf_size": 543321,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5168081600201872977&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "The Chinese University of Hong Kong, Sha Tin, Hong Kong; Central University of Finance and Economics, Beijing, China + The Chinese University of Hong Kong, Sha Tin, Hong Kong; The Chinese University of Hong Kong, Sha Tin, Hong Kong",
        "aff_domain": "SE.CUHK.EDU.HK;GMAIL.COM;SE.CUHK.EDU.HK",
        "email": "SE.CUHK.EDU.HK;GMAIL.COM;SE.CUHK.EDU.HK",
        "github": "https://github.com/LHRYANG/LLP_FLMm",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong;Central University of Finance and Economics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.cufe.edu.cn",
        "aff_unique_abbr": "CUHK;CUFE",
        "aff_campus_unique_index": "0;1+0;0",
        "aff_campus_unique": "Hong Kong SAR;Beijing",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ASD-Conv: Monocular 3D object detection network based on Asymmetrical Segmentation Depth-aware Convolution",
        "site": "https://proceedings.mlr.press/v157/xingyuan21a.html",
        "author": "Yu Xingyuan; Du Neng; Gao Ge; Wen Fan",
        "abstract": "In the field of 3D object recognition, monocular 3D recognition technology is a valuable recognition technology. Compared with binocular technology and lidar technology, its cost is lower. In this paper, based on the existing monocular 3D recognition network, we propose an asymmetrical segmentation depth-aware network: ASD-Conv Network, which is used to better obtain the depth information of monocular images, so as to obtain better recognition results. Compared with other monocular recognition networks, ASD-Conv network performs special segmentation on the image, which can better obtain the depth distribution of the image, and has made a good breakthrough and improvement in the image recognition tasks of 2D, BEV and 3D. The improved algorithm proposed in this paper can improve the detection accuracy while maintaining a certain real-time performance. Experimental results show that compared with the current model, the proposed monocular 3D object detection algorithm based on D-ASDConv has an average improvement rate of 2.82%(AP) in large object detection and the highest average improvement rate of 2.01%(AP) in small object detection on Kitti dataset. The algorithm can effectively learn more advanced features of spatial perception, and the detection results of monocular images are more accurate.",
        "bibtex": "@InProceedings{pmlr-v157-xingyuan21a,\n  title = \t {ASD-Conv: Monocular 3D object detection network based on Asymmetrical Segmentation Depth-aware Convolution},\n  author =       {Xingyuan, Yu and Neng, Du and Ge, Gao and Fan, Wen},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {642--655},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/xingyuan21a/xingyuan21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/xingyuan21a.html},\n  abstract = \t {In the field of 3D object recognition, monocular 3D recognition technology is a valuable recognition technology. Compared with binocular technology and lidar technology, its cost is lower. In this paper, based on the existing monocular 3D recognition network, we propose an asymmetrical segmentation depth-aware network: ASD-Conv Network, which is used to better obtain the depth information of monocular images, so as to obtain better recognition results. Compared with other monocular recognition networks, ASD-Conv network performs special segmentation on the image, which can better obtain the depth distribution of the image, and has made a good breakthrough and improvement in the image recognition tasks of 2D, BEV and 3D. The improved algorithm proposed in this paper can improve the detection accuracy while maintaining a certain real-time performance. Experimental results show that compared with the current model, the proposed monocular 3D object detection algorithm based on D-ASDConv has an average improvement rate of 2.82%(AP) in large object detection and the highest average improvement rate of 2.01%(AP) in small object detection on Kitti dataset. The algorithm can effectively learn more advanced features of spatial perception, and the detection results of monocular images are more accurate.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/xingyuan21a/xingyuan21a.pdf",
        "supp": "",
        "pdf_size": 10047985,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ACkOikd9gwgJ:scholar.google.com/&scioq=ASD-Conv:+Monocular+3D+object+detection+network+based+on+Asymmetrical+Segmentation+Depth-aware+Convolution&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "National Engineering Research Center for Multimedia Software; National Engineering Research Center for Multimedia Software; National Engineering Research Center for Multimedia Software; National Engineering Research Center for Multimedia Software",
        "aff_domain": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "email": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National Engineering Research Center for Multimedia Software",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "An Aligned Subgraph Kernel Based on Discrete-Time Quantum Walk",
        "site": "https://proceedings.mlr.press/v157/liu21a.html",
        "author": "Kai Liu; Lulu Wang; Yi Zhang",
        "abstract": "In this paper, a novel graph kernel is designed by aligning the amplitude representation of the vertices. Firstly, the amplitude representation of a vertex is calculated based on the discrete-time quantum walk. Then a matching-based graph kernel is constructed through identifying the correspondence between the vertices of two graphs. The newly proposed kernel can be regarded as a kind of aligned subgraph kernel that incorporates the explicit local information of substructures. Thus, it can address the disadvantage arising in the classical R-convolution kernel that the relative locations of substructures between the graphs are ignored. Experiments on several standard datasets demonstrate that the proposed kernel has better performance compared with other state-of-the-art graph kernels in terms of classification accuracy.",
        "bibtex": "@InProceedings{pmlr-v157-liu21a,\n  title = \t {An Aligned Subgraph Kernel Based on Discrete-Time Quantum Walk},\n  author =       {Liu, Kai and Wang, Lulu and Zhang, Yi},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {145--157},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/liu21a/liu21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/liu21a.html},\n  abstract = \t {In this paper, a novel graph kernel is designed by aligning the amplitude representation of the vertices. Firstly, the amplitude representation of a vertex is calculated based on the discrete-time quantum walk. Then a matching-based graph kernel is constructed through identifying the correspondence between the vertices of two graphs. The newly proposed kernel can be regarded as a kind of aligned subgraph kernel that incorporates the explicit local information of substructures. Thus, it can address the disadvantage arising in the classical R-convolution kernel that the relative locations of substructures between the graphs are ignored. Experiments on several standard datasets demonstrate that the proposed kernel has better performance compared with other state-of-the-art graph kernels in terms of classification accuracy.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/liu21a/liu21a.pdf",
        "supp": "",
        "pdf_size": 174578,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8412082325380824235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer, National University of Defense Technology, Changsha, 410073, China; National Innovation Institute of Defense Technology, Beijing 100073, China; Central Future Works Technology Co.,Ltd.,Beijing, China",
        "aff_domain": "nudt.edu.cn;163.com;zxwl-cfw.cn",
        "email": "nudt.edu.cn;163.com;zxwl-cfw.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "National University of Defense Technology;National Innovation Institute of Defense Technology;Central Future Works Technology Co., Ltd.",
        "aff_unique_dep": "College of Computer;;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Changsha;Beijing;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "An Optimistic Acceleration of AMSGrad for Nonconvex Optimization",
        "site": "https://proceedings.mlr.press/v157/wang21c.html",
        "author": "Jun-Kun Wang; Xiaoyun Li; Belhal Karimi; Ping Li",
        "abstract": "We propose a new variant of AMSGrad (Reddi et. al., 2018), a popular adaptive gradient based optimization algorithm widely used for training deep neural networks. Our algorithm adds prior knowledge about the sequence of consecutive mini-batch gradients and leverages its underlying structure making the gradients sequentially predictable. By exploiting the predictability process and ideas from optimistic online learning, the proposed algorithm can accelerate the convergence and increase its sample efficiency. After establishing a tighter upper bound under some convexity conditions on the regret, we offer a complimentary view of our algorithm which generalizes to the offline and stochastic nonconvex optimization settings. In the nonconvex case, we establish a non-asymptotic convergence bound independent of the initialization. We illustrate, via numerical experiments, the practical speedup on several deep learning models and benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v157-wang21c,\n  title = \t {An Optimistic Acceleration of AMSGrad for Nonconvex Optimization},\n  author =       {Wang, Jun-Kun and Li, Xiaoyun and Karimi, Belhal and Li, Ping},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {422--437},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wang21c/wang21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wang21c.html},\n  abstract = \t {We propose a new variant of AMSGrad (Reddi et. al., 2018), a popular adaptive gradient based optimization algorithm widely used for training deep neural networks. Our algorithm adds prior knowledge about the sequence of consecutive mini-batch gradients and leverages its underlying structure making the gradients sequentially predictable. By exploiting the predictability process and ideas from optimistic online learning, the proposed algorithm can accelerate the convergence and increase its sample efficiency. After establishing a tighter upper bound under some convexity conditions on the regret, we offer a complimentary view of our algorithm which generalizes to the offline and stochastic nonconvex optimization settings. In the nonconvex case, we establish a non-asymptotic convergence bound independent of the initialization. We illustrate, via numerical experiments, the practical speedup on several deep learning models and benchmark datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wang21c/wang21c.pdf",
        "supp": "",
        "pdf_size": 1365607,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=412781855280901075&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Cognitive Computing Lab + Baidu Research; Cognitive Computing Lab + Baidu Research; Cognitive Computing Lab + Baidu Research; Cognitive Computing Lab + Baidu Research",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Cognitive Computing Lab;Baidu",
        "aff_unique_dep": "Cognitive Computing;Baidu Research",
        "aff_unique_url": ";https://research.baidu.com",
        "aff_unique_abbr": ";Baidu",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "title": "An online semi-definite programming with a generalised log-determinant regularizer and its applications",
        "site": "https://proceedings.mlr.press/v157/liu21e.html",
        "author": "Yaxiong Liu; Ken-ichiro Moridomi; Kohei Hatano; Eiji Takimoto",
        "abstract": "We consider a variant of the online semi-definite programming problem: The decision space consists of positive semi-definite matrices with bounded diagonal entries and bounded $\\Gamma$-trace norm, which is a generalization of the trace norm defined by a positive definite matrix $\\Gamma$. To solve this problem, we propose a follow-the-regularized-leader algorithm with a novel regularizer, which is a generalisation of the log-determinant function parameterized by the matrix $\\Gamma$. Then we apply our algorithm to online binary matrix completion (OBMC) with side information and online similarity prediction with side information, and improve mistake bounds by logarithmic factors. In particular, for OBMC our mistake bound is optimal.",
        "bibtex": "@InProceedings{pmlr-v157-liu21e,\n  title = \t {An online semi-definite programming with a generalised log-determinant regularizer and its applications},\n  author =       {Liu, Yaxiong and Moridomi, Ken-ichiro and Hatano, Kohei and Takimoto, Eiji},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1113--1128},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/liu21e/liu21e.pdf},\n  url = \t {https://proceedings.mlr.press/v157/liu21e.html},\n  abstract = \t {We consider a variant of the online semi-definite programming problem: The decision space consists of positive semi-definite matrices with bounded diagonal entries and bounded $\\Gamma$-trace norm, which is a generalization of the trace norm defined by a positive definite matrix $\\Gamma$. To solve this problem, we propose a follow-the-regularized-leader algorithm with a novel regularizer, which is a generalisation of the log-determinant function parameterized by the matrix $\\Gamma$. Then we apply our algorithm to online binary matrix completion (OBMC) with side information and online similarity prediction with side information, and improve mistake bounds by logarithmic factors. In particular, for OBMC our mistake bound is optimal.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/liu21e/liu21e.pdf",
        "supp": "",
        "pdf_size": 180711,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12809737224778126033&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Informatics, Kyushu University/RIKEN AIP; SMN Corporation, Japan; Faculty of Art and Science, Kyushu University/RIKEN AIP; Department of Informatics, Kyushu University",
        "aff_domain": "inf.kyushu-u.ac.jp;so-netmedia.jp;inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp",
        "email": "inf.kyushu-u.ac.jp;so-netmedia.jp;inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Kyushu University;SMN Corporation",
        "aff_unique_dep": "Department of Informatics;",
        "aff_unique_url": "https://www.kyushu-u.ac.jp;",
        "aff_unique_abbr": "Kyushu U;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Asian Conference on Machine Learning: Preface",
        "site": "https://proceedings.mlr.press/v157/balasubramanian21a.html",
        "author": "Vineeth N. Balasubramanian; Ivor Tsang",
        "abstract": "Preface to ACML 2021.",
        "bibtex": "@InProceedings{pmlr-v157-balasubramanian21a,\n  title = \t {Asian Conference on Machine Learning: Preface},\n  author =       {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {i--xiii},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/balasubramanian21a/balasubramanian21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/balasubramanian21a.html},\n  abstract = \t {Preface to ACML 2021.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/balasubramanian21a/balasubramanian21a.pdf",
        "supp": "",
        "pdf_size": 125881,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:l6k7FqW5IuIJ:scholar.google.com/&scioq=Asian+Conference+on+Machine+Learning:+Preface&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Asymptotically Exact and Fast Gaussian Copula Models for Imputation of Mixed Data Types",
        "site": "https://proceedings.mlr.press/v157/christoffersen21a.html",
        "author": "Benjamin Christoffersen; Mark Clements; Keith Humphreys; Hedvig Kjellstr\u00f6m",
        "abstract": "Missing values with mixed data types is a common problem in a large number of machine learning applications such as processing of surveys and in different medical applications. Recently, Gaussian copula models have been suggested as a means of performing imputation of missing values using a probabilistic framework. While the present Gaussian copula models have shown to yield state of the art performance, they have two limitations: they are based on an approximation that is fast but may be imprecise and they do not support unordered multinomial variables. We address the first limitation using direct and arbitrarily precise approximations both for model estimation and imputation by using randomized quasi-Monte Carlo procedures. The method we provide has lower errors for the estimated model parameters and the imputed values, compared to previously proposed methods. We also extend the previous Gaussian copula models to include unordered multinomial variables in addition to the present support of ordinal, binary, and continuous variables.",
        "bibtex": "@InProceedings{pmlr-v157-christoffersen21a,\n  title = \t {Asymptotically Exact and Fast {G}aussian Copula Models for Imputation of Mixed Data Types},\n  author =       {Christoffersen, Benjamin and Clements, Mark and Humphreys, Keith and Kjellstr{\\\"o}m, Hedvig},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {870--885},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/christoffersen21a/christoffersen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/christoffersen21a.html},\n  abstract = \t {Missing values with mixed data types is a common problem in a large number of machine learning applications such as processing of surveys and in different medical applications. Recently, Gaussian copula models have been suggested as a means of performing imputation of missing values using a probabilistic framework. While the present Gaussian copula models have shown to yield state of the art performance, they have two limitations: they are based on an approximation that is fast but may be imprecise and they do not support unordered multinomial variables. We address the first limitation using direct and arbitrarily precise approximations both for model estimation and imputation by using randomized quasi-Monte Carlo procedures. The method we provide has lower errors for the estimated model parameters and the imputed values, compared to previously proposed methods. We also extend the previous Gaussian copula models to include unordered multinomial variables in addition to the present support of ordinal, binary, and continuous variables.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/christoffersen21a/christoffersen21a.pdf",
        "supp": "",
        "pdf_size": 436242,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8011782435662860828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Division of Robotics, Perception and Learning, KTH Royal Institute of Technology, Sweden; Department of Medical Epidemiology and Biostatistics, Karolinska Institutet, Sweden; Department of Medical Epidemiology and Biostatistics, Karolinska Institutet, Sweden; Division of Robotics, Perception and Learning, KTH Royal Institute of Technology, Sweden",
        "aff_domain": "kth.se;ki.se;ki.se;kth.se",
        "email": "kth.se;ki.se;ki.se;kth.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "KTH Royal Institute of Technology;Karolinska Institutet",
        "aff_unique_dep": "Division of Robotics, Perception and Learning;Department of Medical Epidemiology and Biostatistics",
        "aff_unique_url": "https://www.kth.se;https://ki.se",
        "aff_unique_abbr": "KTH;KI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Augmenting Imbalanced Time-series Data via Adversarial Perturbation in Latent Space",
        "site": "https://proceedings.mlr.press/v157/kim21a.html",
        "author": "Beomsoo Kim; Jang-Ho Choi; Jaegul Choo",
        "abstract": "Success of training deep learning models largely depends on the amount and quality of training data. Although numerous data augmentation techniques have already been pro- posed for certain domains such as computer vision where simple schemes such as rotation and flipping have been shown to be effective, other domains such as time-series data have a relatively smaller set of augmentation techniques readily available. Data imbalance is a phenomenon often observed in real-world data. However, a simple oversampling technique may make a model vulnerable to overfitting, so a proper data augmentation is desired. To tackle these problems, we propose a novel data augmentation method that utilizes the latent vectors of an autoencoder in a novel way. When input data are perturbed in its latent space, their reconstructed data retains properties similar to the original one. In con- trast, adversarial augmentation is a technique to train robust deep neural networks against unforeseen data shifts or corruptions by providing a downstream model with samples that are difficult to predict. Our method adversarially perturbs input data in its latent space so that the augmented data is diverse and conducive to reducing test error of a downstream model. The experimental results demonstrated that our method achieves the right balance, significantly modifying the input data to help generalization while retaining its realism.",
        "bibtex": "@InProceedings{pmlr-v157-kim21a,\n  title = \t {Augmenting Imbalanced Time-series Data via Adversarial Perturbation in Latent Space},\n  author =       {Kim, Beomsoo and Choi, Jang-Ho and Choo, Jaegul},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1633--1644},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/kim21a/kim21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/kim21a.html},\n  abstract = \t {Success of training deep learning models largely depends on the amount and quality of training data. Although numerous data augmentation techniques have already been pro- posed for certain domains such as computer vision where simple schemes such as rotation and flipping have been shown to be effective, other domains such as time-series data have a relatively smaller set of augmentation techniques readily available. Data imbalance is a phenomenon often observed in real-world data. However, a simple oversampling technique may make a model vulnerable to overfitting, so a proper data augmentation is desired. To tackle these problems, we propose a novel data augmentation method that utilizes the latent vectors of an autoencoder in a novel way. When input data are perturbed in its latent space, their reconstructed data retains properties similar to the original one. In con- trast, adversarial augmentation is a technique to train robust deep neural networks against unforeseen data shifts or corruptions by providing a downstream model with samples that are difficult to predict. Our method adversarially perturbs input data in its latent space so that the augmented data is diverse and conducive to reducing test error of a downstream model. The experimental results demonstrated that our method achieves the right balance, significantly modifying the input data to help generalization while retaining its realism.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/kim21a/kim21a.pdf",
        "supp": "",
        "pdf_size": 227535,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15760916715633913490&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "KAIST, Daejeon, Republic of Korea; ETRI, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea",
        "aff_domain": "kaist.ac.kr;etri.re.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;etri.re.kr;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Electronics and Telecommunications Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.etri.re.kr",
        "aff_unique_abbr": "KAIST;ETRI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Daejeon",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "BRAC+: Improved Behavior Regularized Actor Critic for Offline Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/zhang21a.html",
        "author": "Chi Zhang; Sanmukh Kuppannagari; Prasanna Viktor",
        "abstract": "Online interactions with the environment to collect data samples for training a Reinforcement Learning (RL) agent is not always feasible due to economic and safety concerns. The goal of Offline Reinforcement Learning is to address this problem by learning effective policies using previously collected datasets. Standard off-policy RL algorithms are prone to overestimations of the values of out-of-distribution (less explored) actions and are hence unsuitable for Offline RL. Behavior regularization, which constraints the learned policy within the support set of the dataset, has been proposed to tackle the limitations of standard off-policy algorithms. In this paper, we improve the behavior regularized offline reinforcement learning and propose BRAC+. First, we propose quantification of the out-of-distribution actions and conduct comparisons between using Kullback\u2013Leibler divergence versus using Maximum Mean Discrepancy as the regularization protocol. We propose an analytical upper bound on the KL divergence as the behavior regularizer to reduce variance associated with sample based estimations. Second, we mathematically show that the learned Q values can diverge even using behavior regularized policy update under mild assumptions. This leads to large overestimations of the Q values and performance deterioration of the learned policy. To mitigate this issue, we add a gradient penalty term to the policy evaluation objective. By doing so, the Q values are guaranteed to converge. On challenging offline RL benchmarks, BRAC+  outperforms the baseline behavior regularized approaches by $40%\\sim 87%$ and the state-of-the-art approach by $6%$.",
        "bibtex": "@InProceedings{pmlr-v157-zhang21a,\n  title = \t {BRAC+: Improved Behavior Regularized Actor Critic for Offline Reinforcement Learning},\n  author =       {Zhang, Chi and Kuppannagari, Sanmukh and Viktor, Prasanna},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {204--219},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhang21a/zhang21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhang21a.html},\n  abstract = \t {Online interactions with the environment to collect data samples for training a Reinforcement Learning (RL) agent is not always feasible due to economic and safety concerns. The goal of Offline Reinforcement Learning is to address this problem by learning effective policies using previously collected datasets. Standard off-policy RL algorithms are prone to overestimations of the values of out-of-distribution (less explored) actions and are hence unsuitable for Offline RL. Behavior regularization, which constraints the learned policy within the support set of the dataset, has been proposed to tackle the limitations of standard off-policy algorithms. In this paper, we improve the behavior regularized offline reinforcement learning and propose BRAC+. First, we propose quantification of the out-of-distribution actions and conduct comparisons between using Kullback\u2013Leibler divergence versus using Maximum Mean Discrepancy as the regularization protocol. We propose an analytical upper bound on the KL divergence as the behavior regularizer to reduce variance associated with sample based estimations. Second, we mathematically show that the learned Q values can diverge even using behavior regularized policy update under mild assumptions. This leads to large overestimations of the Q values and performance deterioration of the learned policy. To mitigate this issue, we add a gradient penalty term to the policy evaluation objective. By doing so, the Q values are guaranteed to converge. On challenging offline RL benchmarks, BRAC+  outperforms the baseline behavior regularized approaches by $40%\\sim 87%$ and the state-of-the-art approach by $6%$.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhang21a/zhang21a.pdf",
        "supp": "",
        "pdf_size": 1431174,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5335372137871964540&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Hughes Aircraft Electrical Engineering Center, 3740 McClintock Ave, Los Angeles, CA 90089; Hughes Aircraft Electrical Engineering Center, 3740 McClintock Ave, Los Angeles, CA 90089; Hughes Aircraft Electrical Engineering Center, 3740 McClintock Ave, Los Angeles, CA 90089",
        "aff_domain": "usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hughes Aircraft Company",
        "aff_unique_dep": "Electrical Engineering Center",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bayesian Inference for Optimal Transport with Stochastic Cost",
        "site": "https://proceedings.mlr.press/v157/mallasto21a.html",
        "author": "Anton Mallasto; Markus Heinonen; Samuel Kaski",
        "abstract": "In machine learning and computer vision, optimal transport has had significant success inlearning generative models and defining metric distances between structured and stochasticdata objects, that can be cast as probability measures.  The key element of optimal trans-port is the so called lifting of anexactcost (distance) function, defined on the sample space,to a cost (distance) between probability measures over the sample space.  However, in manyreal life applications the cost isstochastic: e.g., the unpredictable traffic flow affects the costof transportation between a factory and an outlet.  To take this stochasticity into account,we introduce a Bayesian framework for inferring the optimal transport plan distributioninduced by the stochastic cost, allowing for a principled way to include prior informationand to model the induced stochasticity on the transport plans.  Additionally, we tailor anHMC method to sample from the resulting transport plan posterior distribution.",
        "bibtex": "@InProceedings{pmlr-v157-mallasto21a,\n  title = \t {Bayesian Inference for Optimal Transport with Stochastic Cost},\n  author =       {Mallasto, Anton and Heinonen, Markus and Kaski, Samuel},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1601--1616},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/mallasto21a/mallasto21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/mallasto21a.html},\n  abstract = \t {In machine learning and computer vision, optimal transport has had significant success inlearning generative models and defining metric distances between structured and stochasticdata objects, that can be cast as probability measures.  The key element of optimal trans-port is the so called lifting of anexactcost (distance) function, defined on the sample space,to a cost (distance) between probability measures over the sample space.  However, in manyreal life applications the cost isstochastic: e.g., the unpredictable traffic flow affects the costof transportation between a factory and an outlet.  To take this stochasticity into account,we introduce a Bayesian framework for inferring the optimal transport plan distributioninduced by the stochastic cost, allowing for a principled way to include prior informationand to model the induced stochasticity on the transport plans.  Additionally, we tailor anHMC method to sample from the resulting transport plan posterior distribution.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/mallasto21a/mallasto21a.pdf",
        "supp": "",
        "pdf_size": 1169845,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17162555537200292825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Aalto University, Finland; Department of Computer Science, Aalto University, Finland; Department of Computer Science, Aalto University, Finland+Department of Computer Science, University of Manchester, UK",
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Aalto University;University of Manchester",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.aalto.fi;https://www.manchester.ac.uk",
        "aff_unique_abbr": "Aalto;UoM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Finland;United Kingdom"
    },
    {
        "title": "Bayesian Latent Factor Model for Higher-order Data",
        "site": "https://proceedings.mlr.press/v157/tao21a.html",
        "author": "Zerui Tao; Xuyang Zhao; Toshihisa Tanaka; Qibin Zhao",
        "abstract": "Latent factor models are canonical tools to learn low-dimensional and linear embedding of original data. Traditional latent factor models are based on low-rank matrix factorization of covariance matrices. However, for higher-order data with multiple modes, i.e., tensors, this simple treatment fails to take into account the mode-specific relations. This ignorance leads to inefficiency in analysis of complex structures as well as poor data compression ability. In this paper, unlike covariance matrices, we investigate high-order covariance tensor directly by exploiting tensor ring (TR) format and propose the Bayesian TR latent factor model, which can represent complex multi-linear correlations and achieves efficient data compression. To overcome the difficulty of finding the optimal TR-ranks and simultaneously imposing sparsity on loading coefficients, a multiplicative Gamma process (MGP) prior is adopted to automatically infer the ranks and obtain sparsity. Then, we establish an efficient parameter-expanded EM algorithm to learn the maximum a posteriori (MAP) estimate of model parameters. Finally, we evaluate our model on covariance estimation, latent factor learning and image inpainting problems.",
        "bibtex": "@InProceedings{pmlr-v157-tao21a,\n  title = \t {Bayesian Latent Factor Model for Higher-order Data},\n  author =       {Tao, Zerui and Zhao, Xuyang and Tanaka, Toshihisa and Zhao, Qibin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1285--1300},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/tao21a/tao21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/tao21a.html},\n  abstract = \t {Latent factor models are canonical tools to learn low-dimensional and linear embedding of original data. Traditional latent factor models are based on low-rank matrix factorization of covariance matrices. However, for higher-order data with multiple modes, i.e., tensors, this simple treatment fails to take into account the mode-specific relations. This ignorance leads to inefficiency in analysis of complex structures as well as poor data compression ability. In this paper, unlike covariance matrices, we investigate high-order covariance tensor directly by exploiting tensor ring (TR) format and propose the Bayesian TR latent factor model, which can represent complex multi-linear correlations and achieves efficient data compression. To overcome the difficulty of finding the optimal TR-ranks and simultaneously imposing sparsity on loading coefficients, a multiplicative Gamma process (MGP) prior is adopted to automatically infer the ranks and obtain sparsity. Then, we establish an efficient parameter-expanded EM algorithm to learn the maximum a posteriori (MAP) estimate of model parameters. Finally, we evaluate our model on covariance estimation, latent factor learning and image inpainting problems.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/tao21a/tao21a.pdf",
        "supp": "",
        "pdf_size": 1253462,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=358460258798969595&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical and Electronic Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan + RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan; Department of Electrical and Electronic Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan; Department of Electrical and Electronic Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan; RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan",
        "aff_domain": "riken.jp;sip.tuat.ac.jp;cc.tuat.ac.jp;riken.jp",
        "email": "riken.jp;sip.tuat.ac.jp;cc.tuat.ac.jp;riken.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Tokyo University of Agriculture and Technology;RIKEN Center for Advanced Intelligence Project",
        "aff_unique_dep": "Department of Electrical and Electronic Engineering;Advanced Intelligence Project",
        "aff_unique_url": "https://www.tuat.ac.jp;https://aipcenter.riken.jp/en/",
        "aff_unique_abbr": "TUAT;RIKEN AIP",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Bayesian neural network unit priors and generalized Weibull-tail property",
        "site": "https://proceedings.mlr.press/v157/vladimirova21a.html",
        "author": "Mariia Vladimirova; Julyan Arbel; St\u00e9phane Girard",
        "abstract": "The connection between Bayesian neural networks and Gaussian processes gained a lot of attention in the last few years. Hidden units are proven to follow a Gaussian process limit when the layer width tends to infinity. Recent work has suggested that finite Bayesian neural networks may outperform their infinite counterparts because they adapt their internal representations flexibly. To establish solid ground for future research on finite-width neural networks, our goal is to study the prior induced on hidden units. Our main result is an accurate description of hidden units tails which shows that unit priors become heavier-tailed going deeper, thanks to the introduced notion of generalized Weibull-tail. This finding sheds light on the behavior of hidden units of finite Bayesian neural networks.",
        "bibtex": "@InProceedings{pmlr-v157-vladimirova21a,\n  title = \t {{B}ayesian neural network unit priors and generalized {W}eibull-tail property},\n  author =       {Vladimirova, Mariia and Arbel, Julyan and Girard, St\\'ephane},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1397--1412},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/vladimirova21a/vladimirova21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/vladimirova21a.html},\n  abstract = \t {The connection between Bayesian neural networks and Gaussian processes gained a lot of attention in the last few years. Hidden units are proven to follow a Gaussian process limit when the layer width tends to infinity. Recent work has suggested that finite Bayesian neural networks may outperform their infinite counterparts because they adapt their internal representations flexibly. To establish solid ground for future research on finite-width neural networks, our goal is to study the prior induced on hidden units. Our main result is an accurate description of hidden units tails which shows that unit priors become heavier-tailed going deeper, thanks to the introduced notion of generalized Weibull-tail. This finding sheds light on the behavior of hidden units of finite Bayesian neural networks. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/vladimirova21a/vladimirova21a.pdf",
        "supp": "",
        "pdf_size": 785757,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17230127271283456412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Univ. Grenoble Alpes, Inria, CNRS, LJK, 38000 Grenoble, France; Univ. Grenoble Alpes, Inria, CNRS, LJK, 38000 Grenoble, France; Univ. Grenoble Alpes, Inria, CNRS, LJK, 38000 Grenoble, France",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universite Grenoble Alpes",
        "aff_unique_dep": "Laboratoire Jean Kuntzmann (LJK)",
        "aff_unique_url": "https://www.univ-grenoble-alpes.fr",
        "aff_unique_abbr": "UGA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Grenoble",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Bayesian nonparametric model for arbitrary cubic partitioning",
        "site": "https://proceedings.mlr.press/v157/nakano21a.html",
        "author": "Masahiro Nakano; Yasuhiro Fujiwara; Akisato Kimura; Takeshi Yamada; Naonori Ueda",
        "abstract": "In this paper, we propose a continuous-time Markov process for cubic partitioning models of three-dimensional (3D) arrays and its application to Bayesian nonparametric relational data analysis of 3D array data. Relational data analysis is a topic that has been actively studied in the field of Bayesian nonparametrics, and in particular, models for analyzing 3D arrays have attracted much attention in recent years. In particular, the cubic partitioning model is very popular due to its practical usefulness, and various models such as the infinite relational model and the Mondrian process have been proposed. However, these conventional models have the disadvantage that they are limited to a certain class of cubic partitions, and there is a need for a model that can represent a broader class of arbitrary cubic partitions, which has long been an open issue in this field. In this study, we propose a stochastic process that can represent arbitrary cubic partitions of 3D arrays as a continuous-time Markov process. Furthermore, by combining it with the Aldous-Hoover-Kallenberg representation theorem, we construct an infinitely exchangeable 3D relational model and apply it to real data to show its application to relational data analysis. Experiments show that the proposed model improves the prediction performance by expanding the class of representable cubic partitioning.",
        "bibtex": "@InProceedings{pmlr-v157-nakano21a,\n  title = \t {Bayesian nonparametric model for arbitrary cubic partitioning},\n  author =       {Nakano, Masahiro and Fujiwara, Yasuhiro and Kimura, Akisato and Yamada, Takeshi and Ueda, Naonori},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1585--1600},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/nakano21a/nakano21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/nakano21a.html},\n  abstract = \t {In this paper, we propose a continuous-time Markov process for cubic partitioning models of three-dimensional (3D) arrays and its application to Bayesian nonparametric relational data analysis of 3D array data. Relational data analysis is a topic that has been actively studied in the field of Bayesian nonparametrics, and in particular, models for analyzing 3D arrays have attracted much attention in recent years. In particular, the cubic partitioning model is very popular due to its practical usefulness, and various models such as the infinite relational model and the Mondrian process have been proposed. However, these conventional models have the disadvantage that they are limited to a certain class of cubic partitions, and there is a need for a model that can represent a broader class of arbitrary cubic partitions, which has long been an open issue in this field. In this study, we propose a stochastic process that can represent arbitrary cubic partitions of 3D arrays as a continuous-time Markov process. Furthermore, by combining it with the Aldous-Hoover-Kallenberg representation theorem, we construct an infinitely exchangeable 3D relational model and apply it to real data to show its application to relational data analysis. Experiments show that the proposed model improves the prediction performance by expanding the class of representable cubic partitioning. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/nakano21a/nakano21a.pdf",
        "supp": "",
        "pdf_size": 5165015,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:L7tTb0DMwzMJ:scholar.google.com/&scioq=Bayesian+nonparametric+model+for+arbitrary+cubic+partitioning&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation",
        "aff_domain": "hco.ntt.co.jp;hco.ntt.co.jp;hco.ntt.co.jp;hco.ntt.co.jp;hco.ntt.co.jp",
        "email": "hco.ntt.co.jp;hco.ntt.co.jp;hco.ntt.co.jp;hco.ntt.co.jp;hco.ntt.co.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "NTT Corporation",
        "aff_unique_dep": "Communication Science Laboratories",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Beyond $L_p$ Clipping: Equalization based Psychoacoustic Attacks against ASRs",
        "site": "https://proceedings.mlr.press/v157/abdullah21a.html",
        "author": "Hadi Abdullah; Muhammad Sajidur Rahman; Christian Peeters; Cassidy Gibson; Washington Garcia; Vincent Bindschaedler; Thomas Shrimpton; Patrick Traynor",
        "abstract": "Automatic Speech Recognition (ASR) systems convert speech into text and can be placed into two broad categories: traditional and fully end-to-end. Both types have been shown to be vulnerable to adversarial audio examples that sound benign to the human ear but force the ASR to produce malicious transcriptions. Of these attacks, only the \u201cpsychoacoustic\u201d attacks can create examples with relatively imperceptible perturbations, as they leverage the knowledge of the human auditory system. Unfortunately, existing psychoacoustic attacks can only be applied against traditional models, and are obsolete against the newer, fully end-to-end ASRs. In this paper, we propose an equalization-based psychoacoustic attack that can exploit both traditional and fully end-to-end ASRs. We successfully demonstrate our attack against real-world ASRs that include DeepSpeech and Wav2Letter. Moreover, we employ a user study to verify that our method creates low audible distortion. Specifically, 80 of the 100 participants voted in favor of\u00a0\\textit{all} our attack audio samples as less noisier than the existing state-of-the-art attack. Through this, we demonstrate both types of existing ASR pipelines can be exploited with minimum degradation to attack audio quality.",
        "bibtex": "@InProceedings{pmlr-v157-abdullah21a,\n  title = \t {Beyond $L_{p}$ Clipping: Equalization based Psychoacoustic Attacks against {ASRs}},\n  author =       {Abdullah, Hadi and Rahman, Muhammad Sajidur and Peeters, Christian and Gibson, Cassidy and Garcia, Washington and Bindschaedler, Vincent and Shrimpton, Thomas and Traynor, Patrick},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {672--688},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/abdullah21a/abdullah21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/abdullah21a.html},\n  abstract = \t {Automatic Speech Recognition (ASR) systems convert speech into text and can be placed into two broad categories: traditional and fully end-to-end. Both types have been shown to be vulnerable to adversarial audio examples that sound benign to the human ear but force the ASR to produce malicious transcriptions. Of these attacks, only the \u201cpsychoacoustic\u201d attacks can create examples with relatively imperceptible perturbations, as they leverage the knowledge of the human auditory system. Unfortunately, existing psychoacoustic attacks can only be applied against traditional models, and are obsolete against the newer, fully end-to-end ASRs. In this paper, we propose an equalization-based psychoacoustic attack that can exploit both traditional and fully end-to-end ASRs. We successfully demonstrate our attack against real-world ASRs that include DeepSpeech and Wav2Letter. Moreover, we employ a user study to verify that our method creates low audible distortion. Specifically, 80 of the 100 participants voted in favor of\u00a0\\textit{all} our attack audio samples as less noisier than the existing state-of-the-art attack. Through this, we demonstrate both types of existing ASR pipelines can be exploited with minimum degradation to attack audio quality.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/abdullah21a/abdullah21a.pdf",
        "supp": "",
        "pdf_size": 584596,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6430915573037684340&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida",
        "aff_domain": "ufl.edu;ufl.edu;ufl.edu;ufl.edu;ufl.edu;cise.ufl.edu;ufl.edu;ufl.edu",
        "email": "ufl.edu;ufl.edu;ufl.edu;ufl.edu;ufl.edu;cise.ufl.edu;ufl.edu;ufl.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bias-tolerant Fair Classification",
        "site": "https://proceedings.mlr.press/v157/zhang21d.html",
        "author": "Yixuan Zhang; Feng Zhou; Zhidong Li; Yang Wang; Fang Chen",
        "abstract": "The label bias and selection bias are acknowledged as two reasons in data that will hinder the fairness of machine-learning outcomes. The label bias occurs when the labeling decision is disturbed by sensitive features, while the selection bias occurs when subjective bias exists during the data sampling. Even worse, models trained on such data can inherit or even intensify the discrimination. Most algorithmic fairness approaches perform an empirical risk minimization with predefined fairness constraints, which tends to trade-off accuracy for fairness. However, such methods would achieve the desired fairness level with the sacrifice of the benefits (receive positive outcomes) for individuals affected by the bias. Therefore, we propose a \\textbf{B}ias-Tolerant \\textbf{FA}ir \\textbf{R}egularized \\textbf{L}oss (B-FARL), which tries to regain the benefits using data affected by label bias and selection bias. B-FARL takes the biased data as input, calls a model that approximates the one trained with fair but latent data, and thus prevents discrimination without constraints required. In addition, we show the effective components by decomposing B-FARL, and we utilize the meta-learning framework for the B-FARL optimization. The experimental results on real-world datasets show that our method is empirically effective in improving fairness towards the direction of true but latent labels.",
        "bibtex": "@InProceedings{pmlr-v157-zhang21d,\n  title = \t {Bias-tolerant Fair Classification},\n  author =       {Zhang, Yixuan and Zhou, Feng and Li, Zhidong and Wang, Yang and Chen, Fang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {840--855},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhang21d/zhang21d.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhang21d.html},\n  abstract = \t {The label bias and selection bias are acknowledged as two reasons in data that will hinder the fairness of machine-learning outcomes. The label bias occurs when the labeling decision is disturbed by sensitive features, while the selection bias occurs when subjective bias exists during the data sampling. Even worse, models trained on such data can inherit or even intensify the discrimination. Most algorithmic fairness approaches perform an empirical risk minimization with predefined fairness constraints, which tends to trade-off accuracy for fairness. However, such methods would achieve the desired fairness level with the sacrifice of the benefits (receive positive outcomes) for individuals affected by the bias. Therefore, we propose a \\textbf{B}ias-Tolerant \\textbf{FA}ir \\textbf{R}egularized \\textbf{L}oss (B-FARL), which tries to regain the benefits using data affected by label bias and selection bias. B-FARL takes the biased data as input, calls a model that approximates the one trained with fair but latent data, and thus prevents discrimination without constraints required. In addition, we show the effective components by decomposing B-FARL, and we utilize the meta-learning framework for the B-FARL optimization. The experimental results on real-world datasets show that our method is empirically effective in improving fairness towards the direction of true but latent labels.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhang21d/zhang21d.pdf",
        "supp": "",
        "pdf_size": 1435245,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15167744808650487697&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Data Science Institute, University of Technology Sydney, Australia; Department of Computer Science and Technology, Tsinghua University, China; Data Science Institute, University of Technology Sydney, Australia; Data Science Institute, University of Technology Sydney, Australia; Data Science Institute, University of Technology Sydney, Australia",
        "aff_domain": "student.uts.edu.au;tsinghua.edu.cn;uts.edu.au;uts.edu.au;uts.edu.au",
        "email": "student.uts.edu.au;tsinghua.edu.cn;uts.edu.au;uts.edu.au;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of Technology Sydney;Tsinghua University",
        "aff_unique_dep": "Data Science Institute;Department of Computer Science and Technology",
        "aff_unique_url": "https://www.uts.edu.au;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UTS;THU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Sydney;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "Australia;China"
    },
    {
        "title": "Bridging Code-Text Representation Gap using Explanation",
        "site": "https://proceedings.mlr.press/v157/han21a.html",
        "author": "Hojae Han; Youngwon Lee; Minsoo Kim; Hwang Seung-won",
        "abstract": "This paper studies Code-Text Representation (CTR) learning, aiming to learn general-purpose representations that support downstream code/text applications such as code search, finding code matching textual queries. However, state-of-the-arts do not focus on matching the gap between code/text modalities. In this paper, we complement this gap by providing an intermediate representation, and view it as  \u201cexplanation.\u201d Our contribution is three fold: First, we propose four types of explanation utilization methods for CTR, and compare their effectiveness. Second, we show that using explanation as the model input is desirable. Third, we confirm that even automatically generated explanation can lead to a drastic performance gain. To the best of our knowledge, this is the first work to define and categorize code explanation, for enhancing code understanding/representation.",
        "bibtex": "@InProceedings{pmlr-v157-han21a,\n  title = \t {Bridging Code-Text Representation Gap using Explanation},\n  author =       {Han, Hojae and Lee, Youngwon and Kim, Minsoo and Seung-won, Hwang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1033--1048},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/han21a/han21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/han21a.html},\n  abstract = \t {This paper studies Code-Text Representation (CTR) learning, aiming to learn general-purpose representations that support downstream code/text applications such as code search, finding code matching textual queries. However, state-of-the-arts do not focus on matching the gap between code/text modalities. In this paper, we complement this gap by providing an intermediate representation, and view it as  \u201cexplanation.\u201d Our contribution is three fold: First, we propose four types of explanation utilization methods for CTR, and compare their effectiveness. Second, we show that using explanation as the model input is desirable. Third, we confirm that even automatically generated explanation can lead to a drastic performance gain. To the best of our knowledge, this is the first work to define and categorize code explanation, for enhancing code understanding/representation.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/han21a/han21a.pdf",
        "supp": "",
        "pdf_size": 346711,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Q12p5SqzgtwJ:scholar.google.com/&scioq=Bridging+Code-Text+Representation+Gap+using+Explanation&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Seoul National University; Seoul National University; Yonsei University; Seoul National University+Yonsei University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;yonsei.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;yonsei.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0+1",
        "aff_unique_norm": "Seoul National University;Yonsei University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "SNU;Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Building Decision Tree for Imbalanced Classification via Deep Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/wen21a.html",
        "author": "Guixuan Wen; Kaigui Wu",
        "abstract": "Data imbalance is prevalent in classification problems and tends to bias the classifier towards the majority of classes. This paper proposes a decision tree building method for imbalanced binary classification via deep reinforcement learning. First, the decision tree building process is regarded as a multi-step game and modeled as a Markov decision process. Then, the tree-based convolution is applied to extract state vectors from the tree structure, and each node is abstracted into a parameterized action. Next, the reward function is designed based on a range of evaluation metrics of imbalanced classification. Finally, a popular deep reinforcement learning algorithm called Multi-Pass DQN is employed to find an optimal decision tree building policy. The experiments on more than 15 imbalanced data sets indicate that our method outperforms the state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v157-wen21a,\n  title = \t {Building Decision Tree for Imbalanced Classification via Deep Reinforcement Learning},\n  author =       {Wen, Guixuan and Wu, Kaigui},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1645--1659},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wen21a/wen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wen21a.html},\n  abstract = \t {Data imbalance is prevalent in classification problems and tends to bias the classifier towards the majority of classes. This paper proposes a decision tree building method for imbalanced binary classification via deep reinforcement learning. First, the decision tree building process is regarded as a multi-step game and modeled as a Markov decision process. Then, the tree-based convolution is applied to extract state vectors from the tree structure, and each node is abstracted into a parameterized action. Next, the reward function is designed based on a range of evaluation metrics of imbalanced classification. Finally, a popular deep reinforcement learning algorithm called Multi-Pass DQN is employed to find an optimal decision tree building policy. The experiments on more than 15 imbalanced data sets indicate that our method outperforms the state-of-the-art methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wen21a/wen21a.pdf",
        "supp": "",
        "pdf_size": 449000,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14505700353127488241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science, Chongqing University, Chongqing, 400044 China; College of Computer Science, Chongqing University, Chongqing, 400044 China",
        "aff_domain": "cqu.edu.cn;cqu.edu.cn",
        "email": "cqu.edu.cn;cqu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chongqing University",
        "aff_unique_dep": "College of Computer Science",
        "aff_unique_url": "http://en.cqu.edu.cn/",
        "aff_unique_abbr": "CQU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chongqing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "CTAB-GAN: Effective Table Data Synthesizing",
        "site": "https://proceedings.mlr.press/v157/zhao21a.html",
        "author": "Zilong Zhao; Aditya Kunar; Robert Birke; Lydia Y. Chen",
        "abstract": "While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation (GDPR)) unfortunately limit its full effectiveness. Synthetic tabular data emerges as an alternative to enable data sharing while fulfilling regulatory and privacy constraints. The state-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks (GAN) and address two main data types in industry, i.e., continuous and categorical. In this paper, we develop CTAB-GAN, a novel conditional table GAN architecture that can effectively model diverse data types, including a mix of continuous and categorical variables. Moreover, we address data imbalance and long tail issues, i.e., certain variables have drastic frequency differences across large values. To achieve those aims, we first introduce the information loss, classification loss and generator loss to the conditional GAN. Secondly, we design a novel conditional vector, which efficiently encodes the mixed data type and skewed distribution of data variable. We extensively evaluate CTAB-GAN with the state of the art GANs that generate synthetic tables, in terms of data similarity and analysis utility. The results on five datasets show that the synthetic data of CTAB-GAN remarkably resembles the real data for all three types of variables and results into higher accuracy for five machine learning algorithms, by up to 17%.",
        "bibtex": "@InProceedings{pmlr-v157-zhao21a,\n  title = \t {CTAB-GAN: Effective Table Data Synthesizing},\n  author =       {Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y.},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {97--112},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhao21a/zhao21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhao21a.html},\n  abstract = \t {While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation (GDPR)) unfortunately limit its full effectiveness. Synthetic tabular data emerges as an alternative to enable data sharing while fulfilling regulatory and privacy constraints. The state-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks (GAN) and address two main data types in industry, i.e., continuous and categorical. In this paper, we develop CTAB-GAN, a novel conditional table GAN architecture that can effectively model diverse data types, including a mix of continuous and categorical variables. Moreover, we address data imbalance and long tail issues, i.e., certain variables have drastic frequency differences across large values. To achieve those aims, we first introduce the information loss, classification loss and generator loss to the conditional GAN. Secondly, we design a novel conditional vector, which efficiently encodes the mixed data type and skewed distribution of data variable. We extensively evaluate CTAB-GAN with the state of the art GANs that generate synthetic tables, in terms of data similarity and analysis utility. The results on five datasets show that the synthetic data of CTAB-GAN remarkably resembles the real data for all three types of variables and results into higher accuracy for five machine learning algorithms, by up to 17%.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhao21a/zhao21a.pdf",
        "supp": "",
        "pdf_size": 3569593,
        "gs_citation": 290,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15524113667381167028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "TU Delft, Delft, The Netherlands; TU Delft, Delft, The Netherlands; ABB Research Switzerland, D\u00a8 attwil, Switzerland; TU Delft, Delft, The Netherlands",
        "aff_domain": "tudelft.nl;student.tudelft.nl;ch.abb.com;ieee.org",
        "email": "tudelft.nl;student.tudelft.nl;ch.abb.com;ieee.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Delft University of Technology;ABB Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tudelft.nl;https://new.abb.com/research",
        "aff_unique_abbr": "TU Delft;ABB",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Delft;D\u00e4ttwil",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Netherlands;Switzerland"
    },
    {
        "title": "CTS2: Time Series Smoothing with Constrained Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/liu21b.html",
        "author": "Yongshuai Liu; Xin Liu",
        "abstract": "Time series smoothing is essential for time series analysis and forecasting. It helps to identify trends and patterns of time series. However, the presence of irregular perturbations disrupt the time series smoothness and distort information. The goal of time series smoothing is to remove these perturbations while preserving as much information as possible. Existing smoothing algorithms have complete freedom to make corrections to the data points which often over smooth the time series and lose information. None of them considers constraining data corrections to the best of our knowledge. Moreover, most existing methods either do not smooth in real-time or their parameters need to be hand-tuned in different scenarios. To improve smoothing performance while considering data correction constraints, we propose a $\\mathbf{C}$onstrained reinforcement learning-based  $\\mathbf{T}$ime  $\\mathbf{S}$eries  $\\mathbf{S}$moothing method, or CTS$^2$. Specifically, we first formulate the smoothing problem as a Constrained Markov Decision Process (CMDP). We then incorporate  data correction constraints to restrict the amount of correction at each point. Finally, we learn a policy network with a linear projection layer to smooth the time series. The linear projection layer ensures that all data corrections satisfy the data correction constraints. We evaluate CTS$^2$ on both synthetic and real-world time series datasets; our results show that CTS$^2$ successfully smooths time series in real-time, satisfies all the correction constraints, and works efficiently in a variety of scenarios.",
        "bibtex": "@InProceedings{pmlr-v157-liu21b,\n  title = \t {CTS2: Time Series Smoothing with Constrained Reinforcement Learning},\n  author =       {Liu, Yongshuai and Liu, Xin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {363--378},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/liu21b/liu21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/liu21b.html},\n  abstract = \t {Time series smoothing is essential for time series analysis and forecasting. It helps to identify trends and patterns of time series. However, the presence of irregular perturbations disrupt the time series smoothness and distort information. The goal of time series smoothing is to remove these perturbations while preserving as much information as possible. Existing smoothing algorithms have complete freedom to make corrections to the data points which often over smooth the time series and lose information. None of them considers constraining data corrections to the best of our knowledge. Moreover, most existing methods either do not smooth in real-time or their parameters need to be hand-tuned in different scenarios. To improve smoothing performance while considering data correction constraints, we propose a $\\mathbf{C}$onstrained reinforcement learning-based  $\\mathbf{T}$ime  $\\mathbf{S}$eries  $\\mathbf{S}$moothing method, or CTS$^2$. Specifically, we first formulate the smoothing problem as a Constrained Markov Decision Process (CMDP). We then incorporate  data correction constraints to restrict the amount of correction at each point. Finally, we learn a policy network with a linear projection layer to smooth the time series. The linear projection layer ensures that all data corrections satisfy the data correction constraints. We evaluate CTS$^2$ on both synthetic and real-world time series datasets; our results show that CTS$^2$ successfully smooths time series in real-time, satisfies all the correction constraints, and works efficiently in a variety of scenarios.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/liu21b/liu21b.pdf",
        "supp": "",
        "pdf_size": 972899,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15901546321765316505&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The University of California, Davis; The University of California, Davis",
        "aff_domain": "ucdavis.edu;ucdavis.edu",
        "email": "ucdavis.edu;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cautious Actor-Critic",
        "site": "https://proceedings.mlr.press/v157/zhu21a.html",
        "author": "Lingwei Zhu; Toshinori Kitamura; Matsubara Takamitsu",
        "abstract": "The oscillating performance of off-policy learning and persisting errors in the actor-critic(AC) setting call for algorithms that can conservatively learn to suit the stability-critical applications  better.   In  this  paper,  we  propose  a  novel  off-policy  AC  algorithm  cautious actor-critic (CAC). The name cautious comes from the doubly conservative nature that we exploit the classic policy interpolation from conservative policy iteration for the actor and the entropy-regularization of conservative value iteration for the critic.  Our key observation is the entropy-regularized critic facilitates and simplifies the unwieldy interpolated actor update while still ensuring robust policy improvement.  We compare CAC to state-of-the-art AC methods on a set of challenging continuous control problems and demonstrate thatCAC achieves comparable performance while significantly stabilizes learning.",
        "bibtex": "@InProceedings{pmlr-v157-zhu21a,\n  title = \t {Cautious Actor-Critic},\n  author =       {Zhu, Lingwei and Kitamura, Toshinori and Takamitsu, Matsubara},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {220--235},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhu21a/zhu21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhu21a.html},\n  abstract = \t {The oscillating performance of off-policy learning and persisting errors in the actor-critic(AC) setting call for algorithms that can conservatively learn to suit the stability-critical applications  better.   In  this  paper,  we  propose  a  novel  off-policy  AC  algorithm  cautious actor-critic (CAC). The name cautious comes from the doubly conservative nature that we exploit the classic policy interpolation from conservative policy iteration for the actor and the entropy-regularization of conservative value iteration for the critic.  Our key observation is the entropy-regularized critic facilitates and simplifies the unwieldy interpolated actor update while still ensuring robust policy improvement.  We compare CAC to state-of-the-art AC methods on a set of challenging continuous control problems and demonstrate thatCAC achieves comparable performance while significantly stabilizes learning.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhu21a/zhu21a.pdf",
        "supp": "",
        "pdf_size": 1379134,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17448832973172517004&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Nara Institute of Science and Technology, JAPAN; Nara Institute of Science and Technology, JAPAN; Nara Institute of Science and Technology, JAPAN",
        "aff_domain": "gmail.com;is.naist.jp;is.naist.jp",
        "email": "gmail.com;is.naist.jp;is.naist.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nist.jp",
        "aff_unique_abbr": "NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Collaborative Novelty Detection for Distributed Data by a Probabilistic Method",
        "site": "https://proceedings.mlr.press/v157/imakura21a.html",
        "author": "Akira Imakura; Xiucai Ye; Tetsuya Sakurai",
        "abstract": "Novelty detection, which detects anomalies based on a training dataset consisting of only the normal data, is an important task in several applications. In addition, in the real world, there may be situations where data is owned by multiple parties in a distributed manner but cannot be shared with each other due to privacy and confidentiality requirements. Therefore, how to develop distributed novelty detection while preserving privacy is essential. To address this challenge, we propose a probabilistic collaborative method that allows distributed novelty detection for multiple parties without sharing the original data. The proposed method constructs a collaborative kernel based on a collaborative data analysis framework, by which intermediate representations are generated from each party and shared for collaborative novelty detection. Numerical experiments demonstrate that the proposed method obtains better performance compared with the individual novelty detection in the local party.",
        "bibtex": "@InProceedings{pmlr-v157-imakura21a,\n  title = \t {Collaborative Novelty Detection for Distributed Data by a Probabilistic Method},\n  author =       {Imakura, Akira and Ye, Xiucai and Sakurai, Tetsuya},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {932--947},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/imakura21a/imakura21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/imakura21a.html},\n  abstract = \t {Novelty detection, which detects anomalies based on a training dataset consisting of only the normal data, is an important task in several applications. In addition, in the real world, there may be situations where data is owned by multiple parties in a distributed manner but cannot be shared with each other due to privacy and confidentiality requirements. Therefore, how to develop distributed novelty detection while preserving privacy is essential. To address this challenge, we propose a probabilistic collaborative method that allows distributed novelty detection for multiple parties without sharing the original data. The proposed method constructs a collaborative kernel based on a collaborative data analysis framework, by which intermediate representations are generated from each party and shared for collaborative novelty detection. Numerical experiments demonstrate that the proposed method obtains better performance compared with the individual novelty detection in the local party.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/imakura21a/imakura21a.pdf",
        "supp": "",
        "pdf_size": 1395927,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17279979016058536469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Tsukuba; University of Tsukuba; University of Tsukuba",
        "aff_domain": "cs.tsukuba.ac.jp;cs.tsukuba.ac.jp;cs.tsukuba.ac.jp",
        "email": "cs.tsukuba.ac.jp;cs.tsukuba.ac.jp;cs.tsukuba.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tsukuba",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tsukuba.ac.jp",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Contrastive Neural Processes for Self-Supervised Learning",
        "site": "https://proceedings.mlr.press/v157/kallidromitis21a.html",
        "author": "Konstantinos Kallidromitis; Denis Gudovskiy; Kozuka Kazuki; Ohama Iku; Luca Rigazio",
        "abstract": "Recent contrastive methods show significant improvement in self-supervised learning in several domains. In particular, contrastive methods are most effective where data augmentation can be easily constructed e.g. in computer vision. However, they are less successful in domains without established data transformations such as time series data. In this paper, we propose a novel self-supervised learning framework that combines contrastive learning with neural processes. It relies on recent advances in neural processes to perform time series forecasting. This allows to generate augmented versions of data by employing a set of various sampling functions and, hence, avoid manually designed augmentations. We extend conventional neural processes and propose a new contrastive loss to learn times series representations in a self-supervised setup. Therefore, unlike previous self-supervised methods, our augmentation pipeline is task-agnostic, enabling our method to perform well across various applications. In particular, a ResNet with a linear classifier trained using our approach is able to outperform state-of-the-art techniques across industrial, medical and audio datasets improving accuracy over 10% in ECG periodic data. We further demonstrate that our self-supervised representations are more efficient in the latent space, improving multiple clustering indexes and that fine-tuning our method on 10% of labels achieves results competitive to fully-supervised learning.",
        "bibtex": "@InProceedings{pmlr-v157-kallidromitis21a,\n  title = \t {Contrastive Neural Processes for Self-Supervised Learning},\n  author =       {Kallidromitis, Konstantinos and Gudovskiy, Denis and Kazuki, Kozuka and Iku, Ohama and Rigazio, Luca},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {594--609},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/kallidromitis21a/kallidromitis21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/kallidromitis21a.html},\n  abstract = \t {Recent contrastive methods show significant improvement in self-supervised learning in several domains. In particular, contrastive methods are most effective where data augmentation can be easily constructed e.g. in computer vision. However, they are less successful in domains without established data transformations such as time series data. In this paper, we propose a novel self-supervised learning framework that combines contrastive learning with neural processes. It relies on recent advances in neural processes to perform time series forecasting. This allows to generate augmented versions of data by employing a set of various sampling functions and, hence, avoid manually designed augmentations. We extend conventional neural processes and propose a new contrastive loss to learn times series representations in a self-supervised setup. Therefore, unlike previous self-supervised methods, our augmentation pipeline is task-agnostic, enabling our method to perform well across various applications. In particular, a ResNet with a linear classifier trained using our approach is able to outperform state-of-the-art techniques across industrial, medical and audio datasets improving accuracy over 10% in ECG periodic data. We further demonstrate that our self-supervised representations are more efficient in the latent space, improving multiple clustering indexes and that fine-tuning our method on 10% of labels achieves results competitive to fully-supervised learning.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/kallidromitis21a/kallidromitis21a.pdf",
        "supp": "",
        "pdf_size": 2748893,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13310330823152268185&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Panasonic AI Lab, USA; Panasonic AI Lab, USA; Panasonic Technology Division, Japan; Panasonic AI Lab, USA; AIoli Labs, USA",
        "aff_domain": "us.panasonic.com;us.panasonic.com;jp.panasonic.com;us.panasonic.com;aiolilabs.com",
        "email": "us.panasonic.com;us.panasonic.com;jp.panasonic.com;us.panasonic.com;aiolilabs.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Panasonic AI Lab;Panasonic;AIoli Labs",
        "aff_unique_dep": "AI Lab;Technology Division;",
        "aff_unique_url": "https://www.panasonic.com/global/innovation/research-development/ai.html;https://panasonic.com;",
        "aff_unique_abbr": "Panasonic AI Lab;Panasonic;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "ContriQ: Ally-Focused Cooperation and Enemy-Concentrated Confrontation in Multi-Agent Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/chenran21a.html",
        "author": "Zhao Chenran; Shi Dianxi; Zhang Yaowen; Yang Huanhuan; Yang Shaowu; Zhang Yongjun",
        "abstract": "Centralized training with decentralized execution (CTDE) is an important setting for cooperative multi-agent reinforcement learning (MARL) due to communication constraints during execution and scalability constraints during training, which has shown superior performance but still suffers from challenges. One branch is to understand the mutual interplay between agents. Due to the communication constraints in practice, agents cannot exchange perceptual information, and thus, many approaches use a centralized attention network with scalability constraints. Contrary to these common approaches, we propose to learn to cooperate in a decentralized way by applying attention mechanism on the local observation so that each agent could focus on allied agents with a decentralized model, and therefore promote understanding. Another branch is to model how agents cooperate and simplify the learning process. Previous approaches that focus on value decomposition have achieved innovative results but still suffer from problems. These approaches either limit the representation expressiveness of their value function classes or relax the IGM consistency to achieve scalability, which may lead to poor performance. We combine value composition with game abstraction by modeling the relationships between agents as a bi-level graph. We propose a novel value decomposition network based on it through a bi-level attention network, which indicates the contribution of allied agents attacking enemies and the priority of attacking each enemy under the situation of each time step, respectively. We show that our method substantially outperforms existing state-of-the-art methods on battle games in StarCraft \u2161, and attention analysis is also comprehensively discussed with sights.",
        "bibtex": "@InProceedings{pmlr-v157-chenran21a,\n  title = \t {ContriQ: Ally-Focused Cooperation and Enemy-Concentrated Confrontation in Multi-Agent Reinforcement Learning},\n  author =       {Chenran, Zhao and Dianxi, Shi and Yaowen, Zhang and Huanhuan, Yang and Shaowu, Yang and Yongjun, Zhang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1049--1064},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/chenran21a/chenran21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/chenran21a.html},\n  abstract = \t {Centralized training with decentralized execution (CTDE) is an important setting for cooperative multi-agent reinforcement learning (MARL) due to communication constraints during execution and scalability constraints during training, which has shown superior performance but still suffers from challenges. One branch is to understand the mutual interplay between agents. Due to the communication constraints in practice, agents cannot exchange perceptual information, and thus, many approaches use a centralized attention network with scalability constraints. Contrary to these common approaches, we propose to learn to cooperate in a decentralized way by applying attention mechanism on the local observation so that each agent could focus on allied agents with a decentralized model, and therefore promote understanding. Another branch is to model how agents cooperate and simplify the learning process. Previous approaches that focus on value decomposition have achieved innovative results but still suffer from problems. These approaches either limit the representation expressiveness of their value function classes or relax the IGM consistency to achieve scalability, which may lead to poor performance. We combine value composition with game abstraction by modeling the relationships between agents as a bi-level graph. We propose a novel value decomposition network based on it through a bi-level attention network, which indicates the contribution of allied agents attacking enemies and the priority of attacking each enemy under the situation of each time step, respectively. We show that our method substantially outperforms existing state-of-the-art methods on battle games in StarCraft \u2161, and attention analysis is also comprehensively discussed with sights.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/chenran21a/chenran21a.pdf",
        "supp": "",
        "pdf_size": 7925429,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ATpZj2yCploJ:scholar.google.com/&scioq=ContriQ:+Ally-Focused+Cooperation+and+Enemy-Concentrated+Confrontation+in+Multi-Agent+Reinforcement+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Convolutional Hypercomplex Embeddings for Link Prediction",
        "site": "https://proceedings.mlr.press/v157/demir21a.html",
        "author": "Caglar Demir; Diego Moussallem; Stefan Heindorf; Axel-Cyrille Ngonga Ngomo",
        "abstract": "Knowledge graph embedding research has mainly focused on the two smallest normed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest that trilinear products of quaternion-valued embeddings can be a more effective means to tackle link prediction. In addition, models based on convolutions on real-valued embeddings often yield state-of-the-art results for link prediction. In this paper, we investigate a composition of convolution operations with hypercomplex multiplications. We propose the four approaches QMult, OMult, ConvQ and ConvO  to tackle the link prediction problem. QMult and OMult can be considered as quaternion and octonion extensions of previous state-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO build upon QMult and OMult by including convolution operations in a way inspired by the residual learning framework. We evaluated our approaches on seven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10. Experimental results suggest that the benefits of learning hypercomplex-valued vector representations become more apparent as the size and complexity of the knowledge graph grows. ConvO outperforms state-of-the-art approaches on FB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO outperform state-of-the-approaches on YAGO3-10 in all metrics. Results also suggest that link prediction performances can be further improved via prediction averaging. To foster reproducible research, we provide an open-source implementation of approaches, including training and evaluation scripts as well as pretrained models.",
        "bibtex": "@InProceedings{pmlr-v157-demir21a,\n  title = \t {Convolutional Hypercomplex Embeddings for Link Prediction},\n  author =       {Demir, Caglar and Moussallem, Diego and Heindorf, Stefan and Ngonga Ngomo, Axel-Cyrille},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {656--671},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/demir21a/demir21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/demir21a.html},\n  abstract = \t {Knowledge graph embedding research has mainly focused on the two smallest normed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest that trilinear products of quaternion-valued embeddings can be a more effective means to tackle link prediction. In addition, models based on convolutions on real-valued embeddings often yield state-of-the-art results for link prediction. In this paper, we investigate a composition of convolution operations with hypercomplex multiplications. We propose the four approaches QMult, OMult, ConvQ and ConvO  to tackle the link prediction problem. QMult and OMult can be considered as quaternion and octonion extensions of previous state-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO build upon QMult and OMult by including convolution operations in a way inspired by the residual learning framework. We evaluated our approaches on seven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10. Experimental results suggest that the benefits of learning hypercomplex-valued vector representations become more apparent as the size and complexity of the knowledge graph grows. ConvO outperforms state-of-the-art approaches on FB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO outperform state-of-the-approaches on YAGO3-10 in all metrics. Results also suggest that link prediction performances can be further improved via prediction averaging. To foster reproducible research, we provide an open-source implementation of approaches, including training and evaluation scripts as well as pretrained models.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/demir21a/demir21a.pdf",
        "supp": "",
        "pdf_size": 617699,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1131699149834915808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Data Science Research Group, Paderborn University; Data Science Research Group, Paderborn University + Globo, Rio de Janeiro, Brazil; Data Science Research Group, Paderborn University; Data Science Research Group, Paderborn University",
        "aff_domain": "upb.de;upb.de;upb.de;upb.de",
        "email": "upb.de;upb.de;upb.de;upb.de",
        "github": "https://github.com/dice-group/Convolutional-Hypercomplex-Embeddings-for-Link-Prediction",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Paderborn University;Globo",
        "aff_unique_dep": "Data Science Research Group;",
        "aff_unique_url": "https://www.uni-paderborn.de;https://www.globo.com",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rio de Janeiro",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "Germany;Brazil"
    },
    {
        "title": "Cross-structural Factor-topic Model: Document Analysis with Sophisticated Covariates",
        "site": "https://proceedings.mlr.press/v157/lu21a.html",
        "author": "Chien Lu; Jaakko Peltonen; Timo Nummenmaa; Jyrki Nummenmaa; Kalervo J\u00e4rvelin",
        "abstract": "Modern text data is increasingly gathered in situations where it is paired with a  high-dimensional collection of covariates:  then both the text, the covariates, and their relationships are of interest to analyze. Despite the growing amount of such data, current topic models are unable to take into account large amounts of covariates successfully:  they fail to model structure among covariates and distort findings of both text and covariates. This paper presents a solution: a novel factor-topic model that enables researchers to analyze latent structure in both text and sophisticated document-level covariates collectively. The key innovation is that besides learning the underlying topical structure,  the model also learns the underlying factorial structure from the covariates and the interactions between the two structures.  A set of tailored variational inference algorithms for efficient computation are provided.  Experiments on three different datasets show the model outperforms comparable topic models in the ability to predict held-out document content.  Two case studies focusing on Finnish parliamentary election candidates and game players on Steam demonstrate the model discovers semantically meaningful topics, factors, and their interactions.  The model both outperforms state-of-the-art models in predictive accuracy and offers new factor-topic insights beyond other topic models.",
        "bibtex": "@InProceedings{pmlr-v157-lu21a,\n  title = \t {Cross-structural Factor-topic Model: Document Analysis with Sophisticated Covariates},\n  author =       {Lu, Chien and Peltonen, Jaakko and Nummenmaa, Timo and Nummenmaa, Jyrki and J\\\"arvelin, Kalervo},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1129--1144},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/lu21a/lu21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/lu21a.html},\n  abstract = \t {Modern text data is increasingly gathered in situations where it is paired with a  high-dimensional collection of covariates:  then both the text, the covariates, and their relationships are of interest to analyze. Despite the growing amount of such data, current topic models are unable to take into account large amounts of covariates successfully:  they fail to model structure among covariates and distort findings of both text and covariates. This paper presents a solution: a novel factor-topic model that enables researchers to analyze latent structure in both text and sophisticated document-level covariates collectively. The key innovation is that besides learning the underlying topical structure,  the model also learns the underlying factorial structure from the covariates and the interactions between the two structures.  A set of tailored variational inference algorithms for efficient computation are provided.  Experiments on three different datasets show the model outperforms comparable topic models in the ability to predict held-out document content.  Two case studies focusing on Finnish parliamentary election candidates and game players on Steam demonstrate the model discovers semantically meaningful topics, factors, and their interactions.  The model both outperforms state-of-the-art models in predictive accuracy and offers new factor-topic insights beyond other topic models.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/lu21a/lu21a.pdf",
        "supp": "",
        "pdf_size": 1473061,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1677280933072604590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Faculty of Information Technology and Communication Sciences, Tampere University, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Finland",
        "aff_domain": "tuni.fi;tuni.fi;tuni.fi;tuni.fi;tuni.fi",
        "email": "tuni.fi;tuni.fi;tuni.fi;tuni.fi;tuni.fi",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tampere University",
        "aff_unique_dep": "Faculty of Information Technology and Communication Sciences",
        "aff_unique_url": "https://www.tuni.fi",
        "aff_unique_abbr": "Tuni",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "DAGSurv: Directed Ayclic Graph Based Survival Analysis Using Deep Neural Networks",
        "site": "https://proceedings.mlr.press/v157/sharma21a.html",
        "author": "Ansh Kumar Sharma; Rahul Kukreja; Ranjitha Prasad; Shilpa Rao",
        "abstract": "Causal structures for observational survival data provide crucial information regarding the relationships between covariates and time-to-event. We derive motivation from the information theoretic source coding argument, and show that incorporating the knowledge of the directed acyclic graph (DAG) can be beneficial if suitable source encoders are employed. As a possible source encoder in this context, we derive a variational inference based conditional variational autoencoder for causal structured survival prediction, which we refer to as \\texttt{DAGSurv}. We illustrate the performance of \\texttt{DAGSurv} on low and high-dimensional synthetic datasets, and real-world datasets such as METABRIC and GBSG. We demonstrate that the proposed method outperforms other survival analysis baselines such as \\texttt{Cox} Proportional Hazards, \\texttt{DeepSurv} and \\texttt{Deephit}, which are oblivious to the underlying causal relationship between data entities.",
        "bibtex": "@InProceedings{pmlr-v157-sharma21a,\n  title = \t {DAGSurv: Directed Ayclic Graph Based Survival Analysis Using Deep Neural Networks},\n  author =       {Sharma, Ansh Kumar and Kukreja, Rahul and Prasad, Ranjitha and Rao, Shilpa},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1065--1080},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/sharma21a/sharma21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/sharma21a.html},\n  abstract = \t {Causal structures for observational survival data provide crucial information regarding the relationships between covariates and time-to-event. We derive motivation from the information theoretic source coding argument, and show that incorporating the knowledge of the directed acyclic graph (DAG) can be beneficial if suitable source encoders are employed. As a possible source encoder in this context, we derive a variational inference based conditional variational autoencoder for causal structured survival prediction, which we refer to as \\texttt{DAGSurv}. We illustrate the performance of \\texttt{DAGSurv} on low and high-dimensional synthetic datasets, and real-world datasets such as METABRIC and GBSG. We demonstrate that the proposed method outperforms other survival analysis baselines such as \\texttt{Cox} Proportional Hazards, \\texttt{DeepSurv} and \\texttt{Deephit}, which are oblivious to the underlying causal relationship between data entities.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/sharma21a/sharma21a.pdf",
        "supp": "",
        "pdf_size": 739098,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2019438423984835011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "ECE dept., IIIT Delhi; ECE dept., IIIT Delhi; ECE dept., IIIT Delhi; ECE dept., IIIT Guwahati",
        "aff_domain": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitg.ac.in",
        "email": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitg.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "IIIT Delhi;IIIT Guwahati",
        "aff_unique_dep": "ECE dept.;ECE dept.",
        "aff_unique_url": "https://www.iiitdelhi.ac.in;https://www.iiitg.ac.in",
        "aff_unique_abbr": "IIITD;IIITG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "DDSAS: Dynamic and Differentiable Space-Architecture Search",
        "site": "https://proceedings.mlr.press/v157/yang21a.html",
        "author": "Longxing Yang; Yu Hu; Shun Lu; Zihao Sun; Jilin Mei; Yiming Zeng; Zhiping Shi; Yinhe Han; Xiaowei Li",
        "abstract": "Neural Architecture Search (NAS) has made remarkable progress in automatically designing neural networks. However, existing differentiable NAS and stochastic NAS methods are either biased towards exploitation and thus may converge to a local minimum, or biased towards exploration and thus converge slowly. In this work, we propose a Dynamic and Differentiable Space-Architecture Search (DDSAS) method to address the exploration-exploitation dilemma. DDSAS dynamically samples space, searches architectures in the sampled subspace with gradient descent, and leverages the Upper Confidence Bound (UCB) to balance exploitation and exploration. The whole search space is elastic, offering flexibility to evolve and to consider resource constraints. Experiments on image classification datasets demonstrate that with only 4GB memory and 3 hours for searching, DDSAS achieves 2.39% test error on CIFAR10, 16.26% test error on CIFAR100, and 23.9% test error when transferring to ImageNet. When directly searching on ImageNet, DDSAS achieves comparable accuracy with more than 6.5 times speedup over state-of-the-art methods. The source codes are available at https://github.com/xingxing-123/DDSAS.",
        "bibtex": "@InProceedings{pmlr-v157-yang21a,\n  title = \t {DDSAS: Dynamic and Differentiable Space-Architecture Search},\n  author =       {Yang, Longxing and Hu, Yu and Lu, Shun and Sun, Zihao and Mei, Jilin and Zeng, Yiming and Shi, Zhiping and Han, Yinhe and Li, Xiaowei},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {284--299},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/yang21a/yang21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/yang21a.html},\n  abstract = \t {Neural Architecture Search (NAS) has made remarkable progress in automatically designing neural networks. However, existing differentiable NAS and stochastic NAS methods are either biased towards exploitation and thus may converge to a local minimum, or biased towards exploration and thus converge slowly. In this work, we propose a Dynamic and Differentiable Space-Architecture Search (DDSAS) method to address the exploration-exploitation dilemma. DDSAS dynamically samples space, searches architectures in the sampled subspace with gradient descent, and leverages the Upper Confidence Bound (UCB) to balance exploitation and exploration. The whole search space is elastic, offering flexibility to evolve and to consider resource constraints. Experiments on image classification datasets demonstrate that with only 4GB memory and 3 hours for searching, DDSAS achieves 2.39% test error on CIFAR10, 16.26% test error on CIFAR100, and 23.9% test error when transferring to ImageNet. When directly searching on ImageNet, DDSAS achieves comparable accuracy with more than 6.5 times speedup over state-of-the-art methods. The source codes are available at https://github.com/xingxing-123/DDSAS.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/yang21a/yang21a.pdf",
        "supp": "",
        "pdf_size": 1868626,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6765644522211648042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "1Research Center for Intelligent Computing Systems, Institute of Computing Technology, CAS + 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences; 1Research Center for Intelligent Computing Systems, Institute of Computing Technology, CAS + 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences; 1Research Center for Intelligent Computing Systems, Institute of Computing Technology, CAS + 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences; 1Research Center for Intelligent Computing Systems, Institute of Computing Technology, CAS + 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences; 1Research Center for Intelligent Computing Systems, Institute of Computing Technology, CAS + 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences; 4Tecent ADlab; 5Capital Normal University; 1Research Center for Intelligent Computing Systems, Institute of Computing Technology, CAS + 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences; 2State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS + 3School of Computer Science and Technology, University of Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;gmail.com;cnu.edu.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;gmail.com;cnu.edu.cn;ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/xingxing-123/DDSAS",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0+1;0+0+1;0+0+1;0+0+1;0+0+1;2;3;0+0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Tencent;Capital Normal University",
        "aff_unique_dep": "Institute of Computing Technology;School of Computer Science and Technology;Tencent AI Lab;",
        "aff_unique_url": "http://www.ict.cas.cn;http://www.ucas.ac.cn;https://ai.tencent.com;http://www.cnu.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;Tencent AI Lab;CNU",
        "aff_campus_unique_index": ";;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0;0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "DPOQ: Dynamic Precision Onion Quantization",
        "site": "https://proceedings.mlr.press/v157/li21a.html",
        "author": "Bowen Li; Kai Huang; Siang Chen; Dongliang Xiong; Luc Claesen",
        "abstract": "With the development of deployment platforms and application scenarios for deep neural networks, traditional fixed network architectures cannot meet the requirements. Meanwhile the dynamic network inference becomes a new research trend. Many slimmable and scalable networks have been proposed to satisfy different resource constraints (e.g., storage, latency and energy). And a single network may support versatile architectural configurations including: depth, width, kernel size, and resolution. In this paper, we propose a novel network architecture reuse strategy enabling dynamic precision in parameters. Since our low-precision networks are wrapped in the high-precision networks like an onion, we name it dynamic precision onion quantization (DPOQ). We train the network by using the joint loss with scaled gradients. To further improve the performance and make different precision network compatible with each other, we propose the precision shift batch normalization (PSBN). And we also propose a scalable input-specific inference mechanism based on this architecture and make the network more adaptable. Experiments on the CIFAR and ImageNet dataset have shown that our DPOQ achieves not only better flexibility but also higher accuracy than the individual quantization.",
        "bibtex": "@InProceedings{pmlr-v157-li21a,\n  title = \t {DPOQ: Dynamic Precision Onion Quantization},\n  author =       {Li, Bowen and Huang, Kai and Chen, Siang and Xiong, Dongliang and Claesen, Luc},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {502--517},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/li21a/li21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/li21a.html},\n  abstract = \t {With the development of deployment platforms and application scenarios for deep neural networks, traditional fixed network architectures cannot meet the requirements. Meanwhile the dynamic network inference becomes a new research trend. Many slimmable and scalable networks have been proposed to satisfy different resource constraints (e.g., storage, latency and energy). And a single network may support versatile architectural configurations including: depth, width, kernel size, and resolution. In this paper, we propose a novel network architecture reuse strategy enabling dynamic precision in parameters. Since our low-precision networks are wrapped in the high-precision networks like an onion, we name it dynamic precision onion quantization (DPOQ). We train the network by using the joint loss with scaled gradients. To further improve the performance and make different precision network compatible with each other, we propose the precision shift batch normalization (PSBN). And we also propose a scalable input-specific inference mechanism based on this architecture and make the network more adaptable. Experiments on the CIFAR and ImageNet dataset have shown that our DPOQ achieves not only better flexibility but also higher accuracy than the individual quantization.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/li21a/li21a.pdf",
        "supp": "",
        "pdf_size": 430050,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11189327886175712039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of VLSI Design, Zhejiang University, Hangzhou, China; Institute of VLSI Design, Zhejiang University, Hangzhou, China; Institute of VLSI Design, Zhejiang University, Hangzhou, China; Institute of VLSI Design, Zhejiang University, Hangzhou, China; Engineering Technology - Electronics-ICT Dept, Hasselt University, 3590 Diepenbeek, Belgium",
        "aff_domain": "zju.edu.cn;vlsi.zju.edu.cn;zju.edu.cn;zju.edu.cn;uhasselt.be",
        "email": "zju.edu.cn;vlsi.zju.edu.cn;zju.edu.cn;zju.edu.cn;uhasselt.be",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Zhejiang University;Hasselt University",
        "aff_unique_dep": "Institute of VLSI Design;Engineering Technology - Electronics-ICT Dept",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.uhasselt.be",
        "aff_unique_abbr": "ZJU;",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Hangzhou;Diepenbeek",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;Belgium"
    },
    {
        "title": "Deep Structural Contrastive Subspace Clustering",
        "site": "https://proceedings.mlr.press/v157/peng21a.html",
        "author": "Bo Peng; Wenjie Zhu",
        "abstract": "Deep subspace clustering based on data self-expression is devoted to learning pairwise affinities in the latent feature space. Existing methods tend to rely on an autoencoder framework to learn representations for an affinity matrix. However, the representation learning driven largely by pixel-level data reconstruction is somewhat incompatible with the subspace clustering task. With the unavailability of ground truth, can structural representations, which is exactly what subspace clustering favors, be achieved by simply exploiting the supervision information in the data itself? In this paper, we formulate this intuition as a structural contrastive prediction task and propose an end-to-end trainable framework referred as Deep Structural Contrastive Subspace Clustering (DSCSC). Specifically, DSCSC makes use of data augmentation technique to mine positive pairs and constructs a data similarity graph in the embedding feature space to search negative pairs. A novel structural contrastive loss is proposed on the latent representations to achieve positive-concentrated and negative-separated property for subspace preserving. Extensive experiments on the benchmark datasets demonstrate that our method outperforms the state-of-the-art deep subspace clustering methods and imply the necessity of the proposed structural contrastive loss.",
        "bibtex": "@InProceedings{pmlr-v157-peng21a,\n  title = \t {Deep Structural Contrastive Subspace Clustering},\n  author =       {Peng, Bo and Zhu, Wenjie},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1145--1160},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/peng21a/peng21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/peng21a.html},\n  abstract = \t {Deep subspace clustering based on data self-expression is devoted to learning pairwise affinities in the latent feature space. Existing methods tend to rely on an autoencoder framework to learn representations for an affinity matrix. However, the representation learning driven largely by pixel-level data reconstruction is somewhat incompatible with the subspace clustering task. With the unavailability of ground truth, can structural representations, which is exactly what subspace clustering favors, be achieved by simply exploiting the supervision information in the data itself? In this paper, we formulate this intuition as a structural contrastive prediction task and propose an end-to-end trainable framework referred as Deep Structural Contrastive Subspace Clustering (DSCSC). Specifically, DSCSC makes use of data augmentation technique to mine positive pairs and constructs a data similarity graph in the embedding feature space to search negative pairs. A novel structural contrastive loss is proposed on the latent representations to achieve positive-concentrated and negative-separated property for subspace preserving. Extensive experiments on the benchmark datasets demonstrate that our method outperforms the state-of-the-art deep subspace clustering methods and imply the necessity of the proposed structural contrastive loss.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/peng21a/peng21a.pdf",
        "supp": "",
        "pdf_size": 849236,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1090669614527731026&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Information Engineering, China Jiliang University, Hangzhou, China + The University of Queensland, Brisbane, Australia; College of Information Engineering, China Jiliang University, Hangzhou, China",
        "aff_domain": "126.com;cjlu.edu.cn",
        "email": "126.com;cjlu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "China Jiliang University;The University of Queensland",
        "aff_unique_dep": "College of Information Engineering;",
        "aff_unique_url": "http://www.cjlu.edu.cn;https://www.uq.edu.au",
        "aff_unique_abbr": ";UQ",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Hangzhou;Brisbane",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection",
        "site": "https://proceedings.mlr.press/v157/zhang21c.html",
        "author": "Shizhao Zhang; Hongya Tuo; Jian Hu; Zhongliang Jing",
        "abstract": "Domain shift is a major challenge for object detectors to generalize well to real world applications. Emerging techniques of domain adaptation for two-stage detectors help to tackle this problem. However, two-stage detectors are not the first choice for industrial applications due to its long time consumption. In this paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve cross-domain performance for one-stage detectors. Image level features alignment is used to strictly match for  local features like texture, and loosely match for global features like illumination. Multi-scale instance level features alignment is presented to reduce instance domain shift effectively, such as variations in object appearance and viewpoint. A consensus regularization to these domain classifiers is employed to help the network generate domain-invariant detections. We evaluate our proposed method on popular datasets like Cityscapes, KITTI, SIM10K and et al.. The results demonstrate considerable improvement when tested under different cross-domain scenarios.",
        "bibtex": "@InProceedings{pmlr-v157-zhang21c,\n  title = \t {Domain Adaptive YOLO for One-Stage Cross-Domain Detection},\n  author =       {Zhang, Shizhao and Tuo, Hongya and Hu, Jian and Jing, Zhongliang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {785--797},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhang21c/zhang21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhang21c.html},\n  abstract = \t {Domain shift is a major challenge for object detectors to generalize well to real world applications. Emerging techniques of domain adaptation for two-stage detectors help to tackle this problem. However, two-stage detectors are not the first choice for industrial applications due to its long time consumption. In this paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve cross-domain performance for one-stage detectors. Image level features alignment is used to strictly match for  local features like texture, and loosely match for global features like illumination. Multi-scale instance level features alignment is presented to reduce instance domain shift effectively, such as variations in object appearance and viewpoint. A consensus regularization to these domain classifiers is employed to help the network generate domain-invariant detections. We evaluate our proposed method on popular datasets like Cityscapes, KITTI, SIM10K and et al.. The results demonstrate considerable improvement when tested under different cross-domain scenarios.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhang21c/zhang21c.pdf",
        "supp": "",
        "pdf_size": 4382413,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14047695568358829495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Baidu Inc., Shanghai, China; Shanghai Jiao Tong University, Shanghai, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Baidu Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.baidu.com",
        "aff_unique_abbr": "SJTU;Baidu",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Dynamic Coordination Graph for Cooperative Multi-Agent Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/siu21a.html",
        "author": "Chapman Siu; Jason Traish; Richard Yi Da Xu",
        "abstract": "This paper introduces Dynamic $Q$-value Coordination Graph (QCGraph) for cooperative multi-agent reinforcement learning. QCGraph aims to dynamically represent and generalize through factorizing the joint value function of all agents according to dynamically created coordination graph based on subsets of agents. The value can be maximized by message passing at both a local and global level along the graph which allows training the value function end-to-end. The coordination graph is dynamically generated and used to generate the payoff functions which are approximated using graph neural networks and parameter sharing to improve generalization over the state-action space. We show that QCGraph can solve a variety of challenging multi-agent tasks being superior to other value factorization approaches.",
        "bibtex": "@InProceedings{pmlr-v157-siu21a,\n  title = \t {Dynamic Coordination Graph for Cooperative Multi-Agent Reinforcement Learning},\n  author =       {Siu, Chapman and Traish, Jason and Xu, Richard Yi Da},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {438--453},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/siu21a/siu21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/siu21a.html},\n  abstract = \t {This paper introduces Dynamic $Q$-value Coordination Graph (QCGraph) for cooperative multi-agent reinforcement learning. QCGraph aims to dynamically represent and generalize through factorizing the joint value function of all agents according to dynamically created coordination graph based on subsets of agents. The value can be maximized by message passing at both a local and global level along the graph which allows training the value function end-to-end. The coordination graph is dynamically generated and used to generate the payoff functions which are approximated using graph neural networks and parameter sharing to improve generalization over the state-action space. We show that QCGraph can solve a variety of challenging multi-agent tasks being superior to other value factorization approaches. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/siu21a/siu21a.pdf",
        "supp": "",
        "pdf_size": 2147795,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8468200308349219911&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Technology Sydney, Australia; University of Technology Sydney, Australia; University of Technology Sydney, Australia",
        "aff_domain": "student.uts.edu.au;uts.edu.au;uts.edu.au",
        "email": "student.uts.edu.au;uts.edu.au;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Technology Sydney",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uts.edu.au",
        "aff_unique_abbr": "UTS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Dynamic Popularity-Aware Contrastive Learning for Recommendation",
        "site": "https://proceedings.mlr.press/v157/lin21b.html",
        "author": "Fangquan Lin; Wei Jiang; Jihai Zhang; Cheng Yang",
        "abstract": "With the development of deep learning techniques, contrastive representation learning has been increasingly employed in large-scale recommender systems. For instance, deep user-item matching models can be trained by contrasting positive and negative examples and learning discriminative user and item representations. Despite their success, the distinguishable properties of the recommender system are often ignored in existing modelling. Standard methods approximate maximum likelihood estimation on user behavior data in a manner similar to language models. Specifically, the way of model optimization corresponds to approximating the user-item pointwise mutual information, which can be regarded as eliminating the influence of global item popularity on user behavior to capture intrinsic user preference. In addition, unlike the situation in language models where word frequency is relatively stable, item popularity is constantly evolving. To address these issues, we propose a novel dynamic popularity-aware (DPA) contrastive learning method for recommendation, which consists of two key components: i) a dynamic negative sampling strategy is involved to enhance the user representation, ii) a dynamic prediction recovery is adopted by the real-time item popularity. The proposed strategy can be naturally overlaid on any contrastive learning-based matching model to more accurately capture user interest and system dynamics. Finally, the effectiveness of the proposed strategy is demonstrated through comprehensive experiments on an e-commerce scenario of Alibaba Group.",
        "bibtex": "@InProceedings{pmlr-v157-lin21b,\n  title = \t {Dynamic Popularity-Aware Contrastive Learning for Recommendation},\n  author =       {Lin, Fangquan and Jiang, Wei and Zhang, Jihai and Yang, Cheng},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {964--968},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/lin21b/lin21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/lin21b.html},\n  abstract = \t {With the development of deep learning techniques, contrastive representation learning has been increasingly employed in large-scale recommender systems. For instance, deep user-item matching models can be trained by contrasting positive and negative examples and learning discriminative user and item representations. Despite their success, the distinguishable properties of the recommender system are often ignored in existing modelling. Standard methods approximate maximum likelihood estimation on user behavior data in a manner similar to language models. Specifically, the way of model optimization corresponds to approximating the user-item pointwise mutual information, which can be regarded as eliminating the influence of global item popularity on user behavior to capture intrinsic user preference. In addition, unlike the situation in language models where word frequency is relatively stable, item popularity is constantly evolving. To address these issues, we propose a novel dynamic popularity-aware (DPA) contrastive learning method for recommendation, which consists of two key components: i) a dynamic negative sampling strategy is involved to enhance the user representation, ii) a dynamic prediction recovery is adopted by the real-time item popularity. The proposed strategy can be naturally overlaid on any contrastive learning-based matching model to more accurately capture user interest and system dynamics. Finally, the effectiveness of the proposed strategy is demonstrated through comprehensive experiments on an e-commerce scenario of Alibaba Group.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/lin21b/lin21b.pdf",
        "supp": "",
        "pdf_size": 415257,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9327935941095001297&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Efficient Coreset Constructions via Sensitivity Sampling",
        "site": "https://proceedings.mlr.press/v157/braverman21a.html",
        "author": "Vladimir Braverman; Dan Feldman; Harry Lang; Adiel Statman; Samson Zhou",
        "abstract": "A coreset for a set of points is a small subset of weighted points that approximately preserves important properties of the original set. Specifically, if $P$ is a set of points, $Q$ is a set of queries, and $f:P\\times Q\\to\\mathbb{R}$ is a cost function, then a set $S\\subseteq P$ with weights $w:P\\to[0,\\infty)$ is an $\\epsilon$-coreset for some parameter $\\epsilon>0$ if $\\sum_{s\\in S}w(s)f(s,q)$ is a $(1+\\epsilon)$ multiplicative approximation to $\\sum_{p\\in P}f(p,q)$ for all $q\\in Q$. Coresets are used to solve fundamental problems in machine learning under various big data models of computation. Many of the suggested coresets in the recent decade used, or could have used a general framework for constructing coresets whose size depends quadratically on the total sensitivity $t$. In this paper we improve this bound from $O(t^2)$ to $O(t\\log t)$. Thus our results imply more space efficient solutions to a number of problems, including projective clustering, $k$-line clustering, and subspace approximation. The main technical result is a generic reduction to the sample complexity of learning a class of functions with bounded VC dimension. We show that obtaining an $(\\nu,\\alpha)$-sample for this class of functions with appropriate parameters $\\nu$ and $\\alpha$ suffices to achieve space efficient $\\epsilon$-coresets. Our result implies more efficient coreset constructions for a number of interesting problems in machine learning; we show applications to $k$-median/$k$-means, $k$-line clustering, $j$-subspace approximation, and the integer $(j,k)$-projective clustering problem.",
        "bibtex": "@InProceedings{pmlr-v157-braverman21a,\n  title = \t {Efficient Coreset Constructions via Sensitivity Sampling},\n  author =       {Braverman, Vladimir and Feldman, Dan and Lang, Harry and Statman, Adiel and Zhou, Samson},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {948--963},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/braverman21a/braverman21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/braverman21a.html},\n  abstract = \t {A coreset for a set of points is a small subset of weighted points that approximately preserves important properties of the original set. Specifically, if $P$ is a set of points, $Q$ is a set of queries, and $f:P\\times Q\\to\\mathbb{R}$ is a cost function, then a set $S\\subseteq P$ with weights $w:P\\to[0,\\infty)$ is an $\\epsilon$-coreset for some parameter $\\epsilon>0$ if $\\sum_{s\\in S}w(s)f(s,q)$ is a $(1+\\epsilon)$ multiplicative approximation to $\\sum_{p\\in P}f(p,q)$ for all $q\\in Q$. Coresets are used to solve fundamental problems in machine learning under various big data models of computation. Many of the suggested coresets in the recent decade used, or could have used a general framework for constructing coresets whose size depends quadratically on the total sensitivity $t$. In this paper we improve this bound from $O(t^2)$ to $O(t\\log t)$. Thus our results imply more space efficient solutions to a number of problems, including projective clustering, $k$-line clustering, and subspace approximation. The main technical result is a generic reduction to the sample complexity of learning a class of functions with bounded VC dimension. We show that obtaining an $(\\nu,\\alpha)$-sample for this class of functions with appropriate parameters $\\nu$ and $\\alpha$ suffices to achieve space efficient $\\epsilon$-coresets. Our result implies more efficient coreset constructions for a number of interesting problems in machine learning; we show applications to $k$-median/$k$-means, $k$-line clustering, $j$-subspace approximation, and the integer $(j,k)$-projective clustering problem. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/braverman21a/braverman21a.pdf",
        "supp": "",
        "pdf_size": 756135,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4433188864797575577&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Johns Hopkins University; University of Haifa; MIT; University of Haifa; Carnegie Mellon University",
        "aff_domain": "cs.jhu.edu;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "cs.jhu.edu;gmail.com;gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;3",
        "aff_unique_norm": "Johns Hopkins University;University of Haifa;Massachusetts Institute of Technology;Carnegie Mellon University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.jhu.edu;https://www.haifa.ac.il;https://web.mit.edu;https://www.cmu.edu",
        "aff_unique_abbr": "JHU;UoH;MIT;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Encoder-decoder-based image transformation approach for integrating precipitation forecasts",
        "site": "https://proceedings.mlr.press/v157/hachiya21a.html",
        "author": "Hirotaka Hachiya; Yusuke Masumoto; Yuki Mori; Naonori Ueda",
        "abstract": "As the damage caused by heavy rainfall is becoming more serious, the improvement of precipitation forecasts is highly demanded. For this purpose, arithmetic and Bayesian average-based methods have been proposed to integrate multiple 2D-grid forecasts. However, since a single weight is shared in the entire grid in these methods, local variations of the importance of forecasts could not be taken into account. Besides, although a variety of information is available in precipitation forecast, it would not be straightforwardly to incorporate the additional information in the existing methods. To overcome these problems, we propose an encoder-decoder-based image transformation method that generates a weight image that is optimized in a pixel-wise manner and additional information could be embedded as the channel of input images and feature maps. Through the experiment of precipitation forecast in the period from April 2018 to March 2019 in Japan, we will show that our proposed integration method outperforms existing methods.",
        "bibtex": "@InProceedings{pmlr-v157-hachiya21a,\n  title = \t {Encoder-decoder-based image transformation approach for integrating precipitation forecasts},\n  author =       {Hachiya, Hirotaka and Masumoto, Yusuke and Mori, Yuki and Ueda, Naonori},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {174--188},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/hachiya21a/hachiya21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/hachiya21a.html},\n  abstract = \t {As the damage caused by heavy rainfall is becoming more serious, the improvement of precipitation forecasts is highly demanded. For this purpose, arithmetic and Bayesian average-based methods have been proposed to integrate multiple 2D-grid forecasts. However, since a single weight is shared in the entire grid in these methods, local variations of the importance of forecasts could not be taken into account. Besides, although a variety of information is available in precipitation forecast, it would not be straightforwardly to incorporate the additional information in the existing methods. To overcome these problems, we propose an encoder-decoder-based image transformation method that generates a weight image that is optimized in a pixel-wise manner and additional information could be embedded as the channel of input images and feature maps. Through the experiment of precipitation forecast in the period from April 2018 to March 2019 in Japan, we will show that our proposed integration method outperforms existing methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/hachiya21a/hachiya21a.pdf",
        "supp": "",
        "pdf_size": 18770312,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11180367805922278196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Graduate School of Systems Engineering, Wakayama University+Center for AIP, RIKEN; Graduate School of Systems Engineering, Wakayama University+Center for AIP, RIKEN; Numerical Prediction Development Center, Japan Meteorological Agency+Fujitsu Limited; Center for AIP, RIKEN",
        "aff_domain": "WAKAYAMA-U.AC.JP;G.WAKAYAMA-U.JP;FUJITSU.COM;RIKEN.JP",
        "email": "WAKAYAMA-U.AC.JP;G.WAKAYAMA-U.JP;FUJITSU.COM;RIKEN.JP",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2+3;1",
        "aff_unique_norm": "Wakayama University;RIKEN;Japan Meteorological Agency;Fujitsu Limited",
        "aff_unique_dep": "Graduate School of Systems Engineering;Center for AIP;Numerical Prediction Development Center;",
        "aff_unique_url": "https://www.wakayama-u.ac.jp;https://www.riken.jp;https://www.jma.go.jp;https://www.fujitsu.com",
        "aff_unique_abbr": ";RIKEN;JMA;Fujitsu",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Ensembling With a Fixed Parameter Budget: When Does It Help and Why?",
        "site": "https://proceedings.mlr.press/v157/deng21a.html",
        "author": "Didan Deng; Emil Bertram Shi",
        "abstract": "Given a fixed parameter budget, one can build a single large neural network or create a memory-split ensemble: a pool of several smaller networks with the same total parameter count as the single network. A memory-split ensemble can outperform its single model counterpart (Lobacheva et al., 2020): a phenomenon known as the memory-split advantage (MSA). The reasons for MSA are still not yet fully understood. In particular, it is difficult in practice to predict when it will exist. This paper sheds light on the reasons underlying MSA using random feature theory. We study the dependence of the MSA on several factors: the parameter budget, the training set size, the L2 regularization and the Stochastic Gradient Descent (SGD) hyper-parameters. Using the bias-variance decomposition, we show that MSA exists when the reduction in variance due to the ensemble (\\ie, \\textit{ensemble gain}) exceeds the increase in squared bias due to the smaller size of the individual networks (\\ie, \\textit{shrinkage cost}). Taken together, our theoretical analysis demonstrates that the MSA mainly exists for the small parameter budgets relative to the training set size, and that memory-splitting can be understood as a type of regularization. Adding other forms of regularization, \\eg L2 regularization, reduces the MSA. Thus, the potential benefit of memory-splitting lies primarily in the possibility of speed-up via parallel computation. Our empirical experiments with deep neural networks and large image datasets show that MSA is not a general phenomenon, but mainly exists when the number of training iterations is small.",
        "bibtex": "@InProceedings{pmlr-v157-deng21a,\n  title = \t {Ensembling With a Fixed Parameter Budget: When Does It Help and Why?},\n  author =       {Deng, Didan and Shi, Emil Bertram},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1176--1191},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/deng21a/deng21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/deng21a.html},\n  abstract = \t {Given a fixed parameter budget, one can build a single large neural network or create a memory-split ensemble: a pool of several smaller networks with the same total parameter count as the single network. A memory-split ensemble can outperform its single model counterpart (Lobacheva et al., 2020): a phenomenon known as the memory-split advantage (MSA). The reasons for MSA are still not yet fully understood. In particular, it is difficult in practice to predict when it will exist. This paper sheds light on the reasons underlying MSA using random feature theory. We study the dependence of the MSA on several factors: the parameter budget, the training set size, the L2 regularization and the Stochastic Gradient Descent (SGD) hyper-parameters. Using the bias-variance decomposition, we show that MSA exists when the reduction in variance due to the ensemble (\\ie, \\textit{ensemble gain}) exceeds the increase in squared bias due to the smaller size of the individual networks (\\ie, \\textit{shrinkage cost}). Taken together, our theoretical analysis demonstrates that the MSA mainly exists for the small parameter budgets relative to the training set size, and that memory-splitting can be understood as a type of regularization. Adding other forms of regularization, \\eg L2 regularization, reduces the MSA. Thus, the potential benefit of memory-splitting lies primarily in the possibility of speed-up via parallel computation. Our empirical experiments with deep neural networks and large image datasets show that MSA is not a general phenomenon, but mainly exists when the number of training iterations is small.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/deng21a/deng21a.pdf",
        "supp": "",
        "pdf_size": 1084791,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16119495832308803787&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Hong Kong University of Science and Technology, Kowloon, Hong Kong; Hong Kong University of Science and Technology, Kowloon, Hong Kong",
        "aff_domain": "connect.ust.hk;ust.hk",
        "email": "connect.ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ExNN-SMOTE: Extended Natural Neighbors Based SMOTE to Deal with Imbalanced Data",
        "site": "https://proceedings.mlr.press/v157/guan21a.html",
        "author": "Hongjiao Guan; Bin Ma; Yingtao Zhang; Xianglong Tang",
        "abstract": "Many practical applications suffer from the problem of imbalanced classification. The minority class has poor classification performance; on the other hand, its misclassification cost is high. One reason for classification difficulty is the intrinsic complicated distribution characteristics (CDCs) in imbalanced data itself. Classical oversampling method SMOTE generates synthetic minority class examples between neighbors, which is parameter dependent. Furthermore, due to blindness of neighbor selection, SMOTE suffers from overgeneralization in the minority class. To solve such problems, we propose an oversampling method, called extended natural neighbors based SMOTE (ExNN-SMOTE). In ExNN-SMOTE, neighbors are determined adaptively by capturing data distribution characteristics. Extensive experiments over synthetic and real datasets demonstrate the effectiveness of ExNN-SMOTE dealing with CDCs and the superiority of ExNN-SMOTE over other SMOTE-related methods.",
        "bibtex": "@InProceedings{pmlr-v157-guan21a,\n  title = \t {ExNN-SMOTE: Extended Natural Neighbors Based SMOTE to Deal with Imbalanced Data},\n  author =       {Guan, Hongjiao and Ma, Bin and Zhang, Yingtao and Tang, Xianglong},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {902--917},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/guan21a/guan21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/guan21a.html},\n  abstract = \t {Many practical applications suffer from the problem of imbalanced classification. The minority class has poor classification performance; on the other hand, its misclassification cost is high. One reason for classification difficulty is the intrinsic complicated distribution characteristics (CDCs) in imbalanced data itself. Classical oversampling method SMOTE generates synthetic minority class examples between neighbors, which is parameter dependent. Furthermore, due to blindness of neighbor selection, SMOTE suffers from overgeneralization in the minority class. To solve such problems, we propose an oversampling method, called extended natural neighbors based SMOTE (ExNN-SMOTE). In ExNN-SMOTE, neighbors are determined adaptively by capturing data distribution characteristics. Extensive experiments over synthetic and real datasets demonstrate the effectiveness of ExNN-SMOTE dealing with CDCs and the superiority of ExNN-SMOTE over other SMOTE-related methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/guan21a/guan21a.pdf",
        "supp": "",
        "pdf_size": 10291799,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13920587024864920307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Cyber Security, Qilu University of Technology (Shandong Academy of Sciences), Shandong+Computer Science Center (National Supercomputer Center in Jinan), Jinan 250353, China+Shandong Provincial Key Laboratory of Computer Networks, Qilu University of Technology (Shandong Academy of Sciences), Jinan 250353, China; School of Cyber Security, Qilu University of Technology (Shandong Academy of Sciences), Shandong+Computer Science Center (National Supercomputer Center in Jinan), Jinan 250353, China+Shandong Provincial Key Laboratory of Computer Networks, Qilu University of Technology (Shandong Academy of Sciences), Jinan 250353, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China",
        "aff_domain": "163.com;126.com;hit.edu.cn;hit.edu.cn",
        "email": "163.com;126.com;hit.edu.cn;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+0;0+1+0;2;2",
        "aff_unique_norm": "Qilu University of Technology;National Supercomputer Center in Jinan;Harbin Institute of Technology",
        "aff_unique_dep": "School of Cyber Security;Computer Science Center;School of Computer Science and Technology",
        "aff_unique_url": "http://www.qilu.edu.cn;;http://www.hit.edu.cn/",
        "aff_unique_abbr": "Qilu Tech;;HIT",
        "aff_campus_unique_index": "1+1;1+1;2;2",
        "aff_campus_unique": ";Jinan;Harbin",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Expert advice problem with noisy low rank loss",
        "site": "https://proceedings.mlr.press/v157/liu21d.html",
        "author": "Yaxiong Liu; Xuanke Jiang; Kohei Hatano; Eiji Takimoto",
        "abstract": "We consider the expert advice problem with a low rank but noisy loss sequence, where a loss vector $l_{t} \\in [-1,1]^N$ in each round $t$ is of the form $l_{t} = U v_{t} + \\epsilon_{t}$ for some fixed but unknown $N \\times d$ matrix $U$ called the kernel, some $d$-dimensional seed vector $v_{t} \\in \\mathbb{R}^{d}$, and some additional noisy term $\\epsilon_t \\in \\mathbb{R}^{N}$ whose norm is bounded by $\\epsilon$. This is a generalization of the works of Hazan et al. and Barman et al., where the former only treats noiseless loss and the latter assumes that the kernel is known in advance. In this paper, we propose an algorithm, where we re-construct the kernel under the assumptions, that the low rank loss is noised and there is no prior information about kernel. In this algorithm, we approximate the kernel by choosing a set of loss vectors with a high degree of independence from each other, and we give a regret bound of $O(d\\sqrt{T}+d^{4/3}(N\\epsilon)^{1/3}\\sqrt{T})$. Moreover, even if in experiment, the proposed algorithm performs better than Hazan\u2019s algorithm and Hedge algorithm.",
        "bibtex": "@InProceedings{pmlr-v157-liu21d,\n  title = \t {Expert advice problem with noisy low rank loss},\n  author =       {Liu, Yaxiong and Jiang, Xuanke and Hatano, Kohei and Takimoto, Eiji},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1097--1112},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/liu21d/liu21d.pdf},\n  url = \t {https://proceedings.mlr.press/v157/liu21d.html},\n  abstract = \t {We consider the expert advice problem with a low rank but noisy loss sequence, where a loss vector $l_{t} \\in [-1,1]^N$ in each round $t$ is of the form $l_{t} = U v_{t} + \\epsilon_{t}$ for some fixed but unknown $N \\times d$ matrix $U$ called the kernel, some $d$-dimensional seed vector $v_{t} \\in \\mathbb{R}^{d}$, and some additional noisy term $\\epsilon_t \\in \\mathbb{R}^{N}$ whose norm is bounded by $\\epsilon$. This is a generalization of the works of Hazan et al. and Barman et al., where the former only treats noiseless loss and the latter assumes that the kernel is known in advance. In this paper, we propose an algorithm, where we re-construct the kernel under the assumptions, that the low rank loss is noised and there is no prior information about kernel. In this algorithm, we approximate the kernel by choosing a set of loss vectors with a high degree of independence from each other, and we give a regret bound of $O(d\\sqrt{T}+d^{4/3}(N\\epsilon)^{1/3}\\sqrt{T})$. Moreover, even if in experiment, the proposed algorithm performs better than Hazan\u2019s algorithm and Hedge algorithm.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/liu21d/liu21d.pdf",
        "supp": "",
        "pdf_size": 228630,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11343931738898178618&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Informatics, Kyushu University/ RIKEN AIP; Department of Informatics, Kyushu University; Faculty of Art and Science, Kyushu University/ RIKEN AIP; Department of Informatics, Kyushu University",
        "aff_domain": "inf.kyushu-u.ac.jp;s.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp",
        "email": "inf.kyushu-u.ac.jp;s.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;inf.kyushu-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Kyushu University",
        "aff_unique_dep": "Department of Informatics",
        "aff_unique_url": "https://www.kyushu-u.ac.jp",
        "aff_unique_abbr": "Kyushu U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Exposing Cyber-Physical System Weaknesses by Implicitly Learning their Underlying Models",
        "site": "https://proceedings.mlr.press/v157/costilla-enriquez21a.html",
        "author": "Napoleon Costilla-Enriquez; Yang Weng",
        "abstract": "Cyber-Physical Systems (CPS) plays a critical role in today\u2019s social life, especially with occasional pandemic events. With more reliance on the cyber operation of infrastructures, it is important to understand attacking mechanisms in CPS for potential solutions and defenses, where False Data Injection Attack (FDIA) is an important class. FDIA methods in the literature require the mathematical CPS model and state variable values to create an efficient attack vector, unrealistic for many attackers in the real world. Also, they do not have performance guarantee. This paper shows that it is possible to deploy a FDIA without having the CPS model and state variables information. Additionally, we prove a theoretic bound for the proposed method. Specifically, we design a scheme that learns an implicit CPS model to create tampered sensor measurements to deploy an attack based only on historical data. The proposed framework utilizes a Wasserstein generative adversarial network with two regularization terms to create such tampered measurements also known as adversarial examples. To build an attack with confidence, we present a proof based on convergence in distribution and Lipschitz norm to show that our method captures the real observed measurement distribution. This means that our model learns the complex underlying processes from the CPSs. We demonstrate the robustness and universality of our proposed framework based on two diversified adversarial examples with different systems, domains, and datasets.",
        "bibtex": "@InProceedings{pmlr-v157-costilla-enriquez21a,\n  title = \t {Exposing Cyber-Physical System Weaknesses by Implicitly Learning their Underlying Models},\n  author =       {Costilla-Enriquez, Napoleon and Weng, Yang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1333--1348},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/costilla-enriquez21a/costilla-enriquez21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/costilla-enriquez21a.html},\n  abstract = \t {Cyber-Physical Systems (CPS) plays a critical role in today\u2019s social life, especially with occasional pandemic events. With more reliance on the cyber operation of infrastructures, it is important to understand attacking mechanisms in CPS for potential solutions and defenses, where False Data Injection Attack (FDIA) is an important class. FDIA methods in the literature require the mathematical CPS model and state variable values to create an efficient attack vector, unrealistic for many attackers in the real world. Also, they do not have performance guarantee. This paper shows that it is possible to deploy a FDIA without having the CPS model and state variables information. Additionally, we prove a theoretic bound for the proposed method. Specifically, we design a scheme that learns an implicit CPS model to create tampered sensor measurements to deploy an attack based only on historical data. The proposed framework utilizes a Wasserstein generative adversarial network with two regularization terms to create such tampered measurements also known as adversarial examples. To build an attack with confidence, we present a proof based on convergence in distribution and Lipschitz norm to show that our method captures the real observed measurement distribution. This means that our model learns the complex underlying processes from the CPSs. We demonstrate the robustness and universality of our proposed framework based on two diversified adversarial examples with different systems, domains, and datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/costilla-enriquez21a/costilla-enriquez21a.pdf",
        "supp": "",
        "pdf_size": 1697256,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13545902848161744760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Arizona State University, Tempe, AZ, US; Arizona State University, Tempe, AZ, US",
        "aff_domain": "asu.edu;asu.edu",
        "email": "asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tempe",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Expressive Neural Voice Cloning",
        "site": "https://proceedings.mlr.press/v157/neekhara21a.html",
        "author": "Paarth Neekhara; Shehzeen Hussain; Shlomo Dubnov; Farinaz Koushanfar; Julian McAuley",
        "abstract": "Voice cloning is the task of learning to synthesize the voice of an unseen speaker from a few samples. While current voice cloning methods achieve promising results in Text-to-Speech (TTS) synthesis for a new voice, these approaches lack the ability to control the expressiveness of synthesized audio. In this work, we propose a controllable voice cloning method that allows fine-grained control over various style aspects of the synthesized speech for an unseen speaker. We achieve this by explicitly conditioning the speech synthesis model on a speaker encoding, pitch contour and latent style tokens during training. Through both quantitative and qualitative evaluations, we show that our framework can be used for various expressive voice cloning tasks using only a few transcribed or untranscribed speech samples for a new speaker. These cloning tasks include style transfer from a reference speech, synthesizing speech directly from text, and fine-grained style control by manipulating the style conditioning variables during inference.",
        "bibtex": "@InProceedings{pmlr-v157-neekhara21a,\n  title = \t {Expressive Neural Voice Cloning},\n  author =       {Neekhara, Paarth and Hussain, Shehzeen and Dubnov, Shlomo and Koushanfar, Farinaz and McAuley, Julian},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {252--267},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/neekhara21a/neekhara21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/neekhara21a.html},\n  abstract = \t {Voice cloning is the task of learning to synthesize the voice of an unseen speaker from a few samples. While current voice cloning methods achieve promising results in Text-to-Speech (TTS) synthesis for a new voice, these approaches lack the ability to control the expressiveness of synthesized audio. In this work, we propose a controllable voice cloning method that allows fine-grained control over various style aspects of the synthesized speech for an unseen speaker. We achieve this by explicitly conditioning the speech synthesis model on a speaker encoding, pitch contour and latent style tokens during training. Through both quantitative and qualitative evaluations, we show that our framework can be used for various expressive voice cloning tasks using only a few transcribed or untranscribed speech samples for a new speaker. These cloning tasks include style transfer from a reference speech, synthesizing speech directly from text, and fine-grained style control by manipulating the style conditioning variables during inference.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/neekhara21a/neekhara21a.pdf",
        "supp": "",
        "pdf_size": 571221,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11383274402465773705&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "eng.ucsd.edu;eng.ucsd.edu;ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "email": "eng.ucsd.edu;eng.ucsd.edu;ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "github": "",
        "project": "https://expressivecloning.github.io/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fairness constraint of Fuzzy C-means Clustering improves clustering fairness",
        "site": "https://proceedings.mlr.press/v157/xia21a.html",
        "author": "Xu Xia; Zhang Hui; Ynag Chunming; Zhao Xujian; Li Bo",
        "abstract": "Fuzzy C-Means (FCM) clustering is a classic clustering algorithm, which is widely used in the real world. Despite the distinct advantages of FCM algorithm, whether the usage of fairness constraint in the FCM could improve clustering fairness remains fully elusive. By introducing a novel fair loss term into the objective function, a Fair Fuzzy C-Means (FFCM) algorithm was proposed in this current study. We proved that the membership value was constrained by distance and fairness in the meantime during the optimization process in the proposed objective function. By studying the Fuzzy C-Means Clustering with fairness constraint problem and proposing a fair fuzzy C-means method, this study provided mechanism understanding in achieving the fairness constraint in Fuzzy C-Means clustering and bridged up the gap of fair fuzzy clustering.",
        "bibtex": "@InProceedings{pmlr-v157-xia21a,\n  title = \t {Fairness constraint of Fuzzy C-means Clustering improves clustering fairness},\n  author =       {Xia, Xu and Hui, Zhang and Chunming, Ynag and Xujian, Zhao and Bo, Li},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {113--128},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/xia21a/xia21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/xia21a.html},\n  abstract = \t {Fuzzy C-Means (FCM) clustering is a classic clustering algorithm, which is widely used in the real world. Despite the distinct advantages of FCM algorithm, whether the usage of fairness constraint in the FCM could improve clustering fairness remains fully elusive. By introducing a novel fair loss term into the objective function, a Fair Fuzzy C-Means (FFCM) algorithm was proposed in this current study. We proved that the membership value was constrained by distance and fairness in the meantime during the optimization process in the proposed objective function. By studying the Fuzzy C-Means Clustering with fairness constraint problem and proposing a fair fuzzy C-means method, this study provided mechanism understanding in achieving the fairness constraint in Fuzzy C-Means clustering and bridged up the gap of fair fuzzy clustering.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/xia21a/xia21a.pdf",
        "supp": "",
        "pdf_size": 161948,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14350077914150351136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Southwest University of Science and Technology, Mianyang, China; Southwest University of Science and Technology, Mianyang, China; Southwest University of Science and Technology, Mianyang, China; Southwest University of Science and Technology, Mianyang, China; Southwest University of Science and Technology, Mianyang, China",
        "aff_domain": "gmail.com;swust.edu.cn;swust.edu.cn;gmail.com;qq.com",
        "email": "gmail.com;swust.edu.cn;swust.edu.cn;gmail.com;qq.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Southwest University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mianyang",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Fast Rate Learning in Stochastic First Price Bidding",
        "site": "https://proceedings.mlr.press/v157/achddou21a.html",
        "author": "Juliette Achddou; Olivier Capp\u00e9; Aur\u00e9lien Garivier",
        "abstract": "First-price auctions have largely replaced traditional bidding approaches based on Vickrey auctions in programmatic advertising. %  As far as learning is concerned, first-price auctions are more challenging because the optimal bidding strategy does not only depend on the value of the item but also requires some knowledge of the other bids. % They have already given rise to several works in sequential learning, % many of which consider models for which the value of the buyer or the opponents\u2019 maximal bid is chosen in an adversarial manner. Even in the simplest settings, this gives rise to algorithms whose pseudo-regret grows as $\\sqrt{T}$ with respect to the time horizon $T$. % Focusing on the case where the buyer plays against a stationary stochastic environment, we show how to achieve significantly lower pseudo-regret: when the opponents\u2019 maximal bid distribution is known we provide an algorithm whose pseudo-regret can be as low as $\\log^2(T)$; in the case where the distribution must be learnt sequentially, a generalization of this algorithm can achieve $T^{1/3+ \\epsilon}$ pseudo-regret, for any $\\epsilon>0$. % To obtain these results, we introduce two novel ideas that can be of interest in their own right. First, by transposing results obtained in the posted price setting, we provide conditions under which the first-price bidding utility is locally quadratic around its optimum. Second, we leverage the observation that, on small sub-intervals, the concentration of the variations of the empirical distribution function may be controlled more accurately than by using the classical Dvoretzky-Kiefer-Wolfowitz inequality. % Numerical simulations confirm that our algorithms converge much faster than alternatives proposed in the literature for various bid distributions, including for bids collected on an actual programmatic advertising platform.",
        "bibtex": "@InProceedings{pmlr-v157-achddou21a,\n  title = \t {Fast Rate Learning in Stochastic First Price Bidding},\n  author =       {Achddou, Juliette and Capp\\'e, Olivier and Garivier, Aur\\'elien},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1754--1769},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/achddou21a/achddou21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/achddou21a.html},\n  abstract = \t {First-price auctions have largely replaced traditional bidding approaches based on Vickrey auctions in programmatic advertising. %  As far as learning is concerned, first-price auctions are more challenging because the optimal bidding strategy does not only depend on the value of the item but also requires some knowledge of the other bids. % They have already given rise to several works in sequential learning, % many of which consider models for which the value of the buyer or the opponents\u2019 maximal bid is chosen in an adversarial manner. Even in the simplest settings, this gives rise to algorithms whose pseudo-regret grows as $\\sqrt{T}$ with respect to the time horizon $T$. % Focusing on the case where the buyer plays against a stationary stochastic environment, we show how to achieve significantly lower pseudo-regret: when the opponents\u2019 maximal bid distribution is known we provide an algorithm whose pseudo-regret can be as low as $\\log^2(T)$; in the case where the distribution must be learnt sequentially, a generalization of this algorithm can achieve $T^{1/3+ \\epsilon}$ pseudo-regret, for any $\\epsilon>0$. % To obtain these results, we introduce two novel ideas that can be of interest in their own right. First, by transposing results obtained in the posted price setting, we provide conditions under which the first-price bidding utility is locally quadratic around its optimum. Second, we leverage the observation that, on small sub-intervals, the concentration of the variations of the empirical distribution function may be controlled more accurately than by using the classical Dvoretzky-Kiefer-Wolfowitz inequality. % Numerical simulations confirm that our algorithms converge much faster than alternatives proposed in the literature for various bid distributions, including for bids collected on an actual programmatic advertising platform.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/achddou21a/achddou21a.pdf",
        "supp": "",
        "pdf_size": 4918459,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6599287716190371242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "DIENS, INRIA, Universit\u00e9 PSL, 1000mercis Group; DIENS, CNRS, INRIA, Universit\u00e9 PSL; UMPA, CNRS, INRIA, ENS Lyon",
        "aff_domain": "gmail.com;cnrs.fr;ens-lyon.fr",
        "email": "gmail.com;cnrs.fr;ens-lyon.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "DIENS;Universit\u00e9 PSL;UMPA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.universite-psl.fr;",
        "aff_unique_abbr": ";PSL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Feature Convolutional Networks",
        "site": "https://proceedings.mlr.press/v157/hu21a.html",
        "author": "He Hu",
        "abstract": "Convolutional neural networks are among the most successful deep learning models used for image processing, computer vision and natural language processing applications. In this paper, we define convolution operator for numerical tabular features and thus propose feature convolutional network model for machine learning tasks. Feature convolutional networks contain feature convolution layer to extract pairwise feature convolutions in the relational feature spaces. Compared with the baseline multi-layer neural network model, the feature convolutional network gains better performance among all the experiments. The experiments results suggest that feature convolutional networks can generate efficient features automatically and provide better performance through automatic feature learning. The demo code is at https://github.com/info-ruc/FeatConvNet.",
        "bibtex": "@InProceedings{pmlr-v157-hu21a,\n  title = \t {Feature Convolutional Networks},\n  author =       {Hu, He},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {830--839},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/hu21a/hu21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/hu21a.html},\n  abstract = \t {Convolutional neural networks are among the most successful deep learning models used for image processing, computer vision and natural language processing applications. In this paper, we define convolution operator for numerical tabular features and thus propose feature convolutional network model for machine learning tasks. Feature convolutional networks contain feature convolution layer to extract pairwise feature convolutions in the relational feature spaces. Compared with the baseline multi-layer neural network model, the feature convolutional network gains better performance among all the experiments. The experiments results suggest that feature convolutional networks can generate efficient features automatically and provide better performance through automatic feature learning. The demo code is at https://github.com/info-ruc/FeatConvNet.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/hu21a/hu21a.pdf",
        "supp": "",
        "pdf_size": 675738,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "",
        "aff_domain": "gmail.com;cnrs.fr;ens-lyon.fr",
        "email": "gmail.com;cnrs.fr;ens-lyon.fr",
        "github": "https://github.com/info-ruc/FeatConvNet",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Generating Deep Networks Explanations with Robust Attribution Alignment",
        "site": "https://proceedings.mlr.press/v157/zeng21b.html",
        "author": "Guohang Zeng; Yousef Kowsar; Sarah Erfani; James Bailey",
        "abstract": "Attribution methods play a key role in generating post-hoc explanations on pre-trained models, however it has been shown that existing methods yield unfaithful and noisy explanations. In this paper, we propose a new paradigm of attribution method: we treat the model\u2019s explanations as a part of network\u2019s outputs then generate attribution maps from the underlying deep network. The generated attribution maps are up-sampled from the last convolutional layer of the network to obtain localization information about the target to be explained. Inspired by recent studies that showed adversarially robust models\u2019 saliency map aligns well with human perception, we utilize attribution maps from the robust model to supervise the learned attributions. Our proposed method can produce visually plausible explanations along with the prediction in inference phase. Experiments on real datasets show that our proposed method yields more faithful explanations than post-hoc attribution methods with lighter computational costs.",
        "bibtex": "@InProceedings{pmlr-v157-zeng21b,\n  title = \t {Generating Deep Networks Explanations with Robust Attribution Alignment},\n  author =       {Zeng, Guohang and Kowsar, Yousef and Erfani, Sarah and Bailey, James},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {753--768},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zeng21b/zeng21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zeng21b.html},\n  abstract = \t {Attribution methods play a key role in generating post-hoc explanations on pre-trained models, however it has been shown that existing methods yield unfaithful and noisy explanations. In this paper, we propose a new paradigm of attribution method: we treat the model\u2019s explanations as a part of network\u2019s outputs then generate attribution maps from the underlying deep network. The generated attribution maps are up-sampled from the last convolutional layer of the network to obtain localization information about the target to be explained. Inspired by recent studies that showed adversarially robust models\u2019 saliency map aligns well with human perception, we utilize attribution maps from the robust model to supervise the learned attributions. Our proposed method can produce visually plausible explanations along with the prediction in inference phase. Experiments on real datasets show that our proposed method yields more faithful explanations than post-hoc attribution methods with lighter computational costs.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zeng21b/zeng21b.pdf",
        "supp": "",
        "pdf_size": 3290584,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13363871486368223176&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing and Information Systems, The University of Melbourne, Australia; School of Computing and Information Systems, The University of Melbourne, Australia; School of Computing and Information Systems, The University of Melbourne, Australia; School of Computing and Information Systems, The University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The University of Melbourne",
        "aff_unique_dep": "School of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/kitamura21a.html",
        "author": "Toshinori Kitamura; Lingwei Zhu; Takamitsu Matsubara",
        "abstract": "The recent boom in the literature on entropy-regularized reinforcement learning (RL) approaches reveals that Kullback-Leibler (KL) regularization brings advantages to RL algorithms by canceling out errors under mild assumptions. However, existing analyses focus on fixed regularization with a constant weighting coefficient and do not consider cases where the coefficient is allowed to change dynamically. In this paper, we study the dynamic coefficient scheme and present the first asymptotic error bound. Based on the dynamic coefficient error bound, we propose an effective scheme to tune the coefficient according to the magnitude of error in favor of more robust learning. Complementing this development, we propose a novel algorithm, Geometric Value Iteration (GVI), that features a dynamic error-aware KL coefficient design with the aim of mitigating the impact of errors on performance. Our experiments demonstrate that GVI can effectively exploit the trade-off between learning speed and robustness over uniform averaging of a constant KL coefficient. The combination of GVI and deep networks shows stable learning behavior even in the absence of a target network, where algorithms with a constant KL coefficient would greatly oscillate or even fail to converge.",
        "bibtex": "@InProceedings{pmlr-v157-kitamura21a,\n  title = \t {Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning},\n  author =       {Kitamura, Toshinori and Zhu, Lingwei and Matsubara, Takamitsu},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {918--931},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/kitamura21a/kitamura21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/kitamura21a.html},\n  abstract = \t { The recent boom in the literature on entropy-regularized reinforcement learning (RL) approaches reveals that Kullback-Leibler (KL) regularization brings advantages to RL algorithms by canceling out errors under mild assumptions. However, existing analyses focus on fixed regularization with a constant weighting coefficient and do not consider cases where the coefficient is allowed to change dynamically. In this paper, we study the dynamic coefficient scheme and present the first asymptotic error bound. Based on the dynamic coefficient error bound, we propose an effective scheme to tune the coefficient according to the magnitude of error in favor of more robust learning. Complementing this development, we propose a novel algorithm, Geometric Value Iteration (GVI), that features a dynamic error-aware KL coefficient design with the aim of mitigating the impact of errors on performance. Our experiments demonstrate that GVI can effectively exploit the trade-off between learning speed and robustness over uniform averaging of a constant KL coefficient. The combination of GVI and deep networks shows stable learning behavior even in the absence of a target network, where algorithms with a constant KL coefficient would greatly oscillate or even fail to converge.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/kitamura21a/kitamura21a.pdf",
        "supp": "",
        "pdf_size": 563190,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12149485785511614140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Nara Institute of Science and Technology, Nara, JAPAN; Nara Institute of Science and Technology, Nara, JAPAN; Nara Institute of Science and Technology, Nara, JAPAN",
        "aff_domain": "is.naist.jp;is.naist.jp;is.naist.jp",
        "email": "is.naist.jp;is.naist.jp;is.naist.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nist.go.jp",
        "aff_unique_abbr": "NIST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nara",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Greedy Search Algorithm for Mixed Precision in Post-Training Quantization of Convolutional Neural Network Inspired by Submodular Optimization",
        "site": "https://proceedings.mlr.press/v157/satoki21a.html",
        "author": "Tsuji Satoki; Kawaguchi Hiroshi; Inoue Atsuki; Sakai Yasufumi; Yamada Fuyuka",
        "abstract": "For lower bit-widths such as less than 8-bit, many quantization strategies include re-training in order to recover accuracy degradation. However, the re-training works against rapid deployment for wide distribution of quantized models. Therefore, post-training quantization has been getting more attention in recent years. In one example, partial quantization according to the layer sensitivity based on the accuracy after each quantization has been proposed; however, the effects of one layer quantization on the other layers has not taken into account. To further reduce the accuracy degradation, we propose a quantization scheme that considers the effects by continuously updating the accuracy after each layer quantization. Additionally, for more data compression, we extend that scheme to mixed precision, which applies a layer-by-layer fitted bit-width. Since the search space for bit allocation per layer increases exponentially with the number of layers $N$, Existing methods require computationally intensive approach such as network training. Here, we derive practical solutions to the bit allocation problem in polynomial time $O(N^2)$ using a deterministic greedy search algorithm inspired by submodular optimization without any training. For example, the proposed algorithm completes a search on ResNet18 for ImageNet in 1 hour for a single GPU. Compared to the case without updating the layer sensitivity, our method improves the accuracy of the quantized model by more than 1% with multiple convolutional neural networks. For examples, 6-bit quantization of MobileNetV2 achieves 80.1% reduction of model size with -1.10% accuracy degradation. 4-bit quantization of ResNet50 achieves 82.9% size reduction with -0.194% accuracy degradation. Furthermore, results show that the proposed method reduces the accuracy degradation by more than about 0.7% compared to various latest post-training quantization strategies.",
        "bibtex": "@InProceedings{pmlr-v157-satoki21a,\n  title = \t {Greedy Search Algorithm for Mixed Precision in Post-Training Quantization of Convolutional Neural Network Inspired by Submodular Optimization},\n  author =       {Satoki, Tsuji and Hiroshi, Kawaguchi and Atsuki, Inoue and Yasufumi, Sakai and Fuyuka, Yamada},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {886--901},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/satoki21a/satoki21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/satoki21a.html},\n  abstract = \t {For lower bit-widths such as less than 8-bit, many quantization strategies include re-training in order to recover accuracy degradation. However, the re-training works against rapid deployment for wide distribution of quantized models. Therefore, post-training quantization has been getting more attention in recent years. In one example, partial quantization according to the layer sensitivity based on the accuracy after each quantization has been proposed; however, the effects of one layer quantization on the other layers has not taken into account. To further reduce the accuracy degradation, we propose a quantization scheme that considers the effects by continuously updating the accuracy after each layer quantization. Additionally, for more data compression, we extend that scheme to mixed precision, which applies a layer-by-layer fitted bit-width. Since the search space for bit allocation per layer increases exponentially with the number of layers $N$, Existing methods require computationally intensive approach such as network training. Here, we derive practical solutions to the bit allocation problem in polynomial time $O(N^2)$ using a deterministic greedy search algorithm inspired by submodular optimization without any training. For example, the proposed algorithm completes a search on ResNet18 for ImageNet in 1 hour for a single GPU. Compared to the case without updating the layer sensitivity, our method improves the accuracy of the quantized model by more than 1% with multiple convolutional neural networks. For examples, 6-bit quantization of MobileNetV2 achieves 80.1% reduction of model size with -1.10% accuracy degradation. 4-bit quantization of ResNet50 achieves 82.9% size reduction with -0.194% accuracy degradation. Furthermore, results show that the proposed method reduces the accuracy degradation by more than about 0.7% compared to various latest post-training quantization strategies.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/satoki21a/satoki21a.pdf",
        "supp": "",
        "pdf_size": 704911,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10243105915448566984&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Fujitsu Research, Fujitsu Ltd., 4-1-1 Kamikodanaka, Nakahara Ward, Kawasaki City, 2118588, Kanagawa Prefecture, Japan; Graduate School of Science, Technology and Innovation, Kobe University, 1-1 Rokkoudai, Nada Ward, Kobe City, 6578501, Hyogo Prefecture, Japan; Graduate School of Science, Technology and Innovation, Kobe University, 1-1 Rokkoudai, Nada Ward, Kobe City, 6578501, Hyogo Prefecture, Japan; Fujitsu Research, Fujitsu Ltd., 4-1-1 Kamikodanaka, Nakahara Ward, Kawasaki City, 2118588, Kanagawa Prefecture, Japan; Fujitsu Research, Fujitsu Ltd., 4-1-1 Kamikodanaka, Nakahara Ward, Kawasaki City, 2118588, Kanagawa Prefecture, Japan",
        "aff_domain": "fujitsu.com;godzilla.kobe-u.ac.jp;godzilla.kobe-u.ac.jp;fujitsu.com;fujitsu.com",
        "email": "fujitsu.com;godzilla.kobe-u.ac.jp;godzilla.kobe-u.ac.jp;fujitsu.com;fujitsu.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "Fujitsu Ltd.;Kobe University",
        "aff_unique_dep": "Fujitsu Research;Graduate School of Science, Technology and Innovation",
        "aff_unique_url": "https://www.fujitsu.com;https://www.kobe-u.ac.jp",
        "aff_unique_abbr": "Fujitsu;Kobe U",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Kobe",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Hierarchical Semantic Segmentation using Psychometric Learning",
        "site": "https://proceedings.mlr.press/v157/yin21a.html",
        "author": "Lu Yin; Vlado Menkovski; Shwei Liu; Mykola Pechenizkiy",
        "abstract": "Assigning meaning to parts of image data is the goal of semantic image segmentation. Machine learning methods, specifically supervised learning is commonly used in a variety of tasks formulated as semantic segmentation. One of the major challenges in the supervised learning approaches is expressing and collecting the rich knowledge that experts have with respect to the meaning present in the image data. Towards this, typically a fixed set of labels is specified and experts are tasked with annotating the pixels, patches or segments in the images with the given labels. In general, however, the set of classes does not fully capture the rich semantic information present in the images. For example, in medical imaging such as histology images, the different parts of cells could be grouped and sub-grouped based on the expertise of the pathologist.  To achieve such a precise semantic representation of the concepts in the image, we need access to the full depth of knowledge of the annotator. In this work, we develop a novel approach to collect segmentation annotations from experts based on psychometric testing. Our method consists of psychometric testing procedure, active query selection,  query enhancement, and a deep metric learning model to achieve a patch-level image embedding that allows for semantic segmentation of images. We show the merits of our method with evaluation on the synthetically generated image, aerial image and histology image.",
        "bibtex": "@InProceedings{pmlr-v157-yin21a,\n  title = \t {Hierarchical Semantic Segmentation using Psychometric Learning},\n  author =       {Yin, Lu and Menkovski, Vlado and Liu, Shwei and Pechenizkiy, Mykola},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {798--813},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/yin21a/yin21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/yin21a.html},\n  abstract = \t {Assigning meaning to parts of image data is the goal of semantic image segmentation. Machine learning methods, specifically supervised learning is commonly used in a variety of tasks formulated as semantic segmentation. One of the major challenges in the supervised learning approaches is expressing and collecting the rich knowledge that experts have with respect to the meaning present in the image data. Towards this, typically a fixed set of labels is specified and experts are tasked with annotating the pixels, patches or segments in the images with the given labels. In general, however, the set of classes does not fully capture the rich semantic information present in the images. For example, in medical imaging such as histology images, the different parts of cells could be grouped and sub-grouped based on the expertise of the pathologist.  To achieve such a precise semantic representation of the concepts in the image, we need access to the full depth of knowledge of the annotator. In this work, we develop a novel approach to collect segmentation annotations from experts based on psychometric testing. Our method consists of psychometric testing procedure, active query selection,  query enhancement, and a deep metric learning model to achieve a patch-level image embedding that allows for semantic segmentation of images. We show the merits of our method with evaluation on the synthetically generated image, aerial image and histology image.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/yin21a/yin21a.pdf",
        "supp": "",
        "pdf_size": 10005866,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13458620069421369263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Eindhoven University of Technology; Eindhoven University of Technology; Eindhoven University of Technology; Eindhoven University of Technology+University of Jyvaskyla",
        "aff_domain": "tue.nl;tue.nl;tue.nl;tue.nl",
        "email": "tue.nl;tue.nl;tue.nl;tue.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Eindhoven University of Technology;University of Jyvaskyla",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tue.nl;https://www.jyu.fi",
        "aff_unique_abbr": "TU/e;JYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Netherlands;Finland"
    },
    {
        "title": "Hybrid Estimation for Open-Ended Questions with Early-Age Students\u2019 Block-Based Programming Answers",
        "site": "https://proceedings.mlr.press/v157/wu21a.html",
        "author": "Hao Wu; Tianyi Chen; Xianzhe Luo; Canghong Jin; Yun Zhang; Minghui Wu",
        "abstract": "Block-based programming is of great significance for cultivating children\u2019s computational thinking. However, due to the following challenges, it is difficult to evaluate students\u2019 programming ability in online learning systems: 1) compared with the traditional Online Judge (OJ) system, there is no standard answer for a given task in block-based programming; 2) in order to promote students\u2019 interests, although the programs are not totally correct and unrelated to the task, the teacher will give a comparatively higher score. Therefore, current approaches involving output comparison and code analysis do not work effectively. Furthermore, deep learning methods also suffer from the problem of how to represent block code for classification. We propose a novel hybrid estimation model to address these challenges. We first learn graph embedding from the parsed Abstract Syntax Tree (AST) to present the logicality of the code. Next, we provide some methods to measure the workload and complexity of the code. Then, we extracted some key variables and task-irrelevant properties, introduced teacher bias. Finally, XGBoost was constructed for classification. Based on real-world data collected from an online Scratch platform by early-age students, our model outperforms KimCNN, ResNet-18, and Graph2Vec+XGBoost. Moreover, we provided statistical analyses and intuitive explanations to interpret the characteristics in various groups.",
        "bibtex": "@InProceedings{pmlr-v157-wu21a,\n  title = \t {Hybrid Estimation for Open-Ended Questions with Early-Age Students\u2019 Block-Based Programming Answers},\n  author =       {Wu, Hao and Chen, Tianyi and Luo, Xianzhe and Jin, Canghong and Zhang, Yun and Wu, Minghui},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {33--48},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wu21a/wu21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wu21a.html},\n  abstract = \t {Block-based programming is of great significance for cultivating children\u2019s computational thinking. However, due to the following challenges, it is difficult to evaluate students\u2019 programming ability in online learning systems: 1) compared with the traditional Online Judge (OJ) system, there is no standard answer for a given task in block-based programming; 2) in order to promote students\u2019 interests, although the programs are not totally correct and unrelated to the task, the teacher will give a comparatively higher score. Therefore, current approaches involving output comparison and code analysis do not work effectively. Furthermore, deep learning methods also suffer from the problem of how to represent block code for classification. We propose a novel hybrid estimation model to address these challenges. We first learn graph embedding from the parsed Abstract Syntax Tree (AST) to present the logicality of the code. Next, we provide some methods to measure the workload and complexity of the code. Then, we extracted some key variables and task-irrelevant properties, introduced teacher bias. Finally, XGBoost was constructed for classification. Based on real-world data collected from an online Scratch platform by early-age students, our model outperforms KimCNN, ResNet-18, and Graph2Vec+XGBoost. Moreover, we provided statistical analyses and intuitive explanations to interpret the characteristics in various groups.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wu21a/wu21a.pdf",
        "supp": "",
        "pdf_size": 1748680,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CPV_WbVd06YJ:scholar.google.com/&scioq=Hybrid+Estimation+for+Open-Ended+Questions+with+Early-Age+Students%E2%80%99+Block-Based+Programming+Answers&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Zhejiang University City College; Zhejiang University City College; Zhejiang University City College; Zhejiang University City College; Zhejiang University City College; Zhejiang University City College",
        "aff_domain": "stu.zucc.edu.cn;stu.zucc.edu.cn;stu.zucc.edu.cn;zucc.edu.cn;zucc.edu.cn;zucc.edu.cn",
        "email": "stu.zucc.edu.cn;stu.zucc.edu.cn;stu.zucc.edu.cn;zucc.edu.cn;zucc.edu.cn;zucc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University City College",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.zucc.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Hybrid Summarization with Semantic Weighting Reward and Latent Structure Detector",
        "site": "https://proceedings.mlr.press/v157/song21a.html",
        "author": "Mingyang Song; Liping Jing; Yi Feng; Zhiwei Sun; Lin Xiao",
        "abstract": "Text summarization has been a significant challenge in the Nature Process Language (NLP) field. The approach of dealing with text summarization can be roughly divided into two main paradigms: extractive and abstractive manner. The former allows capturing the most representative snippets in a document while the latter generates a summary by understanding the latent meaning in a material with a language generation model. Recently, studies found that jointly employing the extractive and abstractive summarization models can take advantage of their complementary advantages, creating both concise and informative summaries. However, the reinforced summarization models mainly depend on the ROUGE-based reward, which only has the ability to quantify the extent of word-matching rather than semantic-matching between document and summary. Meanwhile, documents are usually collected with redundant or noisy information due to the existence of repeated or irrelevant information in real-world applications. Therefore, only depending on ROUGE-based reward to optimize the reinforced summarization models may lead to biased summary generation. In this paper, we propose a novel deep \\bf{Hy}brid \\bf{S}ummarization with semantic weighting \\bf{R}eward and latent structure \\bf{D}etector (HySRD). Specifically, HySRD introduces a new reward mechanism that simultaneously takes advantage of semantic and syntactic information among documents and summaries. To effectively model the accuracy semantics, a latent structure detector is designed to incorporate the high-level latent structures in the sentence representation for information selection. Extensive experiments have been conducted on two well-known benchmark datasets \\emph{CNN/Daily Mail} (short input document) and \\emph{BigPatent} (long input document). The automatic evaluation shows that our approach significantly outperforms the state-of-the-art of hybrid summarization models.",
        "bibtex": "@InProceedings{pmlr-v157-song21a,\n  title = \t {Hybrid Summarization with Semantic Weighting Reward and Latent Structure Detector},\n  author =       {Song, Mingyang and Jing, Liping and Feng, Yi and Sun, Zhiwei and Xiao, Lin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1739--1754},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/song21a/song21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/song21a.html},\n  abstract = \t {Text summarization has been a significant challenge in the Nature Process Language (NLP) field. The approach of dealing with text summarization can be roughly divided into two main paradigms: extractive and abstractive manner. The former allows capturing the most representative snippets in a document while the latter generates a summary by understanding the latent meaning in a material with a language generation model. Recently, studies found that jointly employing the extractive and abstractive summarization models can take advantage of their complementary advantages, creating both concise and informative summaries. However, the reinforced summarization models mainly depend on the ROUGE-based reward, which only has the ability to quantify the extent of word-matching rather than semantic-matching between document and summary. Meanwhile, documents are usually collected with redundant or noisy information due to the existence of repeated or irrelevant information in real-world applications. Therefore, only depending on ROUGE-based reward to optimize the reinforced summarization models may lead to biased summary generation. In this paper, we propose a novel deep \\bf{Hy}brid \\bf{S}ummarization with semantic weighting \\bf{R}eward and latent structure \\bf{D}etector (HySRD). Specifically, HySRD introduces a new reward mechanism that simultaneously takes advantage of semantic and syntactic information among documents and summaries. To effectively model the accuracy semantics, a latent structure detector is designed to incorporate the high-level latent structures in the sentence representation for information selection. Extensive experiments have been conducted on two well-known benchmark datasets \\emph{CNN/Daily Mail} (short input document) and \\emph{BigPatent} (long input document). The automatic evaluation shows that our approach significantly outperforms the state-of-the-art of hybrid summarization models.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/song21a/song21a.pdf",
        "supp": "",
        "pdf_size": 290977,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14526084744977777909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China",
        "aff_domain": "BJTU.EDU.CN;BJTU.EDU.CN;BJTU.EDU.CN;BJTU.EDU.CN;BJTU.EDU.CN",
        "email": "BJTU.EDU.CN;BJTU.EDU.CN;BJTU.EDU.CN;BJTU.EDU.CN;BJTU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beijing Jiaotong University",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining",
        "aff_unique_url": "http://www.bjtu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Improving Gaussian mixture latent variable model convergence with Optimal Transport",
        "site": "https://proceedings.mlr.press/v157/gaujac21a.html",
        "author": "Benoit Gaujac; Ilya Feige; David Barber",
        "abstract": "Generative models with both discrete and continuous latent variables are highly motivated by the structure of many real-world data sets. They present, however, subtleties in training often manifesting in the discrete latent variable not being leveraged. In this paper, we show why such models struggle to train using traditional log-likelihood maximization, and that they are amenable to training using the Optimal Transport framework of Wasserstein Autoencoders. We find our discrete latent variable to be fully leveraged by the model when trained, without any modifications to the objective function or significant fine tuning. Our model generates comparable samples to other approaches while using relatively simple neural networks, since the discrete latent variable carries much of the descriptive burden. Furthermore, the discrete latent provides significant control over generation.",
        "bibtex": "@InProceedings{pmlr-v157-gaujac21a,\n  title = \t {Improving Gaussian mixture latent variable model convergence with Optimal Transport},\n  author =       {Gaujac, Benoit and Feige, Ilya and Barber, David},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {737--752},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/gaujac21a/gaujac21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/gaujac21a.html},\n  abstract = \t {Generative models with both discrete and continuous latent variables are highly motivated by the structure of many real-world data sets. They present, however, subtleties in training often manifesting in the discrete latent variable not being leveraged. In this paper, we show why such models struggle to train using traditional log-likelihood maximization, and that they are amenable to training using the Optimal Transport framework of Wasserstein Autoencoders. We find our discrete latent variable to be fully leveraged by the model when trained, without any modifications to the objective function or significant fine tuning. Our model generates comparable samples to other approaches while using relatively simple neural networks, since the discrete latent variable carries much of the descriptive burden. Furthermore, the discrete latent provides significant control over generation.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/gaujac21a/gaujac21a.pdf",
        "supp": "",
        "pdf_size": 2019143,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13459543000721681687&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "UCL; Faculty; UCL",
        "aff_domain": "UCL.AC.UK;FACULTY.AI;UCL.AC.UK",
        "email": "UCL.AC.UK;FACULTY.AI;UCL.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London;",
        "aff_unique_dep": ";Faculty",
        "aff_unique_url": "https://www.ucl.ac.uk;",
        "aff_unique_abbr": "UCL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "title": "Improving Hashing Algorithms for Similarity Search via MLE and the Control Variates Trick",
        "site": "https://proceedings.mlr.press/v157/kang21a.html",
        "author": "Keegan Kang; Sergey Kushnarev; Wei Pin Wong; Rameshwar Pratap; Haikal Yeo; Chen Yijia",
        "abstract": "Hashing algorithms are continually used for large-scale learning and similarity search, with computationally cheap and better algorithms being proposed every year. In this paper we focus on hashing algorithms which involve estimating a distance measure $d(\\vec{x}_i,\\vec{x}_j)$ between two vectors $\\vec{x}_i, \\vec{x}_j$. Such hashing algorithms require generation of random variables, and we propose two approaches to reduce the variance of our hashed estimates: control variates and maximum likelihood estimates. We explain how these approaches can be immediately applied to a wide subset of hashing algorithms. Further, we evaluate the impact of these methods on various datasets. We finally run empirical simulations to verify our results.",
        "bibtex": "@InProceedings{pmlr-v157-kang21a,\n  title = \t {Improving Hashing Algorithms for Similarity Search \\textit{via} MLE and the Control Variates Trick},\n  author =       {Kang, Keegan and Kushnarev, Sergey and Wong, {Wei Pin} and Pratap, Rameshwar and Yeo, Haikal and Yijia, Chen},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {814--829},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/kang21a/kang21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/kang21a.html},\n  abstract = \t {Hashing algorithms are continually used for large-scale learning and similarity search, with computationally cheap and better algorithms being proposed every year. In this paper we focus on hashing algorithms which involve estimating a distance measure $d(\\vec{x}_i,\\vec{x}_j)$ between two vectors $\\vec{x}_i, \\vec{x}_j$. Such hashing algorithms require generation of random variables, and we propose two approaches to reduce the variance of our hashed estimates: control variates and maximum likelihood estimates. We explain how these approaches can be immediately applied to a wide subset of hashing algorithms. Further, we evaluate the impact of these methods on various datasets. We finally run empirical simulations to verify our results.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/kang21a/kang21a.pdf",
        "supp": "",
        "pdf_size": 643111,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9469180365392265241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Engineering Systems and Design Pillar, Singapore University of Technology and Design; Department of Applied Mathematics and Statistics, Johns Hopkins University; Science, Mathematics, and Technology Cluster, Singapore University of Technology and Design; School of Computing and Electrical Engineering, IIT Mandi; Independent; Singapore University of Technology and Design",
        "aff_domain": "sutd.edu.sg;jhu.edu;sutd.edu.sg;iitmandi.ac.in;gmail.com;mymail.sutd.edu.sg",
        "email": "sutd.edu.sg;jhu.edu;sutd.edu.sg;iitmandi.ac.in;gmail.com;mymail.sutd.edu.sg",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;3;0",
        "aff_unique_norm": "Singapore University of Technology and Design;Johns Hopkins University;IIT Mandi;Independent",
        "aff_unique_dep": "Engineering Systems and Design Pillar;Department of Applied Mathematics and Statistics;School of Computing and Electrical Engineering;",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.jhu.edu;https://www.iitmandi.ac.in;",
        "aff_unique_abbr": "SUTD;JHU;IIT Mandi;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;0",
        "aff_country_unique": "Singapore;United States;India;"
    },
    {
        "title": "Iterative Deep Model Compression and Acceleration in the Frequency Domain",
        "site": "https://proceedings.mlr.press/v157/zeng21a.html",
        "author": "Yao Zeng; Xusheng Liu; Lintan Sun; Wenzhong Li; Yuchu Fang; Sanglu Lu",
        "abstract": "Deep Convolutional Neural Networks (CNNs) are successfully applied in many complex tasks, but their storage and huge computational costs hinder their deployment on edge devices. CNN model compression techniques have been widely studied in the past five years, most of which are conducted in the spatial domain. Inspired by the sparsity and low-rank properties of weight matrices in the frequency domain, we propose a novel frequency pruning framework for model compression and acceleration while maintaining high-performance. We firstly apply Discrete Cosine Transform (DCT) on convolutional kernels and train them in the frequency domain to get sparse representations. Then we propose an iterative model compression method to decompose the frequency matrices with a sampled-based low-rank approximation algorithm, and then fine-tune and recompose the low-rank matrices gradually until a predefined compression ratio is reached. We further demonstrate that model inference can be conducted with the decomposed frequency matrices, where model parameters and inference cost can be significantly reduced. Extensive experiments using well-known CNN models based on three open datasets show that the proposed method outperforms the state-of-the-arts in reduction of both the number of parameters and floating-point operations (FLOPs) without sacrificing too much model accuracy.",
        "bibtex": "@InProceedings{pmlr-v157-zeng21a,\n  title = \t {Iterative Deep Model Compression and Acceleration in the Frequency Domain},\n  author =       {Zeng, Yao and Liu, Xusheng and Sun, Lintan and Li, Wenzhong and Fang, Yuchu and Lu, Sanglu},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {331--346},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zeng21a/zeng21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zeng21a.html},\n  abstract = \t {Deep Convolutional Neural Networks (CNNs) are successfully applied in many complex tasks, but their storage and huge computational costs hinder their deployment on edge devices. CNN model compression techniques have been widely studied in the past five years, most of which are conducted in the spatial domain. Inspired by the sparsity and low-rank properties of weight matrices in the frequency domain, we propose a novel frequency pruning framework for model compression and acceleration while maintaining high-performance. We firstly apply Discrete Cosine Transform (DCT) on convolutional kernels and train them in the frequency domain to get sparse representations. Then we propose an iterative model compression method to decompose the frequency matrices with a sampled-based low-rank approximation algorithm, and then fine-tune and recompose the low-rank matrices gradually until a predefined compression ratio is reached. We further demonstrate that model inference can be conducted with the decomposed frequency matrices, where model parameters and inference cost can be significantly reduced. Extensive experiments using well-known CNN models based on three open datasets show that the proposed method outperforms the state-of-the-arts in reduction of both the number of parameters and floating-point operations (FLOPs) without sacrificing too much model accuracy.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zeng21a/zeng21a.pdf",
        "supp": "",
        "pdf_size": 563962,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7042698797118452628&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Language Representations for Generalization in Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/goodger21a.html",
        "author": "Nikolaj Goodger; Peter Vamplew; Cameron Foale; Richard Dazeley",
        "abstract": "The choice of state and action representation in Reinforcement Learning (RL) has a significant effect on agent performance for the training task.  But its relationship with generalization to new tasks is under-explored.  One approach to improving generalization investigated here is the use of language as a representation. We compare vector-states and discrete-actions to language representations. We find the agents using language representations generalize better and could solve tasks with more entities, new entities, and more complexity than seen in the training task. We attribute this to the compositionality of language.",
        "bibtex": "@InProceedings{pmlr-v157-goodger21a,\n  title = \t {Language Representations for Generalization in Reinforcement Learning},\n  author =       {Goodger, Nikolaj and Vamplew, Peter and Foale, Cameron and Dazeley, Richard},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {390--405},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/goodger21a/goodger21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/goodger21a.html},\n  abstract = \t {The choice of state and action representation in Reinforcement Learning (RL) has a significant effect on agent performance for the training task.  But its relationship with generalization to new tasks is under-explored.  One approach to improving generalization investigated here is the use of language as a representation. We compare vector-states and discrete-actions to language representations. We find the agents using language representations generalize better and could solve tasks with more entities, new entities, and more complexity than seen in the training task. We attribute this to the compositionality of language.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/goodger21a/goodger21a.pdf",
        "supp": "",
        "pdf_size": 4711542,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15701115745511210966&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Federation University Australia \u2014 Mt Helen Campus PO Box 663 Ballarat VIC 3353; Federation University Australia \u2014 Mt Helen Campus PO Box 663 Ballarat VIC 3353; Federation University Australia \u2014 Mt Helen Campus PO Box 663 Ballarat VIC 3353; Deakin University \u2014 Locked Bag 20000, Geelong, VIC 3220",
        "aff_domain": "students.federation.edu.au;federation.edu.au;federation.edu.au;deakin.edu.au",
        "email": "students.federation.edu.au;federation.edu.au;federation.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Federation University Australia;Deakin University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.federation.edu.au;https://www.deakin.edu.au",
        "aff_unique_abbr": "FedUni;Deakin",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Mt Helen;Geelong",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Layer-Wise Neural Network Compression via Layer Fusion",
        "site": "https://proceedings.mlr.press/v157/o-neill21a.html",
        "author": "James O\u2019Neill; Greg V. Steeg; Aram Galstyan",
        "abstract": "This paper proposes \\textit{layer fusion} - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers. Layer fusion can significantly reduce the number of layers of the original network with little additional computation overhead, while maintaining competitive performance. From experiments on CIFAR-10, we find that various deep convolution neural networks can remain within 2% accuracy points of the original networks up to a compression ratio of 3.33 when iteratively retrained with layer fusion. For experiments on the WikiText-2 language modelling dataset, we compress Transformer models to 20% of their original size while being within 5 perplexity points of the original network. We also find that other well-established compression techniques can achieve competitive performance when compared to their original networks given a sufficient number of retraining steps. Generally, we observe a clear inflection point in performance as the amount of compression increases, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance.",
        "bibtex": "@InProceedings{pmlr-v157-o-neill21a,\n  title = \t {Layer-Wise Neural Network Compression via Layer Fusion},\n  author =       {O'Neill, James and V. Steeg, Greg and Galstyan, Aram},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1381--1396},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/o-neill21a/o-neill21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/o-neill21a.html},\n  abstract = \t { This paper proposes \\textit{layer fusion} - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers. Layer fusion can significantly reduce the number of layers of the original network with little additional computation overhead, while maintaining competitive performance. From experiments on CIFAR-10, we find that various deep convolution neural networks can remain within 2% accuracy points of the original networks up to a compression ratio of 3.33 when iteratively retrained with layer fusion. For experiments on the WikiText-2 language modelling dataset, we compress Transformer models to 20% of their original size while being within 5 perplexity points of the original network. We also find that other well-established compression techniques can achieve competitive performance when compared to their original networks given a sufficient number of retraining steps. Generally, we observe a clear inflection point in performance as the amount of compression increases, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance.  }\n}",
        "pdf": "https://proceedings.mlr.press/v157/o-neill21a/o-neill21a.pdf",
        "supp": "",
        "pdf_size": 456192,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17311817942764724299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Liverpool, Liverpool, England; USC Information Sciences Institute, Marina del Rey, California, 90292, USA; USC Information Sciences Institute, Marina del Rey, California, 90292, USA",
        "aff_domain": "LIVERPOOL.AC.UK;ISI.EDU;ISI.EDU",
        "email": "LIVERPOOL.AC.UK;ISI.EDU;ISI.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Liverpool;University of Southern California",
        "aff_unique_dep": "Department of Computer Science;Information Sciences Institute",
        "aff_unique_url": "https://www.liverpool.ac.uk;https://isi.usc.edu",
        "aff_unique_abbr": "Liv Uni;USC ISI",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Liverpool;Marina del Rey",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Learn to Predict Vertical Track Irregularity with Extremely Imbalanced Data",
        "site": "https://proceedings.mlr.press/v157/chen21c.html",
        "author": "Yutao Chen; Yu Zhang; Fei Yang",
        "abstract": "Railway systems require regular manual maintenance, a large part of which is dedicated to inspecting track deformation. Such deformation might severely impact trains\u2019 runtime security, whereas such inspections remain costly for both finance and human resources. Therefore, a more precise and efficient approach to detect railway track deformation is in urgent need. In this paper, we showcase an application framework for predicting vertical track irregularity, based on a real-world, large-scale dataset produced by several operating railways in China. We have conducted extensive experiments on various machine learning & ensemble learning algorithms in an effort to maximize the model\u2019s capability in capturing any irregularity. We also proposed a novel approach for handling imbalanced data in multivariate time series prediction tasks with adaptive data sampling and penalized loss. Such an approach has proven to reduce models\u2019 sensitivity to the imbalanced target domain, thus improving its performance in predicting rare extreme values.",
        "bibtex": "@InProceedings{pmlr-v157-chen21c,\n  title = \t {Learn to Predict Vertical Track Irregularity with Extremely Imbalanced Data},\n  author =       {Chen, Yutao and Zhang, Yu and Yang, Fei},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1493--1504},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/chen21c/chen21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/chen21c.html},\n  abstract = \t {Railway systems require regular manual maintenance, a large part of which is dedicated to inspecting track deformation. Such deformation might severely impact trains\u2019 runtime security, whereas such inspections remain costly for both finance and human resources. Therefore, a more precise and efficient approach to detect railway track deformation is in urgent need. In this paper, we showcase an application framework for predicting vertical track irregularity, based on a real-world, large-scale dataset produced by several operating railways in China. We have conducted extensive experiments on various machine learning & ensemble learning algorithms in an effort to maximize the model\u2019s capability in capturing any irregularity. We also proposed a novel approach for handling imbalanced data in multivariate time series prediction tasks with adaptive data sampling and penalized loss. Such an approach has proven to reduce models\u2019 sensitivity to the imbalanced target domain, thus improving its performance in predicting rare extreme values.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/chen21c/chen21c.pdf",
        "supp": "",
        "pdf_size": 445099,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3246944776351676773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing University of Posts and Telecommunications, China; China Academy of Railway Sciences Corporation Limited; China Academy of Railway Sciences Corporation Limited",
        "aff_domain": "outlook.com;rails.cn;rails.cn",
        "email": "outlook.com;rails.cn;rails.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;China Academy of Railway Sciences Corporation Limited",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;http://www.carsc.com.cn",
        "aff_unique_abbr": "BUPT;CARSCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning 3-opt heuristics for traveling salesman problem via deep reinforcement learning",
        "site": "https://proceedings.mlr.press/v157/sui21a.html",
        "author": "Jingyan Sui; Shizhe Ding; Ruizhi Liu; Liming Xu; Dongbo Bu",
        "abstract": "Traveling salesman problem (TSP) is a classical combinatorial optimization problem. As it represents a large number of important practical problems, it has received extensive studies and a great variety of algorithms have been proposed to solve it, including exact and heuristic algorithms. The success of heuristic algorithms relies heavily on the design of powerful heuristic rules, and most of the existing heuristic rules were manually designed by experienced experts to model their insights and observations on TSP instances and solutions. Recent studies have shown an alternative promising design strategy that directly learns heuristic rules from TSP instances without any manual interference. Here, we report an iterative improvement approach (called Neural-3-OPT) that solves TSP through automatically learning effective 3-opt heuristics via deep reinforcement learning. In the proposed approach, we adopt a pointer network to select 3 links from the current tour,and a feature-wise linear modulation network to select an appropriate way to reconnect the segments after removing the selected 3 links. We demonstrate that our approach achieves state-of-the-art performance on both real TSP instances and randomly-generated instances than, to the best of our knowledge, the existing neural network-based approaches.",
        "bibtex": "@InProceedings{pmlr-v157-sui21a,\n  title = \t {Learning 3-opt heuristics for traveling salesman problem via deep reinforcement learning},\n  author =       {Sui, Jingyan and Ding, Shizhe and Liu, Ruizhi and Xu, Liming and Bu, Dongbo},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1301--1316},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/sui21a/sui21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/sui21a.html},\n  abstract = \t {Traveling salesman problem (TSP) is a classical combinatorial optimization problem. As it represents a large number of important practical problems, it has received extensive studies and a great variety of algorithms have been proposed to solve it, including exact and heuristic algorithms. The success of heuristic algorithms relies heavily on the design of powerful heuristic rules, and most of the existing heuristic rules were manually designed by experienced experts to model their insights and observations on TSP instances and solutions. Recent studies have shown an alternative promising design strategy that directly learns heuristic rules from TSP instances without any manual interference. Here, we report an iterative improvement approach (called Neural-3-OPT) that solves TSP through automatically learning effective 3-opt heuristics via deep reinforcement learning. In the proposed approach, we adopt a pointer network to select 3 links from the current tour,and a feature-wise linear modulation network to select an appropriate way to reconnect the segments after removing the selected 3 links. We demonstrate that our approach achieves state-of-the-art performance on both real TSP instances and randomly-generated instances than, to the best of our knowledge, the existing neural network-based approaches.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/sui21a/sui21a.pdf",
        "supp": "",
        "pdf_size": 630859,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3894702557873427433&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Learning Maximum Margin Markov Networks from examples with missing labels",
        "site": "https://proceedings.mlr.press/v157/franc21a.html",
        "author": "Vojtech Franc; Andrii Yermakov",
        "abstract": "Structured output classifiers based on the framework of Markov Networks provide a transparent way to model statistical dependencies between output labels. The Markov Network (MN) classifier can be efficiently learned by the maximum margin method, which however requires expensive completely annotated examples. We extend the maximum margin algorithm for learning of unrestricted MN classifiers from examples with partially missing annotation of labels. The proposed algorithm translates learning into minimization of a novel loss function which is convex, has a clear connection with the supervised margin-rescaling loss, and can be efficiently optimized by first-order methods. We demonstrate the efficacy of the proposed algorithm on a challenging structured output classification problem where it beats deep neural network models trained from a much higher number of completely annotated examples, while the proposed method used only partial annotations.",
        "bibtex": "@InProceedings{pmlr-v157-franc21a,\n  title = \t {Learning Maximum Margin Markov Networks from examples with missing labels},\n  author =       {Franc, Vojtech and Yermakov, Andrii},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1691--1706},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/franc21a/franc21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/franc21a.html},\n  abstract = \t {Structured output classifiers based on the framework of Markov Networks provide a transparent way to model statistical dependencies between output labels. The Markov Network (MN) classifier can be efficiently learned by the maximum margin method, which however requires expensive completely annotated examples. We extend the maximum margin algorithm for learning of unrestricted MN classifiers from examples with partially missing annotation of labels. The proposed algorithm translates learning into minimization of a novel loss function which is convex, has a clear connection with the supervised margin-rescaling loss, and can be efficiently optimized by first-order methods. We demonstrate the efficacy of the proposed algorithm on a challenging structured output classification problem where it beats deep neural network models trained from a much higher number of completely annotated examples, while the proposed method used only partial annotations.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/franc21a/franc21a.pdf",
        "supp": "",
        "pdf_size": 603233,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=339405029961729629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic; Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic",
        "aff_domain": "fel.cvut.cz;fel.cvut.cz",
        "email": "fel.cvut.cz;fel.cvut.cz",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Czech Technical University in Prague",
        "aff_unique_dep": "Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.cvut.cz",
        "aff_unique_abbr": "CTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Prague",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "title": "Learning to Switch Optimizers for Quadratic Programming",
        "site": "https://proceedings.mlr.press/v157/getzelman21a.html",
        "author": "Grant Getzelman; Prasanna Balaprakash",
        "abstract": "Quadratic programming (QP) seeks to solve optimization problems involving quadratic functions that can include complex boundary constraints. QP in the unrestricted form is $\\mathcal{NP}$-hard; but when restricted to the convex case, it becomes tractable. Active set and interior point methods are used to solve convex problems, and in the nonconvex case various heuristics or relaxations are used to produce high-quality solutions in finite time. Learning to optimize (L2O) is an emerging approach to design solvers for optimization problems. We develop an L2O approach that uses reinforcement learning to learn a stochastic policy to switch between pre-existing optimization algorithms to solve QP problem instances. In particular, our agent switches between three simple optimizers: Adam, gradient descent, and random search. Our experiments show that the learned optimizer minimizes quadratic functions faster and finds better-quality solutions in the long term than do any of the possible optimizers switched between. We also compare our solver with the standard QP algorithms in MATLAB and find better performance in fewer function evaluations.",
        "bibtex": "@InProceedings{pmlr-v157-getzelman21a,\n  title = \t {Learning to Switch Optimizers for Quadratic Programming},\n  author =       {Getzelman, Grant and Balaprakash, Prasanna},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1553--1568},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/getzelman21a/getzelman21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/getzelman21a.html},\n  abstract = \t {Quadratic programming (QP) seeks to solve optimization problems involving quadratic functions that can include complex boundary constraints. QP in the unrestricted form is $\\mathcal{NP}$-hard; but when restricted to the convex case, it becomes tractable. Active set and interior point methods are used to solve convex problems, and in the nonconvex case various heuristics or relaxations are used to produce high-quality solutions in finite time. Learning to optimize (L2O) is an emerging approach to design solvers for optimization problems. We develop an L2O approach that uses reinforcement learning to learn a stochastic policy to switch between pre-existing optimization algorithms to solve QP problem instances. In particular, our agent switches between three simple optimizers: Adam, gradient descent, and random search. Our experiments show that the learned optimizer minimizes quadratic functions faster and finds better-quality solutions in the long term than do any of the possible optimizers switched between. We also compare our solver with the standard QP algorithms in MATLAB and find better performance in fewer function evaluations.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/getzelman21a/getzelman21a.pdf",
        "supp": "",
        "pdf_size": 534896,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14041380230991841710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL 60439, USA; Mathematics and Computer Science Division & Leadership Computing Facility, Argonne National Laboratory, Lemont, IL 60439, USA",
        "aff_domain": "gmail.com;anl.gov",
        "email": "gmail.com;anl.gov",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Argonne National Laboratory",
        "aff_unique_dep": "Mathematics and Computer Science Division",
        "aff_unique_url": "https://www.anl.gov",
        "aff_unique_abbr": "ANL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lemont",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Lifelong Learning with Branching Experts",
        "site": "https://proceedings.mlr.press/v157/wu21c.html",
        "author": "Yi-Shan Wu; Yi-Te Hong; Chi-Jen Lu",
        "abstract": "The problem of branching experts is an extension of the experts problem where the set of experts may grow over time. We compare this problem in different learning settings along several axes: adversarial versus stochastic losses; a fixed versus a growing set of experts (branching experts); and single-task versus lifelong learning with expert advice. First, for the branching experts problem, we achieve tight regret bounds in both adversarial and stochastic setting with a single algorithm. While it was known that the adversarial branching experts problem is strictly harder than the non-branching one, the stochastic branching experts problem is in fact no harder. Next, we study the extension to the lifelong learning with expert advice in which one has to make online predictions with a sequence of tasks. For this problem, we provide a single algorithm which works for both adversarial and stochastic setting, and our bounds when specialized to the case without branching recover the regret bounds previously achieved separately via different algorithms. Furthermore, we prove a regret lower bound which shows that in the lifelong learning scenario, the case with branching experts now becomes strictly harder than the non-branching case in the stochastic setting.",
        "bibtex": "@InProceedings{pmlr-v157-wu21c,\n  title = \t {Lifelong Learning with Branching Experts},\n  author =       {Wu, Yi-Shan and Hong, Yi-Te and Lu, Chi-Jen},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1161--1175},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wu21c/wu21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wu21c.html},\n  abstract = \t {The problem of branching experts is an extension of the experts problem where the set of experts may grow over time. We compare this problem in different learning settings along several axes: adversarial versus stochastic losses; a fixed versus a growing set of experts (branching experts); and single-task versus lifelong learning with expert advice. First, for the branching experts problem, we achieve tight regret bounds in both adversarial and stochastic setting with a single algorithm. While it was known that the adversarial branching experts problem is strictly harder than the non-branching one, the stochastic branching experts problem is in fact no harder. Next, we study the extension to the lifelong learning with expert advice in which one has to make online predictions with a sequence of tasks. For this problem, we provide a single algorithm which works for both adversarial and stochastic setting, and our bounds when specialized to the case without branching recover the regret bounds previously achieved separately via different algorithms. Furthermore, we prove a regret lower bound which shows that in the lifelong learning scenario, the case with branching experts now becomes strictly harder than the non-branching case in the stochastic setting.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wu21c/wu21c.pdf",
        "supp": "",
        "pdf_size": 272802,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:28cSfKWGKI4J:scholar.google.com/&scioq=Lifelong+Learning+with+Branching+Experts&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Copenhagen, Denmark; Institute of Information Science, Academia Sinica, Taiwan+Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Institute of Information Science, Academia Sinica, Taiwan",
        "aff_domain": "di.ku.dk;iis.sinica.edu.tw;iis.sinica.edu.tw",
        "email": "di.ku.dk;iis.sinica.edu.tw;iis.sinica.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1",
        "aff_unique_norm": "University of Copenhagen;Academia Sinica;National Taiwan University",
        "aff_unique_dep": "Department of Computer Science;Institute of Information Science;Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ku.dk;https://www.sinica.edu.tw;https://www.ntu.edu.tw",
        "aff_unique_abbr": "UCPH;AS;NTU",
        "aff_campus_unique_index": "1+1;1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "0;1+1;1",
        "aff_country_unique": "Denmark;China"
    },
    {
        "title": "Lifelong Learning with Sketched Structural Regularization",
        "site": "https://proceedings.mlr.press/v157/li21b.html",
        "author": "Haoran Li; Aditya Krishnan; Jingfeng Wu; Soheil Kolouri; Praveen K. Pilly; Vladimir Braverman",
        "abstract": "Preventing catastrophic forgetting while continually learning new tasks is an essential problem in lifelong learning. Structural regularization (SR) refers to a family of algorithms that mitigate catastrophic forgetting by penalizing the network for changing its \u201ccritical parameters\" from previous tasks while learning a new one. The penalty is often induced via a quadratic regularizer defined by an \\emph{importance matrix}, e.g., the (empirical) Fisher information matrix in the Elastic Weight Consolidation framework. In practice and due to computational constraints, most SR methods crudely approximate the importance matrix by its diagonal. In this paper, we propose \\emph{Sketched Structural Regularization} (Sketched SR) as an alternative approach to compress the importance matrices used for regularizing in SR methods. Specifically, we apply \\emph{linear sketching methods} to better approximate the importance matrices in SR algorithms. We show that sketched SR: (i) is computationally efficient and straightforward to implement, (ii) provides an approximation error that is justified in theory, and (iii) is method oblivious by construction and can be adapted to any method that belongs to the SR class. We show that our proposed approach consistently improves various SR algorithms\u2019 performance on both synthetic experiments and benchmark continual learning tasks, including permuted-MNIST and CIFAR-100.",
        "bibtex": "@InProceedings{pmlr-v157-li21b,\n  title = \t {Lifelong Learning with Sketched Structural Regularization},\n  author =       {Li, Haoran and Krishnan, Aditya and Wu, Jingfeng and Kolouri, Soheil and Pilly, Praveen K. and Braverman, Vladimir},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {985--1000},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/li21b/li21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/li21b.html},\n  abstract = \t {Preventing catastrophic forgetting while continually learning new tasks is an essential problem in lifelong learning. Structural regularization (SR) refers to a family of algorithms that mitigate catastrophic forgetting by penalizing the network for changing its \u201ccritical parameters\" from previous tasks while learning a new one. The penalty is often induced via a quadratic regularizer defined by an \\emph{importance matrix}, e.g., the (empirical) Fisher information matrix in the Elastic Weight Consolidation framework. In practice and due to computational constraints, most SR methods crudely approximate the importance matrix by its diagonal. In this paper, we propose \\emph{Sketched Structural Regularization} (Sketched SR) as an alternative approach to compress the importance matrices used for regularizing in SR methods. Specifically, we apply \\emph{linear sketching methods} to better approximate the importance matrices in SR algorithms. We show that sketched SR: (i) is computationally efficient and straightforward to implement, (ii) provides an approximation error that is justified in theory, and (iii) is method oblivious by construction and can be adapted to any method that belongs to the SR class. We show that our proposed approach consistently improves various SR algorithms\u2019 performance on both synthetic experiments and benchmark continual learning tasks, including permuted-MNIST and CIFAR-100.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/li21b/li21b.pdf",
        "supp": "",
        "pdf_size": 1942774,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13207863202217097504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Johns Hopkins University, Baltimore, MD 21218, USA; Johns Hopkins University, Baltimore, MD 21218, USA; Johns Hopkins University, Baltimore, MD 21218, USA; Vanderbilt University, Nashville, TN 37235, USA; HRL Laboratories, LLC, Malibu, CA 90265, USA; Johns Hopkins University, Baltimore, MD 21218, USA",
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu;vanderbilt.edu;hrl.com;cs.jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu;vanderbilt.edu;hrl.com;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "Johns Hopkins University;Vanderbilt University;HRL Laboratories, LLC",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.jhu.edu;https://www.vanderbilt.edu;https://www.hrl.com",
        "aff_unique_abbr": "JHU;Vanderbilt;HRL",
        "aff_campus_unique_index": "0;0;0;1;2;0",
        "aff_campus_unique": "Baltimore;Nashville;Malibu",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Local Aggressive Adversarial Attacks on 3D Point Cloud",
        "site": "https://proceedings.mlr.press/v157/sun21a.html",
        "author": "Yiming Sun; Feng Chen; Zhiyu Chen; Mingjie Wang",
        "abstract": "Deep neural networks are found to be prone to adversarial examples which could deliberately fool the model to make mistakes. Recently, a few of works expand this task from 2D image to 3D point cloud by using global point cloud optimization. However, the perturbations of global point are not effective for misleading the victim model. First, not all points are important in optimization toward misleading. Abundant points account considerable distortion budget but contribute trivially to attack. Second, the multi-label optimization is suboptimal for adversarial attack, since it consumes extra energy in finding multi-label victim model collapse and causes instance transformation to be dissimilar to any particular instance. Third, the independent adversarial and perceptibility losses, caring misclassification and dissimilarity separately, treat the updating of each point equally without a focus. Therefore, once perceptibility loss approaches its budget threshold, all points would be stock in the surface of hypersphere and attack would be locked in local optimality. Therefore, we propose a local aggressive adversarial attacks (L3A) to solve above issues. Technically, we select a bunch of salient points, the high-score subset of point cloud according to gradient, to perturb. Then a flow of aggressive optimization strategies are developed to reinforce the unperceptive generation of adversarial examples toward misleading victim models. Extensive experiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art performance of our method against existing adversarial attack methods.",
        "bibtex": "@InProceedings{pmlr-v157-sun21a,\n  title = \t {Local Aggressive Adversarial Attacks on 3D Point Cloud},\n  author =       {Sun, Yiming and Chen, Feng and Chen, Zhiyu and Wang, Mingjie},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {65--80},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/sun21a/sun21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/sun21a.html},\n  abstract = \t {Deep neural networks are found to be prone to adversarial examples which could deliberately fool the model to make mistakes. Recently, a few of works expand this task from 2D image to 3D point cloud by using global point cloud optimization. However, the perturbations of global point are not effective for misleading the victim model. First, not all points are important in optimization toward misleading. Abundant points account considerable distortion budget but contribute trivially to attack. Second, the multi-label optimization is suboptimal for adversarial attack, since it consumes extra energy in finding multi-label victim model collapse and causes instance transformation to be dissimilar to any particular instance. Third, the independent adversarial and perceptibility losses, caring misclassification and dissimilarity separately, treat the updating of each point equally without a focus. Therefore, once perceptibility loss approaches its budget threshold, all points would be stock in the surface of hypersphere and attack would be locked in local optimality. Therefore, we propose a local aggressive adversarial attacks (L3A) to solve above issues. Technically, we select a bunch of salient points, the high-score subset of point cloud according to gradient, to perturb. Then a flow of aggressive optimization strategies are developed to reinforce the unperceptive generation of adversarial examples toward misleading victim models. Extensive experiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art performance of our method against existing adversarial attack methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/sun21a/sun21a.pdf",
        "supp": "",
        "pdf_size": 734350,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17095101855226885919&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Nanjing University of Posts and Telecommunications; Nanjing University of Posts and Telecommunications; Nanjing University of Posts and Telecommunications; BNU-HKBU United International College",
        "aff_domain": "njupt.edu.cn;gmail.com;njupt.edu.cn;yahoo.com",
        "email": "njupt.edu.cn;gmail.com;njupt.edu.cn;yahoo.com",
        "github": "https://github.com/Chenfeng1271/L3A",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Nanjing University of Posts and Telecommunications;United International College",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.njupt.edu.cn;https://www.uic.edu.hk",
        "aff_unique_abbr": "NJUPT;UIC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nanjing;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Max-Utility Based Arm Selection Strategy For Sequential Query Recommendations",
        "site": "https://proceedings.mlr.press/v157/puthiya-parambath21a.html",
        "author": "Shameem Puthiya Parambath; Christos Anagnostopoulos; Roderick Murray-Smith; Sean MacAvaney; Evangelos and Zervas",
        "abstract": "We consider the query recommendation problem in closed loop interactive learning settings like online information gathering and exploratory analytics. The problem can be naturally modelled using the Multi-Armed Bandits (MAB) framework with countably many arms. The standard MAB algorithms for countably many arms begin with selecting a random set of candidate arms and then applying standard MAB algorithms, e.g., UCB, on this candidate set downstream. We show that such a selection strategy often results in higher cumulative regret and to this end, we propose a selection strategy based on the maximum utility of the arms. We show that in tasks like online information gathering, where sequential query recommendations are employed, the sequences of queries are correlated and the number of potentially optimal queries can be reduced to a manageable size by selecting queries with maximum utility with respect to the currently executing query. Our experimental results using a recent real online literature discovery service log file demonstrate that the proposed arm selection strategy improves the cumulative regret substantially with respect to the state-of-the-art baseline algorithms. Our data model and source code are available at \u00a0\\url{https://anonymous.4open.science/r/0e5ad6b7-ac02-4577-9212-c9d505d3dbdb/}",
        "bibtex": "@InProceedings{pmlr-v157-puthiya-parambath21a,\n  title = \t {Max-Utility Based Arm Selection Strategy For Sequential Query Recommendations},\n  author =       {Puthiya Parambath, Shameem and Anagnostopoulos, Christos and Murray-Smith, Roderick and MacAvaney, Sean and and Zervas, Evangelos},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {564--579},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/puthiya-parambath21a/puthiya-parambath21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/puthiya-parambath21a.html},\n  abstract = \t {We consider the query recommendation problem in closed loop interactive learning settings like online information gathering and exploratory analytics. The problem can be naturally modelled using the Multi-Armed Bandits (MAB) framework with countably many arms. The standard MAB algorithms for countably many arms begin with selecting a random set of candidate arms and then applying standard MAB algorithms, e.g., UCB, on this candidate set downstream. We show that such a selection strategy often results in higher cumulative regret and to this end, we propose a selection strategy based on the maximum utility of the arms. We show that in tasks like online information gathering, where sequential query recommendations are employed, the sequences of queries are correlated and the number of potentially optimal queries can be reduced to a manageable size by selecting queries with maximum utility with respect to the currently executing query. Our experimental results using a recent real online literature discovery service log file demonstrate that the proposed arm selection strategy improves the cumulative regret substantially with respect to the state-of-the-art baseline algorithms. Our data model and source code are available at \u00a0\\url{https://anonymous.4open.science/r/0e5ad6b7-ac02-4577-9212-c9d505d3dbdb/}}\n}",
        "pdf": "https://proceedings.mlr.press/v157/puthiya-parambath21a/puthiya-parambath21a.pdf",
        "supp": "",
        "pdf_size": 413279,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6469125374393616827&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Glasgow, Glasgow, UK; University of Glasgow, Glasgow, UK; University of Glasgow, Glasgow, UK; University of Glasgow, Glasgow, UK; University of West Attica, Greece",
        "aff_domain": "glasgow.ac.uk;glasgow.ac.uk;glasgow.ac.uk;glasgow.ac.uk;uniwa.gr",
        "email": "glasgow.ac.uk;glasgow.ac.uk;glasgow.ac.uk;glasgow.ac.uk;uniwa.gr",
        "github": "",
        "project": "https://anonymous.4open.science/r/0e5ad6b7-ac02-4577-9212-c9d505d3dbdb/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Glasgow;University of West Attica",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gla.ac.uk;https://www.uoa.gr",
        "aff_unique_abbr": "UoG;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Glasgow;",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United Kingdom;Greece"
    },
    {
        "title": "Maximization of Monotone $k$-Submodular Functions with Bounded Curvature and Non-$k$-Submodular Functions",
        "site": "https://proceedings.mlr.press/v157/matsuoka21b.html",
        "author": "Tatsuya Matsuoka; Naoto Ohsaka",
        "abstract": "The concept of $k$-submodularity is an extension of submodularity, of which maximization has various applications, such as influence maximization and sensor placement. In such situations, to model complicated real problems, we want to deal with multiple factors, such as, more detailed parameter representing a property of a given function or a constraint which should be imposed for a given function, simultaneously. Besides, it is preferable that an algorithm for the modeling problem is simple. In this paper, for both monotone $k$-submodular function maximization with bounded curvature and monotone weakly $k$-submodular function maximization, we give approximation ratio analysis on greedy-type algorithms on the problem with the matroid constraint and that with the individual size constraint. Furthermore, we give an approximation ratio analysis on another type of the relaxation of $k$-submodular functions, approximately $k$-submodular functions, with the matroid constraint.",
        "bibtex": "@InProceedings{pmlr-v157-matsuoka21b,\n  title = \t {Maximization of Monotone $k$-Submodular Functions with Bounded Curvature and Non-$k$-Submodular Functions},\n  author =       {Matsuoka, Tatsuya and Ohsaka, Naoto},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1707--1722},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/matsuoka21b/matsuoka21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/matsuoka21b.html},\n  abstract = \t {The concept of $k$-submodularity is an extension of submodularity, of which maximization has various applications, such as influence maximization and sensor placement. In such situations, to model complicated real problems, we want to deal with multiple factors, such as, more detailed parameter representing a property of a given function or a constraint which should be imposed for a given function, simultaneously. Besides, it is preferable that an algorithm for the modeling problem is simple. In this paper, for both monotone $k$-submodular function maximization with bounded curvature and monotone weakly $k$-submodular function maximization, we give approximation ratio analysis on greedy-type algorithms on the problem with the matroid constraint and that with the individual size constraint. Furthermore, we give an approximation ratio analysis on another type of the relaxation of $k$-submodular functions, approximately $k$-submodular functions, with the matroid constraint.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/matsuoka21b/matsuoka21b.pdf",
        "supp": "",
        "pdf_size": 330226,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1157035089578735746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "NEC Corporation; NEC Corporation",
        "aff_domain": "nec.com;nec.com",
        "email": "nec.com;nec.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec.com",
        "aff_unique_abbr": "NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Meta-Model-Based Meta-Policy Optimization",
        "site": "https://proceedings.mlr.press/v157/hiraoka21a.html",
        "author": "Takuya Hiraoka; Takahisa Imagawa; Voot Tangkaratt; Takayuki Osa; Takashi Onishi; Yoshimasa Tsuruoka",
        "abstract": "Model-based meta-reinforcement learning (RL) methods have recently been shown to be a promising approach to improving the sample efficiency of RL in multi-task settings. However, the theoretical understanding of those methods is yet to be established, and there is currently no theoretical guarantee of their performance in a real-world environment. In this paper, we analyze the performance guarantee of model-based meta-RL methods by extending the theorems proposed by Janner et al. (2019). On the basis of our theoretical results, we propose Meta-Model-Based Meta-Policy Optimization (M3PO), a model-based meta-RL method with a performance guarantee. We demonstrate that M3PO outperforms existing meta-RL methods in continuous-control benchmarks.",
        "bibtex": "@InProceedings{pmlr-v157-hiraoka21a,\n  title = \t {Meta-Model-Based Meta-Policy Optimization},\n  author =       {Hiraoka, Takuya and Imagawa, Takahisa and Tangkaratt, Voot and Osa, Takayuki and Onishi, Takashi and Tsuruoka, Yoshimasa},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {129--144},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/hiraoka21a/hiraoka21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/hiraoka21a.html},\n  abstract = \t {Model-based meta-reinforcement learning (RL) methods have recently been shown to be a promising approach to improving the sample efficiency of RL in multi-task settings. However, the theoretical understanding of those methods is yet to be established, and there is currently no theoretical guarantee of their performance in a real-world environment. In this paper, we analyze the performance guarantee of model-based meta-RL methods by extending the theorems proposed by Janner et al. (2019). On the basis of our theoretical results, we propose Meta-Model-Based Meta-Policy Optimization (M3PO), a model-based meta-RL method with a performance guarantee. We demonstrate that M3PO outperforms existing meta-RL methods in continuous-control benchmarks.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/hiraoka21a/hiraoka21a.pdf",
        "supp": "",
        "pdf_size": 5414042,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13486215565327963611&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Metric Learning for comparison of HMMs using Graph Neural Networks",
        "site": "https://proceedings.mlr.press/v157/soni21a.html",
        "author": "Rajan Kumar Soni; Karthick Seshadri; Balaraman Ravindran",
        "abstract": "Hidden Markov models (HMMs) belong to the class of double embedded stochastic models which were originally leveraged for speech recognition and synthesis. HMMs subsequently became a generic sequence model across multiple domains like NLP, bio-informatics and thermodynamics to name a few. Literature has several heuristic metrics to compare two HMMs by factoring in their structure and emission probability distributions in HMM nodes. However, typical structure-based metrics overlook the similarity between HMMs having different structures yet similar behavior and typical behavior-based metrics rely on the representativeness of the reference sequence used for assessing the similarity in behavior. Further, little exploration has taken place in leveraging the recent advancements in deep graph neural networks for learning effective representations for HMMs. In this paper, we propose two novel deep neural network based approaches to learn embeddings for HMMs and evaluate the validity of the embeddings based on subsequent clustering and classification tasks. Our proposed approaches use a Graph variational Autoencoder and diffpooling based Graph neural network (GNN) to learn embeddings for HMMs. The graph autoencoder infers latent low-dimensional flat embeddings for HMMs in a task-agnostic manner; whereas the diffpooling based graph neural network learns class-label aware embeddings by inferring and aggregating a hierarchical set of clusters and sub-clusters of graph nodes. Empirical results reveal that the HMM embeddings learnt through the Graph variational autoencoders and diffpooling based GNN outperform the popular heuristics as measured by the cluster quality metrics and the classification accuracy in downstream tasks.",
        "bibtex": "@InProceedings{pmlr-v157-soni21a,\n  title = \t {Metric Learning for comparison of HMMs using Graph Neural Networks},\n  author =       {Soni, Rajan Kumar and Seshadri, Karthick and Ravindran, Balaraman},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1365--1380},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/soni21a/soni21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/soni21a.html},\n  abstract = \t {Hidden Markov models (HMMs) belong to the class of double embedded stochastic models which were originally leveraged for speech recognition and synthesis. HMMs subsequently became a generic sequence model across multiple domains like NLP, bio-informatics and thermodynamics to name a few. Literature has several heuristic metrics to compare two HMMs by factoring in their structure and emission probability distributions in HMM nodes. However, typical structure-based metrics overlook the similarity between HMMs having different structures yet similar behavior and typical behavior-based metrics rely on the representativeness of the reference sequence used for assessing the similarity in behavior. Further, little exploration has taken place in leveraging the recent advancements in deep graph neural networks for learning effective representations for HMMs. In this paper, we propose two novel deep neural network based approaches to learn embeddings for HMMs and evaluate the validity of the embeddings based on subsequent clustering and classification tasks. Our proposed approaches use a Graph variational Autoencoder and diffpooling based Graph neural network (GNN) to learn embeddings for HMMs. The graph autoencoder infers latent low-dimensional flat embeddings for HMMs in a task-agnostic manner; whereas the diffpooling based graph neural network learns class-label aware embeddings by inferring and aggregating a hierarchical set of clusters and sub-clusters of graph nodes. Empirical results reveal that the HMM embeddings learnt through the Graph variational autoencoders and diffpooling based GNN outperform the popular heuristics as measured by the cluster quality metrics and the classification accuracy in downstream tasks.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/soni21a/soni21a.pdf",
        "supp": "",
        "pdf_size": 670700,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12645011406807554931&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Robert Bosch Centre for Data Science and AI, Indian Institute of Technology, Madras; Department of Computer Science and Engineering, National Institute of Technology, Andhra Pradesh; Robert Bosch Centre for Data Science and AI, Indian Institute of Technology, Madras",
        "aff_domain": "gmail.com;nitandhra.ac.in;cse.iitm.ac.in",
        "email": "gmail.com;nitandhra.ac.in;cse.iitm.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology, Madras;National Institute of Technology, Andhra Pradesh",
        "aff_unique_dep": "Robert Bosch Centre for Data Science and AI;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.nitap.ac.in",
        "aff_unique_abbr": "IIT Madras;NIT Andhra Pradesh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Modeling Risky Choices in Unknown Environments",
        "site": "https://proceedings.mlr.press/v157/tanskanen21a.html",
        "author": "Ville Tanskanen; Chang Rajani; Homayun Afrabandpey; Aini Putkonen; Aur\u00e9lien Nioche; Arto Klami",
        "abstract": "Decision-theoretic models explain human behavior in choice problems involving uncertainty, in terms of individual tendencies such as risk aversion. However, many classical models of risk require knowing the distribution of possible outcomes (rewards) for all options, limiting their applicability outside of controlled experiments. We study the task of learning such models in contexts where the modeler does not know the distributions but instead can only observe the choices and their outcomes for a user familiar with the decision problems, for example a skilled player playing a digital game. We propose a framework combining two separate components, one for modeling the unknown decision-making environment and another for the risk behavior. By using environment models capable of learning distributions we are able to infer classical models of decision-making under risk from observations of the user\u2019s choices and outcomes alone, and we also demonstrate alternative models for predictive purposes. We validate the approach on artificial data and demonstrate a practical use case in modeling risk attitudes of professional esports teams.",
        "bibtex": "@InProceedings{pmlr-v157-tanskanen21a,\n  title = \t {Modeling Risky Choices in Unknown Environments},\n  author =       {Tanskanen, Ville and Rajani, Chang and Afrabandpey, Homayun and Putkonen, Aini and Nioche, Aur\\'elien and Klami, Arto},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1081--1096},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/tanskanen21a/tanskanen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/tanskanen21a.html},\n  abstract = \t {Decision-theoretic models explain human behavior in choice problems involving uncertainty, in terms of individual tendencies such as risk aversion. However, many classical models of risk require knowing the distribution of possible outcomes (rewards) for all options, limiting their applicability outside of controlled experiments. We study the task of learning such models in contexts where the modeler does not know the distributions but instead can only observe the choices and their outcomes for a user familiar with the decision problems, for example a skilled player playing a digital game. We propose a framework combining two separate components, one for modeling the unknown decision-making environment and another for the risk behavior. By using environment models capable of learning distributions we are able to infer classical models of decision-making under risk from observations of the user\u2019s choices and outcomes alone, and we also demonstrate alternative models for predictive purposes. We validate the approach on artificial data and demonstrate a practical use case in modeling risk attitudes of professional esports teams.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/tanskanen21a/tanskanen21a.pdf",
        "supp": "",
        "pdf_size": 562927,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8182400135241298395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Helsinki, Finland; Department of Computer Science, University of Helsinki, Finland; Nokia, Finland; Department of Communications and Networking, Aalto University, Finland; ; Department of Computer Science, University of Helsinki, Finland",
        "aff_domain": "helsinki.fi;helsinki.fi;nokia.com;aalto.fi;gmail.com;helsinki.fi",
        "email": "helsinki.fi;helsinki.fi;nokia.com;aalto.fi;gmail.com;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Helsinki;Nokia;Aalto University",
        "aff_unique_dep": "Department of Computer Science;;Department of Communications and Networking",
        "aff_unique_url": "https://www.helsinki.fi;https://www.nokia.com;https://www.aalto.fi",
        "aff_unique_abbr": "UH;Nokia;Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Multi-Branch Network for Cross-Subject EEG-based Emotion Recognition",
        "site": "https://proceedings.mlr.press/v157/lin21a.html",
        "author": "Guang Lin; Li Zhu; Bin Ren; Yiteng Hu; Jianhai Zhang",
        "abstract": "In recent years, electrocardiogram (EEG)-based emotion recognition has received increasing attention in affective computing. Since the individual differences of EEG signals are large, most models are trained for specific subjects, and the generalization is poor when applied to new subjects. In this paper, we propose a Multi-Branch Network (MBN) model to solve this problem. According to the characteristics of the cross-subject data, different branch networks are designed to separate the background features and task features of the EEG signals for classification to have better model performance. Besides, there is no new-subject data needed during model training. In order to avoid the negative improvement caused by samples with significant differences to model training, a tiny amount of new-subject data is used to filter the training samples to improve the model performance further. Before training the model, the samples with significant differences from the new subject were deleted by comparing the background features between the subjects. The experimental results show that compared with Single-Branch Network (SBN) model, the accuracy of the MBN model is improved by 20.89% on the SEED dataset. Furthermore, compared with other common methods, the proposed method uses less new-subject data, which improves its practicability in practical application.",
        "bibtex": "@InProceedings{pmlr-v157-lin21a,\n  title = \t {Multi-Branch Network for Cross-Subject EEG-based Emotion Recognition},\n  author =       {Lin, Guang and Zhu, Li and Ren, Bin and Hu, Yiteng and Zhang, Jianhai},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {705--720},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/lin21a/lin21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/lin21a.html},\n  abstract = \t {In recent years, electrocardiogram (EEG)-based emotion recognition has received increasing attention in affective computing. Since the individual differences of EEG signals are large, most models are trained for specific subjects, and the generalization is poor when applied to new subjects. In this paper, we propose a Multi-Branch Network (MBN) model to solve this problem. According to the characteristics of the cross-subject data, different branch networks are designed to separate the background features and task features of the EEG signals for classification to have better model performance. Besides, there is no new-subject data needed during model training. In order to avoid the negative improvement caused by samples with significant differences to model training, a tiny amount of new-subject data is used to filter the training samples to improve the model performance further. Before training the model, the samples with significant differences from the new subject were deleted by comparing the background features between the subjects. The experimental results show that compared with Single-Branch Network (SBN) model, the accuracy of the MBN model is improved by 20.89% on the SEED dataset. Furthermore, compared with other common methods, the proposed method uses less new-subject data, which improves its practicability in practical application.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/lin21a/lin21a.pdf",
        "supp": "",
        "pdf_size": 859156,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9892444248969892013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Hangzhou Dianzi University, Hangzhou, China; Hangzhou Dianzi University, Hangzhou, China; Hangzhou Dianzi University, Hangzhou, China; Hangzhou Dianzi University, Hangzhou, China; Hangzhou Dianzi University, Hangzhou, China",
        "aff_domain": "hdu.edu.cn;hdu.edu.cn;hdu.edu.cn;hdu.edu.cn;hdu.edu.cn",
        "email": "hdu.edu.cn;hdu.edu.cn;hdu.edu.cn;hdu.edu.cn;hdu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Hangzhou Dianzi University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.hdu.edu.cn/",
        "aff_unique_abbr": "HGHDU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-factor Memory Attentive Model for Knowledge Tracing",
        "site": "https://proceedings.mlr.press/v157/liu21c.html",
        "author": "Congjie Liu; Xiaoguang Li",
        "abstract": "The traditional knowledge tracing with neural network usually embeds the required information and predicates the knowledge proficiency by embedded information. Only few information, however, is considered in traditional methods, such as the information of exercises in terms of concept. In this paper, we propose a multi-factor memory attentive model for knowledge tracing (MMAKT). In terms of Neural Cognitive Diagnosis (NeuralCD) framework, MMAKT introduces the factors of the knowledge concept relevancy, the difficulty of each concept, the discrimination among exercises and the student\u2019s proficiency to construct interaction vectors.  Moreover, in order to achieve more accurate prediction precision, MMAKT introduces attention mechanism to enhance the expression of historical relationship between interactions. With the experiments on the real-world datasets, MMAKT shows better performance of knowledge tracing and prediction in comparision with the state-of-the-art approaches.",
        "bibtex": "@InProceedings{pmlr-v157-liu21c,\n  title = \t {Multi-factor Memory Attentive Model for Knowledge Tracing},\n  author =       {Liu, Congjie and Li, Xiaoguang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {856--869},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/liu21c/liu21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/liu21c.html},\n  abstract = \t {The traditional knowledge tracing with neural network usually embeds the required information and predicates the knowledge proficiency by embedded information. Only few information, however, is considered in traditional methods, such as the information of exercises in terms of concept. In this paper, we propose a multi-factor memory attentive model for knowledge tracing (MMAKT). In terms of Neural Cognitive Diagnosis (NeuralCD) framework, MMAKT introduces the factors of the knowledge concept relevancy, the difficulty of each concept, the discrimination among exercises and the student\u2019s proficiency to construct interaction vectors.  Moreover, in order to achieve more accurate prediction precision, MMAKT introduces attention mechanism to enhance the expression of historical relationship between interactions. With the experiments on the real-world datasets, MMAKT shows better performance of knowledge tracing and prediction in comparision with the state-of-the-art approaches.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/liu21c/liu21c.pdf",
        "supp": "",
        "pdf_size": 609695,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3505512502852078962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Information, Liaoning University, 110036, China; School of Information, Liaoning University, 110036, China",
        "aff_domain": "qq.com;lnu.edu.cn",
        "email": "qq.com;lnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Liaoning University",
        "aff_unique_dep": "School of Information",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-scale Salient Instance Segmentation based on Encoder-Decoder",
        "site": "https://proceedings.mlr.press/v157/chen21b.html",
        "author": "Houru Chen; Caijuan Shi; Wei Li; Changyu Duan; jinwei Yan",
        "abstract": "Salient instance segmentation refers to segmenting noticeable instance objects in images. In the face of multi-scale salient instances and overlapping instances, the existing salient instance segmentation methods have great limitations including inaccurate detection of large-scale instances, missing detection of small-scale instances, and wrong segmentation of overlapping instances. In order to solve these problems, a new multi-scale salient instance segmentation network (MSISNet) based on encoder-decoder is proposed. Firstly, a receptive field encoder (RFE) is designed to alleviate the problems of inaccurate detection of large-scale instances, missing detection of small-scale instances, and especially wrong segmentation of overlapping instances. Then, a pyramid decoder (PD) for the detection branch is designed to further alleviate the problem of inaccurate detection of large-scale instances and the difficulty in locating small-scale instances. Finally, a multi-stage decoder (MSD) is designed to improve the quality of the segmentation mask. Experiments on salient instance segmentation dataset Salient Instance Segmentation-1K (SIS-1K) have been conducted and the results show that the proposed method MSISNet is superior to the existing salient instance segmentation methods MSRNet and S4Net, and achieves better segmentation accuracy and speed.",
        "bibtex": "@InProceedings{pmlr-v157-chen21b,\n  title = \t {Multi-scale Salient Instance Segmentation based on Encoder-Decoder},\n  author =       {Chen, Houru and Shi, Caijuan and Li, Wei and Duan, Changyu and Yan, jinwei},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1445--1460},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/chen21b/chen21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/chen21b.html},\n  abstract = \t {Salient instance segmentation refers to segmenting noticeable instance objects in images. In the face of multi-scale salient instances and overlapping instances, the existing salient instance segmentation methods have great limitations including inaccurate detection of large-scale instances, missing detection of small-scale instances, and wrong segmentation of overlapping instances. In order to solve these problems, a new multi-scale salient instance segmentation network (MSISNet) based on encoder-decoder is proposed. Firstly, a receptive field encoder (RFE) is designed to alleviate the problems of inaccurate detection of large-scale instances, missing detection of small-scale instances, and especially wrong segmentation of overlapping instances. Then, a pyramid decoder (PD) for the detection branch is designed to further alleviate the problem of inaccurate detection of large-scale instances and the difficulty in locating small-scale instances. Finally, a multi-stage decoder (MSD) is designed to improve the quality of the segmentation mask. Experiments on salient instance segmentation dataset Salient Instance Segmentation-1K (SIS-1K) have been conducted and the results show that the proposed method MSISNet is superior to the existing salient instance segmentation methods MSRNet and S4Net, and achieves better segmentation accuracy and speed.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/chen21b/chen21b.pdf",
        "supp": "",
        "pdf_size": 6671905,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:76WejImFwBYJ:scholar.google.com/&scioq=Multi-scale+Salient+Instance+Segmentation+based+on+Encoder-Decoder&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "North China University of Science and Technology, Tangshan Hebei 063210, China; North China University of Science and Technology, Tangshan Hebei 063210, China; North China University of Science and Technology, Tangshan Hebei 063210, China; North China University of Science and Technology, Tangshan Hebei 063210, China; North China University of Science and Technology, Tangshan Hebei 063210, China",
        "aff_domain": "outlook.com;163.com;ncst.edu.cn;126.com;163.com",
        "email": "outlook.com;163.com;ncst.edu.cn;126.com;163.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "North China University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Tangshan",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-stream based marked point process",
        "site": "https://proceedings.mlr.press/v157/hong21a.html",
        "author": "Sujun Hong; Hirotaka Hachiya",
        "abstract": "When using a point process, a specific form of the model needs to be designed for intensity function, based on physical and mathematical prior knowledge about the data. Recently, a fully trainable deep learning-based approach has been developed for temporal point processes. This approach models a cumulative hazard function (CHF), which is capable of systematic computation of adaptive intensity function in a data-driven manner. However, this approach does not take the attribute information of events into account although many applications of point processes generate with a variety of marked information such as location, magnitude, and depth of seismic activity. To overcome this limitation, we propose a fully trainable marked point process method, modeling decomposed CHFs for time and mark using multi-stream deep neural networks. In addition, we also propose to encode multiple marked information into a single image and extract necessary information adaptively without detailed knowledge about the data. We show the effectiveness of our proposed method through experiments with simulated toy data and real seismic data.",
        "bibtex": "@InProceedings{pmlr-v157-hong21a,\n  title = \t {Multi-stream based marked point process},\n  author =       {Hong, Sujun and Hachiya, Hirotaka},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1269--1284},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/hong21a/hong21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/hong21a.html},\n  abstract = \t {When using a point process, a specific form of the model needs to be designed for intensity function, based on physical and mathematical prior knowledge about the data. Recently, a fully trainable deep learning-based approach has been developed for temporal point processes. This approach models a cumulative hazard function (CHF), which is capable of systematic computation of adaptive intensity function in a data-driven manner. However, this approach does not take the attribute information of events into account although many applications of point processes generate with a variety of marked information such as location, magnitude, and depth of seismic activity. To overcome this limitation, we propose a fully trainable marked point process method, modeling decomposed CHFs for time and mark using multi-stream deep neural networks. In addition, we also propose to encode multiple marked information into a single image and extract necessary information adaptively without detailed knowledge about the data. We show the effectiveness of our proposed method through experiments with simulated toy data and real seismic data.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/hong21a/hong21a.pdf",
        "supp": "",
        "pdf_size": 1626458,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=395955740920498779&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Graduate School of System Engineering, Wakayama University; Graduate School of System Engineering, Wakayama University",
        "aff_domain": "g.wakayama-u.jp;wakayama-u.ac.jp",
        "email": "g.wakayama-u.jp;wakayama-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Wakayama University",
        "aff_unique_dep": "Graduate School of System Engineering",
        "aff_unique_url": "https://www.wakayama-u.ac.jp",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Multi-task Actor-Critic with Knowledge Transfer via a Shared Critic",
        "site": "https://proceedings.mlr.press/v157/zhang21b.html",
        "author": "Gengzhi Zhang; Liang Feng; Yaqing Hou",
        "abstract": "Multi-task actor-critic is a learning paradigm proposed in the literature to improve the learning efficiency of multiple actor-critics by sharing the learned policies across tasks while the reinforcement learning progresses online. However, existing multi-task actor-critic algorithms can only handle reinforcement learning tasks within the same problem domain, they may fail in cases where tasks possessing diverse state-action spaces. Taking this cue, in this paper, we embark a study on multi-task actor-critic with knowledge transfer via a share critic to enable the multi-task learning of actor-critic in heterogeneous state-action environments. Further, for efficient learning of the proposed multi-task actor-critic, a new formula for calculating the gradient of the actor network is also presented. To evaluate the performance of our approach, comprehensive empirical studies on continuous robotic tasks with different numbers of links. The experimental results confirmed the effectiveness of the proposed multi-task actor-critic algorithm.",
        "bibtex": "@InProceedings{pmlr-v157-zhang21b,\n  title = \t {Multi-task Actor-Critic with Knowledge Transfer via a Shared Critic},\n  author =       {Zhang, Gengzhi and Feng, Liang and Hou, Yaqing},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {580--593},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhang21b/zhang21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhang21b.html},\n  abstract = \t {Multi-task actor-critic is a learning paradigm proposed in the literature to improve the learning efficiency of multiple actor-critics by sharing the learned policies across tasks while the reinforcement learning progresses online. However, existing multi-task actor-critic algorithms can only handle reinforcement learning tasks within the same problem domain, they may fail in cases where tasks possessing diverse state-action spaces. Taking this cue, in this paper, we embark a study on multi-task actor-critic with knowledge transfer via a share critic to enable the multi-task learning of actor-critic in heterogeneous state-action environments. Further, for efficient learning of the proposed multi-task actor-critic, a new formula for calculating the gradient of the actor network is also presented. To evaluate the performance of our approach, comprehensive empirical studies on continuous robotic tasks with different numbers of links. The experimental results confirmed the effectiveness of the proposed multi-task actor-critic algorithm.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhang21b/zhang21b.pdf",
        "supp": "",
        "pdf_size": 1717917,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10006303681737087800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ChongQing University; ChongQing University; Dalian University of Technology",
        "aff_domain": "qq.com;cqu.edu.cn;dlut.edu.cn",
        "email": "qq.com;cqu.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Chongqing University;Dalian University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cqu.edu.cn;http://www.dlut.edu.cn/",
        "aff_unique_abbr": "CQU;DUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-view Latent Subspace Clustering based on both Global and Local Structure",
        "site": "https://proceedings.mlr.press/v157/honghan21a.html",
        "author": "Zhou Honghan; Cai Weiling; Xu Le; Yang Ming",
        "abstract": "Most existing multi-view clustering methods focus on the global structure or local structure among samples, and few methods focus on the two structures at the same time. In this paper, we propose a Multi-view Latent subspace Clustering based on both Global and Local structure (MLCGL). In this method, a latent embedding representation is learned by exploring the complementary information from different views. In the latent space, not only the global reconstruction relationship but also the local geometric structure among the latent variables are discovered. In this way, a unified affinity graph matrix is constructed in the latent space for different views, which indicates a clear between-class relationship. Meanwhile, a rank constraint is introduced on the Laplacian graph to facilitate the division of samples into the required clusters. In MLCGL, the affinity graph also provides positive feedback to optimize the learned latent representation and contribute to divided it into reasonable clusters. Moreover, we present an alternating iterative optimization scheme to optimize objective functions. Compared with the state-of-art algorithms, MLCGL has achieved excellent experimental performance on several real-world datasets.",
        "bibtex": "@InProceedings{pmlr-v157-honghan21a,\n  title = \t {Multi-view Latent Subspace Clustering based on both Global and Local Structure},\n  author =       {Honghan, Zhou and Weiling, Cai and Le, Xu and Ming, Yang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1617--1632},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/honghan21a/honghan21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/honghan21a.html},\n  abstract = \t {Most existing multi-view clustering methods focus on the global structure or local structure among samples, and few methods focus on the two structures at the same time. In this paper, we propose a Multi-view Latent subspace Clustering based on both Global and Local structure (MLCGL). In this method, a latent embedding representation is learned by exploring the complementary information from different views. In the latent space, not only the global reconstruction relationship but also the local geometric structure among the latent variables are discovered. In this way, a unified affinity graph matrix is constructed in the latent space for different views, which indicates a clear between-class relationship. Meanwhile, a rank constraint is introduced on the Laplacian graph to facilitate the division of samples into the required clusters. In MLCGL, the affinity graph also provides positive feedback to optimize the learned latent representation and contribute to divided it into reasonable clusters. Moreover, we present an alternating iterative optimization scheme to optimize objective functions. Compared with the state-of-art algorithms, MLCGL has achieved excellent experimental performance on several real-world datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/honghan21a/honghan21a.pdf",
        "supp": "",
        "pdf_size": 2832053,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13985944956260336262&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer and Electronic Information Nanjing Normal University Nanjing, China; School of Computer and Electronic Information Nanjing Normal University Nanjing, China; School of Computer and Electronic Information Nanjing Normal University Nanjing, China; School of Computer and Electronic Information Nanjing Normal University Nanjing, China",
        "aff_domain": "outlook.com;njnu.edu.cn;163.com;njnu.edu.cn",
        "email": "outlook.com;njnu.edu.cn;163.com;njnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanjing Normal University",
        "aff_unique_dep": "School of Computer and Electronic Information",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "NNU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "NAS-HPO-Bench-II: A Benchmark Dataset on Joint Optimization of Convolutional Neural Network Architecture and Training Hyperparameters",
        "site": "https://proceedings.mlr.press/v157/hirose21a.html",
        "author": "Yoichi Hirose; Nozomu Yoshinari; Shinichi Shirakawa",
        "abstract": "The benchmark datasets for neural architecture search (NAS) have been developed to alleviate the computationally expensive evaluation process and ensure a fair comparison. Recent NAS benchmarks only focus on architecture optimization, although the training hyperparameters affect the obtained model performances. Building the benchmark dataset for joint optimization of architecture and training hyperparameters is essential to further NAS research. The existing NAS-HPO-Bench is a benchmark for joint optimization, but it does not consider the network connectivity design as done in modern NAS algorithms. This paper introduces the first benchmark dataset for joint optimization of network connections and training hyperparameters, which we call NAS-HPO-Bench-II. We collect the performance data of 4K cell-based convolutional neural network architectures trained on the CIFAR-10 dataset with different learning rate and batch size settings, resulting in the data of 192K configurations. The dataset includes the exact data for 12 epoch training. We further build the surrogate model predicting the accuracies after 200 epoch training to provide the performance data of longer training epoch. By analyzing NAS-HPO-Bench-II, we confirm the dependency between architecture and training hyperparameters and the necessity of joint optimization. Finally, we demonstrate the benchmarking of the baseline optimization algorithms using NAS-HPO-Bench-II.",
        "bibtex": "@InProceedings{pmlr-v157-hirose21a,\n  title = \t {{NAS-HPO-Bench-II}: A Benchmark Dataset on Joint Optimization of Convolutional Neural Network Architecture and Training Hyperparameters},\n  author =       {Hirose, Yoichi and Yoshinari, Nozomu and Shirakawa, Shinichi},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1349--1364},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/hirose21a/hirose21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/hirose21a.html},\n  abstract = \t {The benchmark datasets for neural architecture search (NAS) have been developed to alleviate the computationally expensive evaluation process and ensure a fair comparison. Recent NAS benchmarks only focus on architecture optimization, although the training hyperparameters affect the obtained model performances. Building the benchmark dataset for joint optimization of architecture and training hyperparameters is essential to further NAS research. The existing NAS-HPO-Bench is a benchmark for joint optimization, but it does not consider the network connectivity design as done in modern NAS algorithms. This paper introduces the first benchmark dataset for joint optimization of network connections and training hyperparameters, which we call NAS-HPO-Bench-II. We collect the performance data of 4K cell-based convolutional neural network architectures trained on the CIFAR-10 dataset with different learning rate and batch size settings, resulting in the data of 192K configurations. The dataset includes the exact data for 12 epoch training. We further build the surrogate model predicting the accuracies after 200 epoch training to provide the performance data of longer training epoch. By analyzing NAS-HPO-Bench-II, we confirm the dependency between architecture and training hyperparameters and the necessity of joint optimization. Finally, we demonstrate the benchmarking of the baseline optimization algorithms using NAS-HPO-Bench-II.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/hirose21a/hirose21a.pdf",
        "supp": "",
        "pdf_size": 875473,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12607702949043918905&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Yokohama National University, Kanagawa, Japan; Yokohama National University, Kanagawa, Japan; Yokohama National University, Kanagawa, Japan",
        "aff_domain": "ynu.jp;ynu.jp;ynu.ac.jp",
        "email": "ynu.jp;ynu.jp;ynu.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Yokohama National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yokohama-nu.ac.jp",
        "aff_unique_abbr": "YNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Neural Graph Filtering for Context-aware Recommendation",
        "site": "https://proceedings.mlr.press/v157/chuanyan21a.html",
        "author": "Zhang Chuanyan; Hong Xiaoguang",
        "abstract": "With the rapid development of web services, various kinds of context data become available in recommender systems to handler the data sparsity problem, called context-aware recommendation (CAR). It is challenging to develop effective approaches to model and exploit these various and heterogeneous data. Recently, heterogeneous information network (HIN) has been adopted to model the context data due to its flexibility in modelling data heterogeneity. However, most of the HIN-based methods, which rely on meta paths or graph embedding to extract features from HINs, cannot fully mine the network structure and semantic features of users and items. Besides, these methods, utilizing the global dataset to learn personalized latent factors, usually suffer individuality loss problem. In this paper, we propose a neural graph filtering method for context-aware recommendation, called NGF. First, we use an unified HIN to model both the users\u2019 feedback information and the context data. Then, we adopt graph filtering to predict aspect-level ratings on a series of independent subgraphs of the unified HIN and feed a deep neural network (DNN) to fuse the predictions for CAR. Concretely, graph filtering is a case-by-case algorithm for personalized recommendation on HINs, which predicts the further behavior by all its similar historical behaviors. We split the unified HIN into many single-aspect networks according to the semantic relations and utilize graph filtering to predict user\u2019s behavior on each subgraphs. The following deep neural network is to fuse the personalized predictions in aspect-level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our neural graph filtering for CAR.",
        "bibtex": "@InProceedings{pmlr-v157-chuanyan21a,\n  title = \t {Neural Graph Filtering for Context-aware Recommendation},\n  author =       {Chuanyan, Zhang and Xiaoguang, Hong},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {969--984},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/chuanyan21a/chuanyan21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/chuanyan21a.html},\n  abstract = \t { With the rapid development of web services, various kinds of context data become available in recommender systems to handler the data sparsity problem, called context-aware recommendation (CAR). It is challenging to develop effective approaches to model and exploit these various and heterogeneous data. Recently, heterogeneous information network (HIN) has been adopted to model the context data due to its flexibility in modelling data heterogeneity. However, most of the HIN-based methods, which rely on meta paths or graph embedding to extract features from HINs, cannot fully mine the network structure and semantic features of users and items. Besides, these methods, utilizing the global dataset to learn personalized latent factors, usually suffer individuality loss problem. In this paper, we propose a neural graph filtering method for context-aware recommendation, called NGF. First, we use an unified HIN to model both the users\u2019 feedback information and the context data. Then, we adopt graph filtering to predict aspect-level ratings on a series of independent subgraphs of the unified HIN and feed a deep neural network (DNN) to fuse the predictions for CAR. Concretely, graph filtering is a case-by-case algorithm for personalized recommendation on HINs, which predicts the further behavior by all its similar historical behaviors. We split the unified HIN into many single-aspect networks according to the semantic relations and utilize graph filtering to predict user\u2019s behavior on each subgraphs. The following deep neural network is to fuse the personalized predictions in aspect-level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our neural graph filtering for CAR.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/chuanyan21a/chuanyan21a.pdf",
        "supp": "",
        "pdf_size": 577588,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5026875133476472286&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, Shandong University, Jinan, China, 250101; Qilu Software College of Shandong University, Jinan, China, 250101",
        "aff_domain": "sina.cn;sdu.edu.cn",
        "email": "sina.cn;sdu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shandong University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.sdu.edu.cn",
        "aff_unique_abbr": "SDU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jinan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "On the Convex Combination of Determinantal Point Processes",
        "site": "https://proceedings.mlr.press/v157/matsuoka21a.html",
        "author": "Tatsuya Matsuoka; Naoto Ohsaka; Akihiro Yabe",
        "abstract": "Determinantal point processes (DPPs) are attractive probabilistic models for expressing item quality and set diversity simultaneously. Although DPPs are widely-applicable to many subset selection tasks, there exist simple small-size probability distributions that any DPP cannot express. To overcome this drawback while keeping good properties of DPPs, in this paper we investigate the expressive power of \\emph{convex combinations of DPPs}. We provide upper and lower bounds for the number of DPPs required for \\emph{exactly} expressing any probability distribution. For the \\emph{approximation} error, we give an upper bound on the Kullback\u2013Leibler divergence $n-\\lfloor \\log t\\rfloor +\\epsilon$ for any $\\epsilon >0$ of approximate distribution from a given joint probability distribution, where $t$ is the number of DPPs. Our numerical simulation on an online retail dataset empirically verifies that a convex combination of only two DPPs can outperform a nonsymmetric DPP in terms of the Kullback\u2013Leibler divergence. By combining a polynomial number of DPPs, we can express probability distributions induced by bounded-degree pseudo-Boolean functions, which include weighted coverage functions of bounded occurrence.",
        "bibtex": "@InProceedings{pmlr-v157-matsuoka21a,\n  title = \t {On the Convex Combination of Determinantal Point Processes},\n  author =       {Matsuoka, Tatsuya and Ohsaka, Naoto and Yabe, Akihiro},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {158--173},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/matsuoka21a/matsuoka21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/matsuoka21a.html},\n  abstract = \t {Determinantal point processes (DPPs) are attractive probabilistic models for expressing item quality and set diversity simultaneously. Although DPPs are widely-applicable to many subset selection tasks, there exist simple small-size probability distributions that any DPP cannot express. To overcome this drawback while keeping good properties of DPPs, in this paper we investigate the expressive power of \\emph{convex combinations of DPPs}. We provide upper and lower bounds for the number of DPPs required for \\emph{exactly} expressing any probability distribution. For the \\emph{approximation} error, we give an upper bound on the Kullback\u2013Leibler divergence $n-\\lfloor \\log t\\rfloor +\\epsilon$ for any $\\epsilon >0$ of approximate distribution from a given joint probability distribution, where $t$ is the number of DPPs. Our numerical simulation on an online retail dataset empirically verifies that a convex combination of only two DPPs can outperform a nonsymmetric DPP in terms of the Kullback\u2013Leibler divergence. By combining a polynomial number of DPPs, we can express probability distributions induced by bounded-degree pseudo-Boolean functions, which include weighted coverage functions of bounded occurrence.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/matsuoka21a/matsuoka21a.pdf",
        "supp": "",
        "pdf_size": 435791,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pG3a0cn6bZoJ:scholar.google.com/&scioq=On+the+Convex+Combination+of+Determinantal+Point+Processes&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "NEC Corporation; NEC Corporation; Fanfare Inc. + NEC Corporation",
        "aff_domain": "nec.com;nec.com;fanfare-kk.com",
        "email": "nec.com;nec.com;fanfare-kk.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "NEC Corporation;Fanfare Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nec.com;",
        "aff_unique_abbr": "NEC;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Open Images V5 Text Annotation and Yet Another Mask Text Spotter",
        "site": "https://proceedings.mlr.press/v157/krylov21a.html",
        "author": "Ilya Krylov; Sergei Nosov; Vladislav Sovrasov",
        "abstract": "A large scale human-labeled dataset plays an important role in creating high quality deep learning models. In this paper we present text annotation for Open Images V5 dataset. To our knowledge it is the largest among publicly available manually created text annotations. Having this annotation we trained a simple Mask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS), which achieves competitive performance or even outperforms current state-of-the-art approaches in some cases on ICDAR\u00a02013, ICDAR\u00a02015 and {Total-Text} datasets. Code for text spotting model available online at: \\url{https://github.com/openvinotoolkit/training_extensions}. The model can be exported to OpenVINO{\\texttrademark}-format and run on Intel{\\textregistered} CPUs.",
        "bibtex": "@InProceedings{pmlr-v157-krylov21a,\n  title = \t {Open Images V5 Text Annotation and Yet Another Mask Text Spotter},\n  author =       {Krylov, Ilya and Nosov, Sergei and Sovrasov, Vladislav},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {379--389},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/krylov21a/krylov21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/krylov21a.html},\n  abstract = \t {A large scale human-labeled dataset plays an important role in creating high quality deep learning models. In this paper we present text annotation for Open Images V5 dataset. To our knowledge it is the largest among publicly available manually created text annotations. Having this annotation we trained a simple Mask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS), which achieves competitive performance or even outperforms current state-of-the-art approaches in some cases on ICDAR\u00a02013, ICDAR\u00a02015 and {Total-Text} datasets. Code for text spotting model available online at: \\url{https://github.com/openvinotoolkit/training_extensions}. The model can be exported to OpenVINO{\\texttrademark}-format and run on Intel{\\textregistered} CPUs.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/krylov21a/krylov21a.pdf",
        "supp": "",
        "pdf_size": 266871,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17540567570388064282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "IOTG Computer Vision (ICV), Intel; IOTG Computer Vision (ICV), Intel; Lobachevsky State University of Nizhny Novgorod, Russia + IOTG Computer Vision (ICV), Intel",
        "aff_domain": "intel.com;intel.com;itmm.unn.ru",
        "email": "intel.com;intel.com;itmm.unn.ru",
        "github": "https://github.com/openvinotoolkit/training_extensions",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Intel Corporation;Lobachevsky State University",
        "aff_unique_dep": "IOTG Computer Vision (ICV);",
        "aff_unique_url": "https://www.intel.com;https://www.unn.ru",
        "aff_unique_abbr": "Intel;UNN",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Nizhny Novgorod",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "United States;Russia"
    },
    {
        "title": "PFedAtt: Attention-based Personalized Federated Learning on Heterogeneous Clients",
        "site": "https://proceedings.mlr.press/v157/ma21a.html",
        "author": "Zichen Ma; Yu Lu; Wenye Li; Jinfeng Yi; Shuguang Cui",
        "abstract": "In federated learning, heterogeneity among the clients\u2019 local datasets results in large variations in the number of local updates performed by each client in a communication round. Simply aggregating such local models into a global model will confine the capacity of the system, that is, the single global model will be restricted from delivering good performance on each client\u2019s task. This paper provides a general framework to analyze the convergence of personalized federated learning algorithms. It subsumes previously proposed methods and provides a principled understanding of the computational guarantees. Using insights from this analysis, we propose PFedAtt, a personalized federated learning method that incorporates attention-based grouping to facilitate similar clients\u2019 collaborations. Theoretically, we provide the convergence guarantee for the algorithm, and empirical experiments corroborate the competitive performance of PFedAtt on heterogeneous clients.",
        "bibtex": "@InProceedings{pmlr-v157-ma21a,\n  title = \t {PFedAtt: Attention-based Personalized Federated Learning on Heterogeneous Clients},\n  author =       {Ma, Zichen and Lu, Yu and Li, Wenye and Yi, Jinfeng and Cui, Shuguang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1253--1268},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/ma21a/ma21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/ma21a.html},\n  abstract = \t {In federated learning, heterogeneity among the clients\u2019 local datasets results in large variations in the number of local updates performed by each client in a communication round. Simply aggregating such local models into a global model will confine the capacity of the system, that is, the single global model will be restricted from delivering good performance on each client\u2019s task. This paper provides a general framework to analyze the convergence of personalized federated learning algorithms. It subsumes previously proposed methods and provides a principled understanding of the computational guarantees. Using insights from this analysis, we propose PFedAtt, a personalized federated learning method that incorporates attention-based grouping to facilitate similar clients\u2019 collaborations. Theoretically, we provide the convergence guarantee for the algorithm, and empirical experiments corroborate the competitive performance of PFedAtt on heterogeneous clients.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/ma21a/ma21a.pdf",
        "supp": "",
        "pdf_size": 882138,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9593731053148417331&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Chinese University of Hong Kong, Shenzhen, Guangdong, China; The Chinese University of Hong Kong, Shenzhen, Guangdong, China; The Chinese University of Hong Kong, Shenzhen, Guangdong, China; JD AI Research, Beijing, China; The Chinese University of Hong Kong, Shenzhen, Guangdong, China",
        "aff_domain": "link.cuhk.edu.cn;link.cuhk.edu.cn;cuhk.edu.cn;jd.com;cuhk.edu.cn",
        "email": "link.cuhk.edu.cn;link.cuhk.edu.cn;cuhk.edu.cn;jd.com;cuhk.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "The Chinese University of Hong Kong;JD AI Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.cn;",
        "aff_unique_abbr": "CUHK;",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Shenzhen;Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Pedestrian Wind Factor Estimation in Complex Urban Environments",
        "site": "https://proceedings.mlr.press/v157/mokhtar21a.html",
        "author": "Sarah Mokhtar; Matt Beveridge; Yumeng Cao; Iddo Drori",
        "abstract": "Urban planners and policy makers face the challenge of creating livable and enjoyable citiesfor larger populations in much denser urban conditions. While the urban microclimate holdsa key role in defining the quality of urban spaces today and in the future, the integrationof wind microclimate assessment in early urban design and planning processes remains achallenge  due  to  the  complexity  and  high  computational  expense  of  computational  fluiddynamics  (CFD)  simulations.   This  work  develops  a  data-driven  workflow  for  real-timepedestrian  wind  comfort  estimation  in  complex  urban  environments  which  may  enabledesigners,  policy  makers  and  city  residents  to  make  informed  decisions  about  mobility,health, and energy choices.  We use a conditional generative adversarial network (cGAN)architecture to reduce the computational computation while maintaining high confidencelevels and interpretability, adequate representation of urban complexity, and suitability forpedestrian  comfort  estimation.   We  demonstrate  high  quality  wind  field  approximationswhile reducing computation time from days to seconds.",
        "bibtex": "@InProceedings{pmlr-v157-mokhtar21a,\n  title = \t {Pedestrian Wind Factor Estimation in Complex Urban Environments},\n  author =       {Mokhtar, Sarah and Beveridge, Matt and Cao, Yumeng and Drori, Iddo},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {486--501},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/mokhtar21a/mokhtar21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/mokhtar21a.html},\n  abstract = \t {Urban planners and policy makers face the challenge of creating livable and enjoyable citiesfor larger populations in much denser urban conditions. While the urban microclimate holdsa key role in defining the quality of urban spaces today and in the future, the integrationof wind microclimate assessment in early urban design and planning processes remains achallenge  due  to  the  complexity  and  high  computational  expense  of  computational  fluiddynamics  (CFD)  simulations.   This  work  develops  a  data-driven  workflow  for  real-timepedestrian  wind  comfort  estimation  in  complex  urban  environments  which  may  enabledesigners,  policy  makers  and  city  residents  to  make  informed  decisions  about  mobility,health, and energy choices.  We use a conditional generative adversarial network (cGAN)architecture to reduce the computational computation while maintaining high confidencelevels and interpretability, adequate representation of urban complexity, and suitability forpedestrian  comfort  estimation.   We  demonstrate  high  quality  wind  field  approximationswhile reducing computation time from days to seconds.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/mokhtar21a/mokhtar21a.pdf",
        "supp": "",
        "pdf_size": 2872901,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16421878276583699441&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Massachusetts Institute of Technology, Cambridge, MA 02139, USA; Massachusetts Institute of Technology, Cambridge, MA 02139, USA; Massachusetts Institute of Technology, Cambridge, MA 02139, USA; Massachusetts Institute of Technology, Cambridge, MA 02139, USA",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Penalty Method for Inversion-Free Deep Bilevel Optimization",
        "site": "https://proceedings.mlr.press/v157/mehra21a.html",
        "author": "Akshay Mehra; Jihun Hamm",
        "abstract": "Solving a bilevel optimization problem is at the core of several machine learning problems such as hyperparameter tuning, data denoising, meta- and few-shot learning, and trainingdata poisoning. Different from simultaneous or multi-objective optimization, the steepest descent direction for minimizing the upper-level cost in a bilevel problem requires the inverse of the Hessian of the lower-level cost. In this work, we propose a novel algorithm for solving bilevel optimization problems based on the classical penalty function approach. Our method avoids computing the Hessian inverse and can handle constrained bilevel problems easily. We prove the convergence of the method under mild conditions and show that the exact hypergradient is obtained asymptotically. Our method\u2019s simplicity and small space and time complexities enable us to effectively solve large-scale bilevel problems involving deep neural networks. We present results on data denoising, few-shot learning, and training-data poisoning problems in a large-scale setting. Our results show that our approach outperforms or is comparable to previously proposed methods based on automatic differentiation and approximate inversion in terms of accuracy, run-time, and convergence speed",
        "bibtex": "@InProceedings{pmlr-v157-mehra21a,\n  title = \t {Penalty Method for Inversion-Free Deep Bilevel Optimization},\n  author =       {Mehra, Akshay and Hamm, Jihun},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {347--362},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/mehra21a/mehra21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/mehra21a.html},\n  abstract = \t {Solving a bilevel optimization problem is at the core of several machine learning problems such as hyperparameter tuning, data denoising, meta- and few-shot learning, and trainingdata poisoning. Different from simultaneous or multi-objective optimization, the steepest descent direction for minimizing the upper-level cost in a bilevel problem requires the inverse of the Hessian of the lower-level cost. In this work, we propose a novel algorithm for solving bilevel optimization problems based on the classical penalty function approach. Our method avoids computing the Hessian inverse and can handle constrained bilevel problems easily. We prove the convergence of the method under mild conditions and show that the exact hypergradient is obtained asymptotically. Our method\u2019s simplicity and small space and time complexities enable us to effectively solve large-scale bilevel problems involving deep neural networks. We present results on data denoising, few-shot learning, and training-data poisoning problems in a large-scale setting. Our results show that our approach outperforms or is comparable to previously proposed methods based on automatic differentiation and approximate inversion in terms of accuracy, run-time, and convergence speed}\n}",
        "pdf": "https://proceedings.mlr.press/v157/mehra21a/mehra21a.pdf",
        "supp": "",
        "pdf_size": 1109911,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9003828262282493775&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Tulane University, New Orleans, LA, USA; Tulane University, New Orleans, LA, USA",
        "aff_domain": "tulane.edu;tulane.edu",
        "email": "tulane.edu;tulane.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tulane University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tulane.edu",
        "aff_unique_abbr": "Tulane",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New Orleans",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Perturbing Eigenvalues with Residual Learning in Graph Convolutional Neural Networks",
        "site": "https://proceedings.mlr.press/v157/yao21a.html",
        "author": "Shibo Yao; Dantong Yu; Xiangmin Jiao",
        "abstract": "Network structured data is ubiquitous in natural and social science applications. Graph Convolutional Neural Network (GCN) has attracted significant attention recently due to its success in representing, modeling, and predicting large-scale network data. Various types of graph convolutional filters were proposed to process graph signals to boost the performance of graph-based semi-supervised learning. This paper introduces a novel spectral learning technique called EigLearn, which uses residual learning to perturb the eigenvalues of the graph filter matrix to optimize its capability. EigLearn is relatively easy to implement, and yet thorough experimental studies reveal that it is more effective and efficient than the prior works on the specific issue, such as LanczosNet and FisherGCN. EigLearn only perturbs a small number of eigenvalues and does not require a complete eigendecomposition. Our investigation shows that EigLearn reaches the maximal performance improvement by perturbing about 30 to 40 eigenvalues, and the EigLearn-based GCN has comparable efficiency as the standard GCN. Furthermore, EigLearn bears a clear explanation in the spectral domain of the graph filter and shows aggregation effects in performance improvement when coupled with different graph filters. Hence, we anticipate that EigLearn may serve as a useful neural unit in various graph-involved neural net architectures.",
        "bibtex": "@InProceedings{pmlr-v157-yao21a,\n  title = \t {Perturbing Eigenvalues with Residual Learning in Graph Convolutional Neural Networks},\n  author =       {Yao, Shibo and Yu, Dantong and Jiao, Xiangmin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1569--1584},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/yao21a/yao21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/yao21a.html},\n  abstract = \t {Network structured data is ubiquitous in natural and social science applications. Graph Convolutional Neural Network (GCN) has attracted significant attention recently due to its success in representing, modeling, and predicting large-scale network data. Various types of graph convolutional filters were proposed to process graph signals to boost the performance of graph-based semi-supervised learning. This paper introduces a novel spectral learning technique called EigLearn, which uses residual learning to perturb the eigenvalues of the graph filter matrix to optimize its capability. EigLearn is relatively easy to implement, and yet thorough experimental studies reveal that it is more effective and efficient than the prior works on the specific issue, such as LanczosNet and FisherGCN. EigLearn only perturbs a small number of eigenvalues and does not require a complete eigendecomposition. Our investigation shows that EigLearn reaches the maximal performance improvement by perturbing about 30 to 40 eigenvalues, and the EigLearn-based GCN has comparable efficiency as the standard GCN. Furthermore, EigLearn bears a clear explanation in the spectral domain of the graph filter and shows aggregation effects in performance improvement when coupled with different graph filters. Hence, we anticipate that EigLearn may serve as a useful neural unit in various graph-involved neural net architectures.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/yao21a/yao21a.pdf",
        "supp": "",
        "pdf_size": 419665,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=82848076469098567&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Martin Tuchman School of Management, New Jersey Institute of Technology, Newark, NJ 07102; Martin Tuchman School of Management, New Jersey Institute of Technology, Newark, NJ 07102; Department of Applied Mathematics & Statistics, Stony Brook University, Stony Brook, NY 11794",
        "aff_domain": "gmail.com;njit.edu;stonybrook.edu",
        "email": "gmail.com;njit.edu;stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "New Jersey Institute of Technology;Stony Brook University",
        "aff_unique_dep": "Martin Tuchman School of Management;Department of Applied Mathematics & Statistics",
        "aff_unique_url": "https://www.njit.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": "NJIT;SBU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Newark;Stony Brook",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Physics-inspired Learning for Structure-Aware Texture-Sensitive Underwater Image Enhancement",
        "site": "https://proceedings.mlr.press/v157/xue21a.html",
        "author": "Xinwei Xue; Zexuan Li; Long Ma; Risheng Liu; Xin Fan",
        "abstract": "Recently, improving the visual quality of underwater images using deep learning-based methods has drawn considerable attention. Unfortunately, diverse environmental factors (e.g., blue/green color distortion) severely limit their performance in real-world environments. Therefore, strengthening the superiority of the underwater image enhancement method is critical. In this paper, we devote ourselves to develop a new architecture with strong superiority and adaptability. Inspired by the underwater imaging principle, we establish a novel physics-inspired learning model that is easy to realize. A Structure-Aware Texture-Sensitive Network (SATS-Net) is further developed to portray the model. The structure-aware module is responsible for structural information, and the texture-sensitive module is responsible for textural information. Thus, SATS-Net successfully incorporates robust characterization absorbed from the physical principle to achieve strong robustness and adaptability. We conduct extensive experiments to demonstrate that SATS-Net outperforms existing advanced techniques in various real-world underwater environments.",
        "bibtex": "@InProceedings{pmlr-v157-xue21a,\n  title = \t {Physics-inspired Learning for Structure-Aware Texture-Sensitive Underwater Image Enhancement},\n  author =       {Xue, Xinwei and Li, Zexuan and Ma, Long and Liu, Risheng and Fan, Xin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1224--1236},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/xue21a/xue21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/xue21a.html},\n  abstract = \t {Recently, improving the visual quality of underwater images using deep learning-based methods has drawn considerable attention. Unfortunately, diverse environmental factors (e.g., blue/green color distortion) severely limit their performance in real-world environments. Therefore, strengthening the superiority of the underwater image enhancement method is critical. In this paper, we devote ourselves to develop a new architecture with strong superiority and adaptability. Inspired by the underwater imaging principle, we establish a novel physics-inspired learning model that is easy to realize. A Structure-Aware Texture-Sensitive Network (SATS-Net) is further developed to portray the model. The structure-aware module is responsible for structural information, and the texture-sensitive module is responsible for textural information. Thus, SATS-Net successfully incorporates robust characterization absorbed from the physical principle to achieve strong robustness and adaptability. We conduct extensive experiments to demonstrate that SATS-Net outperforms existing advanced techniques in various real-world underwater environments.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/xue21a/xue21a.pdf",
        "supp": "",
        "pdf_size": 4669779,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pwTHogNpQNQJ:scholar.google.com/&scioq=Physics-inspired+Learning+for+Structure-Aware+Texture-Sensitive+Underwater+Image+Enhancement&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "DUT-RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, 116024, China; School of Software, Dalian University of Technology, Dalian, 116024, China; School of Software, Dalian University of Technology, Dalian, 116024, China; DUT-RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, 116024, China; DUT-RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, 116024, China",
        "aff_domain": "dlut.edu.cn;gmail.com;gmail.com;dlut.edu.cn;dlut.edu.cn",
        "email": "dlut.edu.cn;gmail.com;gmail.com;dlut.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "International School of Information Science & Engineering",
        "aff_unique_url": "http://en.dlut.edu.cn/",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Dalian",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Pyramid Correlation based Deep Hough Voting for Visual Object Tracking",
        "site": "https://proceedings.mlr.press/v157/wang21d.html",
        "author": "Ying Wang; Tingfa Xu; Shenwang Jiang; Junjie Chen; Jianan Li",
        "abstract": "Most of the existing Siamese-based trackers treat tracking problem as a parallel task of classification and regression. However, some studies show that the sibling head structure could lead to suboptimal solutions during the network training. Through experiments we find that, without regression, the performance could be equally promising as long as we delicately design the network to suit the training objective. We introduce a novel voting-based classification-only tracking algorithm named Pyramid Correlation based Deep Hough Voting (short for PCDHV), to jointly locate the top-left and bottom-right corners of the target. Specifically we innovatively construct a Pyramid Correlation module to equip the embedded feature with fine-grained local structures and global spatial contexts; The elaborately designed Deep Hough Voting module further take over, integrating long-range dependencies of pixels to perceive corners; In addition, the prevalent discretization gap is simply yet effectively alleviated by increasing the spatial resolution of the feature maps while exploiting channel-space relationships. The algorithm is general, robust and simple. We demonstrate the effectiveness of the module through a series of ablation experiments. Without bells and whistles, our tracker achieves better or comparable performance to the SOTA algorithms on three challenging benchmarks (TrackingNet, GOT-10k and LaSOT) while running at a real-time speed of 80 FPS. Codes and models will be released.",
        "bibtex": "@InProceedings{pmlr-v157-wang21d,\n  title = \t {Pyramid Correlation based Deep Hough Voting for Visual Object Tracking},\n  author =       {Wang, Ying and Xu, Tingfa and Jiang, Shenwang and Chen, Junjie and Li, Jianan},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {610--625},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wang21d/wang21d.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wang21d.html},\n  abstract = \t { Most of the existing Siamese-based trackers treat tracking problem as a parallel task of classification and regression. However, some studies show that the sibling head structure could lead to suboptimal solutions during the network training. Through experiments we find that, without regression, the performance could be equally promising as long as we delicately design the network to suit the training objective. We introduce a novel voting-based classification-only tracking algorithm named Pyramid Correlation based Deep Hough Voting (short for PCDHV), to jointly locate the top-left and bottom-right corners of the target. Specifically we innovatively construct a Pyramid Correlation module to equip the embedded feature with fine-grained local structures and global spatial contexts; The elaborately designed Deep Hough Voting module further take over, integrating long-range dependencies of pixels to perceive corners; In addition, the prevalent discretization gap is simply yet effectively alleviated by increasing the spatial resolution of the feature maps while exploiting channel-space relationships. The algorithm is general, robust and simple. We demonstrate the effectiveness of the module through a series of ablation experiments. Without bells and whistles, our tracker achieves better or comparable performance to the SOTA algorithms on three challenging benchmarks (TrackingNet, GOT-10k and LaSOT) while running at a real-time speed of 80 FPS. Codes and models will be released. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/wang21d/wang21d.pdf",
        "supp": "",
        "pdf_size": 5813118,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5280782855982993483&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China",
        "aff_domain": "gmail.com;bit.edu.cn;gmail.com;gmail.com;bit.edu.cn",
        "email": "gmail.com;bit.edu.cn;gmail.com;gmail.com;bit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beijing Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bit.edu.cn/",
        "aff_unique_abbr": "BIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "QActor: Active Learning on Noisy Labels",
        "site": "https://proceedings.mlr.press/v157/younesian21a.html",
        "author": "Taraneh Younesian; Zilong Zhao; Amirmasoud Ghiassi; Robert Birke; Lydia Y Chen",
        "abstract": "Noisy labeled data is more a norm than a rarity for self-generated content that is continuously published on the web and social media from non-experts. Active querying experts are conventionally adopted to provide labels for the informative samples which don\u2019t have labels, instead of possibly incorrect labels. The new challenge that arises here is how to discern the informative and noisy labels which benefit from expert cleaning.  In this paper, we aim to leverage the stringent oracle budget to robustly maximize learning accuracy. We propose a noise-aware active learning framework, QActor, and a novel measure \\emph{CENT}, which considers both cross-entropy and entropy to select informative and noisy labels for an expert cleansing. QActor iteratively cleans samples via quality models and actively querying an expert on those noisy yet informative samples. To adapt to learning capacity per iteration, QActor dynamically adjusts the query limit according to the learning loss for each learning iteration. We extensively evaluate different image datasets with noise label ratios ranging between 30% and 60%. Our results show that QActor can nearly match the optimal accuracy achieved using only clean data at the cost of only an additional 10% of ground truth data from the oracle.",
        "bibtex": "@InProceedings{pmlr-v157-younesian21a,\n  title = \t {QActor: Active Learning on Noisy Labels},\n  author =       {Younesian, Taraneh and Zhao, Zilong and Ghiassi, Amirmasoud and Birke, Robert and Chen, Lydia Y},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {548--563},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/younesian21a/younesian21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/younesian21a.html},\n  abstract = \t {Noisy labeled data is more a norm than a rarity for self-generated content that is continuously published on the web and social media from non-experts. Active querying experts are conventionally adopted to provide labels for the informative samples which don\u2019t have labels, instead of possibly incorrect labels. The new challenge that arises here is how to discern the informative and noisy labels which benefit from expert cleaning.  In this paper, we aim to leverage the stringent oracle budget to robustly maximize learning accuracy. We propose a noise-aware active learning framework, QActor, and a novel measure \\emph{CENT}, which considers both cross-entropy and entropy to select informative and noisy labels for an expert cleansing. QActor iteratively cleans samples via quality models and actively querying an expert on those noisy yet informative samples. To adapt to learning capacity per iteration, QActor dynamically adjusts the query limit according to the learning loss for each learning iteration. We extensively evaluate different image datasets with noise label ratios ranging between 30% and 60%. Our results show that QActor can nearly match the optimal accuracy achieved using only clean data at the cost of only an additional 10% of ground truth data from the oracle.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/younesian21a/younesian21a.pdf",
        "supp": "",
        "pdf_size": 648151,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2845140512200547710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands; ABB Corporate Research, Baden-D\u00a8 attwil, Switzerland; Delft University of Technology, Delft, Netherlands",
        "aff_domain": "tudelft.nl;tudelft.nl;tudelft.nl;ch.abb.com;ieee.org",
        "email": "tudelft.nl;tudelft.nl;tudelft.nl;ch.abb.com;ieee.org",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Delft University of Technology;ABB Corporate Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tudelft.nl;https://new.abb.com/research",
        "aff_unique_abbr": "TU Delft;ABB",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Delft;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Netherlands;Switzerland"
    },
    {
        "title": "Quaternion Graph Neural Networks",
        "site": "https://proceedings.mlr.press/v157/nguyen21a.html",
        "author": "Dai Quoc Nguyen; Tu Dinh Nguyen; Dinh Phung",
        "abstract": "Recently, graph neural networks (GNNs) have become an important and active research direction in deep learning. It is worth noting that most of the existing GNN-based methods learn graph representations within the Euclidean vector space. Beyond the Euclidean space, learning representation and embeddings in hyper-complex space have also shown to be a promising and effective approach. To this end, we propose Quaternion Graph Neural Networks (QGNN) to learn graph representations within the Quaternion space. As demonstrated, the Quaternion space, a hyper-complex vector space, provides highly meaningful computations and analogical calculus through Hamilton product compared to the Euclidean and complex vector spaces. Our QGNN obtains state-of-the-art results on a range of benchmark datasets for graph classification and node classification. Besides, regarding knowledge graphs, our QGNN-based embedding model achieves state-of-the-art results on three new and challenging benchmark datasets for knowledge graph completion. Our code is available at: \\url{https://github.com/daiquocnguyen/QGNN}.",
        "bibtex": "@InProceedings{pmlr-v157-nguyen21a,\n  title = \t {Quaternion Graph Neural Networks},\n  author =       {Nguyen, Dai Quoc and Nguyen, Tu Dinh and Phung, Dinh},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {236--251},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/nguyen21a/nguyen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/nguyen21a.html},\n  abstract = \t {Recently, graph neural networks (GNNs) have become an important and active research direction in deep learning. It is worth noting that most of the existing GNN-based methods learn graph representations within the Euclidean vector space. Beyond the Euclidean space, learning representation and embeddings in hyper-complex space have also shown to be a promising and effective approach. To this end, we propose Quaternion Graph Neural Networks (QGNN) to learn graph representations within the Quaternion space. As demonstrated, the Quaternion space, a hyper-complex vector space, provides highly meaningful computations and analogical calculus through Hamilton product compared to the Euclidean and complex vector spaces. Our QGNN obtains state-of-the-art results on a range of benchmark datasets for graph classification and node classification. Besides, regarding knowledge graphs, our QGNN-based embedding model achieves state-of-the-art results on three new and challenging benchmark datasets for knowledge graph completion. Our code is available at: \\url{https://github.com/daiquocnguyen/QGNN}.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/nguyen21a/nguyen21a.pdf",
        "supp": "",
        "pdf_size": 407417,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11277378831981662149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Data Science and AI, Monash University, Australia; VinAI Research, Vietnam; Department of Data Science and AI, Monash University, Australia",
        "aff_domain": "monash.edu;vinai.io;monash.edu",
        "email": "monash.edu;vinai.io;monash.edu",
        "github": "https://github.com/daiquocnguyen/QGNN",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Monash University;VinAI Research",
        "aff_unique_dep": "Department of Data Science and AI;",
        "aff_unique_url": "https://www.monash.edu;https://www.vin.ai",
        "aff_unique_abbr": "Monash;VinAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Australia;Vietnam"
    },
    {
        "title": "Regularized Mutual Learning for Personalized Federated Learning",
        "site": "https://proceedings.mlr.press/v157/yang21c.html",
        "author": "Ruihong Yang; Junchao Tian; Yu Zhang",
        "abstract": "Federated Learning (FL) is a privacy-protected learning paradigm, which allows many clients to jointly train a model under the coordination of a server without the local data leakage. In real-world scenarios, data in different clients usually cannot satisfy the independent and identically distributed (i.i.d.) assumption adopted widely in machine learning. Traditionally training a single global model may cause performance degradation and difficulty in ensuring convergence in such a non-i.i.d. case.  To handle this case, various models can be trained for each client to capture the personalization in each client. In this paper, we propose a new personalized FL framework, called Personalized Federated Mutual Learning (PFML), to use the non-i.i.d. characteristics to generate specific models for clients. Specifically, the PFML method integrates mutual learning into the local update process in each client to not only improve the performance of both the global and personalized models but also speed up the convergence compared with state-of-the-art methods. Moreover, the proposed PFML method can help maintain the heterogeneity of client models and protect the information of personalized models. Experiments on benchmark datasets show the effectiveness of the proposed PFML model.",
        "bibtex": "@InProceedings{pmlr-v157-yang21c,\n  title = \t {Regularized Mutual Learning for Personalized Federated Learning},\n  author =       {Yang, Ruihong and Tian, Junchao and Zhang, Yu},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1521--1536},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/yang21c/yang21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/yang21c.html},\n  abstract = \t {Federated Learning (FL) is a privacy-protected learning paradigm, which allows many clients to jointly train a model under the coordination of a server without the local data leakage. In real-world scenarios, data in different clients usually cannot satisfy the independent and identically distributed (i.i.d.) assumption adopted widely in machine learning. Traditionally training a single global model may cause performance degradation and difficulty in ensuring convergence in such a non-i.i.d. case.  To handle this case, various models can be trained for each client to capture the personalization in each client. In this paper, we propose a new personalized FL framework, called Personalized Federated Mutual Learning (PFML), to use the non-i.i.d. characteristics to generate specific models for clients. Specifically, the PFML method integrates mutual learning into the local update process in each client to not only improve the performance of both the global and personalized models but also speed up the convergence compared with state-of-the-art methods. Moreover, the proposed PFML method can help maintain the heterogeneity of client models and protect the information of personalized models. Experiments on benchmark datasets show the effectiveness of the proposed PFML model. }\n}",
        "pdf": "https://proceedings.mlr.press/v157/yang21c/yang21c.pdf",
        "supp": "",
        "pdf_size": 632117,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8938780457658346782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Engineering, Southern University of Science and Technology; Department of Computer Science and Engineering, Southern University of Science and Technology + Peng Cheng Laboratory; Department of Computer Science and Engineering, Southern University of Science and Technology + Peng Cheng Laboratory",
        "aff_domain": "mail.sustech.edu.cn;mail.sustech.edu.cn;gmail.com",
        "email": "mail.sustech.edu.cn;mail.sustech.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0+1",
        "aff_unique_norm": "Southern University of Science and Technology;Peng Cheng Laboratory",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.sustech.edu.cn;http://www.pcl.ac.cn",
        "aff_unique_abbr": "SUSTech;PCL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Relation Also Need Attention: Integrating Relation Information Into Image Captioning",
        "site": "https://proceedings.mlr.press/v157/chen21d.html",
        "author": "Tianyu Chen; Zhixin Li; Tiantao Xian; Canlong Zhang; Huifang Ma",
        "abstract": "Image captioning methods with attention mechanism are leading this field, especially models with global and local attention. But there are few conventional models to integrate the relationship information between various regions of the image. In this paper, this kind of relationship features are embedded into the fused attention mechanism to explore the internal visual and semantic relations between different object regions. Besides, to alleviate the exposure bias problem and make the training process more efficient, we combine Generative Adversarial Network with Reinforcement Learning and employ the greedy decoding method to generate a dynamic baseline reward for self-critical training. Finally, experiments on MSCOCO datasets show that the model can generate more accurate and vivid image captioning sentences and perform better in multiple prevailing metrics than the previous advanced models.",
        "bibtex": "@InProceedings{pmlr-v157-chen21d,\n  title = \t {Relation Also Need Attention: Integrating Relation Information Into Image Captioning},\n  author =       {Chen, Tianyu and Li, Zhixin and Xian, Tiantao and Zhang, Canlong and Ma, Huifang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1537--1552},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/chen21d/chen21d.pdf},\n  url = \t {https://proceedings.mlr.press/v157/chen21d.html},\n  abstract = \t {Image captioning methods with attention mechanism are leading this field, especially models with global and local attention. But there are few conventional models to integrate the relationship information between various regions of the image. In this paper, this kind of relationship features are embedded into the fused attention mechanism to explore the internal visual and semantic relations between different object regions. Besides, to alleviate the exposure bias problem and make the training process more efficient, we combine Generative Adversarial Network with Reinforcement Learning and employ the greedy decoding method to generate a dynamic baseline reward for self-critical training. Finally, experiments on MSCOCO datasets show that the model can generate more accurate and vivid image captioning sentences and perform better in multiple prevailing metrics than the previous advanced models.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/chen21d/chen21d.pdf",
        "supp": "",
        "pdf_size": 2545143,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18428313522690958267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Guangxi Key Lab of Multi-source Information Mining and Security, Guangxi Normal University, Guilin 541004, China; Guangxi Key Lab of Multi-source Information Mining and Security, Guangxi Normal University, Guilin 541004, China; Guangxi Key Lab of Multi-source Information Mining and Security, Guangxi Normal University, Guilin 541004, China; Guangxi Key Lab of Multi-source Information Mining and Security, Guangxi Normal University, Guilin 541004, China; College of Computer Science and Engineering, Northwest Normal University, Lanzhou 730070, China",
        "aff_domain": "stu.gxnu.edu.cn;gxnu.edu.cn;stu.gxnu.edu.cn;gxnu.edu.cn;nwnu.edu.cn",
        "email": "stu.gxnu.edu.cn;gxnu.edu.cn;stu.gxnu.edu.cn;gxnu.edu.cn;nwnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Guangxi Normal University;Northwest Normal University",
        "aff_unique_dep": "Guangxi Key Lab of Multi-source Information Mining and Security;College of Computer Science and Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Guilin;Lanzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Revisiting Weight Initialization of Deep Neural Networks",
        "site": "https://proceedings.mlr.press/v157/skorski21a.html",
        "author": "Maciej Skorski; Alessandro Temperoni; Martin Theobald",
        "abstract": "The proper {\\em initialization of weights} is crucial for the effective training and fast convergence of {\\em deep neural networks} (DNNs). Prior work in this area has mostly focused on the principle of {\\em balancing the variance among weights per layer} to maintain stability of (i) the input data propagated forwards through the network, and (ii) the loss gradients propagated backwards, respectively. This prevalent heuristic is however agnostic of dependencies among gradients across the various layers and captures only first-order effects per layer. In this paper, we investigate a {\\em unifying approach}, based on approximating and controlling the {\\em norm of the layers\u2019 Hessians}, which both generalizes and explains existing initialization schemes such as {\\em smooth activation functions}, {\\em Dropouts}, and {\\em ReLU}.",
        "bibtex": "@InProceedings{pmlr-v157-skorski21a,\n  title = \t {Revisiting Weight Initialization of Deep Neural Networks},\n  author =       {Skorski, Maciej and Temperoni, Alessandro and Theobald, Martin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1192--1207},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/skorski21a/skorski21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/skorski21a.html},\n  abstract = \t {The proper {\\em initialization of weights} is crucial for the effective training and fast convergence of {\\em deep neural networks} (DNNs). Prior work in this area has mostly focused on the principle of {\\em balancing the variance among weights per layer} to maintain stability of (i) the input data propagated forwards through the network, and (ii) the loss gradients propagated backwards, respectively. This prevalent heuristic is however agnostic of dependencies among gradients across the various layers and captures only first-order effects per layer. In this paper, we investigate a {\\em unifying approach}, based on approximating and controlling the {\\em norm of the layers\u2019 Hessians}, which both generalizes and explains existing initialization schemes such as {\\em smooth activation functions}, {\\em Dropouts}, and {\\em ReLU}.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/skorski21a/skorski21a.pdf",
        "supp": "",
        "pdf_size": 1475296,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8160118995786472835&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "University of Luxembourg; University of Luxembourg; University of Luxembourg",
        "aff_domain": "uni.lu;uni.lu;uni.lu",
        "email": "uni.lu;uni.lu;uni.lu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Luxembourg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wwwen.uniluxembourg.lu",
        "aff_unique_abbr": "Uni Lu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Luxembourg"
    },
    {
        "title": "Robust Domain Randomised Reinforcement Learning through Peer-to-Peer Distillation",
        "site": "https://proceedings.mlr.press/v157/zhao21b.html",
        "author": "Chenyang Zhao; Timothy Hospedales",
        "abstract": "In reinforcement learning, domain randomisation is a popular technique for learning general policies that are robust to new environments and domain-shifts at deployment. However, naively aggregating information from randomised domains may lead to high variances in gradient estimation and sub-optimal policies. To address this issue, we present a peer-to-peer online distillation strategy for reinforcement learning termed P2PDRL, where multiple learning agents are each assigned to a different environment, and then exchange knowledge through mutual regularisation based on Kullback\u2013Leibler divergence. Our experiments on continuous control tasks show that P2PDRL enables robust learning across a wider randomisation distribution than baselines, and more robust generalisation performance to new environments at testing.",
        "bibtex": "@InProceedings{pmlr-v157-zhao21b,\n  title = \t {Robust Domain Randomised Reinforcement Learning through Peer-to-Peer Distillation},\n  author =       {Zhao, Chenyang and Hospedales, Timothy},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1237--1252},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhao21b/zhao21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhao21b.html},\n  abstract = \t {In reinforcement learning, domain randomisation is a popular technique for learning general policies that are robust to new environments and domain-shifts at deployment. However, naively aggregating information from randomised domains may lead to high variances in gradient estimation and sub-optimal policies. To address this issue, we present a peer-to-peer online distillation strategy for reinforcement learning termed P2PDRL, where multiple learning agents are each assigned to a different environment, and then exchange knowledge through mutual regularisation based on Kullback\u2013Leibler divergence. Our experiments on continuous control tasks show that P2PDRL enables robust learning across a wider randomisation distribution than baselines, and more robust generalisation performance to new environments at testing.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhao21b/zhao21b.pdf",
        "supp": "",
        "pdf_size": 894752,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12858731469114423536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Edinburgh, United Kingdom; University of Edinburgh, United Kingdom",
        "aff_domain": "ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Robust Model-based Reinforcement Learning for Autonomous Greenhouse Control",
        "site": "https://proceedings.mlr.press/v157/zhang21e.html",
        "author": "Wanpeng Zhang; Xiaoyan Cao; Yao Yao; Zhicheng An; Xi Xiao; Dijun Luo",
        "abstract": "Due to the high efficiency and less weather dependency, autonomous greenhouses provide an ideal solution to meet the increasing demand for fresh food. However, managers are faced with some challenges in finding appropriate control strategies for crop growth, since the decision space of the greenhouse control problem is an astronomical number. Therefore, an intelligent closed-loop control framework is highly desired to generate an automatic control policy. As a powerful tool for optimal control, reinforcement learning (RL) algorithms can surpass human beings\u2019 decision-making and can also be seamlessly integrated into the closed-loop control framework. However, in complex real-world scenarios such as agricultural automation control, where the interaction with the environment is time-consuming and expensive, the application of RL algorithms encounters two main challenges, i.e., sample efficiency and safety. Although model-based RL methods can greatly mitigate the efficiency problem of greenhouse control, the safety problem has not got too much attention. In this paper, we present a model-based robust RL framework for autonomous greenhouse control to meet the sample efficiency and safety challenges. Specifically, our framework introduces an ensemble of environment models to work as a simulator and assist in policy optimization, thereby addressing the low sample efficiency problem. As for the safety concern, we propose a sample dropout module to focus more on worst-case samples, which can help improve the adaptability of the greenhouse planting policy in extreme cases. Experimental results demonstrate that our approach can learn a more effective greenhouse planting policy with better robustness than existing methods.",
        "bibtex": "@InProceedings{pmlr-v157-zhang21e,\n  title = \t {Robust Model-based Reinforcement Learning for Autonomous Greenhouse Control},\n  author =       {Zhang, Wanpeng and Cao, Xiaoyan and Yao, Yao and An, Zhicheng and Xiao, Xi and Luo, Dijun},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1208--1223},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhang21e/zhang21e.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhang21e.html},\n  abstract = \t {Due to the high efficiency and less weather dependency, autonomous greenhouses provide an ideal solution to meet the increasing demand for fresh food. However, managers are faced with some challenges in finding appropriate control strategies for crop growth, since the decision space of the greenhouse control problem is an astronomical number. Therefore, an intelligent closed-loop control framework is highly desired to generate an automatic control policy. As a powerful tool for optimal control, reinforcement learning (RL) algorithms can surpass human beings\u2019 decision-making and can also be seamlessly integrated into the closed-loop control framework. However, in complex real-world scenarios such as agricultural automation control, where the interaction with the environment is time-consuming and expensive, the application of RL algorithms encounters two main challenges, i.e., sample efficiency and safety. Although model-based RL methods can greatly mitigate the efficiency problem of greenhouse control, the safety problem has not got too much attention. In this paper, we present a model-based robust RL framework for autonomous greenhouse control to meet the sample efficiency and safety challenges. Specifically, our framework introduces an ensemble of environment models to work as a simulator and assist in policy optimization, thereby addressing the low sample efficiency problem. As for the safety concern, we propose a sample dropout module to focus more on worst-case samples, which can help improve the adaptability of the greenhouse planting policy in extreme cases. Experimental results demonstrate that our approach can learn a more effective greenhouse planting policy with better robustness than existing methods.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhang21e/zhang21e.pdf",
        "supp": "",
        "pdf_size": 4231685,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17441111895629393038&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Robust Regression for Monocular Depth Estimation",
        "site": "https://proceedings.mlr.press/v157/lienen21a.html",
        "author": "Julian Lienen; Nils Nommensen; Ralph Ewerth; Eyke H\u00fcllermeier",
        "abstract": "Learning accurate models for monocular depth estimation requires precise depth annotation as e.g. gathered through LiDAR scanners. Because the data acquisition with sensors of this kind is costly and does not scale well in general, less advanced depth sources, such as time-of-flight cameras, are often used instead. However, these sensors provide less reliable signals, resulting in imprecise depth data for training regression models. As shown in idealized environments, the noise produced by commonly used RGB-D sensors violates standard statistical assumptions of regression methods, such as least squares estimation. In this paper, we investigate whether robust regression methods, which are more tolerant toward violations of statistical assumptions, can mitigate the effects of low-quality data. As a viable alternative to established approaches of that kind, we propose the use of so-called superset learning, where the original data is replaced by (less precise but more reliable) set-valued data. To evaluate and compare the methods, we provide an extensive empirical study on common benchmark data for monocular depth estimation. Our results clearly show the superiority of robust variants over conventional regression.",
        "bibtex": "@InProceedings{pmlr-v157-lienen21a,\n  title = \t {Robust Regression for Monocular Depth Estimation},\n  author =       {Lienen, Julian and Nommensen, Nils and Ewerth, Ralph and H\\\"ullermeier, Eyke},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1001--1016},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/lienen21a/lienen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/lienen21a.html},\n  abstract = \t {Learning accurate models for monocular depth estimation requires precise depth annotation as e.g. gathered through LiDAR scanners. Because the data acquisition with sensors of this kind is costly and does not scale well in general, less advanced depth sources, such as time-of-flight cameras, are often used instead. However, these sensors provide less reliable signals, resulting in imprecise depth data for training regression models. As shown in idealized environments, the noise produced by commonly used RGB-D sensors violates standard statistical assumptions of regression methods, such as least squares estimation. In this paper, we investigate whether robust regression methods, which are more tolerant toward violations of statistical assumptions, can mitigate the effects of low-quality data. As a viable alternative to established approaches of that kind, we propose the use of so-called superset learning, where the original data is replaced by (less precise but more reliable) set-valued data. To evaluate and compare the methods, we provide an extensive empirical study on common benchmark data for monocular depth estimation. Our results clearly show the superiority of robust variants over conventional regression.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/lienen21a/lienen21a.pdf",
        "supp": "",
        "pdf_size": 451773,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4461430726019479398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Paderborn University, Germany; L3S Research Center, Leibniz University Hannover and TIB Hannover, Germany; L3S Research Center, Leibniz University Hannover and TIB Hannover, Germany; University of Munich (LMU), Germany",
        "aff_domain": "upb.de;tib.eu;tib.eu;ifi.lmu.de",
        "email": "upb.de;tib.eu;tib.eu;ifi.lmu.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Paderborn University;Leibniz University Hannover;University of Munich",
        "aff_unique_dep": ";L3S Research Center;",
        "aff_unique_url": "https://www.uni-paderborn.de;https://www.uni-hannover.de;https://www.lmu.de",
        "aff_unique_abbr": "UPB;LUH;LMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving",
        "site": "https://proceedings.mlr.press/v157/chen21a.html",
        "author": "Weihuang Chen; Fangfang Wang; Hongbin Sun",
        "abstract": "To safely and rationally participate in dense and heterogeneous traffic, autonomous vehicles require to sufficiently analyze the motion patterns of surrounding traffic-agents and accurately predict their future trajectories. This is challenging because the trajectories of traffic-agents are not only influenced by the traffic-agents themselves but also by spatial interaction with each other. Previous methods usually rely on the sequential step-by-step processing of Long Short-Term Memory networks (LSTMs) and merely extract the interactions between spatial neighbors for single type traffic-agents. We propose the Spatio-Temporal Transformer Networks (S2TNet), which models the spatio-temporal interactions by spatio-temporal Transformer and deals with the temporel sequences by temporal Transformer. We input additional category, shape and heading information into our networks to handle the heterogeneity of traffic-agents. The proposed methods outperforms state-of-the-art methods on ApolloScape Trajectory dataset by more than 7% on both the weighted sum of Average and Final Displacement Error.",
        "bibtex": "@InProceedings{pmlr-v157-chen21a,\n  title = \t {S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving},\n  author =       {Chen, Weihuang and Wang, Fangfang and Sun, Hongbin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {454--469},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/chen21a/chen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/chen21a.html},\n  abstract = \t {To safely and rationally participate in dense and heterogeneous traffic, autonomous vehicles require to sufficiently analyze the motion patterns of surrounding traffic-agents and accurately predict their future trajectories. This is challenging because the trajectories of traffic-agents are not only influenced by the traffic-agents themselves but also by spatial interaction with each other. Previous methods usually rely on the sequential step-by-step processing of Long Short-Term Memory networks (LSTMs) and merely extract the interactions between spatial neighbors for single type traffic-agents. We propose the Spatio-Temporal Transformer Networks (S2TNet), which models the spatio-temporal interactions by spatio-temporal Transformer and deals with the temporel sequences by temporal Transformer. We input additional category, shape and heading information into our networks to handle the heterogeneity of traffic-agents. The proposed methods outperforms state-of-the-art methods on ApolloScape Trajectory dataset by more than 7% on both the weighted sum of Average and Final Displacement Error.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/chen21a/chen21a.pdf",
        "supp": "",
        "pdf_size": 818819,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=314428335434400298&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Xi\u2019an Jiaotong University, Xi\u2019an, China; Xi\u2019an Jiaotong University, Xi\u2019an, China; Xi\u2019an Jiaotong University, Xi\u2019an, China",
        "aff_domain": "stu.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn",
        "email": "stu.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Xi'an Jiaotong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.xjtu.edu.cn",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "SPDE-Net: Neural Network based prediction of stabilization parameter for SUPG technique",
        "site": "https://proceedings.mlr.press/v157/yadav21a.html",
        "author": "Sangeeta Yadav; Sashikumaar Ganesan",
        "abstract": "We propose \\textit{SPDE-Net}, an artificial neural network (ANN) to predict the stabilization parameter for the streamline upwind/Petrov-Galerkin (SUPG) stabilization technique for solving singularly perturbed differential equations (SPDEs). The prediction task is modeled as a regression problem and is solved using ANN. Three training strategies for the ANN have been proposed i.e supervised, $L^2$ error minimization (global) and $L^2$ error minimization (local). It has been observed that the proposed method yields accurate results, and even outperforms some of the existing state-of-the-art ANN-based partial differential equation (PDE) solvers such as Physics Informed Neural Network (PINN).",
        "bibtex": "@InProceedings{pmlr-v157-yadav21a,\n  title = \t {SPDE-Net: Neural Network based prediction of stabilization parameter for SUPG technique},\n  author =       {Yadav, Sangeeta and Ganesan, Sashikumaar},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {268--283},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/yadav21a/yadav21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/yadav21a.html},\n  abstract = \t {We propose \\textit{SPDE-Net}, an artificial neural network (ANN) to predict the stabilization parameter for the streamline upwind/Petrov-Galerkin (SUPG) stabilization technique for solving singularly perturbed differential equations (SPDEs). The prediction task is modeled as a regression problem and is solved using ANN. Three training strategies for the ANN have been proposed i.e supervised, $L^2$ error minimization (global) and $L^2$ error minimization (local). It has been observed that the proposed method yields accurate results, and even outperforms some of the existing state-of-the-art ANN-based partial differential equation (PDE) solvers such as Physics Informed Neural Network (PINN).}\n}",
        "pdf": "https://proceedings.mlr.press/v157/yadav21a/yadav21a.pdf",
        "supp": "",
        "pdf_size": 949979,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16279290006779233449&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India",
        "aff_domain": "iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computational and Data Sciences",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Scalable gradient matching based on state space Gaussian Processes",
        "site": "https://proceedings.mlr.press/v157/futami21b.html",
        "author": "Futoshi Futami",
        "abstract": "In many scientific fields, various phenomena are modeled by ordinary differential equations (ODEs). Parameters in ODEs are generally unknown and hard to measure directly. Since analytical solutions for ODEs can rarely be obtained, statistical methods are often used to infer parameters from experimental observations. Among many existing methods, Gaussian process-based gradient matching has been explored extensively. However, the existing method cannot be scaled to a massive dataset. Given $N$ data points, existing algorithms show $\\mathcal{O}(N^3)$ computational cost. In this paper, we propose a novel algorithm using the state space reformulation of Gaussian processes. More specifically, we reformulate Gaussian process gradient matching as a special state-space model problem, then approximate its posterior distribution by a novel Rao-Blackwellization filtering, which enjoys $\\mathcal{O}(N)$ computational cost. Moreover, our algorithm is expressed as closed forms, it is 1000 times more faster than existing methods measured in wall clock time.",
        "bibtex": "@InProceedings{pmlr-v157-futami21b,\n  title = \t {Scalable gradient matching based on state space Gaussian Processes},\n  author =       {Futami, Futoshi},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {769--784},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/futami21b/futami21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/futami21b.html},\n  abstract = \t {In many scientific fields, various phenomena are modeled by ordinary differential equations (ODEs). Parameters in ODEs are generally unknown and hard to measure directly. Since analytical solutions for ODEs can rarely be obtained, statistical methods are often used to infer parameters from experimental observations. Among many existing methods, Gaussian process-based gradient matching has been explored extensively. However, the existing method cannot be scaled to a massive dataset. Given $N$ data points, existing algorithms show $\\mathcal{O}(N^3)$ computational cost. In this paper, we propose a novel algorithm using the state space reformulation of Gaussian processes. More specifically, we reformulate Gaussian process gradient matching as a special state-space model problem, then approximate its posterior distribution by a novel Rao-Blackwellization filtering, which enjoys $\\mathcal{O}(N)$ computational cost. Moreover, our algorithm is expressed as closed forms, it is 1000 times more faster than existing methods measured in wall clock time.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/futami21b/futami21b.pdf",
        "supp": "",
        "pdf_size": 227926,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dM2Hy7PedeMJ:scholar.google.com/&scioq=Scalable+gradient+matching+based+on+state+space+Gaussian+Processes&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Communication Science Laboratories, NTT, KYOTO, JAPAN",
        "aff_domain": "hco.ntt.co.jp",
        "email": "hco.ntt.co.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "NTT Communication Science Laboratories",
        "aff_unique_dep": "Communication Science Laboratories",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Scaling Average-Linkage via Sparse Cluster Embeddings",
        "site": "https://proceedings.mlr.press/v157/lavastida21a.html",
        "author": "Thomas Lavastida; Kefu Lu; Benjamin Moseley; Yuyan Wang",
        "abstract": "Average-linkage is one of the most popular hierarchical clustering  algorithms. It is well known that average-linkage  does not scale to large data sets due to the slow asymptotic running time. The fastest known implementation has running time quadratic in the number of data points. This paper presents a technique that we call cluster embedding.  The embedding maps each cluster into a point in slightly higher dimensions. The pairwise distances between the mapped points approximate the average distance between clusters. By utilizing this embedding we scale the task of finding close pairs of clusters, which is a key step in average-linkage clustering. We achieve an approximate, sub-quadratic time implementation of average-linkage. We show  theoretically the algorithm proposed in this paper achieves a near-linear running time and scales to large data sets.  Moreover, its scalability empirically dominates average-linkage and typically offers 3-10x speed-up on large data sets.",
        "bibtex": "@InProceedings{pmlr-v157-lavastida21a,\n  title = \t {Scaling Average-Linkage via Sparse Cluster Embeddings},\n  author =       {Lavastida, Thomas and Lu, Kefu and Moseley, Benjamin and Wang, Yuyan},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1429--1444},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/lavastida21a/lavastida21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/lavastida21a.html},\n  abstract = \t {Average-linkage is one of the most popular hierarchical clustering  algorithms. It is well known that average-linkage  does not scale to large data sets due to the slow asymptotic running time. The fastest known implementation has running time quadratic in the number of data points. This paper presents a technique that we call cluster embedding.  The embedding maps each cluster into a point in slightly higher dimensions. The pairwise distances between the mapped points approximate the average distance between clusters. By utilizing this embedding we scale the task of finding close pairs of clusters, which is a key step in average-linkage clustering. We achieve an approximate, sub-quadratic time implementation of average-linkage. We show  theoretically the algorithm proposed in this paper achieves a near-linear running time and scales to large data sets.  Moreover, its scalability empirically dominates average-linkage and typically offers 3-10x speed-up on large data sets.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/lavastida21a/lavastida21a.pdf",
        "supp": "",
        "pdf_size": 741317,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4542593043707483650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University; Washington and Lee University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;wlu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;wlu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Washington and Lee University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.wlu.edu",
        "aff_unique_abbr": "CMU;W&L",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Semi-Open Attribute Extraction from Chinese Functional Description Text",
        "site": "https://proceedings.mlr.press/v157/zhang21f.html",
        "author": "Li Zhang; Yanzeng Li; Rouyu Zhang; Wenjie Li",
        "abstract": "Attribute extraction is a task to identify the attribute and the corresponding attribute value from unstructured text, which is important for extensive applications like web information retrieval and the recommended system. The traditional relation extraction-based methods or joint extraction-based systems are often perform attribute classify based on subject and attribute-value pairs, and extract the attribute triples in the scope of ontology schema categories, which is in the assumption of the close-world and cannot satisfy the diversity of attributes. In this work, we propose a semi-open information extraction system for attribute extraction in a multi-component framework. With the proposed semi-open attribute extraction system (SOAE), more attribute-value pairs can be discovered by extracting literal triples without the limitation of pre-defined ontology. An additional co-trained ontology-based attribute extraction model is appended as a component following the assumption of the partial-closed world (PCWA), remission the performance degradation of SOAE caused by missing of the literal predicate in raw text and contribute to extract richer attribute triples and construct more dense knowledge graph. For evaluating the performance of the attribute extraction system, we construct a Chinese functional description text dataset CNShipNet and conduct experiments on it. The experimental results demonstrate that our proposed approach outperforms several state-of-the-art baselines with a large margin.",
        "bibtex": "@InProceedings{pmlr-v157-zhang21f,\n  title = \t {Semi-Open Attribute Extraction from Chinese Functional Description Text},\n  author =       {Zhang, Li and Li, Yanzeng and Zhang, Rouyu and Li, Wenjie},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1505--1520},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhang21f/zhang21f.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhang21f.html},\n  abstract = \t {Attribute extraction is a task to identify the attribute and the corresponding attribute value from unstructured text, which is important for extensive applications like web information retrieval and the recommended system. The traditional relation extraction-based methods or joint extraction-based systems are often perform attribute classify based on subject and attribute-value pairs, and extract the attribute triples in the scope of ontology schema categories, which is in the assumption of the close-world and cannot satisfy the diversity of attributes. In this work, we propose a semi-open information extraction system for attribute extraction in a multi-component framework. With the proposed semi-open attribute extraction system (SOAE), more attribute-value pairs can be discovered by extracting literal triples without the limitation of pre-defined ontology. An additional co-trained ontology-based attribute extraction model is appended as a component following the assumption of the partial-closed world (PCWA), remission the performance degradation of SOAE caused by missing of the literal predicate in raw text and contribute to extract richer attribute triples and construct more dense knowledge graph. For evaluating the performance of the attribute extraction system, we construct a Chinese functional description text dataset CNShipNet and conduct experiments on it. The experimental results demonstrate that our proposed approach outperforms several state-of-the-art baselines with a large margin.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhang21f/zhang21f.pdf",
        "supp": "",
        "pdf_size": 654313,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9717306875354316053&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Sinusoidal Flow: A Fast Invertible Autoregressive Flow",
        "site": "https://proceedings.mlr.press/v157/wei21a.html",
        "author": "Yumou Wei",
        "abstract": "Normalising flows offer a flexible way of modelling continuous probability distributions. We consider expressiveness, fast inversion and exact Jacobian determinant as three desirable properties a normalising flow should possess. However, few flow models have been able to strike a good balance among all these properties. Realising that the integral of a convex sum of sinusoidal functions squared leads to a bijective residual transformation, we propose Sinusoidal Flow, a new type of normalising flows that inherits the expressive power and triangular Jacobian from fully autoregressive flows while guaranteed by Banach fixed-point theorem to remain fast invertible and thereby obviate the need for sequential inversion typically required in fully autoregressive flows. Experiments show that our Sinusoidal Flow is not only able to model complex distributions, but can also be reliably inverted to generate realistic-looking samples even with many layers of transformations stacked.",
        "bibtex": "@InProceedings{pmlr-v157-wei21a,\n  title = \t {Sinusoidal Flow: A Fast Invertible Autoregressive Flow},\n  author =       {Wei, Yumou},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {299--314},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wei21a/wei21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wei21a.html},\n  abstract = \t {Normalising flows offer a flexible way of modelling continuous probability distributions. We consider expressiveness, fast inversion and exact Jacobian determinant as three desirable properties a normalising flow should possess. However, few flow models have been able to strike a good balance among all these properties. Realising that the integral of a convex sum of sinusoidal functions squared leads to a bijective residual transformation, we propose Sinusoidal Flow, a new type of normalising flows that inherits the expressive power and triangular Jacobian from fully autoregressive flows while guaranteed by Banach fixed-point theorem to remain fast invertible and thereby obviate the need for sequential inversion typically required in fully autoregressive flows. Experiments show that our Sinusoidal Flow is not only able to model complex distributions, but can also be reliably inverted to generate realistic-looking samples even with many layers of transformations stacked.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wei21a/wei21a.pdf",
        "supp": "",
        "pdf_size": 1002439,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Ht3iRAwkh_EJ:scholar.google.com/&scioq=Sinusoidal+Flow:+A+Fast+Invertible+Autoregressive+Flow&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "University of Michigan \u2014 Ann Arbor, MI USA",
        "aff_domain": "umich.edu",
        "email": "umich.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Skew-symmetrically perturbed gradient flow for convex optimization",
        "site": "https://proceedings.mlr.press/v157/futami21a.html",
        "author": "Futoshi Futami; Tomoharu Iwata; Naonori Ueda; Ikko Yamane",
        "abstract": "Recently, many methods for optimization and sampling have been developed by designing continuous dynamics followed by discretization. The dynamics that have been used for optimization have their corresponding underlying functionals to be minimized. On the other hand, a wider class of dynamics have been studied for sampling, which is not necessarily limited to functional minimization. For example, dynamics perturbed with skew-symmetric matrices, which cannot be seen as minimization of functionals, have been widely used to reduce asymptotic variance. Following this success in sampling, exploring such perturbed dynamics in the context of optimization can open a new avenue to optimization algorithm design. In this work, we introduce a perturbation technique for sampling into optimization for strongly convex functions. We show that perturbation applied to the gradient flow yields rapid convergence in optimization for strongly convex functions. Based on this continuous dynamics, we propose an optimization algorithm for strongly convex functions with a novel discretization framework that combines the Euler method with the leapfrog method which is used in the Hamilton Monte Carlo method. Our numerical experiments show that the perturbation technique is useful for optimization.",
        "bibtex": "@InProceedings{pmlr-v157-futami21a,\n  title = \t {Skew-symmetrically perturbed gradient flow for convex optimization},\n  author =       {Futami, Futoshi and Iwata, Tomoharu and Ueda, Naonori and Yamane, Ikko},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {721--736},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/futami21a/futami21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/futami21a.html},\n  abstract = \t {Recently, many methods for optimization and sampling have been developed by designing continuous dynamics followed by discretization. The dynamics that have been used for optimization have their corresponding underlying functionals to be minimized. On the other hand, a wider class of dynamics have been studied for sampling, which is not necessarily limited to functional minimization. For example, dynamics perturbed with skew-symmetric matrices, which cannot be seen as minimization of functionals, have been widely used to reduce asymptotic variance. Following this success in sampling, exploring such perturbed dynamics in the context of optimization can open a new avenue to optimization algorithm design. In this work, we introduce a perturbation technique for sampling into optimization for strongly convex functions. We show that perturbation applied to the gradient flow yields rapid convergence in optimization for strongly convex functions. Based on this continuous dynamics, we propose an optimization algorithm for strongly convex functions with a novel discretization framework that combines the Euler method with the leapfrog method which is used in the Hamilton Monte Carlo method. Our numerical experiments show that the perturbation technique is useful for optimization.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/futami21a/futami21a.pdf",
        "supp": "",
        "pdf_size": 591219,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7367342842863935226&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Slice-sampling based 3D Object Classification",
        "site": "https://proceedings.mlr.press/v157/xiangwen21a.html",
        "author": "Zhao Xiangwen; Yang Yi-Jun; Zeng Wei; Yang Liqun; Wang Yao",
        "abstract": "Multiview-based 3D object detection achieved great success in the past years. However, for some complex models with complex inner structures, the performances of these methods are not satisfactory. This paper provides a method based on slide sampling for 3D object classification. First, we slice and sample the model from the different depths and directions to get the model\u2019s features. Then, a deep neural network designed based on the attention mechanism is used to classify the input data. The experiments show that the performance of our method is competitive on ModelNet. Moreover, for some special models with simple surfaces and complex inner structures, the performance of our method is outstanding and stable.",
        "bibtex": "@InProceedings{pmlr-v157-xiangwen21a,\n  title = \t {Slice-sampling based 3D Object Classification},\n  author =       {Xiangwen, Zhao and Yi-Jun, Yang and Wei, Zeng and Liqun, Yang and Yao, Wang},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {689--704},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/xiangwen21a/xiangwen21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/xiangwen21a.html},\n  abstract = \t {Multiview-based 3D object detection achieved great success in the past years. However, for some complex models with complex inner structures, the performances of these methods are not satisfactory. This paper provides a method based on slide sampling for 3D object classification. First, we slice and sample the model from the different depths and directions to get the model\u2019s features. Then, a deep neural network designed based on the attention mechanism is used to classify the input data. The experiments show that the performance of our method is competitive on ModelNet. Moreover, for some special models with simple surfaces and complex inner structures, the performance of our method is outstanding and stable.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/xiangwen21a/xiangwen21a.pdf",
        "supp": "",
        "pdf_size": 2762942,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oTLS6POdZ6kJ:scholar.google.com/&scioq=Slice-sampling+based+3D+Object+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Shandong University, Qingdao, China+State Key Laboratory of Astronautic Dynamics, Xi\u2019an, China; School of Computer Science and Technology, Xi\u2019an Jiaotong University, Xi\u2019an, China; School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, China; School of Computing and Information Science, Florida International University, Miami, U.S.; School of Computer Science and Technology, Shandong University, Qingdao, China",
        "aff_domain": "mail.sdu.edu.cn;xjtu.edu.cn;xjtu.edu.cn;fiu.edu;mail.sdu.edu.cn",
        "email": "mail.sdu.edu.cn;xjtu.edu.cn;xjtu.edu.cn;fiu.edu;mail.sdu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2;3;0",
        "aff_unique_norm": "Shandong University;State Key Laboratory of Astronautic Dynamics;Xi'an Jiaotong University;Florida International University",
        "aff_unique_dep": "School of Computer Science and Technology;;School of Computer Science and Technology;School of Computing and Information Science",
        "aff_unique_url": "http://www.sdu.edu.cn;;https://www.xjtu.edu.cn;https://www.fiu.edu",
        "aff_unique_abbr": "SDU;;XJTU;FIU",
        "aff_campus_unique_index": "0;2;2;3;0",
        "aff_campus_unique": "Qingdao;;Xi'an;Miami",
        "aff_country_unique_index": "0+0;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Solving Machine Learning Problems",
        "site": "https://proceedings.mlr.press/v157/tran21a.html",
        "author": "Sunny Tran; Pranav Krishna; Ishan Pakuwal; Prabhakar Kafle; Nikhil Singh; Jayson Lynch; Iddo Drori",
        "abstract": "Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT\u2019s 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students\u2019 average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.",
        "bibtex": "@InProceedings{pmlr-v157-tran21a,\n  title = \t {Solving Machine Learning Problems},\n  author =       {Tran, Sunny and Krishna, Pranav and Pakuwal, Ishan and Kafle, Prabhakar and Singh, Nikhil and Lynch, Jayson and Drori, Iddo},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {470--485},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/tran21a/tran21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/tran21a.html},\n  abstract = \t {Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT\u2019s 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students\u2019 average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/tran21a/tran21a.pdf",
        "supp": "",
        "pdf_size": 1696797,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9643395886751650870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "MIT EECS; MIT EECS; MIT EECS; MIT EECS; MIT Media Lab; University of Waterloo; MIT EECS",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Waterloo",
        "aff_unique_dep": "Electrical Engineering & Computer Science;",
        "aff_unique_url": "https://web.mit.edu;https://uwaterloo.ca",
        "aff_unique_abbr": "MIT;UW",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Spatial Temporal Enhanced Contrastive and Pretext Learning for Skeleton-based Action Representation",
        "site": "https://proceedings.mlr.press/v157/zhan21a.html",
        "author": "Yiwen Zhan; Yuchen Chen; Pengfei Ren; Haifeng Sun; Jingyu Wang; Qi Qi; Jianxin Liao",
        "abstract": "In this paper, we focus on unsupervised representation learning for skeleton-based action recognition. The critical issue of this task is extracting discriminative spatial-temporal information from skeleton sequences to form action representation. To better solve this, we propose a novel unsupervised framework named contrastive-pretext spatial-temporal network (CP-STN), aiming to achieve accurate action recognition by better exploiting discriminative spatial-temporal enhanced features from massive unlabeled data. We combine contrastive and pretext tasks learning paradigms in one framework by using asymmetric spatial and temporal augmentations to enable network extracting discriminative representations with spatial-temporal information fully. Furthermore, graph-based convolution is used as the backbone to explore natural spatial-temporal graph information in skeleton data. Extensive experimental results show that our CP-STN signi\ufb01cantly boosts the performance of existing skeleton-based action representations learning networks and achieves state-of-the-art accuracy on two challenging benchmarks in both unsupervised and semi-supervised settings.",
        "bibtex": "@InProceedings{pmlr-v157-zhan21a,\n  title = \t {Spatial Temporal Enhanced Contrastive and Pretext Learning for Skeleton-based Action Representation},\n  author =       {Zhan, Yiwen and Chen, Yuchen and Ren, Pengfei and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {534--547},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhan21a/zhan21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhan21a.html},\n  abstract = \t {In this paper, we focus on unsupervised representation learning for skeleton-based action recognition. The critical issue of this task is extracting discriminative spatial-temporal information from skeleton sequences to form action representation. To better solve this, we propose a novel unsupervised framework named contrastive-pretext spatial-temporal network (CP-STN), aiming to achieve accurate action recognition by better exploiting discriminative spatial-temporal enhanced features from massive unlabeled data. We combine contrastive and pretext tasks learning paradigms in one framework by using asymmetric spatial and temporal augmentations to enable network extracting discriminative representations with spatial-temporal information fully. Furthermore, graph-based convolution is used as the backbone to explore natural spatial-temporal graph information in skeleton data. Extensive experimental results show that our CP-STN signi\ufb01cantly boosts the performance of existing skeleton-based action representations learning networks and achieves state-of-the-art accuracy on two challenging benchmarks in both unsupervised and semi-supervised settings.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhan21a/zhan21a.pdf",
        "supp": "",
        "pdf_size": 1705337,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4218009795104594551&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;gmail.com",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Speaker Diarization as a Fully Online Bandit Learning Problem in MiniVox",
        "site": "https://proceedings.mlr.press/v157/lin21c.html",
        "author": "Baihan Lin; Xinxin Zhang",
        "abstract": "We propose a novel machine learning framework to conduct real-time multi-speaker diarization and recognition without prior registration and pretraining in a fully online learning setting. Our contributions are two-fold. First, we propose a new benchmark to evaluate the rarely studied fully online speaker diarization problem. We build upon existing datasets of real world utterances to automatically curate MiniVox, an experimental environment which generates infinite configurations of continuous multi-speaker speech stream. Second, we consider the practical problem of online learning with episodically revealed rewards and introduce a solution based on semi-supervised and self-supervised learning methods. Additionally, we provide a workable web-based recognition system which interactively handles the cold start problem of new user\u2019s addition by transferring representations of old arms to new ones with an extendable contextual bandit. We demonstrate that our proposed method obtains robust performance in the online MiniVox framework given either cepstrum-based representations or deep neural network embeddings.",
        "bibtex": "@InProceedings{pmlr-v157-lin21c,\n  title = \t {Speaker Diarization as a Fully Online Bandit Learning Problem in MiniVox},\n  author =       {Lin, Baihan and Zhang, Xinxin},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1660--1674},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/lin21c/lin21c.pdf},\n  url = \t {https://proceedings.mlr.press/v157/lin21c.html},\n  abstract = \t {We propose a novel machine learning framework to conduct real-time multi-speaker diarization and recognition without prior registration and pretraining in a fully online learning setting. Our contributions are two-fold. First, we propose a new benchmark to evaluate the rarely studied fully online speaker diarization problem. We build upon existing datasets of real world utterances to automatically curate MiniVox, an experimental environment which generates infinite configurations of continuous multi-speaker speech stream. Second, we consider the practical problem of online learning with episodically revealed rewards and introduce a solution based on semi-supervised and self-supervised learning methods. Additionally, we provide a workable web-based recognition system which interactively handles the cold start problem of new user\u2019s addition by transferring representations of old arms to new ones with an extendable contextual bandit. We demonstrate that our proposed method obtains robust performance in the online MiniVox framework given either cepstrum-based representations or deep neural network embeddings.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/lin21c/lin21c.pdf",
        "supp": "",
        "pdf_size": 2835996,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10898281779886503076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Columbia University; New York University",
        "aff_domain": "columbia.edu;nyu.edu",
        "email": "columbia.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Columbia University;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.nyu.edu",
        "aff_unique_abbr": "Columbia;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Temporal Relation based Attentive Prototype Network for Few-shot Action Recognition",
        "site": "https://proceedings.mlr.press/v157/wang21b.html",
        "author": "Guangge Wang; Haihui Ye; Xiao Wang; Weirong Ye; Hanzi Wang",
        "abstract": "Few-shot action recognition aims at recognizing novel action classes with only a small number of labeled video samples. We propose a temporal relation based attentive prototype network (TRAPN) for few-shot action recognition. Concretely, we tackle this challenging task from three aspects. Firstly, we propose a spatio-temporal motion enhancement (STME) module to highlight object motions in videos. The STME module utilizes cues from content displacements in videos to enhance the features in the motion-related regions. Secondly, we learn the core common action transformations by our temporal relation (TR) module, which captures the temporal relations at short-term and long-term time scales. The learned temporal relations are encoded into descriptors to constitute sample-level features. The abstract action transformations are described by multiple groups of temporal relation descriptors. Thirdly, a vanilla prototype for the support class (e.g., the mean of the support class) cannot \ufb01t well for different query samples. We generate an attentive prototype constructed from temporal relation descriptors of support samples, which gives more weight to discriminative samples. We evaluate our TRAPN on Kinetics, UCF101 and HMDB51 real-world few-shot datasets. Results show that our network achieves the state-of-the-art performance.",
        "bibtex": "@InProceedings{pmlr-v157-wang21b,\n  title = \t {Temporal Relation based Attentive Prototype Network for Few-shot Action Recognition},\n  author =       {Wang, Guangge and Ye, Haihui and Wang, Xiao and Ye, Weirong and Wang, Hanzi},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {406--421},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wang21b/wang21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wang21b.html},\n  abstract = \t {Few-shot action recognition aims at recognizing novel action classes with only a small number of labeled video samples. We propose a temporal relation based attentive prototype network (TRAPN) for few-shot action recognition. Concretely, we tackle this challenging task from three aspects. Firstly, we propose a spatio-temporal motion enhancement (STME) module to highlight object motions in videos. The STME module utilizes cues from content displacements in videos to enhance the features in the motion-related regions. Secondly, we learn the core common action transformations by our temporal relation (TR) module, which captures the temporal relations at short-term and long-term time scales. The learned temporal relations are encoded into descriptors to constitute sample-level features. The abstract action transformations are described by multiple groups of temporal relation descriptors. Thirdly, a vanilla prototype for the support class (e.g., the mean of the support class) cannot \ufb01t well for different query samples. We generate an attentive prototype constructed from temporal relation descriptors of support samples, which gives more weight to discriminative samples. We evaluate our TRAPN on Kinetics, UCF101 and HMDB51 real-world few-shot datasets. Results show that our network achieves the state-of-the-art performance.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wang21b/wang21b.pdf",
        "supp": "",
        "pdf_size": 3182717,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2504930756443149905&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Fujian Key Laboratory of Sensing and Computing for Smart City, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Informatics, Xiamen University, Xiamen, China",
        "aff_domain": "stu.xmu.edu.cn;163.com;stu.xmu.edu.cn;stu.xmu.edu.cn;gmail.com",
        "email": "stu.xmu.edu.cn;163.com;stu.xmu.edu.cn;stu.xmu.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Xiamen University",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.xmu.edu.cn",
        "aff_unique_abbr": "XMU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Xiamen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization",
        "site": "https://proceedings.mlr.press/v157/defazio21a.html",
        "author": "Aaron Defazio; Robert M. Gower",
        "abstract": "The convergence rates for convex and non-convex optimization methods depend on the choice of a host of constants, including step-sizes, Lyapunov function constants and momentum constants. In this work we propose the use of factorial powers as a flexible tool for defining constants that appear in convergence proofs. We list a number of remarkable properties that these sequences enjoy, and show how they can be applied to convergence proofs to simplify or improve the convergence rates of the momentum method, accelerated gradient and the stochastic variance reduced method (SVRG).",
        "bibtex": "@InProceedings{pmlr-v157-defazio21a,\n  title = \t {The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization},\n  author =       {Defazio, Aaron and Gower, Robert M.},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {49--64},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/defazio21a/defazio21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/defazio21a.html},\n  abstract = \t {The convergence rates for convex and non-convex optimization methods depend on the choice of a host of constants, including step-sizes, Lyapunov function constants and momentum constants. In this work we propose the use of factorial powers as a flexible tool for defining constants that appear in convergence proofs. We list a number of remarkable properties that these sequences enjoy, and show how they can be applied to convergence proofs to simplify or improve the convergence rates of the momentum method, accelerated gradient and the stochastic variance reduced method (SVRG).}\n}",
        "pdf": "https://proceedings.mlr.press/v157/defazio21a/defazio21a.pdf",
        "supp": "",
        "pdf_size": 787043,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17440081051301406313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Facebook AI Research; Telecom Paris, IPP1",
        "aff_domain": "fb.com;gmail.com",
        "email": "fb.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Facebook;Telecom Paris",
        "aff_unique_dep": "Facebook AI Research;Institut Polytechnique de Paris",
        "aff_unique_url": "https://research.facebook.com;https://www.telecom-paris.fr",
        "aff_unique_abbr": "FAIR;Telecom Paris",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Time-Constrained Multi-Agent Path Finding in Non-Lattice Graphs with Deep Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v157/knippenberg21a.html",
        "author": "Marijn van Knippenberg; Mike Holenderski; Vlado Menkovski",
        "abstract": "Multi-Agent Path Finding (MAPF) is a routing problem in which multiple agents need to each find a lowest-cost collection of routes in a graph that avoids collisions between agents. This problem occurs frequently in the domain of logistics, for example in the routing of trains in shunting yards, airplanes at airports, and picking robots in automated warehouses. A solution is presented for the MAPF problem in which agents operate on an arbitrary directed graph, rather than the commonly assumed grid world, which extends support to use cases where the environment cannot be easily modeled in a grid shape. Furthermore, constraints are introduced on the start and end times of the routing tasks, which is vital in MAPF problems that are part of larger logistics systems. A Reinforcement Learning-based (RL) approach is proposed to learn a local routing policy for an agent in a manner that relieves the need for manually designing heuristics. It relies on a Graph Convolutional Network to handle arbitrary graphs. Both single-agent and multi-agent RL approaches are presented, showing how a multi-agent setup can reduce training time by exploiting the similarities in agent properties and local graph topologies.",
        "bibtex": "@InProceedings{pmlr-v157-knippenberg21a,\n  title = \t {Time-Constrained Multi-Agent Path Finding in Non-Lattice Graphs with Deep Reinforcement Learning},\n  author =       {van Knippenberg, Marijn and Holenderski, Mike and Menkovski, Vlado},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1317--1332},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/knippenberg21a/knippenberg21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/knippenberg21a.html},\n  abstract = \t {Multi-Agent Path Finding (MAPF) is a routing problem in which multiple agents need to each find a lowest-cost collection of routes in a graph that avoids collisions between agents. This problem occurs frequently in the domain of logistics, for example in the routing of trains in shunting yards, airplanes at airports, and picking robots in automated warehouses. A solution is presented for the MAPF problem in which agents operate on an arbitrary directed graph, rather than the commonly assumed grid world, which extends support to use cases where the environment cannot be easily modeled in a grid shape. Furthermore, constraints are introduced on the start and end times of the routing tasks, which is vital in MAPF problems that are part of larger logistics systems. A Reinforcement Learning-based (RL) approach is proposed to learn a local routing policy for an agent in a manner that relieves the need for manually designing heuristics. It relies on a Graph Convolutional Network to handle arbitrary graphs. Both single-agent and multi-agent RL approaches are presented, showing how a multi-agent setup can reduce training time by exploiting the similarities in agent properties and local graph topologies.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/knippenberg21a/knippenberg21a.pdf",
        "supp": "",
        "pdf_size": 2292637,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3220640657968040244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Eindhoven University of Technology; Eindhoven University of Technology; Eindhoven University of Technology",
        "aff_domain": "tue.nl;tue.nl;tue.nl",
        "email": "tue.nl;tue.nl;tue.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Eindhoven University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tue.nl",
        "aff_unique_abbr": "TU/e",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "title": "Transfer Learning with Adaptive Online TrAdaBoost for Data Streams",
        "site": "https://proceedings.mlr.press/v157/wu21b.html",
        "author": "Ocean Wu; Yun Sing Koh; Gillian Dobbie; Thomas Lacombe",
        "abstract": "In many real-world applications, data are often produced in the form of streams. Consider, for example, data produced by sensors. In data streams there can be concept drift where the distribution of the data changes. When we deal with multiple streams from the same domain, concepts that have occurred in one stream may occur in another. Therefore, being able to reuse knowledge across multiple streams can help models recover from concept drifts more quickly. A major challenge is that these data streams may be only partially identical and a direct adoption would not suffice. In this paper, we propose a novel framework to transfer both identical and partially identical concepts across different streams. In particular, we propose a new technique called Adaptive Online TrAdaBoost that tunes weight adjustments during boosting based on model performance. The experiments on synthetic data verify the desired properties of the proposed method, and the experiments on real-world data show the better performance of the method for data stream mining compared with its baselines.",
        "bibtex": "@InProceedings{pmlr-v157-wu21b,\n  title = \t {Transfer Learning with Adaptive Online TrAdaBoost for Data Streams},\n  author =       {Wu, Ocean and Koh, Yun Sing and Dobbie, Gillian and Lacombe, Thomas},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1017--1032},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wu21b/wu21b.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wu21b.html},\n  abstract = \t {In many real-world applications, data are often produced in the form of streams. Consider, for example, data produced by sensors. In data streams there can be concept drift where the distribution of the data changes. When we deal with multiple streams from the same domain, concepts that have occurred in one stream may occur in another. Therefore, being able to reuse knowledge across multiple streams can help models recover from concept drifts more quickly. A major challenge is that these data streams may be only partially identical and a direct adoption would not suffice. In this paper, we propose a novel framework to transfer both identical and partially identical concepts across different streams. In particular, we propose a new technique called Adaptive Online TrAdaBoost that tunes weight adjustments during boosting based on model performance. The experiments on synthetic data verify the desired properties of the proposed method, and the experiments on real-world data show the better performance of the method for data stream mining compared with its baselines.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wu21b/wu21b.pdf",
        "supp": "",
        "pdf_size": 533524,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=632407056373471854&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, The University of Auckland, New Zealand; School of Computer Science, The University of Auckland, New Zealand; School of Computer Science, The University of Auckland, New Zealand; School of Computer Science, The University of Auckland, New Zealand",
        "aff_domain": "aucklanduni.ac.nz;cs.auckland.ac.nz;auckland.ac.nz;auckland.ac.nz",
        "email": "aucklanduni.ac.nz;cs.auckland.ac.nz;auckland.ac.nz;auckland.ac.nz",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The University of Auckland",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.auckland.ac.nz",
        "aff_unique_abbr": "UoA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Auckland",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "title": "Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron",
        "site": "https://proceedings.mlr.press/v157/wang21a.html",
        "author": "Jun-Kun Wang; Jacob Abernethy",
        "abstract": "Over-parametrization has become a popular technique in deep learning. It is observed that by over-parametrization, a larger neural network needs a fewer training iterations than a smaller one to achieve a certain level of performance \u2014 namely, over-parametrization leads to acceleration in optimization. However, despite that over-parametrization is widely used nowadays, little theory is available to explain the acceleration due to over-parametrization. In this paper, we propose understanding it by studying a simple problem first. Specifically, we consider the setting that there is a single teacher neuron with quadratic activation, where over-parametrization is realized by having multiple student neurons learn the data generated from the teacher neuron. We provably show that over-parametrization helps the iterate generated by gradient descent to enter the neighborhood of a global optimal solution that achieves zero testing error faster.",
        "bibtex": "@InProceedings{pmlr-v157-wang21a,\n  title = \t {Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron},\n  author =       {Wang, Jun-Kun and Abernethy, Jacob},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {17--32},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/wang21a/wang21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/wang21a.html},\n  abstract = \t {Over-parametrization has become a popular technique in deep learning. It is observed that by over-parametrization, a larger neural network needs a fewer training iterations than a smaller one to achieve a certain level of performance \u2014 namely, over-parametrization leads to acceleration in optimization. However, despite that over-parametrization is widely used nowadays, little theory is available to explain the acceleration due to over-parametrization. In this paper, we propose understanding it by studying a simple problem first. Specifically, we consider the setting that there is a single teacher neuron with quadratic activation, where over-parametrization is realized by having multiple student neurons learn the data generated from the teacher neuron. We provably show that over-parametrization helps the iterate generated by gradient descent to enter the neighborhood of a global optimal solution that achieves zero testing error faster.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/wang21a/wang21a.pdf",
        "supp": "",
        "pdf_size": 3102027,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10178392630048432320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Yale University; Georgia Institute of Technology",
        "aff_domain": "yale.edu;gatech.edu",
        "email": "yale.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yale University;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yale.edu;https://www.gatech.edu",
        "aff_unique_abbr": "Yale;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Cycle-Consistent Network for Removing Susceptibility Artifacts in Single-shot EPI",
        "site": "https://proceedings.mlr.press/v157/xie21a.html",
        "author": "Weida Xie; Shi Chen; Qingjia Bao; Kewen Liu; Zhao Li; Xiaojun Li; Chongxin Bai; Piqiang Li; Chaoyang Liu; Otikovs Martins",
        "abstract": "Single-shot EPI(ssEPI) is one of the most important ultrafast MRI sequences commonly used for diffusion-weighted MRI and functional MRI. However, ssEPI suffers from susceptibility artifacts, especially in the high field or at the tissue boundaries. The widely used blip/down approaches, such as TOPUP, estimate the underlying distortion field from a pair of images with reversed-phase encoding direction. Typically, the iterative methods are used to find a solution to the ill-posed problem of finding the displacement map that maps up/down acquisitions onto each other. Then the geometric and intensity corrections are applied to obtain the undistorted images based on the estimated displacement map. This paper presents a new unsupervised cycle-consistent deep neural network that takes advantage of both the deep neural network and the gradient reversal method. The proposed method consists of three main components: (1) the Resnet50-Unet to map the pair of images with inverted phase encoding to the displacement maps; (2) the geometric and intensity correction module to obtain the undistorted images; (3) the forward model is applied to get the cycled blip up/down images, and the cycle-consistent loss is optimized. In addition, the CNN network will generate two field maps to overcome motion or field drift during the scan. This new network is trained unsupervised on the clinical datasets downloaded from the Human Connection Project website. And we test this method on both preclinical and clinical datasets. The preclinical dataset is collected from 20 mice based on the modified EPI pulse sequence in 7T scanner. Both simulated and experimental results demonstrate that our method outperforms state-of-the-art methods. In conclusion, we proposed an unsupervised cycle-consistent deep neural network for removing susceptibility artifacts. The results on both preclinical and clinical datasets show this new method\u2019s acceleration and generalization capabilities.",
        "bibtex": "@InProceedings{pmlr-v157-xie21a,\n  title = \t {Unsupervised Cycle-Consistent Network for Removing Susceptibility Artifacts in Single-shot {EPI}},\n  author =       {Xie, Weida and Chen, Shi and Bao, Qingjia and Liu, Kewen and Li, Zhao and Li, Xiaojun and Bai, Chongxin and Li, Piqiang and Liu, Chaoyang and Martins, Otikovs},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1723--1738},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/xie21a/xie21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/xie21a.html},\n  abstract = \t {Single-shot EPI(ssEPI) is one of the most important ultrafast MRI sequences commonly used for diffusion-weighted MRI and functional MRI. However, ssEPI suffers from susceptibility artifacts, especially in the high field or at the tissue boundaries. The widely used blip/down approaches, such as TOPUP, estimate the underlying distortion field from a pair of images with reversed-phase encoding direction. Typically, the iterative methods are used to find a solution to the ill-posed problem of finding the displacement map that maps up/down acquisitions onto each other. Then the geometric and intensity corrections are applied to obtain the undistorted images based on the estimated displacement map. This paper presents a new unsupervised cycle-consistent deep neural network that takes advantage of both the deep neural network and the gradient reversal method. The proposed method consists of three main components: (1) the Resnet50-Unet to map the pair of images with inverted phase encoding to the displacement maps; (2) the geometric and intensity correction module to obtain the undistorted images; (3) the forward model is applied to get the cycled blip up/down images, and the cycle-consistent loss is optimized. In addition, the CNN network will generate two field maps to overcome motion or field drift during the scan. This new network is trained unsupervised on the clinical datasets downloaded from the Human Connection Project website. And we test this method on both preclinical and clinical datasets. The preclinical dataset is collected from 20 mice based on the modified EPI pulse sequence in 7T scanner. Both simulated and experimental results demonstrate that our method outperforms state-of-the-art methods. In conclusion, we proposed an unsupervised cycle-consistent deep neural network for removing susceptibility artifacts. The results on both preclinical and clinical datasets show this new method\u2019s acceleration and generalization capabilities.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/xie21a/xie21a.pdf",
        "supp": "",
        "pdf_size": 12416693,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10030483491710005758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Wuhan University of Technology; Wuhan University of Technology; Chinese Academy of Sciences; Wuhan University of Technology; State Key Laboratory of Magnetic Resonance and Atomic and Molecular Physics, Wuhan Institute of Physics and Math, Innovation Academy for Precision Measurement Science and Technology; Wuhan University of Technology; Weizmann Institute of Science, Rehovot, 76001; Wuhan University of Technology; Chinese Academy of Sciences; State Key Laboratory of Magnetic Resonance and Atomic and Molecular Physics, Wuhan Institute of Physics and Math, Innovation Academy for Precision Measurement Science and Technology",
        "aff_domain": "qq.com;163.com;gmail.com;126.com;qq.com;whut.edu.cn;weizmann.ac.il;qq.com;wipm.ac.cn;wipm.ac.cn",
        "email": "qq.com;163.com;gmail.com;126.com;qq.com;whut.edu.cn;weizmann.ac.il;qq.com;wipm.ac.cn;wipm.ac.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2;0;3;0;1;2",
        "aff_unique_norm": "Wuhan University of Technology;Chinese Academy of Sciences;Wuhan Institute of Physics and Mathematics;Weizmann Institute of Science",
        "aff_unique_dep": ";;State Key Laboratory of Magnetic Resonance and Atomic and Molecular Physics, Innovation Academy for Precision Measurement Science and Technology;",
        "aff_unique_url": "http://www.wut.edu.cn;https://www.cas.cn;http://www.wipm.ac.cn/;https://www.weizmann.ac.il",
        "aff_unique_abbr": "WUT;CAS;;Weizmann",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rehovot",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0;0;0",
        "aff_country_unique": "China;Israel"
    },
    {
        "title": "Uplift Modeling with High Class Imbalance",
        "site": "https://proceedings.mlr.press/v157/nyberg21a.html",
        "author": "Otto Nyberg; Tomasz Ku\u015bmierczyk; Arto Klami",
        "abstract": "Uplift modeling refers to estimating the causal effect of a treatment on an individual observation, used for instance to identify customers worth targeting with a discount in e-commerce. We introduce a simple yet effective undersampling strategy for dealing with the prevalent problem of high class imbalance (low conversion rate) in such applications. Our strategy is agnostic to the base learners and produces a 6.5% improvement over the best published benchmark for the largest public uplift data which incidentally exhibits high class imbalance. We also introduce a new metric on calibration for uplift modeling and present a strategy to improve the calibration of the proposed method.",
        "bibtex": "@InProceedings{pmlr-v157-nyberg21a,\n  title = \t {Uplift Modeling with High Class Imbalance},\n  author =       {Nyberg, Otto and Ku\\'smierczyk, Tomasz and Klami, Arto},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {315--330},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/nyberg21a/nyberg21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/nyberg21a.html},\n  abstract = \t {Uplift modeling refers to estimating the causal effect of a treatment on an individual observation, used for instance to identify customers worth targeting with a discount in e-commerce. We introduce a simple yet effective undersampling strategy for dealing with the prevalent problem of high class imbalance (low conversion rate) in such applications. Our strategy is agnostic to the base learners and produces a 6.5% improvement over the best published benchmark for the largest public uplift data which incidentally exhibits high class imbalance. We also introduce a new metric on calibration for uplift modeling and present a strategy to improve the calibration of the proposed method.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/nyberg21a/nyberg21a.pdf",
        "supp": "",
        "pdf_size": 632853,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=830832178497541012&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland",
        "aff_domain": "helsinki.fi;gmail.com;helsinki.fi",
        "email": "helsinki.fi;gmail.com;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Helsinki",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.helsinki.fi",
        "aff_unique_abbr": "UH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Helsinki",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Vector Transport Free Riemannian LBFGS for Optimization on Symmetric Positive Definite Matrix Manifolds",
        "site": "https://proceedings.mlr.press/v157/godaz21a.html",
        "author": "Reza Godaz; Benyamin Ghojogh; Reshad Hosseini; Reza Monsefi; Fakhri Karray; Mark Crowley",
        "abstract": "This work concentrates on optimization on Riemannian manifolds. The Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) algorithm is a commonly used quasi-Newton method for numerical optimization in Euclidean spaces. Riemannian LBFGS (RLBFGS) is an extension of this method to Riemannian manifolds. RLBFGS involves computationally expensive vector transports as well as unfolding recursions using adjoint vector transports. In this article, we propose two mappings in the tangent space using the inverse second root and Cholesky decomposition. These mappings make both vector transport and adjoint vector transport identity and therefore isometric. Identity vector transport makes RLBFGS less computationally expensive and its isometry is also very useful in convergence analysis of RLBFGS. Moreover, under the proposed mappings, the Riemannian metric reduces to Euclidean inner product, which is much less computationally expensive. We focus on the Symmetric Positive Definite (SPD) manifolds which are beneficial in various fields such as data science and statistics. This work opens a research opportunity for extension of the proposed mappings to other well-known manifolds.",
        "bibtex": "@InProceedings{pmlr-v157-godaz21a,\n  title = \t {Vector Transport Free Riemannian LBFGS for Optimization on Symmetric Positive Definite Matrix Manifolds},\n  author =       {Godaz, Reza and Ghojogh, Benyamin and Hosseini, Reshad and Monsefi, Reza and Karray, Fakhri and Crowley, Mark},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1--16},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/godaz21a/godaz21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/godaz21a.html},\n  abstract = \t {This work concentrates on optimization on Riemannian manifolds. The Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) algorithm is a commonly used quasi-Newton method for numerical optimization in Euclidean spaces. Riemannian LBFGS (RLBFGS) is an extension of this method to Riemannian manifolds. RLBFGS involves computationally expensive vector transports as well as unfolding recursions using adjoint vector transports. In this article, we propose two mappings in the tangent space using the inverse second root and Cholesky decomposition. These mappings make both vector transport and adjoint vector transport identity and therefore isometric. Identity vector transport makes RLBFGS less computationally expensive and its isometry is also very useful in convergence analysis of RLBFGS. Moreover, under the proposed mappings, the Riemannian metric reduces to Euclidean inner product, which is much less computationally expensive. We focus on the Symmetric Positive Definite (SPD) manifolds which are beneficial in various fields such as data science and statistics. This work opens a research opportunity for extension of the proposed mappings to other well-known manifolds.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/godaz21a/godaz21a.pdf",
        "supp": "",
        "pdf_size": 631990,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9388460379340526864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Electrical and Computer Engineering, University of Waterloo, ON, Canada; Department of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Electrical and Computer Engineering, University of Waterloo, ON, Canada; Department of Electrical and Computer Engineering, University of Waterloo, ON, Canada",
        "aff_domain": "mail.um.ac.ir;uwaterloo.ca;ut.ac.ir;um.ac.ir;uwaterloo.ca;uwaterloo.ca",
        "email": "mail.um.ac.ir;uwaterloo.ca;ut.ac.ir;um.ac.ir;uwaterloo.ca;uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;1;1",
        "aff_unique_norm": "Ferdowsi University of Mashhad;University of Waterloo;University of Tehran",
        "aff_unique_dep": "Department of Computer Engineering;Department of Electrical and Computer Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www Ferdowsi.ac.ir;https://uwaterloo.ca;https://www.ut.ac.ir",
        "aff_unique_abbr": "FUM;UW;UT",
        "aff_campus_unique_index": "0;1;2;0;1;1",
        "aff_campus_unique": "Mashhad;Waterloo;Tehran",
        "aff_country_unique_index": "0;1;0;0;1;1",
        "aff_country_unique": "Iran;Canada"
    },
    {
        "title": "Video Action Recognition with Neural Architecture Search",
        "site": "https://proceedings.mlr.press/v157/zhou21a.html",
        "author": "Yuanding Zhou; Baopu Li; Zhihui Wang; Haojie Li",
        "abstract": "Recently, deep convolutional neural networks have been widely used in the field of videoaction  recognition.   Current  approaches  tend  to  concentrate  on  the  structure  design  fordifferent backbone networks, but what kind of network structures can process video botheffectively and quickly still remains to be solved despite the encouraging progress.  With thehelp of neural architecture search (NAS), we search for three hyperparameters in the videoprocessing network, which are the number of frames, the number of layers per residual stageand the channel number for all layers.  We relax the entire search space into a continuoussearch  space,  and  search  for  a  set  of  network  architectures  that  balance  accuracy  andcomputational  efficiency  by  considering  accuracy  as  the  primary  optimization  goal  andcomputational complexity as the secondary optimization goal.  We conduct experiments onUCF101 and Kinetics400 datasets, validating new state-of-the-art results of the proposedNAS based scheme for video action recognition.",
        "bibtex": "@InProceedings{pmlr-v157-zhou21a,\n  title = \t {Video Action Recognition with Neural Architecture Search},\n  author =       {Zhou, Yuanding and Li, Baopu and Wang, Zhihui and Li, Haojie},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {1675--1690},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/zhou21a/zhou21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/zhou21a.html},\n  abstract = \t {Recently, deep convolutional neural networks have been widely used in the field of videoaction  recognition.   Current  approaches  tend  to  concentrate  on  the  structure  design  fordifferent backbone networks, but what kind of network structures can process video botheffectively and quickly still remains to be solved despite the encouraging progress.  With thehelp of neural architecture search (NAS), we search for three hyperparameters in the videoprocessing network, which are the number of frames, the number of layers per residual stageand the channel number for all layers.  We relax the entire search space into a continuoussearch  space,  and  search  for  a  set  of  network  architectures  that  balance  accuracy  andcomputational  efficiency  by  considering  accuracy  as  the  primary  optimization  goal  andcomputational complexity as the secondary optimization goal.  We conduct experiments onUCF101 and Kinetics400 datasets, validating new state-of-the-art results of the proposedNAS based scheme for video action recognition.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/zhou21a/zhou21a.pdf",
        "supp": "",
        "pdf_size": 467366,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5646081529849534045&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "calibrated adversarial training",
        "site": "https://proceedings.mlr.press/v157/huang21a.html",
        "author": "Tianjin Huang; Vlado Menkovski; Yulong Pei; Mykola Pechenizkiy",
        "abstract": "Adversarial training is an approach of increasing the robustness of models to adversarial attacks by including adversarial examples in the training set. One major challenge of producing adversarial examples is to contain sufficient perturbation in the example to flip the model\u2019s output while not making severe changes in the example\u2019s semantical content. Exuberant change in the semantical content could also change the true label of the example. Adding such examples to the training set results in adverse effects. In this paper, we present the Calibrated Adversarial Training, a method that reduces the adverse effects of semantic perturbations in adversarial training. The method produces pixel-level adaptations to the perturbations based on novel calibrated robust error. We provide theoretical analysis on the calibrated robust error and derive an upper bound for it. Our empirical results show a superior performance of the Calibrated Adversarial Training over a number of public datasets.",
        "bibtex": "@InProceedings{pmlr-v157-huang21a,\n  title = \t {calibrated adversarial training},\n  author =       {Huang, Tianjin and Menkovski, Vlado and Pei, Yulong and Pechenizkiy, Mykola},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {626--641},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/huang21a/huang21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/huang21a.html},\n  abstract = \t {Adversarial training is an approach of increasing the robustness of models to adversarial attacks by including adversarial examples in the training set. One major challenge of producing adversarial examples is to contain sufficient perturbation in the example to flip the model\u2019s output while not making severe changes in the example\u2019s semantical content. Exuberant change in the semantical content could also change the true label of the example. Adding such examples to the training set results in adverse effects. In this paper, we present the Calibrated Adversarial Training, a method that reduces the adverse effects of semantic perturbations in adversarial training. The method produces pixel-level adaptations to the perturbations based on novel calibrated robust error. We provide theoretical analysis on the calibrated robust error and derive an upper bound for it. Our empirical results show a superior performance of the Calibrated Adversarial Training over a number of public datasets.}\n}",
        "pdf": "https://proceedings.mlr.press/v157/huang21a/huang21a.pdf",
        "supp": "",
        "pdf_size": 680661,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=749796074833675862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Mathematics and Computer Science, Eindhoven University of Technology; Department of Mathematics and Computer Science, Eindhoven University of Technology; Department of Mathematics and Computer Science, Eindhoven University of Technology; Department of Mathematics and Computer Science, Eindhoven University of Technology + Faculty of Information Technology, University of Jyv\u00e4skyl\u00e4",
        "aff_domain": "tue.nl;tue.nl;tue.nl;tue.nl",
        "email": "tue.nl;tue.nl;tue.nl;tue.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Eindhoven University of Technology;University of Jyv\u00e4skyl\u00e4",
        "aff_unique_dep": "Department of Mathematics and Computer Science;Faculty of Information Technology",
        "aff_unique_url": "https://www.tue.nl;https://www.jyu.fi",
        "aff_unique_abbr": "TU/e;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Eindhoven;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Netherlands;Finland"
    }
]