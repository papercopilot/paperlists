[
    {
        "id": "2025.coling-main.658",
        "title": "A Benchmark and Robustness Study of In-Context-Learning with Large Language Models in Music Entity Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Detecting music entities such as song titles or artist names is a useful application to help use cases like processing music search queries or analyzing music consumption on the web. Recent approaches incorporate smaller language models (SLMs) like BERT and achieve high results. However, further research indicates a high influence of entity exposure during pre-training on the performance of the models. With the advent of large language models (LLMs), these outperform SLMs in a variety of downstream tasks. However, researchers are still divided if this is applicable to tasks like entity detection in texts due to issues like hallucination. In this paper, we provide a novel dataset of user-generated metadata and conduct a benchmark and a robustness study using recent LLMs with in-context-learning (ICL). Our results indicate that LLMs in the ICL setting yield higher performance than SLMs. We further uncover the large impact of entity exposure on the best performing LLM in our study.",
        "author": "Simon Hachmeier; Robert J\u00e4schke",
        "authorids": "/s/simon-hachmeier/; /r/robert-jaschke/",
        "bibtex": "@inproceedings{hachmeier-jaschke-2025-benchmark,\n    title = \"A Benchmark and Robustness Study of In-Context-Learning with Large Language Models in Music Entity Detection\",\n    author = {Hachmeier, Simon  and\n      J{\\\"a}schke, Robert},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.658/\",\n    pages = \"9845--9859\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.658.pdf",
        "site": "https://aclanthology.org/2025.coling-main.658/",
        "pdf_size": 456115,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:M-iGGF6v0ekJ:scholar.google.com/&scioq=A+Benchmark+and+Robustness+Study+of+In-Context-Learning+with+Large+Language+Models+in+Music+Entity+Detection&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Berlin School of Library and Information Science, Humboldt-Universit\u00e4t zu Berlin; Berlin School of Library and Information Science, Humboldt-Universit\u00e4t zu Berlin",
        "aff_domain": "hu-berlin.de;hu-berlin.de",
        "email": "hu-berlin.de;hu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Humboldt-Universit\u00e4t zu Berlin",
        "aff_unique_dep": "Berlin School of Library and Information Science",
        "aff_unique_url": "https://www.hu-berlin.de",
        "aff_unique_abbr": "HU Berlin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.341",
        "title": "A Benchmark of French ASR Systems Based on Error Severity",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic Speech Recognition (ASR) transcription errors are commonly assessed using metrics that compare them with a reference transcription, such as Word Error Rate (WER), which measures spelling deviations from the reference, or semantic score-based metrics. However, these approaches often overlook what is understandable to humans when interpreting transcription errors. To address this limitation, a new evaluation is proposed that categorizes errors into four levels of severity, further divided into subtypes, based on objective linguistic criteria, contextual patterns, and the use of content words as the unit of analysis. This metric is applied to a benchmark of 10 state-of-the-art ASR systems on French language, encompassing both HMM-based and end-to-end models. Our findings reveal the strengths and weaknesses of each system, identifying those that provide the most comfortable reading experience for users.",
        "author": "Antoine Tholly; Jane Wottawa; Mickael Rouvier; Richard Dufour",
        "authorids": "/a/antoine-tholly/; /j/jane-wottawa/; /m/mickael-rouvier/; /r/richard-dufour/",
        "bibtex": "@inproceedings{tholly-etal-2025-benchmark,\n    title = \"A Benchmark of {F}rench {ASR} Systems Based on Error Severity\",\n    author = \"Tholly, Antoine  and\n      Wottawa, Jane  and\n      Rouvier, Mickael  and\n      Dufour, Richard\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.341/\",\n    pages = \"5094--5101\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.341.pdf",
        "site": "https://aclanthology.org/2025.coling-main.341/",
        "pdf_size": 198940,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-eaY4jh_A0AJ:scholar.google.com/&scioq=A+Benchmark+of+French+ASR+Systems+Based+on+Error+Severity&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff": "LS2N, Nantes Universit\u00e9, France; LIUM, Le Mans Universit\u00e9, France; LIA, Avignon Universit\u00e9, France; LS2N, Nantes Universit\u00e9, France",
        "aff_domain": "univ-nantes.fr;univ-lemans.fr;univ-avignon.fr;univ-nantes.fr",
        "email": "univ-nantes.fr;univ-lemans.fr;univ-avignon.fr;univ-nantes.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Nantes Universit\u00e9;Le Mans Universit\u00e9;Avignon Universit\u00e9",
        "aff_unique_dep": "LS2N;LIUM;LIA",
        "aff_unique_url": "https://www.univ-nantes.fr;https://www.univ-lemans.fr;https://www.univ-avignon.fr",
        "aff_unique_abbr": ";Le Mans U;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Nantes;Le Mans;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.106",
        "title": "A Career Interview Dialogue System using Large Language Model-based Dynamic Slot Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study aims to improve the efficiency and quality of career interviews conducted by nursing managers. To this end, we have been developing a slot-filling dialogue system that engages in pre-interview to collect information on staff careers as a preparatory step before the actual interviews. Conventional slot-filling-based interview dialogue systems have limitations in the flexibility of information collection because the dialogue progresses based on predefined slot sets. We therefore propose a method that leverages large language models (LLMs) to dynamically generate new slots according to the flow of the dialogue, achieving more natural conversations. Furthermore, we incorporate abduction into the slot generation process to enable more appropriate and effective slot generation. To validate the effectiveness of the proposed method, we conducted experiments using a user simulator. The results suggest that the proposed method using abduction is effective in enhancing both information-collecting capabilities and the naturalness of the dialogue.",
        "author": "Ekai Hashimoto; Mikio Nakano; Takayoshi Sakurai; Shun Shiramatsu; Toshitake Komazaki; Shiho Tsuchiya",
        "authorids": "/e/ekai-hashimoto/; /m/mikio-nakano/; /t/takayoshi-sakurai/; /s/shun-shiramatsu/; /t/toshitake-komazaki/; /s/shiho-tsuchiya/",
        "bibtex": "@inproceedings{hashimoto-etal-2025-career,\n    title = \"A Career Interview Dialogue System using Large Language Model-based Dynamic Slot Generation\",\n    author = \"Hashimoto, Ekai  and\n      Nakano, Mikio  and\n      Sakurai, Takayoshi  and\n      Shiramatsu, Shun  and\n      Komazaki, Toshitake  and\n      Tsuchiya, Shiho\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.106/\",\n    pages = \"1562--1584\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.106.pdf",
        "site": "https://aclanthology.org/2025.coling-main.106/",
        "pdf_size": 634722,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:voU-CMmgeM4J:scholar.google.com/&scioq=A+Career+Interview+Dialogue+System+using+Large+Language+Model-based+Dynamic+Slot+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Nagoya Institute of Technology; Nagoya Institute of Technology + C4A Research Institute, Inc., Tokyo, Japan; Nagoya Institute of Technology; Nagoya Institute of Technology; Tokyo Healthcare University; Kitasato University Hospital",
        "aff_domain": "stn.nitech.ac.jp; ; ; ; ; ",
        "email": "stn.nitech.ac.jp; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;0;2;3",
        "aff_unique_norm": "Nagoya Institute of Technology;C4A Research Institute, Inc.;Tokyo Healthcare University;Kitasato University",
        "aff_unique_dep": ";;;Hospital",
        "aff_unique_url": "https://www.nitech.ac.jp;;https://www.tokyo-hu.ac.jp;https://www.kitasato-u.ac.jp/hospital/",
        "aff_unique_abbr": "NIT;;THU;Kitasato",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.577",
        "title": "A Chain-of-Task Framework for Instruction Tuning of LLMs Based on Chinese Grammatical Error Correction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Over-correction is a critical issue for large language models (LLMs) to address Grammatical Error Correction (GEC) task, esp. for Chinese. This paper proposes a Chain-of-Task (CoTask) framework to reduce over-correction. The CoTask framework is applied as multi-task instruction tuning of LLMs by decomposing the process of grammatical error analysis to design auxiliary tasks and adjusting the types and combinations of training tasks. A supervised fine-tuning (SFT) strategy is also presented to enhance the performance of LLMs, together with an algorithm for automatic dataset annotation to avoid additional manual costs. Experimental results demonstrate that our method achieves new state-of-the-art results on both FCGEC (in-domain) and NaCGEC (out-of-domain) test sets.",
        "author": "Xinpeng Liu; Bing Xu; Muyun Yang; Hailong Cao; Conghui Zhu; Tiejun Zhao; Wenpeng Lu",
        "authorids": "/x/xinpeng-liu/; /b/bing-xu/; /m/muyun-yang/; /h/hailong-cao/; /c/conghui-zhu/; /t/tiejun-zhao/; /w/wenpeng-lu/",
        "bibtex": "@inproceedings{liu-etal-2025-chain,\n    title = \"A Chain-of-Task Framework for Instruction Tuning of {LLM}s Based on {C}hinese Grammatical Error Correction\",\n    author = \"Liu, Xinpeng  and\n      Xu, Bing  and\n      Yang, Muyun  and\n      Cao, Hailong  and\n      Zhu, Conghui  and\n      Zhao, Tiejun  and\n      Lu, Wenpeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.577/\",\n    pages = \"8623--8639\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.577.pdf",
        "site": "https://aclanthology.org/2025.coling-main.577/",
        "pdf_size": 1235463,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:P84q_99rbQcJ:scholar.google.com/&scioq=A+Chain-of-Task+Framework+for+Instruction+Tuning+of+LLMs+Based+on+Chinese+Grammatical+Error+Correction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China",
        "aff_domain": "stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn;qlu.edu.cn",
        "email": "stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn;qlu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Harbin Institute of Technology;Qilu University of Technology",
        "aff_unique_dep": "Faculty of Computing;Key Laboratory of Computing Power Network and Information Security",
        "aff_unique_url": "http://www.hit.edu.cn/;http://www.qilu.edu.cn",
        "aff_unique_abbr": "HIT;QUT",
        "aff_campus_unique_index": "0;0;0;0;0;0;1",
        "aff_campus_unique": "Harbin;Jinan",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.712",
        "title": "A Collaborative Reasoning Framework Powered by Reinforcement Learning and Large Language Models for Complex Questions Answering over Knowledge Graph",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge Graph Question Answering (KGQA) aims to automatically answer natural language questions by reasoning across multiple triples in knowledge graphs (KGs). Reinforcement learning (RL)-based methods are introduced to enhance model interpretability. Nevertheless, when addressing complex questions requiring long-term reasoning, the RL agent is usually misled by aimless exploration, as it lacks common learning practices with prior knowledge. Recently, large language models (LLMs) have been proven to encode vast amounts of knowledge about the world and possess remarkable reasoning capabilities. However, they often encounter challenges with hallucination issues, failing to address complex questions that demand deep and deliberate reasoning. In this paper, we propose a collaborative reasoning framework (CRF) powered by RL and LLMs to answer complex questions based on the knowledge graph. Our approach leverages the common sense priors contained in LLMs while utilizing RL to provide learning from the environment, resulting in a hierarchical agent that uses LLMs to solve the complex KGQA task. By combining LLMs and the RL policy, the high-level agent accurately identifies constraints encountered during reasoning, while the low-level agent conducts efficient path reasoning by selecting the most promising relations in KG. Extensive experiments conducted on four benchmark datasets clearly demonstrate the effectiveness of the proposed model, which surpasses state-of-the-art approaches.",
        "author": "Zhiqiang Zhang; Wen Zhao",
        "authorids": "/z/zhiqiang-zhang/; /w/wen-zhao/",
        "bibtex": "https://aclanthology.org/2025.coling-main.712.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.712.pdf",
        "site": "https://aclanthology.org/2025.coling-main.712/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ISnSmCW781sJ:scholar.google.com/&scioq=A+Collaborative+Reasoning+Framework+Powered+by+Reinforcement+Learning+and+Large+Language+Models+for+Complex+Questions+Answering+over+Knowledge+Graph&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2025.coling-main.112",
        "title": "A Combinatorial Approach to Neural Emergent Communication",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Substantial research on deep learning-based emergent communication uses the referential game framework, specifically the Lewis signaling game, however we argue that successful communication in this game typically only need one or two symbols for target image classification because of a sampling pitfall in the training data. To address this issue, we provide a theoretical analysis and introduce a combinatorial algorithm SolveMinSym (SMS) to solve the symbolic complexity for classification, which is the minimum number of symbols in the message for successful communication. We use the SMS algorithm to create datasets with different symbolic complexity to empirically show that data with higher symbolic complexity increases the number of effective symbols in the emergent language.",
        "author": "Zheyuan Zhang",
        "authorids": "/z/zheyuan-zhang/",
        "bibtex": "@inproceedings{zhang-2025-combinatorial,\n    title = \"A Combinatorial Approach to Neural Emergent Communication\",\n    author = \"Zhang, Zheyuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.112/\",\n    pages = \"1660--1666\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.112.pdf",
        "site": "https://aclanthology.org/2025.coling-main.112/",
        "pdf_size": 485543,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pPLOgw4otIsJ:scholar.google.com/&scioq=A+Combinatorial+Approach+to+Neural+Emergent+Communication&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "University of Michigan",
        "aff_domain": "umich.edu",
        "email": "umich.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.178",
        "title": "A Compliance Checking Framework Based on Retrieval Augmented Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The text-based compliance checking aims to verify whether a company\u2019s business processes comply with laws, regulations, and industry standards using NLP techniques. Existing methods can be divided into two categories: Logic-based methods offer the advantage of precise and reliable reasoning processes but lack flexibility. Semantic embedding methods are more generalizable; however, they may lose structured information and lack logical coherence. To combine the strengths of both approaches, we propose a compliance checking framework based on Retrieval-Augmented Generation (RAG). This framework includes a static layer for storing factual knowledge, a dynamic layer for storing regulatory and business process information, and a computational layer for retrieval and reasoning. We employ an eventic graph to structurally describe regulatory information as we recognize that the knowledge in regulatory documents is centered not on entities but on actions and states. We conducted experiments on Chinese and English compliance checking datasets. The results demonstrate that our framework consistently achieves state-of-the-art results across various scenarios, surpassing other baselines.",
        "author": "Jingyun Sun; Zhongze Luo; Yang Li",
        "authorids": "/j/jingyun-sun/; /z/zhongze-luo/; /y/yang-li/",
        "bibtex": "@inproceedings{sun-etal-2025-compliance,\n    title = \"A Compliance Checking Framework Based on Retrieval Augmented Generation\",\n    author = \"Sun, Jingyun  and\n      Luo, Zhongze  and\n      Li, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.178/\",\n    pages = \"2603--2615\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.178.pdf",
        "site": "https://aclanthology.org/2025.coling-main.178/",
        "pdf_size": 1459367,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6F28r69jY2AJ:scholar.google.com/&scioq=A+Compliance+Checking+Framework+Based+on+Retrieval+Augmented+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Northeast Forestry University, Harbin, China; Northeast Forestry University, Harbin, China; Northeast Forestry University, Harbin, China",
        "aff_domain": "nefu.edu.cn;nefu.edu.cn;nefu.edu.cn",
        "email": "nefu.edu.cn;nefu.edu.cn;nefu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northeast Forestry University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.nefu.edu.cn",
        "aff_unique_abbr": "NEFU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.85",
        "title": "A Compressive Memory-based Retrieval Approach for Event Argument Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent works have demonstrated the effectiveness of retrieval augmentation in the Event Argument Extraction (EAE) task. However, existing retrieval-based EAE methods have two main limitations: (1) input length constraints and (2) the gap between the retriever and the inference model. These issues limit the diversity and quality of the retrieved information. In this paper, we propose a Compressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the two limitations mentioned above. Our compressive memory, designed as a dynamic matrix that effectively caches retrieved information and supports continuous updates, overcomes the limitations of input length. Additionally, after pre-loading all candidate demonstrations into the compressive memory, the model further retrieves and filters relevant information from the memory based on the input query, bridging the gap between the retriever and the inference model. Extensive experiments show that our method achieves new state-of-the-art performance on three public datasets (RAMS, WikiEvents, ACE05), significantly outperforming existing retrieval-based EAE methods.",
        "author": "Wanlong Liu; Enqi Zhang; Shaohuan Cheng; Dingyi Zeng; Li Zhou; Chen Zhang; Malu Zhang; Wenyu Chen",
        "authorids": "/w/wanlong-liu/; /e/enqi-zhang/; /s/shaohuan-cheng/; /d/dingyi-zeng/; /l/li-zhou/; /c/chen-zhang/; /m/malu-zhang/; /w/wenyu-chen/",
        "bibtex": "@inproceedings{liu-etal-2025-compressive,\n    title = \"A Compressive Memory-based Retrieval Approach for Event Argument Extraction\",\n    author = \"Liu, Wanlong  and\n      Zhang, Enqi  and\n      Cheng, Shaohuan  and\n      Zeng, Dingyi  and\n      Zhou, Li  and\n      Zhang, Chen  and\n      Zhang, Malu  and\n      Chen, Wenyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.85/\",\n    pages = \"1278--1293\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.85.pdf",
        "site": "https://aclanthology.org/2025.coling-main.85/",
        "pdf_size": 812409,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12469349401203826382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.380",
        "title": "A Context-Aware Approach for Enhancing Data Imputation with Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a novel approach named Contextually Relevant Imputation leveraging pre-trained Language Models (CRILM) for handling missing data in tabular datasets. Instead of relying on traditional numerical estimations, CRILM uses pre-trained language models (LMs) to create contextually relevant descriptors for missing values. This method aligns datasets with LMs\u2019 strengths, allowing large LMs to generate these descriptors and small LMs to be fine-tuned on the enriched datasets for enhanced downstream task performance. Our evaluations demonstrate CRILM\u2019s superior performance and robustness across MCAR, MAR, and challenging MNAR scenarios, with up to a 10% improvement over the best-performing baselines. By mitigating biases, particularly in MNAR settings, CRILM improves downstream task performance and offers a cost-effective solution for resource-constrained environments.",
        "author": "Ahatsham Hayat; Mohammad R. Hasan",
        "authorids": "/a/ahatsham-hayat/; /m/mohammad-r-hasan/",
        "bibtex": "@inproceedings{hayat-hasan-2025-context,\n    title = \"A Context-Aware Approach for Enhancing Data Imputation with Pre-trained Language Models\",\n    author = \"Hayat, Ahatsham  and\n      Hasan, Mohammad R.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.380/\",\n    pages = \"5668--5685\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.380.pdf",
        "site": "https://aclanthology.org/2025.coling-main.380/",
        "pdf_size": 892520,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4500655481456880815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Electrical and Computer Engineering, University of Nebraska-Lincoln; Electrical and Computer Engineering, University of Nebraska-Lincoln",
        "aff_domain": "huskers.unl.edu;unl.edu",
        "email": "huskers.unl.edu;unl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Nebraska-Lincoln",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.unl.edu",
        "aff_unique_abbr": "UNL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lincoln",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.756",
        "title": "A Dataset for Expert Reviewer Recommendation with Large Language Models as Zero-shot Rankers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of reviewer recommendation is increasingly important, with main techniques utilizing general models of text relevance. However, state of the art (SotA) systems still have relatively high error rates. Two possible reasons for this are: a lack of large datasets and the fact that large language models (LLMs) have not yet been applied. To fill these gaps, we first create a substantial new dataset, in the domain of Internet specification documents; then we introduce the use of LLMs and evaluate their performance. We find that LLMs with prompting can improve on SotA in some cases, but that they are not a cure-all: this task provides a challenging setting for prompt-based methods",
        "author": "Vanja M. Karan; Stephen McQuistin; Ryo Yanagida; Colin Perkins; Gareth Tyson; Ignacio Castro; Patrick G.T. Healey; Matthew Purver",
        "authorids": "/v/vanja-m-karan/; /s/stephen-mcquistin/; /r/ryo-yanagida/; /c/colin-perkins/; /g/gareth-tyson/; /i/ignacio-castro/; /p/patrick-healey/; /m/matthew-purver/",
        "bibtex": "@inproceedings{karan-etal-2025-dataset,\n    title = \"A Dataset for Expert Reviewer Recommendation with Large Language Models as Zero-shot Rankers\",\n    author = \"Karan, Vanja M.  and\n      McQuistin, Stephen  and\n      Yanagida, Ryo  and\n      Perkins, Colin  and\n      Tyson, Gareth  and\n      Castro, Ignacio  and\n      Healey, Patrick G.T.  and\n      Purver, Matthew\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.756/\",\n    pages = \"11422--11427\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.756.pdf",
        "site": "https://aclanthology.org/2025.coling-main.756/",
        "pdf_size": 260484,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gI0_IOEoxugJ:scholar.google.com/&scioq=A+Dataset+for+Expert+Reviewer+Recommendation+with+Large+Language+Models+as+Zero-shot+Rankers&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Queen Mary University of London; University of St Andrews; University of Glasgow; University of Glasgow; Queen Mary University of London; Queen Mary University of London; Queen Mary University of London; Queen Mary University of London + Jo\u017eef Stefan Institute, Ljubljana",
        "aff_domain": "univie.ac.at; ; ; ; ; ; ;qmul.ac.uk",
        "email": "univie.ac.at; ; ; ; ; ; ;qmul.ac.uk",
        "github": "https://github.com/sodestream/revrec",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;2;0;0;0;0+3",
        "aff_unique_norm": "Queen Mary University of London;University of St Andrews;University of Glasgow;Jo\u017eef Stefan Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.qmul.ac.uk;https://www.st-andrews.ac.uk;https://www.gla.ac.uk;https://www.ijs.si",
        "aff_unique_abbr": "QMUL;St Andrews;Glasgow;JSI",
        "aff_campus_unique_index": "0;0;0;0;0+2",
        "aff_campus_unique": "London;;Ljubljana",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+1",
        "aff_country_unique": "United Kingdom;Slovenia"
    },
    {
        "id": "2025.coling-main.272",
        "title": "A Dual Contrastive Learning Framework for Enhanced Multimodal Conversational Emotion Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal Emotion Recognition in Conversations (MERC) identifies utterance emotions by integrating both contextual and multimodal information from dialogue videos. Existing methods struggle to capture emotion shifts due to label replication and fail to preserve positive independent modality contributions during fusion. To address these issues, we propose a Dual Contrastive Learning Framework (DCLF) that enhances current MERC models without additional data. Specifically, to mitigate label replication effects, we construct context-aware contrastive pairs. Additionally, we assign pseudo-labels to distinguish modality-specific contributions. DCLF works alongside basic models to introduce semantic constraints at the utterance, context, and modality levels. Our experiments on two MERC benchmark datasets demonstrate performance gains of 4.67%-4.98% on IEMOCAP and 5.52%-5.89% on MELD, outperforming state-of-the-art approaches. Perturbation tests further validate DCLF\u2019s ability to reduce label dependence. Additionally, DCLF incorporates emotion-sensitive independent modality features and multimodal fusion representations into final decisions, unlocking the potential contributions of individual modalities.",
        "author": "Yunhe Xie; Chengjie Sun; Ziyi Cao; Bingquan Liu; Zhenzhou Ji; Yuanchao Liu; Lili Shan",
        "authorids": "/y/yunhe-xie/; /c/cheng-jie-sun/; /z/ziyi-cao/; /b/bingquan-liu/; /z/zhenzhou-ji/; /y/yuanchao-liu/; /l/lili-shan/",
        "bibtex": "@inproceedings{xie-etal-2025-dual,\n    title = \"A Dual Contrastive Learning Framework for Enhanced Multimodal Conversational Emotion Recognition\",\n    author = \"Xie, Yunhe  and\n      Sun, Chengjie  and\n      Cao, Ziyi  and\n      Liu, Bingquan  and\n      Ji, Zhenzhou  and\n      Liu, Yuanchao  and\n      Shan, Lili\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.272/\",\n    pages = \"4055--4065\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.272.pdf",
        "site": "https://aclanthology.org/2025.coling-main.272/",
        "pdf_size": 890885,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=104480748907817381&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology",
        "aff_domain": "hit.edu.cn;hit.edu.cn;stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;163.com;hit.edu.cn",
        "email": "hit.edu.cn;hit.edu.cn;stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;163.com;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "Faculty of Computing",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.561",
        "title": "A Flash in the Pan: Better Prompting Strategies to Deploy Out-of-the-Box LLMs as Conversational Recommendation Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational Recommendation Systems (CRSs) are a particularly interesting application for out-of-the-box LLMs due to their potential for eliciting user preferences and making recommendations in natural language across a wide set of domains. Somewhat surprisingly, we find however that in such a conversational application, the more questions a user answers about their preferences, the worse the model\u2019s recommendations become. We demonstrate this phenomenon on a previously published dataset as well as two novel datasets which we contribute. We also explain why earlier benchmarks failed to detect this round-over-round performance loss, highlighting the importance of the evaluation strategy we use and expanding upon Li et al. (2023a). We also present preference elicitation and recommendation strategies that mitigate this degradation in performance, beating state-of-the-art results, and show how three underlying models, GPT-3.5, GPT-4, and Claude 3.5 Sonnet, differently impact these strategies. Our datasets and code are available at https://github.com/CtrlVGustavo/A-Flash- in-the-Pan-CRS.",
        "author": "Gustavo Adolpho Lucas de Carvalho; Simon Benigeri; Jennifer Healey; Victor Bursztyn; David Demeter; Lawrence Birnbaum",
        "authorids": "/g/gustavo-adolpho-lucas-de-carvalho/; /s/simon-benigeri/; /j/jennifer-healey/; /v/victor-bursztyn/; /d/david-demeter/; /l/lawrence-birnbaum/",
        "bibtex": "@inproceedings{lucas-de-carvalho-etal-2025-flash,\n    title = \"A Flash in the Pan: Better Prompting Strategies to Deploy Out-of-the-Box {LLM}s as Conversational Recommendation Systems\",\n    author = \"Lucas de Carvalho, Gustavo Adolpho  and\n      Benigeri, Simon  and\n      Healey, Jennifer  and\n      Bursztyn, Victor  and\n      Demeter, David  and\n      Birnbaum, Lawrence\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.561/\",\n    pages = \"8385--8398\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.561.pdf",
        "site": "https://aclanthology.org/2025.coling-main.561/",
        "pdf_size": 1687049,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4636357491450852270&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "University of Southern California; Northwestern University; Adobe Research; Adobe Research; Northwestern University; Northwestern University",
        "aff_domain": "usc.edu;northwestern.edu;adobe.com;adobe.com;northwestern.edu;northwestern.edu",
        "email": "usc.edu;northwestern.edu;adobe.com;adobe.com;northwestern.edu;northwestern.edu",
        "github": "https://github.com/CtrlVGustavo/A-Flash-in-the-Pan-CRS",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;2;1;1",
        "aff_unique_norm": "University of Southern California;Northwestern University;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "https://www.usc.edu;https://www.northwestern.edu;https://research.adobe.com",
        "aff_unique_abbr": "USC;NU;Adobe",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.464",
        "title": "A Framework for Effective Invocation Methods of Various LLM Services",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown impressive abilities in solving various natural language processing tasks and are now widely offered as services. LLM services enable users to accomplish tasks without requiring specialized knowledge, simply by paying service providers. However, numerous providers offer various LLM services with variations in pricing, latency, and performance. These factors are also affected by different invocation methods, such as the choice of context and the use of cache, which lead to unpredictable and uncontrollable service cost and quality. Consequently, utilizing various LLM services invocation methods to construct an effective (cost-saving, low-latency and high-performance) invocation strategy that best meets task demands becomes a pressing challenge. This paper provides a comprehensive overview of methods help LLM services to be invoked efficiently. Technically, we define the problem of constructing an effective LLM services invocation strategy, and based on this, propose a unified LLM service invocation framework. The framework classifies existing methods into four categories: input abstraction, semantic cache, solution design, and output enhancement, which can be used separately or jointly during the invocation life cycle. We discuss the methods in each category and compare them to provide valuable guidance for researchers. Finally, we emphasize the open challenges in this domain and shed light on future research.",
        "author": "Can Wang; Dianbo Sui; Bolin Zhang; Xiaoyu Liu; Jiabao Kang; Zhidong Qiao; Zhiying Tu",
        "authorids": "/c/can-wang/; /d/dianbo-sui/; /b/bolin-zhang/; /x/xiaoyu-liu/; /j/jiabao-kang/; /z/zhidong-qiao/; /z/zhiying-tu/",
        "bibtex": "@inproceedings{wang-etal-2025-framework,\n    title = \"A Framework for Effective Invocation Methods of Various {LLM} Services\",\n    author = \"Wang, Can  and\n      Sui, Dianbo  and\n      Zhang, Bolin  and\n      Liu, Xiaoyu  and\n      Kang, Jiabao  and\n      Qiao, Zhidong  and\n      Tu, Zhiying\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.464/\",\n    pages = \"6953--6965\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.464.pdf",
        "site": "https://aclanthology.org/2025.coling-main.464/",
        "pdf_size": 1681118,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2156486339742298038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Harbin Institute of Technology (HIT), Weihai, Shandong, China; Harbin Institute of Technology (HIT), Weihai, Shandong, China; Harbin Institute of Technology (HIT), Weihai, Shandong, China; Harbin Institute of Technology (HIT), Weihai, Shandong, China; Harbin Institute of Technology (HIT), Weihai, Shandong, China; Harbin Institute of Technology (HIT), Weihai, Shandong, China; Harbin Institute of Technology (HIT), Weihai, Shandong, China",
        "aff_domain": "stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;hit.edu.cn",
        "email": "stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Weihai",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.82",
        "title": "A Graph Interaction Framework on Relevance for Multimodal Named Entity Recognition with Multiple Images",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Posts containing multiple images have significant research potential in Multimodal Named Entity Recognition nowadays. The previous methods determine whether the images are related to named entities in the text through similarity computation, such as using CLIP. However, it is not effective in some cases and not conducive to task transfer, especially in multi-image scenarios. To address the issue, we propose a graph interaction framework on relevance (GIFR) for Multimodal Named Entity Recognition with multiple images. For humans, they have the abilities to distinguish whether an image is relevant to named entities, but human capabilities are difficult to model. Therefore, we propose using reinforcement learning based on human preference to integrate human abilities into the model to determine whether an image-text pair is relevant, which is referred to as relevance. To better leverage relevance, we construct a heterogeneous graph and introduce graph transformer to enable information interaction. Experiments on benchmark datasets demonstrate that our method achieves the state-of-the-art performance.",
        "author": "Jiachen Zhao; Shizhou Huang; Xin Lin",
        "authorids": "/j/jiachen-zhao/; /s/shizhou-huang/; /x/xin-lin/",
        "bibtex": "@inproceedings{zhao-etal-2025-graph,\n    title = \"A Graph Interaction Framework on Relevance for Multimodal Named Entity Recognition with Multiple Images\",\n    author = \"Zhao, Jiachen  and\n      Huang, Shizhou  and\n      Lin, Xin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.82/\",\n    pages = \"1237--1246\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.82.pdf",
        "site": "https://aclanthology.org/2025.coling-main.82/",
        "pdf_size": 1935603,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3fW0YlwHAM0J:scholar.google.com/&scioq=A+Graph+Interaction+Framework+on+Relevance+for+Multimodal+Named+Entity+Recognition+with+Multiple+Images&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China",
        "aff_domain": "stu.ecnu.edu.cn;ica.stc.sh.cn;cs.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;ica.stc.sh.cn;cs.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "East China Normal University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ecnu.edu.cn",
        "aff_unique_abbr": "ECNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.674",
        "title": "A High-Quality Text-Rich Image Instruction Tuning Dataset via Hybrid Instruction Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large multimodal models still struggle with text-rich images because of inadequate training data. Self-Instruct provides an annotation-free way for generating instruction data, but its quality is poor, as multimodal alignment remains a hurdle even for the largest models. In this work, we propose LLaVAR-2, to enhance multimodal alignment for text-rich images through hybrid instruction generation between human annotators and large language models. Specifically, it involves detailed image captions from human annotators, followed by the use of these annotations in tailored text prompts for GPT-4o to curate a dataset. It also implements several mechanisms to filter out low-quality data, and the resulting dataset comprises 424k high-quality pairs of instructions. Empirical results show that models fine-tuned on this dataset exhibit impressive enhancements over those trained with self-instruct data.",
        "author": "Shijie Zhou; Ruiyi Zhang; Yufan Zhou; Changyou Chen",
        "authorids": "/s/shijie-zhou/; /r/ruiyi-zhang/; /y/yufan-zhou/; /c/changyou-chen/",
        "bibtex": "@inproceedings{zhou-etal-2025-high,\n    title = \"A High-Quality Text-Rich Image Instruction Tuning Dataset via Hybrid Instruction Generation\",\n    author = \"Zhou, Shijie  and\n      Zhang, Ruiyi  and\n      Zhou, Yufan  and\n      Chen, Changyou\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.674/\",\n    pages = \"10091--10110\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.674.pdf",
        "site": "https://aclanthology.org/2025.coling-main.674/",
        "pdf_size": 6970532,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mBim8xv9_YoJ:scholar.google.com/&scioq=A+High-Quality+Text-Rich+Image+Instruction+Tuning+Dataset+via+Hybrid+Instruction+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University at Buffalo; Adobe Research; Adobe Research; University at Buffalo",
        "aff_domain": ";adobe.com;adobe.com;buffalo.edu",
        "email": ";adobe.com;adobe.com;buffalo.edu",
        "github": "https://github.com/llavar/LLaV AR-2",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University at Buffalo;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.buffalo.edu;https://research.adobe.com",
        "aff_unique_abbr": "UB;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.354",
        "title": "A Knowledge Graph Reasoning-Based Model for Computerized Adaptive Testing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The significant of Computerized Adaptive Testing (CAT) is self-evident in contemporary Intelligent Tutoring Systems (ITSs) which aims to recommend suitable questions for students based on their knowledge state. In recent years, Graph Neural Networks (GNNs) and Reinforcement Learning (RL) methods have been increasingly applied to CAT. While these approaches have achieved empirical success, they still face limitations, such as inadequate handling of concept relevance when multiple concepts are involved and incomplete evaluation metrics. To address these issues, we propose a Knowledge Graph Reasoning-Based Model for CAT (KGCAT), which leverages the reasoning power of knowledge graphs (KGs) to capture the semantic and relational information between concepts and questions while focusing on reducing the noise caused by concepts with low relevance by utilizing mutual information. Additionally, a multi-objective reinforcement learning framework is employed to incorporate multiple evaluation objectives, further refining question selection and improving the overall effectiveness of CAT. Empirical evaluations conducted on three authentic educational datasets demonstrate that the proposed model outperforms existing methods in both accuracy and interpretability.",
        "author": "Xinyi Qiu; Zhiyun Chen",
        "authorids": "/x/xinyi-qiu/; /z/zhiyun-chen/",
        "bibtex": "https://aclanthology.org/2025.coling-main.354.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.354.pdf",
        "site": "https://aclanthology.org/2025.coling-main.354/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:anGac_G2LRUJ:scholar.google.com/&scioq=A+Knowledge+Graph+Reasoning-Based+Model+for+Computerized+Adaptive+Testing&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2025.coling-main.378",
        "title": "A Novel Negative Sample Generation Method for Contrastive Learning in Hierarchical Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hierarchical text classification (HTC) is an important task in natural language processing (NLP). Existing methods typically utilize both text features and the hierarchical structure of labels to categorize text effectively. However, these approaches often struggle with fine-grained labels, which are closely similar, leading to difficulties in accurate classification. At the same time, contrastive learning has significant advantages in strengthening fine-grained label features and discrimination. However, the performance of contrastive learning strongly depends on the construction of negative samples. In this paper, we design a hierarchical sequence ranking (HiSR) method for generating diverse negative samples. These samples maximize the effectiveness of contrastive learning to enhance the ability of the model to distinguish between fine-grained labels and improve the performance of the model in HTC. Specifically, we transform the entire label set into linear sequences based on the hierarchical structure and rank these sequences according to their quality. During model training, the most suitable negative samples were dynamically selected from the ranked sequences. Then contrastive learning amplifies the differences between similar fine-grained labels by emphasizing the distinction between the ground truth and the generated negative samples, thereby enhancing the discriminative ability of the model. Our method has been tested on three public datasets and achieves state-of-art (SOTA) on two of them, demonstrating its effectiveness.",
        "author": "Juncheng Zhou; Lijuan Zhang; Yachen He; Rongli Fan; Lei Zhang; Jian Wan",
        "authorids": "/j/juncheng-zhou/; /l/lijuan-zhang/; /y/yachen-he/; /r/rongli-fan/; /l/lei-zhang/; /j/jian-wan/",
        "bibtex": "@inproceedings{zhou-etal-2025-novel,\n    title = \"A Novel Negative Sample Generation Method for Contrastive Learning in Hierarchical Text Classification\",\n    author = \"Zhou, Juncheng  and\n      Zhang, Lijuan  and\n      He, Yachen  and\n      Fan, Rongli  and\n      Zhang, Lei  and\n      Wan, Jian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.378/\",\n    pages = \"5645--5655\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.378.pdf",
        "site": "https://aclanthology.org/2025.coling-main.378/",
        "pdf_size": 1035597,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6210942619889747069&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Zhejiang University of Science and Technology; Zhejiang University of Science and Technology; ByteDance Ltd.; Zhejiang University of Science and Technology; Zhejiang University of Science and Technology; Zhejiang University of Science and Technology",
        "aff_domain": "zust.edu.cn;zust.edu.cn;bytedance.com;zust.edu.cn;zust.edu.cn;zust.edu.cn",
        "email": "zust.edu.cn;zust.edu.cn;bytedance.com;zust.edu.cn;zust.edu.cn;zust.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Zhejiang University of Science and Technology;ByteDance",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.zjust.edu.cn;https://www.bytedance.com",
        "aff_unique_abbr": "ZUST;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-demos.9",
        "title": "A Probabilistic Toolkit for Multi-grained Word Segmentation in Chinese",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "It is practically useful to provide consistent and reliable word segmentation results from different criteria at the same time, which is formulated as the multi-grained word segmentation (MWS) task. This paper describes a probabilistic toolkit for MWS in Chinese. We propose a new MWS approach based on the standard MTL framework. We adopt semi-Markov CRF for single-grained word segmentation (SWS), which can produce marginal probabilities of words during inference. For sentences that contain conflicts among SWS results, we employ the CKY decoding algorithm to resolve conflicts.Our resulting MWS tree can provide the criteria information of words, along with the probabilities. Moreover, we follow the works in SWS, and propose a simple strategy to exploit naturally annotated data for MWS, leading to substantial improvement of MWS performance in the cross-domain scenario.",
        "author": "Xi Ma; Yang Hou; Xuebin Wang; Zhenghua Li",
        "authorids": "/x/xi-ma/; /y/yang-hou/; /x/xuebin-wang/; /z/zhenghua-li/",
        "bibtex": "@inproceedings{ma-etal-2025-probabilistic,\n    title = \"A Probabilistic Toolkit for Multi-grained Word Segmentation in {C}hinese\",\n    author = \"Ma, Xi  and\n      Hou, Yang  and\n      Wang, Xuebin  and\n      Li, Zhenghua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.9/\",\n    pages = \"83--90\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.9.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.9/",
        "pdf_size": 697217,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fQyPhidXROUJ:scholar.google.com/&scioq=A+Probabilistic+Toolkit+for+Multi-grained+Word+Segmentation+in+Chinese&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.18",
        "title": "A Recipe For Building a Compliant Real Estate Chatbot",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In recent years, there has been significant effort to align large language models with human preferences. This work focuses on developing a chatbot specialized in the real estate domain, with an emphasis on incorporating compliant behavior to ensure it can be used without perpetuating discriminatory practices like steering and redlining, which have historically plagued the real estate industry in the United States. Building on prior work, we present a method for generating a synthetic general instruction-following dataset, along with safety data. Through extensive evaluations and benchmarks, we fine-tuned a llama-3-8B-instruct model and demonstrated that we can enhance it\u2019s performance significantly to match huge closed-source models like GPT-4o while making it safer and more compliant. We open-source the model, data and code to support further development and research in the community",
        "author": "Navid Madani; Anusha Bagalkotkar; Supriya Anand; Gabriel Arnson; Rohini K. Srihari; Kenneth Joseph",
        "authorids": "/n/navid-madani/; /a/anusha-bagalkotkar/; /s/supriya-anand/; /g/gabriel-arnson/; /r/rohini-k-srihari/; /k/kenneth-joseph/",
        "bibtex": "@inproceedings{madani-etal-2025-recipe,\n    title = \"A Recipe For Building a Compliant Real Estate Chatbot\",\n    author = \"Madani, Navid  and\n      Bagalkotkar, Anusha  and\n      Anand, Supriya  and\n      Arnson, Gabriel  and\n      Srihari, Rohini K.  and\n      Joseph, Kenneth\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.18/\",\n    pages = \"213--235\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.18.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.18/",
        "pdf_size": 4974006,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XjOU6iXraEMJ:scholar.google.com/&scioq=A+Recipe+For+Building+a+Compliant+Real+Estate+Chatbot&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Zillow Group + University at Buffalo; Zillow Group; Zillow Group; Zillow Group; University at Buffalo; University at Buffalo",
        "aff_domain": "zillowgroup.com;zillowgroup.com;zillowgroup.com;zillowgroup.com;buffalo.edu;buffalo.edu",
        "email": "zillowgroup.com;zillowgroup.com;zillowgroup.com;zillowgroup.com;buffalo.edu;buffalo.edu",
        "github": "https://github.com/zillow/compliant-real-estate-chatbot",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;0;1;1",
        "aff_unique_norm": "Zillow Group;University at Buffalo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zillow.com;https://www.buffalo.edu",
        "aff_unique_abbr": "Zillow;UB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.652",
        "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use, Planning (Including RAG), and Feedback Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on LMPR implementations and workflow usage across different agent paradigms.",
        "author": "Xinzhe Li",
        "authorids": "/x/xinzhe-li/",
        "bibtex": "@inproceedings{li-2025-review,\n    title = \"A Review of Prominent Paradigms for {LLM}-Based Agents: Tool Use, Planning (Including {RAG}), and Feedback Learning\",\n    author = \"Li, Xinzhe\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.652/\",\n    pages = \"9760--9779\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.652.pdf",
        "site": "https://aclanthology.org/2025.coling-main.652/",
        "pdf_size": 950906,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14352459729605501326&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Independent Researcher + Deakin University",
        "aff_domain": "outlook.com",
        "email": "outlook.com",
        "github": "https://github.com/xinzhel/LLM-Agent-Survey",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Independent Researcher;Deakin University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.deakin.edu.au",
        "aff_unique_abbr": ";Deakin",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Australia"
    },
    {
        "id": "2025.coling-industry.44",
        "title": "A Simple yet Efficient Prompt Compression Method for Text Classification Data Annotation Using LLM",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Effectively balancing accuracy and cost is a critical challenge when using large language models (LLMs) for corpus annotation. This paper introduces a novel compression method based on keyword extraction (PCKE) that effectively reduces the number of prompt tokens in text classification annotation tasks, with minimal to no loss in accuracy. Our approach begins with an LLM that generates both category labels and relevant keywords from a small unannotated dataset. These outputs are used to train a BERT-based multi-task model capable of simultaneous classification and keyword extraction. For larger unannotated corpora, this model extracts keywords which are then used in place of full texts for LLM annotation. The significant reduction in prompt tokens result in substantial cost savings. Furthermore, the use of a few well-chosen keywords ensures that classification accuracy is maintained. Extensive experiments validate that our method not only achieves a superior compression rate but also maintains high accuracy, outperforming existing general-purpose compression techniques. Our approach offers a practical and cost-efficient solution for large-scale text classification annotation using LLMs, particularly applicable in industrial settings.",
        "author": "Yiran Xie; Debin Xiao; Ping Wang; Shuming Liu",
        "authorids": "/y/yiran-xie/; /d/debin-xiao/; /p/ping-wang/; /s/shuming-liu/",
        "bibtex": "@inproceedings{xie-etal-2025-simple,\n    title = \"A Simple yet Efficient Prompt Compression Method for Text Classification Data Annotation Using {LLM}\",\n    author = \"Xie, Yiran  and\n      Xiao, Debin  and\n      Wang, Ping  and\n      Liu, Shuming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.44/\",\n    pages = \"511--521\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.44.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.44/",
        "pdf_size": 568333,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:FV3w-wsjHqgJ:scholar.google.com/&scioq=A+Simple+yet+Efficient+Prompt+Compression+Method+for+Text+Classification+Data+Annotation+Using+LLM&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "The Chinese University of Hong Kong, Shenzhen; Guangdong OPPO Mobile Telecommunications Corp., Ltd., Shenzhen; Guangdong OPPO Mobile Telecommunications Corp., Ltd., Shenzhen; Guangdong OPPO Mobile Telecommunications Corp., Ltd., Shenzhen",
        "aff_domain": "link.cuhk.edu.cn;oppo.com;oppo.com;oppo.com",
        "email": "link.cuhk.edu.cn;oppo.com;oppo.com;oppo.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "The Chinese University of Hong Kong;OPPO Mobile Telecommunications Corp., Ltd.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.cn;https://www.oppo.com",
        "aff_unique_abbr": "CUHK;OPPO",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.107",
        "title": "A Simple-Yet-Efficient Instruction Augmentation Method for Zero-Shot Sentiment Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Instruction tuning significantly enhances the performance of large language models in tasks such as sentiment classification. Previous studies have leveraged labeled instances from sentiment benchmark datasets to instruction-tune LLMs, improving zero-shot sentiment classification performance. In this work, we propose a simple-yet-efficient instruction augmentation method which does not rely on any actual labeled sentiment instances. With just 240 pseudo instruction instances, the proposed method significantly improve the classification performance across several LLMs on 12 benchmark datasets, increasing scores by 30 points and outperforming LLMs that utilize more complex instruction tuning methods by 5.1 points. Surprisingly, the models tuned with 240 pseudo-instructions even outperform those tuned with actual domain-specific instruction instances. Despite method\u2019s simplicity, our further analysis suggests that the probability shift toward the positive and negative classes and its generalization ability may be the primary driver of the improvement.",
        "author": "Yang Zhao; Masayasu Muraoka; Issei Yoshida; Bishwaranjan Bhattacharjee; Hiroshi Kanayama",
        "authorids": "/y/yang-zhao/; /m/masayasu-muraoka/; /i/issei-yoshida/; /b/bishwaranjan-bhattacharjee/; /h/hiroshi-kanayama/",
        "bibtex": "@inproceedings{zhao-etal-2025-simple,\n    title = \"A Simple-Yet-Efficient Instruction Augmentation Method for Zero-Shot Sentiment Classification\",\n    author = \"Zhao, Yang  and\n      Muraoka, Masayasu  and\n      Yoshida, Issei  and\n      Bhattacharjee, Bishwaranjan  and\n      Kanayama, Hiroshi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.107/\",\n    pages = \"1585--1599\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.107.pdf",
        "site": "https://aclanthology.org/2025.coling-main.107/",
        "pdf_size": 418159,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hA03Z5D4GwcJ:scholar.google.com/&scioq=A+Simple-Yet-Efficient+Instruction+Augmentation+Method+for+Zero-Shot+Sentiment+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "IBM Research - Tokyo; IBM Research - Tokyo; IBM Research - Tokyo; IBM Research, Yorktown Heights; IBM Research - Tokyo",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/code4coling/code",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "IBM Research",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Tokyo;Yorktown Heights",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2025.coling-main.307",
        "title": "A Survey of Code-switched Arabic NLP: Progress, Challenges, and Future Directions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language in the Arab world presents a complex diglossic and multilingual setting, involving the use of Modern Standard Arabic, various dialects and sub-dialects, as well as multiple European languages. This diverse linguistic landscape has given rise to code-switching, both within Arabic varieties and between Arabic and foreign languages. The widespread occurrence of code-switching across the region makes it vital to address these linguistic needs when developing language technologies. In this paper, we provide a review of the current literature in the field of code-switched Arabic NLP, offering a broad perspective on ongoing efforts, challenges, research gaps, and recommendations for future research directions.",
        "author": "Injy Hamed; Caroline Sabty; Slim Abdennadher; Ngoc Thang Vu; Thamar Solorio; Nizar Habash",
        "authorids": "/i/injy-hamed/; /c/caroline-sabty/; /s/slim-abdennadher/; /n/ngoc-thang-vu/; /t/thamar-solorio/; /n/nizar-habash/",
        "bibtex": "@inproceedings{hamed-etal-2025-survey,\n    title = \"A Survey of Code-switched {A}rabic {NLP}: Progress, Challenges, and Future Directions\",\n    author = \"Hamed, Injy  and\n      Sabty, Caroline  and\n      Abdennadher, Slim  and\n      Vu, Ngoc Thang  and\n      Solorio, Thamar  and\n      Habash, Nizar\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.307/\",\n    pages = \"4561--4585\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.307.pdf",
        "site": "https://aclanthology.org/2025.coling-main.307/",
        "pdf_size": 559801,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4724693665772637099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "MBZUAI+University of Houston+New York University Abu Dhabi; German International University, Egypt; German International University, Egypt; University of Stuttgart; MBZUAI+University of Houston; MBZUAI+New York University Abu Dhabi",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;giu-uni.de;giu-uni.de;ims.uni-stuttgart.de;nyu.edu",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae;giu-uni.de;giu-uni.de;ims.uni-stuttgart.de;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1+2;3;3;4;0+1;0+2",
        "aff_unique_norm": "Mohamed Bin Zayed University of Artificial Intelligence;University of Houston;New York University;German International University;University of Stuttgart",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.uh.edu;https://nyu.edu;;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "MBZUAI;UH;NYU;;USTuttgart",
        "aff_campus_unique_index": "1;;1",
        "aff_campus_unique": ";Abu Dhabi",
        "aff_country_unique_index": "0+1+0;2;2;3;0+1;0+0",
        "aff_country_unique": "United Arab Emirates;United States;Egypt;Germany"
    },
    {
        "id": "2025.coling-main.324",
        "title": "A Survey of Generative Information Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generative information extraction (Generative IE) aims to generate structured text sequences from unstructured text using a generative framework. Scaling in model size yields variations in adaptation and generalization, and also drives fundamental shifts in the techniques and approaches used within this domain. In this survey, we first review generative information extraction (IE) methods based on pre-trained language models (PLMs) and large language models (LLMs), focusing on their adaptation and generalization capabilities. We also discuss the connection between these methods and these two aspects. Furthermore, to balance task performance with the substantial computational demands associated with LLMs, we emphasize the importance of model collaboration. Finally, given the advanced capabilities of LLMs, we explore methods for integrating diverse IE tasks into unified models.",
        "author": "Zikang Zhang; Wangjie You; Tianci Wu; Xinrui Wang; Juntao Li; Min Zhang",
        "authorids": "/z/zikang-zhang/; /w/wangjie-you/; /t/tianci-wu/; /x/xinrui-wang/; /j/juntao-li/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2025-survey,\n    title = \"A Survey of Generative Information Extraction\",\n    author = \"Zhang, Zikang  and\n      You, Wangjie  and\n      Wu, Tianci  and\n      Wang, Xinrui  and\n      Li, Juntao  and\n      Zhang, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.324/\",\n    pages = \"4840--4870\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.324.pdf",
        "site": "https://aclanthology.org/2025.coling-main.324/",
        "pdf_size": 3143832,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18324378491396702674&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Institute of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.110",
        "title": "A Testset for Context-Aware LLM Translation in Korean-to-English Discourse Level Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) demonstrate remarkable performance in machine translation. Recent studies indicate that for high-resource languages, LLM surpasses encoder-decoder neural machine translation (NMT) models. However, evaluation datasets used in many LLM-based translation studies are often compromised by data leakage and lack demanding datasets that accurately gauge the potential and limitations of LLMs in human-like translation. This paper introduces a manually constructed Korean-English discourse-level corpus comprising 600 text instances featuring six linguistic phenomena: lexical ambiguity, zero anaphora, slang, idiom, figurative language, and implicature. Utilizing this challenge test set, we investigated LLM\u2019s Korean-to-English translation capability, particularly in cases requiring inter-sentential context based semantic inference. The findings reveal that state-of-the-art LLM, such as GPT-4o, still struggle with specific linguistic phenomena that can be challenging for machine translation. Additionally, step-by-step prompting, such as Chain-of-Thought (CoT) prompting, significantly enhance the translation performance of LLMs compared to zero-shot prompting.",
        "author": "Minjae Lee; Youngbin Noh; Seung Jin Lee",
        "authorids": "/m/minjae-lee/; /y/youngbin-noh/; /s/seung-jin-lee/",
        "bibtex": "@inproceedings{lee-etal-2025-testset,\n    title = \"A Testset for Context-Aware {LLM} Translation in {K}orean-to-{E}nglish Discourse Level Translation\",\n    author = \"Lee, Minjae  and\n      Noh, Youngbin  and\n      Lee, Seung Jin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.110/\",\n    pages = \"1632--1646\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.110.pdf",
        "site": "https://aclanthology.org/2025.coling-main.110/",
        "pdf_size": 3833573,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Lfmj5Dro54IJ:scholar.google.com/&scioq=A+Testset+for+Context-Aware+LLM+Translation+in+Korean-to-English+Discourse+Level+Translation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Korea University; NCSOFT; NCSOFT",
        "aff_domain": "korea.ac.kr;ncsoft.com;ncsoft.com",
        "email": "korea.ac.kr;ncsoft.com;ncsoft.com",
        "github": "https://github.com/minseye/korean-english-context-aware-translation-dataset",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Korea University;NCSOFT Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.korea.ac.kr;https://www.ncsoft.com",
        "aff_unique_abbr": "KU;NCSOFT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.486",
        "title": "A Text Embedding Model with Contrastive Example Mining for Point-of-Interest Geocoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Geocoding is a fundamental technique that links location mentions to their geographic positions, which is important for understanding texts in terms of where the described events occurred. Unlike most geocoding studies that targeted coarse-grained locations, we focus on geocoding at a fine-grained point-of-interest (POI) level. To address the challenge of finding appropriate geo-database entries from among many candidates with similar POI names, we develop a text embedding-based geocoding model and investigate (1) entry encoding representations and (2) hard negative mining approaches suitable for enhancing the model\u2019s disambiguation ability. Our experiments show that the second factor significantly impact the geocoding accuracy of the model.",
        "author": "Hibiki Nakatani; Hiroki Teranishi; Shohei Higashiyama; Yuya Sawada; Hiroki Ouchi; Taro Watanabe",
        "authorids": "/h/hibiki-nakatani/; /h/hiroki-teranishi/; /s/shohei-higashiyama/; /y/yuya-sawada/; /h/hiroki-ouchi/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{nakatani-etal-2025-text,\n    title = \"A Text Embedding Model with Contrastive Example Mining for Point-of-Interest Geocoding\",\n    author = \"Nakatani, Hibiki  and\n      Teranishi, Hiroki  and\n      Higashiyama, Shohei  and\n      Sawada, Yuya  and\n      Ouchi, Hiroki  and\n      Watanabe, Taro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.486/\",\n    pages = \"7279--7291\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.486.pdf",
        "site": "https://aclanthology.org/2025.coling-main.486/",
        "pdf_size": 813160,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:otwT5GVzt0cJ:scholar.google.com/&scioq=A+Text+Embedding+Model+with+Contrastive+Example+Mining+for+Point-of-Interest+Geocoding&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Nara Institute of Science and Technology+RIKEN; RIKEN; National Institute of Information and Communications Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology+RIKEN; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;riken.jp;nict.go.jp;is.naist.jp;is.naist.jp;is.naist.jp",
        "email": "is.naist.jp;riken.jp;nict.go.jp;is.naist.jp;is.naist.jp;is.naist.jp",
        "github": "https://github.com/naist-nlp/poi-geocoding",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;2;0;0+1;0",
        "aff_unique_norm": "Nara Institute of Science and Technology;RIKEN;National Institute of Information and Communications Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nist.go.jp;https://www.riken.jp;https://www.nict.go.jp/",
        "aff_unique_abbr": "NIST;RIKEN;NICT",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.271",
        "title": "ACE-M3: Automatic Capability Evaluator for Multimodal Medical Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As multimodal large language models (MLLMs) gain prominence in the medical field, the need for precise evaluation methods to assess their effectiveness has become critical. While benchmarks provide a reliable means to evaluate the capabilities of MLLMs, traditional metrics like ROUGE and BLEU employed for open domain evaluation only focus on token overlap and may not align with human judgment. While human evaluation is more reliable, it is labor-intensive, costly, and not scalable. LLM-based evaluation methods have proven promising, but to date, there is still an urgent need for open-source multimodal LLM-based evaluators in the medical field. To address this issue, we introduce ACE-M3, an open-sourced Automatic Capability Evaluator for Multimodal Medical Models that specifically designed to assess the question answering abilities of medical MLLMs. It first utilizes a branch-merge architecture to provide both detailed analysis and a concise final score based on standard medical evaluation criteria. Subsequently, a reward token-based direct preference optimization (RTDPO) strategy is incorporated to save training time without compromising performance of our model. Extensive experiments have demonstrated the effectiveness of our ACE-M3 model in evaluating the capabilities of medical MLLMs.",
        "author": "Xiechi Zhang; Shunfan Zheng; Linlin Wang; Gerard de Melo; Zhu Cao; Xiaoling Wang; Liang He",
        "authorids": "/x/xiechi-zhang/; /s/shunfan-zheng/; /l/linlin-wang/; /g/gerard-de-melo/; /z/zhu-cao/; /x/xiaoling-wang/; /l/liang-he/",
        "bibtex": "@inproceedings{zhang-etal-2025-ace,\n    title = \"{ACE}-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models\",\n    author = \"Zhang, Xiechi  and\n      Zheng, Shunfan  and\n      Wang, Linlin  and\n      de Melo, Gerard  and\n      Cao, Zhu  and\n      Wang, Xiaoling  and\n      He, Liang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.271/\",\n    pages = \"4030--4054\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.271.pdf",
        "site": "https://aclanthology.org/2025.coling-main.271/",
        "pdf_size": 4944671,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:B9yJARahxqcJ:scholar.google.com/&scioq=ACE-M3:+Automatic+Capability+Evaluator+for+Multimodal+Medical+Models&hl=en&as_sdt=0,31",
        "gs_version_total": 3,
        "aff": "East China Normal University; East China Normal University; East China Normal University+Hasso Plattner Institute/University of Potsdam; Hasso Plattner Institute/University of Potsdam; Tongji University; East China Normal University; East China Normal University",
        "aff_domain": "cs.ecnu.edu.cn; ;cs.ecnu.edu.cn; ; ;cs.ecnu.edu.cn; ",
        "email": "cs.ecnu.edu.cn; ;cs.ecnu.edu.cn; ; ;cs.ecnu.edu.cn; ",
        "github": "",
        "project": "https://huggingface.co/collections/AIUSRTMP/ace-m3-67593297ff391b93e3e5d068",
        "author_num": 7,
        "aff_unique_index": "0;0;0+1;1;2;0;0",
        "aff_unique_norm": "East China Normal University;Hasso Plattner Institute;Tongji University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.hpi.de;https://www.tongji.edu.cn",
        "aff_unique_abbr": "ECNU;HPI;Tongji",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;1;0;0;0",
        "aff_country_unique": "China;Germany"
    },
    {
        "id": "2025.coling-main.327",
        "title": "ACL-rlg: A Dataset for Reading List Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Familiarizing oneself with a new scientific field and its existing literature can be daunting due to the large amount of available articles. Curated lists of academic references, or reading lists, compiled by experts, offer a structured way to gain a comprehensive overview of a domain or a specific scientific challenge. In this work, we introduce ACL-rlg, the largest open expert-annotated reading list dataset. We also provide multiple baselines for evaluating reading list generation and formally define it as a retrieval task. Our qualitative study highlights that traditional scholarly search engines and indexing methods perform poorly on this task, and GPT-4o, despite showing better results, exhibits signs of potential data contamination.",
        "author": "Julien Aubert-B\u00e9duchaud; Florian Boudin; B\u00e9atrice Daille; Richard Dufour",
        "authorids": "/j/julien-aubert-beduchaud/; /f/florian-boudin/; /b/beatrice-daille/; /r/richard-dufour/",
        "bibtex": "@inproceedings{aubert-beduchaud-etal-2025-acl,\n    title = \"{ACL}-rlg: A Dataset for Reading List Generation\",\n    author = \"Aubert-B{\\'e}duchaud, Julien  and\n      Boudin, Florian  and\n      Daille, B{\\'e}atrice  and\n      Dufour, Richard\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.327/\",\n    pages = \"4910--4919\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.327.pdf",
        "site": "https://aclanthology.org/2025.coling-main.327/",
        "pdf_size": 881386,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_0EKQwtuJccJ:scholar.google.com/&scioq=ACL-rlg:+A+Dataset+for+Reading+List+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France + JFLI, Tokyo, Japan; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France",
        "aff_domain": "univ-nantes.fr; ; ; ",
        "email": "univ-nantes.fr; ; ; ",
        "github": "github.com/jjbes/aclanthology-tutorial-reading-lists",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Nantes Universit\u00e9;JFLI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.univ-nantes.fr;",
        "aff_unique_abbr": "Nantes Universit\u00e9;",
        "aff_campus_unique_index": "0;0+1;0;0",
        "aff_campus_unique": "Nantes;Tokyo",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "France;Japan"
    },
    {
        "id": "2025.coling-main.392",
        "title": "ADAPTIVE IE: Investigating the Complementarity of Human-AI Collaboration to Adaptively Extract Information on-the-fly",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Information extraction (IE) needs vary over time, where a flexible information extraction (IE) system can be useful. Despite this, existing IE systems are either fully supervised, requiring expensive human annotations, or fully unsupervised, extracting information that often do not cater to user\u2019s needs. To address these issues, we formally introduce the task of \u201cIE on-the-fly\u201d, and address the problem using our proposed Adaptive IE framework that uses human-in-the-loop refinement to adapt to changing user questions. Through human experiments on three diverse datasets, we demonstrate that Adaptive IE is a domain-agnostic, responsive, efficient framework for helping users access useful information while quickly reorganizing information in response to evolving information needs.",
        "author": "Ishani Mondal; Michelle Yuan; Anandhavelu N; Aparna Garimella; Francis Ferraro; Andrew Blair-Stanek; Benjamin Van Durme; Jordan Boyd-Graber",
        "authorids": "/i/ishani-mondal/; /m/michelle-yuan/; /a/anandhavelu-n/; /a/aparna-garimella/; /f/francis-ferraro/; /a/andrew-blair-stanek/; /b/benjamin-van-durme/; /j/jordan-boyd-graber/",
        "bibtex": "@inproceedings{mondal-etal-2025-adaptive,\n    title = \"{ADAPTIVE} {IE}: Investigating the Complementarity of Human-{AI} Collaboration to Adaptively Extract Information on-the-fly\",\n    author = \"Mondal, Ishani  and\n      Yuan, Michelle  and\n      N, Anandhavelu  and\n      Garimella, Aparna  and\n      Ferraro, Francis  and\n      Blair-Stanek, Andrew  and\n      Van Durme, Benjamin  and\n      Boyd-Graber, Jordan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.392/\",\n    pages = \"5870--5889\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.392.pdf",
        "site": "https://aclanthology.org/2025.coling-main.392/",
        "pdf_size": 1674086,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3978259522668980961&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Maryland, College Park; Amazon, NY; Aqxle.ai; Adobe Research; University of Maryland, Baltimore County; University of Maryland, Baltimore County; John Hopkins University; University of Maryland, College Park",
        "aff_domain": "umd.edu; ; ; ; ; ; ;",
        "email": "umd.edu; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;3;4;4;5;0",
        "aff_unique_norm": "University of Maryland;Amazon;Aqxle.ai;Adobe;University of Maryland, Baltimore County;Johns Hopkins University",
        "aff_unique_dep": ";;;Adobe Research;;",
        "aff_unique_url": "https://www/umd.edu;https://www.amazon.com;;https://research.adobe.com;https://www.umbc.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UMD;Amazon;;Adobe;UMBC;JHU",
        "aff_campus_unique_index": "0;1;3;3;0",
        "aff_campus_unique": "College Park;New York;;Baltimore County",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2025.coling-main.56",
        "title": "AGCL: Aspect Graph Construction and Learning for Aspect-level Sentiment Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prior studies on Aspect-level Sentiment Classification (ALSC) emphasize modeling interrelationships among aspects and contexts but overlook the crucial role of aspects themselves as essential domain knowledge. To this end, we propose AGCL, a novel Aspect Graph Construction and Learning method, aimed at furnishing the model with finely tuned aspect information to bolster its task-understanding ability. AGCL\u2019s pivotal innovations reside in Aspect Graph Construction (AGC) and Aspect Graph Learning (AGL), where AGC harnesses intrinsic aspect connections to construct the domain aspect graph, and then AGL iteratively updates the introduced aspect graph to enhance its domain expertise, making it more suitable for the ALSC task. Hence, this domain aspect graph can serve as a bridge connecting unseen aspects with seen aspects, thereby enhancing the model\u2019s generalization capability. Experiment results on three widely used datasets demonstrate the significance of aspect information for ALSC and highlight AGL\u2019s superiority in aspect learning, surpassing state-of-the-art baselines greatly. Code is available at https://github.com/jian-projects/agcl.",
        "author": "Zhongquan Jian; Daihang Wu; Shaopan Wang; Yancheng Wang; Junfeng Yao; Meihong Wang; Qingqiang Wu",
        "authorids": "/z/zhongquan-jian/; /d/daihang-wu/; /s/shaopan-wang/; /y/yancheng-wang/; /j/junfeng-yao/; /m/meihong-wang/; /q/qingqiang-wu/",
        "bibtex": "@inproceedings{jian-etal-2025-agcl,\n    title = \"{AGCL}: Aspect Graph Construction and Learning for Aspect-level Sentiment Classification\",\n    author = \"Jian, Zhongquan  and\n      Wu, Daihang  and\n      Wang, Shaopan  and\n      Wang, Yancheng  and\n      Yao, Junfeng  and\n      Wang, Meihong  and\n      Wu, Qingqiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.56/\",\n    pages = \"841--854\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.56.pdf",
        "site": "https://aclanthology.org/2025.coling-main.56/",
        "pdf_size": 2507261,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:RL4C0C25JA0J:scholar.google.com/&scioq=AGCL:+Aspect+Graph+Construction+and+Learning+for+Aspect-level+Sentiment+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/jian-projects/agcl",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.228",
        "title": "AHVE-CNER: Aligned Hanzi Visual Encoding Enhance Chinese Named Entity Recognition with Multi-Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The integration of multi-modal information, especially the graphic features of Hanzi, is crucial for improving the performance of Chinese Named Entity Recognition (NER) tasks. However, existing glyph-based models frequently neglect the relationship between pictorial elements and radicals. This paper presents AHVE-CNER, a model that integrates multi-source visual and phonetic information of Hanzi, while explicitly aligning pictographic features with their corresponding radicals. We propose the Gated Pangu-\ud835\udf0b Cross Transformer to effectively facilitate the integration of these multi-modal representations. By leveraging a multi-source glyph alignment strategy, AHVE-CNER demonstrates an improved capability to capture the visual and semantic nuances of Hanzi for NER tasks. Extensive experiments on benchmark datasets validate that AHVE-CNER achieves superior performance compared to existing multi-modal Chinese NER methods. Additional ablation studies further confirm the effectiveness of our visual alignment module and the fusion approach.",
        "author": "Xuhui Zheng; Zhiyuan Min; Bin Shi; Hao Wang",
        "authorids": "/x/xuhui-zheng/; /z/zhiyuan-min/; /b/bin-shi/; /h/hao-wang/",
        "bibtex": "@inproceedings{zheng-etal-2025-ahve,\n    title = \"{AHVE}-{CNER}: Aligned Hanzi Visual Encoding Enhance {C}hinese Named Entity Recognition with Multi-Information\",\n    author = \"Zheng, Xuhui  and\n      Min, Zhiyuan  and\n      Shi, Bin  and\n      Wang, Hao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.228/\",\n    pages = \"3391--3400\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.228.pdf",
        "site": "https://aclanthology.org/2025.coling-main.228/",
        "pdf_size": 1211666,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1EVKomMml3EJ:scholar.google.com/&scioq=AHVE-CNER:+Aligned+Hanzi+Visual+Encoding+Enhance+Chinese+Named+Entity+Recognition+with+Multi-Information&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Management, Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China; School of Software Technol, Zhejiang University, China; School of Information Management, Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China; School of Information Management, Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China",
        "aff_domain": "smail.nju.edu.cn;zju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;zju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/zxh20001117/AHVE-CNER",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;1;0+0;0+0",
        "aff_unique_norm": "Nanjing University;Zhejiang University",
        "aff_unique_dep": "School of Information Management;School of Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn;http://www.zju.edu.cn",
        "aff_unique_abbr": "Nanjing U;ZJU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.680",
        "title": "AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Artificial intelligence has significantly revolutionized healthcare, particularly through large language models (LLMs) that demonstrate superior performance in static medical question answering benchmarks. However, evaluating the potential of LLMs for real-world clinical applications remains challenging due to the intricate nature of doctor-patient interactions. To address this, we introduce AI Hospital, a multi-agent framework emulating dynamic medical interactions between Doctor as player and NPCs including Patient and Examiner. This setup allows for more practical assessments of LLMs in simulated clinical scenarios. We develop the Multi-View Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical records and multiple evaluation strategies to quantify the performance of LLM-driven Doctor agents on symptom collection, examination recommendations, and diagnoses. Additionally, a dispute resolution collaborative mechanism is proposed to enhance medical interaction capabilities through iterative discussions. Despite improvements, current LLMs (including GPT-4) still exhibit significant performance gaps in multi-turn interactive scenarios compared to non-interactive scenarios. Our findings highlight the need for further research to bridge these gaps and improve LLMs\u2019 clinical decision-making capabilities. Our data, code, and experimental results are all open-sourced at https://github.com/LibertFan/AI_Hospital.",
        "author": "Zhihao Fan; Lai Wei; Jialong Tang; Wei Chen; Wang Siyuan; Zhongyu Wei; Fei Huang",
        "authorids": "/z/zhihao-fan/; /l/lai-wei/; /j/jialong-tang/; /w/wei-chen/; /w/wang-siyuan/; /z/zhongyu-wei/; /f/fei-huang/",
        "bibtex": "@inproceedings{fan-etal-2025-ai,\n    title = \"{AI} Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator\",\n    author = \"Fan, Zhihao  and\n      Wei, Lai  and\n      Tang, Jialong  and\n      Chen, Wei  and\n      Siyuan, Wang  and\n      Wei, Zhongyu  and\n      Huang, Fei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.680/\",\n    pages = \"10183--10213\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.680.pdf",
        "site": "https://aclanthology.org/2025.coling-main.680/",
        "pdf_size": 6408747,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2272448844978672757&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/LibertFan/AI_Hospital",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-demos.8",
        "title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We introduce AI-Press, an automated news drafting and polishing system based on multi-agent collaboration and Retrieval-Augmented Generation. We develop a feedback simulation system that generates public responses considering demographic distributions. Demo link: https://youtu.be/TmjfJrbzaRU",
        "author": "Xiawei Liu; Shiyue Yang; Xinnong Zhang; Haoyu Kuang; Libo Sun; Yihang Yang; Siming Chen; Xuanjing Huang; Zhongyu Wei",
        "authorids": "/x/xiawei-liu/; /s/shiyue-yang/; /x/xinnong-zhang/; /h/haoyu-kuang/; /l/libo-sun/; /y/yihang-yang/; /s/siming-chen/; /x/xuan-jing-huang/; /z/zhongyu-wei/",
        "bibtex": "@inproceedings{liu-etal-2025-ai,\n    title = \"{AI}-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models\",\n    author = \"Liu, Xiawei  and\n      Yang, Shiyue  and\n      Zhang, Xinnong  and\n      Kuang, Haoyu  and\n      Sun, Libo  and\n      Yang, Yihang  and\n      Chen, Siming  and\n      Huang, Xuanjing  and\n      Wei, Zhongyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.8/\",\n    pages = \"63--82\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.8.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.8/",
        "pdf_size": 2430586,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3971949104547277475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Data Science, Fudan University, China + Research Institute of Intelligent Complex Systems, Fudan University, China",
        "aff_domain": "m.fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn; ; ; ;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn; ; ; ;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0+0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Data Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.625",
        "title": "AIDER: a Robust and Topic-Independent Framework for Detecting AI-Generated Text",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The human-level fluency achieved by large language models in text generation has intensified the challenge of distinguishing between human-written and AI-generated texts. While current fine-tuned detectors exist, they often lack robustness against adversarial attacks and struggle with out-of-distribution topics, limiting their practical applicability. This study introduces AIDER, a robust and topic-independent AI-generated text detection framework. AIDER leverages the ALBERT model for topic content disentanglement, enhancing transferability to unseen topics. It incorporates an augmentor that generates robust adversarial data for training, coupled with contrastive learning techniques to boost resilience. Comprehensive experiments demonstrate AIDER\u2019s significant superiority over state-of-the-art methods, exhibiting exceptional robustness against adversarial attacks with minimal performance degradation. AIDER consistently achieves high accuracy in non-augmented scenarios and demonstrates remarkable generalizability to unseen topics. These attributes establish AIDER as a powerful and versatile tool for LLM-generated text detection across diverse real-world applications, addressing critical challenges in the evolving landscape of AI-generated content.",
        "author": "Jiayi Gui; Baitong Cui; Xiaolian Guo; Ke Yu; Xiaofei Wu",
        "authorids": "/j/jiayi-gui/; /b/baitong-cui/; /x/xiaolian-guo/; /k/ke-yu/; /x/xiaofei-wu/",
        "bibtex": "@inproceedings{gui-etal-2025-aider,\n    title = \"{AIDER}: a Robust and Topic-Independent Framework for Detecting {AI}-Generated Text\",\n    author = \"Gui, Jiayi  and\n      Cui, Baitong  and\n      Guo, Xiaolian  and\n      Yu, Ke  and\n      Wu, Xiaofei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.625/\",\n    pages = \"9299--9310\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.625.pdf",
        "site": "https://aclanthology.org/2025.coling-main.625/",
        "pdf_size": 859953,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9321139534319126850&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn; ; ; ; ",
        "email": "bupt.edu.cn; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Artificial Intelligence",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.664",
        "title": "AIGT: AI Generative Table Based on Prompt",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Tabular data, which accounts for over 80% of enterprise data assets, is vital in various fields. With growing concerns about privacy protection and data-sharing restrictions, generating high-quality synthetic tabular data has become essential. Recent advancements show that large language models (LLMs) can effectively generate realistic tabular data by leveraging semantic information and overcoming the challenges of high-dimensional data that arise from one-hot encoding. However, current methods do not fully utilize the rich information available in tables. To address this, we introduce AI Generative Table based on prompt enhancement, a novel approach that utilizes metadata information, such as table descriptions and schemas, as prompts to generate ultra-high-quality synthetic data. To overcome the token limit constraints of LLMs, we propose long-token partitioning algorithms that enable AIGT to model tables of any scale. AIGT achieves state-of-the-art performance on 14 out of 20 public datasets and two real industry datasets within the Alipay risk control system.",
        "author": "Mingming Zhang; Zhiqing Xiao; Guoshan Lu; Sai Wu; Weiqiang Wang; Xing Fu; Can Yi; Junbo Zhao",
        "authorids": "/m/mingming-zhang/; /z/zhiqing-xiao/; /g/guoshan-lu/; /s/sai-wu/; /w/weiqiang-wang/; /x/xing-fu/; /c/can-yi/; /j/junbo-zhao/",
        "bibtex": "@inproceedings{zhang-etal-2025-aigt,\n    title = \"{AIGT}: {AI} Generative Table Based on Prompt\",\n    author = \"Zhang, Mingming  and\n      Xiao, Zhiqing  and\n      Lu, Guoshan  and\n      Wu, Sai  and\n      Wang, Weiqiang  and\n      Fu, Xing  and\n      Yi, Can  and\n      Zhao, Junbo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.664/\",\n    pages = \"9926--9938\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.664.pdf",
        "site": "https://aclanthology.org/2025.coling-main.664/",
        "pdf_size": 1750821,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16252199504575501028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Ant Group; Ant Group; Ant Group; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;antgroup.com;antgroup.com;antgroup.com;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;antgroup.com;antgroup.com;antgroup.com;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;1;1;0",
        "aff_unique_norm": "Zhejiang University;Ant Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.antgroup.com",
        "aff_unique_abbr": "ZJU;Ant Group",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.613",
        "title": "ALIS: Aligned LLM Instruction Security Strategy for Unsafe Input Prompt",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In large language models, existing instruction tuning methods may fail to balance the performance with robustness against attacks from user input like prompt injection and jailbreaking. Inspired by computer hardware and operating systems, we propose an instruction tuning paradigm named Aligned LLM Instruction Security Strategy (ALIS) to enhance model performance by decomposing user inputs into irreducible atomic instructions and organizing them into instruction streams which will guide the response generation of model. ALIS is a hierarchical structure, in which user inputs and system prompts are treated as user and kernel mode instructions respectively. Based on ALIS, the model can maintain security constraints by ignoring or rejecting the input instructions when user mode instructions attempt to conflict with kernel mode instructions. To build ALIS, we also develop an automatic instruction generation method for training ALIS, and give one instruction decomposition task and respective datasets. Notably, the ALIS framework with a small model to generate instruction streams still improve the resilience of LLM to attacks substantially without any lose on general capabilities.",
        "author": "Xinhao Song; Sufeng Duan; Gongshen Liu",
        "authorids": "/x/xinhao-song/; /s/sufeng-duan/; /g/gongshen-liu/",
        "bibtex": "@inproceedings{song-etal-2025-alis,\n    title = \"{ALIS}: Aligned {LLM} Instruction Security Strategy for Unsafe Input Prompt\",\n    author = \"Song, Xinhao  and\n      Duan, Sufeng  and\n      Liu, Gongshen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.613/\",\n    pages = \"9124--9146\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.613.pdf",
        "site": "https://aclanthology.org/2025.coling-main.613/",
        "pdf_size": 3307458,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14085177719547918004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Cyber Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Cyber Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Cyber Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "https://github.com/XinhaoS0101/Alis",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "School of Cyber Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.193",
        "title": "ALYMPICS: LLM Agents Meet Game Theory",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Game theory is a branch of mathematics that studies strategic interactions among rational agents. We propose Alympics (Olympics for Agents), a systematic framework utilizing Large Language Model (LLM) agents for empirical game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the \u201cWater Allocation Challenge\u201d, we explore Alympics through a challenging strategic game focused on the multi-round auction of scarce survival resources. This study demonstrates the framework\u2019s ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in rational strategic decision-making scenarios. Our findings highlight LLM agents\u2019 potential to advance game theory knowledge and expand the understanding of their proficiency in emulating human strategic behavior.",
        "author": "Shaoguang Mao; Yuzhe Cai; Yan Xia; Wenshan Wu; Xun Wang; Fengyi Wang; Qiang Guan; Tao Ge; Furu Wei",
        "authorids": "/s/shaoguang-mao/; /y/yuzhe-cai/; /y/yan-xia/; /w/wenshan-wu/; /x/xun-wang/; /f/fengyi-wang/; /q/qiang-guan/; /t/tao-ge/; /f/furu-wei/",
        "bibtex": "@inproceedings{mao-etal-2025-alympics,\n    title = \"{ALYMPICS}: {LLM} Agents Meet Game Theory\",\n    author = \"Mao, Shaoguang  and\n      Cai, Yuzhe  and\n      Xia, Yan  and\n      Wu, Wenshan  and\n      Wang, Xun  and\n      Wang, Fengyi  and\n      Guan, Qiang  and\n      Ge, Tao  and\n      Wei, Furu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.193/\",\n    pages = \"2845--2866\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.193.pdf",
        "site": "https://aclanthology.org/2025.coling-main.193/",
        "pdf_size": 3742900,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16483071806148349988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Microsoft Research Asia, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex System, Institute of Automation, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex System, Institute of Automation, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China",
        "aff_domain": "microsoft.com; ; ; ; ; ; ; ; ",
        "email": "microsoft.com; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "ALYMPICS",
        "author_num": 9,
        "aff_unique_index": "0;1+2;0;0;0;0;1+2;0;0",
        "aff_unique_norm": "Microsoft Research Asia;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Research;Institute of Automation;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "MSRA;CAS;UCAS",
        "aff_campus_unique_index": "0;0+0;0;0;0;0;0+0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.45",
        "title": "AMAN: Agent for Mentoring and Assisting Newbies in MMORPG",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In online games with diverse contents and frequent updates, newcomers first learn gameplay mechanics by community intelligence but soon face challenges that require real-time guidance from senior gamers. To provide easy access to such support, we introduce AMAN, Agent for Mentoring and Assisting Newbies in MMORPG (Massively Multiplayer Online Role-Playing Game) - a companion chatbot designed to engage novice gamers. Our model functions as a human-like chat buddy that interacts with users in a friendly manner while providing substantive informational depth. In this light, we propose a multi-stage learning approach that incorporates continual pre-training with a sequence of online resources and instruction tuning on curated dialogues. To align with gamers\u2019 specific needs, we first analyze user-oriented topics from online communities regarding a widely played MMORPG and construct a domain-specific dataset. Furthermore, we develop a multi-turn dialogue data to foster dynamic conversations with users. The evaluation result with the model trained upon publicly available language model shows our practical applicability on how conversational assistant in online games can help novice gamers.",
        "author": "Jeehyun Lee; Seung-Moo Yang; Won Ik Cho",
        "authorids": "/j/jeehyun-lee/; /s/seung-moo-yang/; /w/won-ik-cho/",
        "bibtex": "@inproceedings{lee-etal-2025-aman,\n    title = \"{AMAN}: Agent for Mentoring and Assisting Newbies in {MMORPG}\",\n    author = \"Lee, Jeehyun  and\n      Yang, Seung-Moo  and\n      Cho, Won Ik\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.45/\",\n    pages = \"522--532\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.45.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.45/",
        "pdf_size": 794019,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XqkRPiyGiLUJ:scholar.google.com/&scioq=AMAN:+Agent+for+Mentoring+and+Assisting+Newbies+in+MMORPG&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Sogang University; Seoul National University of Science & Technology; Seoul National University",
        "aff_domain": "sogang.ac.kr;ds.seoultech.ac.kr;snu.ac.kr",
        "email": "sogang.ac.kr;ds.seoultech.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Sogang University;Seoul National University of Science and Technology;Seoul National University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sogang.ac.kr;https://www.snust.ac.kr;https://www.snu.ac.kr",
        "aff_unique_abbr": "Sogang;SNUST;SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.433",
        "title": "Acquired TASTE: Multimodal Stance Detection with Textual and Structural Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Stance detection plays a pivotal role in enabling an extensive range of downstream applications, from discourse parsing to tracing the spread of fake news and the denial of scientific facts. While most stance classification models rely on the textual representation of the utterance in question, prior work has demonstrated the importance of the conversational context in stance detection. In this work, we introduce TASTE \u2013 a multimodal architecture for stance detection that harmoniously fuses Transformer-based content embedding with unsupervised structural embedding. Through the fine-tuning of a pre-trained transformer and the amalgamation with social embedding via a Gated Residual Network (GRN) layer, our model adeptly captures the complex interplay between content and conversational structure in determining stance. TASTE achieves state-of-the-art results on common benchmarks, significantly outperforming an array of strong baselines. Comparative evaluations underscore the benefits of social grounding \u2013 emphasizing the criticality of concurrently harnessing both content and structure for enhanced stance detection.",
        "author": "Guy Barel; Oren Tsur; Dan Vilenchik",
        "authorids": "/g/guy-barel/; /o/oren-tsur/; /d/dan-vilenchik/",
        "bibtex": "@inproceedings{barel-etal-2025-acquired,\n    title = \"Acquired {TASTE}: Multimodal Stance Detection with Textual and Structural Embeddings\",\n    author = \"Barel, Guy  and\n      Tsur, Oren  and\n      Vilenchik, Dan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.433/\",\n    pages = \"6492--6504\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.433.pdf",
        "site": "https://aclanthology.org/2025.coling-main.433/",
        "pdf_size": 1390132,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18008702901619725639&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 3,
        "aff": "Ben Gurion University; Ben Gurion University; Ben Gurion University",
        "aff_domain": "post.bgu.ac.il;bgu.ac.il;bgu.ac.il",
        "email": "post.bgu.ac.il;bgu.ac.il;bgu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ben-Gurion University of the Negev",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bgu.ac.il",
        "aff_unique_abbr": "BGU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2025.coling-main.116",
        "title": "Acquiring Bidirectionality via Large and Small Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Using token representation from bidirectional language models (LMs) such as BERT is still a widely used approach for token-classification tasks. Even though there exist much larger unidirectional LMs such as Llama-2, they are rarely used to replace the token representation of bidirectional LMs. In this work, we hypothesize that their lack of bidirectionality is what is keeping unidirectional LMs behind. To that end, we propose to newly train a small backward LM and concatenate its representations to those of an existing LM for downstream tasks. Through experiments in token-classification tasks, we demonstrate that introducing backward model can improve the benchmark performance by more than 10 points. Furthermore, we show that the proposed method is especially effective for rare domains and in few-shot learning settings.",
        "author": "Takumi Goto; Hiroyoshi Nagao; Yuta Koreeda",
        "authorids": "/0/0009-0006-8124-899X/; /h/hiroyoshi-nagao/; /y/yuta-koreeda/",
        "bibtex": "@inproceedings{goto-etal-2025-acquiring,\n    title = \"Acquiring Bidirectionality via Large and Small Language Models\",\n    author = \"Goto, Takumi  and\n      Nagao, Hiroyoshi  and\n      Koreeda, Yuta\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.116/\",\n    pages = \"1711--1717\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.116.pdf",
        "site": "https://aclanthology.org/2025.coling-main.116/",
        "pdf_size": 304664,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:k_vLOIC2T40J:scholar.google.com/&scioq=Acquiring+Bidirectionality+via+Large+and+Small+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Research & Development Group, Hitachi, Ltd., Tokyo, Japan + NARA Institute of Science and Technology, Nara, Japan; Research & Development Group, Hitachi, Ltd., Tokyo, Japan; Research & Development Group, Hitachi, Ltd., Tokyo, Japan",
        "aff_domain": "hitachi.com; ; ",
        "email": "hitachi.com; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Hitachi, Ltd.;NARA Institute of Science and Technology",
        "aff_unique_dep": "Research & Development Group;",
        "aff_unique_url": "https://www.hitachi.com;https://www.nara-u.ac.jp",
        "aff_unique_abbr": "Hitachi;NIST",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Tokyo;Nara",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.515",
        "title": "AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational Query Reformulation (CQR) has significantly advanced in addressing the challenges of conversational search, particularly those stemming from the latent user intent and the need for historical context. Recent works aimed to boost the performance of CQR through alignment. However, they are designed for one specific retrieval system, which potentially results in sub-optimal generalization. To overcome this limitation, we present a novel framework AdaCQR. By aligning reformulation models with both term-based and semantic-based retrieval systems, AdaCQR enhances the generalizability of information-seeking queries among diverse retrieval environments through a two-stage training strategy. Moreover, two effective approaches are proposed to obtain superior labels and diverse input candidates, boosting the efficiency and robustness of the framework. Experimental results on the TopiOCQA, QReCC and TREC CAsT datasets demonstrate that AdaCQR outperforms the existing methods in a more efficient framework, offering both quantitative and qualitative improvements in conversational query reformulation.",
        "author": "Yilong Lai; Jialong Wu; Congzhi Zhang; Haowen Sun; Deyu Zhou",
        "authorids": "/y/yilong-lai/; /j/jialong-wu/; /c/congzhi-zhang/; /h/haowen-sun/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{lai-etal-2025-adacqr,\n    title = \"{A}da{CQR}: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment\",\n    author = \"Lai, Yilong  and\n      Wu, Jialong  and\n      Zhang, Congzhi  and\n      Sun, Haowen  and\n      Zhou, Deyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.515/\",\n    pages = \"7698--7720\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.515.pdf",
        "site": "https://aclanthology.org/2025.coling-main.515/",
        "pdf_size": 1669913,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15417760112281168320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/init0xyz/AdaCQR",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.40",
        "title": "Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) adapts large language models (LLMs) to specific domains by updating only a small portion of the parameters. Although fine-tuning on a single task within a specific domain has demonstrated promising results, there remains limited exploration on how to effectively integrate these adapters for optimal performance. In this paper, we propose Adapters Selector (AS): a novel framework for better integrating usage of multiple adapters by training a middleman adapter to select the appropriate adapter for inference. Our approach utilizes PEFT to train a selector that determines which input content corresponds to which task in which domain, and subsequently selects the homologous adapter. By the way, The AS has developed the capability to execute cross-domain multi-tasks effectively through the utilization of a compact model in combination with multiple LoRA modules. Our code is publicly available.",
        "author": "Yimin Tian; Bolin Zhang; Zhiying Tu; Dianhui Chu",
        "authorids": "/y/yimin-tian/; /b/bolin-zhang/; /z/zhiying-tu/; /d/dianhui-chu/",
        "bibtex": "@inproceedings{tian-etal-2025-adapters,\n    title = \"Adapters Selector: Cross-domains and Multi-tasks {L}o{RA} Modules Integration Usage Method\",\n    author = \"Tian, Yimin  and\n      Zhang, Bolin  and\n      Tu, Zhiying  and\n      Chu, Dianhui\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.40/\",\n    pages = \"593--605\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.40.pdf",
        "site": "https://aclanthology.org/2025.coling-main.40/",
        "pdf_size": 724092,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Z0xEvxyzQ50J:scholar.google.com/&scioq=Adapters+Selector:+Cross-domains+and+Multi-tasks+LoRA+Modules+Integration+Usage+Method&hl=en&as_sdt=0,31",
        "gs_version_total": 0,
        "aff": "Harbin Institute of Technology, Harbin, China+Harbin Institute of Technology, Weihai, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Weihai, China; Harbin Institute of Technology, Weihai, China",
        "aff_domain": "stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn",
        "email": "stu.hit.edu.cn;hit.edu.cn;hit.edu.cn;hit.edu.cn",
        "github": "https://github.com/tirant35/TASA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0+1;0;1;1",
        "aff_campus_unique": "Harbin;Weihai",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.477",
        "title": "Addressing the Training-Inference Discrepancy in Discrete Diffusion for Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study addresses the discrepancy between training and inference in discrete diffusion models for text generation. We propose two novel strategies: (1) a training schema that considers two-step diffusion processes, allowing the model to use its own predicted output as input for subsequent steps during training and (2) a scheduling technique that gradually increases the probability of using self-generated text as training progresses. Experiments conducted on four widely used text generation benchmark datasets demonstrate that both proposed strategies improve the performance of discrete diffusion models in text generation.",
        "author": "Masaki Asada; Makoto Miwa",
        "authorids": "/m/masaki-asada/; /m/makoto-miwa/",
        "bibtex": "@inproceedings{asada-miwa-2025-addressing,\n    title = \"Addressing the Training-Inference Discrepancy in Discrete Diffusion for Text Generation\",\n    author = \"Asada, Masaki  and\n      Miwa, Makoto\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.477/\",\n    pages = \"7156--7164\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.477.pdf",
        "site": "https://aclanthology.org/2025.coling-main.477/",
        "pdf_size": 430565,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VpXxIdXU5VsJ:scholar.google.com/&scioq=Addressing+the+Training-Inference+Discrepancy+in+Discrete+Diffusion+for+Text+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Japan; Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Japan + Toyota Technological Institute, Japan",
        "aff_domain": "aist.go.jp;toyota-ti.ac.jp",
        "email": "aist.go.jp;toyota-ti.ac.jp",
        "github": "https://github.com/aistairc/text-diff-2step-loss",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;Toyota Technological Institute",
        "aff_unique_dep": "Artificial Intelligence Research Center;",
        "aff_unique_url": "https://www.aist.go.jp;https://www.tti-jp.ac.jp",
        "aff_unique_abbr": "AIST;TTI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.27",
        "title": "AdminSet and AdminBERT: a Dataset and a Pre-trained Language Model to Explore the Unstructured Maze of French Administrative Documents",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, Pre-trained Language Models(PLMs) have been widely used to analyze various documents, playing a crucial role in Natural Language Processing (NLP). However, administrative texts have rarely been used in information extraction tasks, even though this resource is available as open data in many countries. Most of these texts contain many specific domain terms. Moreover, especially in France, they are unstructured because many administrations produce them without a standardized framework. Due to this fact, current language models do not process these documents correctly. In this paper, we propose AdminBERT, the first French pre-trained language models for the administrative domain. Since interesting information in such texts corresponds to named entities and the relations between them, we compare this PLM with general domain language models, fine-tuned on the Named Entity Recognition (NER) task applied to administrative texts, as well as to a Large Language Model (LLM) and to a language model with an architecture different from the BERT one. We show that taking advantage of a PLM for French administrative data increases the performance in the administrative and general domains, on these texts. We also release AdminBERT as well as AdminSet, the pre-training corpus of administrative texts in French and the subset AdminSet-NER, the first NER dataset consisting exclusively of administrative texts in French.",
        "author": "Thomas Sebbag; Solen Quiniou; Nicolas Stucky; Emmanuel Morin",
        "authorids": "/t/thomas-sebbag/; /s/solen-quiniou/; /n/nicolas-stucky/; /e/emmanuel-morin/",
        "bibtex": "@inproceedings{sebbag-etal-2025-adminset,\n    title = \"{A}dmin{S}et and {A}dmin{BERT}: a Dataset and a Pre-trained Language Model to Explore the Unstructured Maze of {F}rench Administrative Documents\",\n    author = \"Sebbag, Thomas  and\n      Quiniou, Solen  and\n      Stucky, Nicolas  and\n      Morin, Emmanuel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.27/\",\n    pages = \"392--406\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.27.pdf",
        "site": "https://aclanthology.org/2025.coling-main.27/",
        "pdf_size": 355573,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8Vi02tCZk2kJ:scholar.google.com/&scioq=AdminSet+and+AdminBERT:+a+Dataset+and+a+Pre-trained+Language+Model+to+Explore+the+Unstructured+Maze+of+French+Administrative+Documents&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France+Explore, Carquefou, France; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France",
        "aff_domain": "univ-nantes.fr;univ-nantes.fr;etu.univ-nantes.fr;univ-nantes.fr",
        "email": "univ-nantes.fr;univ-nantes.fr;etu.univ-nantes.fr;univ-nantes.fr",
        "github": "",
        "project": "https://huggingface.co/datasets/taln-ls2n/Adminset",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Nantes Universit\u00e9;Explore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.univ-nantes.fr;",
        "aff_unique_abbr": "Nantes Universit\u00e9;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Nantes;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.644",
        "title": "AgriCLIP: Adapting CLIP for Agriculture and Livestock via Domain-Specialized Cross-Model Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Capitalizing on a vast amount of image-text data, large-scale vision-language pre-training has demonstrated remarkable zero-shot capabilities and has been utilized in several applications. However, models trained on general everyday web-crawled data often exhibit sub-optimal performance for specialized domains, likely due to domain shift. Recent works have tackled this problem for some domains (e.g., healthcare) by constructing domain-specialized image-text data. However, constructing a dedicated large-scale image-text dataset for sustainable areas of agriculture and livestock is still open to research. Further, this domain desires fine-grained feature learning due to the subtle nature of the downstream tasks (e.g., nutrient deficiency detection and livestock breed classification). To address this, we present AgriCLIP, a vision-language foundational model dedicated to the domain of agriculture and livestock. First, we propose a large-scale dataset named ALive that leverages a customized prompt generation strategy to overcome the scarcity of expert annotations. Our ALive dataset covers crops, livestock, and fishery, with around 600,000 image-text pairs. Second, we propose a training pipeline that integrates both contrastive and self-supervised learning to learn both global semantic and local fine-grained domain-specialized features. Experiments on a diverse set of 20 downstream tasks demonstrate the effectiveness of the AgriCLIP framework, achieving an absolute gain of 9.07% in terms of average zero-shot classification accuracy over the standard CLIP adaptation via domain-specialized ALive dataset. Our ALive dataset and code can be accessible on Github.",
        "author": "Umair Nawaz; Awais Muhammad; Hanan Gani; Muzammal Naseer; Fahad Shahbaz Khan; Salman Khan; Rao Anwer",
        "authorids": "/u/umair-nawaz/; /a/awais-muhammad/; /h/hanan-gani/; /m/muzammal-naseer/; /f/fahad-shahbaz-khan/; /s/salman-khan/; /r/rao-anwer/",
        "bibtex": "@inproceedings{nawaz-etal-2025-agriclip,\n    title = \"{A}gri{CLIP}: Adapting {CLIP} for Agriculture and Livestock via Domain-Specialized Cross-Model Alignment\",\n    author = \"Nawaz, Umair  and\n      Muhammad, Awais  and\n      Gani, Hanan  and\n      Naseer, Muzammal  and\n      Khan, Fahad Shahbaz  and\n      Khan, Salman  and\n      Anwer, Rao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.644/\",\n    pages = \"9630--9639\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.644.pdf",
        "site": "https://aclanthology.org/2025.coling-main.644/",
        "pdf_size": 10158995,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1612136177151453816&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Mohamed bin Zayed University of AI; Mohamed bin Zayed University of AI; Mohamed bin Zayed University of AI; Khalifa University; Mohamed bin Zayed University of AI+Link\u00f6ping University; Mohamed bin Zayed University of AI; Mohamed bin Zayed University of AI",
        "aff_domain": "mbzuai.ac.ae; ; ; ; ; ; ",
        "email": "mbzuai.ac.ae; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0+2;0;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;Khalifa University;Link\u00f6ping University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://mbzuai.ac.ae;https://www.khalifa.edu;https://www.liu.se",
        "aff_unique_abbr": "MBZUAI;KU;LiU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+1;0;0",
        "aff_country_unique": "United Arab Emirates;Sweden"
    },
    {
        "id": "2025.coling-main.267",
        "title": "Aligning Complex Knowledge Graph Question Answering as Knowledge-Aware Constrained Code Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generating executable logical forms (LF) using Large Language Models (LLMs) in a few-shot setting for Knowledge Graph Question Answering (KGQA) is becoming popular. However, their performance is still limited due to very little exposure to the LF during pre-training of LLMs, resulting in many syntactically incorrect LF generation. If the LF generation task can be transformed to a more familiar task for the LLM, it can potentially reduce the syntax errors and elevate the generation quality. On the other hand, there exist specialized LLMs trained/fine-tuned on code in many programming languages. They can be leveraged to generate the LF as step-wise constrained code expression generation using modular functions in the LF. Based on this insight, we propose CodeAlignKGQA: a framework that aligns the LF generation as code generation that incorporates LF-specific constraints. We extract the question-specific subgraph information to enable Knowledge-Aware code generation. We additionally introduce a dynamic self-code-correction mechanism, to be applied as required. Our extensive experiments on Complex KGQA benchmarks such as KQA Pro demonstrate the effectiveness of our approach. CodeAlignKGQA surpasses all few-shot baselines on KQA Pro by 21%, achieving a new state-of-the-art.",
        "author": "Prerna Agarwal; Nishant Kumar; Srikanta Bedathur Jagannath",
        "authorids": "/p/prerna-agarwal/; /n/nishant-kumar/; /s/srikanta-bedathur-jagannath/",
        "bibtex": "@inproceedings{agarwal-etal-2025-aligning,\n    title = \"Aligning Complex Knowledge Graph Question Answering as Knowledge-Aware Constrained Code Generation\",\n    author = \"Agarwal, Prerna  and\n      Kumar, Nishant  and\n      Jagannath, Srikanta Bedathur\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.267/\",\n    pages = \"3952--3978\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.267.pdf",
        "site": "https://aclanthology.org/2025.coling-main.267/",
        "pdf_size": 1427545,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:QaRv-dO3DscJ:scholar.google.com/&scioq=Aligning+Complex+Knowledge+Graph+Question+Answering+as+Knowledge-Aware+Constrained+Code+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Indian Institute of Technology Delhi, India+IBM Research India; Indian Institute of Technology Delhi, India; Indian Institute of Technology Delhi, India",
        "aff_domain": "scai.iitd.ac.in;gmail.com;cse.iitd.ac.in",
        "email": "scai.iitd.ac.in;gmail.com;cse.iitd.ac.in",
        "github": "https://github.com/data-iitd/CodeAlignKGQA",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Indian Institute of Technology Delhi;IBM Research",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.iitdelhi.ac.in;https://www.ibm.com/research/in",
        "aff_unique_abbr": "IIT Delhi;IBM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Delhi;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.511",
        "title": "Aligning LLMs with Individual Preferences via Interaction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As large language models (LLMs) demonstrate increasingly advanced capabilities, aligning their behaviors with human values and preferences becomes crucial for their wide adoption. While previous research focuses on general alignment to principles such as helpfulness, harmlessness, and honesty, the need to account for individual and diverse preferences has been largely overlooked, potentially undermining customized human experiences. To address this gap, we train LLMs that can \u201cinteract to align\u201d, essentially cultivating the meta-skill of LLMs to implicitly infer the unspoken personalized preferences of the current user through multi-turn conversations, and then dynamically align their following behaviors and responses to these inferred preferences. Our approach involves establishing a diverse pool of 3,310 distinct user personas by initially creating seed examples, which are then expanded through iterative self-generation and filtering. Guided by distinct user personas, we leverage multi-LLM collaboration to develop a multi-turn preference dataset containing 3K+ multi-turn conversations in tree structures. Finally, we apply supervised fine-tuning and reinforcement learning to enhance LLMs using this dataset. For evaluation, we establish the ALOE (ALign with custOmized prEferences) benchmark, consisting of 100 carefully selected examples and well-designed metrics to measure the customized alignment performance during conversations. Experimental results demonstrate the effectiveness of our method in enabling dynamic, personalized alignment via interaction. The code and dataset will be made public.",
        "author": "Shujin Wu; Yi R. Fung; Cheng Qian; Jeonghwan Kim; Dilek Hakkani-Tur; Heng Ji",
        "authorids": "/s/shujin-wu/; /y/yi-r-fung/; /c/cheng-qian/; /j/jeonghwan-kim/; /d/dilek-hakkani-tur/; /h/heng-ji/",
        "bibtex": "@inproceedings{wu-etal-2025-aligning,\n    title = \"Aligning {LLM}s with Individual Preferences via Interaction\",\n    author = \"Wu, Shujin  and\n      Fung, Yi R.  and\n      Qian, Cheng  and\n      Kim, Jeonghwan  and\n      Hakkani-Tur, Dilek  and\n      Ji, Heng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.511/\",\n    pages = \"7648--7662\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.511.pdf",
        "site": "https://aclanthology.org/2025.coling-main.511/",
        "pdf_size": 1701383,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13385659927046013208&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign+University of Southern California; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign",
        "aff_domain": "usc.edu;illinois.edu;illinois.edu; ; ; ",
        "email": "usc.edu;illinois.edu;illinois.edu; ; ; ",
        "github": "https://github.com/ShujinWu-0814/ALOEtasks",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.usc.edu",
        "aff_unique_abbr": "UIUC;USC",
        "aff_campus_unique_index": "0+1;0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;Los Angeles",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.172",
        "title": "Aligning Large Language Models with Human Opinions through Persona Selection and Value\u2013Belief\u2013Norm Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Reasoning and predicting human opinions with large language models (LLMs) is essential yet challenging. Current methods employ role-playing with personae but face two major issues: LLMs are sensitive to even a single irrelevant persona, skewing predictions by up to 30%; and LLMs fail to reason strategically over personae. We propose Chain-of-Opinion (COO), a simple four-step solution modeling which and how to reason with personae, inspired by the Value\u2013Belief\u2013Norm (VBN) theory. COO differentiates between explicit personae (demographics and ideology) and implicit personae (historical opinions), involves: (1) filtering irrelevant attributes from explicit personae; (2) ranking implicit personae into a preferential list for selecting top-k; (3) applying novel VBN reasoning to extract user environmental and personal value, belief, and norm variables for accurate and reliable predictions; and (4) iterating VBN reasoning with progressively larger lists of implicit personae to handle potential persona insufficiency. COO efficiently achieves new state-of-the-art opinion prediction via prompting with only 5 inference calls, improving prior techniques by up to 4%. Notably, fine-tuning LMs with COO\u2019s data results in significantly better opinion-aligned models, by up to 23%.",
        "author": "Do Xuan Long; Kenji Kawaguchi; Min-Yen Kan; Nancy Chen",
        "authorids": "/x/xuan-long-do/; /k/kenji-kawaguchi/; /m/min-yen-kan/; /n/nancy-chen/",
        "bibtex": "@inproceedings{do-etal-2025-aligning,\n    title = \"Aligning Large Language Models with Human Opinions through Persona Selection and Value{--}Belief{--}Norm Reasoning\",\n    author = \"Long, Do Xuan  and\n      Kawaguchi, Kenji  and\n      Kan, Min-Yen  and\n      Chen, Nancy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.172/\",\n    pages = \"2526--2547\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.172.pdf",
        "site": "https://aclanthology.org/2025.coling-main.172/",
        "pdf_size": 1696457,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2582855600850801887&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "National University of Singapore; National University of Singapore; National University of Singapore; Institute for Infocomm Research (I2R), A*STAR",
        "aff_domain": "u.nus.edu;nus.edu.sg;nus.edu.sg;i2r.a-star.edu.sg",
        "email": "u.nus.edu;nus.edu.sg;nus.edu.sg;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "National University of Singapore;Institute for Infocomm Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "NUS;I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2025.coling-main.67",
        "title": "Aligning Retrieval with Reader Needs: Reader-Centered Passage Selection for Open-Domain Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Open-Domain Question Answering (ODQA) systems often struggle with the quality of retrieved passages, which may contain conflicting information and be misaligned with the reader\u2019s needs. Existing retrieval methods aim to gather relevant passages but often fail to prioritize consistent and useful information for the reader. In this paper, we introduce a novel Reader-Centered Passage Selection (R-CPS) method, which enhances the performance of the retrieve-then-read pipeline by re-ranking and clustering passages from the reader\u2019s perspective. Our method re-ranks passages based on the reader\u2019s prediction probability distribution and clusters passages according to the predicted answers, prioritizing more useful and relevant passages to the top and reducing inconsistent information. Experiments on ODQA datasets demonstrate the effectiveness of our approach in improving the quality of evidence passages under zero-shot settings.",
        "author": "Chunlei Xin; Shuheng Zhou; Xuanang Chen; Yaojie Lu; Huijia Zhu; Weiqiang Wang; Zhongyi Liu; Xianpei Han; Le Sun",
        "authorids": "/c/chunlei-xin/; /s/shuheng-zhou/; /x/xuanang-chen/; /y/yaojie-lu/; /h/huijia-zhu/; /w/weiqiang-wang/; /z/zhongyi-liu/; /x/xianpei-han/; /l/le-sun/",
        "bibtex": "@inproceedings{xin-etal-2025-aligning,\n    title = \"Aligning Retrieval with Reader Needs: Reader-Centered Passage Selection for Open-Domain Question Answering\",\n    author = \"Xin, Chunlei  and\n      Zhou, Shuheng  and\n      Chen, Xuanang  and\n      Lu, Yaojie  and\n      Zhu, Huijia  and\n      Wang, Weiqiang  and\n      Liu, Zhongyi  and\n      Han, Xianpei  and\n      Sun, Le\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.67/\",\n    pages = \"1000--1012\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.67.pdf",
        "site": "https://aclanthology.org/2025.coling-main.67/",
        "pdf_size": 798238,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4903726826153822001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Ant Group; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Ant Group; Ant Group; Ant Group; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iscas.ac.cn;antgroup.com;iscas.ac.cn;iscas.ac.cn;antgroup.com;antgroup.com;antgroup.com;iscas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;antgroup.com;iscas.ac.cn;iscas.ac.cn;antgroup.com;antgroup.com;antgroup.com;iscas.ac.cn;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;2;0;0;2;2;2;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Ant Group",
        "aff_unique_dep": "Institute of Software;;",
        "aff_unique_url": "https://www.cas.cn;http://www.ucas.ac.cn;https://www.antgroup.com",
        "aff_unique_abbr": "CAS;UCAS;Ant Group",
        "aff_campus_unique_index": "0+0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.252",
        "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Machine unlearning aims to efficiently eliminate the influence of specific training data, known as the forget set, from the model. However, existing unlearning methods for Large Language Models (LLMs) face a critical challenge: they rely solely on negative feedback to suppress responses related to the forget set, which often results in nonsensical or inconsistent outputs, diminishing model utility and posing potential privacy risks. To address this limitation, we propose a novel approach called Alternate Preference Optimization (AltPO), which combines negative feedback with in-domain positive feedback on the forget set. Additionally, we introduce new evaluation metrics to assess the quality of responses related to the forget set. Extensive experiments show that our approach not only enables effective unlearning but also avoids undesirable model behaviors while maintaining overall model performance.",
        "author": "Anmol Mekala; Vineeth Dorna; Shreya Dubey; Abhishek Lalwani; David Koleczek; Mukund Rungta; Sadid Hasan; Elita Lobo",
        "authorids": "/a/anmol-mekala/; /v/vineeth-dorna/; /s/shreya-dubey/; /a/abhishek-lalwani/; /d/david-koleczek/; /m/mukund-rungta/; /s/sadid-a-hasan/; /e/elita-lobo/",
        "bibtex": "@inproceedings{mekala-etal-2025-alternate,\n    title = \"Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models\",\n    author = \"Mekala, Anmol  and\n      Dorna, Vineeth  and\n      Dubey, Shreya  and\n      Lalwani, Abhishek  and\n      Koleczek, David  and\n      Rungta, Mukund  and\n      Hasan, Sadid  and\n      Lobo, Elita\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.252/\",\n    pages = \"3732--3752\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.252.pdf",
        "site": "https://aclanthology.org/2025.coling-main.252/",
        "pdf_size": 1525454,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2648449418425688902&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Massachusetts Amherst; University of Massachusetts Amherst; University of Massachusetts Amherst; Microsoft; Microsoft; Microsoft; Microsoft; University of Massachusetts Amherst",
        "aff_domain": "umass.edu;umass.edu;umass.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;umass.edu",
        "email": "umass.edu;umass.edu;umass.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;umass.edu",
        "github": "https://github.com/molereddy/Alternate-Preference-Optimization",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;1;1;1;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Microsoft Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umass.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UMass Amherst;Microsoft",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.26",
        "title": "Ambiguity-aware Multi-level Incongruity Fusion Network for Multi-Modal Sarcasm Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-modal sarcasm detection aims to identify whether a given image-text pair is sarcastic. The pivotal factor of the task lies in accurately capturing incongruities from different modalities. Although existing studies have achieved impressive success, they primarily committed to fusing the textual and visual information to establish cross-modal correlations, overlooking the significance of original unimodal incongruity information at the text-level and image-level. Furthermore, the utilized fusion strategies of cross-modal information neglected the effect of inherent ambiguity within text and image modalities on multimodal fusion. To overcome these limitations, we propose a novel Ambiguity-aware Multi-level Incongruity Fusion Network (AMIF) for multi-modal sarcasm detection. Our method involves a multi-level incongruity learning module to capture the incongruity information simultaneously at the text-level, image-level and cross-modal-level. Additionally, an ambiguity-based fusion module is developed to dynamically learn reasonable weights and interpretably aggregate incongruity features from different levels. Comprehensive experiments conducted on a publicly available dataset demonstrate the superiority of our proposed model over state-of-the-art methods.",
        "author": "Kuntao Li; Yifan Chen; Qiaofeng Wu; Weixing Mai; Fenghuan Li; Yun Xue",
        "authorids": "/k/kuntao-li/; /y/yifan-chen/; /q/qiaofeng-wu/; /w/weixing-mai/; /f/fenghuan-li/; /y/yun-xue/",
        "bibtex": "@inproceedings{li-etal-2025-ambiguity,\n    title = \"Ambiguity-aware Multi-level Incongruity Fusion Network for Multi-Modal Sarcasm Detection\",\n    author = \"Li, Kuntao  and\n      Chen, Yifan  and\n      Wu, Qiaofeng  and\n      Mai, Weixing  and\n      Li, Fenghuan  and\n      Xue, Yun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.26/\",\n    pages = \"380--391\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.26.pdf",
        "site": "https://aclanthology.org/2025.coling-main.26/",
        "pdf_size": 1131249,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:jL3dysGgSrsJ:scholar.google.com/&scioq=Ambiguity-aware+Multi-level+Incongruity+Fusion+Network+for+Multi-Modal+Sarcasm+Detection&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University; School of Computer Science and Technology, Guangdong University of Technology; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University",
        "aff_domain": "m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;gdut.edu.cn;m.scnu.edu.cn",
        "email": "m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;gdut.edu.cn;m.scnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "South China Normal University;Guangdong University of Technology",
        "aff_unique_dep": "School of Electronic Science and Engineering;School of Computer Science and Technology",
        "aff_unique_url": "http://www.scnu.edu.cn;http://www.gdut.edu.cn",
        "aff_unique_abbr": "SCNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.362",
        "title": "An Active Learning Framework for Inclusive Generation by Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Ensuring that Large Language Models (LLMs) generate text representative of diverse sub-populations is essential, particularly when key concepts related to under-represented groups are scarce in the training data. We address this challenge with a novel clustering-based active learning framework, enhanced with knowledge distillation. The proposed framework transforms the intermediate outputs of the learner model, enabling effective active learning for generative tasks for the first time. Integration of clustering and knowledge distillation yields more representative models without prior knowledge of underlying data distribution and overbearing human efforts. We validate our approach in practice through case studies in counter-narration and style transfer. We construct two new datasets in tandem with model training, showing a performance improvement of 2%\u201310% over baseline models. Our results also show more consistent performance across various data subgroups and increased lexical diversity, underscoring our model\u2019s resilience to skewness in available data. Further, our results show that the data acquired via our approach improves the performance of secondary models not involved in the learning loop, showcasing practical utility of the framework.",
        "author": "Sabit Hassan; Anthony B. Sicilia; Malihe Alikhani",
        "authorids": "/s/sabit-hassan/; /a/anthony-b-sicilia/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{hassan-etal-2025-active,\n    title = \"An Active Learning Framework for Inclusive Generation by Large Language Models\",\n    author = \"Hassan, Sabit  and\n      Sicilia, Anthony B.  and\n      Alikhani, Malihe\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.362/\",\n    pages = \"5403--5414\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.362.pdf",
        "site": "https://aclanthology.org/2025.coling-main.362/",
        "pdf_size": 431699,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1833989376671805393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; Khoury College of Computer Science, Northeastern University, Boston, MA, USA; Khoury College of Computer Science, Northeastern University, Boston, MA, USA",
        "aff_domain": "pitt.edu;northeastern.edu;northeastern.edu",
        "email": "pitt.edu;northeastern.edu;northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Pittsburgh;Northeastern University",
        "aff_unique_dep": "School of Computing and Information;Khoury College of Computer Science",
        "aff_unique_url": "https://www.pitt.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "Pitt;NU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Pittsburgh;Boston",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-industry.52",
        "title": "An Automatic Method to Estimate Correctness of RAG",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In sectors in where data quality is critical, like finance and healthcare, it is crucial to have confidence in not only the outputs generated by retrieval-augmented generation (RAG) models but also the process followed by the model while arriving at the output. Existing methods, such as hallucination detection and input-output entailment measurements, fail to capture the model\u2019s internal state during answer generation. This paper introduces a novel approach to predict the correctness of the generated answer by modeling the model\u2019s uncertainty on quantified perturbations of input. Extensive experiments across multiple large language models (LLMs) demonstrate that our approach quantifies RAG robustness by aligning predictions with ground truth with a Avg.Mean Square Error (MSE) 0.002 while offering flexibility for diverse qualitative metrics.",
        "author": "Chi Zhang; Vivek V. Datla; Aditya Shrivastava; Alfy Samuel; Zhiqi Huang; Anoop Kumar; Daben Liu",
        "authorids": "/c/chi-zhang/; /v/vivek-v-datla/; /a/aditya-shrivastava/; /a/alfy-samuel/; /z/zhiqi-huang/; /a/anoop-kumar/; /d/daben-liu/",
        "bibtex": "@inproceedings{zhang-etal-2025-automatic,\n    title = \"An Automatic Method to Estimate Correctness of {RAG}\",\n    author = \"Zhang, Chi  and\n      Datla, Vivek V.  and\n      Shrivastava, Aditya  and\n      Samuel, Alfy  and\n      Huang, Zhiqi  and\n      Kumar, Anoop  and\n      Liu, Daben\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.52/\",\n    pages = \"603--611\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.52.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.52/",
        "pdf_size": 606228,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14321428320043379566&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Carnegie Mellon University; Capital One + Capital One; Capital One; Capital One; Capital One; Capital One; Capital One",
        "aff_domain": "andrew.cmu.edu;capitalone.com;capitalone.com;capitalone.com;capitalone.com;capitalone.com;capitalone.com",
        "email": "andrew.cmu.edu;capitalone.com;capitalone.com;capitalone.com;capitalone.com;capitalone.com;capitalone.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+1;1;1;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Capital One",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.capitalone.com",
        "aff_unique_abbr": "CMU;Capital One",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.490",
        "title": "An Efficient Dialogue Policy Agent with Model-Based Causal Reinforcement Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dialogue policy trains an agent to select dialogue actions frequently implemented via deep reinforcement learning (DRL). The model-based reinforcement methods built a world model to generate simulated data to alleviate the sample inefficiency. However, traditional world model methods merely consider one-step dialogues, leading to an inaccurate environmental simulation. Furthermore, different users may have different intention preferences, while most existing studies lack consideration of the intention-preferences causal relationship. This paper proposes a novel framework for dialogue policy learning named MCA, implemented through model-based reinforcement learning with automatically constructed causal chains. The MCA model utilizes an autoregressive Transformer to model dialogue trajectories, enabling a more accurate simulation of the environment. Additionally, it constructs a causal chains module that outputs latent preference distributions for intention-action pairs, thereby elucidating the relationship between user intentions and agent actions. The experimental results show that MCA can achieve state-of-the-art performances on three dialogue datasets over the compared dialogue agents, highlighting its effectiveness and robustness.",
        "author": "Kai Xu; Zhenyu Wang; Yangyang Zhao; Bopeng Fang",
        "authorids": "/k/kai-xu/; /z/zhenyu-wang/; /y/yangyang-zhao/; /b/bopeng-fang/",
        "bibtex": "@inproceedings{xu-etal-2025-efficient,\n    title = \"An Efficient Dialogue Policy Agent with Model-Based Causal Reinforcement Learning\",\n    author = \"Xu, Kai  and\n      Wang, Zhenyu  and\n      Zhao, Yangyang  and\n      Fang, Bopeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.490/\",\n    pages = \"7331--7343\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.490.pdf",
        "site": "https://aclanthology.org/2025.coling-main.490/",
        "pdf_size": 500665,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:l4jkg66qKFYJ:scholar.google.com/&scioq=An+Efficient+Dialogue+Policy+Agent+with+Model-Based+Causal+Reinforcement+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Software Engineering, South China University of Technology, Guangdong, China; School of Software Engineering, South China University of Technology, Guangdong, China; Department of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China",
        "aff_domain": "scut.edu.cn;scut.edu.cn;csust.edu.cn;uestc.edu.cn",
        "email": "scut.edu.cn;scut.edu.cn;csust.edu.cn;uestc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "South China University of Technology;Changsha University of Science and Technology;University of Electronic Science and Technology of China",
        "aff_unique_dep": "School of Software Engineering;Department of Computer and Communication Engineering;School of Computer Science and Engineering",
        "aff_unique_url": "https://www.scut.edu.cn;;http://www.uestc.edu.cn",
        "aff_unique_abbr": "SCUT;;UESTC",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Guangdong;Changsha;Chengdu",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.663",
        "title": "An Efficient Retrieval-Based Method for Tabular Prediction with LLM",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Tabular prediction, a well-established problem in machine learning, has consistently garnered significant research attention within academia and industry. Recently, with the rapid development of large language models (LLMs), there has been increasing exploration of how to apply LLMs to tabular prediction tasks. Many existing methods, however, typically rely on extensive pre-training or fine-tuning of LLMs, which demands considerable computational resources. To avoid this, we propose a retrieval-based approach that utilizes the powerful capabilities of LLMs in representation, comprehension, and inference. Our approach eliminates the need for training any modules or performing data augmentation, depending solely on information from target dataset. Experimental results reveal that, even without specialized training for tabular data, our method exhibits strong predictive performance on tabular prediction task, affirming its practicality and effectiveness.",
        "author": "Jie Wu; Mengshu Hou",
        "authorids": "/j/jie-wu/; /m/mengshu-hou/",
        "bibtex": "@inproceedings{wu-hou-2025-efficient,\n    title = \"An Efficient Retrieval-Based Method for Tabular Prediction with {LLM}\",\n    author = \"Wu, Jie  and\n      Hou, Mengshu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.663/\",\n    pages = \"9917--9925\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.663.pdf",
        "site": "https://aclanthology.org/2025.coling-main.663/",
        "pdf_size": 896777,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10079118313308573645&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China",
        "aff_domain": "std.uestc.edu.cn;uestc.edu.cn",
        "email": "std.uestc.edu.cn;uestc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Electronic Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.uestc.edu.cn",
        "aff_unique_abbr": "UESTC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chengdu",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.714",
        "title": "An LLM-based Framework for Biomedical Terminology Normalization in Social Media via Multi-Agent Collaboration",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Biomedical Terminology Normalization aims to identify the standard term in a specified termbase for non-standardized mentions from social media or clinical texts, employing the mainstream \u201cRecall and Re-rank\u201d framework. Instead of the traditional pretraining-finetuning paradigm, we would like to explore the possibility of accomplishing this task through a tuning-free paradigm using powerful Large Language Models (LLMs), hoping to address the costs of re-training due to discrepancies of both standard termbases and annotation protocols. Another major obstacle in this task is that both mentions and terms are short texts. Short texts contain an insufficient amount of information that can introduce ambiguity, especially in a biomedical context. Therefore, besides using the advanced embedding model, we implement a Retrieval-Augmented Generation (RAG) based knowledge card generation module. This module introduces an LLM agent that expands the short texts into accurate, harmonized, and more informative descriptions using a search engine and a domain knowledge base. Furthermore, we present an innovative tuning-free agent collaboration framework for the biomedical terminology normalization task in social media. By leveraging the internal knowledge and the reasoning capabilities of LLM, our framework conducts more sophisticated recall, ranking and re-ranking processes with the collaboration of different LLM agents. Experimental results across multiple datasets indicate that our approach exhibits competitive performance. We release our code and data on the github repository JOHNNY-fans/RankNorm.",
        "author": "Yongqi Fan; Kui Xue; Zelin Li; Xiaofan Zhang; Tong Ruan",
        "authorids": "/y/yongqi-fan/; /k/kui-xue/; /z/zelin-li/; /x/xiaofan-zhang/; /t/tong-ruan/",
        "bibtex": "@inproceedings{fan-etal-2025-llm,\n    title = \"An {LLM}-based Framework for Biomedical Terminology Normalization in Social Media via Multi-Agent Collaboration\",\n    author = \"Fan, Yongqi  and\n      Xue, Kui  and\n      Li, Zelin  and\n      Zhang, Xiaofan  and\n      Ruan, Tong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.714/\",\n    pages = \"10712--10726\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.714.pdf",
        "site": "https://aclanthology.org/2025.coling-main.714/",
        "pdf_size": 709737,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9787011947142135630&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China+Intelligent Healthcare, Shanghai Artificial Intelligence Laboratory, Shanghai, China; Intelligent Healthcare, Shanghai Artificial Intelligence Laboratory, Shanghai, China; Northwestern University, USA; Shanghai Jiao Tong University, Shanghai, China+Intelligent Healthcare, Shanghai Artificial Intelligence Laboratory, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China",
        "aff_domain": "mail.ecust.edu.cn;pjlab.org.cn;u.northwestern.edu;sjtu.edu.cn;ecust.edu.cn",
        "email": "mail.ecust.edu.cn;pjlab.org.cn;u.northwestern.edu;sjtu.edu.cn;ecust.edu.cn",
        "github": "https://github.com/JOHNNY-fans/RankNorm",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;2;3+1;0",
        "aff_unique_norm": "East China University of Science and Technology;Shanghai Artificial Intelligence Laboratory;Northwestern University;Shanghai Jiao Tong University",
        "aff_unique_dep": "School of Information Science and Engineering;Intelligent Healthcare;;",
        "aff_unique_url": "http://www.ecust.edu.cn;https://www.shanghaiai.cn;https://www.northwestern.edu;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "ECUST;SHAIC;NU;SJTU",
        "aff_campus_unique_index": "0+0;0;0+0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0;1;0+0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.452",
        "title": "Analysing Zero-Shot Readability-Controlled Sentence Simplification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Readability-controlled text simplification (RCTS) rewrites texts to lower readability levels while preserving their meaning. RCTS models often depend on parallel corpora with readability annotations on both source and target sides. Such datasets are scarce and difficult to curate, especially at the sentence level. To reduce reliance on parallel data, we explore using instruction-tuned large language models for zero-shot RCTS. Through automatic and manual evaluations, we examine: (1) how different types of contextual information affect a model\u2019s ability to generate sentences with the desired readability, and (2) the trade-off between achieving target readability and preserving meaning. Results show that all tested models struggle to simplify sentences (especially to the lowest levels) due to models\u2019 limitations and characteristics of the source sentences that impede adequate rewriting. Our experiments also highlight the need for better automatic evaluation metrics tailored to RCTS, as standard ones often misinterpret common simplification operations, and inaccurately assess readability and meaning preservation.",
        "author": "Abdullah Barayan; Jose Camacho-Collados; Fernando Alva-Manchego",
        "authorids": "/a/abdullah-barayan/; /j/jose-camacho-collados/; /f/fernando-alva-manchego/",
        "bibtex": "@inproceedings{barayan-etal-2025-analysing,\n    title = \"Analysing Zero-Shot Readability-Controlled Sentence Simplification\",\n    author = \"Barayan, Abdullah  and\n      Camacho-Collados, Jose  and\n      Alva-Manchego, Fernando\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.452/\",\n    pages = \"6762--6781\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.452.pdf",
        "site": "https://aclanthology.org/2025.coling-main.452/",
        "pdf_size": 8019142,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2890844386644217775&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Informatics, Cardiff University, UK + Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; School of Computer Science and Informatics, Cardiff University, UK; School of Computer Science and Informatics, Cardiff University, UK",
        "aff_domain": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "email": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Cardiff University;King Abdulaziz University",
        "aff_unique_dep": "School of Computer Science and Informatics;Faculty of Computing and Information Technology",
        "aff_unique_url": "https://www.cardiff.ac.uk;https://www.kau.edu.sa",
        "aff_unique_abbr": "Cardiff;KAU",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Cardiff;Jeddah",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "United Kingdom;Saudi Arabia"
    },
    {
        "id": "2025.coling-main.109",
        "title": "Analyzing Continuous Semantic Shifts with Diachronic Word Similarity Matrices",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The meanings and relationships of words shift over time. This phenomenon is referred to as semantic shift. Research focused on understanding how semantic shifts occur over multiple time periods is essential for gaining a detailed understanding of semantic shifts. However, detecting change points only between adjacent time periods is insufficient for analyzing detailed semantic shifts, and using BERT-based methods to examine word sense proportions incurs a high computational cost. To address those issues, we propose a simple yet intuitive framework for how semantic shifts occur over multiple time periods by utilizing similarity matrices based on word embeddings. We calculate diachronic word similarity matrices using fast and lightweight word embeddings across arbitrary time periods, making it deeper to analyze continuous semantic shifts. Additionally, by clustering the resulting similarity matrices, we can categorize words that exhibit similar behavior of semantic shift in an unsupervised manner.",
        "author": "Hajime Kiyama; Taichi Aida; Mamoru Komachi; Toshinobu Ogiso; Hiroya Takamura; Daichi Mochihashi",
        "authorids": "/h/hajime-kiyama/; /t/taichi-aida/; /m/mamoru-komachi/; /t/toshinobu-ogiso/; /h/hiroya-takamura/; /d/daichi-mochihashi/",
        "bibtex": "@inproceedings{kiyama-etal-2025-analyzing,\n    title = \"Analyzing Continuous Semantic Shifts with Diachronic Word Similarity Matrices\",\n    author = \"Kiyama, Hajime  and\n      Aida, Taichi  and\n      Komachi, Mamoru  and\n      Ogiso, Toshinobu  and\n      Takamura, Hiroya  and\n      Mochihashi, Daichi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.109/\",\n    pages = \"1613--1631\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.109.pdf",
        "site": "https://aclanthology.org/2025.coling-main.109/",
        "pdf_size": 12878597,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:u7tf7bhjyAAJ:scholar.google.com/&scioq=Analyzing+Continuous+Semantic+Shifts+with+Diachronic+Word+Similarity+Matrices&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Tokyo Metropolitan University; Tokyo Metropolitan University; Hitotsubashi University; National Institute for Japanese Language and Linguistics; National Institute of Advanced Industrial Science and Technology; National Institute for Japanese Language and Linguistics+The Institute of Statistical Mathematics",
        "aff_domain": "ed.tmu.ac.jp;ed.tmu.ac.jp;r.hit-u.ac.jp;ninjal.ac.jp;aist.go.jp;ism.ac.jp",
        "email": "ed.tmu.ac.jp;ed.tmu.ac.jp;r.hit-u.ac.jp;ninjal.ac.jp;aist.go.jp;ism.ac.jp",
        "github": "https://github.com/kiyama-hajime/acss-simmat",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;3;2+4",
        "aff_unique_norm": "Tokyo Metropolitan University;Hitotsubashi University;National Institute for Japanese Language and Linguistics;National Institute of Advanced Industrial Science and Technology;The Institute of Statistical Mathematics",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.tmuc.ac.jp;https://www.hut.ac.jp;http://www.ninjal.ac.jp;https://www.aist.go.jp;https://www.ism.ac.jp",
        "aff_unique_abbr": "TMU;HU;NINJAL;AIST;ISM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.653",
        "title": "Analyzing Offensive Language Dataset Insights from Training Dynamics and Human Agreement Level",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Implicit hate speech detection is challenging due to its subjectivity and context dependence, with existing models often struggling in outof-domain scenarios. We propose CONELA, a novel data refinement strategy that enhances model performance and generalization by integrating human annotation agreement with model training dynamics. By removing both easy and hard instances from the model\u2019s perspective, while also considering whether humans agree or disagree and retaining ambiguous cases crucial for out-of-distribution generalization, CONELA consistently improves performance across multiple datasets and models. We also observe significant improvements in F1 scores and cross-domain generalization with the use of our CONELA strategy. Addressing data scarcity in smaller datasets, we introduce a weighted loss function and an ensemble strategy incorporating disagreement maximization, effectively balancing learning from limited data. Our findings demonstrate that refining datasets by integrating both model and human perspectives significantly enhances the effectiveness and generalization of implicit hate speech detection models. This approach lays a strong foundation for future research on dataset refinement and model robustness.",
        "author": "Do-Kyung Kim; Hyeseon Ahn; Youngwook Kim; Yo-Sub Han",
        "authorids": "/d/do-kyung-kim/; /h/hyeseon-ahn/; /y/youngwook-kim/; /y/yo-sub-han/",
        "bibtex": "@inproceedings{kim-etal-2025-analyzing,\n    title = \"Analyzing Offensive Language Dataset Insights from Training Dynamics and Human Agreement Level\",\n    author = \"Kim, Do-Kyung  and\n      Ahn, Hyeseon  and\n      Kim, Youngwook  and\n      Han, Yo-Sub\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.653/\",\n    pages = \"9780--9792\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.653.pdf",
        "site": "https://aclanthology.org/2025.coling-main.653/",
        "pdf_size": 1628100,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:05ghd7jZCS4J:scholar.google.com/&scioq=Analyzing+Offensive+Language+Dataset+Insights+from+Training+Dynamics+and+Human+Agreement+Level&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, Yonsei University; Department of Computer Science, Yonsei University; KT; Department of Computer Science, Yonsei University",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;kt.com;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;kt.com;yonsei.ac.kr",
        "github": "https://github.com/kdkcode/CONELA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Yonsei University;Korea Telecom",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.yonsei.ac.kr;http://www.kt.com",
        "aff_unique_abbr": "Yonsei;KT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.424",
        "title": "Analyzing the Attention Heads for Pronoun Disambiguation in Context-aware Machine Translation Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we investigate the role of attention heads in Context-aware Machine Translation models for pronoun disambiguation in the English-to-German and English-to-French language directions. We analyze their influence by both observing and modifying the attention scores corresponding to the plausible relations that could impact a pronoun prediction. Our findings reveal that while some heads do attend the relations of interest, not all of them influence the models\u2019 ability to disambiguate pronouns. We show that certain heads are underutilized by the models, suggesting that model performance could be improved if only the heads would attend one of the relations more strongly. Furthermore, we fine-tune the most promising heads and observe the increase in pronoun disambiguation accuracy of up to 5 percentage points which demonstrates that the improvements in performance can be solidified into the models\u2019 parameters.",
        "author": "Pawe\u0142 M\u0105ka; Yusuf Can Semerci; Jan Scholtes; Gerasimos Spanakis",
        "authorids": "/p/pawel-maka/; /y/yusuf-can-semerci/; /j/jan-scholtes/; /g/gerasimos-spanakis/",
        "bibtex": "@inproceedings{maka-etal-2025-analyzing,\n    title = \"Analyzing the Attention Heads for Pronoun Disambiguation in Context-aware Machine Translation Models\",\n    author = \"M{\\k{a}}ka, Pawe{\\l}  and\n      Semerci, Yusuf Can  and\n      Scholtes, Jan  and\n      Spanakis, Gerasimos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.424/\",\n    pages = \"6348--6377\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.424.pdf",
        "site": "https://aclanthology.org/2025.coling-main.424/",
        "pdf_size": 17720457,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11385429506321757625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Advanced Computing Sciences, Maastricht University; Department of Advanced Computing Sciences, Maastricht University; Department of Advanced Computing Sciences, Maastricht University; Department of Advanced Computing Sciences, Maastricht University",
        "aff_domain": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "email": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Maastricht University",
        "aff_unique_dep": "Department of Advanced Computing Sciences",
        "aff_unique_url": "https://www.maastrichtuniversity.nl",
        "aff_unique_abbr": "MU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Maastricht",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2025.coling-main.356",
        "title": "Annotating the French Wiktionary with supersenses for large scale lexical analysis: a use case to assess form-meaning relationships within the nominal lexicon",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Many languages lack broad-coverage, semantically annotated lexical resources, which limits empirical research on lexical semantics for these languages. In this paper, we report on how we automatically enriched the French Wiktionnary with general semantic classes, known as supersenses, using a limited amount of manually annotated data. We trained a classifier combining sense definition classification and sense exemplars classification. The resulting resource, with an evaluated supersense accuracy of nearly 85% (92% for hypersenses), is used in a case study illustrating how such an semantically enriched resource can be leveraged to empirically test linguistic hypotheses about the lexicon, on a large scale.",
        "author": "Nicolas Angleraud; Lucie Barque; Marie Candito",
        "authorids": "/n/nicolas-angleraud/; /l/lucie-barque/; /m/marie-candito/",
        "bibtex": "@inproceedings{angleraud-etal-2025-annotating,\n    title = \"Annotating the {F}rench {W}iktionary with supersenses for large scale lexical analysis: a use case to assess form-meaning relationships within the nominal lexicon\",\n    author = \"Angleraud, Nicolas  and\n      Barque, Lucie  and\n      Candito, Marie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.356/\",\n    pages = \"5321--5332\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.356.pdf",
        "site": "https://aclanthology.org/2025.coling-main.356/",
        "pdf_size": 315304,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3CXcIHaKlCEJ:scholar.google.com/&scioq=Annotating+the+French+Wiktionary+with+supersenses+for+large+scale+lexical+analysis:+a+use+case+to+assess+form-meaning+relationships+within+the+nominal+lexicon&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Universit\u00e9 Paris Cit\u00e9, CNRS; USPN, Villetaneuse, France+LLF, CNRS, Paris, France; Universit\u00e9 Paris Cit\u00e9, CNRS",
        "aff_domain": "gmail.com;univ-paris13.fr;u-paris.fr",
        "email": "gmail.com;univ-paris13.fr;u-paris.fr",
        "github": "https://github.com/NicolasAngleraud/SuperWikt-fr",
        "project": "https://osf.io/7gjem/?view_only=42190678aba442b39664ad05a54bf843",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Universit\u00e9 Paris Cit\u00e9;Universite Paris-Nord;CNRS",
        "aff_unique_dep": ";;LLF",
        "aff_unique_url": "https://www.universite-paris.fr;https://www.univ-paris13.fr;https://www.cnrs.fr",
        "aff_unique_abbr": "UPC;USPN;CNRS",
        "aff_campus_unique_index": "1+2",
        "aff_campus_unique": ";Villetaneuse;Paris",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.283",
        "title": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes \u224845K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We have released the dialectal translation models and benchmarks developed in this study (https://huggingface.co/datasets/QCRI/AraDiCE)",
        "author": "Basel Mousi; Nadir Durrani; Fatema Ahmad; Md. Arid Hasan; Maram Hasanain; Tameem Kabbani; Fahim Dalvi; Shammur Absar Chowdhury; Firoj Alam",
        "authorids": "/b/basel-mousi/; /n/nadir-durrani/; /f/fatema-ahmad/; /m/md-arid-hasan/; /m/maram-hasanain/; /t/tameem-kabbani/; /f/fahim-dalvi/; /s/shammur-absar-chowdhury/; /f/firoj-alam/",
        "bibtex": "@inproceedings{mousi-etal-2025-aradice,\n    title = \"{A}ra{D}i{CE}: Benchmarks for Dialectal and Cultural Capabilities in {LLM}s\",\n    author = \"Mousi, Basel  and\n      Durrani, Nadir  and\n      Ahmad, Fatema  and\n      Hasan, Md. Arid  and\n      Hasanain, Maram  and\n      Kabbani, Tameem  and\n      Dalvi, Fahim  and\n      Chowdhury, Shammur Absar  and\n      Alam, Firoj\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.283/\",\n    pages = \"4186--4218\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.283.pdf",
        "site": "https://aclanthology.org/2025.coling-main.283/",
        "pdf_size": 1575648,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4746471577904720197&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Qatar Computing Research Institute, Qatar; Qatar Computing Research Institute, Qatar; Qatar Computing Research Institute, Qatar; University of New Brunswick, Canada; Qatar Computing Research Institute, Qatar; American University of Sharjah, UAE; Qatar Computing Research Institute, Qatar; Qatar Computing Research Institute, Qatar; Qatar Computing Research Institute, Qatar",
        "aff_domain": "hbku.edu.qa;hbku.edu.qa;hbku.edu.qa; ; ; ; ; ; ",
        "email": "hbku.edu.qa;hbku.edu.qa;hbku.edu.qa; ; ; ; ; ; ",
        "github": "",
        "project": "https://huggingface.co/datasets/QCRI/AraDiCE",
        "author_num": 9,
        "aff_unique_index": "0;0;0;1;0;2;0;0;0",
        "aff_unique_norm": "Qatar Computing Research Institute;University of New Brunswick;American University of Sharjah",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.qcri.org;https://www.unb.ca;https://www.aus.edu",
        "aff_unique_abbr": "QCRI;UNB;AUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;2;0;0;0",
        "aff_country_unique": "Qatar;Canada;United Arab Emirates"
    },
    {
        "id": "2025.coling-main.579",
        "title": "AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 522 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, privacy, illegal activities, mental health, physical health, unfairness, and offensive language. We evaluated a set of LLMs against our benchmark to assess their trustworthiness. GPT-4 was the most trustworthy LLM, while open-source models, particularly AceGPT 7B and Jais 13B, struggled to achieve a score of 60% in our benchmark. The benchmark dataset is publicly available at https://huggingface.co/datasets/asas-ai/AraTrust",
        "author": "Emad A. Alghamdi; Reem Masoud; Deema Alnuhait; Afnan Y. Alomairi; Ahmed Ashraf; Mohamed Zaytoon",
        "authorids": "/e/emad-a-alghamdi/; /r/reem-masoud/; /d/deema-alnuhait/; /a/afnan-y-alomairi/; /a/ahmed-ashraf/; /m/mohamed-zaytoon/",
        "bibtex": "@inproceedings{alghamdi-etal-2025-aratrust,\n    title = \"{A}ra{T}rust: An Evaluation of Trustworthiness for {LLM}s in {A}rabic\",\n    author = \"Alghamdi, Emad A.  and\n      Masoud, Reem  and\n      Alnuhait, Deema  and\n      Alomairi, Afnan Y.  and\n      Ashraf, Ahmed  and\n      Zaytoon, Mohamed\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.579/\",\n    pages = \"8664--8679\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.579.pdf",
        "site": "https://aclanthology.org/2025.coling-main.579/",
        "pdf_size": 3430974,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11796433018993565227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.133",
        "title": "Are Your Keywords Like My Queries? A Corpus-Wide Evaluation of Keyword Extractors with Real Searches",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Keyword Extraction (KE) is essential in Natural Language Processing (NLP) for identifying key terms that represent the main themes of a text, and it is vital for applications such as information retrieval, text summarisation, and document classification. Despite the development of various KE methods \u2014 including statistical approaches and advanced deep learning models \u2014 evaluating their effectiveness remains challenging. Current evaluation metrics focus on keyword quality, balance, and overlap with annotations from authors and professional indexers, but neglect real-world information retrieval needs. This paper introduces a novel evaluation method designed to overcome this limitation by using real query data from Google Trends and can be used with both supervised and unsupervised KE approaches. We applied this method to three popular KE approaches (YAKE, RAKE and KeyBERT) and found that KeyBERT was the most effective in capturing users\u2019 top queries, with RAKE also showing surprisingly good performance. The code is open-access and publicly available.",
        "author": "Martina Galletti; Giulio Prevedello; Emanuele Brugnoli; Donald Ruggiero Lo Sardo; Pietro Gravino",
        "authorids": "/m/martina-galletti/; /g/giulio-prevedello/; /e/emanuele-brugnoli/; /d/donald-ruggiero-lo-sardo/; /p/pietro-gravino/",
        "bibtex": "@inproceedings{galletti-etal-2025-keywords,\n    title = \"Are Your Keywords Like My Queries? A Corpus-Wide Evaluation of Keyword Extractors with Real Searches\",\n    author = \"Galletti, Martina  and\n      Prevedello, Giulio  and\n      Brugnoli, Emanuele  and\n      Lo Sardo, Donald Ruggiero  and\n      Gravino, Pietro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.133/\",\n    pages = \"1943--1951\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.133.pdf",
        "site": "https://aclanthology.org/2025.coling-main.133/",
        "pdf_size": 395345,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TZdsXMTZl1kJ:scholar.google.com/&scioq=Are+Your+Keywords+Like+My+Queries%3F+A+Corpus-Wide+Evaluation+of+Keyword+Extractors+with+Real+Searches&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Sony Computer Science Laboratories - Paris+Enrico Fermi\u2019s Research Center (CREF)+Sapienza University of Rome; Sony Computer Science Laboratories - Paris+Enrico Fermi\u2019s Research Center (CREF); Sony CSL Rome Research+Enrico Fermi\u2019s Research Center (CREF)+Sapienza University of Rome; Sony CSL Rome Research+Enrico Fermi\u2019s Research Center (CREF)+Sapienza University of Rome; Sony Computer Science Laboratories - Paris+Enrico Fermi\u2019s Research Center (CREF)",
        "aff_domain": "sony.com;sony.com; ; ;sony.com",
        "email": "sony.com;sony.com; ; ;sony.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1;0+1+2;0+1+2;0+1",
        "aff_unique_norm": "Sony Computer Science Laboratories;Enrico Fermi\u2019s Research Center;Sapienza University of Rome",
        "aff_unique_dep": "Computer Science;;",
        "aff_unique_url": "https://www.sony.net/SSL/;https://www.fnal.gov/;https://www.uniroma1.it",
        "aff_unique_abbr": "Sony CSL;CREF;Sapienza",
        "aff_campus_unique_index": "0+2;0;2+2;2+2;0",
        "aff_campus_unique": "Paris;;Rome",
        "aff_country_unique_index": "0+1+2;0+1;2+1+2;2+1+2;0+1",
        "aff_country_unique": "France;United States;Italy"
    },
    {
        "id": "2025.coling-main.442",
        "title": "Argument Mining with Fine-Tuned Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "An end-to-end argument mining (AM) pipeline takes a text as input and provides its argumentative structure as output by identifying and classifying the argument units and argument relations in the text. In this work, we approach AM using fine-tuned large language models (LLMs). We model the three main sub-tasks of the AM pipeline, as well as their joint formulation, as text generation tasks. We fine-tune eight popular quantized and non-quantized LLMs \u2013 LLaMA-3, LLaMA-3.1, Gemma-2, Mistral, Phi-3, Qwen-2 \u2013 which are among the most capable open-weight models, on the benchmark PE, AbstRCT, and CDCP datasets that represent diverse data sources. Our approach achieves state-of-the-art results across all AM sub-tasks and datasets, showing significant improvements over previous benchmarks.",
        "author": "J\u00e9r\u00e9mie Cabessa; Hugo Hernault; Umer Mushtaq",
        "authorids": "/j/jeremie-cabessa/; /h/hugo-hernault/; /u/umer-mushtaq/",
        "bibtex": "@inproceedings{cabessa-etal-2025-argument,\n    title = \"Argument Mining with Fine-Tuned Large Language Models\",\n    author = \"Cabessa, J{\\'e}r{\\'e}mie  and\n      Hernault, Hugo  and\n      Mushtaq, Umer\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.442/\",\n    pages = \"6624--6635\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.442.pdf",
        "site": "https://aclanthology.org/2025.coling-main.442/",
        "pdf_size": 272815,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1HweH-2F788J:scholar.google.com/&scioq=Argument+Mining+with+Fine-Tuned+Large+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "DA VID Lab, University of Versailles (UVSQ) \u2013 Paris-Saclay, 78035 Versailles, France+Institute of Computer Science of the Czech Academy of Sciences, 18207 Prague 8, Czech Republic; Playtika Ltd., CH-1003 Lausanne, Switzerland; L3i, University of La Rochelle, 17042 La Rochelle, France",
        "aff_domain": "uvsq.fr;playtika.com;univ-lr.fr",
        "email": "uvsq.fr;playtika.com;univ-lr.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "University of Versailles;Czech Academy of Sciences;Playtika Ltd.;University of La Rochelle",
        "aff_unique_dep": "DA VID Lab;Institute of Computer Science;;L3i",
        "aff_unique_url": "https://www.uvsq.fr;https://www.cas.cz;;https://www.univ-larochelle.fr",
        "aff_unique_abbr": "UVSQ;CAS;;",
        "aff_campus_unique_index": "0+1;3",
        "aff_campus_unique": "Paris-Saclay;Prague;;La Rochelle",
        "aff_country_unique_index": "0+1;2;0",
        "aff_country_unique": "France;Czech Republic;Switzerland"
    },
    {
        "id": "2025.coling-main.621",
        "title": "Argumentation and Domain Discourse in Scholarly Articles on the Theory of International Relations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present the first dataset, an annotation scheme, discourse analysis, and baseline experiments on argumentation and domain content types in scholarly articles on political science, specifically on the theory of International Relations (IR). The dataset comprises over 1 600 sentences stemming from three foundational articles on Neo-Realism, Liberalism, and Constructivism. We show that our annotation scheme enables educationally-relevant insight into the scholarly IR discourse and that state-of-the-art classifiers, while effective in distinguishing basic argumentative elements (Claims and Support/Attack relations) reaching up to 0.97 micro F1 , require domain-specific training and fine-tuning on the more fine-grained tasks of relation and content type prediction.",
        "author": "Magdalena Wolska; Sassan Gholiagha; Mitja Sienknecht; Dora Kiesel; Irene Lopez Garcia; Patrick Riehmann; Matti Wiegmann; Bernd Froehlich; Katrin Girgensohn; J\u00fcrgen Neyer; Benno Stein",
        "authorids": "/m/magdalena-wolska/; /s/sassan-gholiagha/; /m/mitja-sienknecht/; /d/dora-kiesel/; /i/irene-lopez-garcia/; /p/patrick-riehmann/; /m/matti-wiegmann/; /b/bernd-froehlich/; /k/katrin-girgensohn/; /j/jurgen-neyer/; /b/benno-stein/",
        "bibtex": "@inproceedings{wolska-etal-2025-argumentation,\n    title = \"Argumentation and Domain Discourse in Scholarly Articles on the Theory of International Relations\",\n    author = {Wolska, Magdalena  and\n      Gholiagha, Sassan  and\n      Sienknecht, Mitja  and\n      Kiesel, Dora  and\n      Lopez Garcia, Irene  and\n      Riehmann, Patrick  and\n      Wiegmann, Matti  and\n      Froehlich, Bernd  and\n      Girgensohn, Katrin  and\n      Neyer, J{\\\"u}rgen  and\n      Stein, Benno},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.621/\",\n    pages = \"9238--9249\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.621.pdf",
        "site": "https://aclanthology.org/2025.coling-main.621/",
        "pdf_size": 1214003,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TJzISl-NFqoJ:scholar.google.com/&scioq=Argumentation+and+Domain+Discourse+in+Scholarly+Articles+on+the+Theory+of+International+Relations&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Bauhaus-Universit\u00e4t Weimar; Europa-Universit\u00e4t Viadrina; Europa-Universit\u00e4t Viadrina; Bauhaus-Universit\u00e4t Weimar; Bauhaus-Universit\u00e4t Weimar; J\u00f6nk\u00f6ping University; Bauhaus-Universit\u00e4t Weimar; Bauhaus-Universit\u00e4t Weimar; Europa-Universit\u00e4t Viadrina; Europa-Universit\u00e4t Viadrina; Bauhaus-Universit\u00e4t Weimar",
        "aff_domain": "uni-weimar.de;europa-uni.de;europa-uni.de;uni-weimar.de;uni-weimar.de;ju.se;uni-weimar.de;uni-weimar.de;europa-uni.de;europa-uni.de;uni-weimar.de",
        "email": "uni-weimar.de;europa-uni.de;europa-uni.de;uni-weimar.de;uni-weimar.de;ju.se;uni-weimar.de;uni-weimar.de;europa-uni.de;europa-uni.de;uni-weimar.de",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;1;0;0;2;0;0;1;1;0",
        "aff_unique_norm": "Bauhaus-Universit\u00e4t Weimar;Europa-Universit\u00e4t Viadrina;J\u00f6nk\u00f6ping University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.bauhaus-university.de;https://www.europa-uni.de/;https://ju.se/en",
        "aff_unique_abbr": "Bauhaus-Uni Weimar;EUV;JU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Weimar;",
        "aff_country_unique_index": "0;0;0;0;0;1;0;0;0;0;0",
        "aff_country_unique": "Germany;Sweden"
    },
    {
        "id": "2025.coling-main.210",
        "title": "Aspect-Based Sentiment Analysis with Syntax-Opinion-Sentiment Reasoning Chain",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the impressive capabilities of large language models (LLMs) in aspect-based sentiment analysis (ABSA), the role of syntactic information remains underexplored in LLMs. Syntactic structures are known to be crucial for capturing aspect-opinion relationships. To explore whether LLMs can effectively leverage syntactic information to improve ABSA performance, we propose a novel multi-step reasoning framework, the Syntax-Opinion-Sentiment Reasoning Chain (Syn-Chain). Syn-Chain sequentially analyzes syntactic dependencies, extracts opinions, and classifies sentiment. We introduce Syn-Chain into LLMs via zero-shot prompting, and results show that Syn-Chain significantly enhances ABSA performance, though smaller LLM exhibit weaker performance. Furthermore, we enhance smaller LLMs via distillation using GPT-3.5-generated Syn-Chain responses, achieving state-of-the-art ABSA performance. Our findings highlight the importance of syntactic information for improving LLMs in ABSA and offer valuable insights for future research.",
        "author": "Rui Fan; Shu Li; Tingting He; Yu Liu",
        "authorids": "/r/rui-fan/; /s/shu-li/; /t/tingting-he/; /y/yu-liu/",
        "bibtex": "@inproceedings{fan-etal-2025-aspect,\n    title = \"Aspect-Based Sentiment Analysis with Syntax-Opinion-Sentiment Reasoning Chain\",\n    author = \"Fan, Rui  and\n      Li, Shu  and\n      He, Tingting  and\n      Liu, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.210/\",\n    pages = \"3123--3137\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.210.pdf",
        "site": "https://aclanthology.org/2025.coling-main.210/",
        "pdf_size": 1980316,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:RpEofggJBOEJ:scholar.google.com/&scioq=Aspect-Based+Sentiment+Analysis+with+Syntax-Opinion-Sentiment+Reasoning+Chain&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning + National Language Resources Monitor and Research Center for Network Media + Faculty of Artificial Intelligence in Education; Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning + National Language Resources Monitor and Research Center for Network Media + School of Computer, Central China Normal University; Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning + National Language Resources Monitor and Research Center for Network Media + School of Computer, Central China Normal University; Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning + National Language Resources Monitor and Research Center for Network Media + Faculty of Artificial Intelligence in Education",
        "aff_domain": "mails.ccnu.edu.cn;mails.ccnu.edu.cn;mail.ccnu.edu.cn;mails.ccnu.edu.cn",
        "email": "mails.ccnu.edu.cn;mails.ccnu.edu.cn;mail.ccnu.edu.cn;mails.ccnu.edu.cn",
        "github": "https://github.com/rf-x/Syn-Chain-ABSA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+1+3;0+1+3;0+1+2",
        "aff_unique_norm": "Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning;National Language Resources Monitor and Research Center;Faculty of Artificial Intelligence in Education;Central China Normal University",
        "aff_unique_dep": "Artificial Intelligence and Smart Learning;Research Center for Network Media;Artificial Intelligence in Education;School of Computer",
        "aff_unique_url": ";;;http://www.ccnu.edu.cn",
        "aff_unique_abbr": ";;;CCNU",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0+0;0+0+0;0+0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2025.coling-main.239",
        "title": "Assessing the Human Likeness of AI-Generated Counterspeech",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Counterspeech is a targeted response to counteract and challenge abusive or hateful content. It effectively curbs the spread of hatred and fosters constructive online communication. Previous studies have proposed different strategies for automatically generated counterspeech. Evaluations, however, focus on relevance, surface form, and other shallow linguistic characteristics. This paper investigates the human likeness of AI-generated counterspeech, a critical factor influencing effectiveness. We implement and evaluate several LLM-based generation strategies, and discover that AI-generated and human-written counterspeech can be easily distinguished by both simple classifiers and humans. Further, we reveal differences in linguistic characteristics, politeness, and specificity. The dataset used in this study is publicly available for further research.",
        "author": "Xiaoying Song; Sujana Mamidisetty; Eduardo Blanco; Lingzi Hong",
        "authorids": "/x/xiaoying-song/; /s/sujana-mamidisetty/; /e/eduardo-blanco/; /l/lingzi-hong/",
        "bibtex": "@inproceedings{song-etal-2025-assessing,\n    title = \"Assessing the Human Likeness of {AI}-Generated Counterspeech\",\n    author = \"Song, Xiaoying  and\n      Mamidisetty, Sujana  and\n      Blanco, Eduardo  and\n      Hong, Lingzi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.239/\",\n    pages = \"3547--3559\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.239.pdf",
        "site": "https://aclanthology.org/2025.coling-main.239/",
        "pdf_size": 1233218,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18389121336137651876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Information, University of North Texas; TAMS, University of North Texas; Department of Computer Science, University of Arizona; College of Information, University of North Texas",
        "aff_domain": "my.unt.edu;my.unt.edu;arizona.edu;unt.edu",
        "email": "my.unt.edu;my.unt.edu;arizona.edu;unt.edu",
        "github": "https://github.com/oliveeeee25/counterspeech_eval_humanlike",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of North Texas;University of Arizona",
        "aff_unique_dep": "College of Information;Department of Computer Science",
        "aff_unique_url": "https://www.unt.edu;https://www.arizona.edu",
        "aff_unique_abbr": "UNT;UArizona",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.158",
        "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models have shown exceptional capabilities in a wide range of tasks, such as text generation and video generation, among others. However, due to their massive parameter count, these models often require substantial storage space, imposing significant constraints on the machines deploying LLMs. To overcome this limitation, one research direction proposes to compress the models using integer replacements for floating-point numbers, in a process known as Quantization. Some recent studies suggest quantizing the key and value cache (KV Cache) of LLMs, and designing quantization techniques that treat the key and value matrices equivalently. This work delves deeper into the asymmetric structural roles of KV Cache, a phenomenon where the transformer\u2019s output loss is more sensitive to the quantization of key matrices. We conduct a systematic examination of the attention output error resulting from key and value quantization. The phenomenon inspires us to propose an asymmetric quantization strategy. Our approach allows for 1-bit quantization of the KV cache by implementing distinct configurations for key and value matrices. We carry out experiments across a variety of datasets, demonstrating that our proposed model allows for the quantization of up to 75% decoder layers with 1 bit, while simultaneously maintaining performance levels comparable to those of the models with floating parameters.",
        "author": "Qian Tao; Wenyuan Yu; Jingren Zhou",
        "authorids": "/q/qian-tao/; /w/wenyuan-yu/; /j/jingren-zhou/",
        "bibtex": "@inproceedings{tao-etal-2025-asymkv,\n    title = \"{A}sym{KV}: Enabling 1-Bit Quantization of {KV} Cache with Layer-Wise Asymmetric Quantization Configurations\",\n    author = \"Tao, Qian  and\n      Yu, Wenyuan  and\n      Zhou, Jingren\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.158/\",\n    pages = \"2316--2328\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.158.pdf",
        "site": "https://aclanthology.org/2025.coling-main.158/",
        "pdf_size": 676851,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11977603518567774236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tongyi Lab, Alibaba Group; Tongyi Lab, Alibaba Group; Alibaba Cloud Computing, Alibaba Group",
        "aff_domain": "alibaba-inc.com; ; ",
        "email": "alibaba-inc.com; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "Tongyi Lab",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.335",
        "title": "Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper proposes Attention-Seeker, an unsupervised keyphrase extraction method that leverages self-attention maps from a Large Language Model to estimate the importance of candidate phrases. Our approach identifies specific components \u2013 such as layers, heads, and attention vectors \u2013 where the model pays significant attention to the key topics of the text. The attention weights provided by these components are then used to score the candidate phrases. Unlike previous models that require manual tuning of parameters (e.g., selection of heads, prompts, hyperparameters), Attention-Seeker dynamically adapts to the input text without any manual adjustments, enhancing its practical applicability. We evaluate Attention-Seeker on four publicly available datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results demonstrate that, even without parameter tuning, Attention-Seeker outperforms most baseline models, achieving state-of-the-art performance on three out of four datasets, particularly excelling in extracting keyphrases from long documents.",
        "author": "Erwin Daniel Lopez Zapata; Cheng Tang; Atsushi Shimada",
        "authorids": "/e/erwin-daniel-lopez-zapata/; /c/cheng-tang/; /a/atsushi-shimada/",
        "bibtex": "@inproceedings{lopez-zapata-etal-2025-attention,\n    title = \"Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction\",\n    author = \"Lopez Zapata, Erwin Daniel  and\n      Tang, Cheng  and\n      Shimada, Atsushi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.335/\",\n    pages = \"5011--5026\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.335.pdf",
        "site": "https://aclanthology.org/2025.coling-main.335/",
        "pdf_size": 1516068,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:SrG8diPtfjgJ:scholar.google.com/&scioq=Attention-Seeker:+Dynamic+Self-Attention+Scoring+for+Unsupervised+Keyphrase+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Graduate School of Information Science and Electrical Engineering, Kyushu University; Graduate School of Information Science and Electrical Engineering, Kyushu University; Graduate School of Information Science and Electrical Engineering, Kyushu University",
        "aff_domain": "gmail.com;ait.kyushu-u.ac.jp;ait.kyushu-u.ac.jp",
        "email": "gmail.com;ait.kyushu-u.ac.jp;ait.kyushu-u.ac.jp",
        "github": "https://github.com/EruM16/Attention-Seeker",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Kyushu University",
        "aff_unique_dep": "Graduate School of Information Science and Electrical Engineering",
        "aff_unique_url": "https://www.kyushu-u.ac.jp",
        "aff_unique_abbr": "Kyushu U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-industry.56",
        "title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Pretrained language models are integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as Bloom and StarCoder aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We evaluate Aurora-M across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source Aurora-M and its variants to encourage responsible open-source development of large language models at https://huggingface.co/aurora-m.",
        "author": "Taishi Nakamura; Mayank Mishra; Simone Tedeschi; Yekun Chai; Jason T. Stillerman; Felix Friedrich; Prateek Yadav; Tanmay Laud; Vu Minh Chien; Terry Yue Zhuo; Diganta Misra; Ben Bogin; Xuan-Son Vu; Marzena Karpinska; Arnav Varma Dantuluri; Wojciech Kusa; Tommaso Furlanello; Rio Yokota; Niklas Muennighoff; Suhas Pai; Tosin Adewumi; Veronika Laippala; Xiaozhe Yao; Adalberto Barbosa Junior; Aleksandr Drozd; Jordan Clive; Kshitij Gupta; Liangyu Chen; Qi Sun; Ken Tsui; Nour Moustafa-Fahmy; Nicolo Monti; Tai Dang; Ziyang Luo; Tien-Tung Bui; Roberto Navigli; Virendra Mehta; Matthew Blumberg; Victor May; Hiep Nguyen; Sampo Pyysalo",
        "authorids": "/t/taishi-nakamura/; /m/mayank-mishra/; /s/simone-tedeschi/; /y/yekun-chai/; /j/jason-t-stillerman/; /f/felix-friedrich/; /p/prateek-yadav/; /t/tanmay-laud/; /v/vu-minh-chien/; /t/terry-yue-zhuo/; /d/diganta-misra/; /b/ben-bogin/; /x/xuan-son-vu/; /m/marzena-karpinska/; /a/arnav-varma-dantuluri/; /w/wojciech-kusa/; /t/tommaso-furlanello/; /r/rio-yokota/; /n/niklas-muennighoff/; /s/suhas-pai/; /t/tosin-adewumi/; /v/veronika-laippala/; /x/xiaozhe-yao/; /a/adalberto-barbosa-junior/; /a/aleksandr-drozd/; /j/jordan-clive/; /k/kshitij-gupta/; /l/liang-yu-chen/; /q/qi-sun/; /k/ken-tsui/; /n/nour-moustafa-fahmy/; /n/nicolo-monti/; /t/tai-dang/; /z/ziyang-luo/; /t/tien-tung-bui/; /r/roberto-navigli/; /v/virendra-mehta/; /m/matthew-blumberg/; /v/victor-may/; /h/hiep-nguyen/; /s/sampo-pyysalo/",
        "bibtex": "@inproceedings{nakamura-etal-2025-aurora,\n    title = \"Aurora-{M}: Open Source Continual Pre-training for Multilingual Language and Code\",\n    author = \"Nakamura, Taishi  and\n      Mishra, Mayank  and\n      Tedeschi, Simone  and\n      Chai, Yekun  and\n      Stillerman, Jason T.  and\n      Friedrich, Felix  and\n      Yadav, Prateek  and\n      Laud, Tanmay  and\n      Chien, Vu Minh  and\n      Zhuo, Terry Yue  and\n      Misra, Diganta  and\n      Bogin, Ben  and\n      Vu, Xuan-Son  and\n      Karpinska, Marzena  and\n      Dantuluri, Arnav Varma  and\n      Kusa, Wojciech  and\n      Furlanello, Tommaso  and\n      Yokota, Rio  and\n      Muennighoff, Niklas  and\n      Pai, Suhas  and\n      Adewumi, Tosin  and\n      Laippala, Veronika  and\n      Yao, Xiaozhe  and\n      Junior, Adalberto Barbosa  and\n      Drozd, Aleksandr  and\n      Clive, Jordan  and\n      Gupta, Kshitij  and\n      Chen, Liangyu  and\n      Sun, Qi  and\n      Tsui, Ken  and\n      Moustafa-Fahmy, Nour  and\n      Monti, Nicolo  and\n      Dang, Tai  and\n      Luo, Ziyang  and\n      Bui, Tien-Tung  and\n      Navigli, Roberto  and\n      Mehta, Virendra  and\n      Blumberg, Matthew  and\n      May, Victor  and\n      Nguyen, Hiep  and\n      Pyysalo, Sampo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.56/\",\n    pages = \"656--678\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.56.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.56/",
        "pdf_size": 893544,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18395223779372748714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 41
    },
    {
        "id": "2025.coling-industry.36",
        "title": "AutoProteinEngine: A Large Language Model Driven Agent Framework for Multimodal AutoML in Protein Engineering",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Protein engineering is important for various biomedical applications, but traditional approaches are often inefficient and resource-intensive. While deep learning (DL) models have shown promise, their implementation remains challenging for biologists without specialized computational expertise. To address this gap, we propose AutoProteinEngine (AutoPE), an innovative agent framework that leverages large language models (LLMs) for multimodal automated machine learning (AutoML) in protein engineering. AutoPE introduces a conversational interface that allows biologists without DL backgrounds to interact with DL models using natural language, lowering the entry barrier for protein engineering tasks. Our AutoPE uniquely integrates LLMs with AutoML to handle both protein sequence and graph modalities, automate hyperparameter optimization, and facilitate data retrieval from protein databases. We evaluated AutoPE through two real-world protein engineering tasks, demonstrating substantial improvements in model performance compared to traditional zero-shot and manual fine-tuning approaches. By bridging the gap between DL and biologists\u2019 domain expertise, AutoPE empowers researchers to leverage advanced computational tools without extensive programming knowledge.",
        "author": "Yungeng Liu; Zan Chen; Yuguang Wang; Yiqing Shen",
        "authorids": "/y/yungeng-liu/; /z/zan-chen/; /y/yuguang-wang/; /y/yiqing-shen/",
        "bibtex": "@inproceedings{liu-etal-2025-autoproteinengine,\n    title = \"{A}uto{P}rotein{E}ngine: A Large Language Model Driven Agent Framework for Multimodal {A}uto{ML} in Protein Engineering\",\n    author = \"Liu, Yungeng  and\n      Chen, Zan  and\n      Wang, Yuguang  and\n      Shen, Yiqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.36/\",\n    pages = \"422--430\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.36.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.36/",
        "pdf_size": 1084142,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=663626398300482979&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Toursun Synbio, Shanghai, China+City University of Hong Kong, Hong Kong SAR; Toursun Synbio, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Johns Hopkins University, Baltimore, USA",
        "aff_domain": "gmail.com; ; ;gmail.com",
        "email": "gmail.com; ; ;gmail.com",
        "github": "https://github.com/tsynbio/AutoPE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;2;3",
        "aff_unique_norm": "Toursun Synbio;City University of Hong Kong;Shanghai Jiao Tong University;Johns Hopkins University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.cityu.edu.hk;https://www.sjtu.edu.cn;https://www.jhu.edu",
        "aff_unique_abbr": ";CityU;SJTU;JHU",
        "aff_campus_unique_index": "1;2;3",
        "aff_campus_unique": ";Hong Kong SAR;Shanghai;Baltimore",
        "aff_country_unique_index": "0+0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-industry.13",
        "title": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "The extraction of lung lesion information from clinical and medical imaging reports is crucial for research on and clinical care of lung-related diseases. Large language models (LLMs) can be effective at interpreting unstructured text in reports, but they often hallucinate due to a lack of domain-specific knowledge, leading to reduced accuracy and posing challenges for use in clinical settings. To address this, we propose a novel framework that aligns generated internal knowledge with external knowledge through in-context learning (ICL). Our framework employs a retriever to identify relevant units of internal or external knowledge and a grader to evaluate the truthfulness and helpfulness of the retrieved internal-knowledge rules, to align and update the knowledge bases. Experiments with expert-curated test datasets demonstrate that this ICL approach can increase the F1 score for key fields (lesion size, margin and solidity) by an average of 12.9% over existing ICL methods.",
        "author": "Diya Li; Asim Kadav; Aijing Gao; Rui Li; Richard Bourgon",
        "authorids": "/d/diya-li/; /a/asim-kadav/; /a/aijing-gao/; /r/rui-li/; /r/richard-bourgon/",
        "bibtex": "@inproceedings{li-etal-2025-automated,\n    title = \"Automated Clinical Data Extraction with Knowledge Conditioned {LLM}s\",\n    author = \"Li, Diya  and\n      Kadav, Asim  and\n      Gao, Aijing  and\n      Li, Rui  and\n      Bourgon, Richard\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.13/\",\n    pages = \"149--162\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.13.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.13/",
        "pdf_size": 448826,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12847050757710195262&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Freenome, South San Francisco, CA, USA; Freenome, South San Francisco, CA, USA; Freenome, South San Francisco, CA, USA; ; ",
        "aff_domain": "gmail.com;gmail.com;freenome.com;gmail.com;freenome.com",
        "email": "gmail.com;gmail.com;freenome.com;gmail.com;freenome.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Freenome",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "South San Francisco",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.396",
        "title": "Automated Detection of Tropes In Short Texts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Tropes \u2014 recurring narrative elements like the \u201csmoking gun\u201d or the \u201cveil of secrecy\u201d \u2014 are often used in movies to convey familiar patterns. However, they also play a significant role in online communication about societal issues, where they can oversimplify complex matters and deteriorate public discourse. Recognizing these tropes can offer insights into the emotional manipulation and potential bias present in online discussions. This paper addresses the challenge of automatically detecting tropes in social media posts. We define the task, distinguish it from previous work, and create a ground-truth dataset of social media posts related to vaccines and immigration, manually labeled with tropes. Using this dataset, we develop a supervised machine learning technique for multi-label classification, fine-tune a model, and demonstrate its effectiveness experimentally. Our results show that tropes are common across domains and that fine-tuned models can detect them with high accuracy.",
        "author": "Alessandra Flaccavento; Youri Peskine; Paolo Papotti; Riccardo Torlone; Raphael Troncy",
        "authorids": "/a/alessandra-flaccavento/; /y/youri-peskine/; /p/paolo-papotti/; /r/riccardo-torlone/; /r/raphael-troncy/",
        "bibtex": "@inproceedings{flaccavento-etal-2025-automated,\n    title = \"Automated Detection of Tropes In Short Texts\",\n    author = \"Flaccavento, Alessandra  and\n      Peskine, Youri  and\n      Papotti, Paolo  and\n      Torlone, Riccardo  and\n      Troncy, Raphael\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.396/\",\n    pages = \"5936--5951\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.396.pdf",
        "site": "https://aclanthology.org/2025.coling-main.396/",
        "pdf_size": 2005614,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fi8Xa9I3PHAJ:scholar.google.com/&scioq=Automated+Detection+of+Tropes+In+Short+Texts&hl=en&as_sdt=0,5",
        "gs_version_total": 6,
        "aff": "Universit\u00e0 Roma Tre; EURECOM; EURECOM; Universit\u00e0 Roma Tre; EURECOM",
        "aff_domain": "uniroma3.it;eurecom.fr;eurecom.fr;uniroma3.it;eurecom.fr",
        "email": "uniroma3.it;eurecom.fr;eurecom.fr;uniroma3.it;eurecom.fr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "Universit\u00e0 Roma Tre;EURECOM",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uniroma3.it;https://www.eurecom.fr",
        "aff_unique_abbr": "Roma Tre;EURECOM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "Italy;France"
    },
    {
        "id": "2025.coling-main.462",
        "title": "Automated Molecular Concept Generation and Labeling with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Artificial intelligence (AI) is transforming scientific research, with explainable AI methods like concept-based models (CMs) showing promise for new discoveries. However, in molecular science, CMs are less common than black-box models like Graph Neural Networks (GNNs), due to their need for predefined concepts and manual labeling. This paper introduces the Automated Molecular Concept (AutoMolCo) framework, which leverages Large Language Models (LLMs) to automatically generate and label predictive molecular concepts. Through iterative concept refinement, AutoMolCo enables simple linear models to outperform GNNs and LLM in-context learning on several benchmarks. The framework operates without human knowledge input, overcoming limitations of existing CMs while maintaining explainability and allowing easy intervention. Experiments on MoleculeNet and High-Throughput Experimentation (HTE) datasets demonstrate that AutoMolCoinduced explainable CMs are beneficial for molecular science research.",
        "author": "Zimin Zhang; Qianli Wu; Botao Xia; Fang Sun; Ziniu Hu; Yizhou Sun; Shichang Zhang",
        "authorids": "/z/zimin-zhang/; /q/qianli-wu/; /b/botao-xia/; /f/fang-sun/; /z/ziniu-hu/; /y/yizhou-sun/; /s/shichang-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2025-automated,\n    title = \"Automated Molecular Concept Generation and Labeling with Large Language Models\",\n    author = \"Zhang, Zimin  and\n      Wu, Qianli  and\n      Xia, Botao  and\n      Sun, Fang  and\n      Hu, Ziniu  and\n      Sun, Yizhou  and\n      Zhang, Shichang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.462/\",\n    pages = \"6918--6936\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.462.pdf",
        "site": "https://aclanthology.org/2025.coling-main.462/",
        "pdf_size": 2200481,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6yBUi_yfcXQJ:scholar.google.com/&scioq=Automated+Molecular+Concept+Generation+and+Labeling+with+Large+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "University of Illinois Urbana-Champaign; University of California Los Angeles; University of California Los Angeles; University of California Los Angeles; California Institute of Technology; University of California Los Angeles; Harvard University",
        "aff_domain": "illinois.edu;g.ucla.edu;g.ucla.edu;cs.ucla.edu;cs.ucla.edu;gmail.com;hbs.edu",
        "email": "illinois.edu;g.ucla.edu;g.ucla.edu;cs.ucla.edu;cs.ucla.edu;gmail.com;hbs.edu",
        "github": "https://github.com/ziminz19/AutoMolCo",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;2;1;3",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;University of California, Los Angeles;California Institute of Technology;Harvard University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://illinois.edu;https://www.ucla.edu;https://www.caltech.edu;https://www.harvard.edu",
        "aff_unique_abbr": "UIUC;UCLA;Caltech;Harvard",
        "aff_campus_unique_index": "0;1;1;1;2;1",
        "aff_campus_unique": "Urbana-Champaign;Los Angeles;Pasadena;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.260",
        "title": "Automated Progressive Red Teaming",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Ensuring the safety of large language models (LLMs) is paramount, yet identifying potential vulnerabilities is challenging. While manual red teaming is effective, it is time-consuming, costly and lacks scalability. Automated red teaming (ART) offers a more cost-effective alternative, automatically generating adversarial prompts to expose LLM vulnerabilities. However, in current ART efforts, a robust framework is absent, which explicitly frames red teaming as an effectively learnable task. To address this gap, we propose Automated Progressive Red Teaming (APRT) as an effectively learnable framework. APRT leverages three core modules: an Intention Expanding LLM that generates diverse initial attack samples, an Intention Hiding LLM that crafts deceptive prompts, and an Evil Maker to manage prompt diversity and filter ineffective samples. The three modules collectively and progressively explore and exploit LLM vulnerabilities through multi-round interactions. In addition to the framework, we further propose a novel indicator, Attack Effectiveness Rate (AER) to mitigate the limitations of existing evaluation metrics. By measuring the likelihood of eliciting unsafe but seemingly helpful responses, AER aligns closely with human evaluations. Extensive experiments with both automatic and human evaluations, demonstrate the effectiveness of ARPT across both open- and closed-source LLMs. Specifically, APRT effectively elicits 54% unsafe yet useful responses from Meta\u2019s Llama-3-8B-Instruct, 50% from GPT-4o (API access), and 39% from Claude-3.5 (API access), showcasing its robust attack capability and transferability across LLMs (especially from open-source LLMs to closed-source LLMs).",
        "author": "Bojian Jiang; Yi Jing; Tong Wu; Tianhao Shen; Deyi Xiong; Qing Yang",
        "authorids": "/b/bojian-jiang/; /y/yi-jing/; /t/tong-wu/; /t/tianhao-shen/; /d/deyi-xiong/; /q/qing-yang/",
        "bibtex": "@inproceedings{jiang-etal-2025-automated,\n    title = \"Automated Progressive Red Teaming\",\n    author = \"Jiang, Bojian  and\n      Jing, Yi  and\n      Wu, Tong  and\n      Shen, Tianhao  and\n      Xiong, Deyi  and\n      Yang, Qing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.260/\",\n    pages = \"3850--3864\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.260.pdf",
        "site": "https://aclanthology.org/2025.coling-main.260/",
        "pdf_size": 1139223,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=790939584269178349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China+Du Xiaoman Finance, Beijing, China; Du Xiaoman Finance, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Du Xiaoman Finance, Beijing, China; Du Xiaoman Finance, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;duxiaoman.com;tju.edu.cn;duxiaoman.com;duxiaoman.com;tju.edu.cn",
        "email": "tju.edu.cn;duxiaoman.com;tju.edu.cn;duxiaoman.com;duxiaoman.com;tju.edu.cn",
        "github": "https://github.com/tjunlp-lab/APRT",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;0;1;1;0",
        "aff_unique_norm": "Tianjin University;Du Xiaoman Finance",
        "aff_unique_dep": "College of Intelligence and Computing;",
        "aff_unique_url": "http://www.tju.edu.cn;",
        "aff_unique_abbr": "Tianjin University;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tianjin;",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.512",
        "title": "Automatic Evaluation of Language Generation Technology Based on Structure Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language generation techniques require automatic evaluation to carry out efficient and reproducible experiments. While n-gram matching is standard, it fails to capture semantic equivalence with different wording. Recent methods have addressed this issue by using contextual embeddings from pre-trained language models to compute the similarity between reference and hypothesis. However, these methods frequently disregard the syntax of sentences, despite its crucial role in determining meaning, and thus assign unjustifiably high scores. This paper proposes an automatic evaluation metric that considers both the words in sentences and their syntactic structures. We integrate syntactic information into the recent embedding-based approach. Experimental results obtained from two NLP tasks show that our method is at least comparable to standard baselines.",
        "author": "Katsuki Chousa; Tsutomu Hirao",
        "authorids": "/k/katsuki-chousa/; /t/tsutomu-hirao/",
        "bibtex": "@inproceedings{chousa-hirao-2025-automatic,\n    title = \"Automatic Evaluation of Language Generation Technology Based on Structure Alignment\",\n    author = \"Chousa, Katsuki  and\n      Hirao, Tsutomu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.512/\",\n    pages = \"7663--7670\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.512.pdf",
        "site": "https://aclanthology.org/2025.coling-main.512/",
        "pdf_size": 446447,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Q_YS0C8_p7EJ:scholar.google.com/&scioq=Automatic+Evaluation+of+Language+Generation+Technology+Based+on+Structure+Alignment&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation",
        "aff_domain": "ntt.com;ntt.com",
        "email": "ntt.com;ntt.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NTT Corporation",
        "aff_unique_dep": "Communication Science Laboratories",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.448",
        "title": "Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation, Dataset Construction, and Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Extracting metaphors and analogies from free text requires high-level reasoning abilities such as abstraction and language understanding. Our study focuses on the extraction of the concepts forming metaphoric analogies in literary texts. To this end, we construct a novel dataset in this domain with the help of domain experts. We compare the out-of-the-box ability of recent large language models (LLMs) to structure metaphoric mappings from fragments of texts containing rather explicit proportional analogies. The models are further evaluated on the generation of implicit elements of the analogy, which are indirectly suggested in the texts and inferred by human readers. The competitive results obtained by LLMs in our experiments are encouraging and open up new avenues such as automatically extracting analogies and metaphors from text instead of investing resources in domain experts to manually label data.",
        "author": "Joanne Boisson; Zara Siddique; Hsuvas Borkakoty; Dimosthenis Antypas; Luis Espinosa Anke; Jose Camacho-Collados",
        "authorids": "/j/joanne-boisson/; /z/zara-siddique/; /h/hsuvas-borkakoty/; /d/dimosthenis-antypas/; /l/luis-espinosa-anke/; /j/jose-camacho-collados/",
        "bibtex": "@inproceedings{boisson-etal-2025-automatic,\n    title = \"Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation, Dataset Construction, and Evaluation\",\n    author = \"Boisson, Joanne  and\n      Siddique, Zara  and\n      Borkakoty, Hsuvas  and\n      Antypas, Dimosthenis  and\n      Espinosa Anke, Luis  and\n      Camacho-Collados, Jose\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.448/\",\n    pages = \"6692--6704\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.448.pdf",
        "site": "https://aclanthology.org/2025.coling-main.448/",
        "pdf_size": 385309,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7387391343541914583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Cardiff NLP, School of Computer Science and Informatics, Cardiff University, U.K.; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, U.K.; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, U.K.; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, U.K.; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, U.K. + Amplyfi, Cardiff, U.K.; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, U.K.",
        "aff_domain": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "email": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "github": "",
        "project": "https://analogy-angle.github.io/",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0+1;0",
        "aff_unique_norm": "Cardiff University;Amplyfi",
        "aff_unique_dep": "School of Computer Science and Informatics;",
        "aff_unique_url": "https://www.cardiff.ac.uk;",
        "aff_unique_abbr": "Cardiff;",
        "aff_campus_unique_index": "0;0;0;0;0+0;0",
        "aff_campus_unique": "Cardiff",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.597",
        "title": "Automatic Mathematic In-Context Example Generation for LLM Using Multi-Modal Consistency",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have advanced Natural Language Processing (NLP) tasks but are limited in mathematical reasoning. To address this, few-shot examples are used in prompts for in-context learning. However, existing methods require annotated datasets, resulting in higher computational costs and lower quality examples. To mitigate these limitations, we propose AutoMathIC, a framework that automatically generates high-quality in-context examples to enhance LLMs\u2019 mathematical reasoning. AutoMathIC ensures consistency across different modalities (e.g., Chain-of-Thought (CoT), code snippets, and equations) by generating and selecting mutations that improve response consistency. Evaluated on four math problem datasets, AutoMathIC outperforms six baselines, with LLM accuracy ranging from 87.0% to 99.3% for GPT-3.5 and 93.1% to 98.7% for GPT-4o-mini. It surpasses the state-of-the-art in-context example retrieval method in three of the four datasets by 0.3% to 11.8%, without relying on an annotated dataset.",
        "author": "Jaeseong Lee; Wei Yang; Gopal Gupta; Shiyi Wei",
        "authorids": "/j/jaeseong-lee/; /w/wei-yang/; /g/gopal-gupta/; /s/shiyi-wei/",
        "bibtex": "@inproceedings{lee-etal-2025-automatic,\n    title = \"Automatic Mathematic In-Context Example Generation for {LLM} Using Multi-Modal Consistency\",\n    author = \"Lee, Jaeseong  and\n      Yang, Wei  and\n      Gupta, Gopal  and\n      Wei, Shiyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.597/\",\n    pages = \"8908--8924\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.597.pdf",
        "site": "https://aclanthology.org/2025.coling-main.597/",
        "pdf_size": 483206,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LCpaRwBXBiEJ:scholar.google.com/&scioq=Automatic+Mathematic+In-Context+Example+Generation+for+LLM+Using+Multi-Modal+Consistency&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.154",
        "title": "Automatic Multiple-Choice Question Generation and Evaluation Systems Based on LLM: A Study Case With University Resolutions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multiple choice questions (MCQs) are often used in both employee selection and training, providing objectivity, efficiency, and scalability. However, their creation is resource-intensive, requiring significant expertise and financial investment. This study leverages large language models (LLMs) and prompt engineering techniques to automate the generation and validation of MCQs, particularly within the context of university regulations. Mainly, two novel approaches are proposed in this work: an automatic question generation system for university resolution and an automatic evaluation system to assess the performance of MCQ generation systems. The generation system combines different prompt engineering techniques and a review process to create well formulated questions. The evaluation system uses prompt engineering combined with an advanced LLM model to assess the integrity of the generated question. Experimental results demonstrate the effectiveness of both systems. The findings highlight the transformative potential of LLMs in educational assessment, reducing the burden on human resources and enabling scalable, cost-effective MCQ generation.",
        "author": "S\u00e9rgio Silva Mucciaccia; Thiago Meireles Paix\u00e3o; Filipe Wall Mutz; Claudine Santos Badue; Alberto Ferreira de Souza; Thiago Oliveira-Santos",
        "authorids": "/s/sergio-silva-mucciaccia/; /t/thiago-meireles-paixao/; /f/filipe-wall-mutz/; /c/claudine-santos-badue/; /a/alberto-ferreira-de-souza/; /t/thiago-oliveira-santos/",
        "bibtex": "@inproceedings{mucciaccia-etal-2025-automatic,\n    title = \"Automatic Multiple-Choice Question Generation and Evaluation Systems Based on {LLM}: A Study Case With University Resolutions\",\n    author = \"Mucciaccia, S{\\'e}rgio Silva  and\n      Meireles Paix{\\~a}o, Thiago  and\n      Wall Mutz, Filipe  and\n      Santos Badue, Claudine  and\n      Ferreira de Souza, Alberto  and\n      Oliveira-Santos, Thiago\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.154/\",\n    pages = \"2246--2260\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.154.pdf",
        "site": "https://aclanthology.org/2025.coling-main.154/",
        "pdf_size": 678260,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13879591386435278551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Universidade Federal do Esp\u00edrito Santo (UFES); Instituto Federal do Esp\u00edrito Santo (IFES); Universidade Federal do Esp\u00edrito Santo (UFES); Universidade Federal do Esp\u00edrito Santo (UFES); Universidade Federal do Esp\u00edrito Santo (UFES); Universidade Federal do Esp\u00edrito Santo (UFES)",
        "aff_domain": "ufes.br; ; ; ; ; ",
        "email": "ufes.br; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Universidade Federal do Esp\u00edrito Santo;Instituto Federal do Esp\u00edrito Santo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ufes.br;http://www.ifes.edu.br/",
        "aff_unique_abbr": "UFES;IFES",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "2025.coling-demos.20",
        "title": "Autonomous Machine Learning-Based Peer Reviewer Selection System",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The peer review process is essential for academic research, yet it faces challenges such as inefficiencies, biases, and limited access to qualified reviewers. This paper introduces an autonomous peer reviewer selection system that employs the Natural Language Processing (NLP) model to match submitted papers with expert reviewers independently of traditional journals and conferences. Our model performs competitively in comparison with the transformer-based state-of-the-art models while being 10 times faster at inference and 7 times smaller, which makes our platform highly scalable. Additionally, with our paper-reviewer matching model being trained on scientific papers from various academic fields, our system allows scholars from different backgrounds to benefit from this automation.",
        "author": "Nurmukhammed Aitymbetov; Dimitrios Zorbas",
        "authorids": "/n/nurmukhammed-aitymbetov/; /d/dimitrios-zorbas/",
        "bibtex": "@inproceedings{aitymbetov-zorbas-2025-autonomous,\n    title = \"Autonomous Machine Learning-Based Peer Reviewer Selection System\",\n    author = \"Aitymbetov, Nurmukhammed  and\n      Zorbas, Dimitrios\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.20/\",\n    pages = \"199--207\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.20.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.20/",
        "pdf_size": 461605,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YczBoI-mppEJ:scholar.google.com/&scioq=Autonomous+Machine+Learning-Based+Peer+Reviewer+Selection+System&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Nazarbayev University, School of Engineering & Digital Sciences, Astana, Kazakhstan; Nazarbayev University, School of Engineering & Digital Sciences, Astana, Kazakhstan",
        "aff_domain": "nu.edu.kz;nu.edu.kz",
        "email": "nu.edu.kz;nu.edu.kz",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nazarbayev University",
        "aff_unique_dep": "School of Engineering & Digital Sciences",
        "aff_unique_url": "https://www.nu.edu.kz",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Astana",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Kazakhstan"
    },
    {
        "id": "2025.coling-main.89",
        "title": "Awakening Augmented Generation: Learning to Awaken Internal Knowledge of Large Language Models for Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-Augmented-Generation and Generation-Augmented-Generation have been proposed to enhance the knowledge required for question answering with Large Language Models (LLMs) by leveraging richer context. However, the former relies on external resources, and both require incorporating explicit documents into the context, which increases execution costs and susceptibility to noise data during inference. Recent works indicate that LLMs model rich knowledge, but it is often not effectively activated and awakened. Inspired by this, we propose a novel knowledge-augmented framework, Awakening-Augmented-Generation (AAG), which mimics the human ability to answer questions using only thinking and recalling to compensate for knowledge gaps, thereby awaking relevant knowledge in LLMs without relying on external resources. AAG consists of two key components for awakening richer context. Explicit awakening fine-tunes a context generator to create a synthetic, compressed document that functions as symbolic context. Implicit awakening utilizes a hypernetwork to generate adapters based on the question and synthetic document, which are inserted into LLMs to serve as parameter context. Experimental results on three datasets demonstrate that AAG exhibits significant advantages in both open-domain and closed-book settings, as well as in out-of-distribution generalization. Our code will be available at https://github.com/Xnhyacinth/IAG.",
        "author": "Huanxuan Liao; Shizhu He; Yao Xu; Yuanzhe Zhang; Shengping Liu; Kang Liu; Jun Zhao",
        "authorids": "/h/huanxuan-liao/; /s/shizhu-he/; /y/yao-xu/; /y/yuanzhe-zhang/; /s/shengping-liu/; /k/kang-liu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{liao-etal-2025-awakening,\n    title = \"Awakening Augmented Generation: Learning to Awaken Internal Knowledge of Large Language Models for Question Answering\",\n    author = \"Liao, Huanxuan  and\n      He, Shizhu  and\n      Xu, Yao  and\n      Zhang, Yuanzhe  and\n      Liu, Shengping  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.89/\",\n    pages = \"1333--1352\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.89.pdf",
        "site": "https://aclanthology.org/2025.coling-main.89/",
        "pdf_size": 1017720,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9151358671130333292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Unisound, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn; ;unisound.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn; ;unisound.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/Xnhyacinth/IAG",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;0;2;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Unisound",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.691",
        "title": "BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the recent success of two-stage prototypical networks in few-shot named entity recognition (NER), challenges such as over/under-detected false spans in the span detection stage and unaligned entity prototypes in the type classification stage persist. Additionally, LLMs have not proven to be effective few-shot information extractors in general. In this paper, we propose an approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to address these issues. We introduce a boundary-aware contrastive learning strategy to enhance the LLM\u2019s ability to perceive entity boundaries for generalized entity spans. Additionally, we utilize LoRAHub to align information from the target domain to the source domain, thereby enhancing adaptive cross-domain classification capabilities. Extensive experiments across various benchmarks demonstrate that our framework outperforms prior methods, validating its effectiveness. In particular, the proposed strategies demonstrate effectiveness across a range of LLM architectures. The code and data are released on https://github.com/UESTC-GQJ/BANER.",
        "author": "Quanjiang Guo; Yihong Dong; Ling Tian; Zhao Kang; Yu Zhang; Sijie Wang",
        "authorids": "/q/quanjiang-guo/; /y/yihong-dong/; /l/ling-tian/; /z/zhao-kang/; /y/yu-zhang/; /s/sijie-wang/",
        "bibtex": "@inproceedings{guo-etal-2025-baner,\n    title = \"{BANER}: Boundary-Aware {LLM}s for Few-Shot Named Entity Recognition\",\n    author = \"Guo, Quanjiang  and\n      Dong, Yihong  and\n      Tian, Ling  and\n      Kang, Zhao  and\n      Zhang, Yu  and\n      Wang, Sijie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.691/\",\n    pages = \"10375--10389\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.691.pdf",
        "site": "https://aclanthology.org/2025.coling-main.691/",
        "pdf_size": 1856714,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2082773133377203070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; Harbin Institute of Technology, Shenzhen; Nanyang Technological University, Singapore",
        "aff_domain": "163.com;163.com;uestc.edu.cn;uestc.edu.cn;gmail.com;e.ntu.edu.sg",
        "email": "163.com;163.com;uestc.edu.cn;uestc.edu.cn;gmail.com;e.ntu.edu.sg",
        "github": "https://github.com/UESTC-GQJ/BANER",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;2",
        "aff_unique_norm": "University of Electronic Science and Technology of China;Harbin Institute of Technology;Nanyang Technological University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uestc.edu.cn;http://en.hhit.edu.cn/;https://www.ntu.edu.sg",
        "aff_unique_abbr": "UESTC;HIT;NTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-main.409",
        "title": "BERT-based Classical Arabic Poetry Authorship Attribution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study introduces a novel computational approach to authorship attribution (AA) in Arabic poetry, using the entire Classical Arabic Poetry corpus for the first time and offering a direct analysis of real cases of misattribution. AA in Arabic poetry has been a significant issue since the 9th century, particularly due to the loss of pre-Islamic poetry and the misattribution of post-Islamic works to earlier poets. While previous research has predominantly employed qualitative methods, this study uses computational techniques to address these challenges. The corpus was scraped from online sources and enriched with manually curated Date of Death (DoD) information to overcome the problematic traditional sectioning. Additionally, we applied Embedded Topic Modeling (ETM) to label each poem with its topic contributions, further enhancing the dataset\u2019s value. An ensemble model based on CAMeLBERT was developed and tested across three dimensions: topic, number of poets, and number of training examples. After parameter optimization, the model achieved F1 scores ranging from 0.97 to 1.0. The model was also applied to four pre-Islamic misattribution cases, producing results consistent with historical and literary studies.",
        "author": "Lama Alqurashi; Serge Sharoff; Janet Watson; Jacob Blakesley",
        "authorids": "/l/lama-alqurashi/; /s/serge-sharoff/; /j/janet-watson/; /j/jacob-blakesley/",
        "bibtex": "@inproceedings{alqurashi-etal-2025-bert,\n    title = \"{BERT}-based Classical {A}rabic Poetry Authorship Attribution\",\n    author = \"Alqurashi, Lama  and\n      Sharoff, Serge  and\n      Watson, Janet  and\n      Blakesley, Jacob\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.409/\",\n    pages = \"6105--6119\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.409.pdf",
        "site": "https://aclanthology.org/2025.coling-main.409/",
        "pdf_size": 917780,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1077335535385869573&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-industry.40",
        "title": "BackMATH: Towards Backward Reasoning for Solving Math Problems Step by Step",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large language models (LLMs) have achieved impressive results in reasoning, particularly in multi-step reasoning tasks. However, when faced with more complex mathematical problems, the performance of LLMs drops significantly. To address this issue, in this paper, we propose a backward reasoning dataset, BackMATH-Data. The dataset comprises approximately 14K backward reasoning problems and 100K reasoning steps. It follows a result-oriented approach, to construct backward reasoning problems by swapping the reasoning results with specific solving conditions in the original problems.Additionally, we introduce Backward-reasoning Process-supervision Reward Model (BackPRM) and BackMATH-LLM. BackPRM supervises the quality of the generated backward reasoning problems, while BackMATH-LLM is designed for mathematical reasoning. BackMATH-LLM is fine-tuned and enhanced through reinforcement learning by supervising the quality of backward reasoning problems and by providing feedback on reasoning steps, thereby improving the mathematical reasoning capabilities of LLMs.Extensive experiments demonstrate that our model achieves an accuracy of 68.1% on the GSM8K dataset and 21.9% on the MATH dataset, exceeding the SOTA by 1.6% and 2.1% respectively.",
        "author": "Shaowei Zhang; Deyi Xiong",
        "authorids": "/s/shaowei-zhang/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{zhang-xiong-2025-backmath,\n    title = \"{B}ack{MATH}: Towards Backward Reasoning for Solving Math Problems Step by Step\",\n    author = \"Zhang, Shaowei  and\n      Xiong, Deyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.40/\",\n    pages = \"466--482\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.40.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.40/",
        "pdf_size": 701436,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7088324798291284170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "TJUNLP Lab, College of Intelligence and Computing, Tianjin University; TJUNLP Lab, College of Intelligence and Computing, Tianjin University",
        "aff_domain": "tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "TJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.318",
        "title": "BasqBBQ: A QA Benchmark for Assessing Social Biases in LLMs for Basque, a Low-Resource Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rise of pre-trained language models has revolutionized natural language processing (NLP) tasks, but concerns about the propagation of social biases in these models remain, particularly in under-resourced languages like Basque. This paper introduces BasqBBQ, the first benchmark designed to assess social biases in Basque across eight domains, using a multiple-choice question-answering (QA) task. We evaluate various autoregressive large language models (LLMs), including multilingual and those adapted for Basque, to analyze both their accuracy and bias transmission. Our results show that while larger models generally achieve better accuracy, ambiguous cases remain challenging. In terms of bias, larger models exhibit lower negative bias. However, high negative bias persists in specific categories such as Disability Status, Age and Physical Appearance, especially in ambiguous contexts. Conversely, categories such as Sexual Orientation, Gender Identity, and Race/Ethnicity show the least bias in ambiguous contexts. The continual pre-training based adaptation process for Basque has a limited impact on bias when compared with English. This work represents a key step toward creating more ethical LLMs for low-resource languages.",
        "author": "Muitze Zulaika; Xabier Saralegi",
        "authorids": "/m/muitze-zulaika/; /x/xabier-saralegi/",
        "bibtex": "@inproceedings{saralegi-zulaika-2025-basqbbq,\n    title = \"{B}asq{BBQ}: A {QA} Benchmark for Assessing Social Biases in {LLM}s for {B}asque, a Low-Resource Language\",\n    author = \"Zulaika, Muitze  and\n      Saralegi, Xabier\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.318/\",\n    pages = \"4753--4767\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.318.pdf",
        "site": "https://aclanthology.org/2025.coling-main.318/",
        "pdf_size": 909353,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J1gNDwUKWe0J:scholar.google.com/&scioq=BasqBBQ:+A+QA+Benchmark+for+Assessing+Social+Biases+in+LLMs+for+Basque,+a+Low-Resource+Language&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Orai NLP Technologies; Orai NLP Technologies",
        "aff_domain": "orai.eus;orai.eus",
        "email": "orai.eus;orai.eus",
        "github": "https://github.com/orai-nlp/BasqBBQ",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Orai NLP Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2025.coling-demos.7",
        "title": "BeefBot: Harnessing Advanced LLM and RAG Techniques for Providing Scientific and Technology Solutions to Beef Producers",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We propose BeefBot, a LLM-powered chatbot designed for beef producers. It retrieves the latest agricultural technologies (AgTech), practices and scientific insights to provide rapid, domain-specific advice, helping to address on-farm challenges effectively. While generic Large Language Models (LLMs) like ChatGPT are useful for information retrieval, they often hallucinate and fall short in delivering tailored solutions to the specific needs of beef producers, including breed-specific strategies, operational practices, and regional adaptations. There are two common methods for incorporating domain-specific data in LLM applications: Retrieval-Augmented Generation (RAG) and fine-tuning. However, their respective advantages and disadvantages are not well understood. Therefore, we implement a pipeline to apply RAG and fine-tuning using an open-source LLM in BeefBot and evaluate the trade-offs. By doing so, we are able to select the best combination as the backend of BeefBot, delivering actionable recommendations that enhance productivity and sustainability for beef producers with fewer hallucinations. Key benefits of BeefBot include its accessibility as a web-based platform compatible with any browser, continuously updated knowledge through RAG, confidential assurance via local deployment, and a user-friendly experience facilitated by an interactive website. The demo of the BeefBot can be accessed at https://www.youtube.com/watch?v=r7mde1EOG4o.",
        "author": "Zhihao Zhang; Carrie-Ann Wilson; Rachel Hay; Yvette Everingham; Usman Naseem",
        "authorids": "/z/zhihao-zhang/; /c/carrie-ann-wilson/; /r/rachel-hay/; /y/yvette-everingham/; /u/usman-naseem/",
        "bibtex": "@inproceedings{zhang-etal-2025-beefbot,\n    title = \"{B}eef{B}ot: Harnessing Advanced {LLM} and {RAG} Techniques for Providing Scientific and Technology Solutions to Beef Producers\",\n    author = \"Zhang, Zhihao  and\n      Wilson, Carrie-Ann  and\n      Hay, Rachel  and\n      Everingham, Yvette  and\n      Naseem, Usman\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.7/\",\n    pages = \"54--62\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.7.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.7/",
        "pdf_size": 2585837,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14151815011356752864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "James Cook University, Australia; James Cook University, Australia; James Cook University, Australia; James Cook University, Australia; Macquarie University, Australia",
        "aff_domain": "jcu.edu.au;jcu.edu.au;jcu.edu.au;jcu.edu.au;mq.edu.au",
        "email": "jcu.edu.au;jcu.edu.au;jcu.edu.au;jcu.edu.au;mq.edu.au",
        "github": "",
        "project": "https://www.youtube.com/watch?v=r7mde1EOG4o",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "James Cook University;Macquarie University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jcu.edu.au;https://www.mq.edu.au",
        "aff_unique_abbr": "JCU;MQ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2025.coling-main.391",
        "title": "Benchmark Creation for Aspect-Based Sentiment Analysis in Low-Resource Odia Language and Evaluation through Fine-Tuning of Multilingual Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid growth of online product reviews spurs significant interest in Aspect-Based Sentiment Analysis (ABSA), which involves identifying aspect terms and their associated sentiment polarity. While ABSA is widely studied in resource-rich languages like English, Chinese, and Spanish, it remains underexplored in low-resource languages such as Odia. To address this gap, we create a reliable resource for aspect-based sentiment analysis in Odia. The dataset is annotated for two specific tasks: Aspect Term Extraction (ATE) and Aspect Polarity Classification (APC), spanning seven domains and aligned with the SemEval-2014 benchmark. Furthermore, we employ an ensemble data augmentation approach combining back-translation with a fine-tuned T5 paraphrase generation model to enhance the dataset and apply a semantic similarity filter using a Universal Sentence Encoder (USE) to remove low-quality data and ensure a balanced distribution of sample difficulty in the newly augmented dataset. Finally, we validate our dataset by fine-tuning multilingual pre-trained models, XLM-R and IndicBERT, on ATE and APC tasks. Additionally, we use three classical baseline models to evaluate the quality of the proposed dataset for these tasks. We hope the Odia dataset will spur more work for the ABSA task.",
        "author": "Lipika Dewangan; Zoyah Afsheen Sayeed; Chandresh Maurya",
        "authorids": "/l/lipika-dewangan/; /z/zoyah-afsheen-sayeed/; /c/chandresh-maurya/",
        "bibtex": "@inproceedings{dewangan-etal-2025-benchmark,\n    title = \"Benchmark Creation for Aspect-Based Sentiment Analysis in Low-Resource {O}dia Language and Evaluation through Fine-Tuning of Multilingual Models\",\n    author = \"Dewangan, Lipika  and\n      Sayeed, Zoyah Afsheen  and\n      Maurya, Chandresh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.391/\",\n    pages = \"5863--5869\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.391.pdf",
        "site": "https://aclanthology.org/2025.coling-main.391/",
        "pdf_size": 385270,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kjWlrs1gvxoJ:scholar.google.com/&scioq=Benchmark+Creation+for+Aspect-Based+Sentiment+Analysis+in+Low-Resource+Odia+Language+and+Evaluation+through+Fine-Tuning+of+Multilingual+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Indian Institute of Technology, Indore, Madhya Pradesh, India 453552; Kalinga Institute of Industrial Technology, Bhubaneswar, Odisha, India 751024; Indian Institute of Technology, Indore, Madhya Pradesh, India 453552",
        "aff_domain": "iiti.ac.in;kiit.ac.in;iiti.ac.in",
        "email": "iiti.ac.in;kiit.ac.in;iiti.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Indore;Kalinga Institute of Industrial Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iiti.ac.in;",
        "aff_unique_abbr": "IIT Indore;KIIT",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Indore;Bhubaneswar",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.223",
        "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs). We utilize a multi-agent system to reframe new evolving instances with high confidence that extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, shortcut biases and probing their problem-solving sub-abilities. With this framework, we extend datasets across general and specific tasks, through various iterations. Experimental results show a performance decline in most LLMs against their original results under scalable and robust evaluations, offering a more accurate reflection of model capabilities alongside our fine-grained evaluation. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks. We hope this framework contributes the research community for continuously evolving benchmarks alongside LLM development.",
        "author": "Siyuan Wang; Zhuohan Long; Zhihao Fan; Xuanjing Huang; Zhongyu Wei",
        "authorids": "/s/siyuan-wang/; /z/zhuohan-long/; /z/zhihao-fan/; /x/xuan-jing-huang/; /z/zhongyu-wei/",
        "bibtex": "@inproceedings{wang-etal-2025-benchmark,\n    title = \"Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic {LLM} Evaluation\",\n    author = \"Wang, Siyuan  and\n      Long, Zhuohan  and\n      Fan, Zhihao  and\n      Huang, Xuanjing  and\n      Wei, Zhongyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.223/\",\n    pages = \"3310--3328\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.223.pdf",
        "site": "https://aclanthology.org/2025.coling-main.223/",
        "pdf_size": 714183,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11495157436607640022&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California; Fudan University; Alibaba Inc; Fudan University; Fudan University",
        "aff_domain": "usc.edu;m.fudan.edu.cn; ; ; ",
        "email": "usc.edu;m.fudan.edu.cn; ; ; ",
        "github": "https://github.com/NanshineLoong/Self-Evolving-Benchmark.git",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;1;1",
        "aff_unique_norm": "University of Southern California;Fudan University;Alibaba Group Holding Limited",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.fudan.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "USC;Fudan;Alibaba",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2025.coling-industry.11",
        "title": "Best Practices for Distilling Large Language Models into BERT for Web Search Ranking",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recent studies have highlighted the significant potential of Large Language Models (LLMs) as zero-shot relevance rankers. These methods predominantly utilize prompt learning to assess the relevance between queries and documents by generating a ranked list of potential documents. Despite their promise, the substantial costs associated with LLMs pose a significant challenge for their direct implementation in commercial search systems. To overcome this barrier and fully exploit the capabilities of LLMs for text ranking, we explore techniques to transfer the ranking expertise of LLMs to a more compact model similar to BERT, using a ranking loss to enable the deployment of less resource-intensive models. Specifically, we enhance the training of LLMs through Continued Pre-Training, taking the query as input and the clicked title and summary as output. We then proceed with supervised fine-tuning of the LLM using a rank loss, assigning the final token as a representative of the entire sentence. Given the inherent characteristics of autoregressive language models, only the final token </s> can encapsulate all preceding tokens. Additionally, we introduce a hybrid point-wise and margin MSE loss to transfer the ranking knowledge from LLMs to smaller models like BERT. This method creates a viable solution for environments with strict resource constraints. Both offline and online evaluations have confirmed the efficacy of our approach, and our model has been successfully integrated into a commercial web search engine as of February 2024.",
        "author": "Dezhi Ye; Junwei Hu; Jiabin Fan; Bowen Tian; Jie Liu; Haijin Liang; Jin Ma",
        "authorids": "/d/dezhi-ye/; /j/junwei-hu/; /j/jiabin-fan/; /b/bowen-tian/; /j/jie-liu/; /h/haijin-liang/; /j/jin-ma/",
        "bibtex": "@inproceedings{ye-etal-2025-best,\n    title = \"Best Practices for Distilling Large Language Models into {BERT} for Web Search Ranking\",\n    author = \"Ye, Dezhi  and\n      Hu, Junwei  and\n      Fan, Jiabin  and\n      Tian, Bowen  and\n      Liu, Jie  and\n      Liang, Haijin  and\n      Ma, Jin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.11/\",\n    pages = \"128--135\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.11.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.11/",
        "pdf_size": 501477,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:vUfX5ivbrnUJ:scholar.google.com/&scioq=Best+Practices+for+Distilling+Large+Language+Models+into+BERT+for+Web+Search+Ranking&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Tencent; Tencent; Tencent; Tencent; Tencent; Tencent; Tencent",
        "aff_domain": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "email": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Tencent Holdings Limited",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.725",
        "title": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Open Named Entity Recognition (NER), which involves identifying arbitrary types of entities from arbitrary domains, remains challenging for Large Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on extensive NER data can boost their performance. However, training directly on existing datasets neglects their inconsistent entity definitions and redundant data, limiting LLMs to dataset-specific learning and hindering out-of-domain adaptation. To address this, we present B2NERD, a compact dataset designed to guide LLMs\u2019 generalization in Open NER under a universal entity taxonomy. B2NERD is refined from 54 existing English and Chinese datasets using a two-step process. First, we detect inconsistent entity definitions across datasets and clarify them by distinguishable label names to construct a universal taxonomy of 400+ entity types. Second, we address redundancy using a data pruning strategy that selects fewer samples with greater category and semantic diversity. Comprehensive evaluation shows that B2NERD significantly enhances LLMs\u2019 Open NER capabilities. Our B2NER models, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3 out-of-domain benchmarks across 15 datasets and 6 languages. The data, models, and code are publicly available at https://github.com/UmeanNever/B2NER.",
        "author": "Yuming Yang; Wantong Zhao; Caishuang Huang; Junjie Ye; Xiao Wang; Huiyuan Zheng; Yang Nan; Yuran Wang; Xueying Xu; Kaixin Huang; Yunke Zhang; Tao Gui; Qi Zhang; Xuanjing Huang",
        "authorids": "/y/yuming-yang/; /w/wantong-zhao/; /c/caishuang-huang/; /j/junjie-ye/; /x/xiao-wang/; /h/huiyuan-zheng/; /y/yang-nan/; /y/yuran-wang/; /x/xueying-xu/; /k/kaixin-huang/; /y/yunke-zhang/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{yang-etal-2025-beyond,\n    title = \"Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition\",\n    author = \"Yang, Yuming  and\n      Zhao, Wantong  and\n      Huang, Caishuang  and\n      Ye, Junjie  and\n      Wang, Xiao  and\n      Zheng, Huiyuan  and\n      Nan, Yang  and\n      Wang, Yuran  and\n      Xu, Xueying  and\n      Huang, Kaixin  and\n      Zhang, Yunke  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.725/\",\n    pages = \"10902--10923\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.725.pdf",
        "site": "https://aclanthology.org/2025.coling-main.725/",
        "pdf_size": 2254144,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6420937903774163632&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; Honor Device Co., Ltd; Honor Device Co., Ltd; Honor Device Co., Ltd; Honor Device Co., Ltd; Institute of Modern Languages and Linguistics, Fudan University+Pengcheng Laboratory; School of Computer Science, Fudan University+Research Institute of Intelligent Complex Systems, Fudan University+Shanghai Key Laboratory of Intelligent Information Processing; School of Computer Science, Fudan University+Research Institute of Intelligent Complex Systems, Fudan University+Shanghai Key Laboratory of Intelligent Information Processing",
        "aff_domain": "m.fudan.edu.cn; ; ; ; ; ; ; ; ; ; ;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn; ; ; ; ; ; ; ; ; ; ;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "https://github.com/UmeanNever/B2NER",
        "project": "",
        "author_num": 14,
        "aff_unique_index": "0;0;0;0;0;0;0;1;1;1;1;0+2;0+0+3;0+0+3",
        "aff_unique_norm": "Fudan University;Honor Device Co., Ltd;Pengcheng Laboratory;Shanghai Key Laboratory of Intelligent Information Processing",
        "aff_unique_dep": "School of Computer Science;;;Intelligent Information Processing",
        "aff_unique_url": "https://www.fudan.edu.cn;;;",
        "aff_unique_abbr": "Fudan;;;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.719",
        "title": "Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chain-of-Thought (CoT) has been a widely adopted prompting method, eliciting impressive reasoning abilities of Large Language Models (LLMs). Inspired by the sequential thought structure of CoT, a number of Chain-of-X (CoX) methods have been developed to address challenges across diverse domains and tasks. In this paper, we provide a comprehensive survey of Chain-of-X methods for LLMs in different contexts. Specifically, we categorize them by taxonomies of nodes, i.e., the X in CoX, and application tasks. We also discuss the findings and implications of existing CoX methods, as well as potential future directions. Our survey aims to serve as a detailed and up-to-date resource for researchers seeking to apply the idea of CoT to broader scenarios.",
        "author": "Yu Xia; Rui Wang; Xu Liu; Mingyan Li; Tong Yu; Xiang Chen; Julian McAuley; Shuai Li",
        "authorids": "/y/yu-xia/; /r/rui-wang/; /x/xu-liu/; /m/mingyan-li/; /t/tong-yu/; /x/xiang-chen/; /j/julian-mcauley/; /s/shuai-li/",
        "bibtex": "@inproceedings{xia-etal-2025-beyond,\n    title = \"Beyond Chain-of-Thought: A Survey of Chain-of-{X} Paradigms for {LLM}s\",\n    author = \"Xia, Yu  and\n      Wang, Rui  and\n      Liu, Xu  and\n      Li, Mingyan  and\n      Yu, Tong  and\n      Chen, Xiang  and\n      McAuley, Julian  and\n      Li, Shuai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.719/\",\n    pages = \"10795--10809\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.719.pdf",
        "site": "https://aclanthology.org/2025.coling-main.719/",
        "pdf_size": 782775,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3749905221247271705&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University+UC San Diego; Duke University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Adobe Research; Adobe Research; UC San Diego+Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "ucsd.edu;duke.edu;sjtu.edu.cn;sjtu.edu.cn;adobe.com;adobe.com;ucsd.edu;sjtu.edu.cn",
        "email": "ucsd.edu;duke.edu;sjtu.edu.cn;sjtu.edu.cn;adobe.com;adobe.com;ucsd.edu;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;0;0;3;3;1+0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of California, San Diego;Duke University;Adobe",
        "aff_unique_dep": ";;;Adobe Research",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.ucsd.edu;https://www.duke.edu;https://research.adobe.com",
        "aff_unique_abbr": "SJTU;UCSD;Duke;Adobe",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0+1;1;0;0;1;1;1+0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.578",
        "title": "Beyond Dataset Creation: Critical View of Annotation Variation and Bias Probing of a Dataset for Online Radical Content Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The proliferation of radical content on online platforms poses significant risks, including inciting violence and spreading extremist ideologies. Despite ongoing research, existing datasets and models often fail to address the complexities of multilingual and diverse data. To bridge this gap, we introduce a publicly available multilingual dataset annotated with radicalization levels, calls for action, and named entities in English, French, and Arabic. This dataset is pseudonymized to protect individual privacy while preserving contextual information. Beyond presenting our freely available dataset, we analyze the annotation process, highlighting biases and disagreements among annotators and their implications for model performance. Additionally, we use synthetic data to investigate the influence of socio-demographic traits on annotation patterns and model predictions. Our work offers a comprehensive examination of the challenges and opportunities in building robust datasets for radical content detection, emphasizing the importance of fairness and transparency in model development. The Counter dataset is available at https://gitlab.inria.fr/ariabi/counter-dataset-public.",
        "author": "Arij Riabi; Virginie Mouilleron; Menel Mahamdi; Wissam Antoun; Djam\u00e9 Seddah",
        "authorids": "/a/arij-riabi/; /v/virginie-mouilleron/; /m/menel-mahamdi/; /w/wissam-antoun/; /d/djame-seddah/",
        "bibtex": "@inproceedings{riabi-etal-2025-beyond,\n    title = \"Beyond Dataset Creation: Critical View of Annotation Variation and Bias Probing of a Dataset for Online Radical Content Detection\",\n    author = \"Riabi, Arij  and\n      Mouilleron, Virginie  and\n      Mahamdi, Menel  and\n      Antoun, Wissam  and\n      Seddah, Djam{\\'e}\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.578/\",\n    pages = \"8640--8663\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.578.pdf",
        "site": "https://aclanthology.org/2025.coling-main.578/",
        "pdf_size": 1106003,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8K1hILSPHa8J:scholar.google.com/&scioq=Beyond+Dataset+Creation:+Critical+View+of+Annotation+Variation+and+Bias+Probing+of+a+Dataset+for+Online+Radical+Content+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Inria, Paris; Inria, Paris; Inria, Paris; Inria, Paris; Inria, Paris",
        "aff_domain": "inria.fr;inria.fr;inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Inria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.470",
        "title": "Beyond Discrete Personas: Personality Modeling Through Journal Intensive Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have significantly improved personalized conversational capabilities. However, existing datasets like Persona Chat, Synthetic Persona Chat, and Blended Skill Talk rely on static, predefined personas. This approach often results in dialogues that fail to capture human personalities\u2019 fluid and evolving nature. To overcome these limitations, we introduce a novel dataset with around 400,000 dialogues and a framework for generating personalized conversations using long-form journal entries from Reddit. Our approach clusters journal entries for each author and filters them by selecting the most representative cluster, ensuring that the retained entries best reflect the author\u2019s personality. We further refine the data by capturing the Big Five personality traits\u2014openness, conscientiousness, extraversion, agreeableness, and neuroticism\u2014ensuring that dialogues authentically reflect an individual\u2019s personality. Using Llama 3 70B, we generate high-quality, personality-rich dialogues grounded in these journal entries. Fine-tuning models on this dataset leads to an 11% improvement in capturing personality traits on average, outperforming existing approaches in generating more coherent and personality-driven dialogues.",
        "author": "Sayantan Pal; Souvik Das; Rohini K. Srihari",
        "authorids": "/s/sayantan-pal/; /s/souvik-das/; /r/rohini-k-srihari/",
        "bibtex": "@inproceedings{pal-etal-2025-beyond,\n    title = \"Beyond Discrete Personas: Personality Modeling Through Journal Intensive Conversations\",\n    author = \"Pal, Sayantan  and\n      Das, Souvik  and\n      Srihari, Rohini K.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.470/\",\n    pages = \"7055--7074\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.470.pdf",
        "site": "https://aclanthology.org/2025.coling-main.470/",
        "pdf_size": 3065041,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_Ts0a1zU9EgJ:scholar.google.com/&scioq=Beyond+Discrete+Personas:+Personality+Modeling+Through+Journal+Intensive+Conversations&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "State University of New York at Buffalo; State University of New York at Buffalo; State University of New York at Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "State University of New York at Buffalo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.buffalo.edu",
        "aff_unique_abbr": "SUNY Buffalo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Buffalo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.641",
        "title": "Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken Vocabulary?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Word frequency is a key variable in psycholinguistics, useful for modeling human familiarity with words even in the era of large language models (LLMs). Frequency in film subtitles has proved to be a particularly good approximation of everyday language exposure. For many languages, however, film subtitles are not easily available, or are overwhelmingly translated from English. We demonstrate that frequencies extracted from carefully processed YouTube subtitles provide an approximation comparable to, and often better than, the best currently available resources. Moreover, they are available for languages for which a high-quality subtitle or speech corpus does not exist. We use YouTube subtitles to construct frequency norms for five diverse languages, Chinese, English, Indonesian, Japanese, and Spanish, and evaluate their correlation with lexical decision time, word familiarity, and lexical complexity. In addition to being strongly correlated with two psycholinguistic variables, a simple linear regression on the new frequencies achieves a new high score on a lexical complexity prediction task in English and Japanese, surpassing both models trained on film subtitle frequencies and the LLM GPT-4. We publicly release our code, the frequency lists, fastText word embeddings, and statistical language models.",
        "author": "Adam Nohejl; Frederikus Hudi; Eunike Andriani Kardinata; Shintaro Ozaki; Maria Angelica Riera Machin; Hongyu Sun; Justin Vasselli; Taro Watanabe",
        "authorids": "/a/adam-nohejl/; /f/frederikus-hudi/; /e/eunike-andriani-kardinata/; /s/shintaro-ozaki/; /m/maria-angelica-riera-machin/; /h/hongyu-sun/; /j/justin-vasselli/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{nohejl-etal-2025-beyond,\n    title = \"Beyond Film Subtitles: Is {Y}ou{T}ube the Best Approximation of Spoken Vocabulary?\",\n    author = \"Nohejl, Adam  and\n      Hudi, Frederikus  and\n      Kardinata, Eunike Andriani  and\n      Ozaki, Shintaro  and\n      Riera Machin, Maria Angelica  and\n      Sun, Hongyu  and\n      Vasselli, Justin  and\n      Watanabe, Taro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.641/\",\n    pages = \"9566--9585\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.641.pdf",
        "site": "https://aclanthology.org/2025.coling-main.641/",
        "pdf_size": 444230,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11265077690801282917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;is.naist.jp;is.naist.jp;naist.ac.jp;naist.ac.jp;naist.ac.jp;is.naist.jp;is.naist.jp",
        "email": "is.naist.jp;is.naist.jp;is.naist.jp;naist.ac.jp;naist.ac.jp;naist.ac.jp;is.naist.jp;is.naist.jp",
        "github": "https://github.com/naist-nlp/tubelex",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nist.go.jp",
        "aff_unique_abbr": "NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.443",
        "title": "Beyond Surprisal: A Dual Metric Framework for Lexical Skill Acquisition in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Many studies have explored when and how LLMs learn to use specific words, primarily by examining their learning curves. While these curves capture a model\u2019s capacity to use words correctly in context, they often neglect the equally important skill of avoiding incorrect usage. In this paper, we introduce a new metric, anti-surprisal, which measures a model\u2019s capacity to refrain from using words in inappropriate or unexpected contexts. By examining both correct usage and error avoidance, we offer a more comprehensive perspective on the learning dynamics of LLMs.",
        "author": "Nazanin Shafiabadi; Guillaume Wisniewski",
        "authorids": "/n/nazanin-shafiabadi/; /g/guillaume-wisniewski/",
        "bibtex": "@inproceedings{shafiabadi-wisniewski-2025-beyond,\n    title = \"Beyond Surprisal: A Dual Metric Framework for Lexical Skill Acquisition in {LLM}s\",\n    author = \"Shafiabadi, Nazanin  and\n      Wisniewski, Guillaume\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.443/\",\n    pages = \"6636--6641\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.443.pdf",
        "site": "https://aclanthology.org/2025.coling-main.443/",
        "pdf_size": 242583,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17737456426580680985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Sorbonne Universit\u00e9, ISIR, 75013 Paris, France; Universit\u00e9 Paris Cit\u00e9, LLF, CNRS, 75013 Paris, France",
        "aff_domain": "isir.upmc.fr;u-paris.fr",
        "email": "isir.upmc.fr;u-paris.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Sorbonne Universit\u00e9;Universit\u00e9 Paris Cit\u00e9",
        "aff_unique_dep": "Institut des Syst\u00e8mes Intelligents et de Robotique (ISIR);LLF",
        "aff_unique_url": "https://www.sorbonne-universite.fr;https://www.universite-paris.fr",
        "aff_unique_abbr": "Sorbonne U;UPC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-industry.6",
        "title": "Beyond Visual Understanding Introducing PARROT-360V for Vision Language Model Benchmarking",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Current benchmarks for evaluating Vision Language Models (VLMs) often fall short in thoroughly assessing these models\u2019 abilities to understand and process complex visual and textual content. They typically focus on simple tasks that do not require deep reasoning or the integration of multiple data modalities to solve an original problem. To address this gap, we introduce the PARROT-360V Benchmark, a novel and comprehensive benchmark featuring 2487 challenging visual puzzles designed to test VLMs on complex visual reasoning tasks. We evaluated leading models\u2014GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro\u2014using PARROT-360V to assess their capabilities in combining visual clues with language skills to solve tasks in a manner akin to human problem-solving. Our findings reveal a notable performance gap: state-of-the-art models scored between 28% to 56% on our benchmark, significantly lower than their performance on popular benchmarks. This underscores the limitations of current VLMs in handling complex, multi-step reasoning tasks and highlights the need for more robust evaluation frameworks to advance the field.",
        "author": "Harsha Vardhan Khurdula; Basem Rizk; Indus Khaitan",
        "authorids": "/h/harsha-vardhan-khurdula/; /b/basem-rizk/; /i/indus-khaitan/",
        "bibtex": "@inproceedings{khurdula-etal-2025-beyond,\n    title = \"Beyond Visual Understanding Introducing {PARROT}-360{V} for Vision Language Model Benchmarking\",\n    author = \"Khurdula, Harsha Vardhan  and\n      Rizk, Basem  and\n      Khaitan, Indus\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.6/\",\n    pages = \"68--75\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.6.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.6/",
        "pdf_size": 637804,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_nWibeMQw9AJ:scholar.google.com/&scioq=Beyond+Visual+Understanding+Introducing+PARROT-360V+for+Vision+Language+Model+Benchmarking&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.78",
        "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, large language models (LLMs) have shown exceptional capabilities across various natural language processing (NLP) tasks. However, such impressive performance often comes with the trade-off of an increased parameter size, posing significant challenges for widespread deployment. Knowledge distillation (KD) provides a solution by transferring knowledge from a large teacher model to a smaller student model. In this paper, we explore the task-specific distillation of LLMs at the logit level. Our investigation reveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail distribution than those from vision models, with hidden \u201cnoise\u201d in the long tail affecting distillation performance. Furthermore, existing logits distillation methods often struggle to effectively utilize the internal ranking information from the logits. To address these, we propose the Bi-directional Logits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by utilizing only top-k teacher and student logits, and leverages the internal logits ranking information by constructing logits differences. To evaluate BiLD loss, we conduct comprehensive experiments on 13 datasets using two types of LLMs. Our results show that the BiLD loss, with only the top-8 logits, outperforms supervised fine-tuning (SFT), vanilla KL loss, and five other distillation methods from both NLP and CV fields.",
        "author": "Minchong Li; Feng Zhou; Xiaohui Song",
        "authorids": "/m/minchong-li/; /f/feng-zhou/; /x/xiaohui-song/",
        "bibtex": "@inproceedings{li-etal-2025-bild,\n    title = \"{B}i{LD}: Bi-directional Logits Difference Loss for Large Language Model Distillation\",\n    author = \"Li, Minchong  and\n      Zhou, Feng  and\n      Song, Xiaohui\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.78/\",\n    pages = \"1168--1182\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.78.pdf",
        "site": "https://aclanthology.org/2025.coling-main.78/",
        "pdf_size": 4669458,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6600180479222916000&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "KTH Royal Institute of Technology, Stockholm, Sweden + OPPO AI Center, Beijing, China; OPPO AI Center, Beijing, China; OPPO AI Center, Beijing, China",
        "aff_domain": "kth.se;oppo.com;oppo.com",
        "email": "kth.se;oppo.com;oppo.com",
        "github": "https://github.com/fpcsong/BiLD",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "KTH Royal Institute of Technology;OPPO AI Center",
        "aff_unique_dep": ";OPPO AI Center",
        "aff_unique_url": "https://www.kth.se;https://www.oppo.com",
        "aff_unique_abbr": "KTH;OPPO",
        "aff_campus_unique_index": "0+1;1;1",
        "aff_campus_unique": "Stockholm;Beijing",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "Sweden;China"
    },
    {
        "id": "2025.coling-main.190",
        "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The use of language models (LMs) has increased considerably in recent years, and the biases and stereotypes in training data that are reflected in the LM outputs are causing social problems. In this paper, inspired by the task arithmetic, we propose the \u201cBias Vector\u201d method for the mitigation of these LM biases. The Bias Vector method does not require manually created debiasing data. The three main steps of our approach involve: (1) continual training the pre-trained LMs on biased data using masked language modeling; (2) constructing the Bias Vector as the difference between the weights of the biased LMs and those of pre-trained LMs; and (3) subtracting the Bias Vector from the weights of the pre-trained LMs for debiasing. We evaluated the Bias Vector method on the SEAT across three LMs and confirmed an average improvement of 0.177 points. We demonstrated that the Bias Vector method does not degrade the LM performance on downstream tasks in the GLUE benchmark. In addition, we examined the impact of scaling factors, which control the magnitudes of Bias Vectors, with effect sizes on the SEAT and conducted a comprehensive evaluation of our debiased LMs across both the SEAT and GLUE benchmarks.",
        "author": "Daiki Shirafuji; Makoto Takenaka; Shinya Taguchi",
        "authorids": "/d/daiki-shirafuji/; /m/makoto-takenaka/; /s/shinya-taguchi/",
        "bibtex": "@inproceedings{shirafuji-etal-2025-bias,\n    title = \"Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach\",\n    author = \"Shirafuji, Daiki  and\n      Takenaka, Makoto  and\n      Taguchi, Shinya\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.190/\",\n    pages = \"2799--2813\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.190.pdf",
        "site": "https://aclanthology.org/2025.coling-main.190/",
        "pdf_size": 520542,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9949671282423480354&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "https://huggingface.co/datasets/McGill-NLP/stereoset",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.389",
        "title": "Biases in Large Language Model-Elicited Text: A Case Study in Natural Language Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We test whether NLP datasets created with Large Language Models (LLMs) contain annotation artifacts and social biases like NLP datasets elicited from crowd-source workers. We recreate a portion of the Stanford Natural Language Inference corpus using GPT-4, Llama-2 70b for Chat, and Mistral 7b Instruct. We train hypothesis-only classifiers to determine whether LLM-elicited NLI datasets contain annotation artifacts. Next, we use point-wise mutual information to identify the words in each dataset that are associated with gender, race, and age-related terms. On our LLM-generated NLI datasets, fine-tuned BERT hypothesis-only classifiers achieve between 86-96% accuracy. Our analyses further characterize the annotation artifacts and stereotypical biases in LLM-generated datasets.",
        "author": "Grace Proebsting; Adam Poliak",
        "authorids": "/g/grace-proebsting/; /a/adam-poliak/",
        "bibtex": "@inproceedings{proebsting-poliak-2025-biases,\n    title = \"Biases in Large Language Model-Elicited Text: A Case Study in Natural Language Inference\",\n    author = \"Proebsting, Grace  and\n      Poliak, Adam\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.389/\",\n    pages = \"5836--5851\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.389.pdf",
        "site": "https://aclanthology.org/2025.coling-main.389/",
        "pdf_size": 566426,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10733744963763566257&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Haverford College; Bryn Mawr College",
        "aff_domain": "haverford.edu;brynmawr.edu",
        "email": "haverford.edu;brynmawr.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Haverford College;Bryn Mawr College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.haverford.edu;https://www.brynmawr.edu",
        "aff_unique_abbr": "Haverford;BMC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.413",
        "title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and manually translated into English, and have not ever been publicly released, ensuring minimal contamination when evaluating Large Language Models with this dataset. A selection of current open-source and proprietary models are evaluated in a uniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset and on an equivalent subset of MMLU questions. Results show that (i) Smaller models not only perform worse than the largest models, but also degrade faster in Spanish than in English. The performance gap between both languages is negligible for the best models, but grows up to 37% for smaller models; (ii) Model ranking on UNED-ACCESS 2024 is almost identical (0.98 Pearson correlation) to the one obtained with MMLU (a similar, but publicly available benchmark), suggesting that contamination affects similarly to all models, and (iii) As in publicly available datasets, reasoning questions in UNED-ACCESS are more challenging for models of all sizes.",
        "author": "Eva S\u00e1nchez Salido; Roser Morante; Julio Gonzalo; Guillermo Marco; Jorge Carrillo-de-Albornoz; Laura Plaza; Enrique Amigo; Andr\u00e9s Fernandez Garc\u00eda; Alejandro Benito-Santos; Adri\u00e1n Ghajari Espinosa; Victor Fresno",
        "authorids": "/e/eva-sanchez-salido/; /r/roser-morante/; /j/julio-gonzalo/; /g/guillermo-marco/; /j/jorge-carrillo-de-albornoz/; /l/laura-plaza/; /e/enrique-amigo/; /a/andres-fernandez-garcia/; /a/alejandro-benito-santos/; /a/adrian-ghajari-espinosa/; /v/victor-fresno/",
        "bibtex": "@inproceedings{sanchez-salido-etal-2025-bilingual,\n    title = \"Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination\",\n    author = \"S{\\'a}nchez Salido, Eva  and\n      Morante, Roser  and\n      Gonzalo, Julio  and\n      Marco, Guillermo  and\n      Carrillo-de-Albornoz, Jorge  and\n      Plaza, Laura  and\n      Amigo, Enrique  and\n      Garc{\\'i}a, Andr{\\'e}s Fernandez  and\n      Benito-Santos, Alejandro  and\n      Ghajari Espinosa, Adri{\\'a}n  and\n      Fresno, Victor\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.413/\",\n    pages = \"6184--6200\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.413.pdf",
        "site": "https://aclanthology.org/2025.coling-main.413/",
        "pdf_size": 1268480,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3640778071180037831&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "",
        "project": "https://leaderboard.odesia.uned.es",
        "author_num": 11
    },
    {
        "id": "2025.coling-main.728",
        "title": "BinarySelect to Improve Accessibility of Black-Box Attack Research",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Adversarial text attack research is useful for testing the robustness of NLP models, however, the rise of transformers has greatly increased the time required to test attacks. Especially when researchers do not have access to adequate resources (e.g. GPUs). This can hinder attack research, as modifying one example for an attack can require hundreds of queries to a model, especially for black-box attacks. Often these attacks remove one token at a time to find the ideal one to change, requiring n queries (the length of the text) right away. We propose a more efficient selection method called BinarySelect which combines binary search and attack selection methods to greatly reduce the number of queries needed to find a token. We find that BinarySelect only needs log_2(n) * 2 queries to find the first token compared to n queries. We also test BinarySelect in an attack setting against 5 classifiers across 3 datasets and find a viable tradeoff between number of queries saved and attack effectiveness. For example, on the Yelp dataset, the number of queries is reduced by 32% (72 less) with a drop in attack effectiveness of only 5 points. We believe that BinarySelect can help future researchers study adversarial attacks and black-box problems more efficiently and opens the door for researchers with access to less resources.",
        "author": "Shatarupa Ghosh; Jonathan Rusert",
        "authorids": "/s/shatarupa-ghosh/; /j/jonathan-rusert/",
        "bibtex": "@inproceedings{ghosh-rusert-2025-binaryselect,\n    title = \"{B}inary{S}elect to Improve Accessibility of Black-Box Attack Research\",\n    author = \"Ghosh, Shatarupa  and\n      Rusert, Jonathan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.728/\",\n    pages = \"10960--10976\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.728.pdf",
        "site": "https://aclanthology.org/2025.coling-main.728/",
        "pdf_size": 825967,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:IatfGp_TFNEJ:scholar.google.com/&scioq=BinarySelect+to+Improve+Accessibility+of+Black-Box+Attack+Research&hl=en&as_sdt=0,14",
        "gs_version_total": 3,
        "aff": "Purdue University, Fort Wayne; Purdue University, Fort Wayne",
        "aff_domain": "gmail.com;pfw.edu",
        "email": "gmail.com;pfw.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Fort Wayne",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.289",
        "title": "Boosting Text-to-SQL through Multi-grained Error Identification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text-to-SQL is a technology that converts natural language questions into executable SQL queries, allowing users to query and manage relational databases more easily. In recent years, large language models have significantly advanced the development of text-to-SQL. However, existing methods often overlook validation of the generated results during the SQL generation process. Current error identification methods are mainly divided into self-correction approaches based on large models and feedback methods based on SQL execution, both of which have limitations. We categorize SQL errors into three main types: system errors, skeleton errors, and value errors, and propose a multi-grained error identification method. Experimental results demonstrate that this method can be integrated as a plugin into various methods, providing effective error identification and correction capabilities.",
        "author": "Bo Xu; Shufei Li; Hongyu Jing; Ming Du; Hui Song; Hongya Wang; Yanghua Xiao",
        "authorids": "/b/bo-xu/; /s/shufei-li/; /h/hongyu-jing/; /m/ming-du/; /h/hui-song/; /h/hongya-wang/; /y/yanghua-xiao/",
        "bibtex": "@inproceedings{xu-etal-2025-boosting,\n    title = \"Boosting Text-to-{SQL} through Multi-grained Error Identification\",\n    author = \"Xu, Bo  and\n      Li, Shufei  and\n      Jing, Hongyu  and\n      Du, Ming  and\n      Song, Hui  and\n      Wang, Hongya  and\n      Xiao, Yanghua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.289/\",\n    pages = \"4282--4292\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.289.pdf",
        "site": "https://aclanthology.org/2025.coling-main.289/",
        "pdf_size": 767395,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:HgX77QGoAMUJ:scholar.google.com/&scioq=Boosting+Text-to-SQL+through+Multi-grained+Error+Identification&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University",
        "aff_domain": "dhu.edu.cn;mail.dhu.edu.cn;mail.dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;fudan.edu.cn",
        "email": "dhu.edu.cn;mail.dhu.edu.cn;mail.dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Donghua University;Fudan University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science",
        "aff_unique_url": "https://www.dhu.edu.cn;https://www.fudan.edu.cn",
        "aff_unique_abbr": ";Fudan",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.499",
        "title": "Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The data and compute requirements of current language modeling technology pose challenges for the processing and analysis of low-resource languages. Declarative linguistic knowledge has the potential to partially bridge this data scarcity gap by providing models with useful inductive bias in the form of language-specific rules. In this paper, we propose a retrieval augmented generation (RAG) framework backed by a large language model (LLM) to correct the output of a smaller model for the linguistic task of morphological glossing. We leverage linguistic information to make up for the lack of data and trainable parameters, while allowing for inputs from written descriptive grammars interpreted and distilled through an LLM. The results demonstrate that significant leaps in performance and efficiency are possible with the right combination of: a) linguistic inputs in the form of grammars, b) the interpretive power of LLMs, and c) the trainability of smaller token classification networks. We show that a compact, RAG-supported model is highly effective in data-scarce settings, achieving a new state-of-the-art for this task and our target languages. Our work also offers documentary linguists a more reliable and more usable tool for morphological glossing by providing well-reasoned explanations and confidence scores for each output.",
        "author": "Bhargav Shandilya; Alexis Palmer",
        "authorids": "/b/bhargav-shandilya/; /a/alexis-palmer/",
        "bibtex": "@inproceedings{shandilya-palmer-2025-boosting,\n    title = \"Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation\",\n    author = \"Shandilya, Bhargav  and\n      Palmer, Alexis\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.499/\",\n    pages = \"7470--7483\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.499.pdf",
        "site": "https://aclanthology.org/2025.coling-main.499/",
        "pdf_size": 431291,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4883927227754094639&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Colorado Boulder; University of Colorado Boulder",
        "aff_domain": "colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.326",
        "title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, Large language models (LLMs) have revolutionized Natural Language Processing (NLP). Pretrained LLMs, due to limited training context size, struggle with handling long token sequences, limiting their performance on various downstream tasks. Current solutions toward long context modeling often employ multi-stage continual pertaining, which progressively increases the effective context length through several continual pretraining stages. However, those approaches require extensive manual tuning and human expertise. In this paper, we introduce a novel single-stage continual pretraining method, Head-Adaptive Rotary Position Embedding (HARPE), to equip LLMs with long context modeling capabilities while simplifying the training process. Our HARPE leverages different Rotary Position Embedding (RoPE) base frequency values across different attention heads and directly trains LLMs on the target context length. Extensive experiments on 4 language modeling benchmarks, including the latest RULER benchmark, demonstrate that HARPE excels in understanding and integrating long-context tasks with single-stage training, matching and even outperforming existing multi-stage methods. Our results highlight that HARPE successfully breaks the stage barrier for training LLMs with long context modeling capabilities.",
        "author": "Haoran Lian; Junmin Chen; Wei Huang; Yizhe Xiong; Wenping Hu; Guiguang Ding; Hui Chen; Jianwei Niu; Zijia Lin; Fuzheng Zhang; Di Zhang",
        "authorids": "/h/haoran-lian/; /j/junmin-chen/; /w/wei-huang/; /y/yizhe-xiong/; /w/wenping-hu/; /g/guiguang-ding/; /h/hui-chen/; /j/jianwei-niu/; /z/zijia-lin/; /f/fuzheng-zhang/; /d/di-zhang/",
        "bibtex": "@inproceedings{lian-etal-2025-breaking,\n    title = \"Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models\",\n    author = \"Lian, Haoran  and\n      Chen, Junmin  and\n      Huang, Wei  and\n      Xiong, Yizhe  and\n      Hu, Wenping  and\n      Ding, Guiguang  and\n      Chen, Hui  and\n      Niu, Jianwei  and\n      Lin, Zijia  and\n      Zhang, Fuzheng  and\n      Zhang, Di\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.326/\",\n    pages = \"4897--4909\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.326.pdf",
        "site": "https://aclanthology.org/2025.coling-main.326/",
        "pdf_size": 720805,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14599562546099518361&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Beihang University; Kuaishou Technology; BUPT; Tsinghua University; Kuaishou Technology; Tsinghua University; Tsinghua University; Beihang University+Zhongguancun Laboratory+Zhengzhou University; Tsinghua University; Kuaishou Technology; Kuaishou Technology",
        "aff_domain": "buaa.edu.cn; ; ;tsinghua.edu.cn; ; ; ;tsinghua.org.cn; ; ; ",
        "email": "buaa.edu.cn; ; ;tsinghua.edu.cn; ; ; ;tsinghua.org.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;2;3;1;3;3;0+4+5;3;1;1",
        "aff_unique_norm": "Beihang University;Kuaishou Technology;Beijing University of Posts and Telecommunications;Tsinghua University;Zhongguancun Laboratory;Zhengzhou University",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "http://www.buaa.edu.cn/;https://www.kuaishou.com;http://www.bupt.edu.cn/;https://www.tsinghua.edu.cn;;http://www.zzu.edu.cn",
        "aff_unique_abbr": "BUAA;Kuaishou;BUPT;THU;;ZZU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.683",
        "title": "Bridging Context Gaps: Enhancing Comprehension in Long-Form Social Conversations Through Contextualized Excerpts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We focus on enhancing comprehension in small-group recorded conversations, which serve as a medium to bring people together and provide a space for sharing personal stories and experiences on crucial social matters. One way to parse and convey information from these conversations is by sharing highlighted excerpts in subsequent conversations. This can help promote a collective understanding of relevant issues, by highlighting perspectives and experiences to other groups of people who might otherwise be unfamiliar with and thus unable to relate to these experiences. The primary challenge that arises then is that excerpts taken from one conversation and shared in another setting might be missing crucial context or key elements that were previously introduced in the original conversation. This problem is exacerbated when conversations become lengthier and richer in themes and shared experiences. To address this, we explore how Large Language Models (LLMs) can enrich these excerpts by providing socially relevant context. We present approaches for effective contextualization to improve comprehension, readability, and empathy. We show significant improvements in understanding, as assessed through subjective and objective evaluations. While LLMs can offer valuable context, they struggle with capturing key social aspects. We release the Human-annotated Salient Excerpts (HSE) dataset to support future work. Additionally, we show how context-enriched excerpts can provide more focused and comprehensive conversation summaries.",
        "author": "Shrestha Mohanty; Sarah Xuan; Jacob Jobraeel; Anurag Kumar; Deb Roy; Jad Kabbara",
        "authorids": "/s/shrestha-mohanty/; /s/sarah-xuan/; /j/jacob-jobraeel/; /a/anurag-kumar/; /d/deb-roy/; /j/jad-kabbara/",
        "bibtex": "@inproceedings{mohanty-etal-2025-bridging,\n    title = \"Bridging Context Gaps: Enhancing Comprehension in Long-Form Social Conversations Through Contextualized Excerpts\",\n    author = \"Mohanty, Shrestha  and\n      Xuan, Sarah  and\n      Jobraeel, Jacob  and\n      Kumar, Anurag  and\n      Roy, Deb  and\n      Kabbara, Jad\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.683/\",\n    pages = \"10242--10274\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.683.pdf",
        "site": "https://aclanthology.org/2025.coling-main.683/",
        "pdf_size": 2855522,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:PABHG4vPhAAJ:scholar.google.com/&scioq=Bridging+Context+Gaps:+Enhancing+Comprehension+in+Long-Form+Social+Conversations+Through+Contextualized+Excerpts&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Meta; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu; ; ; ; ; ",
        "email": "mit.edu; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Meta Platforms, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://meta.com",
        "aff_unique_abbr": "MIT;Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.123",
        "title": "Bridging Modality Gap for Effective Multimodal Sentiment Analysis in Fashion-related Social Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal sentiment analysis for fashion-related social media is essential for understanding how consumers appraise fashion products across platforms like Instagram and Twitter, where both textual and visual elements contribute to sentiment expression. However, a notable challenge in this task is the modality gap, where the different information density between text and images hinders effective sentiment analysis. In this paper, we propose a novel multimodal framework that addresses this challenge by introducing pseudo data generated by a two-stage framework. We further utilize a multimodal fusion approach that efficiently integrates the information from various modalities for sentiment classification of fashion posts. Experiments conducted on a comprehensive dataset demonstrate that our framework significantly outperforms existing unimodal and multimodal baselines, highlighting its effectiveness in bridging the modality gap for more accurate sentiment classification in fashion-related social media posts.",
        "author": "Zheyu Zhao; Zhongqing Wang; Shichen Li; Hongling Wang; Guodong Zhou",
        "authorids": "/z/zheyu-zhao/; /z/zhongqing-wang/; /s/shichen-li/; /h/hongling-wang/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{zhao-etal-2025-bridging,\n    title = \"Bridging Modality Gap for Effective Multimodal Sentiment Analysis in Fashion-related Social Media\",\n    author = \"Zhao, Zheyu  and\n      Wang, Zhongqing  and\n      Li, Shichen  and\n      Wang, Hongling  and\n      Zhou, Guodong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.123/\",\n    pages = \"1813--1823\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.123.pdf",
        "site": "https://aclanthology.org/2025.coling-main.123/",
        "pdf_size": 1667141,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:858rPIR91_AJ:scholar.google.com/&scioq=Bridging+Modality+Gap+for+Effective+Multimodal+Sentiment+Analysis+in+Fashion-related+Social+Media&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Natural Language Processing Lab",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.619",
        "title": "Bridging the Language Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have revolutionized various domains but still struggle with non-Latin scripts and low-resource languages. This paper addresses the critical challenge of improving multilingual performance without extensive fine-tuning. We introduce a novel dynamic learning approach that optimizes prompt strategy, embedding model, and LLM per query at runtime. By adapting configurations dynamically, our method achieves significant improvements over static, best and random baselines. It operates efficiently in both offline and online settings, generalizing seamlessly across new languages and datasets. Leveraging Retrieval-Augmented Generation (RAG) with state-of-the-art multilingual embeddings, we achieve superior task performance across diverse linguistic contexts. Through systematic investigation and evaluation across18 diverse languages using popular question-answering (QA) datasets we show our approach results in 10-15% improvements in multilingual performance over pre-trained models and 4x gains compared to fine-tuned, language-specific models.",
        "author": "Somnath Kumar; Vaibhav Balloli; Mercy Ranjit; Kabir Ahuja; Sunayana Sitaram; Kalika Bali; Tanuja Ganu; Akshay Nambi",
        "authorids": "/s/somnath-kumar/; /v/vaibhav-balloli/; /m/mercy-ranjit/; /k/kabir-ahuja/; /s/sunayana-sitaram/; /k/kalika-bali/; /t/tanuja-ganu/; /a/akshay-nambi/",
        "bibtex": "@inproceedings{kumar-etal-2025-bridging,\n    title = \"Bridging the Language Gap: Dynamic Learning Strategies for Improving Multilingual Performance in {LLM}s\",\n    author = \"Kumar, Somnath  and\n      Balloli, Vaibhav  and\n      Ranjit, Mercy  and\n      Ahuja, Kabir  and\n      Sitaram, Sunayana  and\n      Bali, Kalika  and\n      Ganu, Tanuja  and\n      Nambi, Akshay\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.619/\",\n    pages = \"9209--9223\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.619.pdf",
        "site": "https://aclanthology.org/2025.coling-main.619/",
        "pdf_size": 699069,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:A16JZsMiLisJ:scholar.google.com/&scioq=Bridging+the+Language+Gap:+Dynamic+Learning+Strategies+for+Improving+Multilingual+Performance+in+LLMs&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-industry.37",
        "title": "Building a Family of Data Augmentation Models for Low-cost LLM Fine-tuning on the Cloud",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Specializing LLMs in various domain-specific tasks has emerged as a critical step towards achieving high performance. However, the construction and annotation of datasets in specific domains are always very costly. Apart from using superior and expensive closed-source LLM APIs to construct datasets, some open-source models have become strong enough to handle dataset construction in many scenarios. Thus, we present a family of data augmentation models designed to significantly improve the efficiency for model fine-tuning. These models, trained based on sufficiently small LLMs, support key functionalities with low inference costs: instruction expansion, instruction refinement, and instruction-response pair expansion. To fulfill this goal, we first construct an automatic data collection system with seed datasets generated from both public repositories and our in-house datasets. This system leverages powerful LLMs to expand, refine and re-write the instructions and responses, incorporating quality assessment techniques. Following this, we introduce the training process of our models, which effectively distills task-solving and text synthesis abilities from teacher LLMs. Finally, we demonstrate how we integrate these functionalities into a machine learning platform to support low-cost LLM fine-tuning from both dataset preparation and training perspectives for users. Experiments and an application study prove the effectiveness of our approach.",
        "author": "Yuanhao Yue; Chengyu Wang; Jun Huang; Peng Wang",
        "authorids": "/y/yuanhao-yue/; /c/chengyu-wang/; /j/jun-huang/; /p/peng-wang/",
        "bibtex": "@inproceedings{wang-etal-2025-building,\n    title = \"Building a Family of Data Augmentation Models for Low-cost {LLM} Fine-tuning on the Cloud\",\n    author = \"Yue, Yuanhao  and\n      Wang, Chengyu  and\n      Huang, Jun  and\n      Wang, Peng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.37/\",\n    pages = \"431--444\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.37.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.37/",
        "pdf_size": 1557568,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10300909371812072754&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Fudan University, Shanghai, China + Alibaba Cloud Computing, Hangzhou, China; Alibaba Cloud Computing, Hangzhou, China; Alibaba Cloud Computing, Hangzhou, China; School of Computer Science, Fudan University, Shanghai, China + Alibaba Cloud Computing, Hangzhou, China",
        "aff_domain": "m.fudan.edu.cn;alibaba-inc.com;alibaba-inc.com;fudan.edu.cn",
        "email": "m.fudan.edu.cn;alibaba-inc.com;alibaba-inc.com;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;0+1",
        "aff_unique_norm": "Fudan University;Alibaba Cloud Computing",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.alibabacloud.com",
        "aff_unique_abbr": "Fudan;Alibaba Cloud",
        "aff_campus_unique_index": "0+1;1;1;0+1",
        "aff_campus_unique": "Shanghai;Hangzhou",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.275",
        "title": "C3LRSO: A Chinese Corpus for Complex Logical Reasoning in Sentence Ordering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Sentence ordering is the task of rearranging a set of unordered sentences into a coherent and logically consistent sequence. Recent work has primarily used pre-trained language models, achieving significant success in the task. However, existing sentence ordering corpora are predominantly in English, and comprehensive benchmark datasets for non-English languages are unavailable. Meanwhile, current datasets often insert specific markers into paragraphs, inadvertently making the logical sequence between sentences more apparent and reducing the models\u2019 ability to handle genuinely unordered sentences in real applications. To address these limitations, we develop C3LRSO, a high-quality Chinese sentence ordering dataset that overcomes the aforementioned shortcomings by providing genuinely unordered sentences without artificial segmentation cues. Furthermore, given the outstanding performance of large language models on NLP tasks, we evaluate these models on our dataset for this task. Additionally, we propose a simple yet effective parameter-free approach that outperforms existing methods on this task. Experiments demonstrate the challenging nature of the dataset and the strong performance of our proposed method. These findings highlight the potential for further research in sentence ordering and the development of more robust language models. Our dataset is freely available at https://github.com/JasonGuo1/C3LRSO.",
        "author": "Xiaotao Guo; Jiang Li; Xiangdong Su; Fujun Zhang",
        "authorids": "/x/xiaotao-guo/; /j/jiang-li/; /x/xiangdong-su/; /f/fujun-zhang/",
        "bibtex": "@inproceedings{guo-etal-2025-c3lrso,\n    title = \"{C}3{LRSO}: A {C}hinese Corpus for Complex Logical Reasoning in Sentence Ordering\",\n    author = \"Guo, Xiaotao  and\n      Li, Jiang  and\n      Su, Xiangdong  and\n      Zhang, Fujun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.275/\",\n    pages = \"4085--4095\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.275.pdf",
        "site": "https://aclanthology.org/2025.coling-main.275/",
        "pdf_size": 721179,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BW-mxU3Z69kJ:scholar.google.com/&scioq=C3LRSO:+A+Chinese+Corpus+for+Complex+Logical+Reasoning+in+Sentence+Ordering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Computer Science, Inner Mongolia University, Hohhot, China+National & Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China; College of Computer Science, Inner Mongolia University, Hohhot, China+National & Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China; College of Computer Science, Inner Mongolia University, Hohhot, China+National & Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China; College of Computer Science, Inner Mongolia University, Hohhot, China+National & Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China",
        "aff_domain": "mail.imu.edu.cn;imu.edu.cn; ; ",
        "email": "mail.imu.edu.cn;imu.edu.cn; ; ",
        "github": "https://github.com/JasonGuo1/C3LRSO",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0;0+1+0;0+1+0;0+1+0",
        "aff_unique_norm": "Inner Mongolia University;National & Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian",
        "aff_unique_dep": "College of Computer Science;",
        "aff_unique_url": "http://www.imu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Hohhot;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.635",
        "title": "CACA: Context-Aware Cross-Attention Network for Extractive Aspect Sentiment Quad Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect Sentiment Quad Prediction(ASQP) enhances the scope of aspect-based sentiment analysis by introducing the necessity to predict both explicit and implicit aspect and opinion terms. Existing leading generative ASQP approaches do not modeling the contextual relationship of the review sentence to predict implicit terms. However, introducing the contextual information into the pre-trained language models framework is non-trivial due to the inflexibility of the generative encoder-decoder architecture. To well utilize the contextual information, we propose an extractive ASQP framework, CACA, which features with Context-Aware Cross-Attention Network. When implicit terms are present, the Context-Aware Cross-Attention Network enhances the alignment of aspects and opinions, through alternating updates of explicit and implicit representations. Additionally, contrastive learning is introduced in the implicit representation learning process. Experimental results on three benchmarks demonstrate the effectiveness of CACA. Our implementation will be open-sourced at https://github.com/DMIRLAB-Group/CACA.",
        "author": "Bingfeng Chen; Haoran Xu; Yongqi Luo; Boyan Xu; Ruichu Cai; Zhifeng Hao",
        "authorids": "/b/bingfeng-chen/; /h/haoran-xu/; /y/yongqi-luo/; /b/boyan-xu/; /r/ruichu-cai/; /z/zhifeng-hao/",
        "bibtex": "https://aclanthology.org/2025.coling-main.635.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.635.pdf",
        "site": "https://aclanthology.org/2025.coling-main.635/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0nr7Fb0vKzgJ:scholar.google.com/&scioq=CACA:+Context-Aware+Cross-Attention+Network+for+Extractive+Aspect+Sentiment+Quad+Prediction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-demos.15",
        "title": "CASE: Large Scale Topic Exploitation for Decision Support Systems",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "In recent years, there has been growing interest in using NLP tools for decision support systems, particularly in Science, Technology, and Innovation (STI). Among these, topic modeling has been widely used for analyzing large document collections, such as scientific articles, research projects, or patents, yet its integration into decision-making systems remains limited. This paper introduces CASE, a tool for exploiting topic information for semantic analysis of large corpora. The core of CASE is a Solr engine with a customized indexing strategy to represent information from Bayesian and Neural topic models that allow efficient topic-enriched searches. Through ad-hoc plug-ins, CASE enables topic inference on new texts and semantic search. We demonstrate the versatility and scalability of CASE through two use cases: the calculation of aggregated STI indicators and the implementation of a web service to help evaluate research projects.",
        "author": "Lorena Calvo Bartolom\u00e9; Jer\u00f3nimo Arenas-Garc\u00eda; David P\u00e9rez Fern\u00e1ndez",
        "authorids": "/l/lorena-calvo-bartolome/; /j/jeronimo-arenas-garcia/; /d/david-perez-fernandez/",
        "bibtex": "@inproceedings{calvo-bartolome-etal-2025-case,\n    title = \"{CASE}: Large Scale Topic Exploitation for Decision Support Systems\",\n    author = \"Calvo Bartolom{\\'e}, Lorena  and\n      Arenas-Garc{\\'i}a, Jer{\\'o}nimo  and\n      P{\\'e}rez Fern{\\'a}ndez, David\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.15/\",\n    pages = \"151--162\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.15.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.15/",
        "pdf_size": 5983931,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6gLYcjFyWXkJ:scholar.google.com/&scioq=CASE:+Large+Scale+Topic+Exploitation+for+Decision+Support+Systems&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Universidad Carlos III; Universidad Carlos III; Universidad Aut\u00f3noma de Madrid",
        "aff_domain": "pa.uc3m.es;ing.uc3m.es;inv.uam.es",
        "email": "pa.uc3m.es;ing.uc3m.es;inv.uam.es",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Universidad Carlos III de Madrid;Universidad Aut\u00f3noma de Madrid",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uc3m.es;https://www.uam.es",
        "aff_unique_abbr": "UC3M;UAM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-main.93",
        "title": "CAST: Cross-modal Alignment Similarity Test for Vision Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Vision Language Models (VLMs) are typically evaluated with Visual Question Answering (VQA) tasks which assess a model\u2019s understanding of scenes. Good VQA performance is taken as evidence that the model will perform well on a broader range of tasks that require both visual and language inputs. However, scene-aware VQA does not fully capture input biases or assess hallucinations caused by a misalignment between modalities. To address this, we propose a Cross-modal Alignment Similarity Test (CAST) to probe VLMs for self-consistency across modalities. This test involves asking the models to identify similarities between two scenes through text-only, image-only, or both and then assess the truthfulness of the similarities they generate. Since there is no ground-truth to compare against, this evaluation does not focus on objective accuracy but rather on whether VLMs are internally consistent in their outputs. We argue that while not all self-consistent models are capable or accurate, all capable VLMs must be self-consistent.",
        "author": "Gautier Dagan; Olga Loginova; Anil Batra",
        "authorids": "/g/gautier-dagan/; /o/olga-loginova/; /a/anil-batra/",
        "bibtex": "@inproceedings{dagan-etal-2025-cast,\n    title = \"{CAST}: Cross-modal Alignment Similarity Test for Vision Language Models\",\n    author = \"Dagan, Gautier  and\n      Loginova, Olga  and\n      Batra, Anil\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.93/\",\n    pages = \"1387--1402\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.93.pdf",
        "site": "https://aclanthology.org/2025.coling-main.93/",
        "pdf_size": 9214059,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3277377900718730084&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Edinburgh; University of Trento; University of Edinburgh",
        "aff_domain": "ed.ac.uk;unitn.it;sms.ed.ac.uk",
        "email": "ed.ac.uk;unitn.it;sms.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Edinburgh;University of Trento",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.unitn.it",
        "aff_unique_abbr": "Edinburgh;UniTN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;Italy"
    },
    {
        "id": "2025.coling-main.6",
        "title": "CDA\u02c62: Counterfactual Diffusion Augmentation for Cross-Domain Adaptation in Low-Resource Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Domain adaptation is widely employed in cross-domain sentiment analysis, enabling the transfer of models from label-rich source domains to target domain with fewer or no labels. However, concerns have been raised regarding their robustness and sensitivity to data distribution shift, particularly when encountering significant disparities in data distribution between the different domains. To tackle this problem, we introduce a framework CDA\u02c62 for cross-domain adaptation in low-resource sentiment analysis, which utilizes counterfactual diffusion augmentation. Specifically, it employs samples derived from domain-relevant word substitutions in source domain samples to guide the diffusion model for generating high-quality counterfactual target domain samples. We adopt a soft absorbing state and MMD loss during the training stage, and use advanced ODE solvers to expedite the sampling process. Our experiments demonstrate that CDA\u02c62 generates high-quality target samples and achieves state-of-the-art performance in cross-domain sentiment analysis.",
        "author": "Dancheng Xin; Kaiqi Zhao; Jingyun Sun; Yang Li",
        "authorids": "/d/dancheng-xin/; /k/kaiqi-zhao/; /j/jingyun-sun/; /y/yang-li/",
        "bibtex": "@inproceedings{xin-etal-2025-cda,\n    title = \"{CDA}{\\textasciicircum}2: Counterfactual Diffusion Augmentation for Cross-Domain Adaptation in Low-Resource Sentiment Analysis\",\n    author = \"Xin, Dancheng  and\n      Zhao, Kaiqi  and\n      Sun, Jingyun  and\n      Li, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.6/\",\n    pages = \"61--72\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.6.pdf",
        "site": "https://aclanthology.org/2025.coling-main.6/",
        "pdf_size": 1844788,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0j0Jfgbp0_sJ:scholar.google.com/&scioq=CDA%CB%862:+Counterfactual+Diffusion+Augmentation+for+Cross-Domain+Adaptation+in+Low-Resource+Sentiment+Analysis&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Northeast Forestry University, China; University of Auckland, New Zealand; Northeast Forestry University, China; Northeast Forestry University, China",
        "aff_domain": "nefu.edu.cn;auckland.ac.nz;nefu.edu.cn;nefu.edu.cn",
        "email": "nefu.edu.cn;auckland.ac.nz;nefu.edu.cn;nefu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Northeast Forestry University;University of Auckland",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nefu.edu.cn;https://www.auckland.ac.nz",
        "aff_unique_abbr": "NEFU;UoA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;New Zealand"
    },
    {
        "id": "2025.coling-main.656",
        "title": "CE-DA: Custom Embedding and Dynamic Aggregation for Zero-Shot Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Zero-shot Relation Extraction (ZSRE) aims to predict novel relations from sentences with given entity pairs, where the relations have not been encountered during training. Prototypebased methods, which achieve ZSRE by aligning the sentence representation and the relation prototype representation, have shown great potential. However, most existing works focus solely on improving the quality of prototype representations, neglecting sentence representations and lacking interaction between different types of relation side information. In this paper, we propose a novel ZSRE framework named CE-DA, which includes two modules: Custom Embedding and Dynamic Aggregation. We employ a two-stage approach to obtain customized embeddings of sentences. In the first stage, we train a sentence encoder through unsupervised contrastive learning, and in the second stage, we highlight the potential relations between entities in sentences using carefully designed entity emphasis prompts to further enhance sentence representations. Additionally, our dynamic aggregation method assigns different weights to different types of relation side information through a learnable network to enhance the quality of relation prototype representations. In contrast to traditional methods that treat the importance of all side information equally, our dynamic aggregation method further strengthen the interaction between different types of relation side information. Our method demonstrates competitive performance across various metrics on two ZSRE datasets.",
        "author": "Fu Zhang; He Liu; Zehan Li; Jingwei Cheng",
        "authorids": "/f/fu-zhang/; /h/he-liu/; /z/zehan-li/; /j/jingwei-cheng/",
        "bibtex": "@inproceedings{zhang-etal-2025-ce,\n    title = \"{CE}-{DA}: Custom Embedding and Dynamic Aggregation for Zero-Shot Relation Extraction\",\n    author = \"Zhang, Fu  and\n      Liu, He  and\n      Li, Zehan  and\n      Cheng, Jingwei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.656/\",\n    pages = \"9814--9823\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.656.pdf",
        "site": "https://aclanthology.org/2025.coling-main.656/",
        "pdf_size": 540647,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:HuqTQp3awJUJ:scholar.google.com/&scioq=CE-DA:+Custom+Embedding+and+Dynamic+Aggregation+for+Zero-Shot+Relation+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; ; ",
        "aff_domain": "mail.neu.edu.cn;stumail.neu.edu.cn;163.com;mail.neu.edu.cn",
        "email": "mail.neu.edu.cn;stumail.neu.edu.cn;163.com;mail.neu.edu.cn",
        "github": "https://github.com/ReveriePoem/CE-DA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.neu.edu.cn/",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.99",
        "title": "CEHA: A Dataset of Conflict Events in the Horn of Africa",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural Language Processing (NLP) of news articles can play an important role in understanding the dynamics and causes of violent conflict. Despite the availability of datasets categorizing various conflict events, the existing labels often do not cover all of the fine-grained violent conflict event types relevant to areas like the Horn of Africa. In this paper, we introduce a new benchmark dataset Conflict Events in the Horn of Africa region (CEHA) and propose a new task for identifying violent conflict events using online resources with this dataset. The dataset consists of 500 English event descriptions regarding conflict events in the Horn of Africa region with fine-grained event-type definitions that emphasize the cause of the conflict. This dataset categorizes the key types of conflict risk according to specific areas required by stakeholders in the Humanitarian-Peace-Development Nexus. Additionally, we conduct extensive experiments on two tasks supported by this dataset: Event-relevance Classification and Event-type Classification. Our baseline models demonstrate the challenging nature of these tasks and the usefulness of our dataset for model evaluations in low-resource settings.",
        "author": "Rui Bai; Di Lu; Shihao Ran; Elizabeth M. Olson; Hemank Lamba; Aoife Cahill; Joel Tetreault; Alejandro Jaimes",
        "authorids": "/r/rui-bai/; /d/di-lu/; /s/shihao-ran/; /e/elizabeth-m-olson/; /h/hemank-lamba/; /a/aoife-cahill/; /j/joel-tetreault/; /a/alejandro-jaimes/",
        "bibtex": "@inproceedings{bai-etal-2025-ceha,\n    title = \"{CEHA}: A Dataset of Conflict Events in the Horn of {A}frica\",\n    author = \"Bai, Rui  and\n      Lu, Di  and\n      Ran, Shihao  and\n      Olson, Elizabeth M.  and\n      Lamba, Hemank  and\n      Cahill, Aoife  and\n      Tetreault, Joel  and\n      Jaimes, Alejandro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.99/\",\n    pages = \"1475--1495\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.99.pdf",
        "site": "https://aclanthology.org/2025.coling-main.99/",
        "pdf_size": 1576459,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:s2c2oBkZUhUJ:scholar.google.com/&scioq=CEHA:+A+Dataset+of+Conflict+Events+in+the+Horn+of+Africa&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "https://www.un.org/peacebuilding/content/humanitarian-development-and-peace-nexus",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.626",
        "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The colossal parameters and computational overhead of Large Language Models (LLMs) challenge their real-world applications. Network pruning, which targets unstructured or structured sparsity by removing redundant parameters, has recently been explored for LLM acceleration. Existing LLM pruning works focus on unstructured pruning, which typically requires special hardware support for a practical speed-up. In contrast, structured pruning can reduce latency on general devices. However, it remains a challenge to perform structured pruning efficiently and maintain performance, especially at high sparsity ratios. To this end, we introduce an efficient structured pruning framework named CFSP, which leverages both Coarse (interblock) and Fine-grained (intrablock) activation information as an importance criterion to guide pruning. The pruning is highly efficient, as it only requires one forward pass to compute feature activations. Specifically, we first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block. In addition, we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance to further improve performance. Experimental results demonstrate that CFSP outperforms existing methods on diverse models across various sparsity budgets. Our code will be available at https://github.com/wyxscir/CFSP.",
        "author": "Yuxin Wang; MingHua Ma; Zekun Wang; Jingchang Chen; Shan Liping; Qing Yang; Dongliang Xu; Ming Liu; Bing Qin",
        "authorids": "/y/yuxin-wang/; /m/minghua-ma/; /z/zekun-wang/; /j/jingchang-chen/; /s/shan-liping/; /q/qing-yang/; /d/dongliang-xu/; /m/ming-liu/; /b/bing-qin/",
        "bibtex": "@inproceedings{wang-etal-2025-cfsp,\n    title = \"{CFSP}: An Efficient Structured Pruning Framework for {LLM}s with Coarse-to-Fine Activation Information\",\n    author = \"Wang, Yuxin  and\n      Ma, MingHua  and\n      Wang, Zekun  and\n      Chen, Jingchang  and\n      Liping, Shan  and\n      Yang, Qing  and\n      Xu, Dongliang  and\n      Liu, Ming  and\n      Qin, Bing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.626/\",\n    pages = \"9311--9328\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.626.pdf",
        "site": "https://aclanthology.org/2025.coling-main.626/",
        "pdf_size": 1196342,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=96553329570783673&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology+Du Xiaoman Science Technology Co., Ltd; Harbin Institute of Technology; Du Xiaoman Science Technology Co., Ltd; Du Xiaoman Science Technology Co., Ltd; Du Xiaoman Science Technology Co., Ltd; Harbin Institute of Technology; Harbin Institute of Technology",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn; ; ; ;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn; ; ; ;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "https://github.com/wyxscir/CFSP",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0+1;0;1;1;1;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Du Xiaoman Science Technology Co., Ltd",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.hit.edu.cn/;",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.398",
        "title": "CHIFRAUD: A Long-term Web Text Dataset for Chinese Fraud Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Detecting fraudulent online text is essential, as these manipulative messages exploit human greed, deceive individuals, and endanger societal security. Currently, this task remains under-explored on the Chinese web due to the lack of a comprehensive dataset of Chinese fraudulent texts. However, creating such a dataset is challenging because it requires extensive annotation within a vast collection of normal texts. Additionally, the creators of fraudulent webpages continuously update their tactics to evade detection by downstream platforms and promote fraudulent messages. To this end, this work firstly presents the comprehensive long-term dataset of Chinese fraudulent texts collected over 12 months, consisting of 59,106 entries extracted from billions of web pages. Furthermore, we design and provide a wide range of baselines, including large language model-based detectors, and pre-trained language model approaches. The necessary dataset and benchmark codes for further research are available via https://github. com/xuemingxxx/ChiFraud.",
        "author": "Min Tang; Lixin Zou; Zhe Jin; ShuJie Cui; Shiuan Ni Liang; Weiqing Wang",
        "authorids": "/m/min-tang/; /l/lixin-zou/; /z/zhe-jin/; /s/shujie-cui/; /s/shiuan-ni-liang/; /w/weiqing-wang/",
        "bibtex": "@inproceedings{tang-etal-2025-chifraud,\n    title = \"{CHIFRAUD}: A Long-term Web Text Dataset for {C}hinese Fraud Detection\",\n    author = \"Tang, Min  and\n      Zou, Lixin  and\n      Jin, Zhe  and\n      Cui, ShuJie  and\n      Liang, Shiuan Ni  and\n      Wang, Weiqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.398/\",\n    pages = \"5962--5974\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.398.pdf",
        "site": "https://aclanthology.org/2025.coling-main.398/",
        "pdf_size": 1423771,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Ckmj1lldiSEJ:scholar.google.com/&scioq=CHIFRAUD:+A+Long-term+Web+Text+Dataset+for+Chinese+Fraud+Detection&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Monash University; Anhui University; Monash University; Wuhan University; Monash University; Monash University",
        "aff_domain": "monash.edu;whu.edu.cn;monash.edu;ahu.edu.cn;monash.edu;monash.edu",
        "email": "monash.edu;whu.edu.cn;monash.edu;ahu.edu.cn;monash.edu;monash.edu",
        "github": "https://github.com/xuemingxxx/ChiFraud",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;2;0;0",
        "aff_unique_norm": "Monash University;Anhui University;Wuhan University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.monash.edu;http://www.ahu.edu.cn/;http://www.whu.edu.cn/",
        "aff_unique_abbr": "Monash;AHU;WHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0;0",
        "aff_country_unique": "Australia;China"
    },
    {
        "id": "2025.coling-main.184",
        "title": "CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid advancements in multimodal large language models, evaluating their multimodal mathematical capabilities continues to receive wide attention. Although datasets such as MathVista have been introduced for evaluating mathematical capabilities in multimodal scenarios, there remains a lack of evaluation tools and datasets tailored for fine-grained assessment in Chinese K12 education. To systematically evaluate the ability of multimodal large models to solve Chinese multimodal mathematical problems, we propose a Chinese Multi-modal Math Skill Evaluation Benchmark (CMMaTH), containing 23,856 multimodal K12 math related questions, making it the largest Chinese multimodal mathematical problem benchmark to date. CMMaTH includes questions ranging from elementary to high school levels, offering greater diversity in problem types, solution goals, visual elements, detailed knowledge points, and standard solution annotations. To facilitate stable, fast, and cost-free model evaluation, we have developed an open-source tool called GradeGPT, which is integrated with the CMMaTH dataset. Our data and code are available at https://github.com/zzli2022/CMMaTH.",
        "author": "Zhongzhi Li; Ming-Liang Zhang; Pei-Jie Wang; Jian Xu; Rui-Song Zhang; Yin Fei; Zhi-Long Ji; Jin-Feng Bai; Zhen-Ru Pan; Jiaxin Zhang; Cheng-Lin Liu",
        "authorids": "/z/zhongzhi-li/; /m/ming-liang-zhang/; /p/pei-jie-wang/; /j/jian-xu/; /r/rui-song-zhang/; /y/yin-fei/; /z/zhi-long-ji/; /j/jin-feng-bai/; /z/zhen-ru-pan/; /j/jiaxin-zhang/; /c/cheng-lin-liu/",
        "bibtex": "https://aclanthology.org/2025.coling-main.184.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.184.pdf",
        "site": "https://aclanthology.org/2025.coling-main.184/",
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7940542810205496393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 11
    },
    {
        "id": "2025.coling-main.217",
        "title": "COF: Adaptive Chain of Feedback for Comparative Opinion Quintuple Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Comparative Opinion Quintuple Extraction (COQE) aims to extract all comparative sentiment quintuples from product review text. Each quintuple comprises five elements: subject, object, aspect, opinion and preference. With the rise of Large Language Models (LLMs), existing work primarily focuses on enhancing the performance of COQE task through data augmentation, supervised fine-tuning and instruction tuning. Instead of the above pre-modeling and in-modeling design techniques, we focus on innovation in the post-processing. We introduce a model-unaware adaptive chain-of-feedback (COF) method from the perspective of inference feedback and extraction revision. This method comprises three core modules: dynamic example selection, self-critique and self-revision. By integrating LLMs, COF enables dynamic iterative self-optimization, making it applicable across different baselines. To validate the effectiveness of our approach, we utilize the outputs of two distinct baselines as inputs for COF: frozen parameters few-shot learning and the SOTA supervised fine-tuned model. We evaluate our approach on three benchmarks: Camera, Car and Ele. Experimental results show that, compared to the few-shot learning method, our approach achieves F1 score improvements of 3.51%, 2.65% and 5.28% for exact matching on the respective dataset. Even more impressively, our method further boosts performance, surpassing the current SOTA results, with additional gains of 0.76%, 6.54%, and 2.36% across the three datasets.",
        "author": "Qingting Xu; Kaisong Song; Chaoqun Liu; Yangyang Kang; Xiabing Zhou; Jun Lin; Yu Hong",
        "authorids": "/q/qingting-xu/; /k/kaisong-song/; /c/chaoqun-liu/; /y/yangyang-kang/; /x/xiabing-zhou/; /j/jun-lin/; /y/yu-hong/",
        "bibtex": "@inproceedings{xu-etal-2025-cof,\n    title = \"{COF}: Adaptive Chain of Feedback for Comparative Opinion Quintuple Extraction\",\n    author = \"Xu, Qingting  and\n      Song, Kaisong  and\n      Liu, Chaoqun  and\n      Kang, Yangyang  and\n      Zhou, Xiabing  and\n      Lin, Jun  and\n      Hong, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.217/\",\n    pages = \"3236--3247\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.217.pdf",
        "site": "https://aclanthology.org/2025.coling-main.217/",
        "pdf_size": 1587386,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xLQ4rv9elW0J:scholar.google.com/&scioq=COF:+Adaptive+Chain+of+Feedback+for+Comparative+Opinion+Quintuple+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; Northeastern University, China+Tongyi Lab, Alibaba Group, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Zhejiang University, Hangzhou, Zhejiang, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Tongyi Lab, Alibaba Group, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "gmail.com;alibaba-inc.com;gmail.com;zju.edu.cn;stu.suda.edu.cn;alibaba-inc.com;gmail.com",
        "email": "gmail.com;alibaba-inc.com;gmail.com;zju.edu.cn;stu.suda.edu.cn;alibaba-inc.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+2;0;3;0;2;0",
        "aff_unique_norm": "Soochow University;Northeastern University;Alibaba Group;Zhejiang University",
        "aff_unique_dep": "School of Computer Science and Technology;;Tongyi Lab;",
        "aff_unique_url": "http://www.soochow.edu.cn;http://www.neu.edu.cn/;https://www.alibaba.com;http://www.zju.edu.cn",
        "aff_unique_abbr": ";NEU;Alibaba;ZJU",
        "aff_campus_unique_index": "0;;0;2;0;0",
        "aff_campus_unique": "Suzhou;;Hangzhou",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.279",
        "title": "CONTRANS: Weak-to-Strong Alignment Engineering via Concept Transplantation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Ensuring large language models (LLM) behave consistently with human goals, values, and intentions is crucial for their safety but yet computationally expensive. To reduce the computational cost of alignment training of LLMs, especially for those with a huge number of parameters, and to reutilize learned value alignment, we propose ConTrans, a novel framework that enables weak-to-strong alignment transfer via concept transplantation. From the perspective of representation engineering, ConTrans refines concept vectors in value alignment from a source LLM (usually a weak yet aligned LLM). The refined concept vectors are then reformulated to adapt to the target LLM (usually a strong yet unaligned base LLM) via affine transformation. In the third step, ConTrans transplants the reformulated concept vectors into the residual stream of the target LLM. Experiments demonstrate the successful transplantation of a wide range of aligned concepts from 7B models to 13B and 70B models across multiple LLMs and LLM families. Remarkably, ConTrans even surpasses instruction-tuned models in terms of truthfulness. Experiment results validate the effectiveness of both inter-LLM-family and intra-LLM-family concept transplantation. Our work successfully demonstrates an alternative way to achieve weak-to-strong alignment generalization and control.",
        "author": "Weilong Dong; Xinwei Wu; Renren Jin; Shaoyang Xu; Deyi Xiong",
        "authorids": "/w/weilong-dong/; /x/xinwei-wu/; /r/renren-jin/; /s/shaoyang-xu/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{dong-etal-2025-contrans,\n    title = \"{CONTRANS}: Weak-to-Strong Alignment Engineering via Concept Transplantation\",\n    author = \"Dong, Weilong  and\n      Wu, Xinwei  and\n      Jin, Renren  and\n      Xu, Shaoyang  and\n      Xiong, Deyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.279/\",\n    pages = \"4130--4148\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.279.pdf",
        "site": "https://aclanthology.org/2025.coling-main.279/",
        "pdf_size": 7322642,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1574195782184040105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University; School of New Media and Communication, Tianjin University; College of Intelligence and Computing, Tianjin University+School of New Media and Communication, Tianjin University",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.745",
        "title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese examination systems. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios. We collect 22k questions from 39 psychology-related subjects across four Chinese examination systems. From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques. Furthermore, we evaluate a range of existing large language models (LLMs), spanning from open-sourced to proprietary models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.",
        "author": "Jiahao Zhao; Jingwei Zhu; Minghuan Tan; Min Yang; Renhao Li; Yang Di; Chenhao Zhang; Guancheng Ye; Chengming Li; Xiping Hu; Derek F. Wong",
        "authorids": "/j/jiahao-zhao/; /j/jingwei-zhu/; /m/minghuan-tan/; /m/min-yang/; /r/renhao-li/; /y/yang-di/; /c/chenhao-zhang/; /g/guancheng-ye/; /c/chengming-li/; /x/xiping-hu/; /d/derek-f-wong/",
        "bibtex": "@inproceedings{zhao-etal-2025-cpsyexam,\n    title = \"{CP}sy{E}xam: A {C}hinese Benchmark for Evaluating Psychology using Examinations\",\n    author = \"Zhao, Jiahao  and\n      Zhu, Jingwei  and\n      Tan, Minghuan  and\n      Yang, Min  and\n      Li, Renhao  and\n      Di, Yang  and\n      Zhang, Chenhao  and\n      Ye, Guancheng  and\n      Li, Chengming  and\n      Hu, Xiping  and\n      Wong, Derek F.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.745/\",\n    pages = \"11248--11260\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.745.pdf",
        "site": "https://aclanthology.org/2025.coling-main.745/",
        "pdf_size": 1432159,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:72bqntb5snoJ:scholar.google.com/&scioq=CPsyExam:+A+Chinese+Benchmark+for+Evaluating+Psychology+using+Examinations&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": ";;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 11
    },
    {
        "id": "2025.coling-demos.21",
        "title": "CULTURALLY YOURS: A Reading Assistant for Cross-Cultural Content",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Users from diverse cultural backgrounds frequently face challenges in understanding content from various online sources that are written by people from a different culture. This paper presents CULTURALLY YOURS (CY), a first-of-its-kind cultural reading assistant tool designed to identify culture-specific items (CSIs) for users from varying cultural contexts. By leveraging principles of relevance feedback and using culture as a prior, our tool personalizes to the user\u2019s preferences based on the interaction of the user with the tool. CY can be powered by any LLM that can reason with cultural background of the user and the input text in English, provided as a part of the prompt that are iteratively refined as the user keeps interacting with the system. In this demo, we use GPT-4o as the back-end. We conduct a user study across 13 users from 8 different geographies. The results demonstrate CY\u2019s effectiveness in enhancing user engagement and personalization alongside comprehension of cross-cultural content.",
        "author": "Saurabh Kumar Pandey; Harshit Budhiraja; Sougata Saha; Monojit Choudhury",
        "authorids": "/s/saurabh-kumar-pandey/; /h/harshit-budhiraja/; /s/sougata-saha/; /m/monojit-choudhury/",
        "bibtex": "@inproceedings{pandey-etal-2025-culturally,\n    title = \"{CULTURALLY} {YOURS}: A Reading Assistant for Cross-Cultural Content\",\n    author = \"Pandey, Saurabh Kumar  and\n      Budhiraja, Harshit  and\n      Saha, Sougata  and\n      Choudhury, Monojit\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.21/\",\n    pages = \"208--216\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.21.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.21/",
        "pdf_size": 2513332,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17313938573848910475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "MBZUAI; Wadhwani AI; MBZUAI; MBZUAI",
        "aff_domain": "mbzuai.ac.ae;gmail.com;mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;gmail.com;mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "https://github.com/skp1999/CULTURALLY_YOURS",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Mohamed Bin Zayed University of Artificial Intelligence;Wadhwani AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.wadhwaniai.com",
        "aff_unique_abbr": "MBZUAI;WAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United Arab Emirates;United States"
    },
    {
        "id": "2025.coling-main.670",
        "title": "CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities in various NLP tasks, significantly enhancing user experience and efficiency. However, this advantage is primarily limited to resource-rich languages. For the diverse array of low-resource languages, support remains inadequate, with the scarcity of training corpora considered the primary cause. We construct and open-source CUTE (Chinese, Uyghur, Tibetan, English) dataset, consisting of two 25GB sets of four-language corpora (one parallel and one non-parallel), obtained through machine translation. CUTE encompasses two resource-rich languages (Chinese and English) and two low-resource languages (Uyghur and Tibetan). Prior to constructing CUTE, human assessment validates that the machine translation quality between Chinese-Uyghur and Chinese-Tibetan approaches that of Chinese-English translation. CUTE represents the largest open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate its effectiveness in enhancing LLMs\u2019 ability to process low-resource languages while investigating the role of corpus parallelism in cross-lingual transfer learning. The CUTE corpus and related models are made publicly available to the research community.",
        "author": "Wenhao Zhuang; Yuan Sun",
        "authorids": "/w/wenhao-zhuang/; /y/yuan-sun/",
        "bibtex": "@inproceedings{zhuang-sun-2025-cute,\n    title = \"{CUTE}: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages\",\n    author = \"Zhuang, Wenhao  and\n      Sun, Yuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.670/\",\n    pages = \"10037--10046\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.670.pdf",
        "site": "https://aclanthology.org/2025.coling-main.670/",
        "pdf_size": 683928,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AKzNisUYT2wJ:scholar.google.com/&scioq=CUTE:+A+Multilingual+Dataset+for+Enhancing+Cross-Lingual+Knowledge+Transfer+in+Low-Resource+Languages&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Minzu University of China, Beijing, China + National Language Resource Monitoring & Research Center Minority Languages Branch; Minzu University of China, Beijing, China + National Language Resource Monitoring & Research Center Minority Languages Branch",
        "aff_domain": "163.com;muc.edu.cn",
        "email": "163.com;muc.edu.cn",
        "github": "https://github.com/CMLI-NLP/CUTE",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Minzu University of China;National Language Resource Monitoring & Research Center",
        "aff_unique_dep": ";Minority Languages Branch",
        "aff_unique_url": "http://www.muc.edu.cn/;",
        "aff_unique_abbr": "MUC;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2025.coling-main.551",
        "title": "CaDRL: Document-level Relation Extraction via Context-aware Differentiable Rule Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level Relation Extraction (DocRE) aims to extract relations from documents. Compared with sentence-level relation extraction, it is necessary to extract long-distance dependencies. Existing methods enhance the output of trained DocRE models either by learning logical rules or by extracting rules from annotated data and then injecting them into the model. However, these approaches can result in suboptimal performance due to incorrect rule set constraints. To mitigate this issue, we propose Context-aware differentiable rule learning or CaDRL for short, a novel differentiable rule-based framework that learns the doc-specific logical rule to avoid generating suboptimal constraints. Specifically, we utilize Transformer-based relation attention to encode document and relation information, thereby learning the contextual information of the relation. We employ a sequence-generated differentiable rule decoder to generate relational probabilistic logic rules at each reasoning step. We also introduce a parameter sharing training mechanism in CaDRL to reconcile the DocRE model and the rule learning module. Extensive experimental results on three DocRE datasets demonstrate that CaDRL outperforms existing rule-based frameworks, significantly improving DocRE performance and making predictions more interpretable and logical.",
        "author": "Kunli Zhang; Pengcheng Wu; Bohan Yu; Kejun Wu; Aoze Zheng; Xiyang Huang; Chenkang Zhu; Min Peng; Hongying Zan; Yu Song",
        "authorids": "/k/kunli-zhang/; /p/pengcheng-wu/; /b/bohan-yu/; /k/kejun-wu/; /a/aoze-zheng/; /x/xiyang-huang/; /c/chenkang-zhu/; /m/min-peng/; /h/hongying-zan/; /y/yu-song/",
        "bibtex": "@inproceedings{zhang-etal-2025-cadrl,\n    title = \"{C}a{DRL}: Document-level Relation Extraction via Context-aware Differentiable Rule Learning\",\n    author = \"Zhang, Kunli  and\n      Wu, Pengcheng  and\n      Yu, Bohan  and\n      Wu, Kejun  and\n      Zheng, Aoze  and\n      Huang, Xiyang  and\n      Zhu, Chenkang  and\n      Peng, Min  and\n      Zan, Hongying  and\n      Song, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.551/\",\n    pages = \"8272--8284\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.551.pdf",
        "site": "https://aclanthology.org/2025.coling-main.551/",
        "pdf_size": 906333,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mShRkyIPi1UJ:scholar.google.com/&scioq=CaDRL:+Document-level+Relation+Extraction+via+Context-aware+Differentiable+Rule+Learning&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science, Wuhan University\u266d; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021; School of Computer Science and Artificial Intelligence, Zhengzhou University\u2021*",
        "aff_domain": "zzu.edu.cn;zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn; ; ; ; ; ; ",
        "email": "zzu.edu.cn;zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;1;0;0",
        "aff_unique_norm": "Zhengzhou University;Wuhan University",
        "aff_unique_dep": "School of Computer Science and Artificial Intelligence;School of Computer Science",
        "aff_unique_url": "http://www.zzu.edu.cn;http://www.whu.edu.cn",
        "aff_unique_abbr": ";WHU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.273",
        "title": "Can LLMs Clarify? Investigation and Enhancement of Large Language Models on Argument Claim Optimization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In argumentation, the claim is the foundational proposition that underpins the argument, serving as the central pillar upon which the argument is constructed. It guides the subsequent presentation of evidence, reasoning, and analysis, thereby facilitating the audience\u2019s understanding of the core issue. Therefore, ensuring that the claim is precise and unambiguous is crucial for constructing a coherent and persuasive argument. While Large Language Models (LLMs) have demonstrated proficiency in text rewriting tasks such as style transfer and query rewriting, their application to claim optimization remains unexplored. Unlike other rewriting tasks, claim clarification requires the model to rewrite ambiguous or unclear segments of the claim, enhance the content by adding omitted key details, and eliminate redundant or verbose elements. Addressing this gap, this paper evaluates the performance of LLMs on the claim clarification task across various settings. While popular rewriting evaluation methods such as BLEU and Rouge rely on exact word matching, this paper introduces a novel semantic evaluation approach based on a sliding window mechanism. Three distinct LLMs, including LLama2, Mistral, and Qwen2, are assessed for their ability to clarify arguments through zero-shot or few-shot prompting, and supervised fine-tuning (SFT). Additionally, we propose a reinforcement learning-based clarification approach that optimally balances content preservation with claim clarity, thereby augmenting the performance of LLMs on the claim clarification task.",
        "author": "Yiran Wang; Ben He; Xuanang Chen; Le Sun",
        "authorids": "/y/yiran-wang/; /b/ben-he/; /x/xuanang-chen/; /l/le-sun/",
        "bibtex": "@inproceedings{wang-etal-2025-llms-clarify,\n    title = \"Can {LLM}s Clarify? Investigation and Enhancement of Large Language Models on Argument Claim Optimization\",\n    author = \"Wang, Yiran  and\n      He, Ben  and\n      Chen, Xuanang  and\n      Sun, Le\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.273/\",\n    pages = \"4066--4077\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.273.pdf",
        "site": "https://aclanthology.org/2025.coling-main.273/",
        "pdf_size": 373790,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:PIxoeQ2JNoQJ:scholar.google.com/&scioq=Can+LLMs+Clarify%3F+Investigation+and+Enhancement+of+Large+Language+Models+on+Argument+Claim+Optimization&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China+Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China+Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "mails.ucas.ac.cn;ucas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "email": "mails.ucas.ac.cn;ucas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;1;1",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Computer Science and Technology;Institute of Software",
        "aff_unique_url": "http://www.ucas.ac.cn;https://www.cas.cn",
        "aff_unique_abbr": "UCAS;CAS",
        "aff_campus_unique_index": "0+0;0+0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.681",
        "title": "Can LLMs Help Create Grammar?: Automating Grammar Creation for Endangered Languages with In-Context Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the present-day documenting and preserving endangered languages, the application of Large Language Models (LLMs) presents a promising approach. This paper explores how LLMs, particularly through in-context learning, can assist in generating grammatical information for low-resource languages with limited amount of data. We takes Moklen as a case study to evaluate the efficacy of LLMs in producing coherent grammatical rules and lexical entries using only bilingual dictionaries and parallel sentences of the unknown language without building the model from scratch. Our methodology involves organising the existing linguistic data and prompting to efficiently enable to generate formal XLE grammar. Our results demonstrate that LLMs can successfully capture key grammatical structures and lexical information, although challenges such as the potential for English grammatical biases remain. This study highlights the potential of LLMs to enhance language documentation efforts, providing a cost-effective solution for generating linguistic data and contributing to the preservation of endangered languages.",
        "author": "Piyapath T. Spencer; Nanthipat Kongborrirak",
        "authorids": "/p/piyapath-t-spencer/; /n/nanthipat-kongborrirak/",
        "bibtex": "@inproceedings{spencer-kongborrirak-2025-llms,\n    title = \"Can {LLM}s Help Create Grammar?: Automating Grammar Creation for Endangered Languages with In-Context Learning\",\n    author = \"Spencer, Piyapath T.  and\n      Kongborrirak, Nanthipat\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.681/\",\n    pages = \"10214--10227\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.681.pdf",
        "site": "https://aclanthology.org/2025.coling-main.681/",
        "pdf_size": 1133537,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17383419894489606431&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Language and Information Technology Programme, Faculty of Arts, CU, Thailand + Center for Information and Language Processing (CIS), LMU Munich, Germany; Language and Information Technology Programme, Faculty of Arts, CU, Thailand",
        "aff_domain": "piyapath.uk;gmail.com",
        "email": "piyapath.uk;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Chulalongkorn University;LMU Munich",
        "aff_unique_dep": "Faculty of Arts;Center for Information and Language Processing (CIS)",
        "aff_unique_url": "https://www.cu.ac.th;https://www.lmu.de",
        "aff_unique_abbr": "CU;LMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Thailand;Germany"
    },
    {
        "id": "2025.coling-main.541",
        "title": "Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study addresses critical gaps in Automatic Essay Scoring (AES) systems and Large Language Models (LLMs) with regard to their ability to effectively identify and score harmful essays. Despite advancements in AES technology, current models often overlook ethically and morally problematic elements within essays, erroneously assigning high scores to essays that may propagate harmful opinions. In this study, we introduce the Harmful Essay Detection (HED) benchmark, which includes essays integrating sensitive topics such as racism and gender bias, to test the efficacy of various LLMs in recognizing and scoring harmful content. Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring. The study underscores the need for developing more robust AES systems that are sensitive to the ethical implications of the content they are scoring.",
        "author": "Hongjin Kim; Jeonghyun Kang; Harksoo Kim",
        "authorids": "/h/hongjin-kim/; /j/jeonghyun-kang/; /h/harksoo-kim/",
        "bibtex": "@inproceedings{kim-etal-2025-large,\n    title = \"Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring\",\n    author = \"Kim, Hongjin  and\n      Kang, Jeonghyun  and\n      Kim, Harksoo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.541/\",\n    pages = \"8121--8147\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.541.pdf",
        "site": "https://aclanthology.org/2025.coling-main.541/",
        "pdf_size": 4526977,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14888138284088883690&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Konkuk University; Konkuk University; Konkuk University",
        "aff_domain": "konkuk.ac.kr;konkuk.ac.kr;konkuk.ac.kr",
        "email": "konkuk.ac.kr;konkuk.ac.kr;konkuk.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Konkuk University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.konkuk.edu",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-industry.14",
        "title": "Can Large Language Models Serve as Effective Classifiers for Hierarchical Multi-Label Classification of Scientific Documents at Industrial Scale?",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We address the task of hierarchical multi-label classification (HMC) of scientific documents at an industrial scale, where hundreds of thousands of documents must be classified across thousands of dynamic labels. The rapid growth of scientific publications necessitates scalable and efficient methods for classification, further complicated by the evolving nature of taxonomies\u2014where new categories are introduced, existing ones are merged, and outdated ones are deprecated. Traditional machine learning approaches, which require costly retraining with each taxonomy update, become impractical due to the high overhead of labelled data collection and model adaptation. Large Language Models (LLMs) have demonstrated great potential in complex tasks such as multi-label classification. However, applying them to large and dynamic taxonomies presents unique challenges as the vast number of labels can exceed LLMs\u2019 input limits. In this paper, we present novel methods that combine the strengths of LLMs with dense retrieval techniques to overcome these challenges. Our approach avoids frequent retraining by leveraging zero-shot and few-shot learning for real-time label assignment. We evaluate the effectiveness of our methods on SSRN, a large repository of preprints spanning multiple disciplines, and demonstrate significant improvements in both classification accuracy and cost-efficiency. By developing a tailored evaluation framework for dynamic taxonomies and publicly releasing our code, this research provides critical insights into applying LLMs for document classification, where the number of classes corresponds to the number of nodes in a large taxonomy, at an industrial scale, significantly contributing to both data science and natural language processing.",
        "author": "Seyed Amin Tabatabaei; Sarah Fancher; Michael Parsons; Arian Askari",
        "authorids": "/s/seyed-amin-tabatabaei/; /s/sarah-fancher/; /m/michael-parsons/; /a/arian-askari/",
        "bibtex": "@inproceedings{tabatabaei-etal-2025-large,\n    title = \"Can Large Language Models Serve as Effective Classifiers for Hierarchical Multi-Label Classification of Scientific Documents at Industrial Scale?\",\n    author = \"Tabatabaei, Seyed Amin  and\n      Fancher, Sarah  and\n      Parsons, Michael  and\n      Askari, Arian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.14/\",\n    pages = \"163--174\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.14.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.14/",
        "pdf_size": 602145,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15628980954984904365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Elsevier; SSRN; SSRN; Leiden University",
        "aff_domain": "elsevier.com;ssrn.com;ssrn.com;liacs.leidenuniv.nl",
        "email": "elsevier.com;ssrn.com;ssrn.com;liacs.leidenuniv.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Elsevier;Social Science Research Network;Leiden University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.elsevier.com;https://www.ssrn.com;https://www.leidenuniv.nl",
        "aff_unique_abbr": "Elsevier;SSRN;LU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "2025.coling-main.339",
        "title": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The Myers-Briggs Type Indicator (MBTI) is one of the most influential personality theories reflecting individual differences in thinking, feeling, and behaving. MBTI personality detection has garnered considerable research interest and has evolved significantly over the years. However, this task tends to be overly optimistic, as it currently does not align well with the natural distribution of population personality traits. Specifically, the self-reported labels in existing datasets result in data quality issues and the hard labels fail to capture the full range of population personality distributions. In this paper, we identify the task by constructing MBTIBench, the first manually annotated MBTI personality detection dataset with soft labels, under the guidance of psychologists. Our experimental results confirm that soft labels can provide more benefits to other psychological tasks than hard labels. We highlight the polarized predictions and biases in LLMs as key directions for future research.",
        "author": "Bohan Li; Jiannan Guan; Longxu Dou; Yunlong Feng; Dingzirui Wang; Yang Xu; Enbo Wang; Qiguang Chen; Bichen Wang; Xiao Xu; Yimeng Zhang; Libo Qin; Yanyan Zhao; Qingfu Zhu; Wanxiang Che",
        "authorids": "/b/bohan-li/; /j/jiannan-guan/; /l/longxu-dou/; /y/yunlong-feng/; /d/dingzirui-wang/; /y/yang-xu/; /e/enbo-wang/; /q/qiguang-chen/; /b/bichen-wang/; /x/xiao-xu/; /y/yimeng-zhang/; /l/libo-qin/; /y/yanyan-zhao/; /q/qingfu-zhu/; /w/wanxiang-che/",
        "bibtex": "@inproceedings{li-etal-2025-large,\n    title = \"Can Large Language Models Understand You Better? An {MBTI} Personality Detection Dataset Aligned with Population Traits\",\n    author = \"Li, Bohan  and\n      Guan, Jiannan  and\n      Dou, Longxu  and\n      Feng, Yunlong  and\n      Wang, Dingzirui  and\n      Xu, Yang  and\n      Wang, Enbo  and\n      Chen, Qiguang  and\n      Wang, Bichen  and\n      Xu, Xiao  and\n      Zhang, Yimeng  and\n      Qin, Libo  and\n      Zhao, Yanyan  and\n      Zhu, Qingfu  and\n      Che, Wanxiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.339/\",\n    pages = \"5071--5081\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.339.pdf",
        "site": "https://aclanthology.org/2025.coling-main.339/",
        "pdf_size": 1319510,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16952493291251122836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;",
        "github": "https://github.com/Personality-NLP/MbtiBench",
        "project": "",
        "author_num": 15
    },
    {
        "id": "2025.coling-main.569",
        "title": "Can Large Language Models perform Relation-based Argument Mining?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Relation-based Argument Mining (RbAM) is the process of automatically determining agreement (support) and disagreement (attack) relations amongst textual arguments (in the binary prediction setting), or neither relation (in the ternary prediction setting). As the number of platforms supporting online debate increases, the need for RbAM becomes ever more urgent, especially in support of downstream tasks. RbAM is a challenging classification task, with existing state-of-the-art methods, based on Language Models (LMs), failing to perform satisfactorily across different datasets. In this paper, we show that general-purpose Large LMs (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) and with GPT-3.5-turbo on several datasets for (binary and ternary) RbAM, as well as with GPT-4o-mini on samples (to limit costs) from the datasets.",
        "author": "Deniz Gorur; Antonio Rago; Francesca Toni",
        "authorids": "/d/deniz-gorur/; /a/antonio-rago/; /f/francesca-toni/",
        "bibtex": "@inproceedings{gorur-etal-2025-large,\n    title = \"Can Large Language Models perform Relation-based Argument Mining?\",\n    author = \"Gorur, Deniz  and\n      Rago, Antonio  and\n      Toni, Francesca\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.569/\",\n    pages = \"8518--8534\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.569.pdf",
        "site": "https://aclanthology.org/2025.coling-main.569/",
        "pdf_size": 3570449,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17318592239300419751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK",
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "github": "",
        "project": "www.kialo.com; www.argucast.herokuapp.com/",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Department of Computing",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "Imperial",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.548",
        "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary Empirical Study",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Utilizing Large Language Models (LLMs) as evaluators to assess the performance of other LLMs has garnered attention. However, this evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs. To address this issue, we propose and explore two many-shot In-Context Learning (ICL) prompt templates to help LLM evaluators mitigate potential biases: Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the former utilizes in-context examples with model-generated rationales as references, while the latter does not include these references. Using these prompt designs, we investigate the impact of increasing the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4, perform better in the many-shot regime than in the zero-shot regime. Furthermore, in most cases, MSwR performs significantly better than MSoR.",
        "author": "Mingyang Song; Mao Zheng; Xuan Luo",
        "authorids": "/m/mingyang-song/; /m/mao-zheng/; /x/xuan-luo/",
        "bibtex": "@inproceedings{song-etal-2025-many,\n    title = \"Can Many-Shot In-Context Learning Help {LLM}s as Evaluators? A Preliminary Empirical Study\",\n    author = \"Song, Mingyang  and\n      Zheng, Mao  and\n      Luo, Xuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.548/\",\n    pages = \"8232--8241\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.548.pdf",
        "site": "https://aclanthology.org/2025.coling-main.548/",
        "pdf_size": 566650,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12487159894744434198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Tencent Hunyuan; Tencent Hunyuan; Tencent Hunyuan",
        "aff_domain": "tencent.com; ; ",
        "email": "tencent.com; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Hunyuan",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.749",
        "title": "Can Model Uncertainty Function as a Proxy for Multiple-Choice Question Item Difficulty?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Estimating the difficulty of multiple-choice questions would be great help for educators who must spend substantial time creating and piloting stimuli for their tests, and for learners who want to practice. Supervised approaches to difficulty estimation have yielded to date mixed results. In this contribution we leverage an aspect of generative large models which might be seen as a weakness when answering questions, namely their uncertainty. Specifically, we exploit model uncertainty towards exploring correlations between two different metrics of uncertainty, and the actual student response distribution. While we observe some present but weak correlations, we also discover that the models\u2019 behaviour is different in the case of correct vs wrong answers, and that correlations differ substantially according to the different question types which are included in our fine-grained, previously unused dataset of 451 questions from a Biopsychology course. In discussing our findings, we also suggest potential avenues to further leverage model uncertainty as an additional proxy for item difficulty.",
        "author": "Leonidas Zotos; Hedderik van Rijn; Malvina Nissim",
        "authorids": "/l/leonidas-zotos/; /h/hedderik-van-rijn/; /m/malvina-nissim/",
        "bibtex": "@inproceedings{zotos-etal-2025-model,\n    title = \"Can Model Uncertainty Function as a Proxy for Multiple-Choice Question Item Difficulty?\",\n    author = \"Zotos, Leonidas  and\n      van Rijn, Hedderik  and\n      Nissim, Malvina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.749/\",\n    pages = \"11304--11316\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.749.pdf",
        "site": "https://aclanthology.org/2025.coling-main.749/",
        "pdf_size": 2184197,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17291449794683392441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Groningen; University of Groningen; University of Groningen",
        "aff_domain": "rug.nl;rug.nl;rug.nl",
        "email": "rug.nl;rug.nl;rug.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Groningen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rug.nl",
        "aff_unique_abbr": "RUG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2025.coling-main.471",
        "title": "Can We Afford The Perfect Prompt? Balancing Cost and Accuracy with the Economical Prompting Index",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As prompt engineering research rapidly evolves, evaluations beyond accuracy are crucial for developing cost-effective techniques. We present the Economical Prompting Index (EPI), a novel metric that combines accuracy scores with token consumption, adjusted by a user-specified cost concern level to reflect different resource constraints. Our study examines 6 advanced prompting techniques, including Chain-of-Thought, Self-Consistency, and Tree of Thoughts, across 10 widely-used language models and 4 diverse datasets. We demonstrate that approaches such as Self-Consistency often provide statistically insignificant gains while becoming cost-prohibitive. For example, on high-performing models like Claude 3.5 Sonnet, the EPI of simpler techniques like Chain-of-Thought (0.72) surpasses more complex methods like Self-Consistency (0.64) at slight cost concern levels. Our findings suggest a reevaluation of complex prompting strategies in resource-constrained scenarios, potentially reshaping future research priorities and improving cost-effectiveness for end-users.",
        "author": "Tyler McDonald; Anthony Colosimo; Yifeng Li; Ali Emami",
        "authorids": "/t/tyler-mcdonald/; /a/anthony-colosimo/; /y/yifeng-li/; /a/ali-emami/",
        "bibtex": "@inproceedings{mcdonald-etal-2025-afford,\n    title = \"Can We Afford The Perfect Prompt? Balancing Cost and Accuracy with the Economical Prompting Index\",\n    author = \"McDonald, Tyler  and\n      Colosimo, Anthony  and\n      Li, Yifeng  and\n      Emami, Ali\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.471/\",\n    pages = \"7075--7086\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.471.pdf",
        "site": "https://aclanthology.org/2025.coling-main.471/",
        "pdf_size": 1175300,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8391485127010397643&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Brock University; Brock University; Brock University; Brock University",
        "aff_domain": "brocku.ca;brocku.ca;brocku.ca;brocku.ca",
        "email": "brocku.ca;brocku.ca;brocku.ca;brocku.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Brock University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.brocku.ca",
        "aff_unique_abbr": "Brock",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-industry.29",
        "title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In today\u2019s assistant landscape, personalisation enhances interactions, fosters long-term relationships, and deepens engagement. However, many systems struggle with retaining user preferences, leading to repetitive user requests and disengagement. Furthermore, the unregulated and opaque extraction of user preferences in industry applications raises significant concerns about privacy and trust, especially in regions with stringent regulations like Europe. In response to these challenges, we propose a long-term memory system for voice assistants, structured around predefined categories. This approach leverages Large Language Models to efficiently extract, store, and retrieve preferences within these categories, ensuring both personalisation and transparency. We also introduce a synthetic multi-turn, multi-session conversation dataset (CarMem), grounded in real industry data, tailored to an in-car voice assistant setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to .95 in preference extraction, depending on category granularity. Our maintenance strategy reduces redundant preferences by 95% and contradictory ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively, the results demonstrate the system\u2019s suitability for industrial applications.",
        "author": "Johannes Kirmayr; Lukas Stappen; Phillip Schneider; Florian Matthes; Elisabeth Andre",
        "authorids": "/j/johannes-kirmayr/; /l/lukas-stappen/; /p/phillip-schneider/; /f/florian-matthes/; /e/elisabeth-andre/",
        "bibtex": "@inproceedings{kirmayr-etal-2025-carmem,\n    title = \"{C}ar{M}em: Enhancing Long-Term Memory in {LLM} Voice Assistants through Category-Bounding\",\n    author = \"Kirmayr, Johannes  and\n      Stappen, Lukas  and\n      Schneider, Phillip  and\n      Matthes, Florian  and\n      Andre, Elisabeth\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.29/\",\n    pages = \"343--357\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.29.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.29/",
        "pdf_size": 1464985,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2431587782848031420&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "BMW Group Research and Technology, Munich, Germany+Chair for Human-Centered Artificial Intelligence, University of Augsburg, Germany; BMW Group Research and Technology, Munich, Germany; Chair for Software Engineering for Business Information Systems, Technical University of Munich, Germany; Chair for Software Engineering for Business Information Systems, Technical University of Munich, Germany; Chair for Human-Centered Artificial Intelligence, University of Augsburg, Germany",
        "aff_domain": "bmwgroup.com; ; ; ; ",
        "email": "bmwgroup.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;2;2;1",
        "aff_unique_norm": "BMW Group Research and Technology;University of Augsburg;Technical University of Munich",
        "aff_unique_dep": ";Chair for Human-Centered Artificial Intelligence;Chair for Software Engineering for Business Information Systems",
        "aff_unique_url": "https://www.bmwgroup.com;https://www.uni-augsburg.de;https://www.tum.de",
        "aff_unique_abbr": "BMW Group;;TUM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.733",
        "title": "Case2Code: Scalable Synthetic Data for Code Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown outstanding breakthroughs in code generation. Recent work improves code LLMs by training on synthetic data generated by some powerful LLMs, which can be challenging to scale due to the dependence on a teacher model and high generation costs. In this paper, we focus on synthesizing code data at scale and propose a Case2Code task by exploiting the expressiveness and correctness of programs. Case2Code is an inductive inference task that aims to infer underlying code implementations by observing input-output examples or program behaviors, By incorporating LLMs to generate program inputs, and executing the program with these inputs to obtain the program outputs, we can synthesize diverse and high-quality Case2Code data at scale for training and evaluating code LLMs. Experimental results show that case-to-code induction is challenging for current representative LLMs if they are untrained. Models trained with Case2Code improve performance not only on distribution case-to-code induction but also various coding-generation tasks, demonstrating the great potential of large-scale synthetic data and inductive learning.",
        "author": "Yunfan Shao; Linyang Li; Yichuan Ma; Peiji Li; Demin Song; Qinyuan Cheng; Shimin Li; Xiaonan Li; Pengyu Wang; Qipeng Guo; Hang Yan; Xipeng Qiu; Xuanjing Huang; Dahua Lin",
        "authorids": "/y/yunfan-shao/; /l/linyang-li/; /y/yichuan-ma/; /p/peiji-li/; /d/demin-song/; /q/qinyuan-cheng/; /s/shimin-li/; /x/xiaonan-li/; /p/pengyu-wang/; /q/qipeng-guo/; /h/hang-yan/; /x/xipeng-qiu/; /x/xuan-jing-huang/; /d/dahua-lin/",
        "bibtex": "@inproceedings{shao-etal-2025-case2code,\n    title = \"{C}ase2{C}ode: Scalable Synthetic Data for Code Generation\",\n    author = \"Shao, Yunfan  and\n      Li, Linyang  and\n      Ma, Yichuan  and\n      Li, Peiji  and\n      Song, Demin  and\n      Cheng, Qinyuan  and\n      Li, Shimin  and\n      Li, Xiaonan  and\n      Wang, Pengyu  and\n      Guo, Qipeng  and\n      Yan, Hang  and\n      Qiu, Xipeng  and\n      Huang, Xuanjing  and\n      Lin, Dahua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.733/\",\n    pages = \"11056--11069\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.733.pdf",
        "site": "https://aclanthology.org/2025.coling-main.733/",
        "pdf_size": 632782,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14691141419073637548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Fudan University+Shanghai AI Laboratory+The Chinese University of Hong Kong; Shanghai AI Laboratory; School of Computer Science, Fudan University+Shanghai AI Laboratory; School of Computer Science, Fudan University+Shanghai AI Laboratory; Shanghai AI Laboratory; School of Computer Science, Fudan University+Shanghai AI Laboratory; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; Shanghai AI Laboratory; Shanghai AI Laboratory+The Chinese University of Hong Kong; School of Computer Science, Fudan University; School of Computer Science, Fudan University; Shanghai AI Laboratory+The Chinese University of Hong Kong",
        "aff_domain": "fudan.edu.cn;pjlab.org.cn; ; ; ; ; ; ; ;pjlab.org.cn;fudan.edu.cn; ; ;",
        "email": "fudan.edu.cn;pjlab.org.cn; ; ; ; ; ; ; ;pjlab.org.cn;fudan.edu.cn; ; ;",
        "github": "https://github.com/choosewhatulike/case2code",
        "project": "",
        "author_num": 14,
        "aff_unique_index": "0+1+2;1;0+1;0+1;1;0+1;0;0;0;1;1+2;0;0;1+2",
        "aff_unique_norm": "Fudan University;Shanghai AI Laboratory;The Chinese University of Hong Kong",
        "aff_unique_dep": "School of Computer Science;;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.shanghai-ai-lab.com;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Fudan;SAIL;CUHK",
        "aff_campus_unique_index": "1;;;;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0+0;0;0+0;0+0;0;0+0;0;0;0;0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.399",
        "title": "CateEA: Enhancing Entity Alignment via Implicit Category Supervision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity Alignment (EA) is essential for integrating Knowledge Graphs (KGs) by matching equivalent entities across diverse KGs. With the rise of multi-modal KGs, which emerged to better depict real-world KGs by integrating visual, textual, and structured data, Multi-Modal Entity Alignment (MMEA) has become crucial in enhancing EA. However, existing MMEA methods often neglect the inherent semantic category information of entities, limiting alignment precision and robustness. To address this, we propose Category-enhanced Entity Alignment (CateEA), which combines implicit entity category information into multi-modal representations. By generating pseudo-category labels from entity embeddings and integrating them into a multi-task learning framework, CateEA captures latent category semantics, enhancing entity representations. CateEA allows for adaptive adjustments of similarity measures, leading to improved alignment precision and robustness in multi-modal contexts. Experiments on benchmark datasets demonstrate that CateEA outperforms state-of-the-art methods in various settings.",
        "author": "Guan Dong Feng; Tao Ren; Jun Hu; Dan dan Wang",
        "authorids": "/g/guan-dong-feng/; /t/tao-ren/; /j/jun-hu/; /d/dan-dan-wang/",
        "bibtex": "@inproceedings{feng-etal-2025-cateea,\n    title = \"{C}ate{EA}: Enhancing Entity Alignment via Implicit Category Supervision\",\n    author = \"Feng, Guan Dong  and\n      Ren, Tao  and\n      Hu, Jun  and\n      Wang, Dan dan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.399/\",\n    pages = \"5975--5986\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.399.pdf",
        "site": "https://aclanthology.org/2025.coling-main.399/",
        "pdf_size": 2085277,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cYs1fC_grzMJ:scholar.google.com/&scioq=CateEA:+Enhancing+Entity+Alignment+via+Implicit+Category+Supervision&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Intelligent Game, Institute of Software, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beiiing, China; State Key Laboratory of Intelligent Game, Institute of Software, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beiiing, China; State Key Laboratory of Intelligent Game, Institute of Software, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beiiing, China; State Key Laboratory of Intelligent Game, Institute of Software, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beiiing, China",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "github": "https://github.com/Melkor0007/CateEA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Software;",
        "aff_unique_url": "http://www.ios.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.161",
        "title": "CausalScore: An Automatic Reference-Free Metric for Assessing Response Relevance in Open-Domain Dialogue Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatically evaluating the quality of responses in dialogue systems is a challenging yet crucial task. Current metrics often fail to align with human judgments, especially when assessing responses that are grammatically correct. To address this issue, we propose a novel metric, called CausalScore, which assesses the relevance of responses by measuring the causal strength between dialogue histories and responses. The causal strength is estimated by utilizing both unconditional dependence and conditional dependencies from dialogue histories to responses. We compare our metric with the existing competitive metrics in terms of their alignment with human judgements. Our experimental results demonstrate that CausalScore significantly surpasses existing state-of-the-art metrics by aligning better with human judgements. Additionally, we collect a dialogue dataset CGDIALOG+ with human-annotated causal relations and a set of pairwise human judgements to facilitate the development of automatic metrics.",
        "author": "Tao Feng; Lizhen Qu; Xiaoxi Kang; Gholamreza Haffari",
        "authorids": "/t/tao-feng/; /l/lizhen-qu/; /x/xiaoxi-kang/; /g/gholamreza-haffari/",
        "bibtex": "@inproceedings{feng-etal-2025-causalscore,\n    title = \"{C}ausal{S}core: An Automatic Reference-Free Metric for Assessing Response Relevance in Open-Domain Dialogue Systems\",\n    author = \"Feng, Tao  and\n      Qu, Lizhen  and\n      Kang, Xiaoxi  and\n      Haffari, Gholamreza\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.161/\",\n    pages = \"2351--2369\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.161.pdf",
        "site": "https://aclanthology.org/2025.coling-main.161/",
        "pdf_size": 1702911,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9866904099836074534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Monash University, Australia; Monash University, Australia; Monash University, Australia; Monash University, Australia",
        "aff_domain": "monash.edu;monash.edu;monash.edu;monash.edu",
        "email": "monash.edu;monash.edu;monash.edu;monash.edu",
        "github": "https://github.com/WilliamsToTo/causalscore_dialogue",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2025.coling-main.734",
        "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Open-ended question answering requires mod- els to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable ev- idence selection and in-depth question analysis. In this paper, we propose a novel Chain-of- Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide more correct and more comprehensive answers for open-ended QA, although they are not strong enough individually. Our exper- iments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers.",
        "author": "Mingxu Tao; Dongyan Zhao; Yansong Feng",
        "authorids": "/m/mingxu-tao/; /d/dongyan-zhao/; /y/yansong-feng/",
        "bibtex": "@inproceedings{tao-etal-2025-chain,\n    title = \"Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering\",\n    author = \"Tao, Mingxu  and\n      Zhao, Dongyan  and\n      Feng, Yansong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.734/\",\n    pages = \"11070--11085\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.734.pdf",
        "site": "https://aclanthology.org/2025.coling-main.734/",
        "pdf_size": 906994,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16485170435176237136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + Key Laboratory of Intelligent Press Media Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + Key Laboratory of Intelligent Press Media Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0+0;0+0+0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.164",
        "title": "Chain-of-Specificity: Enhancing Task-Specific Constraint Adherence in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints, such as being in a specific place or at a specific time, and at times even overlook them, which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing and rewriting input instructions or reflecting on prior failings, yet they fall short in adequately emphasizing specific constraints and unlocking the underlying knowledge, such as programming within the context of software development. In response, this paper proposes a simple yet effective method called Chain-of-Specificity (CoS). Specifically, CoS emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses. Experiments conducted on publicly available and self-built complex datasets demonstrate that CoS outperforms existing methods in enhancing generated content, especially in terms of specificity. Additionally, as the number of specific constraints increases, other baselines falter, while CoS still performs well. Moreover, we show that distilling responses generated by CoS effectively enhances the ability of smaller models to follow constrained instructions.",
        "author": "Kaiwen Wei; Jiang Zhong; Hongzhi Zhang; Fuzheng Zhang; Di Zhang; Li Jin; Yue Yu; Jingyuan Zhang",
        "authorids": "/k/kaiwen-wei/; /j/jiang-zhong/; /h/hongzhi-zhang/; /f/fuzheng-zhang/; /d/di-zhang/; /l/li-jin/; /y/yue-yu/; /j/jingyuan-zhang/",
        "bibtex": "@inproceedings{wei-etal-2025-chain,\n    title = \"Chain-of-Specificity: Enhancing Task-Specific Constraint Adherence in Large Language Models\",\n    author = \"Wei, Kaiwen  and\n      Zhong, Jiang  and\n      Zhang, Hongzhi  and\n      Zhang, Fuzheng  and\n      Zhang, Di  and\n      Jin, Li  and\n      Yu, Yue  and\n      Zhang, Jingyuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.164/\",\n    pages = \"2401--2416\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.164.pdf",
        "site": "https://aclanthology.org/2025.coling-main.164/",
        "pdf_size": 1533269,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2601403447672995654&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.589",
        "title": "CharMoral: A Character Morality Dataset for Morally Dynamic Character Analysis in Long-Form Narratives",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper introduces CharMoral, a dataset designed to analyze the moral evolution of characters in long-form narratives. CharMoral, built from 1,337 movie synopses, includes annotations for character actions, context, and morality labels. To automatically construct CharMoral, we propose a four-stage framework, utilizing Large Language Models, to automatically classify actions as moral or immoral based on context. Human evaluations and various experiments confirm the framework\u2019s effectiveness in moral reasoning tasks in multiple genres. Our code and the CharMoral dataset are publicly available at https://github.com/BaeSuyoung/CharMoral.",
        "author": "Suyoung Bae; Gunhee Cho; Yun-Gyung Cheong; Boyang Li",
        "authorids": "/s/suyoung-bae/; /g/gunhee-cho/; /y/yun-gyung-cheong/; /b/boyang-li/",
        "bibtex": "@inproceedings{bae-etal-2025-charmoral,\n    title = \"{C}har{M}oral: A Character Morality Dataset for Morally Dynamic Character Analysis in Long-Form Narratives\",\n    author = \"Bae, Suyoung  and\n      Cho, Gunhee  and\n      Cheong, Yun-Gyung  and\n      Li, Boyang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.589/\",\n    pages = \"8809--8818\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.589.pdf",
        "site": "https://aclanthology.org/2025.coling-main.589/",
        "pdf_size": 630392,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:f9CYq8NdkssJ:scholar.google.com/&scioq=CharMoral:+A+Character+Morality+Dataset+for+Morally+Dynamic+Character+Analysis+in+Long-Form+Narratives&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Sungkyunkwan University; Sungkyunkwan University; Nanyang Technological University; Sungkyunkwan University",
        "aff_domain": "skku.edu;skku.edu;ntu.edu.sg;skku.edu",
        "email": "skku.edu;skku.edu;ntu.edu.sg;skku.edu",
        "github": "https://github.com/BaeSuyoung/CharMoral",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Sungkyunkwan University;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.skku.edu;https://www.ntu.edu.sg",
        "aff_unique_abbr": "SKKU;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "South Korea;Singapore"
    },
    {
        "id": "2025.coling-industry.54",
        "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. We address these important drawbacks and introduce ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. Our simple approach achieves state-of-the-art results across 5 benchmarks spanning chart summarization, question answering, and fact-checking, and our elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. We release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.",
        "author": "Ahmed Masry; Megh Thakkar; Aayush Bajaj; Aaryaman Kartha; Enamul Hoque; Shafiq Joty",
        "authorids": "/a/ahmed-masry/; /m/megh-thakkar/; /a/aayush-bajaj/; /a/aaryaman-kartha/; /e/enamul-hoque/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{masry-etal-2025-chartgemma,\n    title = \"{C}hart{G}emma: Visual Instruction-tuning for Chart Reasoning in the Wild\",\n    author = \"Masry, Ahmed  and\n      Thakkar, Megh  and\n      Bajaj, Aayush  and\n      Kartha, Aaryaman  and\n      Hoque, Enamul  and\n      Joty, Shafiq\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.54/\",\n    pages = \"625--643\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.54.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.54/",
        "pdf_size": 3726330,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9293659082972519833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "York University, Canada; Chandar Research Lab + MILA - Quebec AI Institute; Chandar Research Lab + MILA - Quebec AI Institute; York University, Canada + Chandar Research Lab + MILA - Quebec AI Institute; York University, Canada; Salesforce Research + Nanyang Technological University, Singapore",
        "aff_domain": "yorku.ca;mila.quebec;mila.quebec;yorku.ca;yorku.ca;salesforce.com",
        "email": "yorku.ca;mila.quebec;mila.quebec;yorku.ca;yorku.ca;salesforce.com",
        "github": "https://github.com/vis-nlp/ChartGemma",
        "project": "https://arxiv.org/abs/2407.04172",
        "author_num": 6,
        "aff_unique_index": "0;1+2;1+2;0+1+2;0;3+4",
        "aff_unique_norm": "York University;Chandar Research Lab;Quebec AI Institute;Salesforce;Nanyang Technological University",
        "aff_unique_dep": ";;MILA;Salesforce Research;",
        "aff_unique_url": "https://www.yorku.ca;;https://mila.quebec;https://research.salesforce.com;https://www.ntu.edu.sg",
        "aff_unique_abbr": "York U;;MILA;Salesforce;NTU",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;2+3",
        "aff_country_unique": "Canada;;United States;Singapore"
    },
    {
        "id": "2025.coling-main.501",
        "title": "Charting the Future: Using Chart Question-Answering for Scalable Evaluation of LLM-Driven Data Visualizations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a novel framework that leverages Visual Question Answering (VQA) models to automate the evaluation of LLM-generated data visualizations. Traditional evaluation methods often rely on human judgment, which is costly and unscalable, or focus solely on data accuracy, neglecting the effectiveness of visual communication. By employing VQA models, we assess data representation quality and the general communicative clarity of charts. Experiments were conducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with visualizations generated by OpenAI\u2019s GPT-3.5 Turbo and Meta\u2019s Llama 3.1 70B-Instruct models. Our results indicate that LLM-generated charts do not match the accuracy of the original non-LLM-generated charts based on VQA performance measures. Moreover, while our results demonstrate that few-shot prompting significantly boosts the accuracy of chart generation, considerable progress remains to be made before LLMs can fully match the precision of human-generated graphs. This underscores the importance of our work, which expedites the research process by enabling rapid iteration without the need for human annotation, thus accelerating advancements in this field.",
        "author": "James Ford; Xingmeng Zhao; Dan Schumacher; Anthony Rios",
        "authorids": "/j/james-ford/; /x/xingmeng-zhao/; /d/dan-schumacher/; /a/anthony-rios/",
        "bibtex": "@inproceedings{ford-etal-2025-charting,\n    title = \"Charting the Future: Using Chart Question-Answering for Scalable Evaluation of {LLM}-Driven Data Visualizations\",\n    author = \"Ford, James  and\n      Zhao, Xingmeng  and\n      Schumacher, Dan  and\n      Rios, Anthony\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.501/\",\n    pages = \"7497--7510\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.501.pdf",
        "site": "https://aclanthology.org/2025.coling-main.501/",
        "pdf_size": 1811840,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6870458227359687878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Information Systems and Cyber Security, The University of Texas at San Antonio; Department of Information Systems and Cyber Security, The University of Texas at San Antonio; Department of Information Systems and Cyber Security, The University of Texas at San Antonio; Department of Information Systems and Cyber Security, The University of Texas at San Antonio",
        "aff_domain": "utsa.edu; ; ;utsa.edu",
        "email": "utsa.edu; ; ;utsa.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The University of Texas at San Antonio",
        "aff_unique_dep": "Department of Information Systems and Cyber Security",
        "aff_unique_url": "https://www.utsa.edu",
        "aff_unique_abbr": "UTSA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Antonio",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.244",
        "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.",
        "author": "Yutong Li; Lu Chen; Aiwei Liu; Kai Yu; Lijie Wen",
        "authorids": "/y/yutong-li/; /l/lu-chen/; /a/aiwei-liu/; /k/kai-yu/; /l/lijie-wen/",
        "bibtex": "@inproceedings{li-etal-2025-chatcite,\n    title = \"{C}hat{C}ite: {LLM} Agent with Human Workflow Guidance for Comparative Literature Summary\",\n    author = \"Li, Yutong  and\n      Chen, Lu  and\n      Liu, Aiwei  and\n      Yu, Kai  and\n      Wen, Lijie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.244/\",\n    pages = \"3613--3630\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.244.pdf",
        "site": "https://aclanthology.org/2025.coling-main.244/",
        "pdf_size": 1206989,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16014920686036068750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tsinghua University, Beijing, China; X-LANCE Lab, Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence, SJTU AI Institute Shanghai Jiao Tong University, Shanghai, China + Suzhou Laboratory, Suzhou, China; Tsinghua University, Beijing, China; X-LANCE Lab, Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence, SJTU AI Institute Shanghai Jiao Tong University, Shanghai, China; Tsinghua University, Beijing, China",
        "aff_domain": "mails.tsinghua.edu.cn;sjtu.edu.cn; ; ;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;sjtu.edu.cn; ; ;tsinghua.edu.cn",
        "github": "https://github.com/miaisamelia/ChatCite",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+2;0;1;0",
        "aff_unique_norm": "Tsinghua University;Shanghai Jiao Tong University;Suzhou Laboratory",
        "aff_unique_dep": ";Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "THU;SJTU;",
        "aff_campus_unique_index": "0;1+2;0;1;0",
        "aff_campus_unique": "Beijing;Shanghai;Suzhou",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.605",
        "title": "Chinese Automatic Readability Assessment Using Adaptive Pre-training and Linguistic Feature Fusion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chinese Automatic Readability Assessment (ARA) aims to classify the reading difficulty of Chinese texts. To address the issues of insufficient high-quality training data and underutilization of linguistic features in existing methods, we propose a method that combines adaptive pre-training with feature fusion based on an interactive attention mechanism. First, we enhance the model\u2019s ability to capture different text difficulties through domain- and task-specific adaptive pre-training. Then, we propose an Adaptive Task-guided Corpus Filtering (ATCF) method, utilizing embeddings generated by the pre-trained model and applying nearest-neighbor search along with a sample balancing mechanism to ensure comprehensive learning across various difficulty levels. Finally, we propose an Interactive Attention-Driven Feature Fusion method that integrates linguistic and deep features, providing rich difficulty information to the model. Experiments on Chinese textbook dataset demonstrate that our method achieves state-of-the-art (SOTA) performance. Transfer learning experiments further indicate that our approach generalizes well to extracurricular reading and Chinese as a Foreign Language (CFL) ARA tasks.",
        "author": "Xusheng Yang; Jincai Yang; Xiao Li",
        "authorids": "/x/xusheng-yang/; /j/jincai-yang/; /x/xiao-li/",
        "bibtex": "@inproceedings{yang-etal-2025-chinese,\n    title = \"{C}hinese Automatic Readability Assessment Using Adaptive Pre-training and Linguistic Feature Fusion\",\n    author = \"Yang, Xusheng  and\n      Yang, Jincai  and\n      Li, Xiao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.605/\",\n    pages = \"9013--9024\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.605.pdf",
        "site": "https://aclanthology.org/2025.coling-main.605/",
        "pdf_size": 1194373,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kWnqpmOMyREJ:scholar.google.com/&scioq=Chinese+Automatic+Readability+Assessment+Using+Adaptive+Pre-training+and+Linguistic+Feature+Fusion&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Faculty of Artificial Intelligence in Education, Central China Normal University; School of Computer Science, Central China Normal University; Chengdu Yandaojie Primary School",
        "aff_domain": "outlook.com;ccnu.edu.cn;qq.com",
        "email": "outlook.com;ccnu.edu.cn;qq.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Central China Normal University;Chengdu Yandaojie Primary School",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.ccnu.edu.cn;",
        "aff_unique_abbr": "CCNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.69",
        "title": "Citation Amnesia: On The Recency Bias of NLP and Other Academic Fields",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study examines the tendency to cite older work across 20 fields of study over 43 years (1980\u20132023). We put NLP\u2019s propensity to cite older work in the context of these 20 other fields to analyze whether NLP shows similar temporal citation patterns to them over time or whether differences can be observed. Our analysis, based on a dataset of ~240 million papers, reveals a broader scientific trend: many fields have markedly declined in citing older works (e.g., psychology, computer science). The trend is strongest in NLP and ML research (-12.8% and -5.5% in citation age from previous peaks). Our results suggest that citing more recent works is not directly driven by the growth in publication rates (-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) \u2014 even when controlling for an increase in the volume of papers. Our findings raise questions about the scientific community\u2019s engagement with past literature, particularly for NLP, and the potential consequences of neglecting older but relevant research. The data and a demo showcasing our results are publicly available.",
        "author": "Jan Philip Wahle; Terry Lima Ruas; Mohamed Abdalla; Bela Gipp; Saif M. Mohammad",
        "authorids": "/j/jan-philip-wahle/; /t/terry-lima-ruas/; /m/mohamed-abdalla/; /b/bela-gipp/; /s/saif-mohammad/",
        "bibtex": "@inproceedings{wahle-etal-2025-citation,\n    title = \"Citation Amnesia: On The Recency Bias of {NLP} and Other Academic Fields\",\n    author = \"Wahle, Jan Philip  and\n      Lima Ruas, Terry  and\n      Abdalla, Mohamed  and\n      Gipp, Bela  and\n      Mohammad, Saif M.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.69/\",\n    pages = \"1027--1044\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.69.pdf",
        "site": "https://aclanthology.org/2025.coling-main.69/",
        "pdf_size": 728664,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18430906625170113728&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of G\u00f6ttingen, Germany+National Research Council, Canada; University of G\u00f6ttingen, Germany; University of Alberta, Canada; University of G\u00f6ttingen, Germany; National Research Council, Canada",
        "aff_domain": "uni-goettingen.de; ; ; ; ",
        "email": "uni-goettingen.de; ; ; ; ",
        "github": "https://github.com/jpwahle/coling2025-citation-age",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;2;0;1",
        "aff_unique_norm": "University of G\u00f6ttingen;National Research Council;University of Alberta",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uni-goettingen.de;https://www.nrc-cnrc.gc.ca;https://www.ualberta.ca",
        "aff_unique_abbr": "Georg-August-Universit\u00e4t;NRC;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1;0;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "id": "2025.coling-main.270",
        "title": "Claim veracity assessment for explainable fake news detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid growth of social network services, misinformation has spread uncontrollably. Most recent approaches to fake news detection use neural network models to predict whether the input text is fake or real. Some of them even provide explanations, in addition to veracity, generated by Large Language Models (LLMs). However, they do not utilize factual evidence, nor do they allude to it or provide evidence/justification, thereby making their predictions less credible. This paper proposes a new fake news detection method that predicts the truth or false-hood of a claim based on relevant factual evidence (if exists) or LLM\u2019s inference mechanisms (such as common-sense reasoning) otherwise. Our method produces the final synthesized prediction, along with well-founded facts or reasoning. Experimental results on several large COVID-19 fake news datasets show that our method achieves state-of-the-art (SOTA) detection and evidence explanation performance. Our source codes are available online.",
        "author": "Bassamtiano Renaufalgi Irnawan; Sheng Xu; Noriko Tomuro; Fumiyo Fukumoto; Yoshimi Suzuki",
        "authorids": "/b/bassamtiano-renaufalgi-irnawan/; /s/sheng-xu/; /n/noriko-tomuro/; /f/fumiyo-fukumoto/; /y/yoshimi-suzuki/",
        "bibtex": "@inproceedings{irnawan-etal-2025-claim,\n    title = \"Claim veracity assessment for explainable fake news detection\",\n    author = \"Irnawan, Bassamtiano Renaufalgi  and\n      Xu, Sheng  and\n      Tomuro, Noriko  and\n      Fukumoto, Fumiyo  and\n      Suzuki, Yoshimi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.270/\",\n    pages = \"4011--4029\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.270.pdf",
        "site": "https://aclanthology.org/2025.coling-main.270/",
        "pdf_size": 1619004,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4E88xj2mGnoJ:scholar.google.com/&scioq=Claim+veracity+assessment+for+explainable+fake+news+detection&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Integrated Graduate School of Medicine, Engineering, and Agricultural Sciences; Integrated Graduate School of Medicine, Engineering, and Agricultural Sciences; College of Computer and Digital Media, Depaul University; Graduate Faculty of Interdisciplinary Research, University of Yamanashi; Graduate Faculty of Interdisciplinary Research, University of Yamanashi",
        "aff_domain": "yamanashi.ac.jp;yamanashi.ac.jp;cs.depaul.edu;yamanashi.ac.jp; ",
        "email": "yamanashi.ac.jp;yamanashi.ac.jp;cs.depaul.edu;yamanashi.ac.jp; ",
        "github": "https://github.com/bassamtiano/covid_efnd",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;2",
        "aff_unique_norm": "Integrated Graduate School of Medicine, Engineering, and Agricultural Sciences;DePaul University;University of Yamanashi",
        "aff_unique_dep": ";College of Computer and Digital Media;Graduate Faculty of Interdisciplinary Research",
        "aff_unique_url": ";https://www.depaul.edu;https://www.u-yamanashi.ac.jp",
        "aff_unique_abbr": ";DePaul;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;2",
        "aff_country_unique": ";United States;Japan"
    },
    {
        "id": "2025.coling-main.151",
        "title": "Clear Up Confusion: Iterative Differential Generation for Fine-grained Intent Detection with Contrastive Feedback",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Fine-grained intent detection involves identifying a large number of classes with subtle variations. Recently, generating pseudo samples via large language models has attracted increasing attention to alleviate the data scarcity caused by emerging new intents. However, these methods generate samples for each class independently and neglect the relationships between classes, leading to ambiguity in pseudo samples, particularly for fine-grained labels. And, they typically rely on one-time generation and overlook feedback from pseudo samples. In this paper, we propose an iterative differential generation framework with contrastive feedback to generate high-quality pseudo samples and accurately capture the crucial nuances in target class distribution. Specifically, we propose differential guidelines that include potential ambiguous labels to reduce confusion for similar labels. Then we conduct rubric-driven refinement, ensuring the validity and diversity of pseudo samples. Finally, despite one generation, we propose to iteratively generate new samples with contrastive feedback to achieve accurate identification and distillation of target knowledge. Extensive experiments in zero/few-shot and full-shot settings on three datasets verify the effectiveness of our method.",
        "author": "Feng Zhang; Wei Chen; Meng Gao; Fei Ding; Tengjiao Wang; Jiahui Yao; Jiabin Zheng",
        "authorids": "/f/feng-zhang/; /w/wei-chen/; /m/meng-gao/; /f/fei-ding/; /t/tengjiao-wang/; /j/jiahui-yao/; /j/jiabin-zheng/",
        "bibtex": "@inproceedings{zhang-etal-2025-clear,\n    title = \"Clear Up Confusion: Iterative Differential Generation for Fine-grained Intent Detection with Contrastive Feedback\",\n    author = \"Zhang, Feng  and\n      Chen, Wei  and\n      Gao, Meng  and\n      Ding, Fei  and\n      Wang, Tengjiao  and\n      Yao, Jiahui  and\n      Zheng, Jiabin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.151/\",\n    pages = \"2207--2221\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.151.pdf",
        "site": "https://aclanthology.org/2025.coling-main.151/",
        "pdf_size": 1996704,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:jC9f39ii_loJ:scholar.google.com/&scioq=Clear+Up+Confusion:+Iterative+Differential+Generation+for+Fine-grained+Intent+Detection+with+Contrastive+Feedback&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Key Lab of High Confidence Software Technologies (MOE), School of Computer Science, Peking University + Research Center for Computational Social Science, Peking University + Institute of Computational Social Science, Peking University (Qingdao); Key Lab of High Confidence Software Technologies (MOE), School of Computer Science, Peking University + Research Center for Computational Social Science, Peking University + Institute of Computational Social Science, Peking University (Qingdao); Key Lab of High Confidence Software Technologies (MOE), School of Computer Science, Peking University + Research Center for Computational Social Science, Peking University + Institute of Computational Social Science, Peking University (Qingdao); School of Intelligence Science and Technology, Peking University + Institute for Artificial Intelligence, Peking University; Key Lab of High Confidence Software Technologies (MOE), School of Computer Science, Peking University + Research Center for Computational Social Science, Peking University + Institute of Computational Social Science, Peking University (Qingdao); Research Center for Computational Social Science, Peking University + Institute of Computational Social Science, Peking University (Qingdao); Key Lab of High Confidence Software Technologies (MOE), School of Computer Science, Peking University + Research Center for Computational Social Science, Peking University + Institute of Computational Social Science, Peking University (Qingdao)",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0+0;0+0+0;0+0+0;0+0;0+0+0;0+0;0+0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1;1;1;;1;1;1",
        "aff_campus_unique": ";Qingdao",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0;0+0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.467",
        "title": "Close or Cloze? Assessing the Robustness of Large Language Models to Adversarial Perturbations via Word Recovery",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The current generation of large language models (LLMs) show a surprising degree of robustness to adversarial perturbations, but it is unclear when these models implicitly recover the original text and when they rely on surrounding context. To isolate this recovery faculty of language models, we study a new diagnostic task \u2014Adversarial Word Recovery \u2014 an extension of spellchecking where the inputs may be adversarial. We collect a new dataset using 9 popular perturbation attack strategies from the literature and organize them using a taxonomy of phonetic, typo, and visual attacks. We use this dataset to study the word recovery performance of the current generation of LLMs, finding that proprietary models (GPT-4, GPT-3.5 and Palm-2) match or surpass human performance. Conversely, open-source models (Llama-2, Mistral, Falcon) demonstrate a material gap between human performance, especially on visual attacks. For these open models, we show that performance of word recovery without context correlates to word recovery with context, and ultimately affects downstream task performance on a hateful, offensive, and toxic classification task. Finally, to show improving word recovery can improve robustness, we mitigate these attacks with a small Byt5 model tuned to recover visually attacked words.",
        "author": "Luke Moffett; Bhuwan Dhingra",
        "authorids": "/l/luke-moffett/; /b/bhuwan-dhingra/",
        "bibtex": "@inproceedings{moffett-dhingra-2025-close,\n    title = \"Close or Cloze? Assessing the Robustness of Large Language Models to Adversarial Perturbations via Word Recovery\",\n    author = \"Moffett, Luke  and\n      Dhingra, Bhuwan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.467/\",\n    pages = \"6999--7019\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.467.pdf",
        "site": "https://aclanthology.org/2025.coling-main.467/",
        "pdf_size": 2001997,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bO65vD6j7EMJ:scholar.google.com/&scioq=Close+or+Cloze%3F+Assessing+the+Robustness+of+Large+Language+Models+to+Adversarial+Perturbations+via+Word+Recovery&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/lmoffett/cloze-or-close",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2025.coling-main.571",
        "title": "CmEAA: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic radiology report generation is pivotal in reducing the workload of radiologists, while simultaneously improving diagnostic accuracy and operational efficiency. Current methods face significant challenges, including the effective alignment of medical visual features with textual features and the mitigation of data bias. In this paper, we propose a method for radiology report generation that utilizes a Cross-modal Enhancement and Alignment Adapter (CmEAA) to connect a vision encoder with a frozen large language model. Specifically, we introduce two novel modules within CmEAA: Cross-modal Feature Enhancement (CFE) and Neural Mutual Information Aligner (NMIA). CFE extracts observation-related contextual features to enhance the visual features of lesions and abnormal regions in radiology images through a cross-modal enhancement transformer. NMIA maximizes neural mutual information between visual and textual representations within a low-dimensional alignment embedding space during training and provides potential global alignment visual representations during inference. Additionally, a weights generator is designed to enable the dynamic adaptation of cross-modal enhanced features and vanilla visual features. Experimental results on two prevailing datasets, namely, IU X-Ray and MIMIC-CXR, demonstrate that the proposed model outperforms previous state-of-the-art methods.",
        "author": "Xiyang Huang; Yingjie Han; Yx L; Runzhi Li; Pengcheng Wu; Kunli Zhang",
        "authorids": "/x/xiyang-huang/; /y/yingjie-han/; /y/yx-l/; /r/runzhi-li/; /p/pengcheng-wu/; /k/kunli-zhang/",
        "bibtex": "@inproceedings{huang-etal-2025-cmeaa,\n    title = \"{C}m{EAA}: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation\",\n    author = \"Huang, Xiyang  and\n      Han, Yingjie  and\n      L, Yx  and\n      Li, Runzhi  and\n      Wu, Pengcheng  and\n      Zhang, Kunli\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.571/\",\n    pages = \"8546--8556\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.571.pdf",
        "site": "https://aclanthology.org/2025.coling-main.571/",
        "pdf_size": 654591,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15015574501426803898&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Artificial Intelligence, Zhengzhou University; School of Computer Science and Artificial Intelligence, Zhengzhou University; School of Computer Science and Artificial Intelligence, Zhengzhou University; Cooperative Innovation Center of Internet Healthcare, Zhengzhou University; School of Computer Science and Artificial Intelligence, Zhengzhou University; School of Computer Science and Artificial Intelligence, Zhengzhou University",
        "aff_domain": "zzu.edu.cn; ; ; ; ; ",
        "email": "zzu.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Zhengzhou University",
        "aff_unique_dep": "School of Computer Science and Artificial Intelligence",
        "aff_unique_url": "http://www.zzu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.492",
        "title": "CoMIF: Modeling of Complex Multiple Interaction Factors for Conversation Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Highly realistic human-machine interaction is challenging for open-domain dialogue systems. Although existing methods have achieved notable progress by leveraging various interaction factors (e.g., emotion, personality, topic) for delivering human-like (e.g., empathetic, personalized and semantically-consistent) responses, they typically model such factor alone and thus easily suffer from low-quality response generation issue. We attribute this limitation to the neglect of implicit-correlations among factors. Furthermore, different factors may alternately dominate token-level response generation during decoding, making it harder to generate high-quality responses by applying various factors at the sentence level. To address the issue, we present a unified response generation framework, which is capable of simultaneously modeling Complex Multiple Interaction Factors (named CoMIF) to generate human-like conversations. To model the implicit correlations among factors, CoMIF first employ a dynamic perception module to construct a directed collaborative-graph to jointly learn the dynamics over time of each factor, as well as the cross-dependencies among them. Additionally, we also design a scalable post-adaptation module to introduce token-level factor signals to generate more human-like responses with appropriately multiple factors. Extensive experiments over multiple datasets demonstrate that the proposed method achieves the superior performance in generating more human-like responses with appropriate multiple-factors, as compared to the state-of-the-art methods.",
        "author": "Yuxuan Chen; Wei Wei; Shixuan Fan; Kaihe Xu; Dangyang Chen",
        "authorids": "/y/yuxuan-chen/; /w/wei-wei/; /s/shixuan-fan/; /k/kaihe-xu/; /d/dangyang-chen/",
        "bibtex": "@inproceedings{chen-etal-2025-comif,\n    title = \"{C}o{MIF}: Modeling of Complex Multiple Interaction Factors for Conversation Generation\",\n    author = \"Chen, Yuxuan  and\n      Wei, Wei  and\n      Fan, Shixuan  and\n      Xu, Kaihe  and\n      Chen, Dangyang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.492/\",\n    pages = \"7355--7366\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.492.pdf",
        "site": "https://aclanthology.org/2025.coling-main.492/",
        "pdf_size": 613604,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18053427545987012458&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL) + Ping An Property & Casualty Insurance company of China, Ltd.; Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL) + Ping An Property & Casualty Insurance company of China, Ltd.",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;gmail.com;pingan.com.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;gmail.com;pingan.com.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;0+1;0+1",
        "aff_unique_norm": "Huazhong University of Science and Technology;Ping An Property & Casualty Insurance Company of China, Ltd.",
        "aff_unique_dep": "School of Computer Science and Technology;",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.pingan.com",
        "aff_unique_abbr": "HUST;Ping An",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.394",
        "title": "CoPrUS: Consistency Preserving Utterance Synthesis towards more realistic benchmark dialogues",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep learning-based dialogue systems. While they are successful as benchmark datasets, they lack certain types of utterances, which would make them more realistic. In this work, we investigate the creation of synthetic communication errors in an automatic pipeline. Based on linguistic theory, we propose and follow a simple error taxonomy. We focus on three types of miscommunications that could happen in real-world dialogues but are underrepresented in the benchmark dataset: misunderstandings, non-understandings and vaguely related questions. Our two-step approach uses a state-of-the-art Large Language Model (LLM) to first create the error and secondly the repairing utterance. We perform Language Model-based evaluation to ensure the quality of the generated utterances. We apply the method to the MultiWOZ dataset and evaluate it both qualitatively and empirically as well as with human judges. Our results indicate that current LLMs can aid in adding post-hoc miscommunications to benchmark datasets as a form of data augmentation. We publish the resulting dataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ to facilitate future work on dialogue systems.",
        "author": "Sebastian Steindl; Ulrich Sch\u00e4fer; Bernd Ludwig",
        "authorids": "/s/sebastian-steindl/; /u/ulrich-schafer/; /b/bernd-ludwig/",
        "bibtex": "@inproceedings{steindl-etal-2025-coprus,\n    title = \"{C}o{P}r{US}: Consistency Preserving Utterance Synthesis towards more realistic benchmark dialogues\",\n    author = {Steindl, Sebastian  and\n      Sch{\\\"a}fer, Ulrich  and\n      Ludwig, Bernd},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.394/\",\n    pages = \"5902--5917\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.394.pdf",
        "site": "https://aclanthology.org/2025.coling-main.394/",
        "pdf_size": 705432,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10755670738351090238&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Ostbayerische Technische Hochschule Amberg-Weiden, Germany; Ostbayerische Technische Hochschule Amberg-Weiden, Germany; University Regensburg, Germany",
        "aff_domain": "oth-aw.de;oth-aw.de;ur.de",
        "email": "oth-aw.de;oth-aw.de;ur.de",
        "github": "https://github.com/sebastian-steindl/CoPrUS_data",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Ostbayerische Technische Hochschule;University of Regensburg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.oth-aw.de;https://www.uni-regensburg.de",
        "aff_unique_abbr": "OTH;UR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amberg-Weiden;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.618",
        "title": "CoSTA: Code-Switched Speech Translation using Aligned Speech-Text Interleaving",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Code-switching is a widely prevalent linguistic phenomenon in multilingual societies like India. Building speech-to-text models for code-switched speech is challenging due to limited availability of datasets. In this work, we focus on the problem of spoken translation (ST) of code-switched speech in Indian languages to English text. We present a new end-to-end model architecture CoSTA that scaffolds on pretrained automatic speech recognition (ASR) and machine translation (MT) modules (that are more widely available for many languages). Speech and ASR text representations are fused using an aligned interleaving scheme and are fed further as input to a pretrained MT module; the whole pipeline is then trained end-to-end for spoken translation using synthetically created ST data. We also release a new evaluation benchmark for code-switched Bengali- English, Hindi-English, Marathi-English and Telugu-English speech to English text. CoSTA significantly outperforms many competitive cascaded and end-to-end multimodal baselines by up to 3.5 BLEU points.",
        "author": "Bhavani Shankar P S V N; Preethi Jyothi; Pushpak Bhattacharyya",
        "authorids": "/b/bhavani-shankar-p-s-v-n/; /p/preethi-jyothi/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{p-s-v-n-etal-2025-costa,\n    title = \"{C}o{STA}: Code-Switched Speech Translation using Aligned Speech-Text Interleaving\",\n    author = \"P S V N, Bhavani Shankar  and\n      Jyothi, Preethi  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.618/\",\n    pages = \"9194--9208\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.618.pdf",
        "site": "https://aclanthology.org/2025.coling-main.618/",
        "pdf_size": 524136,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7641069868606569000&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Indian Institute of Technology Bombay, India; Indian Institute of Technology Bombay, India; Indian Institute of Technology Bombay, India",
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "github": "https://github.com/csalt-research/CoSTA",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Bombay",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bombay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.7",
        "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have showcased impressive code generation capabilities, primarily evaluated through language-to-code benchmarks. However, these benchmarks may not fully capture a model\u2019s code understanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel benchmark designed to assess LLMs\u2019 code understanding abilities from the perspective of code judging rather than code generation. CJ-Eval challenges models to determine the correctness of provided code solutions, encompassing various error types and compilation issues. By leveraging a diverse set of problems and a fine-grained judging system, CJ-Eval addresses the limitations of traditional benchmarks, including the potential memorization of solutions. Evaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art models struggle, highlighting the benchmark\u2019s ability to probe deeper into models\u2019 code understanding abilities. Our benchmark is available at https://github.com/CodeLLM-Research/CodeJudge-Eval .",
        "author": "Yuwei Zhao; Ziyang Luo; Yuchen Tian; Hongzhan Lin; Weixiang Yan; Annan Li; Jing Ma",
        "authorids": "/y/yuwei-zhao/; /z/ziyang-luo/; /y/yuchen-tian/; /h/hongzhan-lin/; /w/weixiang-yan/; /a/annan-li/; /j/jing-ma/",
        "bibtex": "@inproceedings{zhao-etal-2025-codejudge,\n    title = \"{C}ode{J}udge-Eval: Can Large Language Models be Good Judges in Code Understanding?\",\n    author = \"Zhao, Yuwei  and\n      Luo, Ziyang  and\n      Tian, Yuchen  and\n      Lin, Hongzhan  and\n      Yan, Weixiang  and\n      Li, Annan  and\n      Ma, Jing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.7/\",\n    pages = \"73--95\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.7.pdf",
        "site": "https://aclanthology.org/2025.coling-main.7/",
        "pdf_size": 2515875,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15383437266709032186&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Hong Kong Baptist University; Hong Kong Baptist University; Hong Kong Baptist University; Hong Kong Baptist University; UC, Santa Barbara; Beihang University; Hong Kong Baptist University",
        "aff_domain": "buaa.edu.cn;comp.hkbu.edu.hk;comp.hkbu.edu.hk; ; ;buaa.edu.cn; ",
        "email": "buaa.edu.cn;comp.hkbu.edu.hk;comp.hkbu.edu.hk; ; ;buaa.edu.cn; ",
        "github": "https://github.com/CodeLLM-Research/CodeJudge-Eval",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;2;0",
        "aff_unique_norm": "Hong Kong Baptist University;University of California, Santa Barbara;Beihang University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.hkbu.edu.hk;https://www.ucsb.edu;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "HKBU;UCSB;BUAA",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "Hong Kong SAR;Santa Barbara;",
        "aff_country_unique_index": "0;0;0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.496",
        "title": "Cognate Detection for Historical Language Reconstruction of Proto-Sabean Languages: the Case of Ge\u2019ez, Tigrinya, and Amharic",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As languages evolve, we risk losing ancestral languages. In this paper, we explore Historical Language Reconstruction (HLR) for Proto-Sabean languages, starting with the identification of cognates\u2013sets of words in different related languages that are derived from the same ancestral language. We (1) collect semantically related words in three Afro-Semitic languages from a three-way dictionary (2) work with linguists to identify cognates and reconstruct the proto-form of the cognates, (3) experiment with three automatic cognate detection methods and extract cognates from the semantically related words. We then experiment with in-context learning with GPT-4o to generate the proto-language from the cognates and use Sequence-to-Sequence (Seq2Seq) models for HLR.",
        "author": "Elleni Sisay Temesgen; Hellina Hailu Nigatu; Fitsum Assamnew Andargie",
        "authorids": "/e/elleni-sisay-temesgen/; /h/hellina-hailu-nigatu/; /f/fitsum-assamnew-andargie/",
        "bibtex": "@inproceedings{temesgen-etal-2025-cognate,\n    title = \"Cognate Detection for Historical Language Reconstruction of Proto-Sabean Languages: the Case of {G}e{'}ez, {T}igrinya, and {A}mharic\",\n    author = \"Temesgen, Elleni Sisay  and\n      Nigatu, Hellina Hailu  and\n      Andargie, Fitsum Assamnew\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.496/\",\n    pages = \"7415--7422\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.496.pdf",
        "site": "https://aclanthology.org/2025.coling-main.496/",
        "pdf_size": 688065,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ggq7RFNMWI0J:scholar.google.com/&scioq=Cognate+Detection+for+Historical+Language+Reconstruction+of+Proto-Sabean+Languages:+the+Case+of+Ge%E2%80%99ez,+Tigrinya,+and+Amharic&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Addis Ababa Institute of Technology, Ethiopia; University of California Berkeley, USA; Addis Ababa Institute of Technology, Ethiopia",
        "aff_domain": "aait.edu.et;berkeley.edu; ",
        "email": "aait.edu.et;berkeley.edu; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Addis Ababa Institute of Technology;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aait.edu.et;https://www.berkeley.edu",
        "aff_unique_abbr": "AAIT;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Ethiopia;United States"
    },
    {
        "id": "2025.coling-main.120",
        "title": "Cognitive Biases, Task Complexity, and Result Intepretability in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In humans, cognitive biases are systematic deviations from rationality in judgment that simplify complex decisions. They typically manifest as a consequence of learned behaviors or limitations on information processing capabilities. Recent work has shown that these biases can percolate through training data and ultimately be learned by language models. We examine different groups of models, factoring in model size and type (base or instructed) for four kinds of cognitive bias: primacy, recency, common token, and majority class bias. We evaluate the performance of each model for each type of bias in different settings using simple and complex variants of datasets. Our results show that some biases have much stronger effects than others, and that task complexity plays a part in eliciting stronger effects for some of these biases as measured by effect size. We show that some cognitive biases such as common token and majority class bias are not straightforward to evaluate, and that, contrary to some of the previous literature, some effects that have been previously classified as common token bias in the literature are actually due to primacy and recency bias.",
        "author": "Mario Mina; Valle Ruiz-Fern\u00e1ndez; J\u00falia Falc\u00e3o; Luis Vasquez-Reina; Aitor Gonzalez-Agirre",
        "authorids": "/m/mario-mina/; /v/valle-ruiz-fernandez/; /j/julia-falcao/; /l/luis-vasquez-reina/; /a/aitor-gonzalez-agirre/",
        "bibtex": "@inproceedings{mina-etal-2025-cognitive,\n    title = \"Cognitive Biases, Task Complexity, and Result Intepretability in Large Language Models\",\n    author = \"Mina, Mario  and\n      Ruiz-Fern{\\'a}ndez, Valle  and\n      Falc{\\~a}o, J{\\'u}lia  and\n      Vasquez-Reina, Luis  and\n      Gonzalez-Agirre, Aitor\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.120/\",\n    pages = \"1767--1784\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.120.pdf",
        "site": "https://aclanthology.org/2025.coling-main.120/",
        "pdf_size": 652643,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13424674920141673619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Barcelona Supercomputing Center; Barcelona Supercomputing Center; ; ; ",
        "aff_domain": "bsc.es;bsc.es; ; ; ",
        "email": "bsc.es;bsc.es; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Barcelona Supercomputing Center",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bsc.es",
        "aff_unique_abbr": "BSC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-main.295",
        "title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "State-of-the-art neural retrievers predominantly focus on high-resource languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual pretrained language models capable of cross-lingual transfer. However, these models require substantial task-specific fine-tuning across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular dense retrieval model that learns from the rich data of a single high-resource language and effectively zero-shot transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing state-of-the-art multilingual retrievers trained on more extensive datasets in various languages. Further analysis reveals that our modular approach is highly data-efficient, effectively adapts to out-of-distribution data, and significantly reduces energy consumption and carbon emissions. By demonstrating its proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more sustainable and inclusive retrieval systems, enabling effective information accessibility in numerous languages.",
        "author": "Antoine Louis; Vageesh Kumar Saxena; Gijs van Dijck; Gerasimos Spanakis",
        "authorids": "/a/antoine-louis/; /v/vageesh-kumar-saxena/; /g/gijs-van-dijck/; /g/gerasimos-spanakis/",
        "bibtex": "@inproceedings{louis-etal-2025-colbert,\n    title = \"{C}ol{BERT}-{XM}: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval\",\n    author = \"Louis, Antoine  and\n      Saxena, Vageesh Kumar  and\n      van Dijck, Gijs  and\n      Spanakis, Gerasimos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.295/\",\n    pages = \"4370--4383\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.295.pdf",
        "site": "https://aclanthology.org/2025.coling-main.295/",
        "pdf_size": 882111,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9540267526858227966&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Maastricht University, Netherlands; Maastricht University, Netherlands; Maastricht University, Netherlands; Maastricht University, Netherlands",
        "aff_domain": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "email": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Maastricht University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.maastrichtuniversity.nl",
        "aff_unique_abbr": "MU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2025.coling-main.60",
        "title": "Collaborative Document Simplification Using Multi-Agent Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Research on text simplification has been ongoing for many years. However, the task of document simplification (DS) remains a significant challenge due to the need to consider complex factors such as technical terminology, metaphors, and overall coherence. In this work, we introduce a novel multi-agent framework for document simplification (AgentSimp) based on large language models (LLMs). This framework emulates the collaborative process of a human expert team through the roles played by multiple agents, addressing the intricate demands of document simplification. We explore two communication strategies among agents (pipeline-style and synchronous) and two document reconstruction strategies (Direct and Iterative ). According to both automatic evaluation metrics and human evaluation results, the documents simplified by AgentSimp are deemed to be more thoroughly simplified and more coherent on a variety of articles across different types and styles.",
        "author": "Dengzhao Fang; Jipeng Qiang; Xiaoye Ouyang; Yi Zhu; Yunhao Yuan; Yun Li",
        "authorids": "/d/dengzhao-fang/; /j/jipeng-qiang/; /x/xiaoye-ouyang/; /y/yi-zhu/; /y/yunhao-yuan/; /y/yun-li/",
        "bibtex": "@inproceedings{fang-etal-2025-collaborative,\n    title = \"Collaborative Document Simplification Using Multi-Agent Systems\",\n    author = \"Fang, Dengzhao  and\n      Qiang, Jipeng  and\n      Ouyang, Xiaoye  and\n      Zhu, Yi  and\n      Yuan, Yunhao  and\n      Li, Yun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.60/\",\n    pages = \"897--912\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.60.pdf",
        "site": "https://aclanthology.org/2025.coling-main.60/",
        "pdf_size": 2662126,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16758847056609839768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; China Academy of Electronic and Information Technology, Beijing, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China",
        "aff_domain": "gmail.com;yzu.edu.cn;cetc.com.cn;yzu.edu.cn;yzu.edu.cn;yzu.edu.cn",
        "email": "gmail.com;yzu.edu.cn;cetc.com.cn;yzu.edu.cn;yzu.edu.cn;yzu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Yangzhou University;China Academy of Electronic and Information Technology",
        "aff_unique_dep": "School of Information and Engineering;",
        "aff_unique_url": "http://www.yzu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.702",
        "title": "Comet: Dialog Context Fusion Mechanism for End-to-End Task-Oriented Dialog with Multi-task Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing end-to-end task-oriented dialog systems often encounter challenges arising from implicit information, coreference, and the presence of noisy and irrelevant data within the dialog context. These issues hinder the system\u2019s ability to fully comprehend critical information and lead to inaccurate responses. To address these concerns, we propose Comet, a dialog context fusion mechanism for end-to-end task-oriented dialog, augmented with three supplementary tasks: dialog summarization, domain prediction, and slot detection. Dialog summarization facilitates a more comprehensive understanding of important dialog context information by Comet. Domain prediction enables Comet to concentrate on domain-specific information, thus reducing interference from irrelevant information. Slot detection empowers Comet to accurately identify and comprehend essential dialog context information. Additionally, we introduce a data refinement strategy to enhance the comprehensiveness and recommendability of the generated responses. Experimental results demonstrate the superior performance of our proposed methods compared to existing end-to-end task-oriented dialog systems, achieving state-of-the-art results on the MultiWOZ and CrossWOZ datasets.",
        "author": "Haipeng Sun; Junwei Bao; Youzheng Wu; Xiaodong He",
        "authorids": "/h/haipeng-sun/; /j/junwei-bao/; /y/youzheng-wu/; /x/xiaodong-he/",
        "bibtex": "@inproceedings{sun-etal-2025-comet,\n    title = \"Comet: Dialog Context Fusion Mechanism for End-to-End Task-Oriented Dialog with Multi-task Learning\",\n    author = \"Sun, Haipeng  and\n      Bao, Junwei  and\n      Wu, Youzheng  and\n      He, Xiaodong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.702/\",\n    pages = \"10541--10553\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.702.pdf",
        "site": "https://aclanthology.org/2025.coling-main.702/",
        "pdf_size": 1065197,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:95xtfK5CbDUJ:scholar.google.com/&scioq=Comet:+Dialog+Context+Fusion+Mechanism+for+End-to-End+Task-Oriented+Dialog+with+Multi-task+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Zuoyebang, Beijing, China+JD AI Research, Beijing, China; Zuoyebang, Beijing, China+JD AI Research, Beijing, China; JD AI Research, Beijing, China; JD AI Research, Beijing, China",
        "aff_domain": "gmail.com;gmail.com;jd.com;jd.com",
        "email": "gmail.com;gmail.com;jd.com;jd.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;1;1",
        "aff_unique_norm": "Zuoyebang;JD AI Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zuoyebang.com;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+0;0+0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.150",
        "title": "Commonsense Subgraph for Inductive Relation Reasoning with Meta-learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In knowledge graphs (KGs), predicting missing relations is a critical reasoning task. Recent subgraph-based models have delved into inductive settings, which aim to predict relations between newly added entities. While these models have demonstrated the ability for inductive reasoning, they only consider the structural information of the subgraph and neglect the loss of semantic information caused by replacing entities with nodes. To address this problem, we propose a novel Commonsense Subgraph Meta-Learning (CSML) model. Specifically, we extract concepts from entities, which can be viewed as high-level semantic information. Unlike previous methods, we use concepts instead of nodes to construct commonsense subgraphs. By combining these with structural subgraphs, we can leverage both structural and semantic information for more comprehensive and rational predictions. Furthermore, we regard concepts as meta-information and employ meta-learning to facilitate rapid knowledge transfer, thus addressing more complex few-shot scenarios. Experimental results confirm the superior performance of our model in both standard and few-shot inductive reasoning.",
        "author": "Feng Zhao; Zhilu Zhang; Cheng Yan; Xianggan Liu",
        "authorids": "/f/feng-zhao/; /z/zhilu-zhang/; /c/cheng-yan/; /x/xianggan-liu/",
        "bibtex": "@inproceedings{zhao-etal-2025-commonsense,\n    title = \"Commonsense Subgraph for Inductive Relation Reasoning with Meta-learning\",\n    author = \"Zhao, Feng  and\n      Zhang, Zhilu  and\n      Yan, Cheng  and\n      Liu, Xianggan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.150/\",\n    pages = \"2198--2206\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.150.pdf",
        "site": "https://aclanthology.org/2025.coling-main.150/",
        "pdf_size": 2061897,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uUykU-qe6XEJ:scholar.google.com/&scioq=Commonsense+Subgraph+for+Inductive+Relation+Reasoning+with+Meta-learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Natural Language Processing and Knowledge Graph Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Natural Language Processing and Knowledge Graph Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Natural Language Processing and Knowledge Graph Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Natural Language Processing and Knowledge Graph Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;msn.com",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;msn.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-demos.19",
        "title": "CompUGE-Bench: Comparative Understanding and Generation Evaluation Benchmark for Comparative Question Answering",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "This paper presents CompUGE, a comprehensive benchmark designed to evaluate Comparative Question Answering (CompQA) systems. The benchmark is structured around four core tasks: Comparative Question Identification, Object and Aspect Identification, Stance Classification, and Answer Generation. It unifies multiple datasets and provides a robust evaluation platform to compare various models across these sub-tasks. We also create additional all-encompassing CompUGE datasets by filtering and merging the existing ones. The benchmark for comparative question answering sub-tasks is designed as a web application available on HuggingFace Spaces: https://huggingface.co/spaces/uhhlt/CompUGE-Bench",
        "author": "Ahmad Shallouf; Irina Nikishina; Chris Biemann",
        "authorids": "/a/ahmad-shallouf/; /i/irina-nikishina/; /c/chris-biemann/",
        "bibtex": "@inproceedings{shallouf-etal-2025-compuge,\n    title = \"{C}omp{UGE}-Bench: Comparative Understanding and Generation Evaluation Benchmark for Comparative Question Answering\",\n    author = \"Shallouf, Ahmad  and\n      Nikishina, Irina  and\n      Biemann, Chris\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.19/\",\n    pages = \"189--198\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.19.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.19/",
        "pdf_size": 2910656,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1orC-MAIgbsJ:scholar.google.com/&scioq=CompUGE-Bench:+Comparative+Understanding+and+Generation+Evaluation+Benchmark+for+Comparative+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Hamburg; University of Hamburg; University of Hamburg",
        "aff_domain": "uni-hamburg.de;uni-hamburg.de;uni-hamburg.de",
        "email": "uni-hamburg.de;uni-hamburg.de;uni-hamburg.de",
        "github": "",
        "project": "https://huggingface.co/spaces/uhhlt/CompUGE-Bench",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Hamburg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-hamburg.de",
        "aff_unique_abbr": "UHH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.580",
        "title": "Comparative Study of Multilingual Idioms and Similes in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study addresses the gap in the literature concerning the comparative performance of LLMs in interpreting different types of figurative language across multiple languages. By evaluating LLMs using two multilingual datasets on simile and idiom interpretation, we explore the effectiveness of various prompt engineering strategies, including chain-of-thought, few-shot, and English translation prompts. We extend the language of these datasets to Persian as well by building two new evaluation sets. Our comprehensive assessment involves both closed-source (GPT-3.5, GPT-4o mini, Gemini 1.5), and open-source models (Llama 3.1, Qwen2), highlighting significant differences in performance across languages and figurative types. Our findings reveal that while prompt engineering methods are generally effective, their success varies by figurative type, language, and model. We also observe that open-source models struggle particularly with low-resource languages in similes. Additionally, idiom interpretation is nearing saturation for many languages, necessitating more challenging evaluations.",
        "author": "Paria Khoshtab; Danial Namazifard; Mostafa Masoudi; Ali Akhgary; Samin Mahdizadeh Sani; Yadollah Yaghoobzadeh",
        "authorids": "/p/paria-khoshtab/; /d/danial-namazifard/; /m/mostafa-masoudi/; /a/ali-akhgary/; /s/samin-mahdizadeh-sani/; /y/yadollah-yaghoobzadeh/",
        "bibtex": "@inproceedings{khoshtab-etal-2025-comparative,\n    title = \"Comparative Study of Multilingual Idioms and Similes in Large Language Models\",\n    author = \"Khoshtab, Paria  and\n      Namazifard, Danial  and\n      Masoudi, Mostafa  and\n      Akhgary, Ali  and\n      Mahdizadeh Sani, Samin  and\n      Yaghoobzadeh, Yadollah\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.580/\",\n    pages = \"8680--8698\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.580.pdf",
        "site": "https://aclanthology.org/2025.coling-main.580/",
        "pdf_size": 635725,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15831115462361787236&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Electrical and Computer Engineering, College of Engineering University of Tehran, Tehran, Iran + Tehran Institute for Advanced Studies, Khatam University, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering University of Tehran, Tehran, Iran + Tehran Institute for Advanced Studies, Khatam University, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering University of Tehran, Tehran, Iran + Tehran Institute for Advanced Studies, Khatam University, Tehran, Iran",
        "aff_domain": "ut.ac.ir;ut.ac.ir;ut.ac.ir;ut.ac.ir;ut.ac.ir;ut.ac.ir",
        "email": "ut.ac.ir;ut.ac.ir;ut.ac.ir;ut.ac.ir;ut.ac.ir;ut.ac.ir",
        "github": "https://github.com/namazifard/Multilingual-Idioms-Similes",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0;0;0;0+1",
        "aff_unique_norm": "University of Tehran;Tehran Institute for Advanced Studies",
        "aff_unique_dep": "School of Electrical and Computer Engineering;",
        "aff_unique_url": "https://en.ut.ac.ir;",
        "aff_unique_abbr": "UT;",
        "aff_campus_unique_index": "0+0;0+0;0;0;0;0+0",
        "aff_campus_unique": "Tehran",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0+0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2025.coling-main.51",
        "title": "Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a \u201cOne-for-All\u201d approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which integrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we collect the biggest Chinese long-term conversation dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY\u2019s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences.",
        "author": "Nuo Chen; Hongguang Li; Jianhui Chang; Juhua Huang; Baoyuan Wang; Jia Li",
        "authorids": "/n/nuo-chen/; /h/hongguang-li/; /j/jianhui-chang/; /j/juhua-huang/; /b/baoyuan-wang/; /j/jia-li/",
        "bibtex": "@inproceedings{chen-etal-2025-compress,\n    title = \"Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations\",\n    author = \"Chen, Nuo  and\n      Li, Hongguang  and\n      Chang, Jianhui  and\n      Huang, Juhua  and\n      Wang, Baoyuan  and\n      Li, Jia\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.51/\",\n    pages = \"755--773\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.51.pdf",
        "site": "https://aclanthology.org/2025.coling-main.51/",
        "pdf_size": 728637,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17504338163671180226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Hong Kong University of Science and Technology (Guangzhou); Xiaobing.ai+JF SmartInvest Holdings; China Telecom Cloud Computing Research Institute+Zillow Group; Xiaobing.ai; Zillow Group; Hong Kong University of Science and Technology (Guangzhou)",
        "aff_domain": "gmail.com; ; ; ; ;ust.hk",
        "email": "gmail.com; ; ; ; ;ust.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+2;3+4;1;4;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Xiaobing.ai;JF SmartInvest Holdings;China Telecom;Zillow Group",
        "aff_unique_dep": ";;;Cloud Computing Research Institute;",
        "aff_unique_url": "https://www.ust.hk;https://www.xiaobing.ai;;https://www.chinatelecom.com.cn;https://www.zillow.com",
        "aff_unique_abbr": "HKUST;Xiaobing.ai;;CT;Zillow",
        "aff_campus_unique_index": "0;;;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0+2;0;2;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2025.coling-main.68",
        "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts. In this paper, we propose Con-ReCall, a novel approach that leverages the asymmetric distributional shifts induced by member and non-member contexts through contrastive decoding, amplifying subtle differences to enhance membership inference. Extensive empirical evaluations demonstrate that Con-ReCall achieves state-of-the-art performance on the WikiMIA benchmark and is robust against various text manipulation techniques.",
        "author": "Cheng Wang; Yiwei Wang; Bryan Hooi; Yujun Cai; Nanyun Peng; Kai-Wei Chang",
        "authorids": "/c/cheng-wang/; /y/yiwei-wang/; /b/bryan-hooi/; /y/yujun-cai/; /n/nanyun-peng/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{wang-etal-2025-con,\n    title = \"Con-{R}e{C}all: Detecting Pre-training Data in {LLM}s via Contrastive Decoding\",\n    author = \"Wang, Cheng  and\n      Wang, Yiwei  and\n      Hooi, Bryan  and\n      Cai, Yujun  and\n      Peng, Nanyun  and\n      Chang, Kai-Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.68/\",\n    pages = \"1013--1026\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.68.pdf",
        "site": "https://aclanthology.org/2025.coling-main.68/",
        "pdf_size": 1164133,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4148564193971758397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National University of Singapore; University of California, Los Angeles; National University of Singapore; University of Queensland; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "comp.nus.edu.sg; ; ; ; ; ",
        "email": "comp.nus.edu.sg; ; ; ; ; ",
        "github": "https://github.com/WangCheng0116/CON-RECALL",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;2;1;1",
        "aff_unique_norm": "National University of Singapore;University of California, Los Angeles;University of Queensland",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.ucla.edu;https://www.uq.edu.au",
        "aff_unique_abbr": "NUS;UCLA;UQ",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;1;0;2;1;1",
        "aff_country_unique": "Singapore;United States;Australia"
    },
    {
        "id": "2025.coling-main.306",
        "title": "Conditional Semantic Textual Similarity via Conditional Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conditional semantic textual similarity (C-STS) assesses the similarity between pairs of sentence representations under different conditions. The current method encounters the over-estimation issue of positive and negative samples. Specifically, the similarity within positive samples is excessively high, while that within negative samples is excessively low. In this paper, we focus on the C-STS task and develop a conditional contrastive learning framework that constructs positive and negative samples from two perspectives, achieving the following primary objectives: (1) adaptive selection of the optimization direction for positive and negative samples to solve the over-estimation problem, (2) fully balance of the effects of hard and false negative samples. We validate the proposed method with five models based on bi-encoder and tri-encoder architectures, the results show that our proposed method achieves state-of-the-art performance. The code is available at https://github.com/qinzeyang0919/CCL.",
        "author": "Xinyue Liu; Zeyang Qin; Zeyu Wang; Wenxin Liang; Linlin Zong; Bo Xu",
        "authorids": "/x/xinyue-liu/; /z/zeyang-qin/; /z/zeyu-wang/; /w/wenxin-liang/; /l/linlin-zong/; /b/bo-xu/",
        "bibtex": "@inproceedings{liu-etal-2025-conditional,\n    title = \"Conditional Semantic Textual Similarity via Conditional Contrastive Learning\",\n    author = \"Liu, Xinyue  and\n      Qin, Zeyang  and\n      Wang, Zeyu  and\n      Liang, Wenxin  and\n      Zong, Linlin  and\n      Xu, Bo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.306/\",\n    pages = \"4548--4560\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.306.pdf",
        "site": "https://aclanthology.org/2025.coling-main.306/",
        "pdf_size": 1930537,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sTQggid2LcQJ:scholar.google.com/&scioq=Conditional+Semantic+Textual+Similarity+via+Conditional+Contrastive+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China",
        "aff_domain": "dlut.edu.cn;mail.dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "email": "dlut.edu.cn;mail.dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "github": "https://github.com/qinzeyang0919/CCL",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "School of Software",
        "aff_unique_url": "http://www.dlut.edu.cn",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dalian",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.574",
        "title": "Confront Insider Threat: Precise Anomaly Detection in Behavior Logs Based on LLM Fine-Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Anomaly-based detection is effective against evolving insider threats but still suffers from low precision. Current data processing can result in information loss, and models often struggle to distinguish between benign anomalies and actual threats. Both issues hinder precise detection. To address these issues, we propose a precise anomaly detection solution for behavior logs based on Large Language Model (LLM) fine-tuning. By representing user behavior in natural language, we minimize information loss. We fine-tune the LLM with a user behavior pattern contrastive task for anomaly detection, using a two-stage strategy: first learning general behavior patterns, then refining with user-specific data to improve differentiation between benign anomalies and threats. We also implement a fine-grained threat tracing mechanism to provide behavior-level audit trails. To the best of our knowledge, our solution is the first to apply LLM fine-tuning in insider threat detection, achieving an F1 score of 0.8941 on the CERT v6.2 dataset, surpassing all baselines.",
        "author": "Shuang Song; Yifei Zhang; Neng Gao",
        "authorids": "/s/shuang-song/; /y/yifei-zhang/; /n/neng-gao/",
        "bibtex": "@inproceedings{song-etal-2025-confront,\n    title = \"Confront Insider Threat: Precise Anomaly Detection in Behavior Logs Based on {LLM} Fine-Tuning\",\n    author = \"Song, Shuang  and\n      Zhang, Yifei  and\n      Gao, Neng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.574/\",\n    pages = \"8589--8601\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.574.pdf",
        "site": "https://aclanthology.org/2025.coling-main.574/",
        "pdf_size": 500041,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4836338276498980420&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences+Key Laboratory of Cyberspace Security Defense; Institute of Information Engineering, Chinese Academy of Sciences+Key Laboratory of Cyberspace Security Defense; Institute of Information Engineering, Chinese Academy of Sciences+Key Laboratory of Cyberspace Security Defense",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;0+2;0+2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Key Laboratory of Cyberspace Security Defense",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.697",
        "title": "Consistency Rating of Semantic Transparency: an Evaluation Method for Metaphor Competence in Idiom Understanding Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Idioms condense complex semantics into fixed phrases, and their meaning is often not directly connected to the literal meaning of their constituent words, making idiom comprehension a test of metaphor competence. Metaphor, as a cognitive process in human beings, has not yet found an effective evaluation method to assess the metaphor competence of LLMs (Large Language Models). In this paper, we propose a method to evaluate the metaphor competence of LLMs for the idiom understanding task: the Consistency Rating of Semantic Transparency (CR-ST). This strategy assesses the difficulty of understanding idioms through two dimensions: overall semantic transparency and constituent semantic transparency, aiming to gauge LLMs\u2019 mastery of metaphor competence. Subsequently, we introduce a prompt mechanism-Paraphrase Augmentation Strategy with Self-checking (PASS), based on human language logic, which guides the model to enhance its metaphor competence by explicitly generating idiom paraphrases. We conducted a baseline evaluation of seven LLMs on the CINLID and ChID datasets and analyzed the effectiveness of PASS on different subsets of semantic transparency. The experimental results demonstrate that LLMs can achieve performance comparable to PLMs (Pre-trained Language Models) without additional training, and PASS has a positive effect on the metaphor competence of LLMs.",
        "author": "Hui Gao; Jing Zhang; Peng Zhang; Chang Yang",
        "authorids": "/h/hui-gao/; /j/jing-zhang/; /p/peng-zhang/; /c/chang-yang/",
        "bibtex": "@inproceedings{gao-etal-2025-consistency,\n    title = \"Consistency Rating of Semantic Transparency: an Evaluation Method for Metaphor Competence in Idiom Understanding Tasks\",\n    author = \"Gao, Hui  and\n      Zhang, Jing  and\n      Zhang, Peng  and\n      Yang, Chang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.697/\",\n    pages = \"10460--10471\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.697.pdf",
        "site": "https://aclanthology.org/2025.coling-main.697/",
        "pdf_size": 580538,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5-Utnqf9D-0J:scholar.google.com/&scioq=Consistency+Rating+of+Semantic+Transparency:+an+Evaluation+Method+for+Metaphor+Competence+in+Idiom+Understanding+Tasks&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.732",
        "title": "Context Filtering with Reward Modeling in Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question Answering (QA) in NLP is the task of finding answers to a query within a relevant context retrieved by a retrieval system. Yet, the mix of relevant and irrelevant information in these contexts can hinder performance enhancements in QA tasks. To address this, we introduce a context filtering approach that removes non-essential details, summarizing crucial content through Reward Modeling. This method emphasizes keeping vital data while omitting the extraneous during summarization model training. We offer a framework for developing efficient QA models by discerning useful information from dataset pairs, bypassing the need for costly human evaluation. Furthermore, we show that our approach can significantly outperform the baseline, as evidenced by a 6.8-fold increase in the EM Per Token (EPT) metric, which we propose as a measure of token efficiency, indicating a notable token-efficiency boost for low-resource settings.",
        "author": "Sangryul Kim; James Thorne",
        "authorids": "/s/sangryul-kim/; /j/james-thorne/",
        "bibtex": "@inproceedings{kim-thorne-2025-context,\n    title = \"Context Filtering with Reward Modeling in Question Answering\",\n    author = \"Kim, Sangryul  and\n      Thorne, James\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.732/\",\n    pages = \"11048--11055\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.732.pdf",
        "site": "https://aclanthology.org/2025.coling-main.732/",
        "pdf_size": 361516,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GePVyYZK5gQJ:scholar.google.com/&scioq=Context+Filtering+with+Reward+Modeling+in+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "KAIST AI; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/xfactlab/coling2025-context-filtering",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "KAIST AI",
        "aff_unique_url": "https://www.kaist.edu",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.232",
        "title": "Context-Informed Machine Translation of Manga using Multimodal Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Due to the significant time and effort required for handcrafting translations, most manga never leave the domestic Japanese market. Automatic manga translation is a promising potential solution. However, it is a budding and underdeveloped field and presents complexities even greater than those found in standard translation due to the need to effectively incorporate visual elements into the translation process to resolve ambiguities. In this work, we investigate to what extent multimodal large language models (LLMs) can provide effective manga translation, thereby assisting manga authors and publishers in reaching wider audiences. Specifically, we propose a methodology that leverages the vision component of multimodal LLMs to improve translation quality and evaluate the impact of translation unit size, context length, and propose a token efficient approach for manga translation. Moreover, we introduce a new evaluation dataset \u2013 the first parallel Japanese-Polish manga translation dataset \u2013 as part of a benchmark to be used in future research. Finally, we contribute an open-source software suite, enabling others to benchmark LLMs for manga translation. Our findings demonstrate that our proposed methods achieve state-of-the-art results for Japanese-English translation and set a new standard for Japanese-Polish.",
        "author": "Philip Lippmann; Konrad Skublicki; Joshua Tanner; Shonosuke Ishiwatari; Jie Yang",
        "authorids": "/p/philip-lippmann/; /k/konrad-skublicki/; /j/joshua-tanner/; /s/shonosuke-ishiwatari/; /j/jie-yang/",
        "bibtex": "@inproceedings{lippmann-etal-2025-context,\n    title = \"Context-Informed Machine Translation of Manga using Multimodal Large Language Models\",\n    author = \"Lippmann, Philip  and\n      Skublicki, Konrad  and\n      Tanner, Joshua  and\n      Ishiwatari, Shonosuke  and\n      Yang, Jie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.232/\",\n    pages = \"3444--3464\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.232.pdf",
        "site": "https://aclanthology.org/2025.coling-main.232/",
        "pdf_size": 6690688,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8223024575720718584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Delft University of Technology; Delft University of Technology; Mantra Inc.; Mantra Inc.; Delft University of Technology",
        "aff_domain": "tudelft.nl; ; ; ; ",
        "email": "tudelft.nl; ; ; ; ",
        "github": "https://github.com/plippmann/multimodal-manga-translation",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Delft University of Technology;Mantra Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tudelft.nl;",
        "aff_unique_abbr": "TU Delft;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "2025.coling-industry.32",
        "title": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational AI",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives.",
        "author": "Yuya Asano; Sabit Hassan; Paras Sharma; Anthony B. Sicilia; Katherine Atwell; Diane Litman; Malihe Alikhani",
        "authorids": "/y/yuya-asano/; /s/sabit-hassan/; /p/paras-sharma/; /a/anthony-b-sicilia/; /k/katherine-atwell/; /d/diane-litman/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{asano-etal-2025-contextual,\n    title = \"Contextual {ASR} Error Handling with {LLM}s Augmentation for Goal-Oriented Conversational {AI}\",\n    author = \"Asano, Yuya  and\n      Hassan, Sabit  and\n      Sharma, Paras  and\n      Sicilia, Anthony B.  and\n      Atwell, Katherine  and\n      Litman, Diane  and\n      Alikhani, Malihe\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.32/\",\n    pages = \"374--386\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.32.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.32/",
        "pdf_size": 431450,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:vW4vAye2r24J:scholar.google.com/&scioq=Contextual+ASR+Error+Handling+with+LLMs+Augmentation+for+Goal-Oriented+Conversational+AI&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Pittsburgh; University of Pittsburgh; University of Pittsburgh; Northeastern University; Northeastern University; University of Pittsburgh; Northeastern University",
        "aff_domain": "pitt.edu;pitt.edu;pitt.edu;northeastern.edu;northeastern.edu;pitt.edu;northeastern.edu",
        "email": "pitt.edu;pitt.edu;pitt.edu;northeastern.edu;northeastern.edu;pitt.edu;northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;1;0;1",
        "aff_unique_norm": "University of Pittsburgh;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.pitt.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "Pitt;NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.570",
        "title": "Contextual Augmentation for Entity Linking using Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity Linking involves detecting and linking entity mentions in natural language texts to a knowledge graph. Traditional methods use a two-step process with separate models for entity recognition and disambiguation, which can be computationally intensive and less effective. We propose a fine-tuned model that jointly integrates entity recognition and disambiguation in a unified framework. Furthermore, our approach leverages large language models to enrich the context of entity mentions, yielding better disambiguation. We evaluated our approach on benchmark datasets and compared with several baselines. The evaluation results show that our approach achieves state-of-the-art performance on out-of-domain datasets.",
        "author": "Daniel Vollmers; Hamada Zahera; Diego Moussallem; Axel-Cyrille Ngonga Ngomo",
        "authorids": "/d/daniel-vollmers/; /h/hamada-zahera/; /d/diego-moussallem/; /a/axel-cyrille-ngonga-ngomo/",
        "bibtex": "@inproceedings{vollmers-etal-2025-contextual,\n    title = \"Contextual Augmentation for Entity Linking using Large Language Models\",\n    author = \"Vollmers, Daniel  and\n      Zahera, Hamada  and\n      Moussallem, Diego  and\n      Ngonga Ngomo, Axel-Cyrille\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.570/\",\n    pages = \"8535--8545\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.570.pdf",
        "site": "https://aclanthology.org/2025.coling-main.570/",
        "pdf_size": 400627,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZENZGouWLigJ:scholar.google.com/&scioq=Contextual+Augmentation+for+Entity+Linking+using+Large+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Data Science Group, Paderborn University, Germany; Data Science Group, Paderborn University, Germany; Data Science Group, Paderborn University, Germany; Data Science Group, Paderborn University, Germany",
        "aff_domain": "uni-paderborn.de;uni-paderborn.de;uni-paderborn.de;uni-paderborn.de",
        "email": "uni-paderborn.de;uni-paderborn.de;uni-paderborn.de;uni-paderborn.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Paderborn University",
        "aff_unique_dep": "Data Science Group",
        "aff_unique_url": "https://www.uni-paderborn.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.402",
        "title": "Continual Learning Using Only Large Language Model Prompting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce CLOB, a novel continual learning (CL) paradigm wherein a large language model (LLM) is regarded as a black box. Learning is done incrementally via only verbal prompting. CLOB does not fine-tune any part of the LLM or add any trainable parameters to it. It is particularly suitable for LLMs that are accessible via APIs. We also propose a new CL technique, called CIS, based on incremental summarization that also overcomes the LLM\u2019s input length limit. Experiments show CIS outperforms baselines by a very large margin.",
        "author": "Jiabao Qiu; Zixuan Ke; Bing Liu",
        "authorids": "/j/jiabao-qiu/; /z/zixuan-ke/; /b/bing-liu/",
        "bibtex": "@inproceedings{qiu-etal-2025-continual,\n    title = \"Continual Learning Using Only Large Language Model Prompting\",\n    author = \"Qiu, Jiabao  and\n      Ke, Zixuan  and\n      Liu, Bing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.402/\",\n    pages = \"6014--6023\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.402.pdf",
        "site": "https://aclanthology.org/2025.coling-main.402/",
        "pdf_size": 592546,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14099850496132071283&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois Chicago; Salesforce AI Research; Department of Computer Science, University of Illinois Chicago",
        "aff_domain": "uic.edu;salesforce.com;uic.edu",
        "email": "uic.edu;salesforce.com;uic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois Chicago;Salesforce",
        "aff_unique_dep": "Department of Computer Science;Salesforce AI Research",
        "aff_unique_url": "https://www.uic.edu;https://www.salesforce.com",
        "aff_unique_abbr": "UIC;Salesforce AI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.224",
        "title": "Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated Text Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study demonstrates that the modern generation of Large Language Models (LLMs, such as GPT-4) suffers from the same out-of-domain (OOD) performance gap observed in prior research on pre-trained Language Models (PLMs, such as BERT). We demonstrate this across two non-topical classification tasks: (1) genre classification and (2) generated text detection. Our results show that when demonstration examples for In-Context Learning (ICL) come from one domain (e.g., travel) and the system is tested on another domain (e.g., history), classification performance declines significantly. To address this, we introduce a method that controls which predictive indicators are used and which are excluded during classification. For the two tasks studied here, this ensures that topical features are omitted, while the model is guided to focus on stylistic rather than content-based attributes. This approach reduces the OOD gap by up to 20 percentage points in a few-shot setup. Straightforward Chain-of-Thought (CoT) methods, used as the baseline, prove insufficient, while our approach consistently enhances domain transfer performance.",
        "author": "Dmitri Roussinov; Serge Sharoff; Nadezhda Puchnina",
        "authorids": "/d/dmitri-roussinov/; /s/serge-sharoff/; /n/nadezhda-puchnina/",
        "bibtex": "@inproceedings{roussinov-etal-2025-controlling,\n    title = \"Controlling Out-of-Domain Gaps in {LLM}s for Genre Classification and Generated Text Detection\",\n    author = \"Roussinov, Dmitri  and\n      Sharoff, Serge  and\n      Puchnina, Nadezhda\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.224/\",\n    pages = \"3329--3344\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.224.pdf",
        "site": "https://aclanthology.org/2025.coling-main.224/",
        "pdf_size": 466199,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1353241699715765675&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Strathclyde; University of Leeds; Independent consultant",
        "aff_domain": "strath.ac.uk;leeds.ac.uk;gmail.com",
        "email": "strath.ac.uk;leeds.ac.uk;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Strathclyde;University of Leeds;Independent consultant",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.strath.ac.uk;https://www.leeds.ac.uk;",
        "aff_unique_abbr": "Strathclyde;Leeds;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "2025.coling-main.707",
        "title": "Converging to a Lingua Franca: Evolution of Linguistic Regions and Semantics Alignment in Multilingual Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance, particularly in multilingual contexts. While recent studies suggest that LLMs can transfer skills learned in one language to others, the internal mechanisms behind this ability remain unclear. We observed that the neuron activation patterns of LLMs exhibit similarities when processing the same language, revealing the existence and location of key linguistic regions. Additionally, we found that neuron activation patterns are similar when processing sentences with the same semantic meaning in different languages. This indicates that LLMs map semantically identical inputs from different languages into a \u201cLingua Franca\u201d, a common semantic latent space that allows for consistent processing across languages. This semantic alignment becomes more pronounced with training and increased model size, resulting in a more language-agnostic activation pattern. Moreover, we found that key linguistic neurons are concentrated in the first and last layers of LLMs, becoming denser in the first layers as training progresses. Experiments on BLOOM and LLaMA2 support these findings, highlighting the structural evolution of multilingual LLMs during training and scaling up. This paper provides insights into the internal workings of LLMs, offering a foundation for future improvements in their cross-lingual capabilities.",
        "author": "Hongchuan Zeng; Senyu Han; Lu Chen; Kai Yu",
        "authorids": "/h/hongchuan-zeng/; /s/senyu-han/; /l/lu-chen/; /k/kai-yu/",
        "bibtex": "@inproceedings{zeng-etal-2025-converging,\n    title = \"Converging to a Lingua Franca: Evolution of Linguistic Regions and Semantics Alignment in Multilingual Large Language Models\",\n    author = \"Zeng, Hongchuan  and\n      Han, Senyu  and\n      Chen, Lu  and\n      Yu, Kai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.707/\",\n    pages = \"10602--10617\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.707.pdf",
        "site": "https://aclanthology.org/2025.coling-main.707/",
        "pdf_size": 7544793,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7637938248512516801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+2Suzhou Laboratory, Suzhou, China; 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+2Suzhou Laboratory, Suzhou, China; 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+2Suzhou Laboratory, Suzhou, China; 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+2Suzhou Laboratory, Suzhou, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "https://github.com/X-LANCE/LinguaFranca",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Suzhou Laboratory",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Shanghai;Suzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.703",
        "title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs\u2019 inherent reasoning abilities to alleviate hallucination, such as self-correction and diverse sampling methods. However, these methods often overtrust LLMs\u2019 initial answers due to inherent biases. The key to alleviating this issue lies in overriding LLMs\u2019 inherent biases for answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate (CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent biases by compelling LLMs to generate justifications for a predetermined answer\u2019s correctness. The LLMs with different predetermined stances are engaged with a skeptical critic for counterfactual debate on the rationality of generated justifications. Finally, the debate process is evaluated by a third-party judge to determine the final answer. Extensive experiments on four datasets of three tasks demonstrate the superiority of CFMAD over existing methods.",
        "author": "Yi Fang; Moxin Li; Wenjie Wang; Lin Hui; Fuli Feng",
        "authorids": "/y/yi-fang/; /m/moxin-li/; /w/wenjie-wang/; /l/lin-hui/; /f/fuli-feng/",
        "bibtex": "@inproceedings{fang-etal-2025-counterfactual,\n    title = \"Counterfactual Debating with Preset Stances for Hallucination Elimination of {LLM}s\",\n    author = \"Fang, Yi  and\n      Li, Moxin  and\n      Wang, Wenjie  and\n      Hui, Lin  and\n      Feng, Fuli\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.703/\",\n    pages = \"10554--10568\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.703.pdf",
        "site": "https://aclanthology.org/2025.coling-main.703/",
        "pdf_size": 632546,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5076579750876660449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Science and Technology of China; National University of Singapore; National University of Singapore; Electronic Science Research Institute of China Electronics; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;u.nus.edu;gmail.com;whu.edu.cn;gmail.com",
        "email": "mail.ustc.edu.cn;u.nus.edu;gmail.com;whu.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "University of Science and Technology of China;National University of Singapore;China Electronics Corporation",
        "aff_unique_dep": ";;Electronic Science Research Institute",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.nus.edu.sg;http://www.ceict.ac.cn",
        "aff_unique_abbr": "USTC;NUS;CEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-main.253",
        "title": "Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite recent efforts to develop large language models with robust long-context capabilities, the lack of long-context benchmarks means that relatively little is known about their performance. To alleviate this gap, in this paper, we propose Counting-Stars, a multi-evidence, position-aware, and scalable benchmark designed to evaluate the multi-evidence retrieval capabilities of long-context LLMs. Counting-Stars comprises two counting-based multiple pieces of evidence retrieval tasks: searching and reasoning. Using Counting-Stars, we conducted experiments to evaluate several long-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4, and Moonshot-v1. Extensive experimental results demonstrate that Gemini 1.5 Pro achieves the best overall results, while GPT-4 Turbo exhibits the most stable performance across various tasks. Furthermore, our analysis of these LLMs, which have been extended to handle long-context scenarios, indicates that significant room for improvement remains as the length of the input context and the complexity of the tasks increase.",
        "author": "Mingyang Song; Mao Zheng; Xuan Luo",
        "authorids": "/m/mingyang-song/; /m/mao-zheng/; /x/xuan-luo/",
        "bibtex": "@inproceedings{song-etal-2025-counting,\n    title = \"Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models\",\n    author = \"Song, Mingyang  and\n      Zheng, Mao  and\n      Luo, Xuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.253/\",\n    pages = \"3753--3763\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.253.pdf",
        "site": "https://aclanthology.org/2025.coling-main.253/",
        "pdf_size": 846519,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7094961056896068987&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Tencent Hunyuan; Tencent Hunyuan; Tencent Hunyuan",
        "aff_domain": "tencent.com; ; ",
        "email": "tencent.com; ; ",
        "github": "https://github.com/nick7nlp/Counting-Stars",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Hunyuan",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.493",
        "title": "Courtroom-LLM: A Legal-Inspired Multi-LLM Framework for Resolving Ambiguous Text Classifications",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this research, we introduce the Courtroom-LLM framework, a novel multi-LLM structure inspired by legal courtroom processes, aiming to enhance decision-making in ambiguous text classification scenarios. Our approach simulates a courtroom setting within LLMs, assigning roles similar to those of prosecutors, defense attorneys, and judges, to facilitate comprehensive analysis of complex textual cases. We demonstrate that this structured multi-LLM setup can significantly improve decision-making accuracy, particularly in ambiguous situations, by harnessing the synergistic effects of diverse LLM arguments. Our evaluations across various text classification tasks show that the Courtroom-LLM framework outperforms both traditional single-LLM classifiers and simpler multi-LLM setups. These results highlight the advantages of our legal-inspired model in improving decision-making for text classification.",
        "author": "Sangkeun Jung; Jeesu Jung",
        "authorids": "/s/sangkeun-jung/; /j/jeesu-jung/",
        "bibtex": "@inproceedings{jung-jung-2025-courtroom,\n    title = \"Courtroom-{LLM}: A Legal-Inspired Multi-{LLM} Framework for Resolving Ambiguous Text Classifications\",\n    author = \"Jung, Sangkeun  and\n      Jung, Jeesu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.493/\",\n    pages = \"7367--7385\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.493.pdf",
        "site": "https://aclanthology.org/2025.coling-main.493/",
        "pdf_size": 900051,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GSe3fByLn4oJ:scholar.google.com/&scioq=Courtroom-LLM:+A+Legal-Inspired+Multi-LLM+Framework+for+Resolving+Ambiguous+Text+Classifications&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Chungnam National University / 99, Daehak-ro, Yuseong-gu, Daejeon 34134, Republic of Korea; Chungnam National University / 99, Daehak-ro, Yuseong-gu, Daejeon 34134, Republic of Korea",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chungnam National University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.cnu.ac.kr",
        "aff_unique_abbr": "CNU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Daejeon",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.461",
        "title": "Cross Domain Classification of Education Talk Turns",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The study of classroom discourse is essential for enhancing child development and educational outcomes in academic settings. Prior research has focused on the annotation of conversational talk-turns within the classroom, offering a statistical analysis of the various types of discourse prevalent in these environments. In this work, we explore the generalizability and transferability of text classifiers trained to predict these discourse codes across educational domains. We examine two distinct English-language classroom datasets from the domains: literacy and math. Our results show that models exhibit high accuracy and generalizability when the training and test datasets originate from the same or similar domains. In situations where limited training data is available in new domains, few shot and zero shot exhibit more resiliency and aren\u2019t as effected as their supervised counterparts. We also observe that accompanying each talk turn with dialog-level context improves the accuracy of the generative models. We conclude by offering suggestions on how to enhance the generalization of these methods to novel domains, proposing directions for future studies to investigate new methods for boosting the model adaptability across domains.",
        "author": "Achyutarama R. Ganti; Steven R. Wilson; Geoffrey Louie Wing-Yue",
        "authorids": "/a/achyutarama-r-ganti/; /s/steven-r-wilson/; /g/geoffrey-louie-wing-yue/",
        "bibtex": "@inproceedings{ganti-etal-2025-cross,\n    title = \"Cross Domain Classification of Education Talk Turns\",\n    author = \"Ganti, Achyutarama R.  and\n      Wilson, Steven R.  and\n      Wing-Yue, Geoffrey Louie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.461/\",\n    pages = \"6897--6917\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.461.pdf",
        "site": "https://aclanthology.org/2025.coling-main.461/",
        "pdf_size": 735263,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aevZdb_9aCcJ:scholar.google.com/&scioq=Cross+Domain+Classification+of+Education+Talk+Turns&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "University of Michigan-Flint; University of Michigan-Flint; Oakland University",
        "aff_domain": "umich.edu;umich.edu;oakland.edu",
        "email": "umich.edu;umich.edu;oakland.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Michigan;Oakland University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umflint.edu;https://www.oakland.edu",
        "aff_unique_abbr": "UM-Flint;OU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Flint;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.678",
        "title": "Cross-Dialect Information Retrieval: Information Access in Low-Resource and High-Variance Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A large amount of local and culture-specific knowledge (e.g., people, traditions, food) can only be found in documents written in dialects. While there has been extensive research conducted on cross-lingual information retrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received limited attention. Dialect retrieval poses unique challenges due to the limited availability of resources to train retrieval models and the high variability in non-standardized languages. We study these challenges on the example of German dialects and introduce the first German dialect retrieval dataset, dubbed WikiDIR, which consists of seven German dialects extracted from Wikipedia. Using WikiDIR, we demonstrate the weakness of lexical methods in dealing with high lexical variation in dialects. We further show that commonly used CLIR methods such as query translation or zero-shot cross-lingual transfer with multilingual encoders do not transfer well to extremely low-resource setups, motivating the need for resource-lean and dialect-specific retrieval models.",
        "author": "Robert Litschko; Oliver Kraus; Verena Blaschke; Barbara Plank",
        "authorids": "/r/robert-litschko/; /o/oliver-kraus/; /v/verena-blaschke/; /b/barbara-plank/",
        "bibtex": "@inproceedings{litschko-etal-2025-cross,\n    title = \"Cross-Dialect Information Retrieval: Information Access in Low-Resource and High-Variance Languages\",\n    author = \"Litschko, Robert  and\n      Kraus, Oliver  and\n      Blaschke, Verena  and\n      Plank, Barbara\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.678/\",\n    pages = \"10158--10171\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.678.pdf",
        "site": "https://aclanthology.org/2025.coling-main.678/",
        "pdf_size": 859851,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13917423041355068669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "MaiNLP, Center for Information and Language Processing, LMU Munich, Germany+Munich Center for Machine Learning (MCML), Munich, Germany; MaiNLP, Center for Information and Language Processing, LMU Munich, Germany; MaiNLP, Center for Information and Language Processing, LMU Munich, Germany+Munich Center for Machine Learning (MCML), Munich, Germany; MaiNLP, Center for Information and Language Processing, LMU Munich, Germany+Munich Center for Machine Learning (MCML), Munich, Germany",
        "aff_domain": "lmu.de;lmu.de;lmu.de;lmu.de",
        "email": "lmu.de;lmu.de;lmu.de;lmu.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0+1;0+1",
        "aff_unique_norm": "LMU Munich;Munich Center for Machine Learning",
        "aff_unique_dep": "Center for Information and Language Processing;",
        "aff_unique_url": "https://www.lmu.de;",
        "aff_unique_abbr": "LMU;MCML",
        "aff_campus_unique_index": "0+0;0;0+0;0+0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.631",
        "title": "Cross-Domain Fake News Detection based on Dual-Granularity Adversarial Training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-domain fake news detection, aiming to detect fake news in unseen domains, has achieved promising results with the help of pre-trained language models. Existing approaches mainly relied on extracting domain-independent representations or modeling domain discrepancies to achieve domain adaptation. However, we found that the relationship between entities in a piece of news and its corresponding label (fake or real) fluctuates among different domains. Such discrepancy is ignored by existing methods, leading to model entity bias. Therefore, in this paper, we propose a novel cross-domain fake news detection method based on dual-granularity adversarial training from the perspective of document-level and entity-level. Specifically, both the news pieces and their entities are modeled individually to construct an encoder that can generate domain-independent representations using adversarial training. Moreover, the dual-granularity soft prompt, consisting of two independent learnable segments trained on the source domains, is employed to make the model easily adapt to the unseen target domains. In addition, MultiFC, a released dataset for cross domain fake news detection, is not suitable for the evaluation due to its unreasonable domain construction rules. We artificially reconstructed the dataset and named it New-MultiFC, which is a more domain-discriminative dataset. Experimental results on both the newly constructed New-MultiFC and FND3 show the effectiveness of the proposed approach, achieving the state-of-the-art results in unseen domains.",
        "author": "Wenjie Wei; Yanyue Zhang; Jinyan Li; Panfei Liu; Deyu Zhou",
        "authorids": "/w/wenjie-wei/; /y/yanyue-zhang/; /j/jinyan-li/; /p/panfei-liu/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{wei-etal-2025-cross,\n    title = \"Cross-Domain Fake News Detection based on Dual-Granularity Adversarial Training\",\n    author = \"Wei, Wenjie  and\n      Zhang, Yanyue  and\n      Li, Jinyan  and\n      Liu, Panfei  and\n      Zhou, Deyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.631/\",\n    pages = \"9407--9417\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.631.pdf",
        "site": "https://aclanthology.org/2025.coling-main.631/",
        "pdf_size": 7700477,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XdJ4yJ3BQ74J:scholar.google.com/&scioq=Cross-Domain+Fake+News+Detection+based+on+Dual-Granularity+Adversarial+Training&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Southeast University, China; School of Computer Science and Engineering, Southeast University, China; Huawei Technologies Co., Ltd; Huawei Technologies Co., Ltd; School of Computer Science and Engineering, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;hotmail.com;nuaa.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;hotmail.com;nuaa.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Southeast University;Huawei Technologies",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "https://www.seu.edu.cn/;https://www.huawei.com",
        "aff_unique_abbr": "SEU;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.675",
        "title": "Cross-Lingual Knowledge Projection and Knowledge Enhancement for Zero-Shot Question Answering in Low-Resource Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge bases (KBs) in low-resource languages (LRLs) are often incomplete, posing a challenge for developing effective question answering systems over KBs in those languages. On the other hand, the size of training corpora for LRL language models is also limited, restricting the ability to do zero-shot question answering using multilingual language models. To address these issues, we propose a two-fold approach. First, we introduce LeNS-Align, a novel cross-lingual mapping technique which improves the quality of word alignments extracted from parallel English-LRL text by combining lexical alignment, named entity recognition, and semantic alignment. LeNS-Align is applied to perform cross-lingual projection of KB triples. Second, we leverage the projected KBs to enhance multilingual language models\u2019 question answering capabilities by augmenting the models with Graph Neural Networks embedding the projected knowledge. We apply our approach to map triples from two existing English KBs, ConceptNet and DBpedia, to create comprehensive LRL knowledge bases for four low-resource South African languages. Evaluation on three translated test sets show that our approach improves zero-shot question answering accuracy by up to 17% compared to baselines without KB access. The results highlight how our approach contributes to bridging the knowledge gap for low-resource languages by expanding knowledge coverage and question answering capabilities.",
        "author": "Sello Ralethe; Jan Buys",
        "authorids": "/s/sello-ralethe/; /j/jan-buys/",
        "bibtex": "@inproceedings{ralethe-buys-2025-cross,\n    title = \"Cross-Lingual Knowledge Projection and Knowledge Enhancement for Zero-Shot Question Answering in Low-Resource Languages\",\n    author = \"Ralethe, Sello  and\n      Buys, Jan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.675/\",\n    pages = \"10111--10124\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.675.pdf",
        "site": "https://aclanthology.org/2025.coling-main.675/",
        "pdf_size": 372899,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:D_qs1fjn5IgJ:scholar.google.com/&scioq=Cross-Lingual+Knowledge+Projection+and+Knowledge+Enhancement+for+Zero-Shot+Question+Answering+in+Low-Resource+Languages&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Cape Town, South Africa; Department of Computer Science, University of Cape Town, South Africa",
        "aff_domain": "myuct.ac.za;cs.uct.ac.za",
        "email": "myuct.ac.za;cs.uct.ac.za",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cape Town",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uct.ac.za",
        "aff_unique_abbr": "UCT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Africa"
    },
    {
        "id": "2025.coling-main.429",
        "title": "Cross-Lingual Sentence Compression for Length-Constrained Subtitles in Low-Resource Settings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores the joint task of machine translation and sentence compression, emphasizing its application in subtitle generation for broadcast and live media for low-resource languages and hardware. We develop CLSC (Cross-Lingual Sentence Compression), a system trained on openly available parallel corpora organized by compression ratios, where the target length is constrained to a fraction of the source sentence length. We present two training methods: 1) Multiple Models (MM), where individual models are trained separately for each compression ratio, and 2) a Controllable Model (CM), a single model per language using a compression token to encode length constraints. We evaluate both subtitle data and transcriptions from the EuroParl corpus. To accommodate low-resource settings, we constrain data sampling for training and show results for transcriptions in French, Hungarian, Lithuanian, and Polish and subtitles in Albanian, Basque, Malay, and Norwegian. Our models preserve high semantic meaning and metric evaluations for compressed contexts.",
        "author": "Tollef Emil J\u00c3. rgensen; Ole Jakob Mengshoel",
        "authorids": "/t/tollef-emil-ja-rgensen/; /o/ole-jakob-mengshoel/",
        "bibtex": "@inproceedings{ja-rgensen-mengshoel-2025-cross,\n    title = \"Cross-Lingual Sentence Compression for Length-Constrained Subtitles in Low-Resource Settings\",\n    author = \"J{\\~A}, rgensen, Tollef Emil  and\n      Mengshoel, Ole Jakob\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.429/\",\n    pages = \"6447--6458\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.429.pdf",
        "site": "https://aclanthology.org/2025.coling-main.429/",
        "pdf_size": 387292,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:U2qJkeiXqPEJ:scholar.google.com/&scioq=Cross-Lingual+Sentence+Compression+for+Length-Constrained+Subtitles+in+Low-Resource+Settings&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Norwegian University of Science and Technology, Data and Artificial Intelligence Group; Norwegian University of Science and Technology, Data and Artificial Intelligence Group",
        "aff_domain": "ntnu.no;ntnu.no",
        "email": "ntnu.no;ntnu.no",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Norwegian University of Science and Technology",
        "aff_unique_dep": "Data and Artificial Intelligence Group",
        "aff_unique_url": "https://www.ntnu.no",
        "aff_unique_abbr": "NTNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Norway"
    },
    {
        "id": "2025.coling-main.77",
        "title": "Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",
        "author": "Qianli Wang; Tatiana Anikina; Nils Feldhus; Simon Ostermann; Sebastian M\u00f6ller; Vera Schmitt",
        "authorids": "/q/qianli-wang/; /t/tatiana-anikina/; /n/nils-feldhus/; /s/simon-ostermann/; /s/sebastian-moller/; /v/vera-schmitt/",
        "bibtex": "@inproceedings{wang-etal-2025-cross,\n    title = \"Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem\",\n    author = {Wang, Qianli  and\n      Anikina, Tatiana  and\n      Feldhus, Nils  and\n      Ostermann, Simon  and\n      M{\\\"o}ller, Sebastian  and\n      Schmitt, Vera},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.77/\",\n    pages = \"1150--1167\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.77.pdf",
        "site": "https://aclanthology.org/2025.coling-main.77/",
        "pdf_size": 1632552,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11825685001236527191&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "German Research Center for Artificial Intelligence (DFKI)+Technische Universit\u00e4t Berlin; German Research Center for Artificial Intelligence (DFKI)+Saarland Informatics Campus; German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI)+Saarland Informatics Campus+Centre for European Research in Trusted AI (CERTAIN); German Research Center for Artificial Intelligence (DFKI)+Technische Universit\u00e4t Berlin; German Research Center for Artificial Intelligence (DFKI)+Technische Universit\u00e4t Berlin",
        "aff_domain": "dfki.de;dfki.de;dfki.de;dfki.de;dfki.de;dfki.de",
        "email": "dfki.de;dfki.de;dfki.de;dfki.de;dfki.de;dfki.de",
        "github": "https://github.com/qiaw99/Cross-Refine",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+2;0;0+2+3;0+1;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;Technische Universit\u00e4t Berlin;Saarland University;Centre for European Research in Trusted AI",
        "aff_unique_dep": ";;Department of Computer Science;",
        "aff_unique_url": "https://www.dFKI.de;https://www.tu-berlin.de;https://www.uni-saarland.de;",
        "aff_unique_abbr": "DFKI;TU Berlin;Uni Saar;CERTAIN",
        "aff_campus_unique_index": ";1;1;;",
        "aff_campus_unique": ";Saarbr\u00fccken",
        "aff_country_unique_index": "0+0;0+0;0;0+0+1;0+0;0+0",
        "aff_country_unique": "Germany;Unknown"
    },
    {
        "id": "2025.coling-main.520",
        "title": "Cross-lingual Evaluation of Multilingual Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Scaling automatic evaluation of multilingual text generation of LLMs to new tasks, domains, and languages remains a challenge. Traditional evaluation on benchmark datasets carries the risk of reference data leakage in LLM training or involves additional human annotation effort. The alternative strategy of using another LLM as a scorer also faces uncertainty about the ability of this LLM itself to score non-English text. To address these issues, we propose an annotation-free cross-lingual evaluation protocol for multilingual text generation. Given an LLM candidate to be evaluated and a set of non-English inputs for a particular text generation task, our method first generates English references from the translation of the non-English inputs into English. This is done by an LLM that excels in the equivalent English text generation task. The non-English text generated by the LLM candidate is compared against the generated English references using a cross-lingual evaluation metric to assess the ability of the candidate LLM on multilingual text generation. Our protocol shows a high correlation to the reference-based ROUGE metric in four languages on news text summarization. We also evaluate a diverse set of LLMs in over 90 languages with different prompting strategies to study their multilingual generative abilities.",
        "author": "Shamil Chollampatt; Minh Quang Pham; Sathish Reddy Indurthi; Marco Turchi",
        "authorids": "/s/shamil-chollampatt/; /m/minh-quang-pham/; /s/sathish-reddy-indurthi/; /m/marco-turchi/",
        "bibtex": "@inproceedings{chollampatt-etal-2025-cross,\n    title = \"Cross-lingual Evaluation of Multilingual Text Generation\",\n    author = \"Chollampatt, Shamil  and\n      Pham, Minh Quang  and\n      Indurthi, Sathish Reddy  and\n      Turchi, Marco\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.520/\",\n    pages = \"7766--7777\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.520.pdf",
        "site": "https://aclanthology.org/2025.coling-main.520/",
        "pdf_size": 525261,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16759061814518730383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Zoom Communications, Inc.; ; ; ",
        "aff_domain": "zoom.us; ; ; ",
        "email": "zoom.us; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0",
        "aff_unique_norm": "Zoom Communications",
        "aff_unique_dep": "",
        "aff_unique_url": "https://zoom.us",
        "aff_unique_abbr": "Zoom",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.484",
        "title": "Cross-lingual Social Misinformation Detector based on Hierarchical Mixture-of-Experts Adapter",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The spread of social misinformation has been a global concern, particularly affecting non-native speaker users who are more susceptible to misinformation on foreign social media platforms. In light of this, this study focuses on mitigating the challenges faced by social misinformation detectors in quickly regaining capability after crossing linguistic borders, especially for non-native users with only monolingual social media histories. By integrating sentiment analysis as an auxiliary, less sensitive task, we transform the challenging cross-lingual transfer into a manageable multi-task framework. Then, we propose HierMoE-Adpt, a novel, cost-effective parameter efficient finetuning method based on hierarchical mixture-of-experts adaptation, to enhance cross-lingual social misinformation detection. HierMoE-Adpt includes a hierarchical routing strategy and an expert-mask mechanism, effectively merge knowledge about the understanding posts in new language and misinformation detection capabilities, contributing to recover the performance of personal misinformation detectors in sync with the dynamics of personal international travel.",
        "author": "Haofang Fan; Xiran Hu; Geng Zhao",
        "authorids": "/h/haofang-fan/; /x/xiran-hu/; /g/geng-zhao/",
        "bibtex": "@inproceedings{fan-etal-2025-cross,\n    title = \"Cross-lingual Social Misinformation Detector based on Hierarchical Mixture-of-Experts Adapter\",\n    author = \"Fan, Haofang  and\n      Hu, Xiran  and\n      Zhao, Geng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.484/\",\n    pages = \"7253--7265\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.484.pdf",
        "site": "https://aclanthology.org/2025.coling-main.484/",
        "pdf_size": 13956821,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:W1aQPDvf7AYJ:scholar.google.com/&scioq=Cross-lingual+Social+Misinformation+Detector+based+on+Hierarchical+Mixture-of-Experts+Adapter&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Interdisciplinary Center for Scientific Computing, Heidelberg University; Department of Computational Linguistics, Heidelberg University; Department of Computational Linguistics, Heidelberg University",
        "aff_domain": "stud.uni-heidelberg.de;stud.uni-heidelberg.de;cl.uni-heidelberg.de",
        "email": "stud.uni-heidelberg.de;stud.uni-heidelberg.de;cl.uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "Interdisciplinary Center for Scientific Computing",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni Heidelberg",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.97",
        "title": "Cross-lingual Text Classification Transfer: The Case of Ukrainian",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. To support further fair development of NLP models, exploring the possibilities of effective knowledge transfer to new languages is crucial. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks, i.e., different types of style, or harmful speech, or texts relationships. However, the amount of resources required for such corpora collection from scratch is understandable. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks\u2014toxicity classification, formality classification, and natural language inference (NLI)\u2014providing the \u201crecipe\u201d for the optimal setups for each task.",
        "author": "Daryna Dementieva; Valeriia Khylenko; Georg Groh",
        "authorids": "/d/daryna-dementieva/; /v/valeriia-khylenko/; /g/georg-groh/",
        "bibtex": "@inproceedings{dementieva-etal-2025-cross,\n    title = \"Cross-lingual Text Classification Transfer: The Case of {U}krainian\",\n    author = \"Dementieva, Daryna  and\n      Khylenko, Valeriia  and\n      Groh, Georg\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.97/\",\n    pages = \"1451--1464\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.97.pdf",
        "site": "https://aclanthology.org/2025.coling-main.97/",
        "pdf_size": 712457,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1DwnHhUd-KoJ:scholar.google.com/&scioq=Cross-lingual+Text+Classification+Transfer:+The+Case+of+Ukrainian&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Technical University of Munich, School of Computation, Information and Technology, Germany; Technical University of Munich, School of Computation, Information and Technology, Germany; Technical University of Munich, School of Computation, Information and Technology, Germany",
        "aff_domain": "tum.de;gmail.com;in.tum.de",
        "email": "tum.de;gmail.com;in.tum.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technical University of Munich",
        "aff_unique_dep": "School of Computation, Information and Technology",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.736",
        "title": "CryptOpiQA: A new Opinion and Question Answering dataset on Cryptocurrency",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cryptocurrency has attracted a lot of public attention and opinion worldwide. Users have different kinds of information needs regarding such topics and publicly available information is a good resource to satisfy those information needs. In this paper, we investigate the public opinion on cryptocurrency and bitcoin on two social media \u2013 Twitter and Reddit. We have created a multi-level dataset CryptOpiQA and garnered valuable insights. The dataset contains both gold standard (manually annotated) and silver standard (inferred from the gold standard) labels. As a part of this dataset, we have also created a Question Answering sub-corpus. We have used state-of-the-art LLMs and advanced techniques such as retrieval augmented generation (RAG) to improve question-answering (QnA) results. We believe this dataset and the analysis will be useful in studying user opinions and Question-Answering on cryptocurrency in the research community.",
        "author": "Sougata Sarkar; Aditya Badwal; Amartya Roy; Koustav Rudra; Kripabandhu Ghosh",
        "authorids": "/s/sougata-sarkar/; /a/aditya-badwal/; /a/amartya-roy/; /k/koustav-rudra/; /k/kripabandhu-ghosh/",
        "bibtex": "@inproceedings{sarkar-etal-2025-cryptopiqa,\n    title = \"{C}rypt{O}pi{QA}: A new Opinion and Question Answering dataset on Cryptocurrency\",\n    author = \"Sarkar, Sougata  and\n      Badwal, Aditya  and\n      Roy, Amartya  and\n      Rudra, Koustav  and\n      Ghosh, Kripabandhu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.736/\",\n    pages = \"11107--11120\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.736.pdf",
        "site": "https://aclanthology.org/2025.coling-main.736/",
        "pdf_size": 505333,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rD0cCy6kPokJ:scholar.google.com/&scioq=CryptOpiQA:+A+new+Opinion+and+Question+Answering+dataset+on+Cryptocurrency&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Deloitte* + Indian Institute of Science Education and Research Kolkata; Indian Institute of Science Education and Research Kolkata; Bosch; Indian Institute of Technology Kharagpur; Indian Institute of Science Education and Research Kolkata",
        "aff_domain": "gmail.com;gmail.com;in.bosch.com;ai.iitkgp.ac.in;iiserkol.ac.in",
        "email": "gmail.com;gmail.com;in.bosch.com;ai.iitkgp.ac.in;iiserkol.ac.in",
        "github": "",
        "project": "https://doi.org/10.5281/zenodo.14469000",
        "author_num": 5,
        "aff_unique_index": "0+1;1;2;3;1",
        "aff_unique_norm": "Deloitte;Indian Institute of Science Education and Research;Robert Bosch GmbH;Indian Institute of Technology Kharagpur",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.deloitte.com;https://www.iiserkol.ac.in;https://www.bosch.com;https://www.iitkgp.ac.in",
        "aff_unique_abbr": "Deloitte;IISER Kolkata;Bosch;IIT Kharagpur",
        "aff_campus_unique_index": "1;1;2;1",
        "aff_campus_unique": ";Kolkata;Kharagpur",
        "aff_country_unique_index": "0+1;1;2;1;1",
        "aff_country_unique": "United States;India;Germany"
    },
    {
        "id": "2025.coling-main.567",
        "title": "Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede\u2019s Cultural Dimensions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds. While the discourse has focused mainly on political and social biases, our research proposes a Cultural Alignment Test (Hoftede\u2019s CAT) to quantify cultural alignment using Hofstede\u2019s cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to quantitatively evaluate LLMs\u2014namely Llama 2, GPT-3.5, and GPT-4\u2014against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language-specific fine-tuning on the models\u2019 behavioural tendencies and cultural values. Our results quantify the cultural alignment of LLMs and reveal the difference between LLMs in explanatory cultural dimensions. Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings. However, it faces challenges with American and Arab cultures. The research also highlights that fine-tuning LLama 2 models with different languages changes their responses to cultural questions, emphasizing the need for culturally diverse development in AI for worldwide acceptance and ethical use. For more details or to contribute to this research, visit our GitHub page https://github.com/reemim/Hofstedes_CAT",
        "author": "Reem Masoud; Ziquan Liu; Martin Ferianc; Philip C. Treleaven; Miguel Rodrigues Rodrigues",
        "authorids": "/r/reem-masoud/; /z/ziquan-liu/; /m/martin-ferianc/; /p/philip-c-treleaven/; /m/miguel-rodrigues-rodrigues/",
        "bibtex": "@inproceedings{masoud-etal-2025-cultural,\n    title = \"Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede{'}s Cultural Dimensions\",\n    author = \"Masoud, Reem  and\n      Liu, Ziquan  and\n      Ferianc, Martin  and\n      Treleaven, Philip C.  and\n      Rodrigues, Miguel Rodrigues\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.567/\",\n    pages = \"8474--8503\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.567.pdf",
        "site": "https://aclanthology.org/2025.coling-main.567/",
        "pdf_size": 2605793,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13567026031043419530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electronic and Electrical Engineering, University College London + Department of Electrical Engineering, King Abdulaziz University; Department of Electronic and Electrical Engineering, University College London; Department of Electronic and Electrical Engineering, University College London; Department of Computer Science, University College London; Department of Electronic and Electrical Engineering, University College London + AI Centre, University College London",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "email": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "github": "https://github.com/reemim/Hofstedes_CAT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0+0",
        "aff_unique_norm": "University College London;King Abdulaziz University",
        "aff_unique_dep": "Department of Electronic and Electrical Engineering;Department of Electrical Engineering",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.kau.edu.sa",
        "aff_unique_abbr": "UCL;KAU",
        "aff_campus_unique_index": "0;0;0;0;0+0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0+1;0;0;0;0+0",
        "aff_country_unique": "United Kingdom;Saudi Arabia"
    },
    {
        "id": "2025.coling-main.227",
        "title": "CycleOIE: A Low-Resource Training Framework For Open Information Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Open Information Extraction (OpenIE) aims to extract structured information in the form of triples from unstructured text, serving as a foundation for various downstream NLP tasks. Despite the success of neural OpenIE models, their dependence on large-scale annotated datasets poses a challenge, particularly in low-resource settings. In this paper, we introduce a novel approach to address the low-resource OpenIE task through two key innovations: (1) we improve the quality of training data by curating small-scale, high-quality datasets annotated by a large language model (GPT-3.5), leveraging both OpenIE principles and few-shot examples to form LSOIE-g principles and LSOIE-g examples; (2) we propose CycleOIE, a training framework that maximizes data efficiency through a cycle-consistency mechanism, enabling the model to learn effectively from minimal data. Experimental results show that CycleOIE, when trained on only 2k+ instances, achieves comparable results to models trained on over 90k instances. Our contributions are further validated through extensive experiments, demonstrating the superior performance of CycleOIE and our curated LSOIE-g datasets in low-resource OpenIE as well as revealing the internal mechanisms of CycleOIE.",
        "author": "Zhihong Jin; Chunhong Zhang; Zheng Hu; Jibin Yu; Ruiqi Ma; Qingyun Chen; Xiaohao Liao; Yanxing Zhang",
        "authorids": "/z/zhihong-jin/; /c/chunhong-zhang/; /z/zheng-hu/; /j/jibin-yu/; /r/ruiqi-ma/; /q/qingyun-chen/; /x/xiaohao-liao/; /y/yanxing-zhang/",
        "bibtex": "@inproceedings{jin-etal-2025-cycleoie,\n    title = \"{C}ycle{OIE}: A Low-Resource Training Framework For Open Information Extraction\",\n    author = \"Jin, Zhihong  and\n      Zhang, Chunhong  and\n      Hu, Zheng  and\n      Yu, Jibin  and\n      Ma, Ruiqi  and\n      Chen, Qingyun  and\n      Liao, Xiaohao  and\n      Zhang, Yanxing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.227/\",\n    pages = \"3372--3390\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.227.pdf",
        "site": "https://aclanthology.org/2025.coling-main.227/",
        "pdf_size": 600445,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:riRSlATqHScJ:scholar.google.com/&scioq=CycleOIE:+A+Low-Resource+Training+Framework+For+Open+Information+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.393",
        "title": "DAEA: Enhancing Entity Alignment in Real-World Knowledge Graphs Through Multi-Source Domain Adaptation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity Alignment (EA) is a critical task in Knowledge Graph (KG) integration, aimed at identifying and matching equivalent entities that represent the same real-world objects. While EA methods based on knowledge representation learning have shown strong performance on synthetic benchmark datasets such as DBP15K, their effectiveness significantly decline in real-world scenarios which often involve data that is highly heterogeneous, incomplete, and domain-specific, as seen in datasets like DOREMUS and AGROLD. Addressing this challenge, we propose DAEA, a novel EA approach with Domain Adaptation that leverages the data characteristics of synthetic benchmarks for improved performance in real-world datasets. DAEA introduces a multi-source KGs selection mechanism and a specialized domain adaptive entity alignment loss function to bridge the gap between real-world data and optimal benchmark data, mitigating the challenges posed by aligning entities across highly heterogeneous KGs. Experimental results demonstrate that DAEA outperforms state-of-the-art models on real-world datasets, achieving a 29.94% improvement in Hits@1 on DOREMUS and a 5.64% improvement on AGROLD. Code is available at https://github.com/yangxiaoxiaoly/DAEA.",
        "author": "Linyan Yang; Shiqiao Zhou; Jingwei Cheng; Fu Zhang; Jizheng Wan; Shuo Wang; Mark Lee",
        "authorids": "/l/linyan-yang/; /s/shiqiao-zhou/; /j/jingwei-cheng/; /f/fu-zhang/; /j/jizheng-wan/; /s/shuo-wang/; /m/mark-lee/",
        "bibtex": "@inproceedings{yang-etal-2025-daea,\n    title = \"{DAEA}: Enhancing Entity Alignment in Real-World Knowledge Graphs Through Multi-Source Domain Adaptation\",\n    author = \"Yang, Linyan  and\n      Zhou, Shiqiao  and\n      Cheng, Jingwei  and\n      Zhang, Fu  and\n      Wan, Jizheng  and\n      Wang, Shuo  and\n      Lee, Mark\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.393/\",\n    pages = \"5890--5901\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.393.pdf",
        "site": "https://aclanthology.org/2025.coling-main.393/",
        "pdf_size": 526718,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Z3MNvuRd3voJ:scholar.google.com/&scioq=DAEA:+Enhancing+Entity+Alignment+in+Real-World+Knowledge+Graphs+Through+Multi-Source+Domain+Adaptation&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/yangxiaoxiaoly/DAEA",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.507",
        "title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in event argument extraction (EAE) involve incorporating useful auxiliary information into models during training and inference, such as retrieved instances and event templates. These methods face two challenges: (1) the retrieval results may be irrelevant and (2) templates are developed independently for each event without considering their possible relationship. In this work, we propose DEGAP to address these challenges through a simple yet effective components: dual prefixes, i.e. learnable prompt vectors, where the instance-oriented prefix and template-oriented prefix are trained to learn information from different event instances and templates. Additionally, we propose an event-guided adaptive gating mechanism, which can adaptively leverage possible connections between different events and thus capture relevant information from the prefix. Finally, these event-guided prefixes provide relevant information as cues to EAE model without retrieval. Extensive experiments demonstrate that our method achieves new state-of-the-art performance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further analysis shows the impact of different components.",
        "author": "Guanghui Wang; Dexi Liu; Jian-Yun Nie; Qizhi Wan; Rong Hu; Xiping Liu; Wanlong Liu; Jiaming Liu",
        "authorids": "/g/guanghui-wang/; /d/dexi-liu/; /j/jian-yun-nie/; /q/qizhi-wan/; /r/rong-hu/; /x/xiping-liu/; /w/wanlong-liu/; /j/jiaming-liu/",
        "bibtex": "@inproceedings{wang-etal-2025-degap,\n    title = \"{DEGAP}: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying\",\n    author = \"Wang, Guanghui  and\n      Liu, Dexi  and\n      Nie, Jian-Yun  and\n      Wan, Qizhi  and\n      Hu, Rong  and\n      Liu, Xiping  and\n      Liu, Wanlong  and\n      Liu, Jiaming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.507/\",\n    pages = \"7598--7613\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.507.pdf",
        "site": "https://aclanthology.org/2025.coling-main.507/",
        "pdf_size": 1967818,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10040567673366796306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information Management, Jiangxi University of Finance and Economics; School of Information Management, Jiangxi University of Finance and Economics; University of Montreal (UdeM), Department of Computer Science and Operations Research (DIRO); School of Information Management, Jiangxi University of Finance and Economics; School of Information Management, Jiangxi University of Finance and Economics; School of Information Management, Jiangxi University of Finance and Economics; School of Computer Science and Engineering, University of Electronic Science and Technology of China; Department of Statistics, Division of the Physical Sciences, the University of Chicago",
        "aff_domain": "163.com;163.com; ; ; ; ; ; ",
        "email": "163.com;163.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;0;0;0;2;3",
        "aff_unique_norm": "Jiangxi University of Finance and Economics;University of Montreal;University of Electronic Science and Technology of China;University of Chicago",
        "aff_unique_dep": "School of Information Management;Department of Computer Science and Operations Research;School of Computer Science and Engineering;Department of Statistics",
        "aff_unique_url": "http://www.jxufe.edu.cn;https://www.umontreal.ca;https://www.uestc.edu.cn;https://www.uchicago.edu",
        "aff_unique_abbr": ";UdeM;UESTC;UChicago",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "0;0;1;0;0;0;0;2",
        "aff_country_unique": "China;Canada;United States"
    },
    {
        "id": "2025.coling-main.504",
        "title": "DORA: Dynamic Optimization Prompt for Continuous Reflection of LLM-based Agent",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Autonomous agents powered by large language models (LLMs) hold significant potential across various domains. The Reflection framework is designed to help agents learn from past mistakes in complex tasks. While previous research has shown that reflection can enhance performance, our investigation reveals a key limitation: meaningful self-reflection primarily occurs at the beginning of iterations, with subsequent attempts failing to produce further improvements. We term this phenomenon \u201cEarly Stop Reflection,\u201d where the reflection process halts prematurely, limiting the agent\u2019s ability to engage in continuous learning. To address this, we propose the DORA method (Dynamic and Optimized Reflection Advice), which generates task-adaptive and diverse reflection advice. DORA introduces an external open-source small language model (SLM) that dynamically generates prompts for the reflection LLM. The SLM uses feedback from the agent and optimizes the prompt generation process through a non-gradient Bayesian Optimization (BO) algorithm, ensuring the reflection process evolves and adapts over time. Our experiments in the MiniWoB++ and Alfworld environments confirm that DORA effectively mitigates the \u201cEarly Stop Reflection\u201d issue, enabling agents to maintain iterative improvements and boost performance in long-term, complex tasks. Code are available at https://anonymous.4open.science/r/DORA-44FB/.",
        "author": "Kun Li; Tingzhang Zhao; Wei Zhou; Songlin Hu",
        "authorids": "/k/kun-li/; /t/tingzhang-zhao/; /w/wei-zhou/; /s/songlin-hu/",
        "bibtex": "@inproceedings{li-etal-2025-dora,\n    title = \"{DORA}: Dynamic Optimization Prompt for Continuous Reflection of {LLM}-based Agent\",\n    author = \"Li, Kun  and\n      Zhao, Tingzhang  and\n      Zhou, Wei  and\n      Hu, Songlin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.504/\",\n    pages = \"7546--7557\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.504.pdf",
        "site": "https://aclanthology.org/2025.coling-main.504/",
        "pdf_size": 3637255,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:R55NN9LnVI8J:scholar.google.com/&scioq=DORA:+Dynamic+Optimization+Prompt+for+Continuous+Reflection+of+LLM-based+Agent&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "https://github.com/linkseed18612254945/FineRob",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.465",
        "title": "DP-FROST: Differentially Private Fine-tuning of Pre-trained Models with Freezing Model Parameters",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Training models with differential privacy has received a lot of attentions since differential privacy provides theoretical guarantee of privacy preservation. For a task in a specific domain, since a large-scale pre-trained model in the same domain contains general knowledge of the task, using such a model requires less effort in designing and training the model. However, differentially privately fine-tuning such models having a large number of trainable parameters results in large degradation of utility. Thus, we propose methods that effectively fine-tune the large-scale pre-trained models with freezing unimportant parameters for downstream tasks while satisfying differential privacy. To select the parameters to be fine-tuned, we propose several efficient methods based on the gradients of model parameters. We show the effectiveness of the proposed method by performing experiments with real datasets.",
        "author": "Daeyoung Hong; Woohwan Jung; Kyuseok Shim",
        "authorids": "/d/daeyoung-hong/; /w/woohwan-jung/; /k/kyuseok-shim/",
        "bibtex": "@inproceedings{hong-etal-2025-dp,\n    title = \"{DP}-{FROST}: Differentially Private Fine-tuning of Pre-trained Models with Freezing Model Parameters\",\n    author = \"Hong, Daeyoung  and\n      Jung, Woohwan  and\n      Shim, Kyuseok\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.465/\",\n    pages = \"6966--6984\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.465.pdf",
        "site": "https://aclanthology.org/2025.coling-main.465/",
        "pdf_size": 588096,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sk1su6yHMRUJ:scholar.google.com/&scioq=DP-FROST:+Differentially+Private+Fine-tuning+of+Pre-trained+Models+with+Freezing+Model+Parameters&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Myongji University; Hanyang University; Seoul National University",
        "aff_domain": "mju.ac.kr;hanyang.ac.kr;snu.ac.kr",
        "email": "mju.ac.kr;hanyang.ac.kr;snu.ac.kr",
        "github": "https://github.com/daeyounghong/dp-frost",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Myongji University;Hanyang University;Seoul National University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.myongji.ac.kr;https://www.hanyang.ac.kr;https://www.snu.ac.kr",
        "aff_unique_abbr": "Myongji;HYU;SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.415",
        "title": "DROWN: Towards Tighter LiRPA-based Robustness Certification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The susceptibility of deep neural networks to adversarial attacks is a well-established concern. To address this problem, robustness certification is proposed, which, unfortunately, suffers from precision or scalability issues. In this paper, we present DROWN (Dual CROWN), a novel method for certifying the robustness of DNNs. The advantage of DROWN is that it tightens classic LiRPA-based methods yet maintains similar scalability, which comes from refining pre-activation bounds of ReLU relaxations using two pairs of linear bounds derived from different relaxations of ReLU units in previous layers. The extensive evaluations show that DROWN achieves up to 83.39% higher certified robust accuracy than the baseline on CNNs and up to 4.68 times larger certified radii than the baseline on Transformers. Meanwhile, the running time of DROWN is about twice that of the baseline.",
        "author": "Yunruo Zhang; Tianyu Du; Shouling Ji; Shanqing Guo",
        "authorids": "/y/yunruo-zhang/; /t/tianyu-du/; /s/shouling-ji/; /s/shanqing-guo/",
        "bibtex": "@inproceedings{zhang-etal-2025-drown,\n    title = \"{DROWN}: Towards Tighter {L}i{RPA}-based Robustness Certification\",\n    author = \"Zhang, Yunruo  and\n      Du, Tianyu  and\n      Ji, Shouling  and\n      Guo, Shanqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.415/\",\n    pages = \"6212--6229\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.415.pdf",
        "site": "https://aclanthology.org/2025.coling-main.415/",
        "pdf_size": 1224004,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cmWpFM5lGHAJ:scholar.google.com/&scioq=DROWN:+Towards+Tighter+LiRPA-based+Robustness+Certification&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Cyber Science and Technology, Shandong University; School of Software Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; School of Cyber Science and Technology, Shandong University",
        "aff_domain": "mail.sdu.edu.cn;zju.edu.cn;zju.edu.cn;sdu.edu.cn",
        "email": "mail.sdu.edu.cn;zju.edu.cn;zju.edu.cn;sdu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Shandong University;Zhejiang University",
        "aff_unique_dep": "School of Cyber Science and Technology;School of Software Technology",
        "aff_unique_url": "http://www.sdu.edu.cn;http://www.zju.edu.cn",
        "aff_unique_abbr": ";ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.53",
        "title": "DaCoM: Strategies to Construct Domain-specific Low-resource Language Machine Translation Dataset",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Translation of low-resource languages in industrial domains is essential for improving market productivity and ensuring foreign workers have better access to information. However, existing translators struggle with domain-specific terms, and there is a lack of expert annotators for dataset creation. In this work, we propose DaCoM, a methodology for collecting low-resource language pairs from industrial domains to address these challenges. DaCoM is a hybrid translation framework enabling effective data collection. The framework consists of a large language model and neural machine translation. Evaluation verifies existing models perform inadequately on DaCoM-created datasets, with up to 53.7 BLEURT points difference depending on domain inclusion. DaCoM is expected to address the lack of datasets for domain-specific low-resource languages by being easily pluggable into future state-of-the-art models and maintaining an industrial domain-agnostic approach.",
        "author": "Junghoon Kang; Keunjoo Tak; Joungsu Choi; Myunghyun Kim; Junyoung Jang; Youjin Kang",
        "authorids": "/j/junghoon-kang/; /k/keunjoo-tak/; /j/joungsu-choi/; /m/myunghyun-kim/; /j/junyoung-jang/; /y/youjin-kang/",
        "bibtex": "@inproceedings{kang-etal-2025-dacom,\n    title = \"{D}a{C}o{M}: Strategies to Construct Domain-specific Low-resource Language Machine Translation Dataset\",\n    author = \"Kang, Junghoon  and\n      Tak, Keunjoo  and\n      Choi, Joungsu  and\n      Kim, Myunghyun  and\n      Jang, Junyoung  and\n      Kang, Youjin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.53/\",\n    pages = \"612--624\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.53.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.53/",
        "pdf_size": 1579642,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GRqYUeKoY6YJ:scholar.google.com/&scioq=DaCoM:+Strategies+to+Construct+Domain-specific+Low-resource+Language+Machine+Translation+Dataset&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "AI Center, HD Korea Shipbuilding & Offshore Engineering; AI Center, HD Korea Shipbuilding & Offshore Engineering; AI Center, HD Korea Shipbuilding & Offshore Engineering; DT Innovation Department, HD Hyundai Samho; School of Computing, Korea Advanced Institute of Science & Technology; AI Center, HD Korea Shipbuilding & Offshore Engineering",
        "aff_domain": "hd.com; ; ; ; ;hd.com",
        "email": "hd.com; ; ; ; ;hd.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "HD Korea Shipbuilding & Offshore Engineering;HD Hyundai Samho;Korea Advanced Institute of Science & Technology",
        "aff_unique_dep": "AI Center;DT Innovation Department;School of Computing",
        "aff_unique_url": "https://www.hdshipbuilding.com;;https://www.kaist.ac.kr",
        "aff_unique_abbr": ";;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.744",
        "title": "Data Augmentation for Cross-domain Parsing via Lightweight LLM Generation and Tree Hybridization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-domain constituency parsing remains a challenging task due to the lack of high-quality out-of-domain data. In this paper, we propose a data augmentation method via lightweight large language model (LLM) generation and tree hybridization. We utilize LLM to generate phrase structures (subtrees) for the target domain by incorporating grammar rules and lexical head information into the prompt. To better leverage LLM-generated target-domain subtrees, we hybridize them with existing source-domain subtrees to efficiently produce a large number of structurally diverse instances. Experimental results demonstrate that our method achieves significant improvements on five target domains with a lightweight LLM generation cost.",
        "author": "Ziyan Zhang; Yang Hou; Chen Gong; Zhenghua Li",
        "authorids": "/z/ziyan-zhang/; /y/yang-hou/; /c/chen-gong/; /z/zhenghua-li/",
        "bibtex": "@inproceedings{zhang-etal-2025-data,\n    title = \"Data Augmentation for Cross-domain Parsing via Lightweight {LLM} Generation and Tree Hybridization\",\n    author = \"Zhang, Ziyan  and\n      Hou, Yang  and\n      Gong, Chen  and\n      Li, Zhenghua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.744/\",\n    pages = \"11235--11247\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.744.pdf",
        "site": "https://aclanthology.org/2025.coling-main.744/",
        "pdf_size": 525276,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16954549219094535592&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University; School of Computer Science and Technology, Soochow University; School of Computer Science and Technology, Soochow University; School of Computer Science and Technology, Soochow University",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.315",
        "title": "Data Quality Enhancement on the Basis of Diversity with Large Language Models for Text Classification: Uncovered, Difficult, and Noisy",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, the use of large language models (LLMs) for text classification has attracted widespread attention. Despite this, the classification accuracy of LLMs has not yet universally surpassed that of smaller models. LLMs can enhance their performance in text classification through fine-tuning. However, existing data quality research based on LLMs is challenging to apply directly to solve text classification problems. To further improve the performance of LLMs in classification tasks, this paper proposes a data quality enhancement (DQE) method for text classification based on LLMs. This method starts by using a greedy algorithm to select data, dividing the dataset into sampled and unsampled subsets, and then performing fine-tuning of the LLMs using the sampled data. Subsequently, this model is used to predict the outcomes for the unsampled data, categorizing incorrectly predicted data into uncovered, difficult, and noisy data. Experimental results demonstrate that our method effectively enhances the performance of LLMs in text classification tasks and significantly improves training efficiency, saving nearly half of the training time. Our method has achieved state-of-the-art performance in several open-source classification tasks.",
        "author": "Min Zeng; Caiquan Liu; Shiqi Zhang; Li Xie; Chen Sang; Xiaoxin Chen",
        "authorids": "/m/min-zeng/; /c/caiquan-liu/; /s/shiqi-zhang/; /l/li-xie/; /c/chen-sang/; /x/xiaoxin-chen/",
        "bibtex": "@inproceedings{zeng-etal-2025-data,\n    title = \"Data Quality Enhancement on the Basis of Diversity with Large Language Models for Text Classification: Uncovered, Difficult, and Noisy\",\n    author = \"Zeng, Min  and\n      Liu, Caiquan  and\n      Zhang, Shiqi  and\n      Xie, Li  and\n      Sang, Chen  and\n      Chen, Xiaoxin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.315/\",\n    pages = \"4704--4714\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.315.pdf",
        "site": "https://aclanthology.org/2025.coling-main.315/",
        "pdf_size": 352328,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8432210644838470996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "vivo AI Lab; vivo AI Lab; vivo AI Lab; vivo AI Lab; vivo AI Lab; vivo AI Lab",
        "aff_domain": "vivo.com; ; ; ; ; ",
        "email": "vivo.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "vivo",
        "aff_unique_dep": "vivo AI Lab",
        "aff_unique_url": "https://vivo.com",
        "aff_unique_abbr": "vivo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.544",
        "title": "DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While in-context Learning (ICL) has proven to be an effective technique to improve the performance of Large Language Models (LLMs) in a variety of complex tasks, notably in translating natural language questions into Structured Query Language (NL2SQL), the question of how to select the most beneficial demonstration examples remains an open research problem. While prior works often adapted off-the-shelf encoders to retrieve examples dynamically, an inherent discrepancy exists in the representational capacities between the external retrievers and the LLMs. Further, optimizing the selection of examples is a non-trivial task, since there are no straightforward methods to assess the relative benefits of examples without performing pairwise inference. To address these shortcomings, we propose Detriever, a novel demonstration retrieval framework that learns a weighted combination of LLM hidden states, where rich semantic information is encoded. To train the model, we propose a proxy score that estimates the relative benefits of examples based on the similarities between output queries. Experiments on two popular NL2SQL benchmarks demonstrate that our method significantly outperforms the state-of-the-art baselines for the NL2SQL tasks.",
        "author": "Raymond Li; Yuxi Feng; Zhenan Fan; Giuseppe Carenini; Weiwei Zhang; Mohammadreza Pourreza; Yong Zhang",
        "authorids": "/r/raymond-li/; /y/yuxi-feng/; /z/zhenan-fan/; /g/giuseppe-carenini/; /w/weiwei-zhang/; /m/mohammadreza-pourreza/; /y/yong-zhang/",
        "bibtex": "@inproceedings{li-etal-2025-detriever,\n    title = \"{D}e{T}riever: Decoder-representation-based Retriever for Improving {NL}2{SQL} In-Context Learning\",\n    author = \"Li, Raymond  and\n      Feng, Yuxi  and\n      Fan, Zhenan  and\n      Carenini, Giuseppe  and\n      Zhang, Weiwei  and\n      Pourreza, Mohammadreza  and\n      Zhang, Yong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.544/\",\n    pages = \"8173--8183\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.544.pdf",
        "site": "https://aclanthology.org/2025.coling-main.544/",
        "pdf_size": 1227826,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4138318348738515991&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.314",
        "title": "Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Writing arguments is a challenging task for both humans and machines. It entails incorporating high-level beliefs from various perspectives on the topic, along with deliberate reasoning and planning to construct a coherent narrative. Current language models often generate outputs autoregressively, lacking explicit integration of these underlying controls, resulting in limited output diversity and coherence. In this work, we propose a persona-based multi-agent framework for argument writing. Inspired by the human debate, we first assign each agent a persona representing its high-level beliefs from a unique perspective, and then design an agent interaction process so that the agents can collaboratively debate and discuss the idea to form an overall plan for argument writing. Such debate process enables fluid and nonlinear development of ideas. We evaluate our framework on argumentative essay writing. The results show that our framework generates more diverse and persuasive arguments by both automatic and human evaluations.",
        "author": "Zhe Hu; Hou Pong Chan; Jing Li; Yu Yin",
        "authorids": "/z/zhe-hu/; /h/hou-pong-chan/; /j/jing-li/; /y/yu-yin/",
        "bibtex": "@inproceedings{hu-etal-2025-debate,\n    title = \"Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation\",\n    author = \"Hu, Zhe  and\n      Chan, Hou Pong  and\n      Li, Jing  and\n      Yin, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.314/\",\n    pages = \"4689--4703\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.314.pdf",
        "site": "https://aclanthology.org/2025.coling-main.314/",
        "pdf_size": 1309726,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8567474044487419209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computing, The Hong Kong Polytechnic University; DAMO Academy, Alibaba Group; Department of Computing, The Hong Kong Polytechnic University + Research Centre for Data Science & Artificial Intelligence; Department of Computer and Data Sciences, Case Western Reserve University",
        "aff_domain": "connect.polyu.hk;alibaba-inc.com;polyu.edu.hk;case.edu",
        "email": "connect.polyu.hk;alibaba-inc.com;polyu.edu.hk;case.edu",
        "github": "https://github.com/Derekkk/LLM4ArgGen",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0+2;3",
        "aff_unique_norm": "The Hong Kong Polytechnic University;Alibaba Group;Research Centre for Data Science & Artificial Intelligence;Case Western Reserve University",
        "aff_unique_dep": "Department of Computing;DAMO Academy;Data Science & Artificial Intelligence;Department of Computer and Data Sciences",
        "aff_unique_url": "https://www.polyu.edu.hk;https://www.alibaba-group.com;;https://www.case.edu",
        "aff_unique_abbr": "PolyU;Alibaba;;CWRU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;2",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2025.coling-main.42",
        "title": "Debiasing by obfuscating with 007-classifiers promotes fairness in multi-community settings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While there has been considerable amount of research on bias mitigation algorithms, two properties: multi-community perspective and fairness to *all* communities have not been given sufficient attention. Focusing on these, we propose an obfuscation based data augmentation debiasing approach. In it we add to the training data *obfuscated* versions of *all* false positive instances irrespective of source community. We test our approach by debiasing toxicity classifiers built using 5 neural models (multi layer perceptron model and masked language models) and 3 datasets in a 4 communities setting. We also explore 4 different obfuscators for debiasing. Results demonstrate the merits of our approach: bias is reduced for almost all of our runs without sacrificing false positive rates or F1 scores for minority or majority communities. In contrast, the 4 state of the art baselines typically make performance sacrifices (often large) while reducing bias. Crucially, we demonstrate that it is possible to debias while maintaining standards for both minority and majority communities.",
        "author": "Ingroj Shrestha; Padmini Srinivasan",
        "authorids": "/i/ingroj-shrestha/; /p/padmini-srinivasan/",
        "bibtex": "@inproceedings{shrestha-srinivasan-2025-debiasing,\n    title = \"Debiasing by obfuscating with 007-classifiers promotes fairness in multi-community settings\",\n    author = \"Shrestha, Ingroj  and\n      Srinivasan, Padmini\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.42/\",\n    pages = \"621--636\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.42.pdf",
        "site": "https://aclanthology.org/2025.coling-main.42/",
        "pdf_size": 1438128,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Umal07K9GdIJ:scholar.google.com/&scioq=Debiasing+by+obfuscating+with+007-classifiers+promotes+fairness+in+multi-community+settings&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "University of Iowa; University of Iowa",
        "aff_domain": "uiowa.edu;uiowa.edu",
        "email": "uiowa.edu;uiowa.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Iowa",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uiowa.edu",
        "aff_unique_abbr": "UIowa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.668",
        "title": "Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Decoding strategies for generative large language models (LLMs) are a critical but often underexplored aspect of text generation tasks. Guided by specific hyperparameters, these strategies aim to transform the raw probability distributions produced by language models into coherent, fluent text. In this study, we undertake a large-scale empirical assessment of a range of decoding methods, open-source LLMs, textual domains, and evaluation protocols to determine how hyperparameter choices shape the outputs. Our experiments include both factual (e.g., news) and creative (e.g., fiction) domains, and incorporate a broad suite of automatic evaluation metrics alongside human judgments. Through extensive sensitivity analyses, we distill practical recommendations for selecting and tuning hyperparameters, noting that optimal configurations vary across models and tasks. By synthesizing these insights, this study provides actionable guidance for refining decoding strategies, enabling researchers and practitioners to achieve higher-quality, more reliable, and context-appropriate text generation outcomes.",
        "author": "Esteban Garces Arias; Meimingwei Li; Christian Heumann; Matthias Assenmacher",
        "authorids": "/e/esteban-garces-arias/; /m/meimingwei-li/; /c/christian-heumann/; /m/matthias-assenmacher/",
        "bibtex": "@inproceedings{garces-arias-etal-2025-decoding,\n    title = \"Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation\",\n    author = \"Garces Arias, Esteban  and\n      Li, Meimingwei  and\n      Heumann, Christian  and\n      Assenmacher, Matthias\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.668/\",\n    pages = \"9992--10020\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.668.pdf",
        "site": "https://aclanthology.org/2025.coling-main.668/",
        "pdf_size": 1760740,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7419512650353184558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Statistics, LMU Munich + Munich Center for Machine Learning (MCML); Department of Statistics, LMU Munich; Department of Statistics, LMU Munich; Department of Statistics, LMU Munich + Munich Center for Machine Learning (MCML)",
        "aff_domain": "stat.uni-muenchen.de; ; ; ",
        "email": "stat.uni-muenchen.de; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "LMU Munich;Munich Center for Machine Learning",
        "aff_unique_dep": "Department of Statistics;Center for Machine Learning",
        "aff_unique_url": "https://www.lmu.de;https://www.munich-center-for-machine-learning.de",
        "aff_unique_abbr": "LMU;MCML",
        "aff_campus_unique_index": "0+0;0;0;0+0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.264",
        "title": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization in Social Networks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The impact of social media on critical issues such as echo chambers, needs to be addressed, as these phenomena can have disruptive consequences for our society. Traditional research often oversimplifies emotional tendencies and opinion evolution into numbers and formulas, neglecting that news and communication are conveyed through text, which limits these approaches. Hence, in this work, we propose an LLM-based simulation for the social opinion network to evaluate and counter polarization phenomena. We first construct three typical network structures to simulate different characteristics of social interactions. Then, agents interact based on recommendation algorithms and update their strategies through reasoning and analysis. By comparing these interactions with the classic Bounded Confidence Model (BCM), the Friedkin-Johnsen (FJ) model, and using echo chamber-related indices, we demonstrate the effectiveness of our framework in simulating opinion dynamics and reproducing phenomena such as opinion polarization and echo chambers. We propose two mitigation methods\u2014active and passive nudges\u2014that can help reduce echo chambers, specifically within language-based simulations. We hope our work will offer valuable insights and guidance for social polarization mitigation.",
        "author": "Chenxi Wang; Zongfang Liu; Dequan Yang; Xiuying Chen",
        "authorids": "/c/chenxi-wang/; /z/zongfang-liu/; /d/dequan-yang/; /x/xiuying-chen/",
        "bibtex": "@inproceedings{wang-etal-2025-decoding,\n    title = \"Decoding Echo Chambers: {LLM}-Powered Simulations Revealing Polarization in Social Networks\",\n    author = \"Wang, Chenxi  and\n      Liu, Zongfang  and\n      Yang, Dequan  and\n      Chen, Xiuying\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.264/\",\n    pages = \"3913--3923\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.264.pdf",
        "site": "https://aclanthology.org/2025.coling-main.264/",
        "pdf_size": 1746175,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2682926941996802807&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "https://github.com/ZongfangLiu/EchoChamberSim",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Arab Emirates"
    },
    {
        "id": "2025.coling-main.682",
        "title": "Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Theory of Mind (ToM) is the ability to understand and reflect on the mental states of others. Although this capability is crucial for human interaction, testing on Large Language Models (LLMs) reveals that they possess only a rudimentary understanding of it. Although the most capable closed-source LLMs have come close to human performance on some ToM tasks, they still perform poorly on complex variations of the task that involve more structured reasoning. In this work, we utilize the concept of \u201cpretend-play\u201d, or \u201cSimulation Theory\u201d from cognitive psychology to propose \u201cDecompose-ToM\u201d: an LLM-based inference algorithm that improves model performance on complex ToM tasks. We recursively simulate user perspectives and decompose the ToM task into a simpler set of tasks: subject identification, question-reframing, world model updation, and knowledge availability. We test the algorithm on higher-order ToM tasks and a task testing for ToM capabilities in a conversational setting, demonstrating that our approach shows significant improvement across models compared to baseline methods while requiring minimal prompt tuning across tasks and no additional model training. Our code is publicly available.",
        "author": "Sneheel Sarangi; Maha Elgarf; Hanan Salam",
        "authorids": "/s/sneheel-sarangi/; /m/maha-elgarf/; /h/hanan-salam/",
        "bibtex": "@inproceedings{sarangi-etal-2025-decompose,\n    title = \"Decompose-{T}o{M}: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition\",\n    author = \"Sarangi, Sneheel  and\n      Elgarf, Maha  and\n      Salam, Hanan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.682/\",\n    pages = \"10228--10241\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.682.pdf",
        "site": "https://aclanthology.org/2025.coling-main.682/",
        "pdf_size": 363179,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6371661154691488155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "NYU Abu Dhabi; NYU Abu Dhabi; NYU Abu Dhabi",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Abu Dhabi",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Arab Emirates"
    },
    {
        "id": "2025.coling-main.293",
        "title": "DefVerify: Do Hate Speech Models Reflect Their Dataset\u2019s Definition?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "When building a predictive model, it is often difficult to ensure that application-specific requirements are encoded by the model that will eventually be deployed. Consider researchers working on hate speech detection. They will have an idea of what is considered hate speech, but building a model that reflects their view accurately requires preserving those ideals throughout the workflow of data set construction and model training. Complications such as sampling bias, annotation bias, and model misspecification almost always arise, possibly resulting in a gap between the application specification and the model\u2019s actual behavior upon deployment. To address this issue for hate speech detection, we propose DefVerify: a 3-step procedure that (i) encodes a user-specified definition of hate speech, (ii) quantifies to what extent the model reflects the intended definition, and (iii) tries to identify the point of failure in the workflow. We use DefVerify to find gaps between definition and model behavior when applied to six popular hate speech benchmark datasets.",
        "author": "Urja Khurana; Eric Nalisnick; Antske Fokkens",
        "authorids": "/u/urja-khurana/; /e/eric-nalisnick/; /a/antske-fokkens/",
        "bibtex": "@inproceedings{khurana-etal-2025-defverify,\n    title = \"{D}ef{V}erify: Do Hate Speech Models Reflect Their Dataset{'}s Definition?\",\n    author = \"Khurana, Urja  and\n      Nalisnick, Eric  and\n      Fokkens, Antske\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.293/\",\n    pages = \"4341--4358\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.293.pdf",
        "site": "https://aclanthology.org/2025.coling-main.293/",
        "pdf_size": 4468464,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=609030529084575650&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Computational Linguistics and Text Mining Lab, Vrije Universiteit Amsterdam; Department of Computer Science, Johns Hopkins University; Computational Linguistics and Text Mining Lab, Vrije Universiteit Amsterdam",
        "aff_domain": "vu.nl;jhu.edu;vu.nl",
        "email": "vu.nl;jhu.edu;vu.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Vrije Universiteit Amsterdam;Johns Hopkins University",
        "aff_unique_dep": "Computational Linguistics and Text Mining Lab;Department of Computer Science",
        "aff_unique_url": "https://www.vu.nl;https://www.jhu.edu",
        "aff_unique_abbr": "VU Amsterdam;JHU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amsterdam;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "2025.coling-industry.41",
        "title": "Deploying Multi-task Online Server with Large Language Model",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In the industry, numerous tasks are deployed online. Traditional approaches often tackle each task separately by its own network, which leads to excessive costs for developing and scaling models, especially in the context of large language models. Although multi-task methods can save costs through parameter sharing, they often struggle to outperform single-task methods in real-world applications. To tackle these challenges, we present a three-stage multi-task learning framework for large language models. It involves task filtering, followed by fine-tuning on high-resource tasks, and finally fine-tuning on all tasks. We conducted comprehensive experiments in single-task and multi-task settings. Our approach, exemplified on different benchmarks, demonstrates that it is able to achieve performance comparable to the single-task method while reducing up to 90.9% of its overhead.",
        "author": "Yincen Qu; Hengyue Liu; Kun Wang; Xiangying Dai; Xiaoou Lu; Hui Zhou; Chao Ma",
        "authorids": "/y/yincen-qu/; /h/hengyue-liu/; /k/kun-wang/; /x/xiang-dai/; /x/xiaoou-lu/; /h/hui-zhou/; /c/chao-ma/",
        "bibtex": "@inproceedings{qu-etal-2025-deploying,\n    title = \"Deploying Multi-task Online Server with Large Language Model\",\n    author = \"Qu, Yincen  and\n      Liu, Hengyue  and\n      Wang, Kun  and\n      Dai, Xiangying  and\n      Lu, Xiaoou  and\n      Zhou, Hui  and\n      Ma, Chao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.41/\",\n    pages = \"483--495\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.41.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.41/",
        "pdf_size": 3371229,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:OeTvrqEe-DwJ:scholar.google.com/&scioq=Deploying+Multi-task+Online+Server+with+Large+Language+Model&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.616",
        "title": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Mental manipulation severely undermines mental wellness by covertly and negatively distorting decision-making. While there is an increasing interest in mental health care within the natural language processing community, progress in tackling manipulation remains limited due to the complexity of detecting subtle, covert tactics in conversations. In this paper, we propose Intent-Aware Prompting (IAP), a novel approach for detecting mental manipulations using large language models (LLMs), providing a deeper understanding of manipulative tactics by capturing the underlying intents of participants. Experimental results on the MentalManip dataset demonstrate superior effectiveness of IAP against other advanced prompting strategies. Notably, our approach substantially reduces false negatives, helping detect more instances of mental manipulation with minimal misjudgment of positive cases. The code of this paper is available at https://github.com/Anton-Jiayuan-MA/Manip-IAP.",
        "author": "Jiayuan Ma; Hongbin Na; Zimu Wang; Yining Hua; Yue Liu; Wei Wang; Ling Chen",
        "authorids": "/j/jiayuan-ma/; /h/hongbin-na/; /z/zimu-wang/; /y/yining-hua/; /y/yue-liu/; /w/wei-wang/; /l/ling-chen/",
        "bibtex": "@inproceedings{ma-etal-2025-detecting,\n    title = \"Detecting Conversational Mental Manipulation with Intent-Aware Prompting\",\n    author = \"Ma, Jiayuan  and\n      Na, Hongbin  and\n      Wang, Zimu  and\n      Hua, Yining  and\n      Liu, Yue  and\n      Wang, Wei  and\n      Chen, Ling\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.616/\",\n    pages = \"9176--9183\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.616.pdf",
        "site": "https://aclanthology.org/2025.coling-main.616/",
        "pdf_size": 453614,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9478065441981045683&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The University of Sydney; University of Technology Sydney; Xi\u2019an Jiaotong-Liverpool University; Harvard University; University of New South Wales; Xi\u2019an Jiaotong-Liverpool University; University of Technology Sydney",
        "aff_domain": "uni.sydney.edu.au;student.uts.edu.au;student.xjtlu.edu.cn;g.harvard.edu;ad.unsw.edu.au;xjtlu.edu.cn;uts.edu.au",
        "email": "uni.sydney.edu.au;student.uts.edu.au;student.xjtlu.edu.cn;g.harvard.edu;ad.unsw.edu.au;xjtlu.edu.cn;uts.edu.au",
        "github": "https://github.com/Anton-Jiayuan-MA/Manip-IAP",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;4;2;1",
        "aff_unique_norm": "University of Sydney;University of Technology Sydney;Xi'an Jiaotong-Liverpool University;Harvard University;University of New South Wales",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.sydney.edu.au;https://www.uts.edu.au;https://www.xjtu.edu.cn;https://www.harvard.edu;https://www.unsw.edu.au",
        "aff_unique_abbr": "USYD;UTS;XJTLU;Harvard;UNSW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0;1;0",
        "aff_country_unique": "Australia;China;United States"
    },
    {
        "id": "2025.coling-main.608",
        "title": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper focuses on sarcasm detection, which aims to identify whether given statements convey criticism, mockery, or other negative sentiment opposite to the literal meaning. To detect sarcasm, humans often require a comprehensive understanding of the semantics in the statement and even resort to external commonsense to infer the fine-grained incongruity. However, existing methods lack commonsense inferential ability when they face complex real-world scenarios, leading to unsatisfactory performance. To address this problem, we propose a novel framework for sarcasm detection, which conducts incongruity reasoning based on commonsense augmentation, called EICR. Concretely, we first employ retrieval-augmented large language models to supplement the missing but indispensable commonsense background knowledge. To capture complex contextual associations, we construct a dependency graph and obtain the optimized topology via graph refinement. We further introduce an adaptive reasoning skeleton that integrates prior rules to extract sentiment-inconsistent subgraphs explicitly. To eliminate the possible spurious relations between words and labels, we employ adversarial contrastive learning to enhance the robustness of the detector. Experiments conducted on five datasets demonstrate the effectiveness of EICR.",
        "author": "Ziqi Qiu; Jianxing Yu; Yufeng Zhang; Hanjiang Lai; Yanghui Rao; Qinliang Su; Jian Yin",
        "authorids": "/z/ziqi-qiu/; /j/jianxing-yu/; /y/yufeng-zhang/; /h/hanjiang-lai/; /y/yanghui-rao/; /q/qinliang-su/; /j/jian-yin/",
        "bibtex": "@inproceedings{qiu-etal-2025-detecting,\n    title = \"Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning\",\n    author = \"Qiu, Ziqi  and\n      Yu, Jianxing  and\n      Zhang, Yufeng  and\n      Lai, Hanjiang  and\n      Rao, Yanghui  and\n      Su, Qinliang  and\n      Yin, Jian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.608/\",\n    pages = \"9062--9073\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.608.pdf",
        "site": "https://aclanthology.org/2025.coling-main.608/",
        "pdf_size": 2026356,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17892202678781806802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China+School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, 510006, China+Key Laboratory of Sustainable Tourism Smart Assessment Technology, Ministry of Culture and Tourism+Pazhou Lab, Guangzhou, 510330, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China+School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, 510006, China+Key Laboratory of Sustainable Tourism Smart Assessment Technology, Ministry of Culture and Tourism+Pazhou Lab, Guangzhou, 510330, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, 510006, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, 510006, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China+School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, 510006, China+Key Laboratory of Sustainable Tourism Smart Assessment Technology, Ministry of Culture and Tourism+Pazhou Lab, Guangzhou, 510330, China",
        "aff_domain": "mail2.sysu.edu.cn;mail.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0+1+2;0+0+1+2;0;0;0;0;0+0+1+2",
        "aff_unique_norm": "Sun Yat-sen University;Ministry of Culture and Tourism;Pazhou Lab",
        "aff_unique_dep": "School of Artificial Intelligence;Key Laboratory of Sustainable Tourism Smart Assessment Technology;",
        "aff_unique_url": "http://www.sysu.edu.cn;;",
        "aff_unique_abbr": "SYSU;;",
        "aff_campus_unique_index": "0+1+1;0+1+1;0;1;0;1;0+1+1",
        "aff_campus_unique": "Zhuhai;Guangzhou;",
        "aff_country_unique_index": "0+0+0+0;0+0+0+0;0;0;0;0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.564",
        "title": "Detecting deepfakes and false ads through analysis of text and social engineering techniques",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing deepfake detection algorithm frequently fail to successfully identify fabricated materials. These algorithms primarily focus on technical analysis of video and audio, often neglecting the meaning of content itself. In this paper, we introduce a novel approach that emphasizes the analysis of text-based transcripts, particularly those from AI-generated deepfake advertisements, placing the text content at the center of attention. Our method combines linguistic features, evaluation of grammatical mistakes, and the identification of social engineering techniques commonly used in fraudulent content. By examining stylistic inconsistencies and manipulative language patterns, we enhance the accuracy of distinguishing between real and deepfake materials. To ensure interpretability, we employed classical machine learning models, allowing us to provide explainable insights into decision-making processes. Additionally, zero-shot evaluations were conducted using three large language model based solutions to assess their performance in detecting deepfake content. The experimental results show that these factors yield a 90% accuracy in distinguishing between deepfake-based fraudulent advertisements and real ones. This demonstrates the effectiveness of incorporating content-based analysis into deepfake detection, offering a complementary layer to existing audio-visual techniques.",
        "author": "Alicja Martinek; Ewelina Bartuzi-Trokielewicz",
        "authorids": "/a/alicja-martinek/; /e/ewelina-bartuzi-trokielewicz/",
        "bibtex": "@inproceedings{martinek-bartuzi-trokielewicz-2025-detecting,\n    title = \"Detecting deepfakes and false ads through analysis of text and social engineering techniques\",\n    author = \"Martinek, Alicja  and\n      Bartuzi-Trokielewicz, Ewelina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.564/\",\n    pages = \"8432--8448\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.564.pdf",
        "site": "https://aclanthology.org/2025.coling-main.564/",
        "pdf_size": 469799,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_tanirbpjL4J:scholar.google.com/&scioq=Detecting+deepfakes+and+false+ads+through+analysis+of+text+and+social+engineering+techniques&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "NASK National Research Institute + AGH University of Krak\u00f3w; NASK National Research Institute + Warsaw University of Technology",
        "aff_domain": "nask.pl;nask.pl",
        "email": "nask.pl;nask.pl",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+2",
        "aff_unique_norm": "NASK National Research Institute;AGH University of Science and Technology;Warsaw University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nask.pl;https://www.agh.edu.pl;https://www.pw.edu.pl",
        "aff_unique_abbr": "NASK;AGH;WUT",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Krak\u00f3w",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Poland"
    },
    {
        "id": "2025.coling-main.666",
        "title": "Development of Numerical Error Detection Tasks to Analyze the Numerical Capabilities of Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Numbers are used to describe quantities in various scenarios in daily life; therefore, numerical errors can significantly affect the meaning of the entire sentence, and even a single-letter error can be fatal. Detecting numerical errors often requires a high level of commonsense and is difficult even with the recent large language models (LLMs). In this study, we create a benchmark dataset of numerical error detection that uses automatically generated numerical errors. In our analysis, we classify the numerical errors based on the properties of the errors and investigate the ability of the model from several perspectives, including the error class, error size, and passage domain. The experimental results indicate that GPT-3.5, GPT-4, and Llama-3-Instruct (8B) perform well in the numerical error detection task; however, they are not as accurate as humans. We find that the LLMs misidentified correct numbers as errors more frequently than the humans did. In particular, the analysis demonstrates that the current LLMs still need improvement for detecting numerical errors requiring calculations or extensive prior knowledge.",
        "author": "Taku Sakamoto; Saku Sugawara; Akiko Aizawa",
        "authorids": "/t/taku-sakamoto/; /s/saku-sugawara/; /a/akiko-aizawa/",
        "bibtex": "@inproceedings{sakamoto-etal-2025-development,\n    title = \"Development of Numerical Error Detection Tasks to Analyze the Numerical Capabilities of Language Models\",\n    author = \"Sakamoto, Taku  and\n      Sugawara, Saku  and\n      Aizawa, Akiko\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.666/\",\n    pages = \"9957--9976\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.666.pdf",
        "site": "https://aclanthology.org/2025.coling-main.666/",
        "pdf_size": 1496797,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11842799464375440118&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "The University of Tokyo; National Institute of Informatics; National Institute of Informatics",
        "aff_domain": "nii.ac.jp;nii.ac.jp;nii.ac.jp",
        "email": "nii.ac.jp;nii.ac.jp;nii.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Tokyo;National Institute of Informatics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.nii.ac.jp/",
        "aff_unique_abbr": "UTokyo;NII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.481",
        "title": "Dialectal and Low Resource Machine Translation for Aromanian",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a neural machine translation system that can translate between Romanian, English, and Aromanian (an endangered Eastern Romance language); the first of its kind. BLEU scores range from 17 to 32 depending on the direction and genre of the text. Alongside, we release the biggest known Aromanian-Romanian bilingual corpus, consisting of 80k cleaned sentence pairs. Additional tools such as an agnostic sentence embedder (used for both text mining and automatic evaluation) and a diacritics converter are also presented. Lastly, we describe the online deployment of our quantized model, considering a CPU-driven limited resource scenario.",
        "author": "Alexandru-Iulius Jerpelea; Alina Radoi; Sergiu Nisioi",
        "authorids": "/a/alexandru-iulius-jerpelea/; /a/alina-radoi/; /s/sergiu-nisioi/",
        "bibtex": "@inproceedings{jerpelea-etal-2025-dialectal,\n    title = \"Dialectal and Low Resource Machine Translation for {A}romanian\",\n    author = \"Jerpelea, Alexandru-Iulius  and\n      Radoi, Alina  and\n      Nisioi, Sergiu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.481/\",\n    pages = \"7209--7228\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.481.pdf",
        "site": "https://aclanthology.org/2025.coling-main.481/",
        "pdf_size": 826349,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15451261008016580128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tudor Vianu National College of Computer Science, Bucharest; West University of Timisoara; Human Language Technologies Research Center, University of Bucharest",
        "aff_domain": "gmail.com;e-uvt.ro;unibuc.ro",
        "email": "gmail.com;e-uvt.ro;unibuc.ro",
        "github": "",
        "project": "https://arotranslate.com",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Tudor Vianu National College of Computer Science;West University of Timisoara;University of Bucharest",
        "aff_unique_dep": "Computer Science;;Human Language Technologies Research Center",
        "aff_unique_url": ";https://www.uvt.ro;https://www.unibuc.ro",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bucharest;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Romania"
    },
    {
        "id": "2025.coling-main.170",
        "title": "DialogueMMT: Dialogue Scenes Understanding Enhanced Multi-modal Multi-task Tuning for Emotion Recognition in Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Emotion recognition in conversations (ERC) has garnered significant attention from the research community. However, due to the complexity of visual scenes and dialogue contextual dependencies in conversations, previous ERC methods fail to handle emotional cues from both visual sources and discourse structures. Furthermore, existing state-of-the-art ERC models are trained and tested separately on each single ERC dataset, not verifying their effectiveness across multiple datasets simultaneously. To address these challenges, this paper proposes an innovative framework for ERC, called Dialogue Scenes Understanding Enhanced Multi-modal Multi-task Tuning (DialogueMMT). More concretely, a novel video-language connector is applied within the large vision-language model for capturing video features effectively. Additionally, we utilize multi-task instruction tuning with a unified ERC dataset to enhance the model\u2019s understanding of multi-modal dialogue scenes and employ a chain-of-thought strategy to improve emotion classification performance. Extensive experimental results on three benchmark ERC datasets indicate that the proposed DialogueMMT framework consistently outperforms existing state-of-the-art approaches in terms of overall performance.",
        "author": "ChenYuan He; Senbin Zhu; Hongde Liu; Fei Gao; Yuxiang Jia; Hongying Zan; Min Peng",
        "authorids": "/c/chenyuan-he/; /s/senbin-zhu/; /h/hongde-liu/; /f/fei-gao/; /y/yuxiang-jia/; /h/hongying-zan/; /m/min-peng/",
        "bibtex": "@inproceedings{he-etal-2025-dialoguemmt,\n    title = \"{D}ialogue{MMT}: Dialogue Scenes Understanding Enhanced Multi-modal Multi-task Tuning for Emotion Recognition in Conversations\",\n    author = \"He, ChenYuan  and\n      Zhu, Senbin  and\n      Liu, Hongde  and\n      Gao, Fei  and\n      Jia, Yuxiang  and\n      Zan, Hongying  and\n      Peng, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.170/\",\n    pages = \"2497--2512\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.170.pdf",
        "site": "https://aclanthology.org/2025.coling-main.170/",
        "pdf_size": 4029507,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Pe0V4yYoQFsJ:scholar.google.com/&scioq=DialogueMMT:+Dialogue+Scenes+Understanding+Enhanced+Multi-modal+Multi-task+Tuning+for+Emotion+Recognition+in+Conversations&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer Science, Wuhan University, China",
        "aff_domain": "gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn;zzu.edu.cn;zzu.edu.cn;whu.edu.cn",
        "email": "gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn;zzu.edu.cn;zzu.edu.cn;whu.edu.cn",
        "github": "https://github.com/he2720/DialogueMMT",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Zhengzhou University;Wuhan University",
        "aff_unique_dep": "School of Computer and Artificial Intelligence;School of Computer Science",
        "aff_unique_url": "http://www.zzu.edu.cn;http://www.whu.edu.cn",
        "aff_unique_abbr": ";WHU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.352",
        "title": "DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for Text-to-Speech with Diverse and Controllable Styles",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Human speech exhibits rich and flexible prosodic variations. To address the one-to-many mapping problem from text to prosody in a reasonable and flexible manner, we propose DiffStyleTTS, a multi-speaker acoustic model based on a conditional diffusion module and an improved classifier-free guidance, which hierarchically models speech prosodic features, and controls different prosodic styles to guide prosody prediction. Experiments show that our method outperforms all baselines in naturalness and achieves superior synthesis speed compared to three diffusion-based baselines. Additionally, by adjusting the guiding scale, DiffStyleTTS effectively controls the guidance intensity of the synthetic prosody.",
        "author": "Jiaxuan Liu; Zhaoci Liu; Yajun Hu; Yingying Gao; Shilei Zhang; Zhenhua Ling",
        "authorids": "/j/jiaxuan-liu/; /z/zhaoci-liu/; /y/yajun-hu/; /y/yingying-gao/; /s/shilei-zhang/; /z/zhenhua-ling/",
        "bibtex": "@inproceedings{liu-etal-2025-diffstyletts,\n    title = \"{D}iff{S}tyle{TTS}: Diffusion-based Hierarchical Prosody Modeling for Text-to-Speech with Diverse and Controllable Styles\",\n    author = \"Liu, Jiaxuan  and\n      Liu, Zhaoci  and\n      Hu, Yajun  and\n      Gao, Yingying  and\n      Zhang, Shilei  and\n      Ling, Zhenhua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.352/\",\n    pages = \"5265--5272\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.352.pdf",
        "site": "https://aclanthology.org/2025.coling-main.352/",
        "pdf_size": 2922936,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6896496115498303931&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "NERCSLIP, University of Science and Technology of China, Hefei, China; NERCSLIP, University of Science and Technology of China, Hefei, China; iFLYTEK CO.LTD., Hefei, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; NERCSLIP, University of Science and Technology of China, Hefei, China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;iflytek.com;chinamobile.com;chinamobile.com;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;iflytek.com;chinamobile.com;chinamobile.com;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;2;0",
        "aff_unique_norm": "University of Science and Technology of China;iFLYTEK;China Mobile Research Institute",
        "aff_unique_dep": "NERCSLIP;;",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.iflytek.com;http://www.chinamobile.com/en/CMRI",
        "aff_unique_abbr": "USTC;iFLYTEK;CMRI",
        "aff_campus_unique_index": "0;0;2;2;0",
        "aff_campus_unique": "Hefei;;Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.240",
        "title": "Discarding the Crutches: Adaptive Parameter-Efficient Expert Meta-Learning for Continual Semantic Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Continual Semantic Parsing (CSP) enables parsers to generate SQL from natural language questions in task streams, using minimal annotated data to handle dynamically evolving databases in real-world scenarios. Previous works often rely on replaying historical data, which poses privacy concerns. Recently, replay-free continual learning methods based on Parameter-Efficient Tuning (PET) have gained widespread attention. However, they often rely on ideal settings and initial task data, sacrificing the model\u2019s generalization ability, which limits their applicability in real-world scenarios. To address this, we propose a novel Adaptive PET eXpert meta-learning (APEX) approach for CSP. First, SQL syntax guides the LLM to assist experts in adaptively warming up, ensuring better model initialization. Then, a dynamically expanding expert pool stores knowledge and explores the relationship between experts and instances. Finally, a selection/fusion inference strategy based on sample historical visibility promotes expert collaboration. Experiments on two CSP benchmarks show that our method achieves superior performance without data replay or ideal settings, effectively handling cold start scenarios and generalizing to unseen tasks, even surpassing performance upper bounds.",
        "author": "Ruiheng Liu; Jinyu Zhang; Yanqi Song; Yu Zhang; Bailong Yang",
        "authorids": "/r/ruiheng-liu/; /j/jinyu-zhang/; /y/yanqi-song/; /y/yu-zhang/; /b/bailong-yang/",
        "bibtex": "@inproceedings{liu-etal-2025-discarding,\n    title = \"Discarding the Crutches: Adaptive Parameter-Efficient Expert Meta-Learning for Continual Semantic Parsing\",\n    author = \"Liu, Ruiheng  and\n      Zhang, Jinyu  and\n      Song, Yanqi  and\n      Zhang, Yu  and\n      Yang, Bailong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.240/\",\n    pages = \"3560--3578\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.240.pdf",
        "site": "https://aclanthology.org/2025.coling-main.240/",
        "pdf_size": 2912240,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2822118747494455315&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Xi\u2019an Research Institute of High-Tech, Xi\u2019an, China+Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China; Xi\u2019an Research Institute of High-Tech, Xi\u2019an, China+Harbin Institute of Technology, Harbin, China",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;163.com",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;163.com",
        "github": "https://github.com/tom68-ll/APEX",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "1;1;1;1;1",
        "aff_unique_norm": ";Harbin Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": ";http://www.hit.edu.cn/",
        "aff_unique_abbr": ";HIT",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Harbin",
        "aff_country_unique_index": "1;1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2025.coling-main.167",
        "title": "Discrete Subgraph Sampling for Interpretable Graph based Visual Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Explainable artificial intelligence (XAI) aims to make machine learning models more transparent. While many approaches focus on generating explanations post-hoc, interpretable approaches, which generate the explanations intrinsically alongside the predictions, are relatively rare. In this work, we integrate different discrete subset sampling methods into a graph-based visual question answering system to compare their effectiveness in generating interpretable explanatory subgraphs intrinsically. We evaluate the methods on the dataset and show that the integrated methods effectively mitigate the performance trade-off between interpretability and answer accuracy, while also achieving strong co-occurrences between answer and question tokens. Furthermore, we conduct a human evaluation to assess the interpretability of the generated subgraphs using a comparative setting with the extended Bradley-Terry model, showing that the answer and question token co-occurrence metrics strongly correlate with human preferences. Our source code is publicly available.",
        "author": "Pascal Tilli; Ngoc Thang Vu",
        "authorids": "/p/pascal-tilli/; /n/ngoc-thang-vu/",
        "bibtex": "@inproceedings{tilli-vu-2025-discrete,\n    title = \"Discrete Subgraph Sampling for Interpretable Graph based Visual Question Answering\",\n    author = \"Tilli, Pascal  and\n      Vu, Ngoc Thang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.167/\",\n    pages = \"2445--2455\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.167.pdf",
        "site": "https://aclanthology.org/2025.coling-main.167/",
        "pdf_size": 2873033,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nzW39fyTPOUJ:scholar.google.com/&scioq=Discrete+Subgraph+Sampling+for+Interpretable+Graph+based+Visual+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Stuttgart; University of Stuttgart",
        "aff_domain": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "email": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "github": "https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-stuttgart.de",
        "aff_unique_abbr": "USTuttgart",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.660",
        "title": "Disentangle to Decay: Linear Attention with Trainable Decay Factor",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Linear attention enhances inference efficiency of Transformer and has attracted research interests as an efficient backbone of language models. Existing linear attention based models usually exploit decay factor based positional encoding (PE), where attention scores decay exponentially with increasing relative distance. However, most work manually designs a non-trainable decay factor of exponential calculation, which limits further optimization. Our analysis reveals directly training decay factor is unstable because of large gradients. To address this, we propose a novel PE for linear attention named Disentangle to Decay (D2D). D2D disentangles decay factor into two parts to achieve further optimization and stable training. Moreover, D2D can be transformed into recurrent form for efficient inference. Experiments demonstrate that D2D achieves stable training of decay factor, and enhances performance of linear attention in both normal context length and length extrapolation scenarios.",
        "author": "Haibo Tong; Chenyang Zhang; Jiayi Lin; Bingxuan Hou; Qingqing Hong; Junli Wang",
        "authorids": "/h/haibo-tong/; /c/chenyang-zhang/; /j/jiayi-lin/; /b/bingxuan-hou/; /q/qingqing-hong/; /j/junli-wang/",
        "bibtex": "@inproceedings{tong-etal-2025-disentangle,\n    title = \"Disentangle to Decay: Linear Attention with Trainable Decay Factor\",\n    author = \"Tong, Haibo  and\n      Zhang, Chenyang  and\n      Lin, Jiayi  and\n      Hou, Bingxuan  and\n      Hong, Qingqing  and\n      Wang, Junli\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.660/\",\n    pages = \"9877--9890\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.660.pdf",
        "site": "https://aclanthology.org/2025.coling-main.660/",
        "pdf_size": 871473,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pE-Kc_FVeHUJ:scholar.google.com/&scioq=Disentangle+to+Decay:+Linear+Attention+with+Trainable+Decay+Factor&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Key Laboratory of Embedded System and Service Computing (Tongji University), Ministry of Education, Shanghai 201804, China + National (Province-Ministry Joint) Collaborative Innovation Center for Financial Network Security, Tongji University, Shanghai 201804, China; Key Laboratory of Embedded System and Service Computing (Tongji University), Ministry of Education, Shanghai 201804, China + National (Province-Ministry Joint) Collaborative Innovation Center for Financial Network Security, Tongji University, Shanghai 201804, China; Key Laboratory of Embedded System and Service Computing (Tongji University), Ministry of Education, Shanghai 201804, China + National (Province-Ministry Joint) Collaborative Innovation Center for Financial Network Security, Tongji University, Shanghai 201804, China; Key Laboratory of Embedded System and Service Computing (Tongji University), Ministry of Education, Shanghai 201804, China + National (Province-Ministry Joint) Collaborative Innovation Center for Financial Network Security, Tongji University, Shanghai 201804, China; Key Laboratory of Embedded System and Service Computing (Tongji University), Ministry of Education, Shanghai 201804, China + National (Province-Ministry Joint) Collaborative Innovation Center for Financial Network Security, Tongji University, Shanghai 201804, China; Key Laboratory of Embedded System and Service Computing (Tongji University), Ministry of Education, Shanghai 201804, China + National (Province-Ministry Joint) Collaborative Innovation Center for Financial Network Security, Tongji University, Shanghai 201804, China",
        "aff_domain": "tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn",
        "email": "tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn",
        "github": "https://github.com/TongjiNLP/Disentangle-to-Decay-Linear-Attention-with-Trainable-Decay-Factorels",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Tongji University",
        "aff_unique_dep": "Key Laboratory of Embedded System and Service Computing",
        "aff_unique_url": "http://www.tongji.edu.cn",
        "aff_unique_abbr": "Tongji",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.323",
        "title": "Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aligning Large Language Models (LLMs) with general human preferences has been proved crucial in improving the interaction quality between LLMs and human. However, human values are inherently diverse among different individuals, making it insufficient to align LLMs solely with general preferences. To address this, personalizing LLMs according to individual feedback emerges as a promising solution. Nonetheless, this approach presents challenges in terms of the efficiency of alignment algorithms. In this work, we introduce a flexible paradigm for individual preference alignment. Our method fundamentally improves efficiency by disentangling preference representation from text generation in LLMs. We validate our approach across multiple text generation tasks and demonstrate that it can produce aligned quality as well as or better than PEFT-based methods, while reducing additional training time for each new individual preference by 80% to 90% in comparison with them.",
        "author": "Jianfei Zhang; Jun Bai; Bei Li; Yanmeng Wang; Rumei Li; Chenghua Lin; Wenge Rong",
        "authorids": "/j/jianfei-zhang/; /j/jun-bai/; /b/bei-li/; /y/yanmeng-wang/; /r/rumei-li/; /c/chenghua-lin/; /w/wenge-rong/",
        "bibtex": "@inproceedings{zhang-etal-2025-disentangling,\n    title = \"Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment\",\n    author = \"Zhang, Jianfei  and\n      Bai, Jun  and\n      Li, Bei  and\n      Wang, Yanmeng  and\n      Li, Rumei  and\n      Lin, Chenghua  and\n      Rong, Wenge\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.323/\",\n    pages = \"4813--4839\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.323.pdf",
        "site": "https://aclanthology.org/2025.coling-main.323/",
        "pdf_size": 10072709,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Jf-FbcOlmiYJ:scholar.google.com/&scioq=Disentangling+Preference+Representation+and+Text+Generation+for+Efficient+Individual+Preference+Alignment&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Beihang University; Beijing Institute for GAI; Meituan Inc.; Ping An Technology; Beihang University; University of Manchester; Beihang University",
        "aff_domain": "buaa.edu.cn;bigai.ai;meituan.com;pingan.com.cn;buaa.edu.cn;manchester.ac.uk;buaa.edu.cn",
        "email": "buaa.edu.cn;bigai.ai;meituan.com;pingan.com.cn;buaa.edu.cn;manchester.ac.uk;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;0;4;0",
        "aff_unique_norm": "Beihang University;Beijing Institute for General Artificial Intelligence;Meituan Inc.;Ping An Technology;University of Manchester",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "http://www.buaa.edu.cn/;http://www.bgai.cn;https://www.meituan.com;https://www.pingan.com.cn;https://www.manchester.ac.uk",
        "aff_unique_abbr": "BUAA;BIGAI;Meituan;Ping An;UoM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2025.coling-main.284",
        "title": "Distance-Adaptive Quaternion Knowledge Graph Embedding with Bidirectional Rotation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Quaternion contains one real part and three imaginary parts, which provided a more expressive hypercomplex space for learning knowledge graph. Existing quaternion embedding models measure the plausibility of a triplet either through semantic matching or distance scoring functions. However, it appears that semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities. To address this issue, we propose a novel quaternion knowledge graph embedding model. Our model combines semantic matching with entity\u2019s geometric distance to better measure the plausibility of triplets. Specifically, in the quaternion space, we perform a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features. Then, we utilize distance adaptive translations to learn the geometric distance between entities. Furthermore, we provide mathematical proofs to demonstrate our model can handle complex logical relationships. Extensive experimental results and analyses show our model significantly outperforms previous models on well-known knowledge graph completion benchmark datasets. Our code is available at https://anonymous.4open.science/r/l2730.",
        "author": "Weihua Wang; Qiuyu Liang; Feilong Bao; Guanglai Gao",
        "authorids": "/w/weihua-wang/; /q/qiuyu-liang/; /f/feilong-bao/; /g/guanglai-gao/",
        "bibtex": "@inproceedings{wang-etal-2025-distance,\n    title = \"Distance-Adaptive Quaternion Knowledge Graph Embedding with Bidirectional Rotation\",\n    author = \"Wang, Weihua  and\n      Liang, Qiuyu  and\n      Bao, Feilong  and\n      Gao, Guanglai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.284/\",\n    pages = \"4219--4231\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.284.pdf",
        "site": "https://aclanthology.org/2025.coling-main.284/",
        "pdf_size": 2228194,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4908065164157718919&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "1College of Computer Science, Inner Mongolia University, Hohhot, China+2National and Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China; 1College of Computer Science, Inner Mongolia University, Hohhot, China; 1College of Computer Science, Inner Mongolia University, Hohhot, China+2National and Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China; 1College of Computer Science, Inner Mongolia University, Hohhot, China+2National and Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian, Hohhot, China+3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China",
        "aff_domain": "imu.edu.cn; ; ; ",
        "email": "imu.edu.cn; ; ; ",
        "github": "https://github.com/llqy123/DaBR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0;0+1+2;0+1+2",
        "aff_unique_norm": "Inner Mongolia University;National and Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian;Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology",
        "aff_unique_dep": "College of Computer Science;;Multilingual Artificial Intelligence Technology",
        "aff_unique_url": "http://www.imu.edu.cn;;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "0+0;0;0+0;0+0",
        "aff_campus_unique": "Hohhot;",
        "aff_country_unique_index": "0+0+0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.61",
        "title": "Distilling Rule-based Knowledge into Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current paradigm of knowledge learning for LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, this learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can learn new tasks or grasp new knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which targets on encoding rule-based knowledge into LLMs. We further propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textual rules, and then explicitly encode the knowledge into the parameters of LLMs by learning from the above in-context signals produced inside the model. Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability. Warning: This paper may contain examples with offensive content.",
        "author": "Wenkai Yang; Yankai Lin; Jie Zhou; Ji-Rong Wen",
        "authorids": "/w/wenkai-yang/; /y/yankai-lin/; /j/jie-zhou/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{yang-etal-2025-distilling,\n    title = \"Distilling Rule-based Knowledge into Large Language Models\",\n    author = \"Yang, Wenkai  and\n      Lin, Yankai  and\n      Zhou, Jie  and\n      Wen, Ji-Rong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.61/\",\n    pages = \"913--932\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.61.pdf",
        "site": "https://aclanthology.org/2025.coling-main.61/",
        "pdf_size": 956939,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17591530331139070423&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Pattern Recognition Center, WeChat AI, Tencent Inc.; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn; ;ruc.edu.cn",
        "email": "ruc.edu.cn;ruc.edu.cn; ;ruc.edu.cn",
        "github": "https://github.com/RUCBM/rule-distillation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Renmin University of China;Tencent Inc.",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "RUC;Tencent",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.156",
        "title": "DnA-Eval: Enhancing Large Language Model Evaluation through Decomposition and Aggregation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The acceleration of Large Language Models (LLMs) research has opened up new possibilities for evaluating generated text. Though LLMs serve as scalable and economical evaluators, how reliable these evaluators is still under-explored. Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting of an LLM to a single use to obtain a final evaluation decision. They then compute the agreement between LLMs\u2019 outputs and human labels. This lacks interpretability in understanding the evaluation capability of LLMs. In light of this challenge, we propose DnA-Eval, which breaks down the evaluation process into decomposition and aggregation stages based on pedagogical practices. Our experiments show that it not only provides a more interpretable window for how well LLMs evaluate, but also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation benchmarks.",
        "author": "Minzhi Li; Zhengyuan Liu; Shumin Deng; Shafiq Joty; Nancy Chen; Min-Yen Kan",
        "authorids": "/m/minzhi-li/; /z/zhengyuan-liu/; /s/shumin-deng/; /s/shafiq-joty/; /n/nancy-chen/; /m/min-yen-kan/",
        "bibtex": "@inproceedings{li-etal-2025-dna,\n    title = \"{D}n{A}-Eval: Enhancing Large Language Model Evaluation through Decomposition and Aggregation\",\n    author = \"Li, Minzhi  and\n      Liu, Zhengyuan  and\n      Deng, Shumin  and\n      Joty, Shafiq  and\n      Chen, Nancy  and\n      Kan, Min-Yen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.156/\",\n    pages = \"2277--2290\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.156.pdf",
        "site": "https://aclanthology.org/2025.coling-main.156/",
        "pdf_size": 3434924,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16436863970653554092&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.659",
        "title": "Do Current Video LLMs Have Strong OCR Abilities? A Preliminary Study",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rise of multi-modal large language models, accurately extracting and understanding textual information from video content\u2014referred to as video-based optical character recognition (Video OCR)\u2014has become a crucial capability. This paper introduces a novel benchmark designed to evaluate the video OCR performance of multi-modal models in videos. Comprising 1,028 videos and 2,961 question-answer pairs, this benchmark proposes several key challenges through 6 distinct sub-tasks: (1) Recognition of text content itself and its basic visual attributes, (2) Semantic and Spatial Comprehension of OCR objects in videos (3) Dynamic Motion detection and Temporal Localization. We developed this benchmark using a semi-automated approach that integrates the OCR ability of image LLMs with manual refinement, balancing efficiency, cost, and data quality. Our resource aims to help advance research in video LLMs and underscores the need for improving OCR ability for video LLMs. The benchmark will be released on https://github.com/YuHuiGao/FG-Bench.git.",
        "author": "Yulin Fei; Yuhui Gao; Xingyuan Xian; Xiaojin Zhang; Tao Wu; Wei Chen",
        "authorids": "/y/yulin-fei/; /y/yuhui-gao/; /x/xingyuan-xian/; /x/xiaojin-zhang/; /t/tao-wu/; /w/wei-chen/",
        "bibtex": "@inproceedings{fei-etal-2025-current,\n    title = \"Do Current Video {LLM}s Have Strong {OCR} Abilities? A Preliminary Study\",\n    author = \"Fei, Yulin  and\n      Gao, Yuhui  and\n      Xian, Xingyuan  and\n      Zhang, Xiaojin  and\n      Wu, Tao  and\n      Chen, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.659/\",\n    pages = \"9860--9876\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.659.pdf",
        "site": "https://aclanthology.org/2025.coling-main.659/",
        "pdf_size": 5322712,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13050333068791609977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Software Engineering, Huazhong University of Science and Technology+Information Hub, Hongkong University of Science and Technology (Guangzhou); School of Software Engineering, Huazhong University of Science and Technology+Information Hub, Hongkong University of Science and Technology (Guangzhou); Information Hub, Hongkong University of Science and Technology (Guangzhou); School of Software Engineering, Huazhong University of Science and Technology; School of Software Engineering, Huazhong University of Science and Technology; School of Software Engineering, Huazhong University of Science and Technology",
        "aff_domain": "hust.edu.cn; ; ; ; ;hust.edu.cn",
        "email": "hust.edu.cn; ; ; ; ;hust.edu.cn",
        "github": "https://github.com/YuHuiGao/FG-Bench.git",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;1;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology;Hong Kong University of Science and Technology",
        "aff_unique_dep": "School of Software Engineering;Information Hub",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.ust.hk",
        "aff_unique_abbr": "HUST;HKUST",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Guangzhou",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.627",
        "title": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Abstention Ability (AA) is a critical aspect of Large Language Model (LLM) reliability, referring to an LLM\u2019s capability to withhold responses when uncertain or lacking a definitive answer, without compromising performance. Although previous studies have attempted to improve AA, they lack a standardized evaluation method and remain unsuitable for black-box models where token prediction probabilities are inaccessible. This makes comparative analysis challenging, especially for state-of-the-art closed-source commercial LLMs. This paper bridges this gap by introducing a black-box evaluation approach and a new dataset, Abstain-QA, crafted to rigorously assess AA across varied question types (answerable and unanswerable), domains (well-represented and under-represented), and task types (fact-centric and reasoning). We also propose a new confusion matrix, the \u201dAnswerable-Unanswerable Confusion Matrix\u201d (AUCM) which serves as the basis for evaluating AA, by offering a structured and precise approach for assessment. Finally, we explore the impact of three prompting strategies \u2014 Strict Prompting, Verbal Confidence Thresholding, and Chain-of-Thought (CoT) \u2014 on improving AA. Our results indicate that even powerful models like GPT-4, Mixtral 8x22b encounter difficulties with abstention; however, strategic approaches such as Strict prompting and CoT can enhance this capability.",
        "author": "Nishanth Madhusudhan; Sathwik Tejaswi Madhusudhan; Vikas Yadav; Masoud Hashemi",
        "authorids": "/n/nishanth-madhusudhan/; /s/sathwik-tejaswi-madhusudhan/; /v/vikas-yadav/; /m/masoud-hashemi/",
        "bibtex": "@inproceedings{madhusudhan-etal-2025-llms,\n    title = \"Do {LLM}s Know When to {NOT} Answer? Investigating Abstention Abilities of Large Language Models\",\n    author = \"Madhusudhan, Nishanth  and\n      Madhusudhan, Sathwik Tejaswi  and\n      Yadav, Vikas  and\n      Hashemi, Masoud\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.627/\",\n    pages = \"9329--9345\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.627.pdf",
        "site": "https://aclanthology.org/2025.coling-main.627/",
        "pdf_size": 694835,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17686372899300725480&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "ServiceNow; ServiceNow; ServiceNow; ServiceNow",
        "aff_domain": "servicenow.com;servicenow.com;servicenow.com;servicenow.com",
        "email": "servicenow.com;servicenow.com;servicenow.com;servicenow.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ServiceNow",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.servicenow.com",
        "aff_unique_abbr": "ServiceNow",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.360",
        "title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid advancement of large language models (LLMs) for handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions in MDPs adhere to specific probability distributions and require iterative sampling. This arouses curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent\u2019s behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: sequence simulation with explicit probability distribution and sequence simulation with implicit probability distribution. Our analysis indicates that LLM agents can understand probabilities, but they struggle with probability sampling. Their ability to perform probabilistic sampling can be improved to some extent by integrating coding tools, but this level of sampling precision still makes it difficult to simulate human behavior as agents.",
        "author": "Jia Gu; Liang Pang; Huawei Shen; Xueqi Cheng",
        "authorids": "/j/jia-gu/; /l/liang-pang/; /h/huawei-shen/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{gu-etal-2025-llms,\n    title = \"Do {LLM}s Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation\",\n    author = \"Gu, Jia  and\n      Pang, Liang  and\n      Shen, Huawei  and\n      Cheng, Xueqi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.360/\",\n    pages = \"5375--5390\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.360.pdf",
        "site": "https://aclanthology.org/2025.coling-main.360/",
        "pdf_size": 791019,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5289458446312106321&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "mails.ucas.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "mails.ucas.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.ict.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.201",
        "title": "Do Large Language Models Mirror Cognitive Language Processing?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in text comprehension and logical reasoning, indicating that the text representations learned by LLMs can facilitate their language processing capabilities. In neuroscience, brain cognitive processing signals are typically utilized to study human language processing. Therefore, it is natural to ask how well the text embeddings from LLMs align with the brain cognitive processing signals, and how training strategies affect the LLM-brain alignment? In this paper, we employ Representational Similarity Analysis (RSA) to measure the alignment between 23 mainstream LLMs and fMRI signals of the brain to evaluate how effectively LLMs simulate cognitive language processing. We empirically investigate the impact of various factors (e.g., pre-training data size, model scaling, alignment training, and prompts) on such LLM-brain alignment. Experimental results indicate that pre-training data size and model scaling are positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity. Explicit prompts contribute to the consistency of LLMs with brain cognitive language processing, while nonsensical noisy prompts may attenuate such alignment. Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated with the LLM-brain similarity.",
        "author": "Yuqi Ren; Renren Jin; Tongxuan Zhang; Deyi Xiong",
        "authorids": "/y/yuqi-ren/; /r/renren-jin/; /t/tongxuan-zhang/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{ren-etal-2025-large,\n    title = \"Do Large Language Models Mirror Cognitive Language Processing?\",\n    author = \"Ren, Yuqi  and\n      Jin, Renren  and\n      Zhang, Tongxuan  and\n      Xiong, Deyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.201/\",\n    pages = \"2988--3001\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.201.pdf",
        "site": "https://aclanthology.org/2025.coling-main.201/",
        "pdf_size": 806347,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8422703044965289004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University; College of Computer and Information Engineering, Tianjin Normal University; College of Intelligence and Computing, Tianjin University",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tjnu.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tjnu.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Tianjin University;Tianjin Normal University",
        "aff_unique_dep": "College of Intelligence and Computing;College of Computer and Information Engineering",
        "aff_unique_url": "http://www.tju.edu.cn;http://www.tjnu.edu.cn",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tianjin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.80",
        "title": "Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We study language ideologies in text produced by LLMs through a case study on English gendered language reform (related to role nouns like congressperson/-woman/-man, and singular they). First, we find political bias: when asked to use language that is \u201ccorrect\u201d or \u201cnatural\u201d, LLMs use language most similarly to when asked to align with conservative (vs. progressive) values. This shows how LLMs\u2019 metalinguistic preferences can implicitly communicate the language ideologies of a particular political group, even in seemingly non-political contexts. Second, we find LLMs exhibit internal inconsistency: LLMs use gender-neutral variants more often when more explicit metalinguistic context is provided. This shows how the language ideologies expressed in text produced by LLMs can vary, which may be unexpected to users. We discuss the broader implications of these findings for value alignment.",
        "author": "Julia Watson; Sophia S. Lee; Barend Beekhuizen; Suzanne Stevenson",
        "authorids": "/j/julia-watson/; /s/sophia-s-lee/; /b/barend-beekhuizen/; /s/suzanne-stevenson/",
        "bibtex": "@inproceedings{watson-etal-2025-language,\n    title = \"Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in {LLM}s\",\n    author = \"Watson, Julia  and\n      Lee, Sophia S.  and\n      Beekhuizen, Barend  and\n      Stevenson, Suzanne\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.80/\",\n    pages = \"1201--1223\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.80.pdf",
        "site": "https://aclanthology.org/2025.coling-main.80/",
        "pdf_size": 6650557,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XmAwgeKJORYJ:scholar.google.com/&scioq=Do+language+models+practice+what+they+preach%3F+Examining+language+ideologies+about+gendered+language+reform+encoded+in+LLMs&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Language Studies, University of Toronto, Mississauga; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;utoronto.ca;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;utoronto.ca;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Toronto;Mississauga",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-main.669",
        "title": "Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-Augmented Generation (RAG) has recently gained significant attention for its enhanced ability to integrate external knowledge sources into open-domain question answering (QA) tasks. However, it remains unclear how these models address fairness concerns, particularly with respect to sensitive attributes such as gender, geographic location, and other demographic factors. First, as language models evolve to prioritize utility, like improving exact match accuracy, fairness considerations may have been largely overlooked. Second, the complex, multi-component architecture of RAG methods poses challenges in identifying and mitigating biases, as each component is optimized for distinct objectives. In this paper, we aim to empirically evaluate fairness in several RAG methods. We propose a fairness evaluation framework tailored to RAG, using scenario-based questions and analyzing disparities across demographic attributes. Our experimental results indicate that, despite recent advances in utility-driven optimization, fairness issues persist in both the retrieval and generation stages. These findings underscore the need for targeted interventions to address fairness concerns throughout the RAG pipeline. The dataset and code used in this study are publicly available at this GitHub Repository.",
        "author": "Xuyang Wu; Shuowei Li; Hsin-Tai Wu; Zhiqiang Tao; Yi Fang",
        "authorids": "/x/xuyang-wu/; /s/shuowei-li/; /h/hsin-tai-wu/; /z/zhiqiang-tao/; /y/yi-fang/",
        "bibtex": "@inproceedings{wu-etal-2025-rag,\n    title = \"Does {RAG} Introduce Unfairness in {LLM}s? Evaluating Fairness in Retrieval-Augmented Generation Systems\",\n    author = \"Wu, Xuyang  and\n      Li, Shuowei  and\n      Wu, Hsin-Tai  and\n      Tao, Zhiqiang  and\n      Fang, Yi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.669/\",\n    pages = \"10021--10036\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.669.pdf",
        "site": "https://aclanthology.org/2025.coling-main.669/",
        "pdf_size": 841294,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2013879652757751957&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Santa Clara University; Santa Clara University; DOCOMO Innovations, Inc. + Santa Clara University; Rochester Institute of Technology; Santa Clara University",
        "aff_domain": "scu.edu;scu.edu;docomoinnovations.com;rit.edu;scu.edu",
        "email": "scu.edu;scu.edu;docomoinnovations.com;rit.edu;scu.edu",
        "github": "https://github.com/elviswxy/RAG_fairness",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+0;2;0",
        "aff_unique_norm": "Santa Clara University;DOCOMO Innovations;Rochester Institute of Technology",
        "aff_unique_dep": ";Innovations;",
        "aff_unique_url": "https://www.scu.edu;https://www.docomo-innovations.com;https://www.rit.edu",
        "aff_unique_abbr": "SCU;DOCOMO Innovations;RIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.127",
        "title": "Does Vision Accelerate Hierarchical Generalization in Neural Language Learners?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural language models (LMs) are arguably less data-efficient than humans from a language acquisition perspective. One fundamental question is why this human\u2013LM gap arises. This study explores the advantage of grounded language acquisition, specifically the impact of visual information \u2014 which humans can usually rely on but LMs largely do not have access to during language acquisition \u2014 on syntactic generalization in LMs. Our experiments, following the poverty of stimulus paradigm under two scenarios (using artificial vs. naturalistic images), demonstrate that if the alignments between the linguistic and visual components are clear in the input, access to vision data does help with the syntactic generalization of LMs, but if not, visual input does not help. This highlights the need for additional biases or signals, such as mutual gaze, to enhance cross-modal alignment and enable efficient syntactic generalization in multimodal LMs.",
        "author": "Tatsuki Kuribayashi; Timothy Baldwin",
        "authorids": "/t/tatsuki-kuribayashi/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{kuribayashi-baldwin-2025-vision,\n    title = \"Does Vision Accelerate Hierarchical Generalization in Neural Language Learners?\",\n    author = \"Kuribayashi, Tatsuki  and\n      Baldwin, Timothy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.127/\",\n    pages = \"1865--1879\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.127.pdf",
        "site": "https://aclanthology.org/2025.coling-main.127/",
        "pdf_size": 1945696,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uF2DhdpU_cIJ:scholar.google.com/&scioq=Does+Vision+Accelerate+Hierarchical+Generalization+in+Neural+Language+Learners%3F&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "MBZUAI; MBZUAI",
        "aff_domain": "mbzuai.ac.ae; ",
        "email": "mbzuai.ac.ae; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Mohamed Bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Arab Emirates"
    },
    {
        "id": "2025.coling-main.628",
        "title": "Dr.ECI: Infusing Large Language Models with Causal Knowledge for Decomposed Reasoning in Event Causality Identification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the demonstrated potential of Large Language Models (LLMs) in diverse NLP tasks, their causal reasoning capability appears inadequate when evaluated within the context of the event causality identification (ECI) task. The ECI tasks pose significant complexity for LLMs and necessitate comprehensive causal priors for accurate identification. To improve the performance of LLMs for causal reasoning, we propose a multi-agent Decomposed reasoning framework for Event Causality Identification, designated as Dr.ECI. In the discovery stage, Dr.ECI incorporates specialized agents such as Causal Explorer and Mediator Detector, which capture implicit causality and indirect causality more effectively. In the reasoning stage, Dr.ECI introduces the agents Direct Reasoner and Indirect Reasoner, which leverage the knowledge of the generalized causal structure specific to the ECI. Extensive evaluations demonstrate the state-of-the-art performance of Dr.ECI comparing with baselines based on LLMs and supervised training. Our implementation will be open-sourced at https://github.com/DMIRLAB-Group/Dr.ECI.",
        "author": "Ruichu Cai; Shengyin Yu; Jiahao Zhang; Wei Chen; Boyan Xu; Keli Zhang",
        "authorids": "/r/ruichu-cai/; /s/shengyin-yu/; /j/jiahao-zhang/; /w/wei-chen/; /b/boyan-xu/; /k/keli-zhang/",
        "bibtex": "@inproceedings{cai-etal-2025-dr,\n    title = \"{D}r.{ECI}: Infusing Large Language Models with Causal Knowledge for Decomposed Reasoning in Event Causality Identification\",\n    author = \"Cai, Ruichu  and\n      Yu, Shengyin  and\n      Zhang, Jiahao  and\n      Chen, Wei  and\n      Xu, Boyan  and\n      Zhang, Keli\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.628/\",\n    pages = \"9346--9375\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.628.pdf",
        "site": "https://aclanthology.org/2025.coling-main.628/",
        "pdf_size": 961513,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8804588477659291777&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Guangdong University of Technology; School of Computer Science, Guangdong University of Technology; School of Computer Science, Guangdong University of Technology; School of Computer Science, Guangdong University of Technology; School of Computer Science, Guangdong University of Technology+Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;huawei.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;huawei.com",
        "github": "https://github.com/DMIRLAB-Group/Dr.ECI",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0+1;1",
        "aff_unique_norm": "Guangdong University of Technology;Huawei",
        "aff_unique_dep": "School of Computer Science;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.gdut.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": ";Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.715",
        "title": "Driving Chinese Spelling Correction from a Fine-Grained Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores the task: Chinese spelling correction (CSC), from a fine-grained perspec- tive by recognizing that existing evaluations lack nuanced typology for the spelling errors. This deficiency can create a misleading impres- sion of model performance, incurring an \u201cin- visible\u201d bottleneck hindering the advancement of CSC research. In this paper, we first cate- gorize spelling errors into six types and con- duct a fine-grained evaluation across a wide variety of models, including BERT-based mod- els and LLMs. Thus, we are able to pinpoint the underlying weaknesses of existing state-of- the-art models - utilizing contextual clues and handling co-existence of multiple typos, asso- ciated to contextual errors and multi-typo er- rors. However, these errors occur infrequently in conventional training corpus. Therefore, we introduce new error generation methods to aug- ment their occurrence, which can be leveraged to enhance the training of CSC models. We hope this work could provide fresh insight for future CSC research.",
        "author": "Linfeng Liu; Hongqiu Wu; Hai Zhao",
        "authorids": "/l/linfeng-liu/; /h/hongqiu-wu/; /h/hai-zhao/",
        "bibtex": "@inproceedings{liu-etal-2025-driving,\n    title = \"Driving {C}hinese Spelling Correction from a Fine-Grained Perspective\",\n    author = \"Liu, Linfeng  and\n      Wu, Hongqiu  and\n      Zhao, Hai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.715/\",\n    pages = \"10727--10737\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.715.pdf",
        "site": "https://aclanthology.org/2025.coling-main.715/",
        "pdf_size": 1136866,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-DIf0foumdEJ:scholar.google.com/&scioq=Driving+Chinese+Spelling+Correction+from+a+Fine-Grained+Perspective&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + SJTU Paris Elite Institute of Technology; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University",
        "aff_domain": "cs.sjtu.edu.cn; ;cs.sjtu.edu.cn",
        "email": "cs.sjtu.edu.cn; ;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "China;France"
    },
    {
        "id": "2025.coling-main.90",
        "title": "Dying or Departing? Euphemism Detection for Death Discourse in Historical Texts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Euphemisms are a linguistic device used to soften discussions of sensitive or uncomfortable topics, with death being a prominent example. In this paper, we present a study on the detection of death-related euphemisms in historical literary texts from a corpus containing Danish and Norwegian novels from the late 19th century. We introduce an annotated dataset of euphemistic and literal references to death, including both common and rare euphemisms, ranging from well-established terms to more culturally nuanced expressions. We evaluate the performances of state-of-the-art pre-trained language models fine-tuned for euphemism detection. Our findings show that fixed, literal expressions of death became less frequent over time, while metaphorical euphemisms grew in prevalence. Additionally, euphemistic language was more common in historical novels, whereas contemporary novels tended to refer to death more literally, reflecting the rise of secularism. These results shed light on the shifting discourse on death during a period when the concept of death as final became prominent.",
        "author": "Ali Al-Laith; Alexander Conroy; Jens Bjerring-Hansen; Bolette Pedersen; Carsten Levisen; Daniel Hershcovich",
        "authorids": "/a/ali-al-laith/; /a/alexander-conroy/; /j/jens-bjerring-hansen/; /b/bolette-sandford-pedersen/; /c/carsten-levisen/; /d/daniel-hershcovich/",
        "bibtex": "@inproceedings{al-laith-etal-2025-dying,\n    title = \"Dying or Departing? Euphemism Detection for Death Discourse in Historical Texts\",\n    author = \"Al-Laith, Ali  and\n      Conroy, Alexander  and\n      Bjerring-Hansen, Jens  and\n      Pedersen, Bolette  and\n      Levisen, Carsten  and\n      Hershcovich, Daniel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.90/\",\n    pages = \"1353--1364\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.90.pdf",
        "site": "https://aclanthology.org/2025.coling-main.90/",
        "pdf_size": 1564383,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:B4fdv-VOWr8J:scholar.google.com/&scioq=Dying+or+Departing%3F+Euphemism+Detection+for+Death+Discourse+in+Historical+Texts&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/mime-memo/Euphemism-of-Death",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.319",
        "title": "DynRank: Improve Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents DynRank, a novel framework for enhancing passage retrieval in open-domain question-answering systems through dynamic zero-shot question classification. Traditional approaches rely on static prompts and pre-defined templates, which may limit model adaptability across different questions and contexts. In contrast, DynRank introduces a dynamic prompting mechanism, leveraging a pre-trained question classification model that categorizes questions into fine-grained types. Based on these classifications, contextually relevant prompts are generated, enabling more effective passage retrieval. We integrate DynRank into existing retrieval frameworks and conduct extensive experiments on multiple QA benchmark datasets.",
        "author": "Abdelrahman Abdallah; Jamshid Mozafari; Bhawna Piryani; Mohammed M. Abdelgwad; Adam Jatowt",
        "authorids": "/a/abdelrahman-abdallah/; /j/jamshid-mozafari/; /b/bhawna-piryani/; /m/mohammed-m-abdelgwad/; /a/adam-jatowt/",
        "bibtex": "@inproceedings{abdallah-etal-2025-dynrank,\n    title = \"{D}yn{R}ank: Improve Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification\",\n    author = \"Abdallah, Abdelrahman  and\n      Mozafari, Jamshid  and\n      Piryani, Bhawna  and\n      Abdelgwad, Mohammed M.  and\n      Jatowt, Adam\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.319/\",\n    pages = \"4768--4778\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.319.pdf",
        "site": "https://aclanthology.org/2025.coling-main.319/",
        "pdf_size": 1173052,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17663045928645322611&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Innsbruck; University of Innsbruck; University of Innsbruck; University of Innsbruck; University of Innsbruck",
        "aff_domain": "uibk.ac.at;uibk.ac.at;uibk.ac.at;uibk.ac.at;uibk.ac.at",
        "email": "uibk.ac.at;uibk.ac.at;uibk.ac.at;uibk.ac.at;uibk.ac.at",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Innsbruck",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uibk.ac.at",
        "aff_unique_abbr": "UIBK",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "2025.coling-main.18",
        "title": "Dynamic Graph Neural ODE Network for Multi-modal Emotion Recognition in Conversation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal emotion recognition in conversation (MERC) refers to identifying and classifying human emotional states by combining data from multiple different modalities (e.g., audio, images, text, video, etc.). Specifically, human emotional expressions are often complex and diverse, and these complex emotional expressions can be captured and understood more comprehensively through the fusion of multimodal information. Most existing graph-based multimodal emotion recognition methods can only use shallow GCNs to extract emotion features and fail to capture the temporal dependencies caused by dynamic changes in emotions. To address the above problems, we propose a Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) for multimodal emotion recognition in conversation, which combines the dynamic changes of emotions to capture the temporal dependency of speakers\u2019 emotions. Technically, the key idea of DGODE is to use the graph ODE evolution network to characterize the continuous dynamics of node representations over time and capture temporal dependencies. Extensive experiments on two publicly available multimodal emotion recognition datasets demonstrate that the proposed DGODE model has superior performance compared to various baselines. Furthermore, the proposed DGODE can also alleviate the over-smoothing problem, thereby enabling the construction of a deep GCN network.",
        "author": "Yuntao Shou; Tao Meng; Wei Ai; Keqin Li",
        "authorids": "/y/yuntao-shou/; /t/tao-meng/; /w/wei-ai/; /k/keqin-li/",
        "bibtex": "@inproceedings{shou-etal-2025-dynamic,\n    title = \"Dynamic Graph Neural {ODE} Network for Multi-modal Emotion Recognition in Conversation\",\n    author = \"Shou, Yuntao  and\n      Meng, Tao  and\n      Ai, Wei  and\n      Li, Keqin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.18/\",\n    pages = \"256--268\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.18.pdf",
        "site": "https://aclanthology.org/2025.coling-main.18/",
        "pdf_size": 1908772,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3852692629770791813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer and Mathematics, Central South University of Forestry and Technology, Changsha, Hunan, 410004, China; College of Computer and Mathematics, Central South University of Forestry and Technology, Changsha, Hunan, 410004, China; College of Computer and Mathematics, Central South University of Forestry and Technology, Changsha, Hunan, 410004, China; Department of Computer Science, State University of New York, New Paltz, New York 12561, USA",
        "aff_domain": "hnu.edu.cn; ; ; ",
        "email": "hnu.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Central South University of Forestry and Technology;State University of New York at New Paltz",
        "aff_unique_dep": "College of Computer and Mathematics;Department of Computer Science",
        "aff_unique_url": ";https://www.newpaltz.edu",
        "aff_unique_abbr": ";SUNY New Paltz",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Changsha;New Paltz",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.586",
        "title": "Dynamic-prototype Contrastive Fine-tuning for Continual Few-shot Relation Extraction with Unseen Relation Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Continual Few-shot Relation Extraction (CFRE) aims to continually learn new relations from limited labeled data while preserving knowledge about previously learned relations. Facing the inherent issue of catastrophic forgetting, previous approaches predominantly rely on memory replay strategies. However, they often overlook task interference in continual learning and the varying memory requirements for different relations. To address these shortcomings, we propose a novel framework, DPC-FT, which features: 1) a lightweight relation encoder for each task to mitigate negative knowledge transfer across tasks; 2) a dynamic prototype module to allocate less memory for easier relations and more memory for harder relations. Additionally, we introduce the None-Of-The-Above (NOTA) detection in CFRE and propose a threshold criterion to identify relations that have never been learned. Extensive experiments demonstrate the effectiveness and efficiency of our method in CFRE, making our approach more practical and comprehensive for real-world scenarios.",
        "author": "Si Miao Zhao; Zhen Tan; Ning Pang; Wei Dong Xiao; Xiang Zhao",
        "authorids": "/s/si-miao-zhao/; /z/zhen-tan/; /n/ning-pang/; /w/wei-dong-xiao/; /x/xiang-zhao/",
        "bibtex": "@inproceedings{zhao-etal-2025-dynamic,\n    title = \"Dynamic-prototype Contrastive Fine-tuning for Continual Few-shot Relation Extraction with Unseen Relation Detection\",\n    author = \"Zhao, Si Miao  and\n      Tan, Zhen  and\n      Pang, Ning  and\n      Xiao, Wei Dong  and\n      Zhao, Xiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.586/\",\n    pages = \"8763--8773\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.586.pdf",
        "site": "https://aclanthology.org/2025.coling-main.586/",
        "pdf_size": 871223,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9809890580424592258&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 0,
        "aff": "National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; Laboratory for Big Data and Decision, National University of Defense Technology, China",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;vip.sina.com;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;vip.sina.com;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "National Key Laboratory of Information Systems Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.689",
        "title": "Dynamics of Instruction Fine-Tuning for Chinese Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). While numerous studies have examined the impact of factors such as data volume and model size on English models, the scaling properties of instruction tuning in other languages remain largely unexplored. In this work, we systematically investigate the effects of data quantity, model size, and data construction methods on instruction tuning for Chinese LLMs. We utilize a newly curated dataset, DoIT, which includes over 40,000 high-quality instruction instances covering ten underlying abilities, such as creative writing, code generation, and logical reasoning. Our experiments, conducted on models ranging from 7b to 33b parameters, yield three key findings: (i) While these factors directly affect overall model performance, some abilities are more responsive to scaling, whereas others demonstrate significant resistance. (ii) The scaling sensitivity of different abilities to these factors can be explained by two features: Complexity and Transference. (iii) By tailoring training strategies to their varying sensitivities, specific abilities can be efficiently learned, enhancing performance on two public benchmarks.",
        "author": "Chiyu Song; Zhanchao Zhou; Jianhao Yan; Yuejiao Fei; Zhenzhong Lan; Yue Zhang",
        "authorids": "/c/chiyu-song/; /z/zhanchao-zhou/; /j/jianhao-yan/; /y/yuejiao-fei/; /z/zhenzhong-lan/; /y/yue-zhang/",
        "bibtex": "@inproceedings{song-etal-2025-dynamics,\n    title = \"Dynamics of Instruction Fine-Tuning for {C}hinese Large Language Models\",\n    author = \"Song, Chiyu  and\n      Zhou, Zhanchao  and\n      Yan, Jianhao  and\n      Fei, Yuejiao  and\n      Lan, Zhenzhong  and\n      Zhang, Yue\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.689/\",\n    pages = \"10345--10366\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.689.pdf",
        "site": "https://aclanthology.org/2025.coling-main.689/",
        "pdf_size": 5495081,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:88mlC5qMJMMJ:scholar.google.com/&scioq=Dynamics+of+Instruction+Fine-Tuning+for+Chinese+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Zhejiang University+School of Engineering, Westlake University; Zhejiang University+School of Engineering, Westlake University; Zhejiang University+School of Engineering, Westlake University; Zhejiang University+School of Engineering, Westlake University; School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn; ; ; ; ; ",
        "email": "westlake.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1;1+2;1+2",
        "aff_unique_norm": "Zhejiang University;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": ";School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;WIAS",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.159",
        "title": "E-Bench: Towards Evaluating the Ease-of-Use of Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Modern large language models are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results for the model. Composing an optimal prompt for a specific demand lacks theoretical support and relies entirely on human experimentation, which poses a considerable obstacle to popularizing generative artificial intelligence. However, there is no systematic analysis of the stability of large language models to resist prompt perturbations. In this work, we propose to evaluate the ease-of-use of large language models and construct E-Bench, simulating the actual situation of human use from synonymous perturbation (including paraphrasing, simplification, and colloquialism) and typographical perturbation. Besides we also discuss the combination of these two types of perturbation and analyze the main reasons for performance degradation. Experimental results indicate that with the increase of model size, although the ease-of-use could be significantly improved, there is still a long way to go to build a sufficiently user-friendly model.",
        "author": "Zhenyu Zhang; Bingguang Hao; Jinpeng Li; Zekai Zhang; Dongyan Zhao",
        "authorids": "/z/zhenyu-zhang/; /b/bingguang-hao/; /j/jinpeng-li/; /z/zekai-zhang/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{zhang-etal-2025-e,\n    title = \"{E}-Bench: Towards Evaluating the Ease-of-Use of Large Language Models\",\n    author = \"Zhang, Zhenyu  and\n      Hao, Bingguang  and\n      Li, Jinpeng  and\n      Zhang, Zekai  and\n      Zhao, Dongyan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.159/\",\n    pages = \"2329--2339\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.159.pdf",
        "site": "https://aclanthology.org/2025.coling-main.159/",
        "pdf_size": 538150,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7283440129386348500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Baidu Inc.+Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University",
        "aff_domain": "baidu.com;stu.pku.edu.cn; ; ; ",
        "email": "baidu.com;stu.pku.edu.cn; ; ; ",
        "github": "https://github.com/zzysay/E-Bench",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "Baidu Inc.;Peking University",
        "aff_unique_dep": ";Wangxuan Institute of Computer Technology",
        "aff_unique_url": "https://www.baidu.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "Baidu;PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.367",
        "title": "ECC: Synergizing Emotion, Cause and Commonsense for Empathetic Dialogue Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Empathy improves human-machine dialogue systems by enhancing the user\u2019s experience. While traditional models have aimed to detect and express users\u2019 emotions from dialogue history, they neglect the crucial and complex interactions among emotion, emotion causes, and commonsense. To address this, we introduce the ECC (Emotion, Cause, and Commonsense) framework, which leverages specialized encoders to capture the key features of emotion, cause, and commonsense and collaboratively models these through a Conditional Variational Auto-Encoder. ECC further employs novel loss functions to refine the interplay of three factors and generates empathetic responses using an energy-based model supported by ODE sampling. Empirical results on the EmpatheticDialogues dataset demonstrate that ECC outperforms existing baselines, offering a robust solution for empathetic dialogue generation.",
        "author": "Xu Wang; Bo Wang; Yihong Tang; Dongming Zhao; Jing Liu; Ruifang He; Yuexian Hou",
        "authorids": "/x/xu-wang/; /b/bo-wang/; /y/yihong-tang/; /d/dongming-zhao/; /j/jing-liu/; /r/ruifang-he/; /y/yuexian-hou/",
        "bibtex": "@inproceedings{wang-etal-2025-ecc,\n    title = \"{ECC}: Synergizing Emotion, Cause and Commonsense for Empathetic Dialogue Generation\",\n    author = \"Wang, Xu  and\n      Wang, Bo  and\n      Tang, Yihong  and\n      Zhao, Dongming  and\n      Liu, Jing  and\n      He, Ruifang  and\n      Hou, Yuexian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.367/\",\n    pages = \"5475--5485\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.367.pdf",
        "site": "https://aclanthology.org/2025.coling-main.367/",
        "pdf_size": 1564320,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dvVidCysDKMJ:scholar.google.com/&scioq=ECC:+Synergizing+Emotion,+Cause+and+Commonsense+for+Empathetic+Dialogue+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China+Beijing Wenge Technology Co., Ltd., Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of New Media and Communication, Tianjin University, Tianjin, China; AI Lab, China Mobile Communication Group Tianjin Co., Ltd.; AI Lab, China Mobile Communication Group Tianjin Co., Ltd.; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "qq.com;tju.edu.cn; ; ; ; ; ",
        "email": "qq.com;tju.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;2;2;0;0",
        "aff_unique_norm": "Tianjin University;Beijing Wenge Technology Co., Ltd.;China Mobile Communication Group Tianjin Co., Ltd.",
        "aff_unique_dep": "College of Intelligence and Computing;;AI Lab",
        "aff_unique_url": "http://www.tju.edu.cn;;http://www.chinamobileltd.com/",
        "aff_unique_abbr": "Tianjin University;;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Tianjin;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.15",
        "title": "EDAR: A pipeline for Emotion and Dialogue Act Recognition",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Individuals facing financial difficulties often make decisions driven by emotions rather than rational analysis. EDAR, a pipeline for Emotion and Dialogue Act Recognition, is designed specifically for the debt collection process in France. By integrating EDAR into decision-making systems, debt collection outcomes could be improved. The pipeline employs Machine Learning and Deep Learning models, demonstrating that smaller models with fewer parameters can achieve high performance, offering an efficient alternative to large language models.",
        "author": "Elie Dina; Rania Ayachi Kibech; Miguel Couceiro",
        "authorids": "/e/elie-dina/; /r/rania-ayachi-kibech/; /m/miguel-couceiro/",
        "bibtex": "@inproceedings{dina-etal-2025-edar,\n    title = \"{EDAR}: A pipeline for Emotion and Dialogue Act Recognition\",\n    author = \"Dina, Elie  and\n      Ayachi Kibech, Rania  and\n      Couceiro, Miguel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.15/\",\n    pages = \"175--186\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.15.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.15/",
        "pdf_size": 1066514,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:K-ZSb4tWb8UJ:scholar.google.com/&scioq=EDAR:+A+pipeline+for+Emotion+and+Dialogue+Act+Recognition&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "https://www.metcredit.com/blog/the-role-of-emotional-intelligence-in-debt-collection/",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.516",
        "title": "EERPD: Leveraging Emotion and Emotion Regulation for Improving Personality Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Personality is a fundamental construct in psychology, reflecting an individual\u2019s behavior, thinking, and emotional patterns. While previous researches have made progress in personality detection, their designed methods generally overlook the important connection between psychological knowledge \u201cemotion regulation\u201d and personality traits. Based on this, we propose a new personality detection method called EERPD. This method introduces the use of emotion regulation, a psychological concept highly correlated with personality, for personality prediction. By combining this concept with emotion features, EERPD retrieves few-shot examples and provides process CoTs for inferring labels from text. This approach enhances the understanding of LLM for personality implicit within text and improves the performance in personality detection. Experimental results demonstrate that EERPD significantly enhances the accuracy and robustness of personality detection, outperforming previous SOTA by 15.05/4.29 in average F1 on the two benchmark datasets.",
        "author": "Zheng Li; Sujian Li; Dawei Zhu; Qilong Ma; Weimin Xiong",
        "authorids": "/z/zheng-li/; /s/sujian-li/; /d/dawei-zhu/; /q/qilong-ma/; /w/weimin-xiong/",
        "bibtex": "@inproceedings{li-etal-2025-eerpd,\n    title = \"{EERPD}: Leveraging Emotion and Emotion Regulation for Improving Personality Detection\",\n    author = \"Li, Zheng  and\n      Li, Sujian  and\n      Zhu, Dawei  and\n      Ma, Qilong  and\n      Xiong, Weimin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.516/\",\n    pages = \"7721--7734\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.516.pdf",
        "site": "https://aclanthology.org/2025.coling-main.516/",
        "pdf_size": 1217155,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3ZH8SVffsNkJ:scholar.google.com/&scioq=EERPD:+Leveraging+Emotion+and+Emotion+Regulation+for+Improving+Personality+Detection&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Peking University + State Key Laboratory for Multimedia Information Processing, Peking University; School of Computer Science, Peking University; School of Software, BNRist, Tsinghua University; School of Computer Science, Peking University; School of Computer Science, Peking University + State Key Laboratory for Multimedia Information Processing, Peking University + Jiangsu Collaborative Innovation Center for Language Ability, Jiangsu Normal University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;mails.tsinghua.edu.cn; ;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;mails.tsinghua.edu.cn; ;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0;1;0;0+0+2",
        "aff_unique_norm": "Peking University;Tsinghua University;Jiangsu Normal University",
        "aff_unique_dep": "School of Computer Science;School of Software;Jiangsu Collaborative Innovation Center for Language Ability",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.tsinghua.edu.cn;http://www.jsnu.edu.cn",
        "aff_unique_abbr": "PKU;THU;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.313",
        "title": "ELAINE-medLLM: Lightweight English Japanese Chinese Trilingual Large Language Model for Bio-medical Domain",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose ELAINE (EngLish-jApanese-chINesE)-medLLM, a trilingual (English, Japanese, Chinese) large language model adapted for the bio-medical domain based on Llama-3-8B. The training dataset was carefully curated in terms of volume and diversity to adapt to the biomedical domain and endow trilingual capability while preserving the knowledge and abilities of the base model. The training follows 2-stage paths: continued pre-training and supervised fine-tuning (SFT). Our results demonstrate that ELAINE-medLLM exhibits superior trilingual capabilities compared to existing bilingual or multilingual medical LLMs without severely sacrificing the base model\u2019s capability.",
        "author": "Ken Yano; Zheheng Luo; Jimin Huang; Qianqian Xie; Masaki Asada; Chenhan Yuan; Kailai Yang; Makoto Miwa; Sophia Ananiadou; Jun\u2019ichi Tsujii",
        "authorids": "/k/ken-yano/; /z/zheheng-luo/; /j/jimin-huang/; /q/qianqian-xie/; /m/masaki-asada/; /c/chenhan-yuan/; /k/kailai-yang/; /m/makoto-miwa/; /s/sophia-ananiadou/; /j/junichi-tsujii/",
        "bibtex": "@inproceedings{yano-etal-2025-elaine,\n    title = \"{ELAINE}-med{LLM}: Lightweight {E}nglish {J}apanese {C}hinese Trilingual Large Language Model for Bio-medical Domain\",\n    author = \"Yano, Ken  and\n      Luo, Zheheng  and\n      Huang, Jimin  and\n      Xie, Qianqian  and\n      Asada, Masaki  and\n      Yuan, Chenhan  and\n      Yang, Kailai  and\n      Miwa, Makoto  and\n      Ananiadou, Sophia  and\n      Tsujii, Jun{'}ichi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.313/\",\n    pages = \"4670--4688\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.313.pdf",
        "site": "https://aclanthology.org/2025.coling-main.313/",
        "pdf_size": 380506,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:m7kwCcv3besJ:scholar.google.com/&scioq=ELAINE-medLLM:+Lightweight+English+Japanese+Chinese+Trilingual+Large+Language+Model+for+Bio-medical+Domain&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 10
    },
    {
        "id": "2025.coling-main.28",
        "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending the models\u2019 context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, we propose a new benchmark for long-context LLMs focused on a practical meeting assistant scenario in which the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench, augments the existing ELITR corpus by adding 271 manually crafted questions with their ground-truth answers, as well as noisy versions of meeting transcripts altered to target different Word Error Rate levels. Our experiments with 12 long-context LLMs on ELITR-Bench confirm the progress made across successive generations of both proprietary and open models, and point out their discrepancies in terms of robustness to transcript noise. We also provide a thorough analysis of our GPT-4-based evaluation, including insights from a crowdsourcing study. Our findings indicate that while GPT-4\u2019s scores align with human judges, its ability to distinguish beyond three score levels may be limited.",
        "author": "Thibaut Thonet; Laurent Besacier; Jos Rozen",
        "authorids": "/t/thibaut-thonet/; /l/laurent-besacier/; /j/jos-rozen/",
        "bibtex": "@inproceedings{thonet-etal-2025-elitr,\n    title = \"{ELITR}-Bench: A Meeting Assistant Benchmark for Long-Context Language Models\",\n    author = \"Thonet, Thibaut  and\n      Besacier, Laurent  and\n      Rozen, Jos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.28/\",\n    pages = \"407--428\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.28.pdf",
        "site": "https://aclanthology.org/2025.coling-main.28/",
        "pdf_size": 530169,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9002457985458636291&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "NA VER LABS Europe; NA VER LABS Europe; NA VER LABS Europe",
        "aff_domain": "naverlabs.com;naverlabs.com;naverlabs.com",
        "email": "naverlabs.com;naverlabs.com;naverlabs.com",
        "github": "https://github.com/utter-project/ELITR-Bench",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NAVER LABS Europe",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.naverlabs.com/europe",
        "aff_unique_abbr": "NAVER LABS Europe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Unknown"
    },
    {
        "id": "2025.coling-demos.10",
        "title": "EasyJudge: an Easy-to-use Tool for Comprehensive Response Evaluation of LLMs",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Recently, there has been a growing trend of employing large language models (LLMs) to judge the quality of other LLMs. Many studies have adopted closed-source models, mainly using GPT-4 as the evaluator. However, due to the closed-source nature of the GPT-4 model, employing it as an evaluator has resulted in issues including transparency, controllability, and cost-effectiveness. Some researchers have turned to using fine-tuned open-source LLMs as evaluators. However, existing open-source evaluation LLMs generally lack a user-friendly visualization tool, and they have not been optimized for accelerated model inference, which causes inconvenience for researchers with limited resources and those working across different fields. This paper presents EasyJudge, a model developed to evaluate significant language model responses. It is lightweight, precise, efficient, and user-friendly, featuring an intuitive visualization interface for ease of deployment and use. EasyJudge uses detailed datasets and refined prompts for model optimization, achieving strong consistency with human and proprietary model evaluations. The model optimized with quantitative methods enables EasyJudge to run efficiently on consumer-grade GPUs or even CPUs.",
        "author": "Yijie Li; Yuan Sun",
        "authorids": "/y/yijie-li/; /y/yuan-sun/",
        "bibtex": "@inproceedings{li-sun-2025-easyjudge,\n    title = \"{E}asy{J}udge: an Easy-to-use Tool for Comprehensive Response Evaluation of {LLM}s\",\n    author = \"Li, Yijie  and\n      Sun, Yuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.10/\",\n    pages = \"91--103\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.10.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.10/",
        "pdf_size": 3087155,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5yEc5Gt0rQ0J:scholar.google.com/&scioq=EasyJudge:+an+Easy-to-use+Tool+for+Comprehensive+Response+Evaluation+of+LLMs&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Minzu University of China, Beijing, China+National Language Resource Monitoring & Research Center Minority Languages Branch; Minzu University of China, Beijing, China+National Language Resource Monitoring & Research Center Minority Languages Branch",
        "aff_domain": "163.com;muc.edu.cn",
        "email": "163.com;muc.edu.cn",
        "github": "https://github.com/4real3000/EasyJudge",
        "project": "https://youtu.be/3NcSWPf9rzM",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Minzu University of China;National Language Resource Monitoring & Research Center",
        "aff_unique_dep": ";Minority Languages Branch",
        "aff_unique_url": "http://www.muc.edu.cn/;",
        "aff_unique_abbr": "MUC;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2025.coling-main.379",
        "title": "Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Distilling high-accuracy Graph Neural Networks (GNNs) to low-latency multilayer perceptrons (MLPs) on graph tasks has become a hot research topic. However, conventional MLP learning relies almost exclusively on graph nodes and fails to effectively capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose Prototype-Guided Knowledge Distillation (PGKD), which does not require graph edges (edge-free setting) yet learns structure-aware MLPs. Our insight is to distill graph structural information from GNNs. Specifically, we first employ the class prototypes to analyze the impact of graph structures on GNN teachers, and then design two losses to distill such information from GNNs to MLPs. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.",
        "author": "Taiqiang Wu; Zhe Zhao; Jiahao Wang; Xingyu Bai; Lei Wang; Ngai Wong; Yujiu Yang",
        "authorids": "/t/taiqiang-wu/; /z/zhe-zhao/; /j/jiahao-wang/; /x/xingyu-bai/; /l/lei-wang/; /n/ngai-wong/; /y/yujiu-yang/",
        "bibtex": "@inproceedings{wu-etal-2025-edge,\n    title = \"Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from {GNN}s to {MLP}s\",\n    author = \"Wu, Taiqiang  and\n      Zhao, Zhe  and\n      Wang, Jiahao  and\n      Bai, Xingyu  and\n      Wang, Lei  and\n      Wong, Ngai  and\n      Yang, Yujiu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.379/\",\n    pages = \"5656--5667\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.379.pdf",
        "site": "https://aclanthology.org/2025.coling-main.379/",
        "pdf_size": 1579781,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7891834878173268607&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Hong Kong; Tencent AI Lab; The University of Hong Kong; Tsinghua University; Ping An Technology; The University of Hong Kong; Tsinghua University",
        "aff_domain": "connect.hku.hk; ; ; ; ;eee.hku.hk;sz.tsinghua.edu.cn",
        "email": "connect.hku.hk; ; ; ; ;eee.hku.hk;sz.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;2;3;0;2",
        "aff_unique_norm": "The University of Hong Kong;Tencent;Tsinghua University;Ping An Technology",
        "aff_unique_dep": ";Tencent AI Lab;;",
        "aff_unique_url": "https://www.hku.hk;https://ai.tencent.com;https://www.tsinghua.edu.cn;https://www.pingan.com.cn",
        "aff_unique_abbr": "HKU;Tencent AI Lab;THU;Ping An",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.229",
        "title": "Edit-Wise Preference Optimization for Grammatical Error Correction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While large language models (LLMs) have achieved remarkable success in various natural language processing tasks, their strengths have yet to be fully demonstrated in grammatical error correction (GEC). This is partly due to the misalignment between their pre-training objectives and the GEC principle of making minimal edits. In this work, we aim to bridge this gap by introducing a novel method called Edit-wise Preference Optimization (EPO). By distinguishing the importance of different tokens and assigning higher reward weights to edit tokens during preference optimization, our method captures fine-grained distinctions in GEC that traditional preference learning often overlooks. Extensive experiments on both English and Chinese datasets show that our framework consistently outperforms strong baselines, achieving state-of-the-art performance and demonstrating the advantages of LLMs in GEC.",
        "author": "Jiehao Liang; Haihui Yang; Shiping Gao; Xiaojun Quan",
        "authorids": "/j/jiehao-liang/; /h/haihui-yang/; /s/shiping-gao/; /x/xiaojun-quan/",
        "bibtex": "@inproceedings{liang-etal-2025-edit,\n    title = \"Edit-Wise Preference Optimization for Grammatical Error Correction\",\n    author = \"Liang, Jiehao  and\n      Yang, Haihui  and\n      Gao, Shiping  and\n      Quan, Xiaojun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.229/\",\n    pages = \"3401--3414\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.229.pdf",
        "site": "https://aclanthology.org/2025.coling-main.229/",
        "pdf_size": 649037,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:PLYClrh3nz4J:scholar.google.com/&scioq=Edit-Wise+Preference+Optimization+for+Grammatical+Error+Correction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China",
        "aff_domain": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.479",
        "title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA\u2019s effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.",
        "author": "Zixuan Dong; Baoyun Peng; Yufei Wang; Jia Fu; Xiaodong Wang; Xin Zhou; Yongxue Shan; Kangchen Zhu; Weiguo Chen",
        "authorids": "/z/zixuan-dong/; /b/baoyun-peng/; /y/yufei-wang/; /j/jia-fu/; /x/xiaodong-wang/; /x/xin-zhou/; /y/yongxue-shan/; /k/kangchen-zhu/; /w/weiguo-chen/",
        "bibtex": "@inproceedings{dong-etal-2025-effiqa,\n    title = \"{E}ffi{QA}: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs\",\n    author = \"Dong, Zixuan  and\n      Peng, Baoyun  and\n      Wang, Yufei  and\n      Fu, Jia  and\n      Wang, Xiaodong  and\n      Zhou, Xin  and\n      Shan, Yongxue  and\n      Zhu, Kangchen  and\n      Chen, Weiguo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.479/\",\n    pages = \"7180--7194\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.479.pdf",
        "site": "https://aclanthology.org/2025.coling-main.479/",
        "pdf_size": 6406994,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2568055027672028878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "National University of Defense Technology; Academy of Military Sciences+National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology",
        "aff_domain": "alumni.nudt.edu.cn;alumni.nudt.edu.cn; ; ; ; ; ; ; ",
        "email": "alumni.nudt.edu.cn;alumni.nudt.edu.cn; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1+0;0;0;0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology;Academy of Military Sciences",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nudt.edu.cn/;",
        "aff_unique_abbr": "NUDT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.700",
        "title": "Efficient Architectures for High Resolution Vision-Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.",
        "author": "Miguel Carvalho; Bruno Martins",
        "authorids": "/m/miguel-carvalho/; /b/bruno-martins/",
        "bibtex": "@inproceedings{carvalho-martins-2025-efficient,\n    title = \"Efficient Architectures for High Resolution Vision-Language Models\",\n    author = \"Carvalho, Miguel  and\n      Martins, Bruno\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.700/\",\n    pages = \"10520--10530\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.700.pdf",
        "site": "https://aclanthology.org/2025.coling-main.700/",
        "pdf_size": 773983,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:RdhFvpzZakQJ:scholar.google.com/&scioq=Efficient+Architectures+for+High+Resolution+Vision-Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "INESC-ID and Instituto Superior T\u00e9cnico, University of Lisbon; INESC-ID and Instituto Superior T\u00e9cnico, University of Lisbon",
        "aff_domain": "tecnico.ulisboa.pt;tecnico.ulisboa.pt",
        "email": "tecnico.ulisboa.pt;tecnico.ulisboa.pt",
        "github": "https://github.com/miguelscarv/pheye",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Lisbon",
        "aff_unique_dep": "INESC-ID, Instituto Superior T\u00e9cnico",
        "aff_unique_url": "https://www IST.utl.pt",
        "aff_unique_abbr": "ULisbon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "2025.coling-main.282",
        "title": "Efficient Cross-modal Prompt Learning with Semantic Enhancement for Domain-robust Fake News Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the development of multimedia technology, online social media has become a major medium for people to access news, but meanwhile, it has also exacerbated the dissemination of multi-modal fake news. An automatic and efficient multi-modal fake news detection (MFND) method is urgently needed. Existing MFND methods usually conduct cross-modal information interaction at later stage, resulting in insufficient exploration of complementary information between modalities. Another challenge lies in the differences among news data from different domains, leading to the weak generalization ability in detecting news from various domains. In this work, we propose an efficient Cross-modal Prompt Learning with Semantic enhancement method for Domain-robust fake news detection (CPLSD). Specifically, we design an efficient cross-modal prompt interaction module, which utilizes prompt as medium to realize lightweight cross-modal information interaction in the early stage of feature extraction, enabling to exploit rich modality complementary information. We design a domain-general prompt generation module that can adaptively blend domain-specific news features to generate domain-general prompts, for improving the domain generalization ability of the model. Furthermore, an image semantic enhancement module is designed to achieve image-to-text translation, fully exploring the semantic discriminative information of the image modality. Extensive experiments conducted on three MFND benchmarks demonstrate the superiority of our proposed approach over existing state-of-the-art MFND methods.",
        "author": "Fei Wu; Hao Jin; Changhui Hu; Yimu Ji; Xiao-Yuan Jing; Guo-Ping Jiang",
        "authorids": "/f/fei-wu/; /h/hao-jin/; /c/changhui-hu/; /y/yimu-ji/; /x/xiao-yuan-jing/; /g/guo-ping-jiang/",
        "bibtex": "@inproceedings{wu-etal-2025-efficient,\n    title = \"Efficient Cross-modal Prompt Learning with Semantic Enhancement for Domain-robust Fake News Detection\",\n    author = \"Wu, Fei  and\n      Jin, Hao  and\n      Hu, Changhui  and\n      Ji, Yimu  and\n      Jing, Xiao-Yuan  and\n      Jiang, Guo-Ping\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.282/\",\n    pages = \"4175--4185\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.282.pdf",
        "site": "https://aclanthology.org/2025.coling-main.282/",
        "pdf_size": 12745991,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J2ljp0qdo_4J:scholar.google.com/&scioq=Efficient+Cross-modal+Prompt+Learning+with+Semantic+Enhancement+for+Domain-robust+Fake+News+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.748",
        "title": "Efficient Data Labeling by Hierarchical Crowdsourcing with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have received lots of attention for their impressive performance in in-context dialogues and their potential to revolutionize service industries with a new business model, Model-as-a-Service (MaaS). Automated data labeling is a natural and promising service. However, labeling data with LLMs faces two main challenges: 1) the labels from LLMs may contain uncertainty, and 2) using LLMs for data labeling tasks can be prohibitively expensive, as the scales of datasets are usually tremendous. In this paper, we propose a hierarchical framework named LMCrowd that leverages multiple LLMs for efficient data labeling under budget constraints. The proposed LMCrowd framework first aggregates labels from multiple freely available LLMs, and then employs a large, paid MaaS LLM for relabeling selected instances. Furthermore, we formalize the core process as an optimization problem, aiming to select the optimal set of instances for relabeling by the MaaS LLM, given the current belief state. Extensive experimental evaluations across various real-world datasets demonstrate that our framework outperforms human labelers and GPT-4 in terms of both accuracy and efficiency.",
        "author": "Haodi Zhang; Junyu Yang; Jinyin Nie; Peirou Liang; Kaishun Wu; Defu Lian; Rui Mao; Yuanfeng Song",
        "authorids": "/h/haodi-zhang/; /j/junyu-yang/; /j/jinyin-nie/; /p/peirou-liang/; /k/kaishun-wu/; /d/defu-lian/; /r/rui-mao/; /y/yuanfeng-song/",
        "bibtex": "@inproceedings{zhang-etal-2025-efficient,\n    title = \"Efficient Data Labeling by Hierarchical Crowdsourcing with Large Language Models\",\n    author = \"Zhang, Haodi  and\n      Yang, Junyu  and\n      Nie, Jinyin  and\n      Liang, Peirou  and\n      Wu, Kaishun  and\n      Lian, Defu  and\n      Mao, Rui  and\n      Song, Yuanfeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.748/\",\n    pages = \"11290--11303\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.748.pdf",
        "site": "https://aclanthology.org/2025.coling-main.748/",
        "pdf_size": 883598,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7180813840616256643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Shenzhen University; Shenzhen University; Shenzhen University; Shenzhen University; The Hong Kong University of Science and Technology (Guangzhou); University of Science and Technology of China; Shenzhen University; WeBank Co., Ltd",
        "aff_domain": "szu.edu.cn;szu.edu.cn;szu.edu.cn;szu.edu.cn;ust.hk;ustc.edu.cn;szu.edu.cn;webank.com",
        "email": "szu.edu.cn;szu.edu.cn;szu.edu.cn;szu.edu.cn;ust.hk;ustc.edu.cn;szu.edu.cn;webank.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;2;0;3",
        "aff_unique_norm": "Shenzhen University;The Hong Kong University of Science and Technology;University of Science and Technology of China;WeBank",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.szu.edu.cn;https://www.ust.hk;http://www.ustc.edu.cn;https://www.webank.com",
        "aff_unique_abbr": "SZU;HKUST;USTC;WeBank",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.128",
        "title": "Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in comprehending and analyzing lengthy sequential inputs, owing to their extensive context windows that allow processing millions of tokens in a single forward pass. However, this paper uncovers a surprising limitation: LLMs fall short when handling long input sequences. We investigate this issue using three datasets and two tasks (sentiment analysis and news categorization) across various LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct, and Mistral Instruct models. To address this limitation, we propose and evaluate ad-hoc solutions that substantially enhance LLMs\u2019 performance on long input sequences by up to 50%, while reducing API cost and latency by up to 93% and 50%, respectively.",
        "author": "Peyman Hosseini; Ignacio Castro; Iacopo Ghinassi; Matthew Purver",
        "authorids": "/p/peyman-hosseini/; /i/ignacio-castro/; /i/iacopo-ghinassi/; /m/matthew-purver/",
        "bibtex": "@inproceedings{hosseini-etal-2025-efficient,\n    title = \"Efficient Solutions For An Intriguing Failure of {LLM}s: Long Context Window Does Not Mean {LLM}s Can Analyze Long Sequences Flawlessly\",\n    author = \"Hosseini, Peyman  and\n      Castro, Ignacio  and\n      Ghinassi, Iacopo  and\n      Purver, Matthew\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.128/\",\n    pages = \"1880--1891\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.128.pdf",
        "site": "https://aclanthology.org/2025.coling-main.128/",
        "pdf_size": 785178,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6133606313208941962&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of EECS, Queen Mary University of London, London, UK; School of EECS, Queen Mary University of London, London, UK; School of EECS, Queen Mary University of London, London, UK; School of EECS, Queen Mary University of London, London, UK + Department of Knowledge Technologies, Jo\u017eef Stefan Institute, Ljubljana, Slovenia",
        "aff_domain": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Queen Mary University of London;Jo\u017eef Stefan Institute",
        "aff_unique_dep": "School of EECS;Department of Knowledge Technologies",
        "aff_unique_url": "https://www.qmul.ac.uk;https://www.ijs.si",
        "aff_unique_abbr": "QMUL;JSI",
        "aff_campus_unique_index": "0;0;0;0+1",
        "aff_campus_unique": "London;Ljubljana",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "United Kingdom;Slovenia"
    },
    {
        "id": "2025.coling-main.185",
        "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.",
        "author": "Silin Gao; Jane Dwivedi-Yu; Ping Yu; Xiaoqing Ellen Tan; Ramakanth Pasunuru; Olga Golovneva; Koustuv Sinha; Asli Celikyilmaz; Antoine Bosselut; Tianlu Wang",
        "authorids": "/s/silin-gao/; /j/jane-dwivedi-yu/; /p/ping-yu/; /x/xiaoqing-ellen-tan/; /r/ramakanth-pasunuru/; /o/olga-golovneva/; /k/koustuv-sinha/; /a/asli-celikyilmaz/; /a/antoine-bosselut/; /t/tianlu-wang/",
        "bibtex": "@inproceedings{gao-etal-2025-efficient,\n    title = \"Efficient Tool Use with Chain-of-Abstraction Reasoning\",\n    author = \"Gao, Silin  and\n      Dwivedi-Yu, Jane  and\n      Yu, Ping  and\n      Tan, Xiaoqing Ellen  and\n      Pasunuru, Ramakanth  and\n      Golovneva, Olga  and\n      Sinha, Koustuv  and\n      Celikyilmaz, Asli  and\n      Bosselut, Antoine  and\n      Wang, Tianlu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.185/\",\n    pages = \"2727--2743\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.185.pdf",
        "site": "https://aclanthology.org/2025.coling-main.185/",
        "pdf_size": 973701,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13992389665210473896&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "EPFL+FAIR @ Meta; FAIR @ Meta; FAIR @ Meta; FAIR @ Meta; FAIR @ Meta; FAIR @ Meta; FAIR @ Meta; FAIR @ Meta; EPFL+FAIR @ Meta; FAIR @ Meta",
        "aff_domain": "epfl.ch;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;epfl.ch;meta.com",
        "email": "epfl.ch;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;epfl.ch;meta.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;1;1;1;1;1;1;0+1;1",
        "aff_unique_norm": "Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne;Meta AI Research (FAIR)",
        "aff_unique_dep": ";AI Research",
        "aff_unique_url": "https://www.epfl.ch;https://ai.facebook.com",
        "aff_unique_abbr": "EPFL;FAIR",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1;1;1;1;0+1;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2025.coling-industry.64",
        "title": "Efficient Vocabulary Reduction for Small Language Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "The increasing size of large language models (LLMs) poses significant challenges due to their high computational costs and energy consumption, making their deployment in industrial settings difficult. Small language models (SLMs) have been introduced to mitigate these challenges by reducing model size while preserving performance. However, the embedding layer, which occupies a significant portion of the model, remains a bottleneck in model compression efforts. In this paper, we valdated vocabulary reduction as a solution to compress the embedding layer and reduce model size without significant loss of performance. We conduct a series of experiments to investigate how vocabulary reduction affects GPU memory footprint, inference speed, and task performance. Our results show that while performance generally declines with vocabulary reduction, fine-tuning can recover much of the lost performance. Moreover, in some tasks, such as truthfulness and summarization, the vocabulary-reduced models outperform the baseline. Finally, we demonstrate that vocabulary reduction can be effectively applied in domain adaptation, particularly in the medical domain, and in multilingual adaptation, improving task efficiency and cross-lingual robustness.",
        "author": "Yuta Nozaki; Dai Nakashima; Ryo Sato; Naoki Asaba; Shintaro Kawamura",
        "authorids": "/y/yuta-nozaki/; /d/dai-nakashima/; /r/ryo-sato/; /n/naoki-asaba/; /s/shintaro-kawamura/",
        "bibtex": "@inproceedings{nozaki-etal-2025-efficient,\n    title = \"Efficient Vocabulary Reduction for Small Language Models\",\n    author = \"Nozaki, Yuta  and\n      Nakashima, Dai  and\n      Sato, Ryo  and\n      Asaba, Naoki  and\n      Kawamura, Shintaro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.64/\",\n    pages = \"771--783\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.64.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.64/",
        "pdf_size": 362947,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DPFDYT_3xDgJ:scholar.google.com/&scioq=Efficient+Vocabulary+Reduction+for+Small+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Ricoh Company, Ltd.; Ricoh Company, Ltd.; Ricoh Company, Ltd.; Ricoh Company, Ltd.; Ricoh Company, Ltd.",
        "aff_domain": "jp.ricoh.com;jp.ricoh.com;jp.ricoh.com;jp.ricoh.com;jp.ricoh.com",
        "email": "jp.ricoh.com;jp.ricoh.com;jp.ricoh.com;jp.ricoh.com;jp.ricoh.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Ricoh Company, Ltd.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ricoh.com/",
        "aff_unique_abbr": "Ricoh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.400",
        "title": "Egalitarian Language Representation in Language Models: It All Begins with Tokenizers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Tokenizers act as a bridge between human language and the latent space of language models, influencing how language is represented in these models. Despite the dominance of English-Centric (EC) Large Language Models (LLMs), tokenization methods often fail to fairly represent complex scripts like Tamil, Sinhala, and Hindi, primarily due to pre-tokenization choices. This study demonstrates that pre-tokenization has a more significant impact than tokenization algorithms on achieving egalitarian representation. To address this, we introduce an improvement to the Byte Pair Encoding (BPE) algorithm by incorporating graphemes, which we term Grapheme Pair Encoding (GPE). Our experiments show that grapheme-based character extraction outperforms byte-level tokenizers for complex scripts. We validate this approach through experiments on Tamil, Sinhala, and Hindi. The codebase and resources used in this work are publicly available at https://github.com/vmenan/tokenizers-coling2025.",
        "author": "Menan Velayuthan; Kengatharaiyer Sarveswaran",
        "authorids": "/m/menan-velayuthan/; /k/kengatharaiyer-sarveswaran/",
        "bibtex": "@inproceedings{velayuthan-sarveswaran-2025-egalitarian,\n    title = \"Egalitarian Language Representation in Language Models: It All Begins with Tokenizers\",\n    author = \"Velayuthan, Menan  and\n      Sarveswaran, Kengatharaiyer\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.400/\",\n    pages = \"5987--5996\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.400.pdf",
        "site": "https://aclanthology.org/2025.coling-main.400/",
        "pdf_size": 440801,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17061370891685343496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Jaffna, Sri Lanka; Department of Computer Science, University of Jaffna, Sri Lanka",
        "aff_domain": "gmail.com;univ.jfn.ac.lk",
        "email": "gmail.com;univ.jfn.ac.lk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Jaffna",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "http://www.jaffnauniversity.ac.lk",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Sri Lanka"
    },
    {
        "id": "2025.coling-main.236",
        "title": "Embedding Style Beyond Topics: Analyzing Dispersion Effects Across Different Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper analyzes how writing style affects the dispersion of embedding vectors across multiple, state-of-the-art language models. While early transformer models primarily aligned with topic modeling, this study examines the role of writing style in shaping embedding spaces. Using a literary corpus that alternates between topics and styles, we compare the sensitivity of language models across French and English. By analyzing the particular impact of style on embedding dispersion, we aim to better understand how language models process stylistic information, contributing to their overall interpretability.",
        "author": "Benjamin Icard; Evangelia Zve; Lila Sainero; Alice Breton; Jean-Gabriel Ganascia",
        "authorids": "/b/benjamin-icard/; /e/evangelia-zve/; /l/lila-sainero/; /a/alice-breton/; /j/jean-gabriel-ganascia/",
        "bibtex": "@inproceedings{icard-etal-2025-embedding,\n    title = \"Embedding Style Beyond Topics: Analyzing Dispersion Effects Across Different Language Models\",\n    author = \"Icard, Benjamin  and\n      Zve, Evangelia  and\n      Sainero, Lila  and\n      Breton, Alice  and\n      Ganascia, Jean-Gabriel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.236/\",\n    pages = \"3511--3522\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.236.pdf",
        "site": "https://aclanthology.org/2025.coling-main.236/",
        "pdf_size": 1751406,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UpwHnI8OChwJ:scholar.google.com/&scioq=Embedding+Style+Beyond+Topics:+Analyzing+Dispersion+Effects+Across+Different+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "LIP6, Sorbonne University, CNRS, France; LIP6, Sorbonne University, CNRS, France+Infopro Digital, France; LIP6, Sorbonne University, CNRS, France; LIP6, Sorbonne University, CNRS, France; LIP6, Sorbonne University, CNRS, France",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Sorbonne University;Infopro Digital",
        "aff_unique_dep": "LIP6;",
        "aff_unique_url": "https://www.sorbonne.universite.fr;",
        "aff_unique_abbr": "Sorbonne U;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.94",
        "title": "Embedding-Informed Adaptive Retrieval-Augmented Generation of Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. However, it was observed by previous works that retrieval is not always helpful, especially when the LLM is already knowledgable on the query to answer. Motivated by this, Adaptive Retrieval-Augmented Generation (ARAG) studies retrieving only when the knowledge asked by the query is absent in the LLM. Previous works of ARAG either require accessing the pre-training corpus or prompting with additional model inferences. Aiming to avoid such drawbacks, we propose to determine whether the model is knowledgeable on a query via inspecting the (contextualized) pre-trained token embeddings of LLMs. We hypothesize that such embeddings capture rich information on the model\u2019s intrinsic knowledge base, which enables an efficient way of judging the necessity to retrieve from an external corpus. Extensive experiments demonstrate our ARAG approach\u2019s superior performance across various benchmarks.",
        "author": "Chengkai Huang; Yu Xia; Rui Wang; Kaige Xie; Tong Yu; Julian McAuley; Lina Yao",
        "authorids": "/c/chengkai-huang/; /y/yu-xia/; /r/rui-wang/; /k/kaige-xie/; /t/tong-yu/; /j/julian-mcauley/; /l/lina-yao/",
        "bibtex": "@inproceedings{huang-etal-2025-embedding,\n    title = \"Embedding-Informed Adaptive Retrieval-Augmented Generation of Large Language Models\",\n    author = \"Huang, Chengkai  and\n      Xia, Yu  and\n      Wang, Rui  and\n      Xie, Kaige  and\n      Yu, Tong  and\n      McAuley, Julian  and\n      Yao, Lina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.94/\",\n    pages = \"1403--1412\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.94.pdf",
        "site": "https://aclanthology.org/2025.coling-main.94/",
        "pdf_size": 508711,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16171808597036050041&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of New South Wales; University of California San Diego; Duke University; Georgia Institute of Technology; Adobe Research; University of California San Diego; University of New South Wales+CSIRO\u2019s Data61",
        "aff_domain": "unsw.edu.au; ; ; ; ; ; ",
        "email": "unsw.edu.au; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;4;1;0+5",
        "aff_unique_norm": "University of New South Wales;University of California, San Diego;Duke University;Georgia Institute of Technology;Adobe;CSIRO",
        "aff_unique_dep": ";;;;Adobe Research;Data61",
        "aff_unique_url": "https://www.unsw.edu.au;https://ucsd.edu;https://www.duke.edu;https://www.gatech.edu;https://research.adobe.com;https://www.csiro.au",
        "aff_unique_abbr": "UNSW;UCSD;Duke;Georgia Tech;Adobe;CSIRO",
        "aff_campus_unique_index": "1;1;",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;1;1;1;1;1;0+0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "2025.coling-main.248",
        "title": "Empirical Study of Zero-shot Keyphrase Extraction with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study investigates the effectiveness of Large Language Models (LLMs) for zero-shot keyphrase extraction (KE). We propose and evaluate four prompting strategies: vanilla, role prompting, candidate-based prompting, and hybrid prompting. Experiments conducted on six widely-used KE benchmark datasets demonstrate that Llama3-8B-Instruct with vanilla prompting outperforms state-of-the-art unsupervised methods, PromptRank, by an average of 9.43%, 7.68%, and 4.82% in F1@5, F1@10, and F1@15, respectively. Hybrid prompting, which combines the strengths of vanilla and candidate-based prompting, further enhances overall performance. Moreover role prompting, which assigns a task-related role to LLMs, consistently improves performance across various prompting strategies. We also explore the impact of model size and different LLM series: GPT-4o, Gemma2, and Qwen2. Results show that Llama3 and Gemma2 demonstrate the strongest zero-shot KE performance, with hybrid prompting consistently enhancing results across most LLMs. We hope this study provides insights to researchers exploring LLMs in KE tasks, as well as practical guidance for model selection in real-world applications. Our code is available at https://github.com/kangnlp/Zero-shot-KPE-with-LLMs.",
        "author": "Byungha Kang; Youhyun Shin",
        "authorids": "/b/byungha-kang/; /y/youhyun-shin/",
        "bibtex": "@inproceedings{kang-shin-2025-empirical,\n    title = \"Empirical Study of Zero-shot Keyphrase Extraction with Large Language Models\",\n    author = \"Kang, Byungha  and\n      Shin, Youhyun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.248/\",\n    pages = \"3670--3686\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.248.pdf",
        "site": "https://aclanthology.org/2025.coling-main.248/",
        "pdf_size": 957369,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3GMhTjg4cKsJ:scholar.google.com/&scioq=Empirical+Study+of+Zero-shot+Keyphrase+Extraction+with+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Incheon National University; Department of Computer Science and Engineering, Incheon National University",
        "aff_domain": "inu.ac.kr;inu.ac.kr",
        "email": "inu.ac.kr;inu.ac.kr",
        "github": "https://github.com/kangnlp/Zero-shot-KPE-with-LLMs",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Incheon National University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.inu.ac.kr",
        "aff_unique_abbr": "INU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Incheon",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.403",
        "title": "Empirical Study on Data Attributes Insufficiency of Evaluation Benchmarks for LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous benchmarks for evaluating large language models (LLMs) have primarily emphasized quantitative metrics, such as data volume. However, this focus may neglect key qualitative data attributes that can significantly impact the final rankings of LLMs, resulting in unreliable leaderboards. In this paper, we investigate whether current LLM benchmarks adequately consider these data attributes. We specifically examine three attributes: diversity, redundancy, and difficulty. To explore these attributes, we propose a framework with three separate modules, each designed to assess one of the attributes. Using a method that progressively incorporates these attributes, we analyze their influence on the benchmark. Our experimental results reveal a meaningful correlation between LLM rankings on the revised benchmark and the original benchmark when these attributes are accounted for. These findings indicate that existing benchmarks often fail to meet all three criteria, highlighting a lack of consideration for multifaceted data attributes in current evaluation datasets.",
        "author": "Chuang Liu; Renren Jin; Zheng Yao; Tianyi Li; Liang Cheng; Mark Steedman; Deyi Xiong",
        "authorids": "/c/chuang-liu/; /r/renren-jin/; /z/zheng-yao/; /t/tianyi-li/; /l/liang-cheng/; /m/mark-steedman/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{liu-etal-2025-empirical,\n    title = \"Empirical Study on Data Attributes Insufficiency of Evaluation Benchmarks for {LLM}s\",\n    author = \"Liu, Chuang  and\n      Jin, Renren  and\n      Yao, Zheng  and\n      Li, Tianyi  and\n      Cheng, Liang  and\n      Steedman, Mark  and\n      Xiong, Deyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.403/\",\n    pages = \"6024--6038\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.403.pdf",
        "site": "https://aclanthology.org/2025.coling-main.403/",
        "pdf_size": 548182,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:lqdeGvkLWqcJ:scholar.google.com/&scioq=Empirical+Study+on+Data+Attributes+Insufficiency+of+Evaluation+Benchmarks+for+LLMs&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University; School of Electrical Engineering and Computer Science, University of Queensland; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; College of Intelligence and Computing, Tianjin University",
        "aff_domain": "tju.edu.cn;tju.edu.cn; ; ; ; ;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn; ; ; ; ;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;2;2;0",
        "aff_unique_norm": "Tianjin University;University of Queensland;University of Edinburgh",
        "aff_unique_dep": "College of Intelligence and Computing;School of Electrical Engineering and Computer Science;School of Informatics",
        "aff_unique_url": "http://www.tju.edu.cn;https://www.uq.edu.au;https://www.ed.ac.uk",
        "aff_unique_abbr": ";UQ;Edinburgh",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "0;0;1;2;2;2;0",
        "aff_country_unique": "China;Australia;United Kingdom"
    },
    {
        "id": "2025.coling-main.576",
        "title": "Engagement-driven Persona Prompting for Rewriting News Tweets",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text style transfer is a challenging research task which modifies the linguistic style of a given text to meet pre-set objectives such as making the text simpler or more accessible. Though large language models have been found to give promising results, text rewriting to improve audience engagement of social media content is vastly unexplored. Our research investigates the performance of various prompting strategies in the task of rewriting Dutch news tweets in specific linguistic styles (formal, casual and factual). Apart from zero-shot and few-shot prompting variants, with and without personas, we also explore prompting with feedback on predicted engagement. We perform an extensive analysis of 18 different combinations of Large Language Models (GPT-3.5, GPT-4, Mistral-7B) and prompting strategies on three different metrics: ROUGE-L, semantic similarity and predicted engagement. We find that GPT-4 with feedback and persona prompting performs the best in terms of predicted engagement for all three language styles. Our results motivate further application of usage of prompting techniques to rewrite news headlines on Twitter to align with specific style guidelines.",
        "author": "Reshmi Gopalakrishna Pillai; Antske Fokkens; Wouter van Atteveldt",
        "authorids": "/r/reshmi-gopalakrishna-pillai/; /a/antske-fokkens/; /w/wouter-van-atteveldt/",
        "bibtex": "@inproceedings{gopalakrishna-pillai-etal-2025-engagement,\n    title = \"Engagement-driven Persona Prompting for Rewriting News Tweets\",\n    author = \"Gopalakrishna Pillai, Reshmi  and\n      Fokkens, Antske  and\n      van Atteveldt, Wouter\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.576/\",\n    pages = \"8612--8622\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.576.pdf",
        "site": "https://aclanthology.org/2025.coling-main.576/",
        "pdf_size": 207697,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18156140808053414545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Vrije University Amsterdam; Vrije University Amsterdam; Vrije University Amsterdam",
        "aff_domain": "vu.nl; ; ",
        "email": "vu.nl; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Vrije Universiteit Amsterdam",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vu.nl",
        "aff_unique_abbr": "VU Amsterdam",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amsterdam",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2025.coling-main.186",
        "title": "Enhancing Arabic NLP Tasks through Character-Level Models and Data Augmentation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study introduces a character-level approach specifically designed for Arabic NLP tasks, offering a novel and highly effective solution to the unique challenges inherent in Arabic language processing. It presents a thorough comparative study of various character-level models, including Convolutional Neural Networks (CNNs), pre-trained transformers (CANINE), and Bidirectional Long Short-Term Memory networks (BiLSTMs), assessing their performance and exploring the impact of different data augmentation techniques on enhancing their effectiveness. Additionally, it introduces two innovative Arabic-specific data augmentation methods\u2014vowel deletion and style transfer\u2014and rigorously evaluates their effectiveness. The proposed approach was evaluated on Arabic privacy policy classification task as a case study, demonstrating significant improvements in model performance, reporting a micro-averaged F1-score of 93.8%, surpassing state-of-the-art models.",
        "author": "Mohanad Mohamed; Sadam Al-Azani",
        "authorids": "/m/mohanad-mohamed/; /s/sadam-al-azani/",
        "bibtex": "@inproceedings{mohamed-al-azani-2025-enhancing,\n    title = \"Enhancing {A}rabic {NLP} Tasks through Character-Level Models and Data Augmentation\",\n    author = \"Mohamed, Mohanad  and\n      Al-Azani, Sadam\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.186/\",\n    pages = \"2744--2757\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.186.pdf",
        "site": "https://aclanthology.org/2025.coling-main.186/",
        "pdf_size": 520200,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=834364427777306011&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "SDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRC-AI), King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia + Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; SDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRC-AI), King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia",
        "aff_domain": "ksu.edu.sa;kfupm.edu.sa",
        "email": "ksu.edu.sa;kfupm.edu.sa",
        "github": "https://github.com/mohanad-hafez/char_models_arabic_nlp",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "King Fahd University of Petroleum & Minerals;King Saud University",
        "aff_unique_dep": "SDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRC-AI);Department of Computer Science",
        "aff_unique_url": "https://www.kfupm.edu.sa;https://www.ksu.edu.sa",
        "aff_unique_abbr": "KFUPM;KSU",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Dhahran;Riyadh",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "id": "2025.coling-main.334",
        "title": "Enhancing Criminal Investigation Analysis with Summarization and Memory-based Retrieval-Augmented Generation: A Comprehensive Evaluation of Real Case Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study introduces KriRAG, a novel Retrieval-Augmented Generation (RAG) architecture designed to assist criminal investigators in analyzing information and overcoming the challenge of information overload. KriRAG structures and summarizes extensive document collections based on existing investigative queries, providing relevant document references and detailed answers for each query. Working with unstructured data from two homicide case files comprising approximately 3,700 documents and 13,000 pages, a comprehensive evaluation methodology is established, incorporating semantic retrieval, scoring, reasoning, and query response accuracy. The system\u2019s outputs are evaluated against queries and answers provided by criminal investigators, demonstrating promising performance with 97.5% accuracy in relevance assessment and 77.5% accuracy for query responses. These findings provide a rigorous foundation for other query-oriented and open-ended retrieval applications. KriRAG is designed to run offline on limited hardware, ensuring sensitive data handling and on-device availability.",
        "author": "Mads Skipanes; Tollef Emil J\u00c3. rgensen; Kyle Porter; Gianluca Demartini; Sule Yildirim Yayilgan",
        "authorids": "/m/mads-skipanes/; /t/tollef-emil-ja-rgensen/; /k/kyle-porter/; /g/gianluca-demartini/; /s/sule-yildirim-yayilgan/",
        "bibtex": "@inproceedings{skipanes-etal-2025-enhancing,\n    title = \"Enhancing Criminal Investigation Analysis with Summarization and Memory-based Retrieval-Augmented Generation: A Comprehensive Evaluation of Real Case Data\",\n    author = \"Skipanes, Mads  and\n      J{\\~A}, rgensen, Tollef Emil  and\n      Porter, Kyle  and\n      Demartini, Gianluca  and\n      Yayilgan, Sule Yildirim\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.334/\",\n    pages = \"4993--5010\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.334.pdf",
        "site": "https://aclanthology.org/2025.coling-main.334/",
        "pdf_size": 768764,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:A_VLZ7Piie8J:scholar.google.com/&scioq=Enhancing+Criminal+Investigation+Analysis+with+Summarization+and+Memory-based+Retrieval-Augmented+Generation:+A+Comprehensive+Evaluation+of+Real+Case+Data&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Norwegian University of Science and Technology, Gj\u00f8vik, Norway + Norwegian University of Science and Technology, Trondheim, Norway; Norwegian University of Science and Technology, Trondheim, Norway; Norwegian University of Science and Technology, Gj\u00f8vik, Norway; University of Queensland, Brisbane, Australia; Norwegian University of Science and Technology, Gj\u00f8vik, Norway",
        "aff_domain": "politiet.no;ntnu.no;ntnu.no;uq.edu.au;ntnu.no",
        "email": "politiet.no;ntnu.no;ntnu.no;uq.edu.au;ntnu.no",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0;0;1;0",
        "aff_unique_norm": "Norwegian University of Science and Technology;University of Queensland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntnu.edu;https://www.uq.edu.au",
        "aff_unique_abbr": "NTNU;UQ",
        "aff_campus_unique_index": "0+1;1;0;2;0",
        "aff_campus_unique": "Gj\u00f8vik;Trondheim;Brisbane",
        "aff_country_unique_index": "0+0;0;0;1;0",
        "aff_country_unique": "Norway;Australia"
    },
    {
        "id": "2025.coling-main.584",
        "title": "Enhancing Discourse Parsing for Local Structures from Social Media with LLM-Generated Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We explore the use of discourse parsers for extracting a particular discourse structure in a real-world social media scenario. Specifically, we focus on enhancing parser performance through the integration of synthetic data generated by large language models (LLMs). We conduct experiments using a newly developed dataset of 1,170 local RST discourse structures, including 900 synthetic and 270 gold examples, covering three social media platforms: online news comments sections, a discussion forum (Reddit), and a social media messaging platform (Twitter). Our primary goal is to assess the impact of LLM-generated synthetic training data on parser performance in a raw text setting without pre-identified discourse units. While both top-down and bottom-up RST architectures greatly benefit from synthetic data, challenges remain in classifying evaluative discourse structures.",
        "author": "Martial Pastor; Nelleke Oostdijk; Patricia Martin-Rodilla; Javier Parapar",
        "authorids": "/m/martial-pastor/; /n/nelleke-oostdijk/; /p/patricia-martin-rodilla/; /j/javier-parapar/",
        "bibtex": "@inproceedings{pastor-etal-2025-enhancing,\n    title = \"Enhancing Discourse Parsing for Local Structures from Social Media with {LLM}-Generated Data\",\n    author = \"Pastor, Martial  and\n      Oostdijk, Nelleke  and\n      Martin-Rodilla, Patricia  and\n      Parapar, Javier\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.584/\",\n    pages = \"8739--8748\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.584.pdf",
        "site": "https://aclanthology.org/2025.coling-main.584/",
        "pdf_size": 528469,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15655985247911143833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Centre for Language Studies, Radboud University, The Netherlands; Centre for Language Studies, Radboud University, The Netherlands; IEGPS CSIC, Santiago de Compostela, Spain; IRLab, CITIC Research Centre, Universidade da Coru\u00f1a, La Coru\u00f1a, Spain",
        "aff_domain": "ru.nl;ru.nl;iegps.csic.es;udc.es",
        "email": "ru.nl;ru.nl;iegps.csic.es;udc.es",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Radboud University;Instituto de Estudios del Petr\u00f3leo y Gas (IEGPS);Universidade da Coru\u00f1a",
        "aff_unique_dep": "Centre for Language Studies;;IRLab, CITIC Research Centre",
        "aff_unique_url": "https://www.ru.nl;;https://www.udc.es",
        "aff_unique_abbr": "RU;IEGPS;",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Santiago de Compostela;La Coru\u00f1a",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "Netherlands;Spain"
    },
    {
        "id": "2025.coling-main.214",
        "title": "Enhancing Emotional Support Conversations: A Framework for Dynamic Knowledge Filtering and Persona Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the growing need for accessible emotional support, conversational agents are being used more frequently to provide empathetic and meaningful interactions. However, many existing dialogue models struggle to interpret user context accurately due to irrelevant or misclassified knowledge, limiting their effectiveness in real-world scenarios. To address this, we propose a new framework that dynamically filters relevant commonsense knowledge and extracts personalized information to improve empathetic dialogue generation. We evaluate our framework on the ESConv dataset using extensive automatic and human experiments. The results show that our approach outperforms other models in metrics, demonstrating better coherence, emotional understanding, and response relevance.",
        "author": "Jiawang Hao; Fang Kong",
        "authorids": "/j/jiawang-hao/; /f/fang-kong/",
        "bibtex": "@inproceedings{hao-kong-2025-enhancing,\n    title = \"Enhancing Emotional Support Conversations: A Framework for Dynamic Knowledge Filtering and Persona Extraction\",\n    author = \"Hao, Jiawang  and\n      Kong, Fang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.214/\",\n    pages = \"3193--3202\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.214.pdf",
        "site": "https://aclanthology.org/2025.coling-main.214/",
        "pdf_size": 1221031,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6-zUyjTLDjkJ:scholar.google.com/&scioq=Enhancing+Emotional+Support+Conversations:+A+Framework+for+Dynamic+Knowledge+Filtering+and+Persona+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Laboratory for Natural Language Processing + Soochow University School of Computer Science and Technology, Suzhou, China + TAL Education Group, Beijing, China; Laboratory for Natural Language Processing + Soochow University School of Computer Science and Technology, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1+2;0+1",
        "aff_unique_norm": "Laboratory for Natural Language Processing;Soochow University;TAL Education Group",
        "aff_unique_dep": "Natural Language Processing;School of Computer Science and Technology;",
        "aff_unique_url": ";https://www.soochow.edu.cn;https://www.tal.com",
        "aff_unique_abbr": ";;TAL",
        "aff_campus_unique_index": "1+2;1",
        "aff_campus_unique": ";Suzhou;Beijing",
        "aff_country_unique_index": "1+1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2025.coling-main.495",
        "title": "Enhancing Event Causality Identification with LLM Knowledge and Concept-Level Event Relations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event Causality Identification (ECI) aims to identify fine-grained causal relationships between events in an unstructured text. Existing ECI methods primarily rely on knowledge enhanced and graph-based reasoning approaches, but they often overlook the dependencies between similar events. Additionally, the connection between unstructured text and structured knowledge is relatively weak. Therefore, this paper proposes an ECI method enhanced by LLM Knowledge and Concept-Level Event Relations (LKCER). Specifically, LKCER constructs a conceptual-level heterogeneous event graph by leveraging the local contextual information of related event mentions, generating a more comprehensive global semantic representation of event concepts. At the same time, the knowledge generated by COMET is filtered and enriched using LLM, strengthening the associations between event pairs and knowledge. Finally, the joint event conceptual representation and knowledge-enhanced event representation are used to uncover potential causal relationships between events. The experimental results show that our method outperforms previous state-of-the-art methods on both benchmarks, EventStoryLine and Causal-TimeBank.",
        "author": "Ya Su; Hu Zhang; Guangjun Zhang; Yujie Wang; Yue Fan; Ru Li; Yuanlong Wang",
        "authorids": "/y/ya-su/; /h/hu-zhang/; /g/guangjun-zhang/; /y/yujie-wang/; /y/yue-fan/; /r/ru-li/; /y/yuanlong-wang/",
        "bibtex": "@inproceedings{su-etal-2025-enhancing,\n    title = \"Enhancing Event Causality Identification with {LLM} Knowledge and Concept-Level Event Relations\",\n    author = \"Su, Ya  and\n      Zhang, Hu  and\n      Zhang, Guangjun  and\n      Wang, Yujie  and\n      Fan, Yue  and\n      Li, Ru  and\n      Wang, Yuanlong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.495/\",\n    pages = \"7403--7414\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.495.pdf",
        "site": "https://aclanthology.org/2025.coling-main.495/",
        "pdf_size": 5633780,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MzQ0Z2uCE5QJ:scholar.google.com/&scioq=Enhancing+Event+Causality+Identification+with+LLM+Knowledge+and+Concept-Level+Event+Relations&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China",
        "aff_domain": "163.com;sxu.edu.cn;gmail.com;foxmail.com;163.com;sxu.edu.cn;sxu.edu.cn",
        "email": "163.com;sxu.edu.cn;gmail.com;foxmail.com;163.com;sxu.edu.cn;sxu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0;0+0;0;0;0;0+0;0+0",
        "aff_unique_norm": "Shanxi University",
        "aff_unique_dep": "School of Computer and Information Technology",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0+0;0+0;0;0;0;0+0;0+0",
        "aff_campus_unique": "Taiyuan",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.583",
        "title": "Enhancing Extractive Question Answering in Multiparty Dialogues with Logical Inference Memory Network",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multiparty dialogue question answering (QA) in machine reading comprehension (MRC) is a challenging task due to its complex information flow interactions and logical QA inference. Existing models typically handle such QA tasks by decoupling dialogue information at both speaker and utterance levels. However, few of them consider the logical inference relations in multiparty dialogue QA, leading to suboptimal QA performance. To address this issue, this paper proposes a memory network with logical inference (LIMN) for extractive QA in multiparty dialogues. LIMN introduces an inference module, which is pretrained by incorporating plain QA articles as external knowledge. It generates logical inference-aware representations from latent space for multiparty dialogues. To further model complex interactions among logical dialogue contexts, questions and key-utterance information, a key-utterance-based interaction method is proposed for leverage. Moreover, a multitask learning strategy is adopted for robust MRC. Extensive experiments were conducted on Molweni and FriendsQA benchmarks, which included 25k and 10k questions, respectively. Comparative results showed that LIMN achieves state-of-the-art results on both benchmarks, demonstrating the enhancement of logical QA inference in multiparty dialogue QA tasks.",
        "author": "Shu Zhou; Rui Zhao; Zhengda Zhou; Haohan Yi; Xuhui Zheng; Hao Wang",
        "authorids": "/s/shu-zhou/; /r/rui-zhao/; /z/zhengda-zhou/; /h/haohan-yi/; /x/xuhui-zheng/; /h/hao-wang/",
        "bibtex": "@inproceedings{zhou-etal-2025-enhancing,\n    title = \"Enhancing Extractive Question Answering in Multiparty Dialogues with Logical Inference Memory Network\",\n    author = \"Zhou, Shu  and\n      Zhao, Rui  and\n      Zhou, Zhengda  and\n      Yi, Haohan  and\n      Zheng, Xuhui  and\n      Wang, Hao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.583/\",\n    pages = \"8725--8738\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.583.pdf",
        "site": "https://aclanthology.org/2025.coling-main.583/",
        "pdf_size": 548359,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Mpe3KJ5DILsJ:scholar.google.com/&scioq=Enhancing+Extractive+Question+Answering+in+Multiparty+Dialogues+with+Logical+Inference+Memory+Network&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China; University of Technology Sydney, Australia; Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China; Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China; Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China; Nanjing University, China+Key Laboratory of Data Engineering and Knowledge Services in Jiangsu Provincial Universities (Nanjing University), China",
        "aff_domain": "smail.nju.edu.cn;student.uts.edu.au;163.com;163.com;qq.com;nju.edu.cn",
        "email": "smail.nju.edu.cn;student.uts.edu.au;163.com;163.com;qq.com;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;1;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Nanjing University;University of Technology Sydney",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.uts.edu.au",
        "aff_unique_abbr": "Nanjing U;UTS",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2025.coling-main.530",
        "title": "Enhancing Factual Consistency in Text Summarization via Counterfactual Debiasing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite significant progress in abstractive text summarization aimed at generating fluent and informative outputs, how to ensure the factual consistency of generated summaries remains a crucial and challenging issue. In this study, drawing inspiration from advancements in causal inference, we construct causal graphs to analyze the process of abstractive text summarization methods and identify intrinsic causes of factual inconsistency, specifically language bias and irrelevancy bias, and we propose CoFactSum, a novel framework that mitigates the causal effects of these biases through counterfactual estimation for enhancing the factual consistency of the generated content. CoFactSum provides two counterfactual estimation strategies, including Explicit Counterfactual Masking, which employs a dynamic masking approach, and Implicit Counterfactual Training, which utilizes a discriminative cross-attention mechanism. Besides, we propose a Debiasing Degree Adjustment mechanism to dynamically calibrate the level of debiasing at each decoding step. Extensive experiments conducted on two widely used summarization datasets demonstrate the effectiveness and advantages of the proposed CoFactSum in enhancing the factual consistency of generated summaries, outperforming several baseline methods.",
        "author": "Zhenqing Ling; Yuexiang Xie; Chenhe Dong; Ying Shen",
        "authorids": "/z/zhenqing-ling/; /y/yuexiang-xie/; /c/chenhe-dong/; /y/ying-shen/",
        "bibtex": "https://aclanthology.org/2025.coling-main.530.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.530.pdf",
        "site": "https://aclanthology.org/2025.coling-main.530/",
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4598775056700215601&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-industry.25",
        "title": "Enhancing Future Link Prediction in Quantum Computing Semantic Networks through LLM-Initiated Node Features",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Quantum computing is rapidly evolving in both physics and computer science, offering the potential to solve complex problems and accelerate computational processes. The development of quantum chips necessitates understanding the correlations among diverse experimental conditions. Semantic networks built on scientific literature, representing meaningful relationships between concepts, have been used across various domains to identify knowledge gaps and novel concept combinations. Neural network-based approaches have shown promise in link prediction within these networks. This study proposes initializing node features using LLMs to enhance node representations for link prediction tasks in graph neural networks. LLMs can provide rich descriptions, reducing the need for manual feature creation and lowering costs. Our method, evaluated using various link prediction models on a quantum computing semantic network, demonstrated efficacy compared to traditional node embedding techniques.",
        "author": "Gilchan Park; Paul Baity; Byung-Jun Yoon; Adolfy Hoisie",
        "authorids": "/g/gilchan-park/; /p/paul-baity/; /b/byung-jun-yoon/; /a/adolfy-hoisie/",
        "bibtex": "@inproceedings{park-etal-2025-enhancing,\n    title = \"Enhancing Future Link Prediction in Quantum Computing Semantic Networks through {LLM}-Initiated Node Features\",\n    author = \"Park, Gilchan  and\n      Baity, Paul  and\n      Yoon, Byung-Jun  and\n      Hoisie, Adolfy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.25/\",\n    pages = \"295--304\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.25.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.25/",
        "pdf_size": 1265615,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:vWLv_y24R0YJ:scholar.google.com/&scioq=Enhancing+Future+Link+Prediction+in+Quantum+Computing+Semantic+Networks+through+LLM-Initiated+Node+Features&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Brookhaven National Laboratory, Computing and Data Sciences (CDS); Brookhaven National Laboratory, Computing and Data Sciences (CDS); Brookhaven National Laboratory, Computing and Data Sciences (CDS); Brookhaven National Laboratory, Computing and Data Sciences (CDS)",
        "aff_domain": "bnl.gov; ; ; ",
        "email": "bnl.gov; ; ; ",
        "github": "https://github.com/boxorange/QC-LinkPrediction",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Brookhaven National Laboratory",
        "aff_unique_dep": "Computing and Data Sciences (CDS)",
        "aff_unique_url": "https://www.bnl.gov",
        "aff_unique_abbr": "BNL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.169",
        "title": "Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge distillation (KD) is an effective model compression method that can transfer the internal capabilities of large language models (LLMs) to smaller ones. However, the multi-modal probability distribution predicted by teacher LLMs causes difficulties for student models to learn. In this paper, we first demonstrate the importance of multi-modal distribution alignment with experiments and then highlight the inefficiency of existing KD approaches in learning multi-modal distributions. To address this problem, we propose Ranking Loss based Knowledge Distillation (RLKD), which encourages the consistency of the ranking of peak predictions between the teacher and student models. By incorporating word-level ranking loss, we ensure excellent compatibility with existing distillation objectives while fully leveraging the fine-grained information between different categories in peaks of two predicted distribution. Experimental results demonstrate that our method enables the student model to better learn the multi-modal distributions of the teacher model, leading to a significant performance improvement in various downstream tasks.",
        "author": "Tianyu Peng; Jiajun Zhang",
        "authorids": "/t/tianyu-peng/; /j/jiajun-zhang/",
        "bibtex": "@inproceedings{peng-zhang-2025-enhancing,\n    title = \"Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment\",\n    author = \"Peng, Tianyu  and\n      Zhang, Jiajun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.169/\",\n    pages = \"2478--2496\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.169.pdf",
        "site": "https://aclanthology.org/2025.coling-main.169/",
        "pdf_size": 628610,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2930969259618445656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences + Institute of Automation, Chinese Academy of Sciences + Wuhan AI Research; School of Artificial Intelligence, University of Chinese Academy of Sciences + Institute of Automation, Chinese Academy of Sciences + Wuhan AI Research + Shanghai Artificial Intelligence Laboratory, Shanghai, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/Pty72/RLKD",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1+2;0+1+2+3",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences;Wuhan AI Research;Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": "School of Artificial Intelligence;Institute of Automation;;",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ia.cas.cn;;",
        "aff_unique_abbr": "UCAS;CAS;;",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.591",
        "title": "Enhancing Large Language Models for Document-Level Translation Post-Editing Using Monolingual Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The translation capabilities of neural machine translation (NMT) models based on the encoder-decoder framework are extremely potent. Although Large Language Models (LLMs) have achieved remarkable results in many tasks, they have not reached state-of-the-art performance in NMT. However, traditional NMT still faces significant challenges in areas of document translation such as context consistency, tense, and pronoun resolution, where LLMs inherently possess substantial advantages. Instead of directly using LLMs for translation, employing them for Automatic Post-Editing (APE) to post-edit NMT outputs proves to be a viable option. However, document-level bilingual data is extremely scarce. This paper proposes a method that can effectively leverage the capabilities of LLMs to optimize document translation using only monolingual data. By employing two NMT models in opposite directions (Source-to-Target and Target-to-Source), we generate pseudo-document training data for the training of APE. We have identified and resolved the issue between training and inference mode inconsistency brought about by the pseudo-document training data. The final experimental results demonstrate that by using only document-level monolingual data, we can significantly improve the quality of NMT and greatly enhance issues such as reference and contextual consistency in NMT.",
        "author": "Zongyao Li; Zhiqiang Rao; Hengchao Shang; Jiaxin Guo; Shaojun Li; Daimeng Wei; Hao Yang",
        "authorids": "/z/zongyao-li/; /z/zhiqiang-rao/; /h/hengchao-shang/; /j/jiaxin-guo/; /s/shaojun-li/; /d/daimeng-wei/; /h/hao-yang/",
        "bibtex": "@inproceedings{li-etal-2025-enhancing-large,\n    title = \"Enhancing Large Language Models for Document-Level Translation Post-Editing Using Monolingual Data\",\n    author = \"Li, Zongyao  and\n      Rao, Zhiqiang  and\n      Shang, Hengchao  and\n      Guo, Jiaxin  and\n      Li, Shaojun  and\n      Wei, Daimeng  and\n      Yang, Hao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.591/\",\n    pages = \"8830--8840\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.591.pdf",
        "site": "https://aclanthology.org/2025.coling-main.591/",
        "pdf_size": 615428,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fL06-2p23bsJ:scholar.google.com/&scioq=Enhancing+Large+Language+Models+for+Document-Level+Translation+Post-Editing+Using+Monolingual+Data&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Huawei Translation Service Center, Beijing, China; Huawei Translation Service Center, Beijing, China; Huawei Translation Service Center, Beijing, China; Huawei Translation Service Center, Beijing, China; Huawei Translation Service Center, Beijing, China; Huawei Translation Service Center, Beijing, China; Huawei Translation Service Center, Beijing, China",
        "aff_domain": "huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Translation Service Center",
        "aff_unique_url": "https://www.huawei.com",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.22",
        "title": "Enhancing Large Language Models for Scientific Multimodal Summarization with Multimodal Output",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "The increasing integration of multimedia such as videos and graphical abstracts in scientific publications necessitates advanced summarization techniques. This paper introduces Uni-SciSum, a framework for Scientific Multimodal Summarization with Multimodal Output (SMSMO), addressing the challenges of fusing heterogeneous data sources (e.g., text, images, video, audio) and outputting multimodal summary within a unified architecture. Uni-SciSum leverages the power of large language models (LLMs) and extends its capability to cross-modal understanding through BridgeNet, a query-based transformer that fuses diverse modalities into a fixed-length embedding. A two-stage training process, involving modal-to-modal pre-training and cross-modal instruction tuning, aligns different modalities with summaries and optimizes for multimodal summary generation. Experiments on two new SMSMO datasets show Uni-SciSum outperforms uni- and multi-modality methods, advancing LLM applications in the increasingly multimodal realm of scientific communication.",
        "author": "Zusheng Tan; Xinyi Zhong; Jing-Yu Ji; Wei Jiang; Billy Chiu",
        "authorids": "/z/zusheng-tan/; /x/xinyi-zhong/; /j/jing-yu-ji/; /w/wei-jiang/; /b/billy-chiu/",
        "bibtex": "@inproceedings{tan-etal-2025-enhancing,\n    title = \"Enhancing Large Language Models for Scientific Multimodal Summarization with Multimodal Output\",\n    author = \"Tan, Zusheng  and\n      Zhong, Xinyi  and\n      Ji, Jing-Yu  and\n      Jiang, Wei  and\n      Chiu, Billy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.22/\",\n    pages = \"263--275\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.22.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.22/",
        "pdf_size": 2872186,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1438071318074478309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Data Science, Lingnan University; School of Data Science, Lingnan University; School of Data Science, Lingnan University; School of Economics, Qingdao University; School of Data Science, Lingnan University",
        "aff_domain": "ln.hk;ln.edu.hk;ln.edu.hk;pku.edu.cn;ln.edu.hk",
        "email": "ln.hk;ln.edu.hk;ln.edu.hk;pku.edu.cn;ln.edu.hk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Lingnan University;Qingdao University",
        "aff_unique_dep": "School of Data Science;School of Economics",
        "aff_unique_url": "http://www.lingnan.edu.cn;https://www.qdu.edu.cn",
        "aff_unique_abbr": "Lingnan U;QDU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.148",
        "title": "Enhancing Long-range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect-based Sentiment Analysis (ABSA) evaluates sentiments toward specific aspects of entities within the text. However, attention mechanisms and neural network models struggle with syntactic constraints. The quadratic complexity of attention mechanisms also limits their adoption for capturing long-range dependencies between aspect and opinion words in ABSA. This complexity can lead to the misinterpretation of irrelevant contextual words, restricting their effectiveness to short-range dependencies. To address the above problem, we present a novel approach to enhance long-range dependencies between aspect and opinion words in ABSA (MambaForGCN). This approach incorporates syntax-based Graph Convolutional Network (SynGCN) and MambaFormer (Mamba-Transformer) modules to encode input with dependency relations and semantic information. The Multihead Attention (MHA) and Selective State Space model (Mamba) blocks in the MambaFormer module serve as channels to enhance the model with short and long-range dependencies between aspect and opinion words. We also introduce the Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptive feature representation system that integrates SynGCN and MambaFormer and captures non-linear, complex dependencies. Experimental results on three benchmark datasets demonstrate MambaForGCN\u2019s effectiveness, outperforming state-of-the-art (SOTA) baseline models.",
        "author": "Adamu Lawan; Juhua Pu; Haruna Yunusa; Aliyu Umar; Muhammad Lawan",
        "authorids": "/a/adamu-lawan/; /j/juhua-pu/; /h/haruna-yunusa/; /a/aliyu-umar/; /m/muhammad-lawan/",
        "bibtex": "@inproceedings{lawan-etal-2025-enhancing,\n    title = \"Enhancing Long-range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-based Sentiment Analysis\",\n    author = \"Lawan, Adamu  and\n      Pu, Juhua  and\n      Yunusa, Haruna  and\n      Umar, Aliyu  and\n      Lawan, Muhammad\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.148/\",\n    pages = \"2176--2186\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.148.pdf",
        "site": "https://aclanthology.org/2025.coling-main.148/",
        "pdf_size": 1535555,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8308856209539519488&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Beihang University, Beijing, China; School of Computer Science and Technology, Beihang University, Beijing, China + National Research Institute for Teaching Materials in Information Science and Tech., China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Computing, University of Portsmouth, Portsmouth, UK; Department of Information and Communication Technology, Federal University, Gusau, Nigeria",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn; ; ",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;2;3",
        "aff_unique_norm": "Beihang University;National Research Institute for Teaching Materials in Information Science and Technology;University of Portsmouth;Federal University Gusau",
        "aff_unique_dep": "School of Computer Science and Technology;;School of Computing;Department of Information and Communication Technology",
        "aff_unique_url": "http://www.buaa.edu.cn;;https://www.port.ac.uk;",
        "aff_unique_abbr": "BUAA;;UoP;",
        "aff_campus_unique_index": "0;0;0;2;3",
        "aff_campus_unique": "Beijing;;Portsmouth;Gusau",
        "aff_country_unique_index": "0;0+0;0;1;2",
        "aff_country_unique": "China;United Kingdom;Nigeria"
    },
    {
        "id": "2025.coling-main.103",
        "title": "Enhancing Multi-party Dialogue Discourse Parsing with Explanation Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-party dialogue discourse parsing is an important and challenging task in natural language processing (NLP). Previous studies struggled to fully understand the deep semantics of dialogues, especially when dealing with complex topic interleaving and ellipsis. To address the above issues, we propose a novel model DDPE (Dialogue Discourse Parsing with Explanations) to integrate external knowledge from Large Language Models (LLMs), which consists of three components, i.e., explanation generation, structural parsing, and contrastive learning. DDPE employs LLMs to generate explanatory and contrastive information about discourse structure, thereby providing additional reasoning cues that enhance the understanding of dialogue semantics. The experimental results on the two public datasets STAC and Molweni show that our DDPE significantly outperforms the state-of-the-art (SOTA) baselines.",
        "author": "Shannan Liu; Peifeng Li; Yaxin Fan; Qiaoming Zhu",
        "authorids": "/s/shannan-liu/; /p/peifeng-li/; /y/yaxin-fan/; /q/qiaoming-zhu/",
        "bibtex": "@inproceedings{liu-etal-2025-enhancing,\n    title = \"Enhancing Multi-party Dialogue Discourse Parsing with Explanation Generation\",\n    author = \"Liu, Shannan  and\n      Li, Peifeng  and\n      Fan, Yaxin  and\n      Zhu, Qiaoming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.103/\",\n    pages = \"1531--1544\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.103.pdf",
        "site": "https://aclanthology.org/2025.coling-main.103/",
        "pdf_size": 720895,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6060105403948692613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.122",
        "title": "Enhancing Multimodal Named Entity Recognition through Adaptive Mixup Image Augmentation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal named entity recognition (MNER) extends traditional named entity recognition (NER) by integrating visual and textual information. However, current methods still face significant challenges due to the text-image mismatch problem. Recent advancements in text-to-image synthesis provide promising solutions, as synthesized images can introduce additional visual context to enhance MNER model performance. To fully leverage the benefits of both original and synthesized images, we propose an adaptive mixup image augmentation method. This method generates augmented images by determining the mixing ratio based on the matching score between the text and image, utilizing a triplet loss-based Gaussian Mixture Model (TL-GMM). Our approach is highly adaptable and can be seamlessly integrated into existing MNER models. Extensive experiments demonstrate consistent performance improvements, and detailed ablation studies and case studies confirm the effectiveness of our method.",
        "author": "Bo Xu; Haiqi Jiang; Jie Wei; Hongyu Jing; Ming Du; Hui Song; Hongya Wang; Yanghua Xiao",
        "authorids": "/b/bo-xu/; /h/haiqi-jiang/; /j/jie-wei/; /h/hongyu-jing/; /m/ming-du/; /h/hui-song/; /h/hongya-wang/; /y/yanghua-xiao/",
        "bibtex": "@inproceedings{xu-etal-2025-enhancing,\n    title = \"Enhancing Multimodal Named Entity Recognition through Adaptive Mixup Image Augmentation\",\n    author = \"Xu, Bo  and\n      Jiang, Haiqi  and\n      Wei, Jie  and\n      Jing, Hongyu  and\n      Du, Ming  and\n      Song, Hui  and\n      Wang, Hongya  and\n      Xiao, Yanghua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.122/\",\n    pages = \"1802--1812\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.122.pdf",
        "site": "https://aclanthology.org/2025.coling-main.122/",
        "pdf_size": 2631044,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hRaLd5E_IZMJ:scholar.google.com/&scioq=Enhancing+Multimodal+Named+Entity+Recognition+through+Adaptive+Mixup+Image+Augmentation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; School of Computer Science and Technology, Donghua University; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University",
        "aff_domain": "dhu.edu.cn;mail.dhu.edu.cn;mail.dhu.edu.cn;mail.dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;fudan.edu.cn",
        "email": "dhu.edu.cn;mail.dhu.edu.cn;mail.dhu.edu.cn;mail.dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;dhu.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Donghua University;Fudan University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science",
        "aff_unique_url": "https://www.dhu.edu.cn;https://www.fudan.edu.cn",
        "aff_unique_abbr": ";Fudan",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.673",
        "title": "Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores the application of large language models (LLMs) in nursing and elderly care, focusing on AI-driven patient monitoring and interaction. We introduce a novel Chinese nursing dataset and implement incremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to enhance LLM performance in specialized tasks. Using LangChain, we develop an interactable nursing assistant capable of real-time care and personalized interventions. Experimental results demonstrate significant improvements, paving the way for AI-driven solutions to meet the growing demands of healthcare in aging populations.",
        "author": "Qiao Sun; Jiexin Xie; Nanyang Ye; Qinying Gu; Shijie Guo",
        "authorids": "/q/qiao-sun/; /j/jiexin-xie/; /n/nanyang-ye/; /q/qinying-gu/; /s/shijie-guo/",
        "bibtex": "@inproceedings{sun-etal-2025-enhancing,\n    title = \"Enhancing Nursing and Elderly Care with Large Language Models: An {AI}-Driven Framework\",\n    author = \"Sun, Qiao  and\n      Xie, Jiexin  and\n      Ye, Nanyang  and\n      Gu, Qinying  and\n      Guo, Shijie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.673/\",\n    pages = \"10083--10090\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.673.pdf",
        "site": "https://aclanthology.org/2025.coling-main.673/",
        "pdf_size": 296815,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=49118144392763991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Academy for Engineering and Technology, Fudan University, China+Shanghai Artificial Intelligence Laboratory, China; Guilin University of Electronic Technology, China; Shanghai Jiao Tong University, China; Shanghai Artificial Intelligence Laboratory, China; Academy for Engineering and Technology, Fudan University, China",
        "aff_domain": "m.fudan.edu.cn;fudan.edu.cn;gmail.com;pjlab.org.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn;fudan.edu.cn;gmail.com;pjlab.org.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;3;1;0",
        "aff_unique_norm": "Fudan University;Shanghai Artificial Intelligence Laboratory;Guilin University of Electronic Technology;Shanghai Jiao Tong University",
        "aff_unique_dep": "Academy for Engineering and Technology;;;",
        "aff_unique_url": "https://www.fudan.edu.cn;;;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "Fudan;;;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.117",
        "title": "Enhancing One-Shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ retraining-free one-shot techniques to compress PLMs; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experiments demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 5.16 on Raw-Wikitext2 and improves average accuracy by 3.86% across multiple zero-shot benchmarks for LLaMA-3-8B compared to Wanda with 2:4 sparsity.",
        "author": "Guanchen Li; Xiandong Zhao; Lian Liu; Zeping Li; Yixing Xu; Dong Li; Lu Tian; Jie He; Ashish Sirasao; Emad Barsoum",
        "authorids": "/g/guanchen-li/; /x/xiandong-zhao/; /l/lian-liu/; /z/zeping-li/; /y/yixing-xu/; /d/dong-li/; /l/lu-tian/; /j/jie-he/; /a/ashish-sirasao/; /e/emad-barsoum/",
        "bibtex": "@inproceedings{li-etal-2025-enhancing,\n    title = \"Enhancing One-Shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism\",\n    author = \"Li, Guanchen  and\n      Zhao, Xiandong  and\n      Liu, Lian  and\n      Li, Zeping  and\n      Xu, Yixing  and\n      Li, Dong  and\n      Tian, Lu  and\n      He, Jie  and\n      Sirasao, Ashish  and\n      Barsoum, Emad\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.117/\",\n    pages = \"1718--1735\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.117.pdf",
        "site": "https://aclanthology.org/2025.coling-main.117/",
        "pdf_size": 1093718,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YegPI6E-Og0J:scholar.google.com/&scioq=Enhancing+One-Shot+Pruned+Pre-trained+Language+Models+through+Sparse-Dense-Sparse+Mechanism&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.; University of Science and Technology Beijing; Advanced Micro Devices, Inc.; Advanced Micro Devices, Inc.",
        "aff_domain": "amd.com;amd.com; ; ; ; ; ; ; ; ",
        "email": "amd.com;amd.com; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;1;0;0",
        "aff_unique_norm": "Advanced Micro Devices;University of Science and Technology Beijing",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amd.com;http://www.ustb.edu.cn",
        "aff_unique_abbr": "AMD;USTB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2025.coling-main.160",
        "title": "Enhancing Online Grooming Detection via Backtranslation Augmentation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Grooming minors for sexual exploitation become an increasingly significant concern in online conversation platforms. For a safer online experience for minors, machine learning models have been proposed to tap into explicit textual remarks and automate detecting predatory conversations. Such models, however, fall short of real-world applications for the sparse distribution of predatory conversations. In this paper, we propose backtranslation augmentation to augment training datasets with more predatory conversations. Through our experiments on 8 languages from 4 language families using 3 neural translators, we demonstrate that backtranslation augmentation improves models\u2019 performance with fewer training epochs for better classification efficacy. Our code and experimental results are available at https://github.com/fani-lab/osprey/tree/coling25.",
        "author": "Hamed Waezi; Hossein Fani",
        "authorids": "/h/hamed-waezi/; /h/hossein-fani/",
        "bibtex": "@inproceedings{waezi-fani-2025-enhancing,\n    title = \"Enhancing Online Grooming Detection via Backtranslation Augmentation\",\n    author = \"Waezi, Hamed  and\n      Fani, Hossein\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.160/\",\n    pages = \"2340--2350\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.160.pdf",
        "site": "https://aclanthology.org/2025.coling-main.160/",
        "pdf_size": 472283,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:a1Q-xTDRYUQJ:scholar.google.com/&scioq=Enhancing+Online+Grooming+Detection+via+Backtranslation+Augmentation&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "School of Computer Science, University of Windsor, Windsor, ON, Canada; School of Computer Science, University of Windsor, Windsor, ON, Canada",
        "aff_domain": "uwindsor.ca;uwindsor.ca",
        "email": "uwindsor.ca;uwindsor.ca",
        "github": "github.com/fani-lab/osprey/tree/coling25",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Windsor",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.uwindsor.ca",
        "aff_unique_abbr": "UWindsor",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Windsor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-main.45",
        "title": "Enhancing Reranking for Recommendation with LLMs through User Preference Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, large language models (LLMs) have shown the potential to enhance recommendations due to their sufficient knowledge and remarkable summarization ability. However, the existing LLM-powered recommendation may create redundant output, which generates irrelevant information about the user\u2019s preferences on candidate items from user behavior sequences. To address the issues, we propose a framework UR4Rec that enhances reranking for recommendation with large language models through user preference retrieval. Specifically, UR4Rec develops a small transformer-based user preference retriever towards candidate items to build the bridge between LLMs and recommendation, which focuses on producing the essential knowledge through LLMs from user behavior sequences to enhance reranking for recommendation. Our experimental results on three real-world public datasets demonstrate the superiority of UR4Rec over existing baseline models.",
        "author": "Haobo Zhang; Qiannan Zhu; Zhicheng Dou",
        "authorids": "/h/haobo-zhang/; /q/qiannan-zhu/; /z/zhicheng-dou/",
        "bibtex": "@inproceedings{zhang-etal-2025-enhancing,\n    title = \"Enhancing Reranking for Recommendation with {LLM}s through User Preference Retrieval\",\n    author = \"Zhang, Haobo  and\n      Zhu, Qiannan  and\n      Dou, Zhicheng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.45/\",\n    pages = \"658--671\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.45.pdf",
        "site": "https://aclanthology.org/2025.coling-main.45/",
        "pdf_size": 732606,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4057896125433486116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; School of Artificial Intelligence, Beijing Normal University + Engineering Research Center of Intelligent Technology and Educational Application, MOE, China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn;bnu.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;bnu.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Renmin University of China;Beijing Normal University;Engineering Research Center of Intelligent Technology and Educational Application",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.bnu.edu.cn;",
        "aff_unique_abbr": "RUC;BNU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.449",
        "title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-Augmented Generation (RAG) systems have recently shown remarkable advancements by integrating retrieval mechanisms into language models, enhancing their ability to produce more accurate and contextually relevant responses. However, the influence of various components and configurations within RAG systems remains underexplored. A comprehensive understanding of these elements is essential for tailoring RAG systems to complex retrieval tasks and ensuring optimal performance across diverse applications. In this paper, we develop several advanced RAG system designs that incorporate query expansion, various novel retrieval strategies, and a novel Contrastive In-Context Learning RAG. Our study systematically investigates key factors, including language model size, prompt design, document chunk size, knowledge base size, retrieval stride, query expansion techniques, Contrastive In-Context Learning knowledge bases, multilingual knowledge bases, and Focus Mode retrieving relevant context at sentence-level. Through extensive experimentation, we provide a detailed analysis of how these factors influence response quality. Our findings offer actionable insights for developing RAG systems, striking a balance between contextual richness and retrieval-generation efficiency, thereby paving the way for more adaptable and high-performing RAG frameworks in diverse real-world scenarios. Our code and implementation details are publicly available.",
        "author": "Siran Li; Linus Stenzel; Carsten Eickhoff; Seyed Ali Bahrainian",
        "authorids": "/s/siran-li/; /l/linus-stenzel/; /c/carsten-eickhoff/; /s/seyed-ali-bahrainian/",
        "bibtex": "@inproceedings{li-etal-2025-enhancing-retrieval,\n    title = \"Enhancing Retrieval-Augmented Generation: A Study of Best Practices\",\n    author = \"Li, Siran  and\n      Stenzel, Linus  and\n      Eickhoff, Carsten  and\n      Bahrainian, Seyed Ali\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.449/\",\n    pages = \"6705--6717\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.449.pdf",
        "site": "https://aclanthology.org/2025.coling-main.449/",
        "pdf_size": 434086,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14529543501657408687&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of T\u00fcbingen; University of T\u00fcbingen; University of T\u00fcbingen; University of T\u00fcbingen",
        "aff_domain": "uni-tuebingen.de;student.uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "email": "uni-tuebingen.de;student.uni-tuebingen.de;uni-tuebingen.de;uni-tuebingen.de",
        "github": "https://github.com/ali-bahrainian/RAG_best_practices",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of T\u00fcbingen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "Uni T\u00fcbingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.587",
        "title": "Enhancing Rhetorical Figure Annotation: An Ontology-Based Web Application with RAG Integration",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Rhetorical figures play an important role in our communication. They are used to convey subtle, implicit meaning, or to emphasize statements. We notice them in hate speech, fake news, and propaganda. By improving the systems for computational detection of rhetorical figures, we can also improve tasks such as hate speech and fake news detection, sentiment analysis, opinion mining, or argument mining. Unfortunately, there is a lack of annotated data, as well as qualified annotators that would help us build large corpora to train machine learning models for the detection of rhetorical figures. The situation is particularly difficult in languages other than English, and for rhetorical figures other than metaphor, sarcasm, and irony. To overcome this issue, we develop a web application called \u201cFind your Figure\u201d that facilitates the identification and annotation of German rhetorical figures. The application is based on the German Rhetorical ontology GRhOOT which we have specially adapted for this purpose. In addition, we improve the user experience with Retrieval Augmented Generation (RAG). In this paper, we present the restructuring of the ontology, the development of the web application, and the built-in RAG pipeline. We also identify the optimal RAG settings for our application. Our approach is one of the first to practically use rhetorical ontologies in combination with RAG and shows promising results.",
        "author": "Ramona K\u00fchn; Jelena Mitrovi\u0107; Michael Granitzer",
        "authorids": "/r/ramona-kuhn/; /j/jelena-mitrovic/; /m/michael-granitzer/",
        "bibtex": "@inproceedings{kuhn-etal-2025-enhancing,\n    title = \"Enhancing Rhetorical Figure Annotation: An Ontology-Based Web Application with {RAG} Integration\",\n    author = {K{\\\"u}hn, Ramona  and\n      Mitrovi{\\'c}, Jelena  and\n      Granitzer, Michael},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.587/\",\n    pages = \"8774--8786\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.587.pdf",
        "site": "https://aclanthology.org/2025.coling-main.587/",
        "pdf_size": 609290,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:iJui0QAurS4J:scholar.google.com/&scioq=Enhancing+Rhetorical+Figure+Annotation:+An+Ontology-Based+Web+Application+with+RAG+Integration&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "University of Passau; University of Passau+Institute for AI Research and Development of Serbia; University of Passau",
        "aff_domain": "uni-passau.de;uni-passau.de;uni-passau.de",
        "email": "uni-passau.de;uni-passau.de;uni-passau.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Passau;Institute for AI Research and Development",
        "aff_unique_dep": ";AI Research and Development",
        "aff_unique_url": "https://www.uni-passau.de;",
        "aff_unique_abbr": "UP;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Germany;Serbia"
    },
    {
        "id": "2025.coling-main.478",
        "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pretrained Language Models (PLMs) have excelled in various Natural Language Processing tasks, benefiting from large-scale pretraining and self-attention mechanism\u2019s ability to capture long-range dependencies. However, their performance on social media application tasks like rumor detection remains suboptimal. We attribute this to mismatches between pretraining corpora and social texts, inadequate handling of unique social symbols, and pretraining tasks ill-suited for modeling user engagements implicit in propagation structures. To address these issues, we propose a continue pretraining strategy called Post Engagement Prediction (PEP) to infuse information from propagation structures into PLMs. PEP makes models to predict root, branch, and parent relations between posts, capturing interactions of stance and sentiment crucial for rumor detection. We also curate and release large-scale Twitter corpus: TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments demonstrate PEP significantly boosts rumor detection performance across universal and social media PLMs, even in few-shot scenarios. On benchmark datasets, PEP enhances baseline models by 1.0-3.7% accuracy, even enabling it to outperform current state-of-the-art methods on multiple datasets. SoLM alone, without high-level modules, also achieves competitive results, highlighting the strategy\u2019s effectiveness in learning discriminative post interaction features.",
        "author": "Chaoqun Cui; Siyuan Li; Kunkun Ma; Caiyan Jia",
        "authorids": "/c/chaoqun-cui/; /s/siyuan-li/; /k/kunkun-ma/; /c/caiyan-jia/",
        "bibtex": "@inproceedings{cui-etal-2025-enhancing,\n    title = \"Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model\",\n    author = \"Cui, Chaoqun  and\n      Li, Siyuan  and\n      Ma, Kunkun  and\n      Jia, Caiyan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.478/\",\n    pages = \"7165--7179\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.478.pdf",
        "site": "https://aclanthology.org/2025.coling-main.478/",
        "pdf_size": 887669,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:2-xN5C-gSG0J:scholar.google.com/&scioq=Enhancing+Rumor+Detection+Methods+with+Propagation+Structure+Infused+Language+Model&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology & Beijing Key Lab of Traffic Data Analysis and Mining Beijing Jiaotong University, Beijing 100044, China; School of Computer Science and Technology & Beijing Key Lab of Traffic Data Analysis and Mining Beijing Jiaotong University, Beijing 100044, China; School of Computer Science and Technology & Beijing Key Lab of Traffic Data Analysis and Mining Beijing Jiaotong University, Beijing 100044, China; School of Computer Science and Technology & Beijing Key Lab of Traffic Data Analysis and Mining Beijing Jiaotong University, Beijing 100044, China",
        "aff_domain": "gmail.com;bjtu.edu.cn; ;bjtu.edu.cn",
        "email": "gmail.com;bjtu.edu.cn; ;bjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing Jiaotong University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.bjtu.edu.cn",
        "aff_unique_abbr": "BJTU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.513",
        "title": "Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Human tutoring interventions play a crucial role in supporting student learning, improving academic performance, and promoting personal growth. This paper focuses on analyzing mathematics tutoring discourse using talk moves\u2014a framework of dialogue acts grounded in Accountable Talk theory. However, scaling the collection, annotation, and analysis of extensive tutoring dialogues to develop machine learning models is a challenging and resource-intensive task. To address this, we present SAGA22, a compact dataset, and explore various modeling strategies, including dialogue context, speaker information, pretraining datasets, and further fine-tuning. By leveraging existing datasets and models designed for classroom teaching, our results demonstrate that supplementary pretraining on classroom data enhances model performance in tutoring settings, particularly when incorporating longer context and speaker information. Additionally, we conduct extensive ablation studies to underscore the challenges in talk move modeling.",
        "author": "Jie Cao; Abhijit Suresh; Jennifer Jacobs; Charis Clevenger; Amanda Howard; Chelsea Brown; Brent Milne; Tom Fischaber; Tamara Sumner; James H. Martin",
        "authorids": "/j/jie-cao/; /a/abhijit-suresh/; /j/jennifer-jacobs/; /c/charis-clevenger/; /a/amanda-howard/; /c/chelsea-brown/; /b/brent-milne/; /t/tom-fischaber/; /t/tamara-sumner/; /j/james-h-martin/",
        "bibtex": "@inproceedings{cao-etal-2025-enhancing,\n    title = \"Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse\",\n    author = \"Cao, Jie  and\n      Suresh, Abhijit  and\n      Jacobs, Jennifer  and\n      Clevenger, Charis  and\n      Howard, Amanda  and\n      Brown, Chelsea  and\n      Milne, Brent  and\n      Fischaber, Tom  and\n      Sumner, Tamara  and\n      Martin, James H.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.513/\",\n    pages = \"7671--7684\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.513.pdf",
        "site": "https://aclanthology.org/2025.coling-main.513/",
        "pdf_size": 845774,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12136855801506209138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, University of Oklahoma + Institute of Cognitive Science, University of Colorado Boulder; Institute of Cognitive Science, University of Colorado Boulder; Institute of Cognitive Science, University of Colorado Boulder; Institute of Cognitive Science, University of Colorado Boulder; Institute of Cognitive Science, University of Colorado Boulder; Institute of Cognitive Science, University of Colorado Boulder; Saga Education; Saga Education; Institute of Cognitive Science, University of Colorado Boulder; Institute of Cognitive Science, University of Colorado Boulder",
        "aff_domain": "ou.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu;saga.org;saga.org;colorado.edu;colorado.edu",
        "email": "ou.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu;saga.org;saga.org;colorado.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;1;1;1;1;2;2;1;1",
        "aff_unique_norm": "University of Oklahoma;University of Colorado Boulder;Saga Education",
        "aff_unique_dep": "School of Computer Science;Institute of Cognitive Science;",
        "aff_unique_url": "https://www.ou.edu;https://www.colorado.edu;https://www.sagaeducation.org",
        "aff_unique_abbr": "OU;UCB;",
        "aff_campus_unique_index": "1;1;1;1;1;1;1;1",
        "aff_campus_unique": ";Boulder",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.137",
        "title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chain-of-thought (CoT) prompting has significantly enhanced the the capability of large language models (LLMs) by structuring their reasoning processes. However, existing methods face critical limitations: handcrafted demonstrations require extensive human expertise, while trigger phrases are prone to inaccuracies. In this paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method, a novel approach that improves CoT prompting by utilizing uncertainty estimates to select effective demonstrations without needing access to model parameters. Unlike traditional methods, ZEUS offers high sensitivity in distinguishing between helpful and ineffective questions, ensuring more precise and reliable selection. Our extensive evaluation shows that ZEUS consistently outperforms existing CoT strategies across four challenging reasoning benchmarks, demonstrating its robustness and scalability.",
        "author": "Shanu Kumar; Saish Mendke; Karody Lubna Abdul Rahman; Santosh Kurasa; Parag Agrawal; Sandipan Dandapat",
        "authorids": "/s/shanu-kumar/; /s/saish-mendke/; /k/karody-lubna-abdul-rahman/; /s/santosh-kurasa/; /p/parag-agrawal/; /s/sandipan-dandapat/",
        "bibtex": "@inproceedings{kumar-etal-2025-enhancing,\n    title = \"Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection\",\n    author = \"Kumar, Shanu  and\n      Mendke, Saish  and\n      Rahman, Karody Lubna Abdul  and\n      Kurasa, Santosh  and\n      Agrawal, Parag  and\n      Dandapat, Sandipan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.137/\",\n    pages = \"2003--2025\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.137.pdf",
        "site": "https://aclanthology.org/2025.coling-main.137/",
        "pdf_size": 3076471,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:vpZ3HtJYgn4J:scholar.google.com/&scioq=Enhancing+Zero-shot+Chain+of+Thought+Prompting+via+Uncertainty-Guided+Strategy+Selection&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.65",
        "title": "Enhancing multi-modal Relation Extraction with Reinforcement Learning Guided Graph Diffusion Framework",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the massive growth of multi-modal information such as text, images, and other data, how should we analyze and align these data becomes very important. In our work, we introduce a new framework based on Reinforcement Learning Guided Graph Diffusion to address the complexity of multi-modal graphs and enhance the interpretability, making it clearer to understand the alignment of multi-modal information. Our approach leverages pre-trained models to encode multi-modal data into scene graphs and combines them into a cross-modal graph (CMG). We design a reinforcement learning agent to filter nodes and modify edges based on the observation of the graph state to dynamically adjust the graph structure, providing coarse-grained refinement. Then we will iteratively optimize edge weights and node selection to achieve fine-grained adjustment. We conduct extensive experimental results on multi-modal relation extraction task datasets and show that our model significantly outperforms existing multi-modal methods such as MEGA and MKGFormer. We also conduct an ablation study to demonstrate the importance of each key component, showing that performance drops significantly when any key element is removed. Our method uses reinforcement learning methods to better mine potential multi-modal information relevance, and adjustments based on graph structure make our method more interpretable.",
        "author": "Rui Yang; Rajiv Gupta",
        "authorids": "/r/rui-yang/; /r/rajiv-gupta/",
        "bibtex": "@inproceedings{yang-gupta-2025-enhancing,\n    title = \"Enhancing multi-modal Relation Extraction with Reinforcement Learning Guided Graph Diffusion Framework\",\n    author = \"Yang, Rui  and\n      Gupta, Rajiv\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.65/\",\n    pages = \"978--988\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.65.pdf",
        "site": "https://aclanthology.org/2025.coling-main.65/",
        "pdf_size": 888063,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11152587765857268319&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of California, Riverside; University of California, Riverside",
        "aff_domain": "ucr.edu;ucr.edu",
        "email": "ucr.edu;ucr.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.609",
        "title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Advances in prompt engineering and fine-tuning techniques have further enhanced their ability to address complex reasoning challenges. However, these advanced capabilities are often exclusive to models exceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning methods have been explored for smaller models (under 10 billion parameters), they typically depend on extensive CoT training data, which can introduce inconsistencies and limit effectiveness in low-data settings. To overcome these limitations, this paper introduce a new reasoning strategy Solution Guidance (SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT) for enhancing the reasoning capabilities of small language models. SG focuses on problem understanding and decomposition at the semantic and logical levels, rather than specific computations, which can effectively improve the SLMs\u2019 generalization and reasoning abilities. With only a small amount of SG training data, SGFT can fine-tune a SLM to produce accurate problem-solving guidances, which can then be flexibly fed to any SLM as prompts, enabling it to generate correct answers directly. Experimental results demonstrate that our method significantly improves the performance of SLMs on various reasoning tasks, enhancing both their practicality and efficiency within resource-constrained environments.",
        "author": "Jing Bi; Yuting Wu; Weiwei Xing; Zhenjie Wei",
        "authorids": "/j/jing-bi/; /y/yuting-wu/; /w/weiwei-xing/; /z/zhenjie-wei/",
        "bibtex": "@inproceedings{bi-etal-2025-enhancing,\n    title = \"Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning\",\n    author = \"Bi, Jing  and\n      Wu, Yuting  and\n      Xing, Weiwei  and\n      Wei, Zhenjie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.609/\",\n    pages = \"9074--9084\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.609.pdf",
        "site": "https://aclanthology.org/2025.coling-main.609/",
        "pdf_size": 582049,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17959030843784352031&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China",
        "aff_domain": "foxmail.com;bjtu.edu.cn;bjtu.edu.cn;outlook.com",
        "email": "foxmail.com;bjtu.edu.cn;bjtu.edu.cn;outlook.com",
        "github": "https://github.com/BiJings/SGFT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing Jiaotong University",
        "aff_unique_dep": "School of Software Engineering",
        "aff_unique_url": "http://www.bjtu.edu.cn",
        "aff_unique_abbr": "BJTU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.439",
        "title": "Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination \u2013 generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality in decoding by leveraging LLMs\u2019 hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting logits from a lower layer with the final layer to exploit information related factuality within the model forward procedure. However, such methods often assume the final layer is most reliable one and the lower layer selection process depends on it. In this work, we first propose logit extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
        "author": "Souvik Das; Lifeng Jin; Linfeng Song; Haitao Mi; Baolin Peng; Dong Yu",
        "authorids": "/s/souvik-das/; /l/lifeng-jin/; /l/linfeng-song/; /h/haitao-mi/; /b/baolin-peng/; /d/dong-yu/",
        "bibtex": "@inproceedings{das-etal-2025-entropy,\n    title = \"Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models\",\n    author = \"Das, Souvik  and\n      Jin, Lifeng  and\n      Song, Linfeng  and\n      Mi, Haitao  and\n      Peng, Baolin  and\n      Yu, Dong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.439/\",\n    pages = \"6589--6600\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.439.pdf",
        "site": "https://aclanthology.org/2025.coling-main.439/",
        "pdf_size": 1794828,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14219126294233654826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, University at Buffalo, NY + Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA",
        "aff_domain": "buffalo.edu; ; ; ; ; ",
        "email": "buffalo.edu; ; ; ; ; ",
        "github": "https://github.com/souvikdgp16/extrapolative_decoding",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "University at Buffalo;Tencent",
        "aff_unique_dep": "Department of Computer Science and Engineering;AI Lab",
        "aff_unique_url": "https://www.buffalo.edu;https://ai.tencent.com",
        "aff_unique_abbr": "UB;Tencent AI Lab",
        "aff_campus_unique_index": "0+1;1;1;1;1;1",
        "aff_campus_unique": "Buffalo;Bellevue",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.330",
        "title": "Evaluating Generalization Capability of Language Models across Abductive, Deductive and Inductive Logical Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Transformer-based language models (LMs) have demonstrated remarkable performance on many natural language tasks, yet to what extent LMs possess the capability of generalizing to unseen logical rules remains not explored sufficiently. In classical logic category, abductive, deductive and inductive (ADI) reasoning are defined as the fundamental reasoning types, sharing the identical reasoning primitives and properties, and some research have proposed that there exists mutual generalization across them. However, in the field of natural language processing, previous research generally study LMs\u2019 ADI reasoning capabilities separately, overlooking the generalization across them. To bridge this gap, we propose UniADILR, a novel logical reasoning dataset crafted for assessing the generalization capabilities of LMs across different logical rules. Based on UniADILR, we conduct extensive investigations from various perspectives of LMs\u2019 performance on ADI reasoning. The experimental results reveal the weakness of current LMs in terms of extrapolating to unseen rules and inspire a new insight for future research in logical reasoning.",
        "author": "Yu Sheng; Wanting Wen; Linjing Li; Daniel Zeng",
        "authorids": "/y/yu-sheng/; /w/wanting-wen/; /l/linjing-li/; /d/daniel-zeng/",
        "bibtex": "https://aclanthology.org/2025.coling-main.330.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.330.pdf",
        "site": "https://aclanthology.org/2025.coling-main.330/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:u_7PPmriHW8J:scholar.google.com/&scioq=Evaluating+Generalization+Capability+of+Language+Models+across+Abductive,+Deductive+and+Inductive+Logical+Reasoning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.466",
        "title": "Evaluating LLMs\u2019 Capability to Identify Lexical Semantic Equivalence: Probing with the Word-in-Context Task",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study proposes a method to evaluate the capability of large language models (LLMs) in identifying lexical semantic equivalence. The Word-in-Context (WiC) task, a benchmark designed to determine whether the meanings of a target word remain identical across different contexts, is employed as a probing task. Experiments are conducted with several LLMs, including proprietary GPT models and open-source models, using zero-shot prompting with adjectives that represent varying levels of semantic equivalence (e.g., \u201cthe same\u201d) or inequivalence (e.g., \u201cdifferent\u201d). The fundamental capability to identify lexical semantic equivalence in context is measured using standard accuracy metrics. Consistency across different levels of semantic equivalence is assessed via rank correlation with the expected canonical ranking of precision and recall, reflecting anticipated trends in performance across prompts. The proposed method demonstrates its effectiveness, highlighting the superior capability of GPT-4o, as it consistently outperforms other explored LLMs. Analysis of the WiC dataset, the discriminative properties of adjectives (i.e., their ability to differentiate between levels of semantic equivalence), and linguistic patterns in erroneous cases offer insights into the LLM\u2019s capability and sensitivity. These findings could inform improvements in WiC task performance, although performance enhancement is not the primary focus of this study.",
        "author": "Yoshihiko Hayashi",
        "authorids": "/y/yoshihiko-hayashi/",
        "bibtex": "@inproceedings{hayashi-2025-evaluating,\n    title = \"Evaluating {LLM}s' Capability to Identify Lexical Semantic Equivalence: Probing with the Word-in-Context Task\",\n    author = \"Hayashi, Yoshihiko\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.466/\",\n    pages = \"6985--6998\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.466.pdf",
        "site": "https://aclanthology.org/2025.coling-main.466/",
        "pdf_size": 469780,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6318212033524596410&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Perceptual Computing Laboratory, Waseda University",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Waseda University",
        "aff_unique_dep": "Perceptual Computing Laboratory",
        "aff_unique_url": "https://www.waseda.jp/top",
        "aff_unique_abbr": "Waseda",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.757",
        "title": "Evaluating Model Alignment with Human Perception: A Study on Shitsukan in LLMs and LVLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We evaluate the alignment of large language models (LLMs) and large vision-language models (LVLMs) with human perception, focusing on the Japanese concept of *shitsukan*, which reflects the sensory experience of perceiving objects. We created a dataset of *shitsukan* terms elicited from individuals in response to object images. With it, we designed benchmark tasks for three dimensions of understanding *shitsukan*: (1) accurate perception in object images, (2) commonsense knowledge of typical *shitsukan* terms for objects, and (3) distinction of valid *shitsukan* terms. Models demonstrated mixed accuracy across benchmark tasks, with limited overlap between model- and human-generated terms. However, manual evaluations revealed that the model-generated terms were still natural to humans. This work identifies gaps in culture-specific understanding and contributes to aligning models with human sensory perception. We publicly release the dataset to encourage further research in this area.",
        "author": "Daiki Shiono; Ana Brassard; Yukiko Ishizuki; Jun Suzuki",
        "authorids": "/d/daiki-shiono/; /a/ana-brassard/; /y/yukiko-ishizuki/; /j/jun-suzuki/",
        "bibtex": "@inproceedings{shiono-etal-2025-evaluating,\n    title = \"Evaluating Model Alignment with Human Perception: A Study on Shitsukan in {LLM}s and {LVLM}s\",\n    author = \"Shiono, Daiki  and\n      Brassard, Ana  and\n      Ishizuki, Yukiko  and\n      Suzuki, Jun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.757/\",\n    pages = \"11428--11444\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.757.pdf",
        "site": "https://aclanthology.org/2025.coling-main.757/",
        "pdf_size": 10807356,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:u1HpnTUDakEJ:scholar.google.com/&scioq=Evaluating+Model+Alignment+with+Human+Perception:+A+Study+on+Shitsukan+in+LLMs+and+LVLMs&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "Tohoku University+RIKEN+National Institute of Informatics; RIKEN+Tohoku University; Tohoku University+RIKEN; Tohoku University+RIKEN+National Institute of Informatics",
        "aff_domain": "dc.tohoku.ac.jp;riken.jp;dc.tohoku.ac.jp;tohoku.ac.jp",
        "email": "dc.tohoku.ac.jp;riken.jp;dc.tohoku.ac.jp;tohoku.ac.jp",
        "github": "cl-tohoku/shitsukan-eval",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;1+0;0+1;0+1+2",
        "aff_unique_norm": "Tohoku University;RIKEN;National Institute of Informatics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.riken.jp;https://www.nii.ac.jp/",
        "aff_unique_abbr": "Tohoku U;RIKEN;NII",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.336",
        "title": "Evaluating Open-Source ASR Systems: Performance Across Diverse Audio Conditions and Error Correction Methods",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite significant advances in automatic speech recognition (ASR) accuracy, challenges remain. Naturally occurring conversation often involves multiple overlapping speakers, of different ages, accents and genders, as well as noisy environments and suboptimal audio recording equipment, all of which reduce ASR accuracy. In this study, we evaluate the accuracy of state of the art open source ASR systems across diverse conversational speech datasets, examining the impact of audio and speaker characteristics on WER. We then explore the potential of ASR ensembling and post-ASR correction methods to improve transcription accuracy. Our findings emphasize the need for robust error correction techniques and of continuing to address demographic biases to enhance ASR performance and inclusivity.",
        "author": "Saki Imai; Tahiya Chowdhury; Amanda J. Stent",
        "authorids": "/s/saki-imai/; /t/tahiya-chowdhury/; /a/amanda-stent/",
        "bibtex": "@inproceedings{imai-etal-2025-evaluating,\n    title = \"Evaluating Open-Source {ASR} Systems: Performance Across Diverse Audio Conditions and Error Correction Methods\",\n    author = \"Imai, Saki  and\n      Chowdhury, Tahiya  and\n      Stent, Amanda J.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.336/\",\n    pages = \"5027--5039\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.336.pdf",
        "site": "https://aclanthology.org/2025.coling-main.336/",
        "pdf_size": 3875991,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:jcNUJhLHSJ8J:scholar.google.com/&scioq=Evaluating+Open-Source+ASR+Systems:+Performance+Across+Diverse+Audio+Conditions+and+Error+Correction+Methods&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Northeastern University + Colby College; Colby College; Colby College",
        "aff_domain": "northeastern.edu;colby.edu;gmail.com",
        "email": "northeastern.edu;colby.edu;gmail.com",
        "github": "https://github.com/sakimai/asr-evaluations",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Northeastern University;Colby College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.northeastern.edu;https://www.colby.edu",
        "aff_unique_abbr": "NEU;Colby",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.427",
        "title": "Evaluating Pixel Language Models on Non-Standardized Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We explore the potential of pixel-based models for transfer learning from standard languages to dialects. These models convert text into images that are divided into patches, enabling a continuous vocabulary representation that proves especially useful for out-of-vocabulary words common in dialectal data. Using German as a case study, we compare the performance of pixel-based models to token-based models across various syntactic and semantic tasks. Our results show that pixel-based models outperform token-based models in part-of-speech tagging, dependency parsing and intent detection for zero-shot dialect evaluation by up to 26 percentage points in some scenarios, though not in Standard German. However, pixel-based models fall short in topic classification. These findings emphasize the potential of pixel-based models for handling dialectal data, though further research should be conducted to assess their effectiveness in various linguistic contexts.",
        "author": "Alberto Mu\u00f1oz-Ortiz; Verena Blaschke; Barbara Plank",
        "authorids": "/a/alberto-munoz-ortiz/; /v/verena-blaschke/; /b/barbara-plank/",
        "bibtex": "@inproceedings{munoz-ortiz-etal-2025-evaluating,\n    title = \"Evaluating Pixel Language Models on Non-Standardized Languages\",\n    author = \"Mu{\\~n}oz-Ortiz, Alberto  and\n      Blaschke, Verena  and\n      Plank, Barbara\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.427/\",\n    pages = \"6412--6419\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.427.pdf",
        "site": "https://aclanthology.org/2025.coling-main.427/",
        "pdf_size": 267549,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17713125369603449430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Universidade da Coru\u00f1a, CITIC, Spain; LMU Munich, Center for Information and Language Processing (CIS), Germany + Munich Center for Machine Learning (MCML), Germany; LMU Munich, Center for Information and Language Processing (CIS), Germany + Munich Center for Machine Learning (MCML), Germany",
        "aff_domain": "udc.es;lmu.de;lmu.de",
        "email": "udc.es;lmu.de;lmu.de",
        "github": "",
        "project": "https://huggingface.co/amunozo/pixel-base-german",
        "author_num": 3,
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "Universidade da Coru\u00f1a;LMU Munich;Munich Center for Machine Learning",
        "aff_unique_dep": "CITIC;Center for Information and Language Processing (CIS);Center for Machine Learning",
        "aff_unique_url": "https://www.udc.es;https://www.lmu.de;https://www.munich-center-for-machine-learning.de",
        "aff_unique_abbr": ";LMU;MCML",
        "aff_campus_unique_index": "1+1;1+1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0;1+1;1+1",
        "aff_country_unique": "Spain;Germany"
    },
    {
        "id": "2025.coling-main.405",
        "title": "Evaluating Readability Metrics for German Medical Text Simplification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Clinical reports and scientific health information sources are usually written for medical experts preventing patients from understanding the main messages of these texts. Making them comprehensible for patients is important to enable patients to make informed health decisions. Metrics are required to assess readability and to evaluate text simplification methods. However, research has mainly focused on English medical texts. We collected a set of 18 statistical, part-of-speech-based, syntactic, semantic and fluency metrics from related studies and evaluate their suitability to measure readability of German medical texts. We perform multiple t-tests on technical abstracts from English and German scientific articles and related simplified summaries, respectively. While semantic and fluency metrics can be successfully transferred to German medical texts, multiple statistical, part-of-speech-based, and syntactic metrics behave differently when they are applied to German medical texts requiring careful interpretation.",
        "author": "Karen Scholz; Markus Wenzel",
        "authorids": "/k/karen-scholz/; /m/markus-wenzel/",
        "bibtex": "@inproceedings{scholz-wenzel-2025-evaluating,\n    title = \"Evaluating Readability Metrics for {G}erman Medical Text Simplification\",\n    author = \"Scholz, Karen  and\n      Wenzel, Markus\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.405/\",\n    pages = \"6049--6062\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.405.pdf",
        "site": "https://aclanthology.org/2025.coling-main.405/",
        "pdf_size": 1502486,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dNza73YpdJEJ:scholar.google.com/&scioq=Evaluating+Readability+Metrics+for+German+Medical+Text+Simplification&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany + University of Bremen, Faculty 3: Mathematics/Computer Science, Germany; Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany",
        "aff_domain": "mevis.fraunhofer.de;mevis.fraunhofer.de",
        "email": "mevis.fraunhofer.de;mevis.fraunhofer.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Fraunhofer Institute for Digital Medicine MEVIS;University of Bremen",
        "aff_unique_dep": ";Faculty 3: Mathematics/Computer Science",
        "aff_unique_url": "https://www.mevis.fraunhofer.de;https://www.uni-bremen.de",
        "aff_unique_abbr": "MEVIS;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bremen;",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.690",
        "title": "Evaluating Transformers for OCR Post-Correction in Early Modern Dutch Theatre",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores the effectiveness of two types of transformer models \u2014 large generative models and sequence-to-sequence models \u2014 for automatically post-correcting Optical Character Recognition (OCR) output in early modern Dutch plays. To address the need for optimally aligned data, we create a parallel dataset based on the OCRed and ground truth versions from the EmDComF corpus using state-of-the-art alignment techniques. By combining character-based and semantic methods, we design and release a qualitative OCR-to-gold parallel dataset, selecting the alignment with the lowest Character Error Rate (CER) for all alignment pairs. We then fine-tune and evaluate five generative models and four sequence-to-sequence models on the OCR post-correction dataset. Results show that sequence-to-sequence models generally outperform generative models in this task, correcting more OCR errors and overgenerating and undergenerating less, with mBART as the best performing system.",
        "author": "Florian Debaene; Aaron Maladry; Els Lefever; Veronique Hoste",
        "authorids": "/f/florian-debaene/; /a/aaron-maladry/; /e/els-lefever/; /v/veronique-hoste/",
        "bibtex": "@inproceedings{debaene-etal-2025-evaluating,\n    title = \"Evaluating Transformers for {OCR} Post-Correction in Early {M}odern {D}utch Theatre\",\n    author = \"Debaene, Florian  and\n      Maladry, Aaron  and\n      Lefever, Els  and\n      Hoste, Veronique\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.690/\",\n    pages = \"10367--10374\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.690.pdf",
        "site": "https://aclanthology.org/2025.coling-main.690/",
        "pdf_size": 150155,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=94345744381669140&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.237",
        "title": "Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) show promising learning and reasoning abilities. Compared to other NLP tasks, multilingual and multi-label emotion evaluation tasks are under-explored in LLMs. In this paper, we present EthioEmo, a multi-label emotion classification dataset for four Ethiopian languages, namely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). We perform extensive experiments with an additional English multi-label emotion dataset from SemEval 2018 Task 1. Our evaluation includes encoder-only, encoder-decoder, and decoder-only language models. We compare zero and few-shot approaches of LLMs to fine-tuning smaller language models. The results show that accurate multi-label emotion classification is still insufficient even for high-resource languages such as English, and there is a large gap between the performance of high-resource and low-resource languages. The results also show varying performance levels depending on the language and model type. EthioEmo is available publicly to further improve the understanding of emotions in language models and how people convey emotions through various languages.",
        "author": "Tadesse Destaw Belay; Israel Abebe Azime; Abinew Ali Ayele; Grigori Sidorov; Dietrich Klakow; Philip Slusallek; Olga Kolesnikova; Seid Muhie Yimam",
        "authorids": "/t/tadesse-destaw-belay/; /i/israel-abebe-azime/; /a/abinew-ali-ayele/; /g/grigori-sidorov/; /d/dietrich-klakow/; /p/philip-slusallek/; /o/olga-kolesnikova/; /s/seid-muhie-yimam/",
        "bibtex": "@inproceedings{belay-etal-2025-evaluating,\n    title = \"Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding\",\n    author = \"Belay, Tadesse Destaw  and\n      Azime, Israel Abebe  and\n      Ayele, Abinew Ali  and\n      Sidorov, Grigori  and\n      Klakow, Dietrich  and\n      Slusallek, Philip  and\n      Kolesnikova, Olga  and\n      Yimam, Seid Muhie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.237/\",\n    pages = \"3523--3540\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.237.pdf",
        "site": "https://aclanthology.org/2025.coling-main.237/",
        "pdf_size": 4639533,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15610674693821351321&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Instituto Polit\u00e9cnico Nacional (IPN), CIC+Wollo University; Saarland University; Bahir Dar University+University of Hamburg; Instituto Polit\u00e9cnico Nacional (IPN), CIC; Saarland University; Saarland University; Instituto Polit\u00e9cnico Nacional (IPN), CIC; University of Hamburg",
        "aff_domain": "gmail.com; ; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ; ",
        "github": "https://github.com/Tadesse-Destaw/EthioEmo",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;3+4;0;2;2;0;4",
        "aff_unique_norm": "Instituto Polit\u00e9cnico Nacional;Wollo University;Saarland University;Bahir Dar University;University of Hamburg",
        "aff_unique_dep": "CIC;;;;",
        "aff_unique_url": "https://www.ipn.mx;http://www.wollou.edu.et;https://www.uni-saarland.de;https://www.bdu.edu.et;https://www.uni-hamburg.de",
        "aff_unique_abbr": "IPN;;UdS;BDU;UHH",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2;1+2;0;2;2;0;2",
        "aff_country_unique": "Mexico;Ethiopia;Germany"
    },
    {
        "id": "2025.coling-main.710",
        "title": "Evaluating the Consistency of LLM Evaluators",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.",
        "author": "Noah Lee; Jiwoo Hong; James Thorne",
        "authorids": "/n/noah-lee/; /j/jiwoo-hong/; /j/james-thorne/",
        "bibtex": "@inproceedings{lee-etal-2025-evaluating,\n    title = \"Evaluating the Consistency of {LLM} Evaluators\",\n    author = \"Lee, Noah  and\n      Hong, Jiwoo  and\n      Thorne, James\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.710/\",\n    pages = \"10650--10659\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.710.pdf",
        "site": "https://aclanthology.org/2025.coling-main.710/",
        "pdf_size": 336887,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16381006202694537963&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "KAIST AI; KAIST AI; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "KAIST AI",
        "aff_unique_url": "https://www.kaist.edu",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.345",
        "title": "EvoPrompt: Evolving Prompts for Enhanced Zero-Shot Named Entity Recognition with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) possess extensive prior knowledge and powerful in-context learning (ICL) capabilities, presenting significant opportunities for low-resource tasks. Though effective, several key issues still have not been well-addressed when focusing on zero-shot named entity recognition (NER), including the misalignment between model and human definitions of entity types, and confusion of similar types. This paper proposes an Evolving Prompts framework that guides the model to better address these issues through continuous prompt refinement. Specifically, we leverage the model to summarize the definition of each entity type and the distinctions between similar types (i.e., entity type guidelines). An iterative process is introduced to continually adjust and improve these guidelines. Additionally, since high-quality demonstrations are crucial for effective learning yet challenging to obtain in zero-shot scenarios, we design a strategy motivated by self-consistency and prototype learning to extract reliable and diverse pseudo samples from the model\u2019s predictions. Experiments on four benchmarks demonstrate the effectiveness of our framework, showing consistent performance improvements.",
        "author": "Zeliang Tong; Zhuojun Ding; Wei Wei",
        "authorids": "/z/zeliang-tong/; /z/zhuojun-ding/; /w/wei-wei/",
        "bibtex": "@inproceedings{tong-etal-2025-evoprompt,\n    title = \"{E}vo{P}rompt: Evolving Prompts for Enhanced Zero-Shot Named Entity Recognition with Large Language Models\",\n    author = \"Tong, Zeliang  and\n      Ding, Zhuojun  and\n      Wei, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.345/\",\n    pages = \"5136--5153\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.345.pdf",
        "site": "https://aclanthology.org/2025.coling-main.345/",
        "pdf_size": 1978014,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16077588349167114322&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology+Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology+Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology+Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL)",
        "aff_domain": "gmail.com;hust.edu.cn;hust.edu.cn",
        "email": "gmail.com;hust.edu.cn;hust.edu.cn",
        "github": "https://github.com/tongzeliang/EvoPrompt",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.489",
        "title": "Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hateful memes continuously evolve as new ones emerge by blending progressive cultural ideas, rendering existing methods that rely on extensive training obsolete or ineffective. In this work, we propose Evolver, which incorporates Large Multimodal Models (LMMs) via Chain-of-Evolution (CoE) Prompting, by integrating the evolution attribute and in-context information of memes. Specifically, Evolver simulates the evolving and expressing process of memes and reasons through LMMs in a step-by-step manner using an evolutionary pair mining module, an evolutionary information extractor, and a contextual relevance amplifier. Extensive experiments on public FHM, MAMI, and HarM datasets show that CoE prompting can be incorporated into existing LMMs to improve their performance. More encouragingly, it can serve as an interpretive tool to promote the understanding of the evolution of memes.",
        "author": "Jinfa Huang; Jinsheng Pan; Zhongwei Wan; Hanjia Lyu; Jiebo Luo",
        "authorids": "/j/jinfa-huang/; /j/jinsheng-pan/; /z/zhongwei-wan/; /h/hanjia-lyu/; /j/jiebo-luo/",
        "bibtex": "@inproceedings{huang-etal-2025-evolver,\n    title = \"Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection\",\n    author = \"Huang, Jinfa  and\n      Pan, Jinsheng  and\n      Wan, Zhongwei  and\n      Lyu, Hanjia  and\n      Luo, Jiebo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.489/\",\n    pages = \"7321--7330\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.489.pdf",
        "site": "https://aclanthology.org/2025.coling-main.489/",
        "pdf_size": 4734439,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10557509792702036836&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Rochester; University of Rochester; The Ohio State University; University of Rochester; University of Rochester",
        "aff_domain": "ur.rochester.edu;ur.rochester.edu;osu.edu;ur.rochester.edu;cs.rochester.edu",
        "email": "ur.rochester.edu;ur.rochester.edu;osu.edu;ur.rochester.edu;cs.rochester.edu",
        "github": "https://github.com/inFaaa/Evolver",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Rochester;The Ohio State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rochester.edu;https://www.osu.edu",
        "aff_unique_abbr": "U of R;OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.475",
        "title": "Explain-Analyze-Generate: A Sequential Multi-Agent Collaboration Method for Complex Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Exploring effective collaboration among multiple large language models (LLMs) represents an active research direction, with multiagent debate (MAD) emerging as a popular approach. MAD involves LLMs independently generating responses and refining their own responses by incorporating feedback from other agents in a debate manner. However,empirical experiments reveal the suboptimal performance of MAD in complex reasoning scenarios. We attribute this to the potential misleading caused by peer agents with limited individual capabilities. To address this, we propose a novel sequential collaboration framework named Explain-Analyze-Generate(EAG). By decomposing complex tasks into essential subtasks and employing a pipeline approach, EAG enable agents provide constructive assistance to peers, ultimately yielding higher performance. We conduct experiments on the comprehensive complex language reasoning benchmark: BIG-Bench-Hard (BBH). Our method achieves the highest performance on 19 out of 23 tasks, with an average improvement of 8% across all tasks, and incurs lower costs compared to MAD, demonstrating its effectiveness and efficiency.",
        "author": "WenYuan Gu; JiaLe Han; HaoWen Wang; Xiang Li; Bo Cheng",
        "authorids": "/w/wenyuan-gu/; /j/jiale-han/; /h/haowen-wang/; /x/xiang-li/; /b/bo-cheng/",
        "bibtex": "@inproceedings{gu-etal-2025-explain,\n    title = \"Explain-Analyze-Generate: A Sequential Multi-Agent Collaboration Method for Complex Reasoning\",\n    author = \"Gu, WenYuan  and\n      Han, JiaLe  and\n      Wang, HaoWen  and\n      Li, Xiang  and\n      Cheng, Bo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.475/\",\n    pages = \"7127--7140\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.475.pdf",
        "site": "https://aclanthology.org/2025.coling-main.475/",
        "pdf_size": 871315,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1813053533931508554&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; Hong Kong University of Science and Technology; School of Computer Science and Technology, Anhui University; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;ust.hk;ahu.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;ust.hk;ahu.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Hong Kong University of Science and Technology;Anhui University",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology;;School of Computer Science and Technology",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.ust.hk;http://www.ahu.edu.cn/",
        "aff_unique_abbr": "BUPT;HKUST;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.73",
        "title": "Explaining Relationships Among Research Papers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid pace of research publications makes it challenging for researchers to stay up to date. There is a growing need for automatically generated, concise literature reviews to help researchers quickly identify papers relevant to their interests. Prior work over the past decade has focused on summarizing individual research papers, typically in the context of citation generation, while the relationships among multiple papers have largely been overlooked. Existing approaches primarily generate standalone citation sentences without addressing the need for expository and transition sentences to explain the relationships among multiple citations. In this work, we propose a feature-based, LLM-prompting approach to generate richer citation texts and simultaneously capture the complex relationships among multiple papers. Our expert evaluation reveals a strong correlation between human preference and integrative writing styles, indicating that readers favor high-level, abstract citations with transition sentences that weave them into a coherent narrative.",
        "author": "Xiangci Li; Jessica Ouyang",
        "authorids": "/x/xiangci-li/; /j/jessica-ouyang/",
        "bibtex": "@inproceedings{li-ouyang-2025-explaining,\n    title = \"Explaining Relationships Among Research Papers\",\n    author = \"Li, Xiangci  and\n      Ouyang, Jessica\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.73/\",\n    pages = \"1080--1105\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.73.pdf",
        "site": "https://aclanthology.org/2025.coling-main.73/",
        "pdf_size": 732455,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13575680649537818984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon Web Services; The University of Texas at Dallas",
        "aff_domain": "gmail.com;utdallas.edu",
        "email": "gmail.com;utdallas.edu",
        "github": "",
        "project": "https://www.semanticscholar.org/faq/what-are-research-feeds",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Amazon Web Services;University of Texas at Dallas",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://aws.amazon.com;https://www.utdallas.edu",
        "aff_unique_abbr": "AWS;UT Dallas",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.436",
        "title": "Explanation Regularisation through the Lens of Attributions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Explanation regularisation (ER) has been introduced as a way to guide text classifiers to form their predictions relying on input tokens that humans consider plausible. This is achieved by introducing an auxiliary explanation loss that measures how well the output of an input attribution technique for the model agrees with human-annotated rationales. The guidance appears to benefit performance in out-of-domain (OOD) settings, presumably due to an increased reliance on plausible tokens. However, previous work has under-explored the impact of guidance on that reliance, particularly when reliance is measured using attribution techniques different from those used to guide the model. In this work, we seek to close this gap, and also explore the relationship between reliance on plausible features and OOD performance. We find that the connection between ER and the ability of a classifier to rely on plausible features has been overstated and that a stronger reliance on plausible tokens does not seem to be the cause for OOD improvements.",
        "author": "Pedro Ferreira; Ivan Titov; Wilker Aziz",
        "authorids": "/p/pedro-ferreira/; /i/ivan-titov/; /w/wilker-aziz/",
        "bibtex": "@inproceedings{ferreira-etal-2025-explanation,\n    title = \"Explanation Regularisation through the Lens of Attributions\",\n    author = \"Ferreira, Pedro  and\n      Titov, Ivan  and\n      Aziz, Wilker\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.436/\",\n    pages = \"6530--6551\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.436.pdf",
        "site": "https://aclanthology.org/2025.coling-main.436/",
        "pdf_size": 1930690,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16630832382129571547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Amsterdam; University of Amsterdam + University of Edinburgh; University of Amsterdam",
        "aff_domain": "uva.nl;inf.ed.ac.uk;uva.nl",
        "email": "uva.nl;inf.ed.ac.uk;uva.nl",
        "github": "https://github.com/PedroMLF/ER_through_the_lens_of_attributions",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Amsterdam;University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uva.nl;https://www.ed.ac.uk",
        "aff_unique_abbr": "UvA;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "id": "2025.coling-main.305",
        "title": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at https://github.com/jiah-li/magic.",
        "author": "Jiahui Li; Yongchang Hao; Haoyu Xu; Xing Wang; Yu Hong",
        "authorids": "/j/jiahui-li/; /y/yongchang-hao/; /h/haoyu-xu/; /x/xing-wang/; /y/yu-hong/",
        "bibtex": "@inproceedings{li-etal-2025-exploiting,\n    title = \"Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models\",\n    author = \"Li, Jiahui  and\n      Hao, Yongchang  and\n      Xu, Haoyu  and\n      Wang, Xing  and\n      Hong, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.305/\",\n    pages = \"4535--4547\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.305.pdf",
        "site": "https://aclanthology.org/2025.coling-main.305/",
        "pdf_size": 833945,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1505461672460144292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China+1; Dept. Computing Science, Alberta Machine Intelligence Institute (Amii) University of Alberta, Canada+2; Tencent+3; School of Computer Science and Technology, Soochow University, Suzhou, China+1; School of Computer Science and Technology, Soochow University, Suzhou, China+1",
        "aff_domain": "gmail.com;ualberta.ca;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;ualberta.ca;gmail.com;gmail.com;gmail.com",
        "github": "https://github.com/jiah-li/magic",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;2;3;0;0",
        "aff_unique_norm": "Soochow University;;University of Alberta;Tencent Holdings Limited",
        "aff_unique_dep": "School of Computer Science and Technology;;Dept. Computing Science;",
        "aff_unique_url": "http://www.soochow.edu.cn;;https://www.ualberta.ca;https://www.tencent.com",
        "aff_unique_abbr": ";;UAlberta;Tencent",
        "aff_campus_unique_index": "0;;;0;0",
        "aff_campus_unique": "Suzhou;",
        "aff_country_unique_index": "0;2;0;0;0",
        "aff_country_unique": "China;;Canada"
    },
    {
        "id": "2025.coling-main.62",
        "title": "Exploring Backdoor Vulnerabilities of Chat Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on single-turn instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor cannot be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic examples.",
        "author": "Wenkai Yang; Yunzhuo Hao; Yankai Lin",
        "authorids": "/w/wenkai-yang/; /y/yunzhuo-hao/; /y/yankai-lin/",
        "bibtex": "@inproceedings{yang-etal-2025-exploring,\n    title = \"Exploring Backdoor Vulnerabilities of Chat Models\",\n    author = \"Yang, Wenkai  and\n      Hao, Yunzhuo  and\n      Lin, Yankai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.62/\",\n    pages = \"933--946\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.62.pdf",
        "site": "https://aclanthology.org/2025.coling-main.62/",
        "pdf_size": 883116,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7613911805706790890&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; School of Information and Software Engineering, University of Electronic Science and Technology of China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn;gmail.com;ruc.edu.cn",
        "email": "ruc.edu.cn;gmail.com;ruc.edu.cn",
        "github": "https://github.com/hychaochao/Chat-Models-Backdoor-Attacking",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Renmin University of China;University of Electronic Science and Technology of China",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;School of Information and Software Engineering",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.uestc.edu.cn",
        "aff_unique_abbr": "RUC;UESTC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.37",
        "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of \u201cConcept Depth\u201d to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at https://github.com/Luckfort/CD.",
        "author": "Mingyu Jin; Qinkai Yu; Jingyuan Huang; Qingcheng Zeng; Zhenting Wang; Wenyue Hua; Haiyan Zhao; Kai Mei; Yanda Meng; Kaize Ding; Fan Yang; Mengnan Du; Yongfeng Zhang",
        "authorids": "/m/mingyu-jin/; /q/qinkai-yu/; /j/jingyuan-huang/; /q/qingcheng-zeng/; /z/zhenting-wang/; /w/wenyue-hua/; /h/haiyan-zhao/; /k/kai-mei/; /y/yanda-meng/; /k/kaize-ding/; /f/fan-yang/; /m/mengnan-du/; /y/yongfeng-zhang/",
        "bibtex": "@inproceedings{jin-etal-2025-exploring,\n    title = \"Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?\",\n    author = \"Jin, Mingyu  and\n      Yu, Qinkai  and\n      Huang, Jingyuan  and\n      Zeng, Qingcheng  and\n      Wang, Zhenting  and\n      Hua, Wenyue  and\n      Zhao, Haiyan  and\n      Mei, Kai  and\n      Meng, Yanda  and\n      Ding, Kaize  and\n      Yang, Fan  and\n      Du, Mengnan  and\n      Zhang, Yongfeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.37/\",\n    pages = \"558--573\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.37.pdf",
        "site": "https://aclanthology.org/2025.coling-main.37/",
        "pdf_size": 834296,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=517934738908813118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Rutgers University; University of Exeter; Rutgers University; Northwestern University; Rutgers University; Rutgers University; New Jersey Institute of Technology; Rutgers University; University of Exeter; Northwestern University; Wake Forest University; New Jersey Institute of Technology; Rutgers University",
        "aff_domain": "rutgers.edu;exeter.ac.uk;rutgers.edu;njit.edu;rutgers.edu; ; ; ; ; ; ; ; ",
        "email": "rutgers.edu;exeter.ac.uk;rutgers.edu;njit.edu;rutgers.edu; ; ; ; ; ; ; ; ",
        "github": "https://github.com/Luckfort/CD",
        "project": "https://luckfort.github.io/explore_CD/",
        "author_num": 13,
        "aff_unique_index": "0;1;0;2;0;0;3;0;1;2;4;3;0",
        "aff_unique_norm": "Rutgers University;University of Exeter;Northwestern University;New Jersey Institute of Technology;Wake Forest University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.rutgers.edu;https://www.exeter.ac.uk;https://www.northwestern.edu;https://www.njit.edu;https://www.wfu.edu",
        "aff_unique_abbr": "Rutgers;Exeter;NU;NJIT;WFU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2025.coling-main.532",
        "title": "Exploring Content Predictability in Turn-Taking Through Different Computer-Mediated Communications",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous studies of face-to-face (f2f) communication have suggested that speakers rely heavily on a variety of multi-modal cues to make real-time predictions about upcoming words in rapid turn-taking. To understand how computer-mediated communication (CMC) differs from f2f communication in terms of the prediction mechanism, this study assessed how the loss of multi-modal cues would affect word predictability in turn-taking. Participants watched videos, listened to audio, or read a transcript of f2f conversations. Across these three conditions, they predicted the same set of omitted words with different levels of predictability and semantic relatedness to other words in the context. Results showed that words of higher predictability were more accurately predicted regardless of CMC types. Higher response accuracy but longer response time were observed in conditions with richer cues, and for participants with more positive and less negative self-emotions. Meanwhile, semantic relatedness did not affect predictability. These results confirmed the key role of prediction in language processing and conversation smoothness, especially its importance in CMC.",
        "author": "Wanqing He; Calen C. MacDonald; Yejoon Yoo; Marcos Eizayaga; Ryun Shim; Lev D. Katreczko; Susan R. Fussell",
        "authorids": "/w/wanqing-he/; /c/calen-c-macdonald/; /y/yejoon-yoo/; /m/marcos-eizayaga/; /r/ryun-shim/; /l/lev-d-katreczko/; /s/susan-r-fussell/",
        "bibtex": "@inproceedings{he-etal-2025-exploring,\n    title = \"Exploring Content Predictability in Turn-Taking Through Different Computer-Mediated Communications\",\n    author = \"He, Wanqing  and\n      MacDonald, Calen C.  and\n      Yoo, Yejoon  and\n      Eizayaga, Marcos  and\n      Shim, Ryun  and\n      Katreczko, Lev D.  and\n      Fussell, Susan R.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.532/\",\n    pages = \"7949--7962\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.532.pdf",
        "site": "https://aclanthology.org/2025.coling-main.532/",
        "pdf_size": 748104,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZBw9OSrbM90J:scholar.google.com/&scioq=Exploring+Content+Predictability+in+Turn-Taking+Through+Different+Computer-Mediated+Communications&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Cornell University, United States; Cornell University, United States; Cornell University, United States; Cornell University, United States; Cornell University, United States; Cornell University, United States; Cornell University, United States",
        "aff_domain": "cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu",
        "email": "cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.351",
        "title": "Exploring Fine-Grained Human Motion Video Captioning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Detailed descriptions of human motion are crucial for effective fitness training, which highlights the importance of research in fine-grained human motion video captioning. Existing video captioning models often fail to capture the nuanced semantics of videos, resulting in the generated descriptions that are coarse and lack details, especially when depicting human motions. To benchmark the Body Fitness Training scenario, in this paper, we construct a fine-grained human motion video captioning dataset named BoFiT and design a state-of-the-art baseline model named BoFiT-Gen (Body Fitness Training Text Generation). BoFiT-Gen makes use of computer vision techniques to extract angular representations of human motions from videos and LLMs to generate fine-grained descriptions of human motions via prompting. Results show that BoFiT-Gen outperforms previous methods on comprehensive metrics. We aim for this dataset to serve as a useful evaluation set for visio-linguistic models and drive further progress in this field. Our dataset is released at https://github.com/colmon46/bofit.",
        "author": "Bingchan Zhao; Xinyi Liu; Zhuocheng Yu; Tongchen Yang; Yifan Song; Mingyu Jin; Sujian Li; Yizhou Wang",
        "authorids": "/b/bingchan-zhao/; /x/xinyi-liu/; /z/zhuocheng-yu/; /t/tongchen-yang/; /y/yifan-song/; /m/mingyu-jin/; /s/sujian-li/; /y/yizhou-wang/",
        "bibtex": "@inproceedings{zhao-etal-2025-exploring,\n    title = \"Exploring Fine-Grained Human Motion Video Captioning\",\n    author = \"Zhao, Bingchan  and\n      Liu, Xinyi  and\n      Yu, Zhuocheng  and\n      Yang, Tongchen  and\n      Song, Yifan  and\n      Jin, Mingyu  and\n      Li, Sujian  and\n      Wang, Yizhou\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.351/\",\n    pages = \"5247--5264\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.351.pdf",
        "site": "https://aclanthology.org/2025.coling-main.351/",
        "pdf_size": 2922089,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UDMMbcekNtgJ:scholar.google.com/&scioq=Exploring+Fine-Grained+Human+Motion+Video+Captioning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University + Jiangsu Collaborative Innovation Center for Language Ability, Jiangsu Normal University; National Key Laboratory for Multimedia Information Processing, Peking University",
        "aff_domain": "pku.edu.cn;stu.pku.edu.cn; ; ; ; ;pku.edu.cn; ",
        "email": "pku.edu.cn;stu.pku.edu.cn; ; ; ; ;pku.edu.cn; ",
        "github": "https://github.com/colmon46/bofit",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0+1;0",
        "aff_unique_norm": "Peking University;Jiangsu Normal University",
        "aff_unique_dep": "National Key Laboratory for Multimedia Information Processing;Jiangsu Collaborative Innovation Center for Language Ability",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.jsnu.edu.cn",
        "aff_unique_abbr": "PKU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.474",
        "title": "Exploring Language Model Generalization in Low-Resource Extractive QA",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we investigate Extractive Question Answering (EQA) with Large Language Models (LLMs) under domain drift, i.e., can LLMs generalize to domains that require specific knowledge such as medicine and law in a zero-shot fashion without additional in-domain training? To this end, we devise a series of experiments to explain the performance gap empirically. Our findings suggest that: (a) LLMs struggle with dataset demands of closed do- mains such as retrieving long answer spans; (b) Certain LLMs, despite showing strong overall performance, display weaknesses in meeting basic requirements as discriminating between domain-specific senses of words which we link to pre-processing decisions; (c) Scaling model parameters is not always effective for cross-domain generalization; and (d) Closed-domain datasets are quantitatively much different than open-domain EQA datasets and current LLMs struggle to deal with them. Our findings point out important directions for improving existing LLMs.",
        "author": "Saptarshi Sengupta; Wenpeng Yin; Preslav Nakov; Shreya Ghosh; Suhang Wang",
        "authorids": "/s/saptarshi-sengupta/; /w/wenpeng-yin/; /p/preslav-nakov/; /s/shreya-ghosh/; /s/suhang-wang/",
        "bibtex": "@inproceedings{sengupta-etal-2025-exploring,\n    title = \"Exploring Language Model Generalization in Low-Resource Extractive {QA}\",\n    author = \"Sengupta, Saptarshi  and\n      Yin, Wenpeng  and\n      Nakov, Preslav  and\n      Ghosh, Shreya  and\n      Wang, Suhang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.474/\",\n    pages = \"7106--7126\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.474.pdf",
        "site": "https://aclanthology.org/2025.coling-main.474/",
        "pdf_size": 2449921,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=25375565550018037&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Pennsylvania State University, USA; Pennsylvania State University, USA; Mohamed bin Zayed University of Artificial Intelligence, UAE; Indian Institute of Technology (IIT) Bhubaneswar, India; Pennsylvania State University, USA",
        "aff_domain": "psu.edu;psu.edu;mbzuai.ac.ae;iitbbs.ac.in;psu.edu",
        "email": "psu.edu;psu.edu;mbzuai.ac.ae;iitbbs.ac.in;psu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Pennsylvania State University;Mohamed bin Zayed University of Artificial Intelligence;Indian Institute of Technology Bhubaneswar",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.psu.edu;https://mbzuai.ac.ae;https://www.iitbbs.ac.in",
        "aff_unique_abbr": "PSU;MBZUAI;IIT Bhubaneswar",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bhubaneswar",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "United States;United Arab Emirates;India"
    },
    {
        "id": "2025.coling-main.115",
        "title": "Exploring Unified Training Framework for Multimodal User Profiling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the emergence of social media and e-commerce platforms, accurate user profiling has become increasingly vital for recommendation systems and personalized services. Recent studies have focused on generating detailed user profiles by extracting various aspects of user attributes from textual reviews. Nevertheless, these investigations have not fully exploited the potential of the abundant multimodal data at hand. In this study, we propose a novel task called multimodal user profiling. This task emphasizes the utilization of both review texts and their accompanying images to create comprehensive user profiles. By integrating textual and visual data, we leverage their complementary strengths, enabling the generation of more holistic user representations. Additionally, we explore a unified joint training framework with various multimodal training strategies that incorporate users\u2019 historical review texts and images for user profile generation. Our experimental results underscore the significance of multimodal data in enhancing user profile generation and demonstrate the effectiveness of the proposed unified joint training approach.",
        "author": "Minjie Qiang; Zhongqing Wang; Shoushan Li; Guodong Zhou",
        "authorids": "/m/minjie-qiang/; /z/zhongqing-wang/; /s/shoushan-li/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{qiang-etal-2025-exploring,\n    title = \"Exploring Unified Training Framework for Multimodal User Profiling\",\n    author = \"Qiang, Minjie  and\n      Wang, Zhongqing  and\n      Li, Shoushan  and\n      Zhou, Guodong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.115/\",\n    pages = \"1699--1710\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.115.pdf",
        "site": "https://aclanthology.org/2025.coling-main.115/",
        "pdf_size": 1147321,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MeEKptVBVyUJ:scholar.google.com/&scioq=Exploring+Unified+Training+Framework+for+Multimodal+User+Profiling&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China; Natural Language Processing Lab, Soochow University, Suzhou, China",
        "aff_domain": "gmail.com;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "gmail.com;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Natural Language Processing Lab",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.162",
        "title": "Exploring the Impact of Language Switching on Personality Traits in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper investigates the extent to which LLMs align with humans when personality shifts are associated with language changes. Based on three experiments, that focus on GPT-4o and the Eysenck Personality Questionnaire-Revised (EPQR-A), our initial results reveal a weak yet significant variation in GPT-4o\u2019s personality across languages, indicating that some stem from a language-switching effect rather than translation. Further analysis across five English-speaking countries shows that GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits.",
        "author": "Jacopo Amidei; Jose Gregorio Ferreira De S\u00e1; Rub\u00e9n Nieto Luna; Andreas Kaltenbrunner",
        "authorids": "/j/jacopo-amidei/; /j/jose-gregorio-ferreira-de-sa/; /r/ruben-nieto-luna/; /a/andreas-kaltenbrunner/",
        "bibtex": "@inproceedings{amidei-etal-2025-exploring,\n    title = \"Exploring the Impact of Language Switching on Personality Traits in {LLM}s\",\n    author = \"Amidei, Jacopo  and\n      Ferreira De S{\\'a}, Jose Gregorio  and\n      Luna, Rub{\\'e}n Nieto  and\n      Kaltenbrunner, Andreas\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.162/\",\n    pages = \"2370--2378\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.162.pdf",
        "site": "https://aclanthology.org/2025.coling-main.162/",
        "pdf_size": 299518,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11233820340235607511&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "IN3, Universitat Oberta de Catalunya, Barcelona, Spain; IN3, Universitat Oberta de Catalunya, Barcelona, Spain; eHealth Research Lab, Universitat Oberta de Catalunya, Barcelona, Spain; IN3, Universitat Oberta de Catalunya, Barcelona, Spain + ISI Foundation, Torino, Italy",
        "aff_domain": "uoc.edu; ; ; ",
        "email": "uoc.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Universitat Oberta de Catalunya;ISI Foundation",
        "aff_unique_dep": "IN3;",
        "aff_unique_url": "https://www.uoc.edu;https://www.isi.it",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0;0;0+1",
        "aff_campus_unique": "Barcelona;Torino",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Spain;Italy"
    },
    {
        "id": "2025.coling-main.522",
        "title": "Exploring the Impacts of Feature Fusion Strategy in Multi-modal Entity Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-modal entity alignment aims to identify equivalent entities between two different multi-modal knowledge graphs, which consist of structural triples and images associated with entities. Unfortunately, prior works fuse the multi-modal knowledge of all entities only via solely one single fusion strategy. Therefore, the impact of the fusion strategy on individual entities could be largely ignored. To solve this challenge, we propose AMF2SEA, an adaptive multi-modal feature fusion strategy for entity alignment, which dynamically selects the optimal entity-level feature fusion strategy. Additionally, we build a new dataset based on DBP15K, which includes a full set of entity images from multiple inconsistent web sources, making it more representative of the real world. Experimental results demonstrate that our model achieves state-of-the-art (SOTA) performance compared to models using the same modality on DBP15K and its variants with richer image sources and styles. Our code and data are available at https://github.com/ChenxiaoLiJoe/AMFFSEA.",
        "author": "Chenxiao Li; Jingwei Cheng; Qiang Tong; Fu Zhang",
        "authorids": "/c/chenxiao-li/; /j/jingwei-cheng/; /q/qiang-tong/; /f/fu-zhang/",
        "bibtex": "@inproceedings{li-etal-2025-exploring,\n    title = \"Exploring the Impacts of Feature Fusion Strategy in Multi-modal Entity Alignment\",\n    author = \"Li, Chenxiao  and\n      Cheng, Jingwei  and\n      Tong, Qiang  and\n      Zhang, Fu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.522/\",\n    pages = \"7809--7818\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.522.pdf",
        "site": "https://aclanthology.org/2025.coling-main.522/",
        "pdf_size": 1455420,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NPqG5Bx8KxMJ:scholar.google.com/&scioq=Exploring+the+Impacts+of+Feature+Fusion+Strategy+in+Multi-modal+Entity+Alignment&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Northeastern University, China; Northeastern University, China; Northeastern University, China; Northeastern University, China",
        "aff_domain": "163.com;mail.neu.edu.cn;mail.neu.edu.cn;mail.neu.edu.cn",
        "email": "163.com;mail.neu.edu.cn;mail.neu.edu.cn;mail.neu.edu.cn",
        "github": "https://github.com/ChenxiaoLi-Joe/AMFFSEA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.neu.edu.cn/",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.288",
        "title": "Exploring the Limitations of Detecting Machine-Generated Text",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent improvements in the quality of the generations by large language models have spurred research into identifying machine-generated text. Such work often presents high-performing detectors. However, humans and machines can produce text in different styles and domains, yet the the performance impact of such on machine generated text detection systems remains unclear. In this paper, we audit the classification performance for detecting machine-generated text by evaluating on texts with varying writing styles. We find that classifiers are highly sensitive to stylistic changes and differences in text complexity, and in some cases degrade entirely to random classifiers. We further find that detection systems are particularly susceptible to misclassify easy-to-read texts while they have high performance for complex texts, leading to concerns about the reliability of detection systems. We recommend that future work attends to stylistic factors and reading difficulty levels of human-written and machine-generated text.",
        "author": "Jad Doughman; Osama Mohammed Afzal; Hawau Olamide Toyin; Shady Shehata; Preslav Nakov; Zeerak Talat",
        "authorids": "/j/jad-doughman/; /o/osama-mohammed-afzal/; /h/hawau-olamide-toyin/; /s/shady-shehata/; /p/preslav-nakov/; /z/zeerak-talat/",
        "bibtex": "@inproceedings{doughman-etal-2025-exploring,\n    title = \"Exploring the Limitations of Detecting Machine-Generated Text\",\n    author = \"Doughman, Jad  and\n      Mohammed Afzal, Osama  and\n      Toyin, Hawau Olamide  and\n      Shehata, Shady  and\n      Nakov, Preslav  and\n      Talat, Zeerak\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.288/\",\n    pages = \"4274--4281\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.288.pdf",
        "site": "https://aclanthology.org/2025.coling-main.288/",
        "pdf_size": 2162027,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14841870099850223415&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence; University of Edinburgh",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;zeerak.org",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;zeerak.org",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.ed.ac.uk",
        "aff_unique_abbr": "MBZUAI;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "United Arab Emirates;United Kingdom"
    },
    {
        "id": "2025.coling-main.688",
        "title": "Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous work adopts large language models (LLMs) as evaluators to evaluate natural language process (NLP) tasks. However, certain shortcomings, e.g., fairness, scope, and accuracy, persist for current LLM evaluators. To analyze whether LLMs can serve as reliable alternatives to humans, we examine the fine-grained alignment between LLM evaluators and human annotators, particularly in understanding the target evaluation tasks and conducting evaluations that meet diverse criteria. This paper explores both conventional tasks (e.g., story generation) and alignment tasks (e.g., math reasoning), each with different evaluation criteria. Our analysis shows that 1) LLM evaluators can generate unnecessary criteria or omit crucial criteria, resulting in a slight deviation from the experts. 2) LLM evaluators excel in general criteria, such as fluency, but face challenges with complex criteria, such as numerical reasoning. We also find that LLM-pre-drafting before human evaluation can help reduce the impact of human subjectivity and minimize annotation outliers in pure human evaluation, leading to more objective evaluation. All resources are available at https://github.com/qtli/CoEval.",
        "author": "Qintong Li; Leyang Cui; Lingpeng Kong; Wei Bi",
        "authorids": "/q/qintong-li/; /l/leyang-cui/; /l/lingpeng-kong/; /w/wei-bi/",
        "bibtex": "@inproceedings{li-etal-2025-exploring-reliability,\n    title = \"Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse {NLP} Tasks\",\n    author = \"Li, Qintong  and\n      Cui, Leyang  and\n      Kong, Lingpeng  and\n      Bi, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.688/\",\n    pages = \"10325--10344\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.688.pdf",
        "site": "https://aclanthology.org/2025.coling-main.688/",
        "pdf_size": 2288361,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:92KT1cC030UJ:scholar.google.com/&scioq=Exploring+the+Reliability+of+Large+Language+Models+as+Customized+Evaluators+for+Diverse+NLP+Tasks&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "The University of Hong Kong\u2661; Tencent AI lab\u2663; The University of Hong Kong\u2661; Tencent AI lab\u2663",
        "aff_domain": "cs.hku.hk;tencent.com;cs.hku.hk;tencent.com",
        "email": "cs.hku.hk;tencent.com;cs.hku.hk;tencent.com",
        "github": "https://github.com/qtli/CoEval",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "The University of Hong Kong;Tencent",
        "aff_unique_dep": ";AI lab",
        "aff_unique_url": "https://www.hku.hk;https://ai.tencent.com",
        "aff_unique_abbr": "HKU;Tencent AI lab",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.594",
        "title": "Extending LLMs to New Languages: A Case Study of Llama and Persian Adaptation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have made great progress in classification and text generation tasks. However, they are mainly trained on English data and often struggle with low-resource languages. In this study, we explore adding a new language, i.e., Persian, to Llama (a model with a limited understanding of Persian) using parameter-efficient fine-tuning. We employ a multi-stage approach involving pretraining on monolingual Persian data, aligning representations through bilingual pretraining and instruction datasets, and instruction-tuning with task-specific datasets. We evaluate the model\u2019s performance at each stage on generation and classification tasks. Our findings suggest that incorporating the Persian language, through bilingual data alignment, can enhance classification accuracy for Persian tasks, with no adverse impact and sometimes even improvements on English tasks. Additionally, the results highlight the model\u2019s initial strength as a critical factor when working with limited training data, with cross-lingual alignment offering minimal benefits for the low-resource language. Knowledge transfer from English to Persian has a marginal effect, primarily benefiting simple classification tasks.",
        "author": "Samin Mahdizadeh Sani; Pouya Sadeghi; Thuy-Trang Vu; Yadollah Yaghoobzadeh; Gholamreza Haffari",
        "authorids": "/s/samin-mahdizadeh-sani/; /p/pouya-sadeghi/; /t/thuy-vu/; /y/yadollah-yaghoobzadeh/; /g/gholamreza-haffari/",
        "bibtex": "@inproceedings{mahdizadeh-sani-etal-2025-extending,\n    title = \"Extending {LLM}s to New Languages: A Case Study of Llama and {P}ersian Adaptation\",\n    author = \"Mahdizadeh Sani, Samin  and\n      Sadeghi, Pouya  and\n      Vu, Thuy-Trang  and\n      Yaghoobzadeh, Yadollah  and\n      Haffari, Gholamreza\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.594/\",\n    pages = \"8868--8884\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.594.pdf",
        "site": "https://aclanthology.org/2025.coling-main.594/",
        "pdf_size": 1689527,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:24jtv4ejkqAJ:scholar.google.com/&scioq=Extending+LLMs+to+New+Languages:+A+Case+Study+of+Llama+and+Persian+Adaptation&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Electrical and Computer Engineering, University of Tehran, Iran; Department of Electrical and Computer Engineering, University of Tehran, Iran; Department of Data Science & AI, Monash University, Australia; Department of Electrical and Computer Engineering, University of Tehran, Iran+Tehran Institute for Advanced Studies, Khatam University, Iran; Department of Data Science & AI, Monash University, Australia",
        "aff_domain": "ut.ac.ir;ut.ac.ir;monash.edu;ut.ac.ir;monash.edu",
        "email": "ut.ac.ir;ut.ac.ir;monash.edu;ut.ac.ir;monash.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0+2;1",
        "aff_unique_norm": "University of Tehran;Monash University;Tehran Institute for Advanced Studies",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Data Science & AI;",
        "aff_unique_url": "https://www.ut.ac.ir;https://www.monash.edu;",
        "aff_unique_abbr": "UT;Monash;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0+0;1",
        "aff_country_unique": "Iran;Australia"
    },
    {
        "id": "2025.coling-main.329",
        "title": "Extracting structure from an LLM - how to improve on surprisal-based models of Human Language Processing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prediction and reanalysis are considered two key processes that underly humans\u2019 capacity to comprehend language in real time. Computational models capture it using Large Language Models (LLMs) and a statistical measure known as \u2018surprisal\u2019. Despite successes of LLMs, surprisal-based models face challenges when it comes to sentences requiring reanalysis due to pervasive temporary structural ambiguities, such as garden path sentences. We ask whether structural information can be extracted from LLM\u2019s and develop a model that integrates it with their learnt statistics. When applied to a dataset of garden path sentences, the model achieved a significantly higher correlation with human reading times than surprisal. It also provided a better prediction of the garden path effect and could distinguish between sentence types with different levels of difficulty.",
        "author": "Daphne P. Wang; Mehrnoosh Sadrzadeh; Milo\u0161 Stanojevi\u0107; Wing-Yee Chow; Richard Breheny",
        "authorids": "/d/daphne-p-wang/; /m/mehrnoosh-sadrzadeh/; /m/milos-stanojevic/; /w/wing-yee-chow/; /r/richard-breheny/",
        "bibtex": "@inproceedings{wang-etal-2025-extracting,\n    title = \"Extracting structure from an {LLM} - how to improve on surprisal-based models of Human Language Processing\",\n    author = \"Wang, Daphne P.  and\n      Sadrzadeh, Mehrnoosh  and\n      Stanojevi{\\'c}, Milo{\\v{s}}  and\n      Chow, Wing-Yee  and\n      Breheny, Richard\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.329/\",\n    pages = \"4938--4944\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.329.pdf",
        "site": "https://aclanthology.org/2025.coling-main.329/",
        "pdf_size": 286179,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0_CsE3cuZN0J:scholar.google.com/&scioq=Extracting+structure+from+an+LLM+-+how+to+improve+on+surprisal-based+models+of+Human+Language+Processing&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Quandela; University College London; University College London; University College London; University College London",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Quandela;University College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.quandela.com;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "id": "2025.coling-main.704",
        "title": "Extracting the Essence and Discarding the Dross: Enhancing Code Generation with Contrastive Execution Feedback",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements have integrated the execution process and feedback into the training of large language models for code generation, demonstrating enhanced model performance. However, current methods amalgamate erroneous code with feedback and the final correct code as target sentences, inadvertently increasing the probability of generating both correct and incorrect code during inference. While multiple iterations of feedback can eventually yield the correct answer, this iterative process is cumbersome and time-consuming for users who prefer immediate accurate results. To address this challenge, we propose ConCoder, a contrastive learning-based code generation model with execution feedback. This approach enables the model to efficiently produce accurate code from the outset while rectifying and optimizing the incorrect code. Furthermore, our training emphasizes learning from the causes of errors, allowing the model to understand and avoid mistakes. Through extensive experiments, ConCoder demonstrates significant improvements in generating accurate code and understanding error correction, paving the way for more reliable code generation models.",
        "author": "Xuanyu Zhang; Qing Yang",
        "authorids": "/x/xuanyu-zhang/; /q/qing-yang/",
        "bibtex": "@inproceedings{zhang-yang-2025-extracting,\n    title = \"Extracting the Essence and Discarding the Dross: Enhancing Code Generation with Contrastive Execution Feedback\",\n    author = \"Zhang, Xuanyu  and\n      Yang, Qing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.704/\",\n    pages = \"10569--10575\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.704.pdf",
        "site": "https://aclanthology.org/2025.coling-main.704/",
        "pdf_size": 417861,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16292065687777433331&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Du Xiaoman Financial, Beijing, China; Du Xiaoman Financial, Beijing, China",
        "aff_domain": "duxiaoman.com;duxiaoman.com",
        "email": "duxiaoman.com;duxiaoman.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Du Xiaoman Financial",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.573",
        "title": "Extracting, Detecting, and Generating Research Questions for Scientific Articles",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The volume of academic articles is increasing rapidly, reflecting the growing emphasis on research and scholarship across different science disciplines. This rapid growth necessitates the development of tools for more efficient and rapid understanding of these articles. Clear and well-defined Research Questions (RQs) in research articles can help guide scholarly inquiries. However, many academic studies lack a proper definition of RQs in their articles. This research addresses this gap by presenting a comprehensive framework for the systematic extraction, detection, and generation of RQs from scientific articles. The extraction component uses a set of regular expressions to identify articles containing well-defined RQs. The detection component aims to identify more complex RQs in articles, beyond those captured by the rule-based extraction method. The RQ generation focuses on creating RQs for articles that lack them. We integrate all these components to build a pipeline to extract RQs or generate them based on the articles\u2019 full text. We evaluate the performance of the designed pipeline on a set of metrics designed to assess the quality of RQs. Our results indicate that the proposed pipeline can reliably detect RQs and generate high-quality ones.",
        "author": "Sina Taslimi; Artemis Capari; Hosein Azarbonyad; Zi Long Zhu; Zubair Afzal; Evangelos Kanoulas; George Tsatsaronis",
        "authorids": "/s/sina-taslimi/; /a/artemis-capari/; /h/hosein-azarbonyad/; /z/zi-long-zhu/; /z/zubair-afzal/; /e/evangelos-kanoulas/; /g/george-tsatsaronis/",
        "bibtex": "@inproceedings{taslimi-etal-2025-extracting,\n    title = \"Extracting, Detecting, and Generating Research Questions for Scientific Articles\",\n    author = \"Taslimi, Sina  and\n      Capari, Artemis  and\n      Azarbonyad, Hosein  and\n      Zhu, Zi Long  and\n      Afzal, Zubair  and\n      Kanoulas, Evangelos  and\n      Tsatsaronis, George\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.573/\",\n    pages = \"8573--8588\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.573.pdf",
        "site": "https://aclanthology.org/2025.coling-main.573/",
        "pdf_size": 832352,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZTtSulA9iOoJ:scholar.google.com/&scioq=Extracting,+Detecting,+and+Generating+Research+Questions+for+Scientific+Articles&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Elsevier+University of Amsterdam; Elsevier; Elsevier; Elsevier; Elsevier; University of Amsterdam; Elsevier",
        "aff_domain": "yahoo.com;elsevier.com;elsevier.com;elsevier.com;elsevier.com;uva.nl;elsevier.com",
        "email": "yahoo.com;elsevier.com;elsevier.com;elsevier.com;elsevier.com;uva.nl;elsevier.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;0;0;1;0",
        "aff_unique_norm": "Elsevier;University of Amsterdam",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.elsevier.com;https://www.uva.nl",
        "aff_unique_abbr": "Elsevier;UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2025.coling-main.523",
        "title": "Extrapolating to Unknown Opinions Using LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "From ice cream flavors to climate change, people exhibit a wide array of opinions on various topics, and understanding the rationale for these opinions can promote healthy discussion and consensus among them. As such, it can be valuable for a large language model (LLM), particularly as an AI assistant, to be able to empathize with or even explain these various standpoints. In this work, we hypothesize that different topic stances often manifest correlations that can be used to extrapolate to topics with unknown opinions. We explore various prompting and fine-tuning methods to improve an LLM\u2019s ability to (a) extrapolate from opinions on known topics to unknown ones and (b) support their extrapolation with reasoning. Our findings suggest that LLMs possess inherent knowledge from training data about these opinion correlations, and with minimal data, the similarities between human opinions and model-extrapolated opinions can be improved by more than 50%. Furthermore, LLM can generate the reasoning process behind their extrapolation of opinions.",
        "author": "Kexun Zhang; Jane Dwivedi-Yu; Zhaojiang Lin; Yuning Mao; William Yang Wang; Lei Li; Yi-Chia Wang",
        "authorids": "/k/kexun-zhang/; /j/jane-dwivedi-yu/; /z/zhaojiang-lin/; /y/yuning-mao/; /w/william-yang-wang/; /l/lei-li/; /y/yi-chia-wang/",
        "bibtex": "@inproceedings{zhang-etal-2025-extrapolating,\n    title = \"Extrapolating to Unknown Opinions Using {LLM}s\",\n    author = \"Zhang, Kexun  and\n      Dwivedi-Yu, Jane  and\n      Lin, Zhaojiang  and\n      Mao, Yuning  and\n      Wang, William Yang  and\n      Li, Lei  and\n      Wang, Yi-Chia\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.523/\",\n    pages = \"7819--7830\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.523.pdf",
        "site": "https://aclanthology.org/2025.coling-main.523/",
        "pdf_size": 408386,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17095728423761743764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-demos.22",
        "title": "FEAT-writing: An Interactive Training System for Argumentative Writing",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Recent developments in Natural Language Processing (NLP) for argument mining offer new opportunities to analyze the argumentative units (AUs) in student essays. These advancements can be leveraged to provide automatically generated feedback and exercises for students engaging in online argumentative essay writing practice. Writing standards for both native English speakers (L1) and English-as-a-foreign-language (L2) learners require students to understand formal essay structures and different AUs. To address this need, we developed FEAT-writing (Feedback and Exercises for Argumentative Training in writing), an interactive system that provides students with automatically generated exercises and distinct feedback on their argumentative writing. In a preliminary evaluation involving 346 students, we assessed the impact of six different automated feedback types on essay quality, with results showing general improvements in writing after receiving feedback from the system.",
        "author": "Yuning Ding; Franziska Wehrhahn; Andrea Horbach",
        "authorids": "/y/yuning-ding/; /f/franziska-wehrhahn/; /a/andrea-horbach/",
        "bibtex": "@inproceedings{ding-etal-2025-feat,\n    title = \"{FEAT}-writing: An Interactive Training System for Argumentative Writing\",\n    author = \"Ding, Yuning  and\n      Wehrhahn, Franziska  and\n      Horbach, Andrea\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.22/\",\n    pages = \"217--225\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.22.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.22/",
        "pdf_size": 628794,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:v7izkl7N4XQJ:scholar.google.com/&scioq=FEAT-writing:+An+Interactive+Training+System+for+Argumentative+Writing&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "CATALPA, FernUniversit\u00e4t in Hagen, Germany; CATALPA, FernUniversit\u00e4t in Hagen, Germany; CATALPA, FernUniversit\u00e4t in Hagen, Germany + Leibniz Institute for Science and Mathematics Education, Kiel, Germany + University of Kiel, Germany",
        "aff_domain": "fernuni-hagen.de;mail.de;leibniz-ipn.de",
        "email": "fernuni-hagen.de;mail.de;leibniz-ipn.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1+2",
        "aff_unique_norm": "FernUniversit\u00e4t in Hagen;Leibniz Institute for Science and Mathematics Education;Christian-Albrechts-Universit\u00e4t zu Kiel",
        "aff_unique_dep": "CATALPA;;",
        "aff_unique_url": "https://www.fernuni-hagen.de;https://www.ipn.uni-kiel.de/;https://www.uni-kiel.de",
        "aff_unique_abbr": ";;CAU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Kiel",
        "aff_country_unique_index": "0;0;0+0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.731",
        "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "When carefully optimized by human experts, naive prompts can significantly enhance the task performance of large language models (LLMs). However, such expert-driven prompt optimizations are resource-intensive. To address this, some studies have proposed Automatic Prompt Optimization (APO), which refines naive prompts according to task outputs from in-box testing models, utilizing advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, current approaches face challenges in generalization and privacy risks. To overcome these limitations, we have developed the first large-scale Prompt Optimization Preference (POP) dataset, fine-tuned offline local LLM-based optimizers, and conducted fairly evaluations across various downstream models. Our method, named Free-from Instruction-oriented Prompt Optimization (FIPO), allows precise optimization of the core task instructions in naive prompts in a model-agnostic manner. FIPO uses a modular APO template that dynamically incorporates the naive task instructions, optional instruction responses, and optional ground truth to produce refined prompts. The POP dataset is meticulously constructed using advanced LLMs, undergoing rigorous cross-validation by human experts and analytical models. By leveraging insights from this dataset, along with Tulu2 models and diverse fine-tuning strategies, we validate the efficacy of the FIPO framework across five public benchmarks and six testing models. Our dataset and codes are available at: https://github.com/LuJunru/FIPO_Project.",
        "author": "Junru Lu; Siyu An; Min Zhang; Yulan He; Di Yin; Xing Sun",
        "authorids": "/j/junru-lu/; /s/siyu-an/; /m/min-zhang/; /y/yulan-he/; /d/di-yin/; /x/xing-sun/",
        "bibtex": "@inproceedings{lu-etal-2025-fipo,\n    title = \"{FIPO}: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema\",\n    author = \"Lu, Junru  and\n      An, Siyu  and\n      Zhang, Min  and\n      He, Yulan  and\n      Yin, Di  and\n      Sun, Xing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.731/\",\n    pages = \"11029--11047\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.731.pdf",
        "site": "https://aclanthology.org/2025.coling-main.731/",
        "pdf_size": 2303086,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9734701354982260489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Warwick + Tecent YouTu Lab; Tecent YouTu Lab; East China Normal University; University of Warwick + King\u2019s College London + The Alan Turing Institute; Tecent YouTu Lab; Tecent YouTu Lab",
        "aff_domain": "warwick.ac.uk;tencent.com;cs.ecnu.edu.cn;kcl.ac.uk;tencent.com;tencent.com",
        "email": "warwick.ac.uk;tencent.com;cs.ecnu.edu.cn;kcl.ac.uk;tencent.com;tencent.com",
        "github": "https://github.com/LuJunru/FIPO_Project",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;2;0+3+4;1;1",
        "aff_unique_norm": "University of Warwick;Tencent;East China Normal University;King's College London;The Alan Turing Institute",
        "aff_unique_dep": ";YouTu Lab;;;",
        "aff_unique_url": "https://www.warwick.ac.uk;https://www.tencent.com;http://www.ecnu.edu.cn;https://www.kcl.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "Warwick;Tencent;ECNU;KCL;ATI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;0+0+0;1;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "2025.coling-industry.9",
        "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG\u2019s capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance.",
        "author": "Amit Agarwal; Srikant Panda; Kulbhushan Pachauri",
        "authorids": "/a/amit-agarwal/; /s/srikant-panda/; /k/kulbhushan-pachauri/",
        "bibtex": "@inproceedings{agarwal-etal-2025-fs,\n    title = \"{FS}-{DAG}: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding\",\n    author = \"Agarwal, Amit  and\n      Panda, Srikant  and\n      Pachauri, Kulbhushan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.9/\",\n    pages = \"100--114\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.9.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.9/",
        "pdf_size": 3107954,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10873311548219153747&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "OCI, Oracle USA; OCI, Oracle India; OCI, Oracle India",
        "aff_domain": "oracle.com; ; ",
        "email": "oracle.com; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Oracle USA;Oracle India",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.oracle.com;https://www.oracle.com/in/",
        "aff_unique_abbr": "Oracle;OCI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2025.coling-main.86",
        "title": "FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the massive success of fine-tuning Pre-trained Language Models (PLMs), they remain susceptible to out-of-distribution input. Dataset cartography is a simple yet effective dual-model approach that improves the robustness of fine-tuned PLMs. It involves fine-tuning a model on the original training set (i.e. reference model), selecting a subset of important training instances based on the training dynamics, % of the reference model, and fine-tuning again only on these selected examples (i.e. main model). However, this approach requires fine-tuning the same model twice, which is computationally expensive for large PLMs. In this paper, we show that 1) training dynamics are highly transferable across model sizes and pre-training methods, and that 2) fine-tuning main models using these selected training instances achieves higher training efficiency than empirical risk minimization (ERM). Building on these observations, we propose a novel fine-tuning approach: Fine-Tuning by transFerring Training dynamics (FTFT). Compared with dataset cartography, FTFT uses more efficient reference models and aggressive early stopping. FTFT achieves robustness improvements over ERM while lowering the training cost by up to ~50%",
        "author": "Yupei Du; Albert Gatt; Dong Nguyen",
        "authorids": "/y/yupei-du/; /a/albert-gatt/; /d/dong-nguyen/",
        "bibtex": "@inproceedings{du-etal-2025-ftft,\n    title = \"{FTFT}: Efficient and Robust Fine-Tuning by Transferring Training Dynamics\",\n    author = \"Du, Yupei  and\n      Gatt, Albert  and\n      Nguyen, Dong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.86/\",\n    pages = \"1294--1308\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.86.pdf",
        "site": "https://aclanthology.org/2025.coling-main.86/",
        "pdf_size": 582308,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3UIAVWzIc4gJ:scholar.google.com/&scioq=FTFT:+Efficient+and+Robust+Fine-Tuning+by+Transferring+Training+Dynamics&hl=en&as_sdt=0,5",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/nlpsoc/FTFT",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.302",
        "title": "Factual Dialogue Summarization via Learning from Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Factual consistency is an important quality in dialogue summarization. Large language model (LLM)-based automatic text summarization models generate more factually consistent summaries compared to those by smaller pretrained language models, but they face deployment challenges in real-world applications due to privacy or resource constraints. In this paper, we investigate the use of symbolic knowledge distillation to improve the factual consistency of smaller pretrained models for dialogue summarization. We employ zero-shot learning to extract symbolic knowledge from LLMs, generating both factually consistent (positive) and inconsistent (negative) summaries. We then apply two contrastive learning objectives on these summaries to enhance smaller summarization models. Experiments with BART, PEGASUS, and Flan-T5 indicate that our approach surpasses strong baselines that rely on complex data augmentation strategies. Our approach demonstrates improved factual consistency while preserving coherence, fluency, and relevance, as verified by both automatic evaluation metrics and human assessments. We provide access to the data and code to facilitate future research.",
        "author": "Rongxin Zhu; Jey Han Lau; Jianzhong Qi",
        "authorids": "/r/rongxin-zhu/; /j/jey-han-lau/; /j/jianzhong-qi/",
        "bibtex": "@inproceedings{zhu-etal-2025-factual,\n    title = \"Factual Dialogue Summarization via Learning from Large Language Models\",\n    author = \"Zhu, Rongxin  and\n      Lau, Jey Han  and\n      Qi, Jianzhong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.302/\",\n    pages = \"4474--4492\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.302.pdf",
        "site": "https://aclanthology.org/2025.coling-main.302/",
        "pdf_size": 6207186,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17563992256244369240&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "https://github.com/731935354/symbolic_distill_contrastive_summ",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Melbourne",
        "aff_unique_dep": "School of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2025.coling-main.537",
        "title": "Factual Knowledge Assessment of Language Models Using Distractors",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often relies on cloze sentences, which can lead to erroneous conclusions due to the complexity of natural language (out-of-subject continuations, the existence of many correct answers and the several ways of expressing them). In this paper, we introduce a new interpretable knowledge assessment method that mitigates these issues by leveraging distractors\u2014incorrect but plausible alternatives to the correct answer. We propose several strategies for retrieving distractors and determine the most effective one through experimentation. Our method is evaluated against existing approaches, demonstrating solid alignment with human judgment and stronger robustness to verbalization artifacts. The code and data to reproduce our experiments are available on GitHub.",
        "author": "Hichem Ammar Khodja; Abderrahmane Ait gueni ssaid; Frederic Bechet; Quentin Brabant; Alexis Nasr; Gw\u00e9nol\u00e9 Lecorv\u00e9",
        "authorids": "/h/hichem-ammar-khodja/; /a/abderrahmane-ait-gueni-ssaid/; /f/frederic-bechet/; /q/quentin-brabant/; /a/alexis-nasr/; /g/gwenole-lecorve/",
        "bibtex": "@inproceedings{ammar-khodja-etal-2025-factual,\n    title = \"Factual Knowledge Assessment of Language Models Using Distractors\",\n    author = \"Ammar Khodja, Hichem  and\n      Ait gueni ssaid, Abderrahmane  and\n      Bechet, Frederic  and\n      Brabant, Quentin  and\n      Nasr, Alexis  and\n      Lecorv{\\'e}, Gw{\\'e}nol{\\'e}\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.537/\",\n    pages = \"8043--8056\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.537.pdf",
        "site": "https://aclanthology.org/2025.coling-main.537/",
        "pdf_size": 560372,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:EdH00DHFq7oJ:scholar.google.com/&scioq=Factual+Knowledge+Assessment+of+Language+Models+Using+Distractors&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "github.com/Orange-OpenSource/DistFactAssessLM",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.311",
        "title": "Faithful Inference Chains Extraction for Fact Verification over Multi-view Heterogeneous Graph with Causal Intervention",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "KG-based fact verification verifies the truthfulness of claims by retrieving evidence graphs from the knowledge graph. The *faithful inference chains*, which are precise relation paths between the mentioned entities and evidence entities, retrieve precise evidence graphs addressing poor performance and weak logic for fact verification. Due to the diversity of relation paths, existing methods rarely extract faithful inference chains. To alleviate these issues, we propose Multi-view Heterogeneous Graph with Causal Intervention (MHGCI): (i) We construct a Multi-view Heterogeneous Graph enhancing relation path extraction from the view of different mentioned entities. (ii) We propose a self-optimizing causal intervention model to generate assistant entities mitigating the out-of-distribution problem caused by counterfactual relations. (iii) We propose a grounding method to extract evidence graphs from the KG by faithful inference chains. Experiments on the public KG-based fact verification dataset FactKG demonstrate that our model provides precise evidence graphs and achieves state-of-the-art performance.",
        "author": "Daoqi Chen; Yaxin Li; Zizhong Zhu; Xiaowang Zhang; Zhiyong Feng",
        "authorids": "/d/daoqi-chen/; /y/yaxin-li/; /z/zizhong-zhu/; /x/xiaowang-zhang/; /z/zhiyong-feng/",
        "bibtex": "@inproceedings{chen-etal-2025-faithful,\n    title = \"Faithful Inference Chains Extraction for Fact Verification over Multi-view Heterogeneous Graph with Causal Intervention\",\n    author = \"Chen, Daoqi  and\n      Li, Yaxin  and\n      Zhu, Zizhong  and\n      Zhang, Xiaowang  and\n      Feng, Zhiyong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.311/\",\n    pages = \"4634--4645\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.311.pdf",
        "site": "https://aclanthology.org/2025.coling-main.311/",
        "pdf_size": 1010755,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0uoh-OWf57kJ:scholar.google.com/&scioq=Faithful+Inference+Chains+Extraction+for+Fact+Verification+over+Multi-view+Heterogeneous+Graph+with+Causal+Intervention&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University + The Center of National Railway Intelligent Transportation System Engineering and Technology; College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University; College of Intelligence and Computing, Tianjin University",
        "aff_domain": "tju.edu.cn; ; ;tju.edu.cn; ",
        "email": "tju.edu.cn; ; ;tju.edu.cn; ",
        "github": "https://github.com/CarlosChen1999/MHGCI",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Tianjin University;Center of National Railway Intelligent Transportation System Engineering and Technology",
        "aff_unique_dep": "College of Intelligence and Computing;",
        "aff_unique_url": "http://www.tju.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.676",
        "title": "FarExStance: Explainable Stance Detection for Farsi",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce FarExStance, a new dataset for explainable stance detection in Farsi. Each instance in this dataset contains a claim, the stance of an article or social media post towards that claim, and an extractive explanation which provides evidence for the stance label. We compare the performance of a fine-tuned multilingual RoBERTa model to several large language models in zero-shot, few-shot, and parameter-efficient fine-tuned settings on our new dataset. On stance detection, the most accurate models are the fine-tuned RoBERTa model, the LLM Aya-23-8B which has been fine-tuned using parameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the quality of the explanations, our automatic evaluation metrics indicate that few-shot GPT-4o generates the most coherent explanations, while our human evaluation reveals that the best Overall Explanation Score (OES) belongs to few-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced explanations most closely aligned with the reference explanations.",
        "author": "Majid Zarharan; Maryam Hashemi; Malika Behroozrazegh; Sauleh Eetemadi; Mohammad Taher Pilehvar; Jennifer Foster",
        "authorids": "/m/majid-zarharan/; /m/maryam-hashemi/; /m/malika-behroozrazegh/; /s/sauleh-eetemadi/; /m/mohammad-taher-pilehvar/; /j/jennifer-foster/",
        "bibtex": "@inproceedings{zarharan-etal-2025-farexstance,\n    title = \"{F}ar{E}x{S}tance: Explainable Stance Detection for {F}arsi\",\n    author = \"Zarharan, Majid  and\n      Hashemi, Maryam  and\n      Behroozrazegh, Malika  and\n      Eetemadi, Sauleh  and\n      Pilehvar, Mohammad Taher  and\n      Foster, Jennifer\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.676/\",\n    pages = \"10125--10147\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.676.pdf",
        "site": "https://aclanthology.org/2025.coling-main.676/",
        "pdf_size": 6303025,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7686774178821339545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computing, Dublin City University + Dadmatech, Tehran, Iran; Iran University of Science and Technology + Dadmatech, Tehran, Iran; Dadmatech, Tehran, Iran; School of Computer Science, University of Birmingham; School of Computer Science and Informatics, Cardiff University; School of Computing, Dublin City University",
        "aff_domain": "mail.dcu.ie; ; ; ; ; ",
        "email": "mail.dcu.ie; ; ; ; ; ",
        "github": "https://github.com/Zarharan/FarExStance",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2+1;1;3;4;0",
        "aff_unique_norm": "Dublin City University;Dadmatech;Iran University of Science and Technology;University of Birmingham;Cardiff University",
        "aff_unique_dep": "School of Computing;;;School of Computer Science;School of Computer Science and Informatics",
        "aff_unique_url": "https://www.dcu.ie;;https://www.iust.ac.ir;https://www.birmingham.ac.uk;https://www.cardiff.ac.uk",
        "aff_unique_abbr": "DCU;;IUST;UoB;Cardiff",
        "aff_campus_unique_index": "0;;2;3;0",
        "aff_campus_unique": "Dublin;;Birmingham;Cardiff",
        "aff_country_unique_index": "0+1;1+1;1;2;2;0",
        "aff_country_unique": "Ireland;Iran;United Kingdom"
    },
    {
        "id": "2025.coling-main.581",
        "title": "FedCSR: A Federated Framework for Multi-Platform Cross-Domain Sequential Recommendation with Dual Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-domain sequential recommendation (CSR) has garnered significant attention. Current federated frameworks for CSR leverage information across multiple domains but often rely on user alignment, which increases communication costs and privacy risks. In this work, we propose FedCSR, a novel federated cross-domain sequential recommendation framework that eliminates the need for user alignment between platforms. FedCSR fully utilizes cross-domain knowledge to address the key challenges related to data heterogeneity both inter- and intra-platform. To tackle the heterogeneity of data patterns between platforms, we introduce Model Contrastive Learning (MCL) to reduce the gap between local and global models. Additionally, we design Sequence Contrastive Learning (SCL) to address the heterogeneity of user preferences across different domains within a platform by employing tailored sequence augmentation techniques. Extensive experiments conducted on multiple real-world datasets demonstrate that FedCSR achieves superior performance compared to existing baseline methods.",
        "author": "Dongyi Zheng; Hongyu Zhang; Jianyang Zhai; Lin Zhong; Lingzhi Wang; Jiyuan Feng; Xiangke Liao; Yonghong Tian; Nong Xiao; Qing Liao",
        "authorids": "/d/dongyi-zheng/; /h/hongyu-zhang/; /j/jianyang-zhai/; /l/lin-zhong/; /l/lingzhi-wang/; /j/jiyuan-feng/; /x/xiangke-liao/; /y/yonghong-tian/; /n/nong-xiao/; /q/qing-liao/",
        "bibtex": "@inproceedings{zheng-etal-2025-fedcsr,\n    title = \"{F}ed{CSR}: A Federated Framework for Multi-Platform Cross-Domain Sequential Recommendation with Dual Contrastive Learning\",\n    author = \"Zheng, Dongyi  and\n      Zhang, Hongyu  and\n      Zhai, Jianyang  and\n      Zhong, Lin  and\n      Wang, Lingzhi  and\n      Feng, Jiyuan  and\n      Liao, Xiangke  and\n      Tian, Yonghong  and\n      Xiao, Nong  and\n      Liao, Qing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.581/\",\n    pages = \"8699--8713\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.581.pdf",
        "site": "https://aclanthology.org/2025.coling-main.581/",
        "pdf_size": 1268896,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:g3IBc2tGnH4J:scholar.google.com/&scioq=FedCSR:+A+Federated+Framework+for+Multi-Platform+Cross-Domain+Sequential+Recommendation+with+Dual+Contrastive+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Pengcheng Laboratory+Sun Yat-sen University; Harbin Institute of Technology (Shenzhen); Pengcheng Laboratory+Sun Yat-sen University; Harbin Institute of Technology (Shenzhen); Harbin Institute of Technology (Shenzhen); Pengcheng Laboratory+Harbin Institute of Technology (Shenzhen); Sun Yat-sen University+Pengcheng Laboratory; Peking University+Pengcheng Laboratory; Sun Yat-sen University+Pengcheng Laboratory; Harbin Institute of Technology (Shenzhen)+Pengcheng Laboratory",
        "aff_domain": "mail2.sysu.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;pcl.ac.cn;gmail.com;nudt.edu.cn;nudt.edu.cn;pku.edu.cn;hit.edu.cn",
        "email": "mail2.sysu.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;pcl.ac.cn;gmail.com;nudt.edu.cn;nudt.edu.cn;pku.edu.cn;hit.edu.cn",
        "github": "https://github.com/zdy769243418/FedCSR-v1",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;2;0+1;2;2;0+2;1+0;3+0;1+0;2+0",
        "aff_unique_norm": "Pengcheng Laboratory;Sun Yat-sen University;Harbin Institute of Technology;Peking University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";http://www.sysu.edu.cn/;http://en.hhit.edu.cn/;http://www.pku.edu.cn",
        "aff_unique_abbr": ";SYSU;HIT;Peking U",
        "aff_campus_unique_index": ";1;;1;1;1;;;;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;0;0+0;0;0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.17",
        "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server\u2019s LLM and clients\u2019 SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server\u2019s LLM to clients\u2019 SLMs while concurrently enhancing the LLM with clients\u2019 unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, we evaluate the effectiveness of FedMKT by utilizing diverse public LLMs and SLMs on a variety of NLP text generation tasks. Empirical results demonstrate that FedMKT simultaneously boosts the performance of both LLMs and SLMs. Our code has been contributed to the FATE open-source project and is now publicly accessible at https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/fedmkt",
        "author": "Tao Fan; Guoqiang Ma; Yan Kang; Hanlin Gu; Yuanfeng Song; Lixin Fan; Kai Chen; Qiang Yang",
        "authorids": "/t/tao-fan/; /g/guoqiang-ma/; /y/yan-kang/; /h/hanlin-gu/; /y/yuanfeng-song/; /l/lixin-fan/; /k/kai-chen/; /q/qiang-yang/",
        "bibtex": "@inproceedings{fan-etal-2025-fedmkt,\n    title = \"{F}ed{MKT}: Federated Mutual Knowledge Transfer for Large and Small Language Models\",\n    author = \"Fan, Tao  and\n      Ma, Guoqiang  and\n      Kang, Yan  and\n      Gu, Hanlin  and\n      Song, Yuanfeng  and\n      Fan, Lixin  and\n      Chen, Kai  and\n      Yang, Qiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.17/\",\n    pages = \"243--255\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.17.pdf",
        "site": "https://aclanthology.org/2025.coling-main.17/",
        "pdf_size": 639794,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16417437534238589471&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The Hong Kong University of Science and Technology, Hong Kong, China + WeBank Co., Ltd, Shenzhen, China; WeBank Co., Ltd, Shenzhen, China; WeBank Co., Ltd, Shenzhen, China; WeBank Co., Ltd, Shenzhen, China; WeBank Co., Ltd, Shenzhen, China; WeBank Co., Ltd, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, China + WeBank Co., Ltd, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, China + WeBank Co., Ltd, Shenzhen, China",
        "aff_domain": "cse.ust.hk; ; ; ; ; ; ; ",
        "email": "cse.ust.hk; ; ; ; ; ; ; ",
        "github": "https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/fedmkt",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;1;0+1;0+1",
        "aff_unique_norm": "The Hong Kong University of Science and Technology;WeBank Co., Ltd",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://www.webank.com",
        "aff_unique_abbr": "HKUST;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.13",
        "title": "Federated Incremental Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Federated learning-based Named Entity Recognition (FNER) has attracted widespread attention through decentralized training on local clients. However, most FNER models assume that entity types are pre-fixed, so in practical applications, local clients constantly receive new entity types without enough storage to access old entity types, resulting in severe forgetting on previously learned knowledge. In addition, new clients collecting only new entity types may join the global training of FNER irregularly, further exacerbating catastrophic forgetting. To overcome the above challenges, we propose a Forgetting-Subdued Learning (FSL) model which solves the forgetting problem on old entity types from both intra-client and inter-client two aspects. Specifically, for intra-client aspect, we propose a prototype-guided adaptive pseudo labeling and a prototypical relation distillation loss to surmount catastrophic forgetting of old entity types with semantic shift. Furthermore, for inter-client aspect, we propose a task transfer detector. It can identify the arrival of new entity types that are protected by privacy and store the latest old global model for relation distillation. Qualitative experiments have shown that our model has made significant improvements compared to several baseline methods.",
        "author": "Zesheng Liu; Qiannan Zhu; Cuiping Li; Hong Chen",
        "authorids": "/z/zesheng-liu/; /q/qiannan-zhu/; /c/cuiping-li/; /h/hong-chen/",
        "bibtex": "@inproceedings{liu-etal-2025-federated,\n    title = \"Federated Incremental Named Entity Recognition\",\n    author = \"Liu, Zesheng  and\n      Zhu, Qiannan  and\n      Li, Cuiping  and\n      Chen, Hong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.13/\",\n    pages = \"188--198\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.13.pdf",
        "site": "https://aclanthology.org/2025.coling-main.13/",
        "pdf_size": 888771,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fFQkaJwbBnYJ:scholar.google.com/&scioq=Federated+Incremental+Named+Entity+Recognition&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "School of Information, Renmin University of China, Beijing, China+Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China+Engineering Research Center of Database and Business Intelligence, MOE, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China+Engineering Research Center of Intelligent Technology and Educational Application, MOE, China; School of Information, Renmin University of China, Beijing, China+Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China; Engineering Research Center of Database and Business Intelligence, MOE, China",
        "aff_domain": "ruc.edu.cn;bnu.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;bnu.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;3+4;0+1;2",
        "aff_unique_norm": "Renmin University of China;Key Laboratory of Data Engineering and Knowledge Engineering;Engineering Research Center of Database and Business Intelligence;Beijing Normal University;Engineering Research Center of Intelligent Technology and Educational Application",
        "aff_unique_dep": "School of Information;Data Engineering and Knowledge Engineering;MOE;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ruc.edu.cn;;;https://www.bnu.edu.cn;",
        "aff_unique_abbr": "RUC;;;BNU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.33",
        "title": "Federated Retrieval Augmented Generation for Multi-Product Question Answering",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recent advancements in Large Language Models and Retrieval-Augmented Generation have boosted interest in domain-specific question-answering for enterprise products. However, AI Assistants often face challenges in multi-product QA settings, requiring accurate responses across diverse domains. Existing multi-domain RAG-QA approaches either query all domains indiscriminately, increasing computational costs and LLM hallucinations, or rely on rigid resource selection, which can limit search results. We introduce MKP-QA, a novel multi-product knowledge-augmented QA framework with probabilistic federated search across domains and relevant knowledge. This method enhances multi-domain search quality by aggregating query-domain and query-passage probabilistic relevance. To address the lack of suitable benchmarks for multi-product QAs, we also present new datasets focused on three Adobe products: Adobe Experience Platform, Target, and Customer Journey Analytics. Our experiments show that MKP-QA significantly boosts multi-product RAG-QA performance in terms of both retrieval accuracy and response quality.",
        "author": "Parshin Shojaee; Sai Sree Harsha; Dan Luo; Akash Maharaj; Tong Yu; Yunyao Li",
        "authorids": "/p/parshin-shojaee/; /s/sai-sree-harsha/; /d/dan-luo/; /a/akash-maharaj/; /t/tong-yu/; /y/yunyao-li/",
        "bibtex": "@inproceedings{shojaee-etal-2025-federated,\n    title = \"Federated Retrieval Augmented Generation for Multi-Product Question Answering\",\n    author = \"Shojaee, Parshin  and\n      Harsha, Sai Sree  and\n      Luo, Dan  and\n      Maharaj, Akash  and\n      Yu, Tong  and\n      Li, Yunyao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.33/\",\n    pages = \"387--397\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.33.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.33/",
        "pdf_size": 2081105,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7294914577890576496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Virginia Tech; Adobe; Adobe; Adobe; Adobe; Adobe",
        "aff_domain": "vt.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "vt.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Virginia Tech;Adobe Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vt.edu;https://www.adobe.com",
        "aff_unique_abbr": "VT;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.662",
        "title": "Few-shot domain adaptation for named-entity recognition via joint constrained k-means and subspace selection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly-supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints, and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER, demonstrating its effectiveness in leveraging unlabeled data and adapting to domain-specific challenges.",
        "author": "Ayoub Hammal; Benno Uthayasooriyar; Caio Corro",
        "authorids": "/a/ayoub-hammal/; /b/benno-uthayasooriyar/; /c/caio-corro/",
        "bibtex": "@inproceedings{hammal-etal-2025-shot,\n    title = \"Few-shot domain adaptation for named-entity recognition via joint constrained k-means and subspace selection\",\n    author = \"Hammal, Ayoub  and\n      Uthayasooriyar, Benno  and\n      Corro, Caio\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.662/\",\n    pages = \"9902--9916\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.662.pdf",
        "site": "https://aclanthology.org/2025.coling-main.662/",
        "pdf_size": 572515,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14608855054891023187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Universit\u00e9 Paris-Saclay, CNRS, LISN; Data Analytics Solutions, SCOR+LMBA, CNRS, Universit\u00e9 de Brest; INSA Rennes, IRISA, Inria, CNRS, Universit\u00e9 de Rennes",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay;SCOR;Universit\u00e9 de Brest;INSA Rennes",
        "aff_unique_dep": "CNRS, LISN;Data Analytics Solutions;LMBA;",
        "aff_unique_url": "https://www.universite-paris-saclay.fr;https://www.scor.com;https://www.univ-brest.fr;https://www.insa-rennes.fr",
        "aff_unique_abbr": "UPS;SCOR;UBrest;INSA",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Rennes",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.740",
        "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a filter-then-generate paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at https://github.com/LB0828/FtG.",
        "author": "Ben Liu; Jihai Zhang; Fangquan Lin; Cheng Yang; Min Peng",
        "authorids": "/b/ben-liu/; /j/jihai-zhang/; /f/fangquan-lin/; /c/cheng-yang/; /m/min-peng/",
        "bibtex": "@inproceedings{liu-etal-2025-filter,\n    title = \"Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion\",\n    author = \"Liu, Ben  and\n      Zhang, Jihai  and\n      Lin, Fangquan  and\n      Yang, Cheng  and\n      Peng, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.740/\",\n    pages = \"11181--11195\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.740.pdf",
        "site": "https://aclanthology.org/2025.coling-main.740/",
        "pdf_size": 1140653,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6340188584180447372&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Wuhan University, China+DAMO Academy, Alibaba Group, Hangzhou, 310023, China; DAMO Academy, Alibaba Group, Hangzhou, 310023, China; DAMO Academy, Alibaba Group, Hangzhou, 310023, China; DAMO Academy, Alibaba Group, Hangzhou, 310023, China; School of Computer Science, Wuhan University, China",
        "aff_domain": "whu.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;whu.edu.cn",
        "email": "whu.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;whu.edu.cn",
        "github": "https://github.com/LB0828/FtG",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;0",
        "aff_unique_norm": "Wuhan University;Alibaba Group",
        "aff_unique_dep": "School of Computer Science;DAMO Academy",
        "aff_unique_url": "http://www.whu.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "WHU;Alibaba",
        "aff_campus_unique_index": "0+1;1;1;1;0",
        "aff_campus_unique": "Wuhan;Hangzhou",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.48",
        "title": "FinDABench: Benchmarking Financial Data Analysis Ability of Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of financial data analysis, particularly focusing on data-driven thinking, remain uncertain. To bridge this gap, we introduce FinDABench, a comprehensive benchmark designed to evaluate the financial data analysis capabilities of LLMs within this context. The benchmark comprises 15,200 training instances and 8,900 test instances, all meticulously crafted by human experts. FinDABench assesses LLMs across three dimensions: 1) Core Ability, evaluating the models\u2019 ability to perform financial indicator calculation and corporate sentiment risk assessment; 2) Analytical Ability, determining the models\u2019 ability to quickly comprehend textual information and analyze abnormal financial reports; and 3) Technical Ability, examining the models\u2019 use of technical knowledge to address real-world data analysis challenges involving analysis generation and charts visualization from multiple perspectives. We will release FinDABench, and the evaluation scripts at https://github.com/xxx. FinDABench aims to provide a measure for in-depth analysis of LLM abilities and foster the advancement of LLMs in the field of financial data analysis.",
        "author": "Shu Liu; Shangqing Zhao; Chenghao Jia; Xinlin Zhuang; Zhaoguang Long; Jie Zhou; Aimin Zhou; Man Lan; Yang Chong",
        "authorids": "/s/shu-liu/; /s/shangqing-zhao/; /c/chenghao-jia/; /x/xinlin-zhuang/; /z/zhaoguang-long/; /j/jie-zhou/; /a/aimin-zhou/; /m/man-lan/; /y/yang-chong/",
        "bibtex": "@inproceedings{liu-etal-2025-findabench,\n    title = \"{F}in{DAB}ench: Benchmarking Financial Data Analysis Ability of Large Language Models\",\n    author = \"Liu, Shu  and\n      Zhao, Shangqing  and\n      Jia, Chenghao  and\n      Zhuang, Xinlin  and\n      Long, Zhaoguang  and\n      Zhou, Jie  and\n      Zhou, Aimin  and\n      Lan, Man  and\n      Chong, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.48/\",\n    pages = \"710--725\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.48.pdf",
        "site": "https://aclanthology.org/2025.coling-main.48/",
        "pdf_size": 1882395,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7452625017655929301&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 1Lab of Artificial Intelligence for Education, East China Normal University + 2Shanghai Institute of Artificial Intelligence for Education, East China Normal University + 3School of Computer Science and Technology, East China Normal University; 4Bytedance",
        "aff_domain": "stu.ecnu.edu.cn; ; ; ; ; ; ;cs.ecnu.edu.cn; ",
        "email": "stu.ecnu.edu.cn; ; ; ; ; ; ;cs.ecnu.edu.cn; ",
        "github": "https://github.com/cubenlp/FinDABench",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;1",
        "aff_unique_norm": "East China Normal University;ByteDance",
        "aff_unique_dep": "Lab of Artificial Intelligence for Education;",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.bytedance.com",
        "aff_unique_abbr": ";ByteDance",
        "aff_campus_unique_index": "1;1;1;1;1;1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.482",
        "title": "Fine-Grained Features-based Code Search for Precise Query-Code Matching",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Code search aims to quickly locate target code snippets from databases using natural language queries, which promotes code reusability. Existing methods can effectively obtain aligned token-level and query word-level features. However, these studies usually represent the semantics of code and query by averaging the features of each token and word respectively, which makes it difficult to accurately capture the code details that are closely related to the query. To address this issue, we propose a fine-grained code search model that consists of a cross-modal encoder, a mapping layer, and a classification layer. Specifically, we utilize a pre-trained model, GraphCodeBERT, in the cross-modal encoder to align features. In the mapping layer, we introduce a co-attention network to capture the fine-grained interactions between code and query, ensuring a model can precisely identify key code segments relevant to the query. Finally, in the classification layer, we incorporate instruction learning techniques that leverage contextual reasoning to improve the accuracy of query-code matching. Experimental results show that our proposed model significantly outperforms existing methods across multiple programming language datasets.",
        "author": "Xinting Zhang; Mengqiu Cheng; Mengzhen Wang; Songwen Gong; Jiayuan Xie; Yi Cai; Qing Li",
        "authorids": "/x/xinting-zhang/; /m/mengqiu-cheng/; /m/mengzhen-wang/; /s/songwen-gong/; /j/jiayuan-xie/; /y/yi-cai/; /q/qing-li/",
        "bibtex": "@inproceedings{zhang-etal-2025-fine,\n    title = \"Fine-Grained Features-based Code Search for Precise Query-Code Matching\",\n    author = \"Zhang, Xinting  and\n      Cheng, Mengqiu  and\n      Wang, Mengzhen  and\n      Gong, Songwen  and\n      Xie, Jiayuan  and\n      Cai, Yi  and\n      Li, Qing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.482/\",\n    pages = \"7229--7238\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.482.pdf",
        "site": "https://aclanthology.org/2025.coling-main.482/",
        "pdf_size": 660680,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=894931077027032406&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Mathematics, The University of Hong Kong; Guangdong Neusoft University; School of Software Engineering, South China University of Technology; School of Software Engineering, South China University of Technology; School of Software Engineering, South China University of Technology; Department of Computing, The Hong Kong Polytechnic University; Department of Computing, The Hong Kong Polytechnic University",
        "aff_domain": "; ; ; ; ;polyu.edu.hk; ",
        "email": "; ; ; ; ;polyu.edu.hk; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;2;2;3;3",
        "aff_unique_norm": "The University of Hong Kong;Guangdong Neusoft University;South China University of Technology;The Hong Kong Polytechnic University",
        "aff_unique_dep": "Department of Mathematics;;School of Software Engineering;Department of Computing",
        "aff_unique_url": "https://www.hku.hk;http://www.neusoft.edu.cn;https://www.scut.edu.cn;https://www.polyu.edu.hk",
        "aff_unique_abbr": "HKU;;SCUT;PolyU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.21",
        "title": "Fine-Tuning Medium-Scale LLMs for Joint Intent Classification and Slot Filling: A Data-Efficient and Cost-Effective Solution for SMEs",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Dialogue Systems (DS) are increasingly in demand for automating tasks through natural language interactions. However, the core techniques for user comprehension in DS depend heavily on large amounts of labeled data, limiting their applicability in data-scarce environments common to many companies. This paper identifies best practices for data-efficient development and cost-effective deployment of DS in real-world application scenarios. We evaluate whether fine-tuning a medium-sized Large Language Model (LLM) for joint Intent Classification (IC) and Slot Filling (SF), with moderate hardware resource requirements still affordable by SMEs, can achieve competitive performance using less data compared to current state-of-the-art models. Experiments on the Spanish and English portions of the MASSIVE corpus demonstrate that the Llama-3-8B-Instruct model fine-tuned with only 10% of the data outperforms the JointBERT architecture and GPT-4o in a zero-shot prompting setup in monolingual settings. In cross-lingual scenarios, Llama-3-8B-Instruct drastically outperforms multilingual JointBERT demonstrating a vastly superior performance when fine-tuned in a language and evaluated in the other.",
        "author": "Maia Aguirre; Ariane M\u00e9ndez; Arantza del Pozo; Maria Ines Torres; Manuel Torralbo",
        "authorids": "/m/maia-aguirre/; /a/ariane-mendez/; /a/arantza-del-pozo/; /m/maria-ines-torres/; /m/manuel-torralbo/",
        "bibtex": "@inproceedings{aguirre-etal-2025-fine,\n    title = \"Fine-Tuning Medium-Scale {LLM}s for Joint Intent Classification and Slot Filling: A Data-Efficient and Cost-Effective Solution for {SME}s\",\n    author = \"Aguirre, Maia  and\n      M{\\'e}ndez, Ariane  and\n      del Pozo, Arantza  and\n      Torres, Maria Ines  and\n      Torralbo, Manuel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.21/\",\n    pages = \"251--262\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.21.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.21/",
        "pdf_size": 658799,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Wluorb5Qf4EJ:scholar.google.com/&scioq=Fine-Tuning+Medium-Scale+LLMs+for+Joint+Intent+Classification+and+Slot+Filling:+A+Data-Efficient+and+Cost-Effective+Solution+for+SMEs&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Vicomtech Foundation, Basque Research and Technology Alliance (BRTA) + University of the Basque Country (UPV/EHU); Vicomtech Foundation, Basque Research and Technology Alliance (BRTA); Vicomtech Foundation, Basque Research and Technology Alliance (BRTA); University of the Basque Country (UPV/EHU); Vicomtech Foundation, Basque Research and Technology Alliance (BRTA)",
        "aff_domain": "vicomtech.org;vicomtech.org;vicomtech.org;vicomtech.org;vicomtech.org",
        "email": "vicomtech.org;vicomtech.org;vicomtech.org;vicomtech.org;vicomtech.org",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;1;0",
        "aff_unique_norm": "Vicomtech Foundation;University of the Basque Country",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vicomtech.org;https://www.ehu.eus/en",
        "aff_unique_abbr": "Vicomtech;UPV/EHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-main.298",
        "title": "Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination rate in legal QA, we first introduce a benchmark called LegalHalBench and three automatic metrics to evaluate the common hallucinations when LLMs answer legal questions. We then propose a hallucination mitigation method that integrates behavior cloning and a novel Hard Sample-aware Iterative Direct Preference Optimization (HIPO). We conduct extensive real-data experiments to validate the effectiveness of our approach. Our results demonstrate remarkable improvements in various metrics, including the newly proposed Non-Hallucinated Statute Rate, Statute Relevance Rate, Legal Claim Truthfulness, as well as traditional metrics such as METEOR, BERTScore, ROUGE-L, and win rates.",
        "author": "Yinghao Hu; Leilei Gan; Wenyi Xiao; Kun Kuang; Fei Wu",
        "authorids": "/y/yinghao-hu/; /l/leilei-gan/; /w/wenyi-xiao/; /k/kun-kuang/; /f/fei-wu/",
        "bibtex": "@inproceedings{hu-etal-2025-fine,\n    title = \"Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering\",\n    author = \"Hu, Yinghao  and\n      Gan, Leilei  and\n      Xiao, Wenyi  and\n      Kuang, Kun  and\n      Wu, Fei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.298/\",\n    pages = \"4410--4427\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.298.pdf",
        "site": "https://aclanthology.org/2025.coling-main.298/",
        "pdf_size": 1616665,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16932547768540479682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Software Technology, Zhejiang University; School of Software Technology, Zhejiang University; School of Software Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/YinghaoHu/LegalHalBench",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "School of Software Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.741",
        "title": "FineRAG: Fine-grained Retrieval-Augmented Text-to-Image Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in text-to-image generation, notably the series of Stable Diffusion methods, have enabled the production of diverse, high-quality photo-realistic images. Nevertheless, these techniques still exhibit limitations in terms of knowledge access. Retrieval-augmented image generation is a straightforward way to tackle this problem. Current studies primarily utilize coarse-grained retrievers, employing initial prompts as search queries for knowledge retrieval. This approach, however, is ineffective in accessing valuable knowledge in long-tail text-to-image generation scenarios. To alleviate this problem, we introduce FineRAG, a fine-grained model that systematically breaks down the retrieval-augmented image generation task into four critical stages: query decomposition, candidate selection, retrieval-augmented diffusion, and self-reflection. Experimental results on both general and long-tailed benchmarks show that our proposed method significantly reduces the noise associated with retrieval-augmented image generation and performs better in complex, open-world scenarios.",
        "author": "Huaying Yuan; Ziliang Zhao; Shuting Wang; Shitao Xiao; Minheng Ni; Zheng Liu; Zhicheng Dou",
        "authorids": "/h/huaying-yuan/; /z/ziliang-zhao/; /s/shuting-wang/; /s/shitao-xiao/; /m/minheng-ni/; /z/zheng-liu/; /z/zhicheng-dou/",
        "bibtex": "@inproceedings{yuan-etal-2025-finerag,\n    title = \"{F}ine{RAG}: Fine-grained Retrieval-Augmented Text-to-Image Generation\",\n    author = \"Yuan, Huaying  and\n      Zhao, Ziliang  and\n      Wang, Shuting  and\n      Xiao, Shitao  and\n      Ni, Minheng  and\n      Liu, Zheng  and\n      Dou, Zhicheng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.741/\",\n    pages = \"11196--11205\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.741.pdf",
        "site": "https://aclanthology.org/2025.coling-main.741/",
        "pdf_size": 5802146,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7446707311767373896&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Academy of Artificial Intelligence; The Hong Kong Polytechnic University; Beijing Academy of Artificial Intelligence; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn; ; ; ; ; ;ruc.edu.cn",
        "email": "ruc.edu.cn; ; ; ; ; ;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;2;1;0",
        "aff_unique_norm": "Renmin University of China;Beijing Academy of Artificial Intelligence;The Hong Kong Polytechnic University",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;;",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.baaic.cn;https://www.polyu.edu.hk",
        "aff_unique_abbr": "RUC;BAAI;PolyU",
        "aff_campus_unique_index": "0;0;0;2;0",
        "aff_campus_unique": "Beijing;;Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.225",
        "title": "Finetuning LLMs for Comparative Assessment Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automated assessment in natural language generation is a challenging task. Instruction-tuned large language models (LLMs) have shown promise in reference-free evaluation, particularly through comparative assessment. However, the quadratic computational complexity of pairwise comparisons limits its scalability. To address this, efficient comparative assessment has been explored by applying comparative strategies on zero-shot LLM probabilities. We propose a framework for finetuning LLMs for comparative assessment to align the model\u2019s output with the target distribution of comparative probabilities. By training on soft probabilities, our approach improves state-of-the-art performance while maintaining high performance with an efficient subset of comparisons.",
        "author": "Vatsal Raina; Adian Liusie; Mark Gales",
        "authorids": "/v/vatsal-raina/; /a/adian-liusie/; /m/mark-gales/",
        "bibtex": "@inproceedings{raina-etal-2025-finetuning,\n    title = \"Finetuning {LLM}s for Comparative Assessment Tasks\",\n    author = \"Raina, Vatsal  and\n      Liusie, Adian  and\n      Gales, Mark\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.225/\",\n    pages = \"3345--3352\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.225.pdf",
        "site": "https://aclanthology.org/2025.coling-main.225/",
        "pdf_size": 774637,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nUCmYhL83jYJ:scholar.google.com/&scioq=Finetuning+LLMs+for+Comparative+Assessment+Tasks&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "ALTA Institute, University of Cambridge; ALTA Institute, University of Cambridge; ALTA Institute, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "ALTA Institute",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.575",
        "title": "Flashback: Memory Mechanism for Enhancing Memory Efficiency and Speed in Deep Sequential Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this study, we tackle three main challenges of deep sequential processing models in previous research: (1) memory degradation, (2) inaccurate gradient backpropagation, and (3) compatibility with next-token prediction. Specifically, to address (1-2), we define a Flashback property in which memory is preserved perfectly as an identity mapping of its stored value in a memory region until it is overwritten by a hidden state at a different time step. We propose a Flashback mechanism that satisfies this property in a fully differentiable, end-to-end manner. Further, to tackle (3), we propose architectures that incorporate the Flashback mechanism into Transformers and Mamba, enabling next-token prediction for language modeling tasks. In experiments, we trained on The Pile dataset, which includes diverse texts, to evaluate tradeoffs between commonsense reasoning accuracy, processing speed, and memory usage after introducing the Flashback mechanism into existing methods. The evaluations confirmed the effectiveness of the Flashback mechanism.",
        "author": "Taiki Sekii",
        "authorids": "/t/taiki-sekii/",
        "bibtex": "@inproceedings{sekii-2025-flashback,\n    title = \"Flashback: Memory Mechanism for Enhancing Memory Efficiency and Speed in Deep Sequential Models\",\n    author = \"Sekii, Taiki\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.575/\",\n    pages = \"8602--8611\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.575.pdf",
        "site": "https://aclanthology.org/2025.coling-main.575/",
        "pdf_size": 430798,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hiFMhbM84U8J:scholar.google.com/&scioq=Flashback:+Memory+Mechanism+for+Enhancing+Memory+Efficiency+and+Speed+in+Deep+Sequential+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "CyberAgent",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "CyberAgent",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cyberagent.co.jp",
        "aff_unique_abbr": "CA",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.723",
        "title": "From Chaotic OCR Words to Coherent Document: A Fine-to-Coarse Zoom-Out Network for Complex-Layout Document Image Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document Image Translation (DIT) aims to translate documents in images from one language to another. It requires visual layouts and textual contents understanding, as well as document coherence capturing. However, current methods often rely on the quality of OCR output, which, particularly in complex-layout scenarios, frequently loses the crucial document coherence, leading to chaotic text. To overcome this problem, we introduce a novel end-to-end network, named Zoom-out DIT (ZoomDIT), inspired by human translation procedures. It jointly accomplishes the multi-level tasks including word positioning, sentence recognition & translation, and document organization, based on a fine-to-coarse zoom-out framework, to progressively realize \u201cchaotic words to coherent document\u201d and improve translation. We further contribute a new large-scale DIT dataset with multi-level fine-grained labels. Extensive experiments on public and our new dataset demonstrate significant improvements in translation quality towards complex-layout document images, offering a robust solution for reorganizing the chaotic OCR outputs to a coherent document translation.",
        "author": "Zhiyang Zhang; Yaping Zhang; Yupu Liang; Lu Xiang; Yang Zhao; Yu Zhou; Chengqing Zong",
        "authorids": "/z/zhiyang-zhang/; /y/yaping-zhang/; /y/yupu-liang/; /l/lu-xiang/; /y/yang-zhao/; /y/yu-zhou/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{zhang-etal-2025-chaotic,\n    title = \"From Chaotic {OCR} Words to Coherent Document: A Fine-to-Coarse Zoom-Out Network for Complex-Layout Document Image Translation\",\n    author = \"Zhang, Zhiyang  and\n      Zhang, Yaping  and\n      Liang, Yupu  and\n      Xiang, Lu  and\n      Zhao, Yang  and\n      Zhou, Yu  and\n      Zong, Chengqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.723/\",\n    pages = \"10877--10890\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.723.pdf",
        "site": "https://aclanthology.org/2025.coling-main.723/",
        "pdf_size": 5318123,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=371578023077753747&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.141",
        "title": "From Detection to Explanation: Effective Learning Strategies for LLMs in Online Abusive Language Research",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Abusive language detection relies on understanding different levels of intensity, expressiveness and targeted groups, which requires commonsense reasoning, world knowledge and linguistic nuances that evolve over time. Here, we frame the problem as a knowledge-guided learning task, and demonstrate that LLMs\u2019 implicit knowledge without an accurate strategy is not suitable for multi-class detection nor explanation generation. We publicly release GLlama Alarm, the knowledge-Guided version of Llama-2 instruction fine-tuned for multi-class abusive language detection and explanation generation. By being fine-tuned on structured explanations and external reliable knowledge sources, our model mitigates bias and generates explanations that are relevant to the text and coherent with human reasoning, with an average 48.76% better alignment with human judgment according to our expert survey.",
        "author": "Chiara Di Bonaventura; Lucia Siciliani; Pierpaolo Basile; Albert Merono Penuela; Barbara McGillivray",
        "authorids": "/c/chiara-di-bonaventura/; /l/lucia-siciliani/; /p/pierpaolo-basile/; /a/albert-merono-penuela/; /b/barbara-mcgillivray/",
        "bibtex": "@inproceedings{di-bonaventura-etal-2025-detection,\n    title = \"From Detection to Explanation: Effective Learning Strategies for {LLM}s in Online Abusive Language Research\",\n    author = \"Di Bonaventura, Chiara  and\n      Siciliani, Lucia  and\n      Basile, Pierpaolo  and\n      Merono Penuela, Albert  and\n      McGillivray, Barbara\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.141/\",\n    pages = \"2067--2084\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.141.pdf",
        "site": "https://aclanthology.org/2025.coling-main.141/",
        "pdf_size": 995578,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J49sARyInU8J:scholar.google.com/&scioq=From+Detection+to+Explanation:+Effective+Learning+Strategies+for+LLMs+in+Online+Abusive+Language+Research&hl=en&as_sdt=0,14",
        "gs_version_total": 4,
        "aff": "King\u2019s College London + Imperial College London; University of Bari Aldo Moro; University of Bari Aldo Moro; King\u2019s College London; King\u2019s College London",
        "aff_domain": "kcl.ac.uk; ; ; ; ",
        "email": "kcl.ac.uk; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;2;0;0",
        "aff_unique_norm": "King's College London;Imperial College London;University of Bari Aldo Moro",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kcl.ac.uk;https://www.imperial.ac.uk;https://www.uniba.it",
        "aff_unique_abbr": "KCL;ICL;UNIBA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;1;0;0",
        "aff_country_unique": "United Kingdom;Italy"
    },
    {
        "id": "2025.coling-main.705",
        "title": "From Facts to Insights: A Study on the Generation and Evaluation of Analytical Reports for Deciphering Earnings Calls",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores the use of Large Language Models (LLMs) in the generation and evaluation of analytical reports derived from Earnings Calls (ECs). Addressing a current gap in research, we explore the generation of analytical reports with LLMs in a multi-agent framework, designing specialized agents that introduce diverse viewpoints and desirable topics of analysis into the report generation process. Through multiple analyses, we examine the alignment between generated and human-written reports and the impact of both individual and collective agents. Our findings suggest that the introduction of additional agents results in more insightful reports, although reports generated by human experts remain preferred in the majority of cases. Finally, we address the challenging issue of report evaluation, we examine the limitations and strengths of LLMs in assessing the quality of generated reports in different settings, revealing a significant correlation with human experts across multiple dimensions.",
        "author": "Tomas Goldsack; Yang Wang; Chenghua Lin; Chung-Chi Chen",
        "authorids": "/t/tomas-goldsack/; /y/yang-wang/; /c/chenghua-lin/; /c/chung-chi-chen/",
        "bibtex": "@inproceedings{goldsack-etal-2025-facts,\n    title = \"From Facts to Insights: A Study on the Generation and Evaluation of Analytical Reports for Deciphering Earnings Calls\",\n    author = \"Goldsack, Tomas  and\n      Wang, Yang  and\n      Lin, Chenghua  and\n      Chen, Chung-Chi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.705/\",\n    pages = \"10576--10593\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.705.pdf",
        "site": "https://aclanthology.org/2025.coling-main.705/",
        "pdf_size": 1259230,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2362146380274903480&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Sheffield, UK + Department of Computer Science, University of Manchester, UK; Department of Computer Science, University of Sheffield, UK; Department of Computer Science, University of Manchester, UK; Artificial Intelligence Research Center, AIST, Japan",
        "aff_domain": "sheffield.ac.uk;sheffield.ac.uk;manchester.ac.uk;acm.org",
        "email": "sheffield.ac.uk;sheffield.ac.uk;manchester.ac.uk;acm.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;1;2",
        "aff_unique_norm": "University of Sheffield;University of Manchester;Advanced Institute of Science and Technology",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;Artificial Intelligence Research Center",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.manchester.ac.uk;https://www.aist.go.jp",
        "aff_unique_abbr": "Sheffield;UoM;AIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;1",
        "aff_country_unique": "United Kingdom;Japan"
    },
    {
        "id": "2025.coling-main.147",
        "title": "From Form to Meaning: The Case of Particles within the Prague Dependency Treebank Annotation Scheme",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the last decades, computational linguistics has become increasingly interested in annotation schemes that aim at an adequate description of the meaning of the sentences and texts. Discussions are ongoing on an appropriate annotation scheme for a large and complex amount of diverse information. In this contribution devoted to description of polyfunctional uninflected words (namely particles), i.e. words which, although having only one paradigmatic form, can have several different syntactic functions and even express relatively different semantic distinctions, we argue that it is the multi-layer system (linked from meaning to text) that allows a comprehensive description of the relations between morphological properties, syntactic function and expressed meaning, and thus contributes to greater accuracy in the description of the phenomena concerned and to the overall consistency of the annotated data. These aspects are demonstrated within the Prague Dependency Treebank annotation scheme, whose pioneering proposal can be found in the first COLING proceedings from 1965 (Sgall 1965), and to this day, the concept has proved to be sound and serves very well for complex annotation.",
        "author": "Marie Mikulova; Barbora \u0160t\u011bp\u00e1nkov\u00e1; Jan \u0160t\u011bp\u00e1nek",
        "authorids": "/m/marie-mikulova/; /b/barbora-stepankova/; /j/jan-stepanek/",
        "bibtex": "@inproceedings{mikulova-etal-2025-form,\n    title = \"From Form to Meaning: The Case of Particles within the {P}rague Dependency Treebank Annotation Scheme\",\n    author = \"Mikulova, Marie  and\n      {\\v{S}}t{\\v{e}}p{\\'a}nkov{\\'a}, Barbora  and\n      {\\v{S}}t{\\v{e}}p{\\'a}nek, Jan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.147/\",\n    pages = \"2163--2175\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.147.pdf",
        "site": "https://aclanthology.org/2025.coling-main.147/",
        "pdf_size": 580180,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J0-D9baG7b4J:scholar.google.com/&scioq=From+Form+to+Meaning:+The+Case+of+Particles+within+the+Prague+Dependency+Treebank+Annotation+Scheme&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Charles University, Faculty of Mathematics and Physics; Charles University, Faculty of Mathematics and Physics; Charles University, Faculty of Mathematics and Physics",
        "aff_domain": "ufal.mff.cuni.cz;ufal.mff.cuni.cz;ufal.mff.cuni.cz",
        "email": "ufal.mff.cuni.cz;ufal.mff.cuni.cz;ufal.mff.cuni.cz",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Charles University",
        "aff_unique_dep": "Faculty of Mathematics and Physics",
        "aff_unique_url": "https://www.cuni.cz",
        "aff_unique_abbr": "Charles U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "2025.coling-main.74",
        "title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have significantly transformed our daily life and established a new paradigm in natural language processing (NLP). However, the predominant pretraining of LLMs on extensive web-based texts remains insufficient for advanced scientific discovery, particularly in chemistry. The scarcity of specialized chemistry data, coupled with the complexity of multi-modal data such as 2D graph, 3D structure and spectrum, present distinct challenges. Although several studies have reviewed Pretrained Language Models (PLMs) in chemistry, there is a conspicuous absence of a systematic survey specifically focused on chemistry-oriented LLMs. In this paper, we outline methodologies for incorporating domain-specific chemistry knowledge and multi-modal information into LLMs, we also conceptualize chemistry LLMs as agents using chemistry tools and investigate their potential to accelerate scientific research. Additionally, we conclude the existing benchmarks to evaluate chemistry ability of LLMs. Finally, we critically examine the current challenges and identify promising directions for future research. Through this comprehensive survey, we aim to assist researchers in staying at the forefront of developments in chemistry LLMs and to inspire innovative applications in the field.",
        "author": "Yang Han; Ziping Wan; Lu Chen; Kai Yu; Xin Chen",
        "authorids": "/y/yang-han/; /z/ziping-wan/; /l/lu-chen/; /k/kai-yu/; /x/xin-chen/",
        "bibtex": "@inproceedings{han-etal-2025-generalist,\n    title = \"From Generalist to Specialist: A Survey of Large Language Models for Chemistry\",\n    author = \"Han, Yang  and\n      Wan, Ziping  and\n      Chen, Lu  and\n      Yu, Kai  and\n      Chen, Xin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.74/\",\n    pages = \"1106--1123\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.74.pdf",
        "site": "https://aclanthology.org/2025.coling-main.74/",
        "pdf_size": 6865976,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7820499623761475435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China + Suzhou Laboratory, Suzhou, China; Suzhou Laboratory, Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China + Suzhou Laboratory, Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China + Suzhou Laboratory, Suzhou, China; Suzhou Laboratory, Suzhou, China",
        "aff_domain": "sjtu.edu.cn; ;sjtu.edu.cn; ;gmail.com",
        "email": "sjtu.edu.cn; ;sjtu.edu.cn; ;gmail.com",
        "github": "https://github.com/OpenDFM/LLM4Chemistry",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0+1;0+1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Suzhou Laboratory",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "0+1;1;0+1;0+1;1",
        "aff_campus_unique": "Shanghai;Suzhou",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.168",
        "title": "From Multiple-Choice to Extractive QA: A Case Study for English and Arabic",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid evolution of Natural Language Processing (NLP) has favoured major languages such as English, leaving a significant gap for many others due to limited resources. This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly. Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific. Here, we explore the feasibility of repurposing an existing multilingual dataset for a new NLP task: we repurpose a subset of the BELEBELE dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable the more practical task of extractive QA (EQA) in the style of machine reading comprehension. We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA). We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects. We aim to help others adapt our approach for the remaining 120 BELEBELE language variants, many of which are deemed under-resourced. We also provide a thorough analysis and share insights to deepen understanding of the challenges and opportunities in NLP task reformulation.",
        "author": "Teresa Lynn; Malik H. Altakrori; Samar M. Magdy; Rocktim Jyoti Das; Chenyang Lyu; Mohamed Nasr; Younes Samih; Kirill Chirkunov; Alham Fikri Aji; Preslav Nakov; Shantanu Godbole; Salim Roukos; Radu Florian; Nizar Habash",
        "authorids": "/t/teresa-lynn/; /m/malik-h-altakrori/; /s/samar-m-magdy/; /r/rocktim-jyoti-das/; /c/chenyang-lyu/; /m/mohamed-nasr/; /y/younes-samih/; /k/kirill-chirkunov/; /a/alham-fikri-aji/; /p/preslav-nakov/; /s/shantanu-godbole/; /s/salim-roukos/; /r/radu-florian/; /n/nizar-habash/",
        "bibtex": "@inproceedings{lynn-etal-2025-multiple,\n    title = \"From Multiple-Choice to Extractive {QA}: A Case Study for {E}nglish and {A}rabic\",\n    author = \"Lynn, Teresa  and\n      Altakrori, Malik H.  and\n      Magdy, Samar M.  and\n      Das, Rocktim Jyoti  and\n      Lyu, Chenyang  and\n      Nasr, Mohamed  and\n      Samih, Younes  and\n      Chirkunov, Kirill  and\n      Aji, Alham Fikri  and\n      Nakov, Preslav  and\n      Godbole, Shantanu  and\n      Roukos, Salim  and\n      Florian, Radu  and\n      Habash, Nizar\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.168/\",\n    pages = \"2456--2477\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.168.pdf",
        "site": "https://aclanthology.org/2025.coling-main.168/",
        "pdf_size": 842161,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xpRcBjvkK3oJ:scholar.google.com/&scioq=From+Multiple-Choice+to+Extractive+QA:+A+Case+Study+for+English+and+Arabic&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": ";;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 14
    },
    {
        "id": "2025.coling-main.450",
        "title": "From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Embeddings play a pivotal role in the efficacy of large language models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the aforementioned seminal work of (CITATION) and (CITATION) and propose DeepSoftDebias, an algorithm that uses a neural network to perform \u2018soft debiasing\u2019. We exhaustively evaluate this algorithm across a variety of state-of-the-art datasets, accuracy metrics, and challenging NLP tasks. On a wide range of metrics, we find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.",
        "author": "Aishik Rakshit; Smriti Singh; Shuvam Keshari; Arijit Ghosh Chowdhury; Vinija Jain; Aman Chadha",
        "authorids": "/a/aishik-rakshit/; /s/smriti-singh/; /s/shuvam-keshari/; /a/arijit-ghosh-chowdhury/; /v/vinija-jain/; /a/aman-chadha/",
        "bibtex": "@inproceedings{rakshit-etal-2025-prejudice,\n    title = \"From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings\",\n    author = \"Rakshit, Aishik  and\n      Singh, Smriti  and\n      Keshari, Shuvam  and\n      Ghosh Chowdhury, Arijit  and\n      Jain, Vinija  and\n      Chadha, Aman\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.450/\",\n    pages = \"6718--6747\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.450.pdf",
        "site": "https://aclanthology.org/2025.coling-main.450/",
        "pdf_size": 1969623,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9587179786631140237&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Indian Institute of Technology, Guwahati; University of Texas at Austin; University of Texas at Austin; Amazon GenAI; Meta AI; Amazon GenAI",
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2;3;2",
        "aff_unique_norm": "Indian Institute of Technology Guwahati;University of Texas at Austin;Amazon;Meta Platforms, Inc.",
        "aff_unique_dep": ";;Amazon GenAI;Meta AI",
        "aff_unique_url": "https://www.iitg.ac.in;https://www.utexas.edu;https://www.amazon.com;https://meta.com",
        "aff_unique_abbr": "IIT Guwahati;UT Austin;Amazon;Meta",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Guwahati;Austin;",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2025.coling-main.472",
        "title": "From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Many of the world\u2019s languages have insufficient data to train high-performing general neural machine translation (NMT) models, let alone domain-specific models, and often the only available parallel data are small amounts of religious texts. Hence, domain adaptation (DA) is a crucial issue faced by contemporary NMT and has, so far, been underexplored for low-resource languages. In this paper, we evaluate a set of methods from both low-resource NMT and DA in a realistic setting, in which we aim to translate between a high-resource and a low-resource language with access to only: a) parallel Bible data, b) a bilingual dictionary, and c) a monolingual target-domain corpus in the high-resource language. Our results show that the effectiveness of the tested methods varies, with the simplest one, DALI, being most effective. We follow up with a small human evaluation of DALI, which shows that there is still a need for more careful investigation of how to accomplish DA for low-resource NMT.",
        "author": "Ali Marashian; Enora Rice; Luke Gessler; Alexis Palmer; Katharina von der Wense",
        "authorids": "/a/ali-marashian/; /e/enora-rice/; /l/luke-gessler/; /a/alexis-palmer/; /k/katharina-von-der-wense/",
        "bibtex": "@inproceedings{marashian-etal-2025-priest,\n    title = \"From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine Translation\",\n    author = \"Marashian, Ali  and\n      Rice, Enora  and\n      Gessler, Luke  and\n      Palmer, Alexis  and\n      von der Wense, Katharina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.472/\",\n    pages = \"7087--7098\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.472.pdf",
        "site": "https://aclanthology.org/2025.coling-main.472/",
        "pdf_size": 366638,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11682573538013839876&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Colorado Boulder; University of Colorado Boulder; Indiana University Bloomington; University of Colorado Boulder; University of Colorado Boulder + Johannes Gutenberg University Mainz",
        "aff_domain": "colorado.edu; ; ; ; ",
        "email": "colorado.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0+2",
        "aff_unique_norm": "University of Colorado;Indiana University;Johannes Gutenberg University Mainz",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.colorado.edu;https://www.indiana.edu;https://www.jgu.de",
        "aff_unique_abbr": "CU Boulder;IU;JGU",
        "aff_campus_unique_index": "0;0;1;0;0+2",
        "aff_campus_unique": "Boulder;Bloomington;Mainz",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "2025.coling-main.55",
        "title": "From Superficial to Deep: Integrating External Knowledge for Follow-up Question Generation Using Knowledge Graph and LLM",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In a conversational system, dynamically generating follow-up questions based on context can help users explore information and provide a better user experience. Humans are usually able to ask questions that involve some general life knowledge and demonstrate higher order cognitive skills. However, the questions generated by existing methods are often limited to shallow contextual questions that are uninspiring and have a large gap to the human level. In this paper, we propose a three-stage external knowledge-enhanced follow-up question generation method, which generates questions by identifying contextual topics, constructing a knowledge graph (KG) online, and finally combining these with a large language model to generate the final question. The model generates information-rich and exploratory follow-up questions by introducing external common sense knowledge and performing a knowledge fusion operation. Experiments show that compared to baseline models, our method generates questions that are more informative and closer to human questioning levels while maintaining contextual relevance.",
        "author": "Jianyu Liu; Yi Huang; Sheng Bi; Junlan Feng; Guilin Qi",
        "authorids": "/j/jianyu-liu/; /y/yi-huang/; /s/sheng-bi/; /j/junlan-feng/; /g/guilin-qi/",
        "bibtex": "@inproceedings{liu-etal-2025-superficial,\n    title = \"From Superficial to Deep: Integrating External Knowledge for Follow-up Question Generation Using Knowledge Graph and {LLM}\",\n    author = \"Liu, Jianyu  and\n      Huang, Yi  and\n      Bi, Sheng  and\n      Feng, Junlan  and\n      Qi, Guilin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.55/\",\n    pages = \"828--840\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.55.pdf",
        "site": "https://aclanthology.org/2025.coling-main.55/",
        "pdf_size": 1010720,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10953938937492113972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Southeast University, China; China Mobile Research Institute, China; Law and Innovation Lab, Law School, Southeast University, China + China Mobile Research Institute, China; China Mobile Research Institute, China; School of Computer Science and Engineering, Southeast University, China",
        "aff_domain": "seu.edu.cn;chinamobile.com;seu.edu.cn;chinamobile.com;seu.edu.cn",
        "email": "seu.edu.cn;chinamobile.com;seu.edu.cn;chinamobile.com;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0+1;1;0",
        "aff_unique_norm": "Southeast University;China Mobile Research Institute",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "https://www.seu.edu.cn/;http://www.chinamobile.com/en/abstract/research-institute",
        "aff_unique_abbr": "SEU;CMRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.598",
        "title": "From Traits to Empathy: Personality-Aware Multimodal Empathetic Response Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Empathetic dialogue systems improve user experience across various domains. Existing approaches mainly focus on acquiring affective and cognitive knowledge from text, but neglect the unique personality traits of individuals and the inherently multimodal nature of human face-to-face conversation. To this end, we enhance the dialogue system with the ability to generate empathetic responses from a multimodal perspective, and consider the diverse personality traits of users. We incorporate multimodal data, such as images and texts, to understand the user\u2019s emotional state and situation. Concretely, we first identify the user\u2019s personality trait. Then, the dialogue system comprehends the user\u2019s emotions and situation by the analysis of multimodal inputs. Finally, the response generator models the correlations among the personality, emotion, and multimodal data, to generate empathetic responses. Experiments on the MELD dataset and the MEDIC dataset validate the effectiveness of the proposed approach.",
        "author": "Jiaqiang Wu; Xuandong Huang; Zhouan Zhu; Shangfei Wang",
        "authorids": "/j/jiaqiang-wu/; /x/xuandong-huang/; /z/zhouan-zhu/; /s/shangfei-wang/",
        "bibtex": "@inproceedings{wu-etal-2025-traits,\n    title = \"From Traits to Empathy: Personality-Aware Multimodal Empathetic Response Generation\",\n    author = \"Wu, Jiaqiang  and\n      Huang, Xuandong  and\n      Zhu, Zhouan  and\n      Wang, Shangfei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.598/\",\n    pages = \"8925--8938\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.598.pdf",
        "site": "https://aclanthology.org/2025.coling-main.598/",
        "pdf_size": 5220689,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9718780955678406586&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "github": "https://github.com/personalityempathy/Personality-Aware-MERG",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.294",
        "title": "Fusion meets Function: The Adaptive Selection-Generation Approach in Event Argument Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event Argument Extraction is a critical task of Event Extraction, focused on identifying event arguments within text. This paper presents a novel Fusion Selection-Generation-Based Approach, by combining the precision of selective methods with the semantic generation capability of generative methods to enhance argument extraction accuracy. This synergistic integration, achieved through fusion prompt, element-based extraction, and fusion learning, addresses the challenges of input, process, and output fusion, effectively blending the unique characteristics of both methods into a cohesive model. Comprehensive evaluations on the RAMS and WikiEvents demonstrate the model\u2019s state-of-the-art performance and efficiency.",
        "author": "Guoxuan Ding; Xiaobo Guo; Xin Wang; Lei Wang; Tianshu Fu; Nan Mu; Daren Zha",
        "authorids": "/g/guoxuan-ding/; /x/xiaobo-guo/; /x/xin-wang/; /l/lei-wang/; /t/tianshu-fu/; /n/nan-mu/; /d/daren-zha/",
        "bibtex": "@inproceedings{ding-etal-2025-fusion,\n    title = \"Fusion meets Function: The Adaptive Selection-Generation Approach in Event Argument Extraction\",\n    author = \"Ding, Guoxuan  and\n      Guo, Xiaobo  and\n      Wang, Xin  and\n      Wang, Lei  and\n      Fu, Tianshu  and\n      Mu, Nan  and\n      Zha, Daren\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.294/\",\n    pages = \"4359--4369\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.294.pdf",
        "site": "https://aclanthology.org/2025.coling-main.294/",
        "pdf_size": 602205,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gD8j3Y8Rht8J:scholar.google.com/&scioq=Fusion+meets+Function:+The+Adaptive+Selection-Generation+Approach+in+Event+Argument+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China + School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.718",
        "title": "GADFA: Generator-Assisted Decision-Focused Approach for Opinion Expressing Timing Identification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The advancement of text generation models has granted us the capability to produce coherent and convincing text on demand. Yet, in real-life circumstances, individuals do not continuously generate text or voice their opinions. For instance, consumers pen product reviews after weighing the merits and demerits of a product, and professional analysts issue reports following significant news releases. In essence, opinion expression is typically prompted by particular reasons or signals. Despite long-standing developments in opinion mining, the appropriate timing for expressing an opinion remains largely unexplored. To address this deficit, our study introduces an innovative task - the identification of news-triggered opinion expressing timing. We ground this task in the actions of professional stock analysts and develop a novel dataset for investigation. Our Generator-Assisted Decision-Focused Approach (GADFA) is decision-focused, leveraging text generation models to steer the classification model, thus enhancing overall performance. Our experimental findings demonstrate that the text generated by our model contributes fresh insights from various angles, effectively aiding in identifying the optimal timing for opinion expression.",
        "author": "Chung-Chi Chen; Hiroya Takamura; Ichiro Kobayashi; Yusuke Miyao; Hsin-Hsi Chen",
        "authorids": "/c/chung-chi-chen/; /h/hiroya-takamura/; /i/ichiro-kobayashi/; /y/yusuke-miyao/; /h/hsin-hsi-chen/",
        "bibtex": "@inproceedings{chen-etal-2025-gadfa,\n    title = \"{GADFA}: Generator-Assisted Decision-Focused Approach for Opinion Expressing Timing Identification\",\n    author = \"Chen, Chung-Chi  and\n      Takamura, Hiroya  and\n      Kobayashi, Ichiro  and\n      Miyao, Yusuke  and\n      Chen, Hsin-Hsi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.718/\",\n    pages = \"10781--10794\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.718.pdf",
        "site": "https://aclanthology.org/2025.coling-main.718/",
        "pdf_size": 607494,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AI5qI3XHeJgJ:scholar.google.com/&scioq=GADFA:+Generator-Assisted+Decision-Focused+Approach+for+Opinion+Expressing+Timing+Identification&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.661",
        "title": "GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pretrained transformer-based Language Models (LMs) are well-known for their ability to achieve significant improvement on text classification tasks with their powerful word embeddings, but their black-box nature, which leads to a lack of interpretability, has been a major concern. In this work, we introduce GAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical Network designed to explain the decisions of text classification models built with LM encoders. In our approach, the input vector and prototypes are regarded as nodes within a graph, and we utilize multi-head graph attention to selectively construct edges between the input node and prototype nodes to learn an interpretable prototypical representation. During inference, the model makes decisions based on a linear combination of activated prototypes weighted by the attention score assigned for each prototype, allowing its choices to be transparently explained by the attention weights and the prototypes. Experiments on multiple public datasets show our approach achieves superior results without sacrificing the accuracy of the original black-box LMs. We also compare with four alternative prototypical network variations and our approach achieves the best accuracy and F1 among all. Our case study and visualization of prototype clusters also demonstrate the efficiency in explaining the decisions of black-box models built with LMs.",
        "author": "Ximing Wen; Wenjuan Tan; Rosina Weber",
        "authorids": "/x/ximing-wen/; /w/wenjuan-tan/; /r/rosina-weber/",
        "bibtex": "@inproceedings{wen-etal-2025-gaprotonet,\n    title = \"{GAP}roto{N}et: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification\",\n    author = \"Wen, Ximing  and\n      Tan, Wenjuan  and\n      Weber, Rosina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.661/\",\n    pages = \"9891--9901\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.661.pdf",
        "site": "https://aclanthology.org/2025.coling-main.661/",
        "pdf_size": 7981549,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11241264621184031895&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII",
        "gs_version_total": 5,
        "aff": "College of Computing and Informatics, Drexel University, Philadelphia, USA; Department of Computer Science, Tsinghua University, Beijing, China; College of Computing and Informatics, Drexel University, Philadelphia, USA",
        "aff_domain": "drexel.edu;mails.tsinghua.edu.cn;drexel.edu",
        "email": "drexel.edu;mails.tsinghua.edu.cn;drexel.edu",
        "github": "https://github.com/ximingwen/GAProtoNet",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Drexel University;Tsinghua University",
        "aff_unique_dep": "College of Computing and Informatics;Department of Computer Science",
        "aff_unique_url": "https://drexel.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Drexel;THU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Philadelphia;Beijing",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2025.coling-main.549",
        "title": "GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for Unsupervised Reverse Dictionary",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Reverse Dictionary (RD) is the task of obtaining the most relevant word or set of words given a textual description or dictionary definition. Effective RD methods have applications in accessibility, translation or writing support systems. Moreover, in NLP research we find RD to be used to benchmark text encoders at various granularities, as it often requires word, definition and sentence embeddings. In this paper, we propose a simple approach to RD that leverages LLMs in combination with embedding models. Despite its simplicity, this approach outperforms supervised baselines in well studied RD datasets, while also showing less overfitting. We also conduct a number of experiments on different dictionaries and analyze how different styles, registers and target audiences impact the quality of RD systems. We conclude that, on average, untuned embeddings alone fare way below an LLM-only baseline (although they are competitive in highly technical dictionaries), but are crucial for boosting performance in combined methods.",
        "author": "Fatemah Yousef Almeman; Luis Espinosa Anke",
        "authorids": "/f/fatemah-yousef-almeman/; /l/luis-espinosa-anke/",
        "bibtex": "@inproceedings{almeman-espinosa-anke-2025-gear,\n    title = \"{GEAR}: A Simple {GENERATE}, {EMBED}, {AVERAGE} {AND} {RANK} Approach for Unsupervised Reverse Dictionary\",\n    author = \"Almeman, Fatemah Yousef  and\n      Espinosa Anke, Luis\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.549/\",\n    pages = \"8242--8254\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.549.pdf",
        "site": "https://aclanthology.org/2025.coling-main.549/",
        "pdf_size": 666226,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Iorqya_diXIJ:scholar.google.com/&scioq=GEAR:+A+Simple+GENERATE,+EMBED,+AVERAGE+AND+RANK+Approach+for+Unsupervised+Reverse+Dictionary&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "CardiffNLP, School of Computer Science and Informatics, Cardiff University, UK+College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, KSA; CardiffNLP, School of Computer Science and Informatics, Cardiff University, UK+AMPLYFI, UK",
        "aff_domain": "cardiff.ac.uk; ",
        "email": "cardiff.ac.uk; ",
        "github": "https://github.com/F-Almeman/GEAR_RD",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+2",
        "aff_unique_norm": "Cardiff University;Princess Nourah bint Abdulrahman University;AMPLYFI",
        "aff_unique_dep": "School of Computer Science and Informatics;College of Computer and Information Sciences;",
        "aff_unique_url": "https://www.cardiff.ac.uk;https://www.pnu.edu.sa;https://www.amplyfi.com",
        "aff_unique_abbr": "Cardiff;PNU;AMPLYFI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cardiff;",
        "aff_country_unique_index": "0+1;0+0",
        "aff_country_unique": "United Kingdom;Saudi Arabia"
    },
    {
        "id": "2025.coling-demos.16",
        "title": "GECTurk WEB: An Explainable Online Platform for Turkish Grammatical Error Detection and Correction",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Sophisticated grammatical error detection/correction tools are available for a small set of languages such as English and Chinese. However, it is not straightforward\u2014if not impossible\u2014to adapt them to morphologically rich languages with complex writing rules like Turkish which has more than 80 million speakers. Even though several tools exist for Turkish, they primarily focus on spelling errors rather than grammatical errors and lack features such as web interfaces, error explanations and feedback mechanisms. To fill this gap, we introduce GECTurk WEB, a light, open-source, and flexible web-based system that can detect and correct the most common forms of Turkish writing errors, such as the misuse of diacritics, compound and foreign words, pronouns, light verbs along with spelling mistakes. Our system provides native speakers and second language learners an easily accessible tool to detect/correct such mistakes and also to learn from their mistakes by showing the explanation for the violated rule(s). The proposed system achieves 88,3 system usability score, and is shown to help learn/remember a grammatical rule (confirmed by 80% of the participants). The GECTurk WEB is available both as an offline tool (https://github.com/GGLAB-KU/gecturkweb) or at www.gecturk.net.",
        "author": "Ali Gebe\u015f\u00e7e; G\u00f6zde G\u00fcl \u015eahin",
        "authorids": "/a/ali-gebesce/; /g/gozde-gul-sahin/",
        "bibtex": "@inproceedings{gebesce-sahin-2025-gecturk,\n    title = \"{GECT}urk {WEB}: An Explainable Online Platform for {T}urkish Grammatical Error Detection and Correction\",\n    author = {Gebe{\\c{s}}{\\c{c}}e, Ali  and\n      {\\c{S}}ahin, G{\\\"o}zde G{\\\"u}l},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.16/\",\n    pages = \"163--173\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.16.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.16/",
        "pdf_size": 1615084,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CUiTNa-56xwJ:scholar.google.com/&scioq=GECTurk+WEB:+An+Explainable+Online+Platform+for+Turkish+Grammatical+Error+Detection+and+Correction&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Computer Engineering Department, Ko\u00e7 University, Istanbul, Turkey+KUIS AI Lab, Istanbul, Turkey; Computer Engineering Department, Ko\u00e7 University, Istanbul, Turkey+KUIS AI Lab, Istanbul, Turkey",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/GGLAB-KU/gecturkweb",
        "project": "www.gecturk.net",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Ko\u00e7 University;Koc University",
        "aff_unique_dep": "Computer Engineering Department;AI Lab",
        "aff_unique_url": "https://www.ku.edu.tr;http://www.ku.edu.tr",
        "aff_unique_abbr": "Ko\u00e7;KU",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Istanbul",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Turkey"
    },
    {
        "id": "2025.coling-main.166",
        "title": "GL-GAN: Perceiving and Integrating Global and Local Styles for Handwritten Text Generation with Mamba",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Handwritten text generation (HTG) aims to synthesize handwritten samples by imitating a specific writer, which has a wide range of applications and thus has significant research value. However, current studies on HTG are confronted with a main bottleneck: dominant models lack the ability to perceive and integrate handwriting styles, which affects the realism of the synthesized samples. In this paper, we propose GL-GAN, which effectively captures and integrates global and local styles. Specifically, we propose a Hybrid Style Encoder (HSE) that combines a state space model (SSM) and convolution to capture multilevel style features through various receptive fields. The captured style features are then fed to the proposed Dynamic Feature Enhancement Module (DFEM), which integrates these features by adaptively modeling the entangled relationships between multilevel styles and removing redundant details. Extensive experiments on two widely used handwriting datasets demonstrate that our GL-GAN is an effective HTG model and outperforms state-of-the-art models remarkably. Our code is publicly available at:https://github.com/Fyzjym/GL-GAN.",
        "author": "Yiming Wang; Hongxi Wei; Heng Wang; Shiwen Sun; Chao He",
        "authorids": "/y/yiming-wang/; /h/hongxi-wei/; /h/heng-wang/; /s/shiwen-sun/; /c/chao-he/",
        "bibtex": "@inproceedings{wang-etal-2025-gl,\n    title = \"{GL}-{GAN}: Perceiving and Integrating Global and Local Styles for Handwritten Text Generation with Mamba\",\n    author = \"Wang, Yiming  and\n      Wei, Hongxi  and\n      Wang, Heng  and\n      Sun, Shiwen  and\n      He, Chao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.166/\",\n    pages = \"2434--2444\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.166.pdf",
        "site": "https://aclanthology.org/2025.coling-main.166/",
        "pdf_size": 1386876,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-4fHgubkp_sJ:scholar.google.com/&scioq=GL-GAN:+Perceiving+and+Integrating+Global+and+Local+Styles+for+Handwritten+Text+Generation+with+Mamba&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Inner Mongolia University, Hohhot, China+National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China; School of Computer Science, Inner Mongolia University, Hohhot, China+National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China; School of Computer Science, Inner Mongolia University, Hohhot, China+National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China; School of Computer Science, Inner Mongolia University, Hohhot, China+National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China; School of Computer Science, Inner Mongolia University, Hohhot, China+National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China",
        "aff_domain": "imu.edu.cn; ; ; ; ",
        "email": "imu.edu.cn; ; ; ; ",
        "github": "https://github.com/Fyzjym/GL-GAN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Inner Mongolia University;National and Local Joint Engineering Research Center of Mongolian Information Processing Technology",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.imu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Hohhot",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.458",
        "title": "GLoCIM: Global-view Long Chain Interest Modeling for news recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Accurately recommending candidate news articles to users has always been the core challenge of news recommendation system. News recommendations often require modeling of user interest to match candidate news. Recent efforts have primarily focused on extracting local subgraph information in a global click graph constructed by the clicked news sequence of all users. However, the computational complexity of extracting global click graph information has hindered the ability to utilize far-reaching linkage which is hidden between two distant nodes in global click graph collaboratively among similar users. To overcome the problem above, we propose a Global-view Long Chain Interests Modeling for news recommendation (GLoCIM), which combines neighbor interest with long chain interest distilled from a global click graph, leveraging the collaboration among similar users to enhance news recommendation. We therefore design a long chain selection algorithm and long chain interest encoder to obtain global-view long chain interest from the global click graph. We design a gated network to integrate long chain interest with neighbor interest to achieve the collaborative interest among similar users. Subsequently we aggregate it with local news category-enhanced representation to generate final user representation. Then candidate news representation can be formed to match user representation to achieve news recommendation. Experimental results on real-world datasets validate the effectiveness of our method to improve the performance of news recommendation.",
        "author": "Zhen Yang; Wenhui Wang; Tao Qi; Peng Zhang; TianYun Zhang; Ru Zhang; Jianyi Liu; Yongfeng Huang",
        "authorids": "/z/zhen-yang/; /w/wenhui-wang/; /t/tao-qi/; /p/peng-zhang/; /t/tianyun-zhang/; /r/ru-zhang/; /j/jianyi-liu/; /y/yongfeng-huang/",
        "bibtex": "@inproceedings{yang-etal-2025-glocim,\n    title = \"{GL}o{CIM}: Global-view Long Chain Interest Modeling for news recommendation\",\n    author = \"Yang, Zhen  and\n      Wang, Wenhui  and\n      Qi, Tao  and\n      Zhang, Peng  and\n      Zhang, TianYun  and\n      Zhang, Ru  and\n      Liu, Jianyi  and\n      Huang, Yongfeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.458/\",\n    pages = \"6855--6865\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.458.pdf",
        "site": "https://aclanthology.org/2025.coling-main.458/",
        "pdf_size": 820312,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1102012005809530305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Cyberspace Security, Beijing University of Posts and Telecommunications; School of Cyberspace Security, Beijing University of Posts and Telecommunications; Department of Electronic Engineering, Tsinghua University; School of Cyberspace Security, Beijing University of Posts and Telecommunications; School of Cyberspace Security, Beijing University of Posts and Telecommunications; School of Cyberspace Security, Beijing University of Posts and Telecommunications; School of Cyberspace Security, Beijing University of Posts and Telecommunications; Department of Electronic Engineering, Tsinghua University + Zhongguancun Laboratory, Beijing, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;gmail.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;tsinghua.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;gmail.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;0;0;0;0;1+2",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Tsinghua University;Zhongguancun Laboratory",
        "aff_unique_dep": "School of Cyberspace Security;Department of Electronic Engineering;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "BUPT;THU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-demos.17",
        "title": "GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP) toolkit developed specifically for modern Greek. The toolkit provides state-of-the-art performance in five core NLP tasks, namely part-of-speech tagging, morphological tagging, dependency parsing, named entity recognition, and Greeklish-to-Greek transliteration. The toolkit is based on pre-trained Transformers, it is freely available, and can be easily installed in Python (pip install gr-nlp-toolkit). It is also accessible through a demonstration platform on HuggingFace, along with a publicly available API for non-commercial use. We discuss the functionality provided for each task, the underlying methods, experiments against comparable open-source toolkits, and future possible enhancements. The toolkit is available at: https://github.com/nlpaueb/gr-nlp-toolkit",
        "author": "Lefteris Loukas; Nikolaos Smyrnioudis; Chrysa Dikonomaki; Spiros Barbakos; Anastasios Toumazatos; John Koutsikakis; Manolis Kyriakakis; Mary Georgiou; Stavros Vassos; John Pavlopoulos; Ion Androutsopoulos",
        "authorids": "/l/lefteris-loukas/; /n/nikolaos-smyrnioudis/; /c/chrysa-dikonomaki/; /s/spiros-barbakos/; /a/anastasios-toumazatos/; /j/john-koutsikakis/; /m/manolis-kyriakakis/; /m/mary-georgiou/; /s/stavros-vassos/; /j/john-pavlopoulos/; /i/ion-androutsopoulos/",
        "bibtex": "@inproceedings{loukas-etal-2025-gr,\n    title = \"{GR}-{NLP}-{TOOLKIT}: An Open-Source {NLP} Toolkit for {M}odern {G}reek\",\n    author = \"Loukas, Lefteris  and\n      Smyrnioudis, Nikolaos  and\n      Dikonomaki, Chrysa  and\n      Barbakos, Spiros  and\n      Toumazatos, Anastasios  and\n      Koutsikakis, John  and\n      Kyriakakis, Manolis  and\n      Georgiou, Mary  and\n      Vassos, Stavros  and\n      Pavlopoulos, John  and\n      Androutsopoulos, Ion\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.17/\",\n    pages = \"174--182\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.17.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.17/",
        "pdf_size": 575628,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:K3rJrOwcjHcJ:scholar.google.com/&scioq=GR-NLP-TOOLKIT:+An+Open-Source+NLP+Toolkit+for+Modern+Greek&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Department of Informatics, Athens University of Economics and Business, Greece + helvia.ai; Department of Informatics, Athens University of Economics and Business, Greece; Department of Informatics, Athens University of Economics and Business, Greece; Department of Informatics, Athens University of Economics and Business, Greece; Department of Informatics, Athens University of Economics and Business, Greece; Department of Informatics, Athens University of Economics and Business, Greece; Department of Informatics, Athens University of Economics and Business, Greece; Department of Informatics, Athens University of Economics and Business, Greece; helvia.ai; Department of Informatics, Athens University of Economics and Business, Greece + Archimedes/Athena RC, Greece; Department of Informatics, Athens University of Economics and Business, Greece + Archimedes/Athena RC, Greece",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "https://github.com/nlpaueb/gr-nlp-toolkit",
        "project": "https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo",
        "author_num": 11,
        "aff_unique_index": "0+1;0;0;0;0;0;0;0;1;0+2;0+2",
        "aff_unique_norm": "Athens University of Economics and Business;helvia.ai;Archimedes Research Center",
        "aff_unique_dep": "Department of Informatics;;",
        "aff_unique_url": "https://www.aueb.gr;https://www.helvia.ai;",
        "aff_unique_abbr": "AUEB;;ARC",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Athens;",
        "aff_country_unique_index": "0+1;0;0;0;0;0;0;0;1;0+0;0+0",
        "aff_country_unique": "Greece;United States"
    },
    {
        "id": "2025.coling-main.256",
        "title": "Gen-SQL: Efficient Text-to-SQL By Bridging Natural Language Question And Database Schema With Pseudo-Schema",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the prevalence of Large Language Models (LLMs), recent studies have shifted paradigms and leveraged LLMs to tackle the challenging task of Text-to-SQL. Because of the complexity of real world databases, previous works adopt the retrieve-then-generate framework to retrieve relevant database schema and then to generate the SQL query. However, efficient embedding-based retriever suffers from lower retrieval accuracy, and more accurate LLM-based retriever is far more expensive to use, which hinders their applicability for broader applications. To overcome this issue, this paper proposes Gen-SQL, a novel generate-ground-regenerate framework, where we exploit prior knowledge from the LLM to enhance embedding-based retriever and reduce cost. Experiments on several datasets are conducted to demonstrate the effectiveness and scalability of our proposed method. We release our code and data at https://github.com/jieshi10/gensql.",
        "author": "Jie Shi; Bo Xu; Jiaqing Liang; Yanghua Xiao; Jia Chen; Chenhao Xie; Peng Wang; Wei Wang",
        "authorids": "/j/jie-shi/; /b/bo-xu/; /j/jiaqing-liang/; /y/yanghua-xiao/; /j/jia-chen/; /c/chenhao-xie/; /p/peng-wang/; /w/wei-wang/",
        "bibtex": "@inproceedings{shi-etal-2025-gen,\n    title = \"Gen-{SQL}: Efficient Text-to-{SQL} By Bridging Natural Language Question And Database Schema With Pseudo-Schema\",\n    author = \"Shi, Jie  and\n      Xu, Bo  and\n      Liang, Jiaqing  and\n      Xiao, Yanghua  and\n      Chen, Jia  and\n      Xie, Chenhao  and\n      Wang, Peng  and\n      Wang, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.256/\",\n    pages = \"3794--3807\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.256.pdf",
        "site": "https://aclanthology.org/2025.coling-main.256/",
        "pdf_size": 974626,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gs8DrFMIJT0J:scholar.google.com/&scioq=Gen-SQL:+Efficient+Text-to-SQL+By+Bridging+Natural+Language+Question+And+Database+Schema+With+Pseudo-Schema&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "https://github.com/jieshi10/gensql",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.259",
        "title": "GenWebNovel: A Genre-oriented Corpus of Entities in Chinese Web Novels",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entities are important to understanding literary works, which emphasize characters, plots and environment. The research on entity recognition, especially nested entity recognition in the literary domain is still insufficient partly due to insufficient annotated data. To address this issue, we construct the first Genre-oriented Corpus for Entity Recognition in Chinese Web Novels, namely GenWebNovel, comprising 400 chapters totaling 1,214,283 tokens under two genres, XuanHuan (Eastern Fantasy) and History. Based on the corpus, we analyze the distribution of different types of entities, including person, location, and organization. We also compare the nesting patterns of nested entities between GenWebNovel and the English corpus LitBank. Even though both belong to the literary domain, entities in different genres share few overlaps, making genre adaptation of NER (Named Entity Recognition) a hard problem. We propose a novel method that utilizes a pre-trained language model as an In-context learning example retriever to boost the performance of large language models. Our experiments show that this approach significantly enhances entity recognition, matching state-of-the-art (SOTA) models without requiring additional training data. Our code, dataset, and model are available at https://github.com/hjzhao73/GenWebNovel.",
        "author": "Hanjie Zhao; Yuchen Yan; Senbin Zhu; Hongde Liu; Yuxiang Jia; Hongying Zan; Min Peng",
        "authorids": "/h/hanjie-zhao/; /y/yuchen-yan/; /s/senbin-zhu/; /h/hongde-liu/; /y/yuxiang-jia/; /h/hongying-zan/; /m/min-peng/",
        "bibtex": "@inproceedings{zhao-etal-2025-genwebnovel,\n    title = \"{G}en{W}eb{N}ovel: A Genre-oriented Corpus of Entities in {C}hinese Web Novels\",\n    author = \"Zhao, Hanjie  and\n      Yan, Yuchen  and\n      Zhu, Senbin  and\n      Liu, Hongde  and\n      Jia, Yuxiang  and\n      Zan, Hongying  and\n      Peng, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.259/\",\n    pages = \"3836--3849\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.259.pdf",
        "site": "https://aclanthology.org/2025.coling-main.259/",
        "pdf_size": 1758851,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:lSBIqDJc4kMJ:scholar.google.com/&scioq=GenWebNovel:+A+Genre-oriented+Corpus+of+Entities+in+Chinese+Web+Novels&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer Science, Wuhan University, China",
        "aff_domain": "foxmail.com;gs.zzu.edu.cn; ; ;zzu.edu.cn; ;whu.edu.cn",
        "email": "foxmail.com;gs.zzu.edu.cn; ; ;zzu.edu.cn; ;whu.edu.cn",
        "github": "https://github.com/hjzhao73/GenWebNovel",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Zhengzhou University;Wuhan University",
        "aff_unique_dep": "School of Computer and Artificial Intelligence;School of Computer Science",
        "aff_unique_url": "http://www.zzu.edu.cn;http://www.whu.edu.cn",
        "aff_unique_abbr": ";WHU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.155",
        "title": "Generating Commonsense Reasoning Questions with Controllable Complexity through Multi-step Structural Composition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper studies the task of generating commonsense reasoning questions (QG) with desired difficulty levels. Compared to traditional shallow questions that can be solved by simple term matching, ours are more challenging. Our answering process requires reasoning over multiple contextual and commonsense clues. That involves advanced comprehension skills, such as abstract semantics learning and missing knowledge inference. Existing work mostly learns to map the given text into questions, lacking a mechanism to control results with the desired complexity. To address this problem, we propose a novel controllable framework. We first derive contextual and commonsense clues involved in reasoning questions from the text. These clues are used to create simple sub-questions. We then aggregate multiple sub-questions to compose complex ones under the guidance of prior reasoning structures. By iterating this process, we can compose a complex QG task based on a series of smaller and simpler QG subtasks. Each subtask serves as a building block for a larger one. Each composition corresponds to an increase in the reasoning step. Moreover, we design a voting verifier to ensure results\u2019 validity from multiple views, including answer consistency, reasoning difficulty, and context correlation. Finally, we can learn the optimal QG model to yield thought-provoking results. Evaluations on two typical datasets validate our method.",
        "author": "Jianxing Yu; Shiqi Wang; Hanjiang Lai; Wenqing Chen; Yanghui Rao; Qinliang Su; Jian Yin",
        "authorids": "/j/jianxing-yu/; /s/shiqi-wang/; /h/hanjiang-lai/; /w/wenqing-chen/; /y/yanghui-rao/; /q/qinliang-su/; /j/jian-yin/",
        "bibtex": "@inproceedings{yu-etal-2025-generating,\n    title = \"Generating Commonsense Reasoning Questions with Controllable Complexity through Multi-step Structural Composition\",\n    author = \"Yu, Jianxing  and\n      Wang, Shiqi  and\n      Lai, Hanjiang  and\n      Chen, Wenqing  and\n      Rao, Yanghui  and\n      Su, Qinliang  and\n      Yin, Jian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.155/\",\n    pages = \"2261--2276\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.155.pdf",
        "site": "https://aclanthology.org/2025.coling-main.155/",
        "pdf_size": 1175556,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4287774038281003250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, 519082, China; School of Computer Science and Engineering, Sun Yat-sen University+Key Laboratory of Sustainable Tourism Smart Assessment Technology, Ministry of Culture and Tourism+Pazhou Lab, Guangzhou, 510330, China",
        "aff_domain": "mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0+1+2",
        "aff_unique_norm": "Sun Yat-sen University;Ministry of Culture and Tourism;Pazhou Lab",
        "aff_unique_dep": "School of Artificial Intelligence;Key Laboratory of Sustainable Tourism Smart Assessment Technology;",
        "aff_unique_url": "http://www.sysu.edu.cn;;",
        "aff_unique_abbr": "SYSU;;",
        "aff_campus_unique_index": "0;0;0;0;0;0;2",
        "aff_campus_unique": "Zhuhai;;Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.274",
        "title": "Generation-Augmented and Embedding Fusion in Document-Level Event Argument Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level event argument extraction is a crucial task that aims to extract arguments from the entire document, beyond sentence-level analysis. Prior classification-based models still fail to explicitly capture significant relationships and heavily relies on large-scale datasets. In this study, we propose a novel approach called Generation-Augmented and Embedding Fusion. This approach first uses predefined templates and generative language models to produce an embedding capturing role relationship information, then integrates it into the foundational embedding derived from a classification model through a noval embedding fusion mechanism. We conduct the extensive experiments on the RAMS and WikiEvents datasets to demonstrate that our approach is more effective than the baselines, and that it is also data-efficient in low-resource scenarios.",
        "author": "Xingjian Lin; Shengfei Lyu; Xin Wang; Qiuju Chen; Huanhuan Chen",
        "authorids": "/x/xingjian-lin/; /s/shengfei-lyu/; /x/xin-wang/; /q/qiuju-chen/; /h/huanhuan-chen/",
        "bibtex": "@inproceedings{lin-etal-2025-generation,\n    title = \"Generation-Augmented and Embedding Fusion in Document-Level Event Argument Extraction\",\n    author = \"Lin, Xingjian  and\n      Lyu, Shengfei  and\n      Wang, Xin  and\n      Chen, Qiuju  and\n      Chen, Huanhuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.274/\",\n    pages = \"4078--4084\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.274.pdf",
        "site": "https://aclanthology.org/2025.coling-main.274/",
        "pdf_size": 367491,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Yfd6HnDad5sJ:scholar.google.com/&scioq=Generation-Augmented+and+Embedding+Fusion+in+Document-Level+Event+Argument+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; Nanyang Technological University; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;ntu.edu.sg;mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;ntu.edu.sg;mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "USTC;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-main.623",
        "title": "Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user\u2019s current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system\u2019s memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system\u2019s ability to respond meaningfully in open-domain conversations.",
        "author": "Jeonghyun Kang; Hongjin Kim; Harksoo Kim",
        "authorids": "/j/jeonghyun-kang/; /h/hongjin-kim/; /h/harksoo-kim/",
        "bibtex": "@inproceedings{kang-etal-2025-generation,\n    title = \"Generation-Based and Emotion-Reflected Memory Update: Creating the {KEEM} Dataset for Better Long-Term Conversation\",\n    author = \"Kang, Jeonghyun  and\n      Kim, Hongjin  and\n      Kim, Harksoo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.623/\",\n    pages = \"9260--9277\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.623.pdf",
        "site": "https://aclanthology.org/2025.coling-main.623/",
        "pdf_size": 7218563,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:04YaUqvwXmoJ:scholar.google.com/&scioq=Generation-Based+and+Emotion-Reflected+Memory+Update:+Creating+the+KEEM+Dataset+for+Better+Long-Term+Conversation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Konkuk University; Konkuk University; Konkuk University",
        "aff_domain": "konkuk.ac.kr;konkuk.ac.kr;konkuk.ac.kr",
        "email": "konkuk.ac.kr;konkuk.ac.kr;konkuk.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Konkuk University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.konkuk.edu",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.438",
        "title": "Generics are puzzling. Can language models find the missing piece?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generic sentences express generalisations about the world without explicit quantification. Although generics are central to everyday communication, building a precise semantic framework has proven difficult, in part because speakers use generics to generalise properties with widely different statistical prevalence. In this work, we study the implicit quantification and context-sensitivity of generics by leveraging language models as models of language. We create ConGen, a dataset of 2873 naturally occurring generic and quantified sentences in context, and define p-acceptability, a metric based on surprisal that is sensitive to quantification. Our experiments show generics are more context-sensitive than determiner quantifiers and about 20% of naturally occurring generics we analyze express weak generalisations. We also explore how human biases in stereotypes can be observed in language models.",
        "author": "Gustavo Cilleruelo; Emily Allaway; Barry Haddow; Alexandra Birch",
        "authorids": "/g/gustavo-cilleruelo/; /e/emily-allaway/; /b/barry-haddow/; /a/alexandra-birch/",
        "bibtex": "@inproceedings{cilleruelo-etal-2025-generics,\n    title = \"Generics are puzzling. Can language models find the missing piece?\",\n    author = \"Cilleruelo, Gustavo  and\n      Allaway, Emily  and\n      Haddow, Barry  and\n      Birch, Alexandra\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.438/\",\n    pages = \"6571--6588\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.438.pdf",
        "site": "https://aclanthology.org/2025.coling-main.438/",
        "pdf_size": 524733,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qEFAsIlB6qUJ:scholar.google.com/&scioq=Generics+are+puzzling.+Can+language+models+find+the+missing+piece%3F&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "email": "sms.ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "github": "https://github.com/ilyocoris/generics_are_puzzling",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-industry.19",
        "title": "Geo-Spatially Informed Models for Geocoding Unstructured Addresses",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Geocoding customer addresses and determining precise locations is a crucial component for any e-commerce company. Shipment delivery costs make up a significant portion of overall expenses, and having exact customer locations not only improves operational efficiency but also reduces costs and enhances the customer experience. While state-of-the-art geocoding systems are well-suited for developed countries with structured city layouts and high-quality reference corpora, they are less effective in developing countries like India, where addresses are highly unstructured and reliable reference data is scarce. Recent research has focused on creating geocoding systems tailored for developing nations such as India. In this work, we propose a method to geocode addresses in such environments. We explored various approaches to incorporate geo-spatial relationships using an LLM backbone, which provided insights into how the model learns these relationships both explicitly and implicitly. Our proposed approach outperforms the current state-of-the-art system by 20% in drift accuracy within 100 meters, and the state-of-the-art commercial system by 54%. This has a potential to reduce the incorrect delivery hub assignments by 8% which leads to significant customer experience improvements and business savings.",
        "author": "Uddeshya Singh; Devanapalli Ravi Shankar; Gowtham Bellala; Vikas Goel",
        "authorids": "/u/uddeshya-singh/; /d/devanapalli-ravi-shankar/; /g/gowtham-bellala/; /v/vikas-goel/",
        "bibtex": "@inproceedings{singh-etal-2025-geo,\n    title = \"Geo-Spatially Informed Models for Geocoding Unstructured Addresses\",\n    author = \"Singh, Uddeshya  and\n      Ravi Shankar, Devanapalli  and\n      Bellala, Gowtham  and\n      Goel, Vikas\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.19/\",\n    pages = \"236--242\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.19.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.19/",
        "pdf_size": 2327884,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qJkSWxsJWSsJ:scholar.google.com/&scioq=Geo-Spatially+Informed+Models+for+Geocoding+Unstructured+Addresses&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Indian Institute of Technology Bombay; Flipkart; Flipkart; Flipkart",
        "aff_domain": "gmail.com;flipkart.com;flipkart.com;flipkart.com",
        "email": "gmail.com;flipkart.com;flipkart.com;flipkart.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Flipkart",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.flipkart.com",
        "aff_unique_abbr": "IIT Bombay;Flipkart",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bombay;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.726",
        "title": "Get Confused Cautiously: Textual Sequence Memorization Erasure with Selective Entropy Maximization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have been found to memorize and recite some of the textual sequences from their training set verbatim, raising broad concerns about privacy and copyright issues. This Textual Sequence Memorization (TSM) phenomenon leads to a high demand to regulate LLM output to prevent generating certain memorized text that a user wants to be forgotten. However, our empirical study reveals that existing methods for TSM erasure fail to unlearn large numbers of memorized samples without substantially jeopardizing the model utility. To achieve a better trade-off between the effectiveness of TSM erasure and model utility in LLMs, our paper proposes a new method, named Entropy Maximization with Selective Optimization (EMSO), where the model parameters are updated sparsely based on novel optimization and selection criteria, in a manner that does not require additional models or data other than that in the forget set. More specifically, we propose an entropy-based loss that is shown to lead to more stable optimization and better preserves model utility than existing methods. In addition, we propose a contrastive gradient metric that takes both the gradient magnitude and direction into consideration, so as to localize model parameters to update in a sparse model updating scehme. Extensive experiments across three model scales demonstrate that our method excels in handling large-scale forgetting requests while preserving model ability in language generation and understanding.",
        "author": "Zhaohan Zhang; Ziquan Liu; Ioannis Patras",
        "authorids": "/z/zhaohan-zhang/; /z/ziquan-liu/; /i/ioannis-patras/",
        "bibtex": "@inproceedings{zhang-etal-2025-get,\n    title = \"Get Confused Cautiously: Textual Sequence Memorization Erasure with Selective Entropy Maximization\",\n    author = \"Zhang, Zhaohan  and\n      Liu, Ziquan  and\n      Patras, Ioannis\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.726/\",\n    pages = \"10924--10939\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.726.pdf",
        "site": "https://aclanthology.org/2025.coling-main.726/",
        "pdf_size": 1261102,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4872189879102104166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Queen Mary University of London, London, UK; Queen Mary University of London, London, UK; Queen Mary University of London, London, UK",
        "aff_domain": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Queen Mary University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.qmul.ac.uk",
        "aff_unique_abbr": "QMUL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.531",
        "title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Evaluating the graph comprehension and reasoning abilities of Large Language Models (LLMs) is challenging and often incomplete. Existing benchmarks focus primarily on pure graph understanding, lacking a comprehensive evaluation across all graph types and detailed capability definitions. This paper presents GraCoRe, a benchmark for systematically assessing LLMs\u2019 graph comprehension and reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and test models on pure graph and heterogeneous graphs, subdividing capabilities into 10 distinct areas tested through 19 tasks. Our benchmark includes 11 datasets with 5,140 graphs of varying complexity. We evaluate four closed-source and eight open-source LLMs, conducting thorough analyses from both ability and task perspectives. Key findings reveal that OpenAI o1 model has amazing comprehension and reasoning capabilities, semantic enrichment enhances reasoning performance, node ordering impacts task success, and the ability to process longer texts does not necessarily improve graph comprehension or reasoning.",
        "author": "Zike Yuan; Ming Liu; Hui Wang; Bing Qin",
        "authorids": "/z/zike-yuan/; /m/ming-liu/; /h/hui-wang/; /b/bing-qin/",
        "bibtex": "@inproceedings{yuan-etal-2025-gracore,\n    title = \"{G}ra{C}o{R}e: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models\",\n    author = \"Yuan, Zike  and\n      Liu, Ming  and\n      Wang, Hui  and\n      Qin, Bing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.531/\",\n    pages = \"7925--7948\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.531.pdf",
        "site": "https://aclanthology.org/2025.coling-main.531/",
        "pdf_size": 1767506,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2172425167672081121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Harbin Institute of Technology, Shenzhen, China + Peng Cheng Laboratory, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China + Peng Cheng Laboratory, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China + Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "pcl.ac.cn;ir.hit.edu.cn;pcl.ac.cn;ir.hit.edu.cn",
        "email": "pcl.ac.cn;ir.hit.edu.cn;pcl.ac.cn;ir.hit.edu.cn",
        "github": "https://github.com/ZIKEYUAN/GraCoRe",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;1;0+1",
        "aff_unique_norm": "Harbin Institute of Technology;Peng Cheng Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://en.hhit.edu.cn/;",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0+0;0+0;0;0+0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.220",
        "title": "Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Backdoor attacks remain significant security threats to generative large language models (LLMs). Since generative LLMs output sequences of high-dimensional token logits instead of low-dimensional classification logits, most existing backdoor defense methods designed for discriminative models like BERT are ineffective for generative LLMs. Inspired by the observed differences in learning behavior between backdoor and clean mapping in the frequency space, we transform gradients of each training sample, directly influencing parameter updates, into the frequency space. Our findings reveal a distinct separation between the gradients of backdoor and clean samples in the frequency space. Based on this phenomenon, we propose Gradient Clustering in the Frequency Space for Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients in the frequency space to effectively identify backdoor samples without requiring retraining LLMs. Experimental results show that GraCeFul outperforms baselines significantly. Notably, GraCeFul exhibits remarkable computational efficiency, achieving nearly 100% recall and F1 scores in identifying backdoor samples, reducing the average success rate of various backdoor attacks to 0% with negligible drops in clean accuracy across multiple free-style question answering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna. The codes are publicly available at https://github.com/ZrW00/GraceFul.",
        "author": "Zongru Wu; Pengzhou Cheng; Lingyong Fang; Zhuosheng Zhang; Gongshen Liu",
        "authorids": "/z/zongru-wu/; /p/pengzhou-cheng/; /l/lingyong-fang/; /z/zhuosheng-zhang/; /g/gongshen-liu/",
        "bibtex": "@inproceedings{wu-etal-2025-gracefully,\n    title = \"Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining\",\n    author = \"Wu, Zongru  and\n      Cheng, Pengzhou  and\n      Fang, Lingyong  and\n      Zhang, Zhuosheng  and\n      Liu, Gongshen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.220/\",\n    pages = \"3267--3282\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.220.pdf",
        "site": "https://aclanthology.org/2025.coling-main.220/",
        "pdf_size": 2762597,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7699836541851305427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "https://github.com/ZrW00/GraceFul",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "School of Electronic Information and Electrical Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.176",
        "title": "Gradient Inversion Attack in Federated Learning: Exposing Text Data through Discrete Optimization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Federated learning has emerged as a potential solution to overcome the bottleneck posed by the near exhaustion of public text data in training large language models. There are claims that the strategy of exchanging gradients allows using text data including private information. Although recent studies demonstrate that data can be reconstructed from gradients, the threat for text data seems relatively small due to its sensitivity to even a few token errors. However, we propose a novel attack method FET, indicating that it is possible to Fully Expose Text data from gradients. Unlike previous methods that optimize continuous embedding vectors, we directly search for a text sequence with gradients that match the known gradients. First, we infer the total number of tokens and the unique tokens in the target text data from the gradients of the embedding layer. Then we develop a discrete optimization algorithm, which globally explores the solution space and precisely refines the obtained solution, incorporating both global and local search strategies. We also find that gradients of the fully connected layer are dominant, providing sufficient guidance for the optimization process. Our experiments show a significant improvement in attack performance, with an average increase of 39% for TinyBERT-6, 20% for BERT-base and 15% for BERT-large in exact match rates across three datasets. These findings highlight serious privacy risks in text data, suggesting that using smaller models is not an effective privacy-preserving strategy.",
        "author": "Ying Gao; Yuxin Xie; Huanghao Deng; Zukun Zhu",
        "authorids": "/y/ying-gao/; /y/yuxin-xie/; /h/huanghao-deng/; /z/zukun-zhu/",
        "bibtex": "@inproceedings{gao-etal-2025-gradient,\n    title = \"Gradient Inversion Attack in Federated Learning: Exposing Text Data through Discrete Optimization\",\n    author = \"Gao, Ying  and\n      Xie, Yuxin  and\n      Deng, Huanghao  and\n      Zhu, Zukun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.176/\",\n    pages = \"2582--2591\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.176.pdf",
        "site": "https://aclanthology.org/2025.coling-main.176/",
        "pdf_size": 530426,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17673214728116120780&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Cyber Science and Technology, Beihang University, Beijing, China+Zhongguancun Laboratory, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Beihang University;Zhongguancun Laboratory",
        "aff_unique_dep": "School of Cyber Science and Technology;",
        "aff_unique_url": "http://www.buaa.edu.cn;",
        "aff_unique_abbr": "BUAA;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.263",
        "title": "Grading Massive Open Online Courses Using Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Massive open online courses (MOOCs) offer free education globally. Despite this democratization of learning, the massive enrollment in these courses makes it impractical for an instructor to assess every student\u2019s writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, we explore the feasibility of using large language models (LLMs) to replace peer grading in MOOCs. To this end, we adapt the zero-shot chain-of-thought (ZCoT) prompting technique to automate the feedback process once the LLM assigns a score to an assignment. Specifically, to instruct LLMs for grading, we use three distinct prompts based on ZCoT: (1) ZCoT with instructor-provided correct answers, (2) ZCoT with both instructor-provided correct answers and rubrics, and (3) ZCoT with instructor-provided correct answers and LLM-generated rubrics. We tested these prompts in 18 different scenarios using two LLMs\u2014GPT-4 and GPT-3.5\u2014across three MOOCs: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. Our results show that ZCoT, when augmented with instructor-provided correct answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading. Finally, our findings indicate a promising potential for automated grading systems in MOOCs, especially in subjects with well-defined rubrics, to improve the learning experience for millions of online learners worldwide.",
        "author": "Shahriar Golchin; Nikhil Garuda; Christopher Impey; Matthew Wenger",
        "authorids": "/s/shahriar-golchin/; /n/nikhil-garuda/; /c/christopher-impey/; /m/matthew-wenger/",
        "bibtex": "@inproceedings{golchin-etal-2025-grading,\n    title = \"Grading Massive Open Online Courses Using Large Language Models\",\n    author = \"Golchin, Shahriar  and\n      Garuda, Nikhil  and\n      Impey, Christopher  and\n      Wenger, Matthew\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.263/\",\n    pages = \"3899--3912\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.263.pdf",
        "site": "https://aclanthology.org/2025.coling-main.263/",
        "pdf_size": 314541,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8902524402694973074&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Arizona; Department of Astronomy, University of Arizona; Department of Astronomy, University of Arizona; Department of Astronomy, University of Arizona",
        "aff_domain": "arizona.edu; ; ; ",
        "email": "arizona.edu; ; ; ",
        "github": "",
        "project": "https://www.mooc.org/",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Arizona",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UArizona",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.43",
        "title": "Graph Representation Learning in Hyperbolic Space via Dual-Masked",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Graph representation learning (GRL) in hyperbolic space has gradually emerged as a promising approach. Meanwhile, masking and reconstruction-based (MR-based) methods lead to state-of-the-art self-supervised graph representation. However, existing MR-based methods do not fully consider deep node and structural information. Inspired by the recent active and emerging field of self-supervised learning, we propose a novel node and edge dual-masked self-supervised graph representation learning framework in hyperbolic space, named HDM-GAE. We have designed a graph dual-masked module and a hyperbolic structural self-attention encoder module to mask nodes or edges and perform node aggregation within hyperbolic space, respectively. Comprehensive experiments and ablation studies on real-world multi-category datasets, demonstrate the superiority of our method in downstream tasks such as node classification and link prediction.",
        "author": "Rui Gong; Zuyun Jiang; Daren Zha",
        "authorids": "/r/rui-gong/; /z/zuyun-jiang/; /d/daren-zha/",
        "bibtex": "@inproceedings{gong-etal-2025-graph,\n    title = \"Graph Representation Learning in Hyperbolic Space via Dual-Masked\",\n    author = \"Gong, Rui  and\n      Jiang, Zuyun  and\n      Zha, Daren\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.43/\",\n    pages = \"637--646\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.43.pdf",
        "site": "https://aclanthology.org/2025.coling-main.43/",
        "pdf_size": 671147,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6754428184755987573&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; China Telecom Digital Intelligence Technology Co, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;chinatelecom.cn;iie.ac.cn",
        "email": "iie.ac.cn;chinatelecom.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;China Telecom Digital Intelligence Technology Co",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.chinatelecom.com.cn",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.27",
        "title": "Graph-Augmented Open-Domain Multi-Document Summarization",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In the open-domain multi-document summarization (ODMDS) task, retrieving relevant documents from large repositories and generating coherent summaries are crucial. However, existing methods often treat retrieval and summarization as separate tasks, neglecting the relationships among documents. To address these limitations, we propose an integrated retrieval-summarization framework that captures global document relationships through graph-based clustering, guiding the re-ranking of retrieved documents. This cluster-level thematic information is then used to guide large language models (LLMs) in refining the retrieved documents and generating more accurate, coherent summaries. Experimental results on the ODSUM benchmark demonstrate that our method significantly improves retrieval accuracy and produces summaries that surpass those derived from the oracle documents. These findings highlight the potential of our framework to improve both retrieval and summarization tasks in ODMDS.",
        "author": "Xiaoping Shen; Yekun Chai",
        "authorids": "/x/xiaoping-shen/; /y/yekun-chai/",
        "bibtex": "@inproceedings{shen-chai-2025-graph,\n    title = \"Graph-Augmented Open-Domain Multi-Document Summarization\",\n    author = \"Shen, Xiaoping  and\n      Chai, Yekun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.27/\",\n    pages = \"318--330\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.27.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.27/",
        "pdf_size": 3401044,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TNUL2pH-wWYJ:scholar.google.com/&scioq=Graph-Augmented+Open-Domain+Multi-Document+Summarization&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Hong Kong University of Science and Technology; Baidu",
        "aff_domain": "connect.ust.hk;gmail.com",
        "email": "connect.ust.hk;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Baidu, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://www.baidu.com",
        "aff_unique_abbr": "HKUST;Baidu",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.368",
        "title": "GraphOTTER: Evolving LLM-based Graph Reasoning for Complex Table Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Complex Table Question Answering involves providing accurate answers to specific questions based on intricate tables that exhibit complex layouts and flexible header locations. Despite considerable progress having been made in the LLM era, the reasoning processes of existing methods are often implicit, feeding the entire table into prompts, making it difficult to effectively filter out irrelevant information in the table. To this end, we propose GraphOTTER that explicitly establishes the reasoning process to pinpoint the correct answers. In particular, GraphOTTER leverages a graph-based representation, transforming the complex table into an undirected graph. It then conducts step-by-step reasoning on the graph, with each step guided by a set of pre-defined intermediate reasoning actions. As such, it constructs a clear reasoning path and effectively identifies the answer to a given question. Comprehensive experiments on two benchmark datasets and two LLM backbones demonstrate the effectiveness of GraphOTTER. Further analysis indicates that its success may be attributed to the ability to efficiently filter out irrelevant information, thereby focusing the reasoning process on the most pertinent data. Our code and experimental datasets are available at https://github.com/JDing0521/GraphOTTER.",
        "author": "Qianlong Li; Chen Huang; Shuai Li; Yuanxin Xiang; Deng Xiong; Wenqiang Lei",
        "authorids": "/q/qianlong-li/; /c/chen-huang/; /s/shuai-li/; /y/yuanxin-xiang/; /d/deng-xiong/; /w/wenqiang-lei/",
        "bibtex": "@inproceedings{li-etal-2025-graphotter,\n    title = \"{G}raph{OTTER}: Evolving {LLM}-based Graph Reasoning for Complex Table Question Answering\",\n    author = \"Li, Qianlong  and\n      Huang, Chen  and\n      Li, Shuai  and\n      Xiang, Yuanxin  and\n      Xiong, Deng  and\n      Lei, Wenqiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.368/\",\n    pages = \"5486--5506\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.368.pdf",
        "site": "https://aclanthology.org/2025.coling-main.368/",
        "pdf_size": 1191918,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=816267595763200497&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Sichuan University + Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China; Sichuan University + Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China; Sichuan University + Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China; Sichuan University + Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China; Stevens Institute of Technology; Sichuan University + Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;stevens.edu;scu.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;stevens.edu;scu.edu.cn",
        "github": "https://github.com/JDing0521/GraphOTTER",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1;2;0+1",
        "aff_unique_norm": "Sichuan University;Engineering Research Center of Machine Learning and Industry Intelligence;Stevens Institute of Technology",
        "aff_unique_dep": ";Ministry of Education;",
        "aff_unique_url": "https://www.scu.edu.cn;;https://www.stevens.edu",
        "aff_unique_abbr": "SCU;;SIT",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.304",
        "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge. To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4\u2019s judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection. We further show that finetuning Llama-3 on GPT-4\u2019s reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4\u2019s evaluations and calibration on reference situations",
        "author": "Sacha Muller; Antonio Loison; Bilel Omrani; Gautier Viaud",
        "authorids": "/s/sacha-muller/; /a/antonio-loison/; /b/bilel-omrani/; /g/gautier-viaud/",
        "bibtex": "@inproceedings{muller-etal-2025-grouse,\n    title = \"{G}ro{USE}: A Benchmark to Evaluate Evaluators in Grounded Question Answering\",\n    author = \"Muller, Sacha  and\n      Loison, Antonio  and\n      Omrani, Bilel  and\n      Viaud, Gautier\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.304/\",\n    pages = \"4510--4534\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.304.pdf",
        "site": "https://aclanthology.org/2025.coling-main.304/",
        "pdf_size": 1194525,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3887178309697978688&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Illuin Technology; Illuin Technology; Illuin Technology; Illuin Technology",
        "aff_domain": "illuin.tech.fr;illuin.tech.fr;illuin.tech.fr;illuin.tech.fr",
        "email": "illuin.tech.fr;illuin.tech.fr;illuin.tech.fr;illuin.tech.fr",
        "github": "https://github.com/illuin-tech/grouse",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Illuin Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2025.coling-main.19",
        "title": "HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Object categories are typically organized into a multi-granularity taxonomic hierarchy. When classifying categories at different hierarchy levels, traditional uni-modal approaches focus primarily on image features, revealing limitations in complex scenarios. Recent studies integrating Vision-Language Models (VLMs) with class hierarchies have shown promise, yet they fall short of fully exploiting the hierarchical relationships. These efforts are constrained by their inability to perform effectively across varied granularity of categories. To tackle this issue, we propose a novel framework (**HGCLIP**) that effectively combines **CLIP** with a deeper exploitation of the **H**ierarchical class structure via **G**raph representation learning. We explore constructing the class hierarchy into a graph, with its nodes representing the textual or image features of each category. After passing through a graph encoder, the textual features incorporate hierarchical structure information, while the image features emphasize class-aware features derived from prototypes through the attention mechanism. Our approach demonstrates significant improvements on 11 diverse visual recognition benchmarks. Our codes are fully available at https: //github.com/richard-peng-xia/HGCLIP.",
        "author": "Peng Xia; Xingtong Yu; Ming Hu; Lie Ju; Zhiyong Wang; Peibo Duan; Zongyuan Ge",
        "authorids": "/p/peng-xia/; /x/xingtong-yu/; /m/ming-hu/; /l/lie-ju/; /z/zhiyong-wang/; /p/peibo-duan/; /z/zongyuan-ge/",
        "bibtex": "@inproceedings{xia-etal-2025-hgclip,\n    title = \"{HGCLIP}: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding\",\n    author = \"Xia, Peng  and\n      Yu, Xingtong  and\n      Hu, Ming  and\n      Ju, Lie  and\n      Wang, Zhiyong  and\n      Duan, Peibo  and\n      Ge, Zongyuan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.19/\",\n    pages = \"269--280\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.19.pdf",
        "site": "https://aclanthology.org/2025.coling-main.19/",
        "pdf_size": 1043296,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11302988192372423318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Monash University+UNC-Chapel Hill; Singapore Management University; Monash University; Monash University; The University of Sydney; Monash University; Monash University",
        "aff_domain": "cs.unc.edu; ; ; ; ; ;monash.edu",
        "email": "cs.unc.edu; ; ; ; ; ;monash.edu",
        "github": "https://github.com/richard-peng-xia/HGCLIP",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;0;0;3;0;0",
        "aff_unique_norm": "Monash University;University of North Carolina at Chapel Hill;Singapore Management University;University of Sydney",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.monash.edu;https://www.unc.edu;https://www.smu.edu.sg;https://www.sydney.edu.au",
        "aff_unique_abbr": "Monash;UNC;SMU;USYD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chapel Hill",
        "aff_country_unique_index": "0+1;2;0;0;0;0;0",
        "aff_country_unique": "Australia;United States;Singapore"
    },
    {
        "id": "2025.coling-main.235",
        "title": "HLU: Human Vs LLM Generated Text Detection Dataset for Urdu at Multiple Granularities",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rise of large language models (LLMs) generating human-like text has raised concerns about misuse, especially in low-resource languages like Urdu. To address this gap, we introduce the HLU dataset, which consists of three datasets: Document, Paragraph, and Sentence level. The document-level dataset contains 1,014 instances of human-written and LLM-generated articles across 13 domains, while the paragraph and sentence-level datasets each contain 667 instances. We conducted both human and automatic evaluations. In the human evaluation, the average accuracy at the document level was 35%, while at the paragraph and sentence levels, accuracies were 75.68% and 88.45%, respectively. For automatic evaluation, we finetuned the XLMRoBERTa model for both monolingual and multilingual settings achieving consistent results in both. Additionally, we assessed the performance of GPT4 and Claude3Opus using zero-shot prompting. Our experiments and evaluations indicate that distinguishing between human and machine-generated text is challenging for both humans and LLMs, marking a significant step in addressing this issue in Urdu.",
        "author": "Iqra Ali; Jesse Atuhurra; Hidetaka Kamigaito; Taro Watanabe",
        "authorids": "/i/iqra-ali/; /j/jesse-atuhurra/; /h/hidetaka-kamigaito/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{ali-etal-2025-hlu,\n    title = \"{HLU}: Human Vs {LLM} Generated Text Detection Dataset for {U}rdu at Multiple Granularities\",\n    author = \"Ali, Iqra  and\n      Atuhurra, Jesse  and\n      Kamigaito, Hidetaka  and\n      Watanabe, Taro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.235/\",\n    pages = \"3495--3510\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.235.pdf",
        "site": "https://aclanthology.org/2025.coling-main.235/",
        "pdf_size": 4578967,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UH67ciy7BEAJ:scholar.google.com/&scioq=HLU:+Human+Vs+LLM+Generated+Text+Detection+Dataset+for+Urdu+at+Multiple+Granularities&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;is.naist.jp;is.naist.jp;is.naist.jp",
        "email": "is.naist.jp;is.naist.jp;is.naist.jp;is.naist.jp",
        "github": "",
        "project": "https://huggingface.co/datasets/iqraali/Urdu-HumanvsMachine-Dataset",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nist.go.jp",
        "aff_unique_abbr": "NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.420",
        "title": "HYDEN: Hyperbolic Density Representations for Medical Images and Reports",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In light of the inherent entailment relations between images and text, embedding point vectors in hyperbolic space has been employed to leverage its hierarchical modeling advantages for visual semantic representation learning. However, point vector embeddings struggle to address semantic uncertainty, where an image may have multiple interpretations, and text may correspond to different images\u2014a challenge especially prevalent in the medical domain. Therefor, we propose HYDEN, a novel hyperbolic density embedding based image-text representation learning approach tailored for specific medical domain data. This method integrates text-aware local features with global features from images, mapping image-text features to density features in hyperbolic space via using hyperbolic pseudo-Gaussian distributions. An encapsulation loss function is employed to model the partial order relations between image-text density distributions. Experimental results demonstrate the interpretability of our approach and its superior performance compared to the baseline methods across various zero-shot tasks and fine-tuning task on different datasets.",
        "author": "Zhi Qiao; Linbin Han; Xiantong Zhen; Jiahong Gao; Zhen Qian",
        "authorids": "/z/zhi-qiao/; /l/linbin-han/; /x/xiantong-zhen/; /j/jiahong-gao/; /z/zhen-qian/",
        "bibtex": "@inproceedings{qiao-etal-2025-hyden,\n    title = \"{HYDEN}: Hyperbolic Density Representations for Medical Images and Reports\",\n    author = \"Qiao, Zhi  and\n      Han, Linbin  and\n      Zhen, Xiantong  and\n      Gao, Jiahong  and\n      Qian, Zhen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.420/\",\n    pages = \"6285--6297\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.420.pdf",
        "site": "https://aclanthology.org/2025.coling-main.420/",
        "pdf_size": 1433254,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:jwxZNUQOVu4J:scholar.google.com/&scioq=HYDEN:+Hyperbolic+Density+Representations+for+Medical+Images+and+Reports&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.640",
        "title": "Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Instruction-guided image editing consists in taking an image and an instruction and delivering that image altered according to that instruction. State-of-the-art approaches to this task suffer from the typical scaling up and domain adaptation hindrances related to supervision as they eventually resort to some kind of task-specific labelling, masking or training. We propose a novel approach that does without any such task-specific supervision and offers thus a better potential for improvement. Its assessment demonstrates that it is highly effective, achieving very competitive performance.",
        "author": "Rodrigo Santos; Ant\u00f3nio Branco; Jo\u00e3o Ricardo Silva; Joao Rodrigues",
        "authorids": "/r/rodrigo-santos/; /a/antonio-branco/; /j/joao-silva/; /j/joao-rodrigues/",
        "bibtex": "@inproceedings{santos-etal-2025-hands,\n    title = \"Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training\",\n    author = \"Santos, Rodrigo  and\n      Branco, Ant{\\'o}nio  and\n      Silva, Jo{\\~a}o Ricardo  and\n      Rodrigues, Joao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.640/\",\n    pages = \"9546--9565\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.640.pdf",
        "site": "https://aclanthology.org/2025.coling-main.640/",
        "pdf_size": 11984471,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:H9ZR9CW5DeoJ:scholar.google.com/&scioq=Hands-off+Image+Editing:+Language-guided+Editing+without+any+Task-specific+Labeling,+Masking+or+even+Training&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Lisbon; University of Lisbon; University of Lisbon; University of Lisbon",
        "aff_domain": "fc.ul.pt;fc.ul.pt;fc.ul.pt;fc.ul.pt",
        "email": "fc.ul.pt;fc.ul.pt;fc.ul.pt;fc.ul.pt",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Lisbon",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ulisboa.pt",
        "aff_unique_abbr": "ULisbon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "2025.coling-main.446",
        "title": "HateBRXplain: A Benchmark Dataset with Human-Annotated Rationales for Explainable Hate Speech Detection in Brazilian Portuguese",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nowadays, hate speech technologies are surely relevant in Brazil. Nevertheless, the inability of these technologies to provide reasons (rationales) for their decisions is the limiting factor to their adoption since they comprise bias, which may perpetuate social inequalities when propagated at scale. This scenario highlights the urgency of proposing explainable technologies to address hate speech. However, explainable models heavily depend on data availability with human-annotated rationales, which are scarce, especially for low-resource languages. To fill this relevant gap, we introduce HateBRXplain, the first benchmark dataset for hate speech detection in Portuguese, with text span annotations capturing rationales. We evaluated our corpus using mBERT, BERTimbau, DistilBERTimbau, and PTT5 models, which outperformed the current baselines. We further assessed these models\u2019 explainability using model-agnostic explanation methods (LIME and SHAP). Results demonstrate plausible post-hoc explanations when compared to human annotations. However, the best-performing hate speech detection models failed to provide faithful rationales.",
        "author": "Isadora Salles; Francielle Vargas; Fabr\u00edcio Benevenuto",
        "authorids": "/i/isadora-salles/; /f/francielle-vargas/; /f/fabricio-benevenuto/",
        "bibtex": "@inproceedings{salles-etal-2025-hatebrxplain,\n    title = \"{H}ate{BRX}plain: A Benchmark Dataset with Human-Annotated Rationales for Explainable Hate Speech Detection in {B}razilian {P}ortuguese\",\n    author = \"Salles, Isadora  and\n      Vargas, Francielle  and\n      Benevenuto, Fabr{\\'i}cio\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.446/\",\n    pages = \"6659--6669\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.446.pdf",
        "site": "https://aclanthology.org/2025.coling-main.446/",
        "pdf_size": 922174,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ETB1cIHNEtEJ:scholar.google.com/&scioq=HateBRXplain:+A+Benchmark+Dataset+with+Human-Annotated+Rationales+for+Explainable+Hate+Speech+Detection+in+Brazilian+Portuguese&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Federal University of Minas Gerais; University of S\u00e3o Paulo; Federal University of Minas Gerais",
        "aff_domain": "dcc.ufmg.br;usp.br;dcc.ufmg.br",
        "email": "dcc.ufmg.br;usp.br;dcc.ufmg.br",
        "github": "https://github.com/isadorasalles/HateBRXplain",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Federal University of Minas Gerais;University of S\u00e3o Paulo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ufmg.br;https://www.usp.br",
        "aff_unique_abbr": "UFMG;USP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "2025.coling-main.198",
        "title": "Hawkes based Representation Learning for Reasoning over Scale-free Community-structured Temporal Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Temporal knowledge graph (TKG) reasoning has become a hot topic due to its great value in many practical tasks. The key to TKG reasoning is modeling the structural information and evolutional patterns of the TKGs. While great efforts have been devoted to TKG reasoning, the structural and evolutional characteristics of real-world networks have not been considered. In the aspect of structure, real-world networks usually exhibit clear community structure and scale-free (long-tailed distribution) properties. In the aspect of evolution, the impact of an event decays with the time elapsing. In this paper, we propose a novel TKG reasoning model called Hawkes process-based Evolutional Representation Learning Network (HERLN), which learns structural information and evolutional patterns of a TKG simultaneously, considering the characteristics of real-world networks: community structure, scale-free and temporal decaying. First, we find communities in the input TKG to make the encoding get more similar intra-community embeddings. Second, we design a Hawkes process-based relational graph convolutional network to cope with the event impact-decaying phenomenon. Third, we design a conditional decoding method to alleviate biases towards frequent entities caused by long-tailed distribution. Experimental results show that HERLN achieves significant improvements over the state-of-the-art models.",
        "author": "Yuwei Du; Xinyue Liu; Wenxin Liang; Linlin Zong; Xianchao Zhang",
        "authorids": "/y/yuwei-du/; /x/xinyue-liu/; /w/wenxin-liang/; /l/linlin-zong/; /x/xianchao-zhang/",
        "bibtex": "@inproceedings{du-etal-2025-hawkes,\n    title = \"{H}awkes based Representation Learning for Reasoning over Scale-free Community-structured Temporal Knowledge Graphs\",\n    author = \"Du, Yuwei  and\n      Liu, Xinyue  and\n      Liang, Wenxin  and\n      Zong, Linlin  and\n      Zhang, Xianchao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.198/\",\n    pages = \"2935--2946\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.198.pdf",
        "site": "https://aclanthology.org/2025.coling-main.198/",
        "pdf_size": 597471,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uBWTqyog7xQJ:scholar.google.com/&scioq=Hawkes+based+Representation+Learning+for+Reasoning+over+Scale-free+Community-structured+Temporal+Knowledge+Graphs&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China",
        "aff_domain": "mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "email": "mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "School of Software",
        "aff_unique_url": "http://www.dlut.edu.cn",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.226",
        "title": "Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study of LLM Hallucination on North Korea",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hallucination in large language models (LLMs) remains a significant challenge for their safe deployment, particularly due to its potential to spread misinformation. Most existing solutions address this challenge by focusing on aligning the models with credible sources or by improving how models communicate their confidence (or lack thereof) in their outputs. While these measures may be effective in most contexts, they may fall short in scenarios requiring more nuanced approaches, especially in situations where access to accurate data is limited or determining credible sources is challenging. In this study, we take North Korea - a country characterised by an extreme lack of reliable sources and the prevalence of sensationalist falsehoods - as a case study. We explore and evaluate how some of the best-performing multilingual LLMs and specific language-based models generate information about North Korea in three languages spoken in countries with significant geo-political interests: English (United States, United Kingdom), Korean (South Korea), and Mandarin Chinese (China). Our findings reveal significant differences, suggesting that the choice of model and language can lead to vastly different understandings of North Korea, which has important implications given the global security challenges the country poses.",
        "author": "Eunjung Cho; Won Ik Cho; Soomin Seo",
        "authorids": "/e/eunjung-cho/; /w/won-ik-cho/; /s/soomin-seo/",
        "bibtex": "@inproceedings{cho-etal-2025-hermit,\n    title = \"Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study of {LLM} Hallucination on {N}orth {K}orea\",\n    author = \"Cho, Eunjung  and\n      Cho, Won Ik  and\n      Seo, Soomin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.226/\",\n    pages = \"3353--3371\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.226.pdf",
        "site": "https://aclanthology.org/2025.coling-main.226/",
        "pdf_size": 3925872,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:OV2KS6GRXQQJ:scholar.google.com/&scioq=Hermit+Kingdom+Through+the+Lens+of+Multiple+Perspectives:+A+Case+Study+of+LLM+Hallucination+on+North+Korea&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "ETH Zurich; Seoul National University; Sogang University",
        "aff_domain": "ethz.ch;snu.ac.kr;sogang.ac.kr",
        "email": "ethz.ch;snu.ac.kr;sogang.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "ETH Zurich;Seoul National University;Sogang University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;https://www.snu.ac.kr;https://www.sogang.ac.kr",
        "aff_unique_abbr": "ETHZ;SNU;Sogang",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Switzerland;South Korea"
    },
    {
        "id": "2025.coling-main.406",
        "title": "Hi-GEC: Hindi Grammar Error Correction in Low Resource Scenario",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automated Grammatical Error Correction (GEC) has been extensively researched in Natural Language Processing (NLP), primarily focusing on English and other resource-rich languages. This paper shifts the focus to GEC for a scarcely explored low-resource language, specifically Hindi, which presents unique challenges due to its intricate morphology and complex syntax. To address data resource limitations, this work explores various GEC data generation techniques. Our research introduces a carefully extracted and filtered, high-quality dataset, HiWikiEdits, which includes human-edited 8,137 instances sourced from Wikipedia, encompassing 17 diverse grammatical error types, with annotations performed using the ERRANT toolkit. Furthermore, we investigate Round Trip Translation (RTT) using diverse languages for synthetic Hindi GEC data generation, revealing that leveraging high-resource linguistically distant language for error generation outperforms mid-resource linguistically closer languages. Specifically, using English as a pivot language resulted in a 6.25% improvement in GLEU score compared to using Assamese or Marathi. Finally, we also investigate the neural model-based synthetic error-generation technique and show that it achieves comparable performance to other synthetic data generation methods, even in low-resource settings.",
        "author": "Ujjwal Sharma; Pushpak Bhattacharyya",
        "authorids": "/u/ujjwal-sharma/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{sharma-bhattacharyya-2025-hi,\n    title = \"Hi-{GEC}: {H}indi Grammar Error Correction in Low Resource Scenario\",\n    author = \"Sharma, Ujjwal  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.406/\",\n    pages = \"6063--6075\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.406.pdf",
        "site": "https://aclanthology.org/2025.coling-main.406/",
        "pdf_size": 975338,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UTopyL9BFLQJ:scholar.google.com/&scioq=Hi-GEC:+Hindi+Grammar+Error+Correction+in+Low+Resource+Scenario&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2025.coling-main.529",
        "title": "Hire Me or Not? Examining Language Model\u2019s Behavior with Occupation Attributes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, such as recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs\u2019 behavior via multi-round question answering. Inspired by prior work, we constructed a dataset using a standard occupation classification knowledge base released by authoritative agencies. We tested it on three families of LMs (RoBERTa, GPT, and Llama) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat, along with additional analysis indicating GPT-4o-mini favors female subjects, may imply that the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes. Our contribution includes a 73,500 prompts dataset constructed with a taxonomy of real-world occupations and a multi-step verification framework to evaluate model\u2019s behavior regarding gender stereotype.",
        "author": "Damin Zhang; Yi Zhang; Geetanjali Bihani; Julia Rayz",
        "authorids": "/d/damin-zhang/; /y/yi-zhang/; /g/geetanjali-bihani/; /j/julia-rayz/",
        "bibtex": "@inproceedings{zhang-etal-2025-hire,\n    title = \"Hire Me or Not? Examining Language Model{'}s Behavior with Occupation Attributes\",\n    author = \"Zhang, Damin  and\n      Zhang, Yi  and\n      Bihani, Geetanjali  and\n      Rayz, Julia\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.529/\",\n    pages = \"7891--7911\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.529.pdf",
        "site": "https://aclanthology.org/2025.coling-main.529/",
        "pdf_size": 654941,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9226258034298232616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.555",
        "title": "Hit the Sweet Spot! Span-Level Ensemble for Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Ensembling various LLMs to unlock their complementary potential and leverage their individual strengths is highly valuable. Previous studies typically focus on two main paradigms: sample-level and token-level ensembles. Sample-level ensemble methods either select or blend fully generated outputs, which hinders dynamic correction and enhancement of outputs during the generation process. On the other hand, token-level ensemble methods enable real-time correction through fine-grained ensemble at each generation step. However, the information carried by an individual token is quite limited, leading to suboptimal decisions at each step. To address these issues, we propose SweetSpan, a span-level ensemble method that effectively balances the need for real-time adjustments and the information required for accurate ensemble decisions. Our approach involves two key steps: First, we have each candidate model independently generate candidate spans based on the shared prefix. Second, we calculate perplexity scores to facilitate mutual evaluation among the candidate models and achieve robust span selection by filtering out unfaithful scores. To comprehensively evaluate ensemble methods, we propose a new challenging setting (ensemble models with significant performance gaps) in addition to the standard setting (ensemble the best-performing models) to assess the performance of model ensembles in more realistic scenarios. Experimental results in both standard and challenging settings across various language generation tasks demonstrate the effectiveness, robustness, and versatility of our approach compared with previous ensemble methods.",
        "author": "Yangyifan Xu; Jianghao Chen; Junhong Wu; Jiajun Zhang",
        "authorids": "/y/yangyifan-xu/; /j/jianghao-chen/; /j/junhong-wu/; /j/jiajun-zhang/",
        "bibtex": "@inproceedings{xu-etal-2025-hit,\n    title = \"Hit the Sweet Spot! Span-Level Ensemble for Large Language Models\",\n    author = \"Xu, Yangyifan  and\n      Chen, Jianghao  and\n      Wu, Junhong  and\n      Zhang, Jiajun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.555/\",\n    pages = \"8314--8325\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.555.pdf",
        "site": "https://aclanthology.org/2025.coling-main.555/",
        "pdf_size": 903357,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8331576588209539846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences + Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences + Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences + Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences + Institute of Automation, Chinese Academy of Sciences + Wuhan AI Research + Shanghai Artificial Intelligence Laboratory, Shanghai, China",
        "aff_domain": "ia.ac.cn;ia.ac.cn;ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/xydaytoy/SweetSpan",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1+2+3",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences;Wuhan AI Research;Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": "School of Artificial Intelligence;Institute of Automation;;",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ia.cas.cn;;",
        "aff_unique_abbr": "UCAS;CAS;;",
        "aff_campus_unique_index": ";;;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.671",
        "title": "How Ambiguous Are the Rationales for Natural Language Reasoning? A Simple Approach to Handling Rationale Uncertainty",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The quality of rationales is essential in the reasoning capabilities of language models. Rationales not only enhance reasoning performance in complex natural language tasks but also justify model decisions. However, obtaining impeccable rationales is often impossible. Our study aims to investigate how ambiguous rationales play in model performances of natural language reasoning. We first assess the ambiguity of rationales through the lens of entropy and uncertainty in model prior beliefs, exploring its impact on task performance. We then propose a simple way to guide models to choose between two different reasoning paths depending on the ambiguity of the rationales. Our empirical results demonstrate that this approach leads to robust performance, particularly in adversarial scenarios where rationale quality is inconsistent.",
        "author": "Hazel H. Kim",
        "authorids": "/h/hazel-h-kim/",
        "bibtex": "@inproceedings{kim-2025-ambiguous,\n    title = \"How Ambiguous Are the Rationales for Natural Language Reasoning? A Simple Approach to Handling Rationale Uncertainty\",\n    author = \"Kim, Hazel H.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.671/\",\n    pages = \"10047--10053\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.671.pdf",
        "site": "https://aclanthology.org/2025.coling-main.671/",
        "pdf_size": 355535,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kkh-kOQxfpwJ:scholar.google.com/&scioq=How+Ambiguous+Are+the+Rationales+for+Natural+Language+Reasoning%3F+A+Simple+Approach+to+Handling+Rationale+Uncertainty&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Oxford + Classting AI Research",
        "aff_domain": "cs.ox.ac.uk",
        "email": "cs.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0+1",
        "aff_unique_norm": "University of Oxford;Classting",
        "aff_unique_dep": ";AI Research",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.classting.com",
        "aff_unique_abbr": "Oxford;Classting",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1",
        "aff_country_unique": "United Kingdom;South Korea"
    },
    {
        "id": "2025.coling-main.285",
        "title": "How Credible Is an Answer From Retrieval-Augmented LLMs? Investigation and Evaluation With Multi-Hop QA",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-augmented Large Language Models (RaLLMs) are reshaping knowledge acquisition, offering long-form, knowledge-grounded answers through advanced reasoning and generation capabilities. Despite the emergence of impactful systems like WebGPT and New Bing, the reliability of RaLLMs, especially in complex situations, is under scrutiny. Our study tackles this concern by evaluating RaLLMs\u2019 question-answering performance using a novel benchmark focusing on Correctness and Groundedness. Correctness measures the logical soundness of the responses, and Groundedness checks for support by relevant references. We introduce an automated model-based evaluation pipeline for multi-hop question-answering tasks, revealing RaLLMs\u2019 proneness to generating inaccuracies when dealing with flawed or partial knowledge. To improve accuracy, we introduce two reasoning strategies, Self-Reflection\u2019 and Self-Completion,\u2019 enabling RaLLMs to identify and fill knowledge gaps, significantly improving answer quality without extensive model retraining.",
        "author": "Yujia Zhou; Zheng Liu; Zhicheng Dou",
        "authorids": "/y/yujia-zhou/; /z/zheng-liu/; /z/zhicheng-dou/",
        "bibtex": "@inproceedings{zhou-etal-2025-credible,\n    title = \"How Credible Is an Answer From Retrieval-Augmented {LLM}s? Investigation and Evaluation With Multi-Hop {QA}\",\n    author = \"Zhou, Yujia  and\n      Liu, Zheng  and\n      Dou, Zhicheng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.285/\",\n    pages = \"4232--4242\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.285.pdf",
        "site": "https://aclanthology.org/2025.coling-main.285/",
        "pdf_size": 1499581,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:JhmpXRE3S-AJ:scholar.google.com/&scioq=How+Credible+Is+an+Answer+From+Retrieval-Augmented+LLMs%3F+Investigation+and+Evaluation+With+Multi-Hop+QA&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Tsinghua University; BAAI; Renmin University of China",
        "aff_domain": "mail.tsinghua.edu.cn;gmail.com;ruc.edu.cn",
        "email": "mail.tsinghua.edu.cn;gmail.com;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Tsinghua University;Beijing Academy of Artificial Intelligence;Renmin University of China",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baaic.cn;http://www.ruc.edu.cn",
        "aff_unique_abbr": "THU;BAAI;RUC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.524",
        "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",
        "author": "Guangsheng Bao; Hongbo Zhang; Cunxiang Wang; Linyi Yang; Yue Zhang",
        "authorids": "/g/guangsheng-bao/; /h/hongbo-zhang/; /c/cunxiang-wang/; /l/linyi-yang/; /y/yue-zhang/",
        "bibtex": "@inproceedings{bao-etal-2025-likely,\n    title = \"How Likely Do {LLM}s with {C}o{T} Mimic Human Reasoning?\",\n    author = \"Bao, Guangsheng  and\n      Zhang, Hongbo  and\n      Wang, Cunxiang  and\n      Yang, Linyi  and\n      Zhang, Yue\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.524/\",\n    pages = \"7831--7850\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.524.pdf",
        "site": "https://aclanthology.org/2025.coling-main.524/",
        "pdf_size": 715491,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=340980971704050252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Zhejiang University+School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study; Zhejiang University+School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study; Zhejiang University+School of Engineering, Westlake University; Institute of Advanced Technology, Westlake Institute for Advanced Study; Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "github": "https://github.com/StevenZHB/CoT_Causal_Analysis",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1+2;0+1;2;2",
        "aff_unique_norm": "Zhejiang University;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": ";School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;WIAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.175",
        "title": "How Many Languages Make Good Multilingual Instruction Tuning? A Case Study on BLOOM",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Instruction tuning a large language model with multiple languages can prepare it for multilingual downstream tasks. Nonetheless, it is yet to be determined whether having a handful of languages is sufficient, or whether the benefits increase with the inclusion of more. By fine-tuning large multilingual models on 1 to 52 languages, we present a case study on BLOOM to understand three pertinent factors affecting performance: the number of languages, language exposure, and similarity between training and test languages. Overall we found that 1) expanding language coverage in multilingual instruction tuning proves to be beneficial; 2) accuracy often significantly boots if the test language appears in the instruction mixture; 3) languages\u2019 genetic features correlate with cross-lingual transfer more than merely the number of language but different languages benefit to various degrees.",
        "author": "Shaoxiong Ji; Pinzhen Chen",
        "authorids": "/s/shaoxiong-ji/; /p/pinzhen-chen/",
        "bibtex": "@inproceedings{ji-chen-2025-many,\n    title = \"How Many Languages Make Good Multilingual Instruction Tuning? A Case Study on {BLOOM}\",\n    author = \"Ji, Shaoxiong  and\n      Chen, Pinzhen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.175/\",\n    pages = \"2575--2581\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.175.pdf",
        "site": "https://aclanthology.org/2025.coling-main.175/",
        "pdf_size": 664542,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dcHdR80LZGoJ:scholar.google.com/&scioq=How+Many+Languages+Make+Good+Multilingual+Instruction+Tuning%3F+A+Case+Study+on+BLOOM&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Technical University of Darmstadt + University of Helsinki; University of Edinburgh",
        "aff_domain": "tu-darmstadt.de;ed.ac.uk",
        "email": "tu-darmstadt.de;ed.ac.uk",
        "github": "",
        "project": "huggingface.co/collections/MaLA-LM \u2192Lucky52",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Technical University of Darmstadt;University of Helsinki;University of Edinburgh",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://www.helsinki.fi;https://www.ed.ac.uk",
        "aff_unique_abbr": "TUD;UH;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2",
        "aff_country_unique": "Germany;Finland;United Kingdom"
    },
    {
        "id": "2025.coling-main.165",
        "title": "How Transliterations Improve Crosslingual Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent studies have shown that post-aligning multilingual pretrained language models (mPLMs) using alignment objectives on both original and transliterated data can improve crosslingual alignment. This improvement further leads to better crosslingual transfer performance. However, it remains unclear how and why a better crosslingual alignment is achieved, as this technique only involves transliterations, and does not use any parallel data. This paper attempts to explicitly evaluate the crosslingual alignment and identify the key elements in transliteration-based approaches that contribute to better performance. For this, we train multiple models under varying setups for two pairs of related languages: (1) Polish and Ukrainian and (2) Hindi and Urdu. To assess alignment, we define four types of similarities based on sentence representations. Our experimental results show that adding transliterations alone improves the overall similarities, even for random sentence pairs. With the help of auxiliary transliteration-based alignment objectives, especially the contrastive objective, the model learns to distinguish matched from random pairs, leading to better crosslingual alignment. However, we also show that better alignment does not always yield better downstream performance, suggesting that further research is needed to clarify the connection between alignment and performance. The code implementation is based on https://github.com/cisnlp/Transliteration-PPA.",
        "author": "Yihong Liu; Mingyang Wang; Amir Hossein Kargaran; Ayyoob ImaniGooghari; Orgest Xhelili; Haotian Ye; Chunlan Ma; Fran\u00e7ois Yvon; Hinrich Sch\u00fctze",
        "authorids": "/y/yihong-liu/; /m/mingyang-wang/; /a/amir-hossein-kargaran/; /a/ayyoob-imanigooghari/; /o/orgest-xhelili/; /h/haotian-ye/; /c/chunlan-ma/; /f/francois-yvon/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{liu-etal-2025-transliterations,\n    title = \"How Transliterations Improve Crosslingual Alignment\",\n    author = {Liu, Yihong  and\n      Wang, Mingyang  and\n      Kargaran, Amir Hossein  and\n      ImaniGooghari, Ayyoob  and\n      Xhelili, Orgest  and\n      Ye, Haotian  and\n      Ma, Chunlan  and\n      Yvon, Fran{\\c{c}}ois  and\n      Sch{\\\"u}tze, Hinrich},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.165/\",\n    pages = \"2417--2433\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.165.pdf",
        "site": "https://aclanthology.org/2025.coling-main.165/",
        "pdf_size": 1547939,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=698566712064655011&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML)+Bosch Center for Artificial Intelligence; Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML)+Bosch Center for Artificial Intelligence; Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML); Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML); Technical University of Munich; Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML); Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML); Sorbonne Universit\u00e9, CNRS, ISIR, France; Center for Information and Language Processing, LMU Munich+Munich Center for Machine Learning (MCML)",
        "aff_domain": "cis.lmu.de;cis.lmu.de; ; ; ; ; ; ; ",
        "email": "cis.lmu.de;cis.lmu.de; ; ; ; ; ; ; ",
        "github": "https://github.com/cisnlp/Transliteration-PPA",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1+2;0+1+2;0+1;0+1;3;0+1;0+1;4;0+1",
        "aff_unique_norm": "LMU Munich;Munich Center for Machine Learning;Bosch Center for Artificial Intelligence;Technical University of Munich;Sorbonne Universit\u00e9",
        "aff_unique_dep": "Center for Information and Language Processing;Center for Machine Learning;Center for Artificial Intelligence;;CNRS, ISIR",
        "aff_unique_url": "https://www.lmu.de;https://www.munich-center-for-machine-learning.de;https://www.bosch-ai.com;https://www.tum.de;https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "LMU;MCML;BCAI;TUM;SU",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0+0;0;0+0;0+0;1;0+0",
        "aff_country_unique": "Germany;France"
    },
    {
        "id": "2025.coling-main.135",
        "title": "How Well Can Large Language Models Reflect? A Human Evaluation of LLM-generated Reflections for Motivational Interviewing Dialogues",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Motivational Interviewing (MI) is a counseling technique that promotes behavioral change through reflective responses to mirror or refine client statements. While advanced Large Language Models (LLMs) can generate engaging dialogues, challenges remain for applying them in a sensitive context such as MI. This work assesses the potential of LLMs to generate MI reflections via three LLMs: GPT-4, Llama-2, and BLOOM, and explores the effect of dialogue context size and integration of MI strategies for reflection generation by LLMs. We conduct evaluations using both automatic metrics and human judges on four criteria: appropriateness, relevance, engagement, and naturalness, to assess whether these LLMs can accurately generate the nuanced therapeutic communication required in MI. While we demonstrate LLMs\u2019 potential in generating MI reflections comparable to human therapists, content analysis shows that significant challenges remain. By identifying the strengths and limitations of LLMs in generating empathetic and contextually appropriate reflections in MI, this work contributes to the ongoing dialogue in enhancing LLM\u2019s role in therapeutic counseling.",
        "author": "Erkan Basar; Xin Sun; Iris Hendrickx; Jan de Wit; Tibor Bosse; Gert-Jan De Bruijn; Jos A. Bosch; Emiel Krahmer",
        "authorids": "/e/erkan-basar/; /x/xin-sun/; /i/iris-hendrickx/; /j/jan-de-wit/; /t/tibor-bosse/; /g/gert-jan-de-bruijn/; /j/jos-a-bosch/; /e/emiel-krahmer/",
        "bibtex": "@inproceedings{basar-etal-2025-well,\n    title = \"How Well Can Large Language Models Reflect? A Human Evaluation of {LLM}-generated Reflections for Motivational Interviewing Dialogues\",\n    author = \"Basar, Erkan  and\n      Sun, Xin  and\n      Hendrickx, Iris  and\n      de Wit, Jan  and\n      Bosse, Tibor  and\n      De Bruijn, Gert-Jan  and\n      Bosch, Jos A.  and\n      Krahmer, Emiel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.135/\",\n    pages = \"1964--1982\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.135.pdf",
        "site": "https://aclanthology.org/2025.coling-main.135/",
        "pdf_size": 563704,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3956784694903875612&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.3",
        "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architectural Inductive Biases on Long-Context Abilities",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Long sequences occur in abundance within real-world scenarios, hence properly modelling them opens numerous down-stream use-cases. Deep neural networks, however, have often struggled with these for a variety of reasons. Recent advances, both in system engineering as well as model design, have enabled the scaling up of model that are purported to support extended context length. In particular, the state-space and linear recurrent neural network families of models hypothetically can entend to infinite sequence length. However, is this too good to be true? We conduct an evaluation to show that while such claims may be sound theoretically, there remain large practical gaps that are empirically observed. In particular, recurrent models still suffer in the same settings as long-context LLMs with attention. We further show that different inductive biases have inconsistent extrapolation capabilities, highlighting the need to further study such paradigms and investigate why long-context models seemingly fail to behave as one might expect.",
        "author": "Jerry Huang",
        "authorids": "/j/jerry-huang/",
        "bibtex": "@inproceedings{huang-2025-well,\n    title = \"How Well Can a Long Sequence Model Model Long Sequences? Comparing Architectural Inductive Biases on Long-Context Abilities\",\n    author = \"Huang, Jerry\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.3/\",\n    pages = \"29--39\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.3.pdf",
        "site": "https://aclanthology.org/2025.coling-main.3/",
        "pdf_size": 359914,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15275314457864901402&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Mila - Quebec AI Institute & Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "mila.quebec",
        "email": "mila.quebec",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "Quebec AI Institute",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-main.514",
        "title": "How to Leverage Digit Embeddings to Represent Numbers?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Within numerical reasoning, understanding numbers themselves is still a challenge for existing language models. Simple generalisations, such as solving 100+200 instead of 1+2, can substantially affect model performance (Sivakumar and Moosavi, 2023). Among various techniques, character-level embeddings of numbers have emerged as a promising approach to improve number representation. However, this method has limitations as it leaves the task of aggregating digit representations to the model, which lacks direct supervision for this process. In this paper, we explore the use of mathematical priors to compute aggregated digit embeddings and explicitly incorporate these aggregates into transformer models. This can be achieved either by adding a special token to the input embeddings or by introducing an additional loss function to enhance correct predictions. We evaluate the effectiveness of incorporating this explicit aggregation, analysing its strengths and shortcomings, and discuss future directions to better benefit from this approach. Our methods, while simple, are compatible with any pretrained model, easy to implement, and have been made publicly available.",
        "author": "Jasivan Alex Sivakumar; Nafise Sadat Moosavi",
        "authorids": "/j/jasivan-alex-sivakumar/; /n/nafise-sadat-moosavi/",
        "bibtex": "@inproceedings{sivakumar-moosavi-2025-leverage,\n    title = \"How to Leverage Digit Embeddings to Represent Numbers?\",\n    author = \"Sivakumar, Jasivan Alex  and\n      Moosavi, Nafise Sadat\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.514/\",\n    pages = \"7685--7697\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.514.pdf",
        "site": "https://aclanthology.org/2025.coling-main.514/",
        "pdf_size": 544689,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2246998408278045556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, The University of Sheffield, United Kingdom; School of Computer Science, The University of Sheffield, United Kingdom",
        "aff_domain": "sheffield.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;sheffield.ac.uk",
        "github": "https://github.com/jasivan/Number-Embeddings",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The University of Sheffield",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.754",
        "title": "Human Interest Framing across Cultures: A Case Study on Climate Change",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Human Interest (HI) framing is a narrative strategy that injects news stories with a relatable, emotional angle and a human face to engage the audience. In this study we investigate the use of HI framing across different English-speaking cultures in news articles about climate change. Despite its demonstrated impact on the public\u2019s behaviour and perception of an issue, HI framing has been under-explored in NLP to date. We perform a systematic analysis of HI stories to understand its role in climate change reporting in English-speaking countries from four continents. Our findings reveal key differences in how climate change is portrayed across countries, encompassing aspects such as narrative roles, article polarity, pronoun prevalence, and topics. We also demonstrate that these linguistic aspects boost the performance of fine-tuned pre-trained language models on HI story classification.",
        "author": "Gisela Vallejo; Christine de Kock; Timothy Baldwin; Lea Frermann",
        "authorids": "/g/gisela-vallejo/; /c/christine-de-kock/; /t/timothy-baldwin/; /l/lea-frermann/",
        "bibtex": "@inproceedings{vallejo-etal-2025-human,\n    title = \"Human Interest Framing across Cultures: A Case Study on Climate Change\",\n    author = \"Vallejo, Gisela  and\n      de Kock, Christine  and\n      Baldwin, Timothy  and\n      Frermann, Lea\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.754/\",\n    pages = \"11380--11398\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.754.pdf",
        "site": "https://aclanthology.org/2025.coling-main.754/",
        "pdf_size": 2202603,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LJiwlR7o1PkJ:scholar.google.com/&scioq=Human+Interest+Framing+across+Cultures:+A+Case+Study+on+Climate+Change&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "The University of Melbourne; The University of Melbourne; The University of Melbourne + MBZUAI; The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au; ; ; ",
        "email": "student.unimelb.edu.au; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "University of Melbourne;Mohamed Bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UniMelb;MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Australia;United Arab Emirates"
    },
    {
        "id": "2025.coling-demos.14",
        "title": "Human-Like Embodied AI Interviewer: Employing Android ERICA in Real International Conference",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "This paper introduces the human-like embodied AI interviewer which integrates android robots equipped with advanced conversational capabilities, including attentive listening, conversational repairs, and user fluency adaptation. Moreover, it can analyze and present results post-interview. We conducted a real-world case study at SIGDIAL 2024 with 42 participants, of whom 69% reported positive experiences. This study demonstrated the system\u2019s effectiveness in conducting interviews just like a human and marked the first employment of such a system at an international conference. The demonstration video is available at https://youtu.be/jCuw9g99KuE.",
        "author": "Zi Haur Pang; Yahui Fu; Divesh Lala; Mikey Elmers; Koji Inoue; Tatsuya Kawahara",
        "authorids": "/z/zi-haur-pang/; /y/yahui-fu/; /d/divesh-lala/; /m/mikey-elmers/; /k/koji-inoue/; /t/tatsuya-kawahara/",
        "bibtex": "@inproceedings{pang-etal-2025-human,\n    title = \"Human-Like Embodied {AI} Interviewer: Employing Android {ERICA} in Real International Conference\",\n    author = \"Pang, Zi Haur  and\n      Fu, Yahui  and\n      Lala, Divesh  and\n      Elmers, Mikey  and\n      Inoue, Koji  and\n      Kawahara, Tatsuya\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.14/\",\n    pages = \"136--150\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.14.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.14/",
        "pdf_size": 6397098,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8409675610583194657&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan",
        "aff_domain": "sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp",
        "email": "sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp;sap.ist.i.kyoto-u.ac.jp",
        "github": "",
        "project": "https://youtu.be/jCuw9g99KuE",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.258",
        "title": "HyperHatePrompt: A Hypergraph-based Prompting Fusion Model for Multimodal Hate Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal hate detection aims to identify hate content across multiple modalities for promoting a harmonious online environment. Despite promising progress, three critical challenges, the absence of implicit hateful cues, the cross-modal-induced hate, and the diversity of hate target groups, inherent in the multimodal hate detection task, have been overlooked. To address these challenges, we propose a hypergraph-based prompting fusion model. Our model first uses tailored prompts to infer implicit hateful cues. It then introduces hyperedges to capture cross-modal-induced hate and applies a diversity-oriented hyperedge expansion strategy to account for different hate target groups. Finally, hypergraph convolution fuses diverse hateful cues, enhancing the exploration of cross-modal hate and targeting specific groups. Experimental results on two benchmark datasets show that our model achieves state-of-the-art performance in multimodal hate detection.",
        "author": "Bo Xu; Erchen Yu; Jiahui Zhou; Hongfei Lin; Linlin Zong",
        "authorids": "/b/bo-xu/; /e/erchen-yu/; /j/jiahui-zhou/; /h/hongfei-lin/; /l/linlin-zong/",
        "bibtex": "@inproceedings{xu-etal-2025-hyperhateprompt,\n    title = \"{H}yper{H}ate{P}rompt: A Hypergraph-based Prompting Fusion Model for Multimodal Hate Detection\",\n    author = \"Xu, Bo  and\n      Yu, Erchen  and\n      Zhou, Jiahui  and\n      Lin, Hongfei  and\n      Zong, Linlin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.258/\",\n    pages = \"3825--3835\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.258.pdf",
        "site": "https://aclanthology.org/2025.coling-main.258/",
        "pdf_size": 883675,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uY5G3W4CY_oJ:scholar.google.com/&scioq=HyperHatePrompt:+A+Hypergraph-based+Prompting+Fusion+Model+for+Multimodal+Hate+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Software, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Software, Dalian University of Technology",
        "aff_domain": "dlut.edu.cn;mail.dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "email": "dlut.edu.cn;mail.dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://en.dlut.edu.cn/",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Dalian",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.64",
        "title": "HyperIDP: Customizing Temporal Hypergraph Neural Networks for Multi-Scale Information Diffusion Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Information diffusion prediction is crucial for understanding how information spreads within social networks, addressing both macroscopic and microscopic prediction tasks. Macroscopic prediction assesses the overall impact of diffusion, while microscopic prediction focuses on identifying the next user likely to be influenced. However, few studies have focused on both scales of diffusion. This paper presents HyperIDP, a novel Hypergraph-based model designed to manage both macroscopic and microscopic Information Diffusion Prediction tasks. The model captures interactions and dynamics of cascades at the macro level with hypergraph neural networks (HGNNs) while integrating social homophily at the micro level. Considering the diverse data distributions across social media platforms, which necessitate extensive tuning of HGNN architectures, a search space is constructed to accommodate diffusion hypergraphs, with optimal architectures derived through differentiable search strategies. Additionally, cooperative-adversarial loss, inspired by multi-task learning, is introduced to ensure that the model can leverage the advantages of the shared representation when handling both tasks, while also avoiding potential conflicts. Experimental results show that the proposed model significantly outperforms baselines.",
        "author": "Haowei Xu; Chao Gao; Xianghua Li; Zhen Wang",
        "authorids": "/h/haowei-xu/; /c/chao-gao/; /x/xianghua-li/; /z/zhen-wang/",
        "bibtex": "@inproceedings{xu-etal-2025-hyperidp,\n    title = \"{H}yper{IDP}: Customizing Temporal Hypergraph Neural Networks for Multi-Scale Information Diffusion Prediction\",\n    author = \"Xu, Haowei  and\n      Gao, Chao  and\n      Li, Xianghua  and\n      Wang, Zhen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.64/\",\n    pages = \"964--977\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.64.pdf",
        "site": "https://aclanthology.org/2025.coling-main.64/",
        "pdf_size": 1327013,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:iuSNela54q4J:scholar.google.com/&scioq=HyperIDP:+Customizing+Temporal+Hypergraph+Neural+Networks+for+Multi-Scale+Information+Diffusion+Prediction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, P.R. China; School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, P.R. China; School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, P.R. China + School of Cybersecurity, Northwestern Polytechnical University, P.R. China; School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, P.R. China + School of Cybersecurity, Northwestern Polytechnical University, P.R. China",
        "aff_domain": "nwpu.edu.cn; ;nwpu.edu.cn; ",
        "email": "nwpu.edu.cn; ;nwpu.edu.cn; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0+0;0+0",
        "aff_unique_norm": "Northwestern Polytechnical University",
        "aff_unique_dep": "School of Artificial Intelligence, Optics and Electronics (iOPEN)",
        "aff_unique_url": "https://www.nwpu.edu.cn",
        "aff_unique_abbr": "NPU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.693",
        "title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our understanding of how this ability is acquired at the training stage. However, existing evaluation frameworks primarily focus on language abilities and knowledge, often overlooking the assessment of ICL ability. In this work, we introduce the ICLEval benchmark to evaluate the ICL abilities of LLMs, which encompasses two key sub-abilities: exact copying and rule learning. Through the ICLEval benchmark, we demonstrate that ICL ability is universally present in different LLMs, and model size is not the sole determinant of ICL efficacy. Surprisingly, we observe that ICL abilities, particularly copying, develop early in the pretraining process and stabilize afterward.",
        "author": "Wentong Chen; Yankai Lin; ZhenHao Zhou; HongYun Huang; YanTao Jia; Zhao Cao; Ji-Rong Wen",
        "authorids": "/w/wentong-chen/; /y/yankai-lin/; /z/zhenhao-zhou/; /h/hongyun-huang/; /y/yantao-jia/; /z/zhao-cao/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{chen-etal-2025-icleval,\n    title = \"{ICLE}val: Evaluating In-Context Learning Ability of Large Language Models\",\n    author = \"Chen, Wentong  and\n      Lin, Yankai  and\n      Zhou, ZhenHao  and\n      Huang, HongYun  and\n      Jia, YanTao  and\n      Cao, Zhao  and\n      Wen, Ji-Rong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.693/\",\n    pages = \"10398--10422\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.693.pdf",
        "site": "https://aclanthology.org/2025.coling-main.693/",
        "pdf_size": 1136184,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11369290990345223333&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Huawei Poisson Lab; Huawei Poisson Lab; Huawei Poisson Lab; Huawei Poisson Lab; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods+School of Information, Renmin University of China",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/RUCBM/ICLEval",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;1;1;1;0+2+0",
        "aff_unique_norm": "Renmin University of China;Huawei;Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Poisson Lab;Big Data Management and Analysis",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.huawei.com;",
        "aff_unique_abbr": "RUC;Huawei;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.665",
        "title": "IRR: Image Review Ranking Framework for Evaluating Vision-Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large-scale Vision-Language Models (LVLMs) process both images and text, excelling in multimodal tasks such as image captioning and description generation. However, while these models excel at generating factual content, their ability to generate and evaluate texts reflecting perspectives on the same image, depending on the context, has not been sufficiently explored. To address this, we propose IRR: Image Review Rank, a novel evaluation framework designed to assess critic review texts from multiple perspectives. IRR evaluates LVLMs by measuring how closely their judgments align with human interpretations. We validate it using a dataset of images from 15 categories, each with five critic review texts and annotated rankings in both English and Japanese, totaling over 2,000 data instances. Our results indicate that, although LVLMs exhibited consistent performance across languages, their correlation with human annotations was insufficient, highlighting the need for further advancements. These findings highlight the limitations of current evaluation methods and the need for approaches that better capture human reasoning in Vision & Language tasks.",
        "author": "Kazuki Hayashi; Kazuma Onishi; Toma Suzuki; Yusuke Ide; Seiji Gobara; Shigeki Saito; Yusuke Sakai; Hidetaka Kamigaito; Katsuhiko Hayashi; Taro Watanabe",
        "authorids": "/k/kazuki-hayashi/; /k/kazuma-onishi/; /t/toma-suzuki/; /y/yusuke-ide/; /s/seiji-gobara/; /s/shigeki-saito/; /y/yusuke-sakai/; /h/hidetaka-kamigaito/; /k/katsuhiko-hayashi/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{hayashi-etal-2025-irr,\n    title = \"{IRR}: Image Review Ranking Framework for Evaluating Vision-Language Models\",\n    author = \"Hayashi, Kazuki  and\n      Onishi, Kazuma  and\n      Suzuki, Toma  and\n      Ide, Yusuke  and\n      Gobara, Seiji  and\n      Saito, Shigeki  and\n      Sakai, Yusuke  and\n      Kamigaito, Hidetaka  and\n      Hayashi, Katsuhiko  and\n      Watanabe, Taro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.665/\",\n    pages = \"9939--9956\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.665.pdf",
        "site": "https://aclanthology.org/2025.coling-main.665/",
        "pdf_size": 5327120,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6640028607680730934&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Nara Institute of Science and Technology; Hokkaido University; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Hokkaido University; Nara Institute of Science and Technology; Nara Institute of Science and Technology; The University of Tokyo; Nara Institute of Science and Technology",
        "aff_domain": "naist.ac.jp; ; ; ; ; ;is.naist.jp;is.naist.jp;is.naist.jp;g.ecc.u-tokyo.ac.jp",
        "email": "naist.ac.jp; ; ; ; ; ;is.naist.jp;is.naist.jp;is.naist.jp;g.ecc.u-tokyo.ac.jp",
        "github": "",
        "project": "https://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0",
        "author_num": 10,
        "aff_unique_index": "0;1;0;0;0;1;0;0;2;0",
        "aff_unique_norm": "Nara Institute of Science and Technology;Hokkaido University;University of Tokyo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nist.go.jp;https://www.hokudai.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "NIST;Hokkaido U;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.434",
        "title": "IRUEX: A Study on Large Language Models Problem-Solving Skills in Iran\u2019s University Entrance Exam",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we present the IRUEX dataset, a novel multiple-choice educational resource specifically designed to evaluate the performance of Large Language Models (LLMs) across seven distinct categories. The dataset contains 868 Iran university entrance exam questions (Konkour) and 36,485 additional questions. Each additional question is accompanied by detailed solutions, and the dataset also includes relevant high school textbooks, providing comprehensive study material. A key feature of IRUEX is its focus on underrepresented languages, particularly assessing problem-solving skills, language proficiency, and reasoning. Our evaluation shows that GPT-4o outperforms the other LLMs tested on the IRUEX dataset. Techniques such as few-shot learning and retrieval-augmented generation (RAG) display varied effects across different categories, highlighting their unique strengths in specific areas. Additionally, a comprehensive user study classifies the errors made by LLMs into ten problem-solving ability categories. The analysis highlights that calculations and linguistic knowledge, particularly in low-resource languages, remain significant weaknesses in current LLMs. IRUEX has the potential to serve as a benchmark for evaluating the reasoning capabilities of LLMs in non-English settings, providing a foundation for improving their performance in diverse languages and contexts",
        "author": "Hamed Khademi Khaledi; Heshaam Faili",
        "authorids": "/h/hamed-khademi-khaledi/; /h/heshaam-faili/",
        "bibtex": "@inproceedings{khademi-khaledi-faili-2025-iruex,\n    title = \"{IRUEX}: A Study on Large Language Models Problem-Solving Skills in {I}ran{'}s University Entrance Exam\",\n    author = \"Khademi Khaledi, Hamed  and\n      Faili, Heshaam\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.434/\",\n    pages = \"6505--6519\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.434.pdf",
        "site": "https://aclanthology.org/2025.coling-main.434/",
        "pdf_size": 457065,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:RfRws-7VyMgJ:scholar.google.com/&scioq=IRUEX:+A+Study+on+Large+Language+Models+Problem-Solving+Skills+in+Iran%E2%80%99s+University+Entrance+Exam&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran",
        "aff_domain": "ut.ac.ir;ut.ac.ir",
        "email": "ut.ac.ir;ut.ac.ir",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tehran",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://en.ut.ac.ir",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tehran",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2025.coling-main.91",
        "title": "ITERATE: Image-Text Enhancement, Retrieval, and Alignment for Transmodal Evolution with LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Inspired by human cognitive behavior, we introduce visual modality to enhance the performance of pure text-based question-answering tasks with the development of multimodal models. However, obtaining corresponding images through manual annotation often entails high costs. Faced with this challenge, an intuitive strategy is to use search engines or use web scraping techniques to automatically obtain relevant image information. However, the images obtained by this strategy may be of low quality and may not match the context of the original task, which could fail to improve or even decrease performance on downstream tasks. In this paper, we propose a novel framework named \u201cITERATE\u201d, aimed at retrieving and optimizing the quality of images to improve the alignment between text and images. Inspired by evolutionary algorithms in reinforcement learning and driven by the synergy of large language models (LLMs) and multimodal models, ITERATE employs a series of strategic actions such as filtering, optimizing, and retrieving to acquire higher quality images, and repeats this process over multiple generations to enhance the quality of the entire image cluster. Our experimental results on the ScienceQA, ARC-Easy, and OpenDataEval datasets also verify the effectiveness of our method, showing improvements of 3.5%, 5%, and 7%, respectively.",
        "author": "Chenhan Fu; Guoming Wang; Juncheng Li; Wenqiao Zhang; Rongxing Lu; Siliang Tang",
        "authorids": "/c/chenhan-fu/; /g/guoming-wang/; /j/juncheng-li/; /w/wenqiao-zhang/; /r/rongxing-lu/; /s/siliang-tang/",
        "bibtex": "@inproceedings{fu-etal-2025-iterate,\n    title = \"{ITERATE}: Image-Text Enhancement, Retrieval, and Alignment for Transmodal Evolution with {LLM}s\",\n    author = \"Fu, Chenhan  and\n      Wang, Guoming  and\n      Li, Juncheng  and\n      Zhang, Wenqiao  and\n      Lu, Rongxing  and\n      Tang, Siliang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.91/\",\n    pages = \"1365--1376\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.91.pdf",
        "site": "https://aclanthology.org/2025.coling-main.91/",
        "pdf_size": 1921159,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3i_gB-iqaqwJ:scholar.google.com/&scioq=ITERATE:+Image-Text+Enhancement,+Retrieval,+and+Alignment+for+Transmodal+Evolution+with+LLMs&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Software Technology; Ningbo research institute; School of Software Technology; School of Software Technology; Faculty of Computer Science, University of New Brunswick; College of Computer Science and Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;unb.ca;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;unb.ca;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;2;3",
        "aff_unique_norm": "School of Software Technology;Ningbo Research Institute;University of New Brunswick;Zhejiang University",
        "aff_unique_dep": "Software Technology;;Faculty of Computer Science;College of Computer Science and Technology",
        "aff_unique_url": ";;https://www.unb.ca;http://www.zju.edu.cn",
        "aff_unique_abbr": ";;UNB;ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;1",
        "aff_country_unique": ";China;Canada"
    },
    {
        "id": "2025.coling-main.699",
        "title": "IberoBench: A Benchmark for LLM Evaluation in Iberian Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The current best practice to measure the performance of base Large Language Models is to establish a multi-task benchmark that covers a range of capabilities of interest. Currently, however, such benchmarks are only available in a few high-resource languages. To address this situation, we present IberoBench, a multilingual, multi-task benchmark for Iberian languages (i.e., Basque, Catalan, Galician, European Spanish and European Portuguese) built on the LM Evaluation Harness framework. The benchmark consists of 62 tasks divided into 179 subtasks. We evaluate 33 existing LLMs on IberoBench on 0- and 5-shot settings. We also explore the issues we encounter when working with the Harness and our approach to solving them to ensure high-quality evaluation.",
        "author": "Irene Baucells; Javier Aula-Blasco; Iria de-Dios-Flores; Silvia Paniagua Su\u00e1rez; Naiara Perez; Anna Salles; Susana Sotelo Docio; J\u00falia Falc\u00e3o; Jose Javier Saiz; Robiert Sepulveda Torres; Jeremy Barnes; Pablo Gamallo; Aitor Gonzalez-Agirre; German Rigau; Marta Villegas",
        "authorids": "/i/irene-baucells/; /j/javier-aula-blasco/; /i/iria-de-dios-flores/; /s/silvia-paniagua-suarez/; /n/naiara-perez/; /a/anna-salles/; /s/susana-sotelo-docio/; /j/julia-falcao/; /j/jose-javier-saiz/; /r/robiert-sepulveda-torres/; /j/jeremy-barnes/; /p/pablo-gamallo/; /a/aitor-gonzalez-agirre/; /g/german-rigau/; /m/marta-villegas/",
        "bibtex": "@inproceedings{baucells-etal-2025-iberobench,\n    title = \"{I}bero{B}ench: A Benchmark for {LLM} Evaluation in {I}berian Languages\",\n    author = \"Baucells, Irene  and\n      Aula-Blasco, Javier  and\n      de-Dios-Flores, Iria  and\n      Paniagua Su{\\'a}rez, Silvia  and\n      Perez, Naiara  and\n      Salles, Anna  and\n      Sotelo Docio, Susana  and\n      Falc{\\~a}o, J{\\'u}lia  and\n      Saiz, Jose Javier  and\n      Sepulveda Torres, Robiert  and\n      Barnes, Jeremy  and\n      Gamallo, Pablo  and\n      Gonzalez-Agirre, Aitor  and\n      Rigau, German  and\n      Villegas, Marta\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.699/\",\n    pages = \"10491--10519\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.699.pdf",
        "site": "https://aclanthology.org/2025.coling-main.699/",
        "pdf_size": 1023785,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8101310607774855854&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;",
        "github": "https://github.com/EleutherAI/lm-evaluation-harness",
        "project": "",
        "author_num": 15
    },
    {
        "id": "2025.coling-main.280",
        "title": "Idea23D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the success of 2D diffusion models, 2D AIGC content has already transformed our lives. Recently, this success has been extended to 3D AIGC, with state-of-the-art methods generating textured 3D models from single images or text. However, we argue that current 3D AIGC methods still don\u2019t fully unleash human creativity. We often imagine 3D content made from multimodal inputs, such as what it would look like if my pet bunny were eating a doughnut on the table. In this paper, we explore a novel 3D AIGC approach: generating 3D content from IDEAs. An IDEA is a multimodal input composed of text, image, and 3D models. To our knowledge, this challenging and exciting 3D AIGC setting has not been studied before. We propose the new framework Idea23D, which combines three agents based on large multimodal models (LMMs) and existing algorithmic tools. These three LMM-based agents are tasked with prompt generation, model selection, and feedback reflection. They collaborate and critique each other in a fully automated loop, without human intervention. The framework then generates a text prompt to create 3D models that align closely with the input IDEAs. We demonstrate impressive 3D AIGC results that surpass previous methods. To comprehensively assess the 3D AIGC capabilities of Idea23D, we introduce the Eval3DAIGC-198 dataset, containing 198 multimodal inputs for 3D generation tasks. This dataset evaluates the alignment between generated 3D content and input IDEAs. Our user study and quantitative results show that Idea23D significantly improves the success rate and accuracy of 3D generation, with excellent compatibility across various LMM, Text-to-Image, and Image-to-3D models. Code and dataset are available at https://idea23d.github.io/.",
        "author": "Junhao Chen; Xiang Li; Xiaojun Ye; Chao Li; Zhaoxin Fan; Hao Zhao",
        "authorids": "/j/junhao-chen/; /x/xiang-li/; /x/xiaojun-ye/; /c/chao-li/; /z/zhaoxin-fan/; /h/hao-zhao/",
        "bibtex": "@inproceedings{chen-etal-2025-idea23d,\n    title = \"{I}dea23{D}: Collaborative {LMM} Agents Enable 3{D} Model Generation from Interleaved Multimodal Inputs\",\n    author = \"Chen, Junhao  and\n      Li, Xiang  and\n      Ye, Xiaojun  and\n      Li, Chao  and\n      Fan, Zhaoxin  and\n      Zhao, Hao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.280/\",\n    pages = \"4149--4166\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.280.pdf",
        "site": "https://aclanthology.org/2025.coling-main.280/",
        "pdf_size": 15880483,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8023591786708766798&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://idea23d.github.io/",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.174",
        "title": "Implicit Discourse Relation Classification For Nigerian Pidgin",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nigerian Pidgin (NP) is an English-based creole language spoken by nearly 100 million people across Nigeria, and is still low-resource in NLP. In particular, there are currently no available discourse parsing tools, which, if available, would have the potential to improve various downstream tasks. Our research focuses on implicit discourse relation classification (IDRC) for NP, a task which, even in English, is not easily solved by prompting LLMs, but requires supervised training. % With this in mind, we have developed a framework for the task, which could also be used by researchers for other English-lexified languages. We systematically compare different approaches to the low resource IDRC task: in one approach, we use English IDRC tools directly on the NP text as well as on their English translations (followed by a back-projection of labels). In another approach, we create a synthetic discourse corpus for NP, in which we automatically translate the English discourse-annotated corpus PDTB to NP, project PDTB labels, and then train an NP IDR classifier. The latter approach of training a \u201cnative\u201d NP classifier outperforms our baseline by 13.27% and 33.98% in f1 score for 4-way and 11-way classification, respectively.",
        "author": "Muhammed Saeed; Peter Bourgonje; Vera Demberg",
        "authorids": "/m/muhammed-saeed/; /p/peter-bourgonje/; /v/vera-demberg/",
        "bibtex": "@inproceedings{saeed-etal-2025-implicit,\n    title = \"Implicit Discourse Relation Classification For {N}igerian {P}idgin\",\n    author = \"Saeed, Muhammed  and\n      Bourgonje, Peter  and\n      Demberg, Vera\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.174/\",\n    pages = \"2561--2574\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.174.pdf",
        "site": "https://aclanthology.org/2025.coling-main.174/",
        "pdf_size": 2464914,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14437389519769802999&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Saarland University, Saarbr\u00fccken, Germany; Saarland University, Saarbr\u00fccken, Germany; Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "lst.uni-saarland.de;lst.uni-saarland.de;coli.uni-saarland.de",
        "email": "lst.uni-saarland.de;lst.uni-saarland.de;coli.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "UdS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.730",
        "title": "Imposter: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present ImPoster, a novel algorithm for generating a target image of a \u2018source\u2019 subject performing a \u2018driving\u2019 action. The inputs to our algorithm are a single pair of a source image with the subject that we wish to edit and a driving image with a subject of an arbitrary class performing the driving action, along with the text descriptions of the two images. Our approach is completely unsupervised and does not require any access to additional annotations like keypoints or pose. Our approach builds on a pretrained text-to-image latent diffusion model and learns the characteristics of the source and the driving image by finetuning the diffusion model for a small number of iterations. At inference time, ImPoster performs step-wise text prompting i.e. it denoises by first moving in the direction of the image manifold corresponding to the driving image followed by the direction of the image manifold corresponding to the text description of the desired target image. We propose a novel diffusion guidance formulation, image frequency guidance, to steer the generation towards the manifold of the source subject and the driving action at every step of the inference denoising. Our frequency guidance formulations are derived from the frequency domain properties of images. We extensively evaluate ImPoster on a diverse set of source-driving image pairs to demonstrate improvements over baselines. To the best of our knowledge, ImPoster is the first approach towards achieving both subject-driven as well as action-driven image personalization.",
        "author": "Divya Kothandaraman; Kuldeep Kulkarni; Sumit Shekhar; Balaji Vasan Srinivasan; Dinesh Manocha",
        "authorids": "/d/divya-kothandaraman/; /k/kuldeep-kulkarni/; /s/sumit-shekhar/; /b/balaji-vasan-srinivasan/; /d/dinesh-manocha/",
        "bibtex": "@inproceedings{kothandaraman-etal-2025-imposter,\n    title = \"Imposter: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models\",\n    author = \"Kothandaraman, Divya  and\n      Kulkarni, Kuldeep  and\n      Shekhar, Sumit  and\n      Srinivasan, Balaji Vasan  and\n      Manocha, Dinesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.730/\",\n    pages = \"11013--11028\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.730.pdf",
        "site": "https://aclanthology.org/2025.coling-main.730/",
        "pdf_size": 21108764,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=624779157578861341&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Maryland College Park; Adobe Research*; Adobe Research*; Adobe Research*; University of Maryland College Park",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/divyakraman/ImPosterDif-fusion2024",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Maryland;Adobe Systems Incorporated",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www/umd.edu;https://research.adobe.com",
        "aff_unique_abbr": "UMD;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.612",
        "title": "Impromptu Cybercrime Euphemism Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Detecting euphemisms is essential for content security on various social media platforms, but existing methods designed for detecting euphemisms are ineffective in impromptu euphemisms. In this work, we make a first attempt to an exploration of impromptu euphemism detection and introduce the Impromptu Cybercrime Euphemisms Detection (ICED) dataset. Moreover, we propose a detection framework tailored to this problem, which employs context augmentation modeling and multi-round iterative training. Our detection framework mainly consists of a coarse-grained and a fine-grained classification model. The coarse-grained classification model removes most of the harmless content in the corpus to be detected. The fine-grained model, impromptu euphemisms detector, integrates context augmentation and multi-round iterations training to better predicts the actual meaning of a masked token. In addition, we leverage ChatGPT to evaluate the mode\u2019s capability. Experimental results demonstrate that our approach achieves a remarkable 76-fold improvement compared to the previous state-of-the-art euphemism detector.",
        "author": "Xiang Li; Yucheng Zhou; Laiping Zhao; Jing Li; Fangming Liu",
        "authorids": "/x/xiang-li/; /y/yucheng-zhou/; /l/laiping-zhao/; /j/jing-li/; /f/fangming-liu/",
        "bibtex": "@inproceedings{li-etal-2025-impromptu,\n    title = \"Impromptu Cybercrime Euphemism Detection\",\n    author = \"Li, Xiang  and\n      Zhou, Yucheng  and\n      Zhao, Laiping  and\n      Li, Jing  and\n      Liu, Fangming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.612/\",\n    pages = \"9112--9123\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.612.pdf",
        "site": "https://aclanthology.org/2025.coling-main.612/",
        "pdf_size": 1530097,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6913597482174694656&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-industry.28",
        "title": "Improve Speech Translation Through Text Rewrite",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Despite recent progress in Speech Translation (ST) research, the challenges posed by inherent speech phenomena that distinguish transcribed speech from written text are not well addressed. The informal and erroneous nature of spontaneous speech is inadequately represented in the typical parallel text available for building translation models. We propose to address these issues through a text rewrite approach that aims to transform transcribed speech into a cleaner style more in line with the expectations of translation models built from written text. Moreover, the advantages of the rewrite model can be effectively distilled into a standalone translation model. Experiments on several benchmarks, using both publicly available and in-house translation models, demonstrate that adding a rewrite model to a traditional ST pipeline is a cost-effect way to address a variety of speech irregularities and improve speech translation quality for multiple language directions and domains.",
        "author": "Jing Wu; Shushu Wang; Kai Fan; Wei Luo; Minpeng Liao; Zhongqiang Huang",
        "authorids": "/j/jing-wu/; /s/shushu-wang/; /k/kai-fan/; /w/wei-luo/; /m/minpeng-liao/; /z/zhongqiang-huang/",
        "bibtex": "@inproceedings{wu-etal-2025-improve,\n    title = \"Improve Speech Translation Through Text Rewrite\",\n    author = \"Wu, Jing  and\n      Wang, Shushu  and\n      Fan, Kai  and\n      Luo, Wei  and\n      Liao, Minpeng  and\n      Huang, Zhongqiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.28/\",\n    pages = \"331--342\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.28.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.28/",
        "pdf_size": 336614,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:lXqQhA4YABYJ:scholar.google.com/&scioq=Improve+Speech+Translation+Through+Text+Rewrite&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Tongyi Lab; Zhejiang University + Tongyi Lab; Tongyi Lab; Tongyi Lab; Tongyi Lab; Tongyi Lab",
        "aff_domain": "alibaba-inc.com;zju.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;zju.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+0;0;0;0;0",
        "aff_unique_norm": "Tongyi Lab;Zhejiang University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.zju.edu.cn",
        "aff_unique_abbr": ";ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2025.coling-main.636",
        "title": "Improved Sparse Upcycling for Instruction Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The Mixture-of-Experts (MoE) architecture has demonstrated significant potential in both large-scale pre-training and instruction tuning by offering increased parameter capacity without additional inference costs. However, developing MoE models faces challenges including training instability and the need for substantial high-quality training data. While efficient methodologies like sparse upcycling exist, they often lead to performance degradation in instruction tuning scenarios. We introduce representation-based sparse upcycling, a straightforward yet effective technique for converting dense language models into sparsely activated ones while maintaining similar computational costs. Unlike conventional sparse upcycling, our approach leverages intermediate representations from language models to initialize router weights. This strategy addresses the mismatch between randomly initialized and well-trained parameters while providing prior knowledge to guide expert specialization during training. Extensive experiments across diverse benchmarks demonstrate significant improvements in both model capabilities and routing consistency compared to existing approaches.",
        "author": "Wangyi Jiang; Yaojie Lu; Hongyu Lin; Xianpei Han; Le Sun",
        "authorids": "/w/wangyi-jiang/; /y/yaojie-lu/; /h/hongyu-lin/; /x/xianpei-han/; /l/le-sun/",
        "bibtex": "@inproceedings{jiang-etal-2025-improved,\n    title = \"Improved Sparse Upcycling for Instruction Tuning\",\n    author = \"Jiang, Wangyi  and\n      Lu, Yaojie  and\n      Lin, Hongyu  and\n      Han, Xianpei  and\n      Sun, Le\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.636/\",\n    pages = \"9485--9498\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.636.pdf",
        "site": "https://aclanthology.org/2025.coling-main.636/",
        "pdf_size": 1081921,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12257169208175256440&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences; Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Software;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.219",
        "title": "Improvement in Sign Language Translation Using Text CTC Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current sign language translation (SLT) approaches often rely on gloss-based supervision with Connectionist Temporal Classification (CTC), limiting their ability to handle non-monotonic alignments between sign language video and spoken text. In this work, we propose a novel method combining joint CTC/Attention and transfer learning. The joint CTC/Attention introduces hierarchical encoding and integrates CTC with the attention mechanism during decoding, effectively managing both monotonic and non-monotonic alignments. Meanwhile, transfer learning helps bridge the modality gap between vision and language in SLT. Experimental results on two widely adopted benchmarks, RWTH-PHOENIX-Weather 2014 T and CSL-Daily, show that our method achieves results comparable to state-of-the-art and outperforms the pure-attention baseline. Additionally, this work opens a new door for future research into gloss-free SLT using text-based CTC alignment.",
        "author": "Sihan Tan; Taro Miyazaki; Nabeela Khan; Kazuhiro Nakadai",
        "authorids": "/s/sihan-tan/; /t/taro-miyazaki/; /n/nabeela-khan/; /k/kazuhiro-nakadai/",
        "bibtex": "@inproceedings{tan-etal-2025-improvement,\n    title = \"Improvement in Sign Language Translation Using Text {CTC} Alignment\",\n    author = \"Tan, Sihan  and\n      Miyazaki, Taro  and\n      Khan, Nabeela  and\n      Nakadai, Kazuhiro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.219/\",\n    pages = \"3255--3266\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.219.pdf",
        "site": "https://aclanthology.org/2025.coling-main.219/",
        "pdf_size": 922127,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2732149828850067373&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Science Tokyo+NHK Science & Technology Research Laboratories; NHK Science & Technology Research Laboratories; Institute of Science Tokyo; Institute of Science Tokyo",
        "aff_domain": "ra.sc.e.titech.ac.jp;nhk.or.jp;ra.sc.e.titech.ac.jp;ra.sc.e.titech.ac.jp",
        "email": "ra.sc.e.titech.ac.jp;nhk.or.jp;ra.sc.e.titech.ac.jp;ra.sc.e.titech.ac.jp",
        "github": "https://github.com/Claire874/TextCTC-SLT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "Institute of Science, Tokyo;NHK Science & Technology Research Laboratories",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iost.jp;https://www.nhk.or.jp/st/index.html",
        "aff_unique_abbr": "IoST;NHK STRL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.456",
        "title": "Improving Accessibility of SCOTUS Opinions: A Benchmark Study and a New Dataset for Generic Heading Prediction and Specific Heading Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The opinions of the U.S. Supreme Court (SCOTUS) are known for their extensive length, complex legal language, and lack of titled sections, which pose significant challenges for accessibility and comprehension. This paper defines the task of automatic section titling by proposing both generic and specific headings for each section. Given the scarcity of sections with headings in SCOTUS, we study the possibility of using data from lower courts for training models. A dataset of sections with generic or specific headings covering three courts (SCOTUS and two lower courts) was compiled. A supplementary SCOTUS set was manually annotated with these two types of titles. In order to establish a benchmark, we provide the performance of different systems trained for each subtask: For generic heading prediction, we compare the performance of fine-tuning non-contextual, general and domain-oriented pretrained language models. Transformer-based sequence-to-sequence models are considered for specific heading generation. Our results show that a fine-tuned LegalBERT can achieve a F1 score of about 0.90 % in predicting generic headings. They also show that BART and T5 have similar performance in generating specific headings and that, although this performance is good, there is still room for improvement. In addition, we provide a human assessment to support the generation experiment and show a quasi-linear correlation between human degrees of agreement and the results of conventional measures such as ROUGE and BERTScore.",
        "author": "Malek Yaich; Nicolas Hernandez",
        "authorids": "/m/malek-yaich/; /n/nicolas-hernandez/",
        "bibtex": "@inproceedings{yaich-hernandez-2025-improving,\n    title = \"Improving Accessibility of {SCOTUS} Opinions: A Benchmark Study and a New Dataset for Generic Heading Prediction and Specific Heading Generation\",\n    author = \"Yaich, Malek  and\n      Hernandez, Nicolas\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.456/\",\n    pages = \"6827--6839\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.456.pdf",
        "site": "https://aclanthology.org/2025.coling-main.456/",
        "pdf_size": 363982,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ifhFAeOqp2gJ:scholar.google.com/&scioq=Improving+Accessibility+of+SCOTUS+Opinions:+A+Benchmark+Study+and+a+New+Dataset+for+Generic+Heading+Prediction+and+Specific+Heading+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France; Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France",
        "aff_domain": "etudiant-fst.utm.tn;univ-nantes.fr",
        "email": "etudiant-fst.utm.tn;univ-nantes.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nantes Universit\u00e9",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.univ-nantes.fr",
        "aff_unique_abbr": "Nantes Universit\u00e9",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nantes",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.189",
        "title": "Improving Automatic Grammatical Error Annotation for Chinese Through Linguistically-Informed Error Typology",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Comprehensive error annotation is essential for developing effective Grammatical Error Correction (GEC) systems and delivering meaningful feedback to learners. This paper introduces improvements to automatic grammatical error annotation for Chinese. Our refined framework addresses language-specific challenges that cause common spelling errors in Chinese, including pronunciation similarity, visual shape similarity, specialized participles, and word ordering. In a case study, we demonstrated our system\u2019s ability to provide detailed feedback on 12-16% of all errors by identifying them under our new error typology, specific enough to uncover subtle differences in error patterns between L1 and L2 writings. In addition to improving automated feedback for writers, this work also highlights the value of incorporating language-specific features in NLP systems.",
        "author": "Yang Gu; Zihao Huang; Min Zeng; Mengyang Qiu; Jungyeul Park",
        "authorids": "/y/yang-gu/; /z/zihao-huang/; /m/min-zeng/; /m/mengyang-qiu/; /j/jungyeul-park/",
        "bibtex": "@inproceedings{gu-etal-2025-improving,\n    title = \"Improving Automatic Grammatical Error Annotation for {C}hinese Through Linguistically-Informed Error Typology\",\n    author = \"Gu, Yang  and\n      Huang, Zihao  and\n      Zeng, Min  and\n      Qiu, Mengyang  and\n      Park, Jungyeul\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.189/\",\n    pages = \"2781--2798\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.189.pdf",
        "site": "https://aclanthology.org/2025.coling-main.189/",
        "pdf_size": 837835,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2071423037534218591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Open Writing Evaluation, France; Open Writing Evaluation, France; TEKsystems, Canada; Department of Psychology, Trent University, Canada + Department of Linguistics, The University of British Columbia, Canada; Department of Linguistics, The University of British Columbia, Canada",
        "aff_domain": "; ; ;trentu.ca;ubc.ca",
        "email": "; ; ;trentu.ca;ubc.ca",
        "github": "",
        "project": "http://open-writing-evaluation.github.io",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2+3;3",
        "aff_unique_norm": "Open Writing Evaluation;TEKsystems;Trent University;The University of British Columbia",
        "aff_unique_dep": ";;Department of Psychology;Department of Linguistics",
        "aff_unique_url": ";https://www.teksystems.com;https://www.trentu.ca;https://www.ubc.ca",
        "aff_unique_abbr": ";;Trent;UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1+1;1",
        "aff_country_unique": "France;Canada"
    },
    {
        "id": "2025.coling-main.108",
        "title": "Improving Explainable Fact-Checking with Claim-Evidence Correlations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic fact-checking systems that employ large language models (LLMs) have achieved human-level performance in combating widespread misinformation. However, current LLM-based fact-checking systems fail to reveal the reasoning principles behind their decision-making for the claim verdict. In this work, we propose Correlation-Enhanced Explainable Fact-Checking (CorXFact), an LLM-based fact-checking system that simulates the reasoning principle of human fact-checkers for evidence-based claim verification: assessing and weighing the correlations between the claim and each piece of evidence. Following this principle, CorXFact enables efficient claim verification and transparent explanation generation. Furthermore, we contribute the CorFEVER test set to comprehensively evaluate the CorXFact system in claim-evidence correlation identification and claim verification in both closed-domain and real-world fact-checking scenarios. Experimental results show that our proposed CorXFact significantly outperforms four strong fact-checking baselines in claim authenticity prediction and verdict explanation.",
        "author": "Xin Tan; Bowei Zou; Ai Ti Aw",
        "authorids": "/x/xin-tan/; /b/bowei-zou/; /a/aiti-aw/",
        "bibtex": "@inproceedings{tan-etal-2025-improving,\n    title = \"Improving Explainable Fact-Checking with Claim-Evidence Correlations\",\n    author = \"Tan, Xin  and\n      Zou, Bowei  and\n      Aw, Ai Ti\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.108/\",\n    pages = \"1600--1612\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.108.pdf",
        "site": "https://aclanthology.org/2025.coling-main.108/",
        "pdf_size": 807648,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8v34DEu87vAJ:scholar.google.com/&scioq=Improving+Explainable+Fact-Checking+with+Claim-Evidence+Correlations&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore",
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute for Infocomm Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2025.coling-main.241",
        "title": "Improving Multilingual Sign Language Translation with Automatically Clustered Language Family Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Sign Language Translation (SLT) bridges the communication gap between deaf and hearing individuals by converting sign language videos into spoken language texts. While most SLT research has focused on bilingual translation models, the recent surge in interest has led to the exploration of Multilingual Sign Language Translation (MSLT). However, MSLT presents unique challenges due to the diversity of sign languages across nations. This diversity can lead to cross-linguistic conflicts and hinder translation accuracy. To use the similarity of actions and semantics between sign languages to alleviate conflict, we propose a novel approach that leverages sign language families to improve MSLT performance. Sign languages were clustered into families automatically based on their Language distribution in the MSLT network. We compare the results of our proposed family clustering method with the analysis conducted by sign language linguists and then train dedicated translation models for each family in the many-to-one translation scenario. Our experiments on the SP-10 dataset demonstrate that our approach can achieve a balance between translation accuracy and computational cost by regulating the number of language families.",
        "author": "Ruiquan Zhang; Cong Hu; Pei Yu; Yidong Chen",
        "authorids": "/r/ruiquan-zhang/; /c/cong-hu/; /p/pei-yu/; /y/yidong-chen/",
        "bibtex": "@inproceedings{zhang-etal-2025-improving,\n    title = \"Improving Multilingual Sign Language Translation with Automatically Clustered Language Family Information\",\n    author = \"Zhang, Ruiquan  and\n      Hu, Cong  and\n      Yu, Pei  and\n      Chen, Yidong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.241/\",\n    pages = \"3579--3588\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.241.pdf",
        "site": "https://aclanthology.org/2025.coling-main.241/",
        "pdf_size": 3005947,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12999264669105303284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "1Department of Artificial Intelligence, School of Informatics, Xiamen University, 361005, P.R. China+2Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, 361005, P.R. China+3National Language Resources Monitoring and Research Center for Education and Teaching Media, Xiamen University, 361005, P.R. China+4Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China; 1Department of Artificial Intelligence, School of Informatics, Xiamen University, 361005, P.R. China+2Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, 361005, P.R. China+3National Language Resources Monitoring and Research Center for Education and Teaching Media, Xiamen University, 361005, P.R. China+4Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China; 1Department of Artificial Intelligence, School of Informatics, Xiamen University, 361005, P.R. China+2Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, 361005, P.R. China+3National Language Resources Monitoring and Research Center for Education and Teaching Media, Xiamen University, 361005, P.R. China+4Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China; 1Department of Artificial Intelligence, School of Informatics, Xiamen University, 361005, P.R. China+2Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, 361005, P.R. China+3National Language Resources Monitoring and Research Center for Education and Teaching Media, Xiamen University, 361005, P.R. China+4Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China",
        "aff_domain": "stu.xmu.edu.cn;stu.xmu.edu.cn;stu.xmu.edu.cn;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;stu.xmu.edu.cn;stu.xmu.edu.cn;xmu.edu.cn",
        "github": "",
        "project": "/gtbFamilyCluST",
        "author_num": 4,
        "aff_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0",
        "aff_unique_norm": "Xiamen University",
        "aff_unique_dep": "Department of Artificial Intelligence, School of Informatics",
        "aff_unique_url": "https://www.xmu.edu.cn",
        "aff_unique_abbr": "XMU",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.545",
        "title": "Improving NMT Models by Retrofitting Quality Estimators into Trainable Energy Loss",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Reinforcement learning has shown great promise in aligning language models with human preferences in a variety of text generation tasks, including machine translation. For translation tasks, rewards can easily be obtained from quality estimation (QE) models which can generate rewards for unlabeled data. Despite its usefulness, reinforcement learning cannot exploit the gradients with respect to the QE score. We propose QE-EBM, a method of employing quality estimators as trainable loss networks that can directly backpropagate to the NMT model. We examine our method on several low and high resource target languages with English as the source language. QE-EBM outperforms strong baselines such as REINFORCE and proximal policy optimization (PPO) as well as supervised fine-tuning for all target languages, especially low-resource target languages. Most notably, for English-to-Mongolian translation, our method achieves improvements of 2.5 BLEU, 7.1 COMET-KIWI, 5.3 COMET, and 6.4 XCOMET relative to the supervised baseline.",
        "author": "Gahyun Yoo; Jay-Yoon Lee",
        "authorids": "/g/gahyun-yoo/; /j/jay-yoon-lee/",
        "bibtex": "@inproceedings{yoo-lee-2025-improving,\n    title = \"Improving {NMT} Models by Retrofitting Quality Estimators into Trainable Energy Loss\",\n    author = \"Yoo, Gahyun  and\n      Lee, Jay-Yoon\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.545/\",\n    pages = \"8184--8196\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.545.pdf",
        "site": "https://aclanthology.org/2025.coling-main.545/",
        "pdf_size": 386299,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nH_oPk2BQlYJ:scholar.google.com/&scioq=Improving+NMT+Models+by+Retrofitting+Quality+Estimators+into+Trainable+Energy+Loss&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Seoul National University; Graduate School of Data Science, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.473",
        "title": "Improving Relation Extraction by Sequence-to-sequence-based Dependency Parsing Pre-training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Relation extraction is a crucial natural language processing task that extracts relational triplets from raw text. Syntactic dependencies information has shown its effectiveness for relation extraction tasks. However, in most existing studies, dependency information is used only for traditional encoder-only-based relation extraction, not for generative sequence-to-sequence (seq2seq)-based relation extraction. In this study, we propose a syntax-aware seq2seq pre-trained model for seq2seq-based relation extraction. The model incorporates dependency information into a seq2seq pre-trained language model by continual pre-training with a seq2seq-based dependency parsing task. Experimental results on two widely used relation extraction benchmark datasets show that dependency parsing pre-training can improve the relation extraction performance.",
        "author": "Masaki Asada; Makoto Miwa",
        "authorids": "/m/masaki-asada/; /m/makoto-miwa/",
        "bibtex": "@inproceedings{asada-miwa-2025-improving,\n    title = \"Improving Relation Extraction by Sequence-to-sequence-based Dependency Parsing Pre-training\",\n    author = \"Asada, Masaki  and\n      Miwa, Makoto\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.473/\",\n    pages = \"7099--7105\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.473.pdf",
        "site": "https://aclanthology.org/2025.coling-main.473/",
        "pdf_size": 415114,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:g9cWp8KNmBQJ:scholar.google.com/&scioq=Improving+Relation+Extraction+by+Sequence-to-sequence-based+Dependency+Parsing+Pre-training&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Japan; Toyota Technological Institute, Japan",
        "aff_domain": "aist.go.jp;toyota-ti.ac.jp",
        "email": "aist.go.jp;toyota-ti.ac.jp",
        "github": "https://github.com/aistairc/DepParsingRE",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;Toyota Technological Institute",
        "aff_unique_dep": "Artificial Intelligence Research Center;",
        "aff_unique_url": "https://www.aist.go.jp;https://www.tti-jp.ac.jp",
        "aff_unique_abbr": "AIST;TTI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-industry.3",
        "title": "Improving Tool Retrieval by Leveraging Large Language Models for Query Generation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Using tools by Large Language Models (LLMs) is a promising avenue to extend their reach beyond language or conversational settings. The number of tools can scale to thousands as they enable accessing sensory information, fetching updated factual knowledge, or taking actions in the real world. In such settings, in-context learning by providing a short list of relevant tools in the prompt is a viable approach. To retrieve relevant tools, various approaches have been suggested, ranging from simple frequency-based matching to dense embedding-based semantic retrieval. However, such approaches lack the contextual and common-sense understanding required to retrieve the right tools for complex user requests. Rather than increasing the complexity of the retrieval component itself, we propose leveraging LLM understanding to generate a retrieval query. Then, the generated query is embedded and used to find the most relevant tools via a nearest-neighbor search. We investigate three approaches for query generation: zero-shot prompting, supervised fine-tuning on tool descriptions, and alignment learning by iteratively optimizing a reward metric measuring retrieval performance. By conducting extensive experiments on a dataset covering complex and multi-tool scenarios, we show that leveraging LLMs for query generation improves the retrieval for in-domain (seen tools) and out-of-domain (unseen tools) settings.",
        "author": "Mohammad Kachuee; Sarthak Ahuja; Vaibhav Kumar; Puyang Xu; Xiaohu Liu",
        "authorids": "/m/mohammad-kachuee/; /s/sarthak-ahuja/; /v/vaibhav-kumar/; /p/puyang-xu/; /x/xiaohu-liu/",
        "bibtex": "@inproceedings{kachuee-etal-2025-improving,\n    title = \"Improving Tool Retrieval by Leveraging Large Language Models for Query Generation\",\n    author = \"Kachuee, Mohammad  and\n      Ahuja, Sarthak  and\n      Kumar, Vaibhav  and\n      Xu, Puyang  and\n      Liu, Xiaohu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.3/\",\n    pages = \"29--38\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.3.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.3/",
        "pdf_size": 954118,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16735896532465384956&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Alexa AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.343",
        "title": "Improving the Efficiency of Visually Augmented Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, i.e. they do not know much about the visual world and its properties. To augment LMs with visual knowledge, existing solutions often rely on explicit images, requiring time-consuming retrieval or image generation systems. This paper shows that explicit images are not necessary to visually augment an LM. Instead, we use visually-grounded text representations obtained from the well-known CLIP multimodal system. For a fair comparison, we modify VALM, a visually-augmented LM which uses image retrieval and representation, to work directly with visually-grounded text representations. We name this new model BLIND-VALM. We show that BLIND-VALM performs on par with VALM for Visual Language Understanding (VLU), Natural Language Understanding (NLU) and Language Modeling tasks, despite being significantly more efficient and simpler. We also show that scaling up our model within the compute budget of VALM, either increasing the model or pre-training corpus size, we outperform VALM for all the evaluation tasks.",
        "author": "Paula Ontalvilla; Aitor Ormazabal; Gorka Azkune",
        "authorids": "/p/paula-ontalvilla/; /a/aitor-ormazabal/; /g/gorka-azkune/",
        "bibtex": "@inproceedings{ontalvilla-etal-2025-improving,\n    title = \"Improving the Efficiency of Visually Augmented Language Models\",\n    author = \"Ontalvilla, Paula  and\n      Ormazabal, Aitor  and\n      Azkune, Gorka\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.343/\",\n    pages = \"5115--5122\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.343.pdf",
        "site": "https://aclanthology.org/2025.coling-main.343/",
        "pdf_size": 833709,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:y0MzU3BkBBMJ:scholar.google.com/&scioq=Improving+the+Efficiency+of+Visually+Augmented+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "HiTZ Center - Ixa, University of the Basque Country (UPV/EHU); HiTZ Center - Ixa, University of the Basque Country (UPV/EHU); HiTZ Center - Ixa, University of the Basque Country (UPV/EHU)",
        "aff_domain": "ehu.eus;ehu.eus;ehu.eus",
        "email": "ehu.eus;ehu.eus;ehu.eus",
        "github": "https://github.com/paulaonta/Blind-VaLM",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of the Basque Country",
        "aff_unique_dep": "HiTZ Center - Ixa",
        "aff_unique_url": "https://www.ehu.eus/en",
        "aff_unique_abbr": "UPV/EHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-main.692",
        "title": "In-Context Reinforcement Learning with Retrieval-Augmented Generation for Text-to-SQL",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text-to-SQL simplifies database interactions by enabling non-experts to convert their natural language (NL) questions to Structured Query Language (SQL) queries. With advancements in Large Language Models (LLM), in-context learning (ICL) has emerged as a popular choice for building Text-to-SQL systems. Real world, industry-scale databases, often comprise thousands of tables and hundreds of columns, and makes passing the entire schema as context to an LLM infeasibly expensive. This requisites access to the correct database and the set of tables. Recently Retrieval Augmented Generation (RAG) based methods have been proposed for retrieving relevant subset of databases and tables for a given query. However, we observe that the existing methods of synthetic query generation can generate predominantly simple queries which might not be sufficiently representative of complex, real world queries, thus, negatively affecting the quality of the generated SQL. To address this, we propose an innovative in-context reinforcement learning (ICRL) based framework which refines the question generation process by enhancing the model\u2019s ability to produce intricate queries that practitioners may pose during inference. In contrast to the existing approaches, our framework ensures the generation of synthetic SQL queries which are diverse and complex. We demonstrate the effectiveness of our approach via multiple experiments comparing against the representative state-of-the-art models on public benchmark datasets and observe substantial improvements in performance and scalability. Our method achieves 15-20% higher recall in database/table retrieval task compared to the existing state-of-the-art models for schema identification and upto 2% higher execution accuracy for SQL generation.",
        "author": "Rishit Toteja; Arindam Sarkar; Prakash Mandayam Comar",
        "authorids": "/r/rishit-toteja/; /a/arindam-sarkar/; /p/prakash-mandayam-comar/",
        "bibtex": "@inproceedings{toteja-etal-2025-context,\n    title = \"In-Context Reinforcement Learning with Retrieval-Augmented Generation for Text-to-{SQL}\",\n    author = \"Toteja, Rishit  and\n      Sarkar, Arindam  and\n      Comar, Prakash Mandayam\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.692/\",\n    pages = \"10390--10397\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.692.pdf",
        "site": "https://aclanthology.org/2025.coling-main.692/",
        "pdf_size": 1412714,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xDsdtmCYpIIJ:scholar.google.com/&scioq=In-Context+Reinforcement+Learning+with+Retrieval-Augmented+Generation+for+Text-to-SQL&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon.com, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.487",
        "title": "In-context Continual Learning Assisted by an External Continual Learner",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing continual learning (CL) methods mainly rely on fine-tuning or adapting large language models (LLMs). They still suffer from catastrophic forgetting (CF). Little work has been done to exploit in-context learning (ICL) to leverage the extensive knowledge within LLMs for CL without updating any parameters. However, incrementally learning each new task in ICL necessitates adding training examples from each class of the task to the prompt, which hampers scalability as the prompt length increases. This issue not only leads to excessively long prompts that exceed the input token limit of the underlying LLM but also degrades the model\u2019s performance due to the overextended context. To address this, we introduce InCA, a novel approach that integrates an external continual learner (ECL) with ICL to enable scalable CL without CF. The ECL is built incrementally to pre-select a small subset of likely classes for each test instance. By restricting the ICL prompt to only these selected classes, InCA prevents prompt lengths from becoming excessively long, while maintaining high performance. Experimental results demonstrate that InCA significantly outperforms existing CL baselines, achieving substantial performance gains.",
        "author": "Saleh Momeni; Sahisnu Mazumder; Zixuan Ke; Bing Liu",
        "authorids": "/s/saleh-momeni/; /s/sahisnu-mazumder/; /z/zixuan-ke/; /b/bing-liu/",
        "bibtex": "@inproceedings{momeni-etal-2025-context,\n    title = \"In-context Continual Learning Assisted by an External Continual Learner\",\n    author = \"Momeni, Saleh  and\n      Mazumder, Sahisnu  and\n      Ke, Zixuan  and\n      Liu, Bing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.487/\",\n    pages = \"7292--7306\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.487.pdf",
        "site": "https://aclanthology.org/2025.coling-main.487/",
        "pdf_size": 856144,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8-oHWmS9L8YJ:scholar.google.com/&scioq=In-context+Continual+Learning+Assisted+by+an+External+Continual+Learner&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois Chicago, USA; Intel Labs, USA; Salesforce AI Research, USA; Department of Computer Science, University of Illinois Chicago, USA",
        "aff_domain": "uic.edu;gmail.com;salesforce.com;uic.edu",
        "email": "uic.edu;gmail.com;salesforce.com;uic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Illinois Chicago;Intel Labs;Salesforce AI Research",
        "aff_unique_dep": "Department of Computer Science;;AI Research",
        "aff_unique_url": "https://www.uic.edu;https://www.intel.com/research;https://research.salesforce.com",
        "aff_unique_abbr": "UIC;Intel;Salesforce AI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.527",
        "title": "Incorporating Review-missing Interactions for Generative Explainable Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Explainable recommendation has attracted much attention from the academic and industry communities. Traditional models usually leverage user reviews as ground truths for model training, and the interactions without reviews are totally ignored. However, in practice, a large amount of users may not leave reviews after purchasing items. In this paper, we argue that the interactions without reviews may also contain comprehensive user preferences, and incorporating them to build explainable recommender model may further improve the explanation quality. To follow such intuition, we first leverage generative models to predict the missing reviews, and then train the recommender model based on all the predicted and original reviews. In specific, since the reviews are discrete tokens, we regard the review generation process as a reinforcement learning problem, where each token is an action at one step. We hope that the generated reviews are indistinguishable with the real ones. Thus, we introduce an discriminator as a reward model to evaluate the quality of the generated reviews. At last, to smooth the review generation process, we introduce a self-paced learning strategy to first generate shorter reviews and then predict the longer ones. We conduct extensive experiments on three publicly available datasets to demonstrate the effectiveness of our model.",
        "author": "Xi Li; Xiaohe Bo; Chen Ma; Xu Chen",
        "authorids": "/x/xi-li/; /x/xiaohe-bo/; /c/chen-ma/; /x/xu-chen/",
        "bibtex": "@inproceedings{li-etal-2025-incorporating,\n    title = \"Incorporating Review-missing Interactions for Generative Explainable Recommendation\",\n    author = \"Li, Xi  and\n      Bo, Xiaohe  and\n      Ma, Chen  and\n      Chen, Xu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.527/\",\n    pages = \"7870--7880\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.527.pdf",
        "site": "https://aclanthology.org/2025.coling-main.527/",
        "pdf_size": 1310338,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:p-8YGXICOeAJ:scholar.google.com/&scioq=Incorporating+Review-missing+Interactions+for+Generative+Explainable+Recommendation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Renmin University of China",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence",
        "aff_unique_url": "http://www.ruc.edu.cn",
        "aff_unique_abbr": "RUC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.590",
        "title": "Incremental Transformer: Efficient Encoder for Incremented Text Over MRC and Conversation Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Some encoder inputs such as conversation histories are frequently extended with short additional inputs like new responses. However, to obtain the real-time encoding of the extended input, existing Transformer-based encoders like BERT have to encode the whole extended input again without utilizing the existing encoding of the original input, which may be prohibitively slow for real-time applications. In this paper, we introduce Incremental Transformer, an efficient encoder dedicated for faster encoding of incremented input. It takes only added input as input but attends to cached representations of original input in lower layers for better performance. By treating questions as additional inputs of a passage, Incremental Transformer can also be applied to accelerate MRC tasks. Experimental results show tiny decline in effectiveness but significant speedup against traditional full encoder across various MRC and multi-turn conversational question answering tasks. With the help from simple distillation-like auxiliary losses, Incremental Transformer achieves a speedup of 6.2x, with a mere 2.2 point accuracy reduction in comparison to RoBERTa-Large on SQuADV1.1.",
        "author": "Weisheng Li; Yuechen Wang; Jiaxin Shi; Wengang Zhou; Qi Tian; Houqiang Li",
        "authorids": "/w/weisheng-li/; /y/yuechen-wang/; /j/jiaxin-shi/; /w/wengang-zhou/; /q/qi-tian/; /h/houqiang-li/",
        "bibtex": "@inproceedings{li-etal-2025-incremental,\n    title = \"Incremental Transformer: Efficient Encoder for Incremented Text Over {MRC} and Conversation Tasks\",\n    author = \"Li, Weisheng  and\n      Wang, Yuechen  and\n      Shi, Jiaxin  and\n      Zhou, Wengang  and\n      Tian, Qi  and\n      Li, Houqiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.590/\",\n    pages = \"8819--8829\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.590.pdf",
        "site": "https://aclanthology.org/2025.coling-main.590/",
        "pdf_size": 1836107,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BNz8YvIu94gJ:scholar.google.com/&scioq=Incremental+Transformer:+Efficient+Encoder+for+Incremented+Text+Over+MRC+and+Conversation+Tasks&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China1; University of Science and Technology of China1; Huawei Cloud Computing Technologies Co., Ltd.3; University of Science and Technology of China1+Institute of Artificial Intelligence, Hefei Comprehensive National Science Center2; University of Science and Technology of China1+Institute of Artificial Intelligence, Hefei Comprehensive National Science Center2; Huawei Cloud Computing Technologies Co., Ltd.3",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;gmail.com;ustc.edu.cn;ustc.edu.cn;huawei.com",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;gmail.com;ustc.edu.cn;ustc.edu.cn;huawei.com",
        "github": "https://github.com/li1117heex/IncrementalTransformer",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0+2;0+2;1",
        "aff_unique_norm": "University of Science and Technology of China;Huawei Cloud Computing Technologies Co., Ltd.;Hefei Comprehensive National Science Center",
        "aff_unique_dep": ";;Institute of Artificial Intelligence",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.huawei.com/en/cloud;",
        "aff_unique_abbr": "USTC;Huawei Cloud;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hefei",
        "aff_country_unique_index": "0;0;0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.565",
        "title": "Indigenous Languages Spoken in Argentina: A Survey of NLP and Speech Resources",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Argentina has a diverse, yet little-known, Indigenous language heritage. Most of these languages are at risk of disappearing, resulting in a significant loss of world heritage and cultural knowledge. Currently, no unified information on speakers and computational tools is available for these languages. In this work, we present a systematization of the Indigenous languages spoken in Argentina, along with national demographic data on the country\u2019s Indigenous population. The languages are classified into seven families: Mapuche, Tup\u00ed-Guaran\u00ed, Guaycur\u00fa, Quechua, Mataco-Mataguaya, Aymara, and Chon. We also provide an introductory survey of the computational resources available for these languages, whether or not they are specifically developed for Argentine varieties.",
        "author": "Belu Ticona; Fernando Mart\u00edn Carranza; Viviana Cotik",
        "authorids": "/b/belu-ticona/; /f/fernando-martin-carranza/; /v/viviana-cotik/",
        "bibtex": "@inproceedings{ticona-etal-2025-indigenous,\n    title = \"Indigenous Languages Spoken in {A}rgentina: A Survey of {NLP} and Speech Resources\",\n    author = \"Ticona, Belu  and\n      Carranza, Fernando Mart{\\'i}n  and\n      Cotik, Viviana\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.565/\",\n    pages = \"8449--8461\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.565.pdf",
        "site": "https://aclanthology.org/2025.coling-main.565/",
        "pdf_size": 15308460,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sH4VfcMyN6gJ:scholar.google.com/&scioq=Indigenous+Languages+Spoken+in+Argentina:+A+Survey+of+NLP+and+Speech+Resources&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "George Mason University, United States + Departamento de Computaci\u00f3n, FCEyN, Universidad de Buenos Aires (UBA), Argentina; Departamento de Letras, FFyL, UBA, Argentina + Instituto de Filolog\u00eda y Literaturas Hisp\u00e1nicas \u201cDr. Amado Alonso\u201d, UBA, Argentina; Departamento de Computaci\u00f3n, FCEyN, Universidad de Buenos Aires (UBA), Argentina + Instituto de Investigaci\u00f3n en Ciencias de la Computaci\u00f3n (ICC), CONICET-UBA, Argentina",
        "aff_domain": "dc.uba.ar; ; ",
        "email": "dc.uba.ar; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1+1;1+2",
        "aff_unique_norm": "George Mason University;Universidad de Buenos Aires;Instituto de Investigaci\u00f3n en Ciencias de la Computaci\u00f3n",
        "aff_unique_dep": ";Departamento de Computaci\u00f3n;CONICET-UBA",
        "aff_unique_url": "https://www.gmu.edu;https://www.uba.ar;",
        "aff_unique_abbr": "GMU;UBA;ICC",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1+1;1+1",
        "aff_country_unique": "United States;Argentina"
    },
    {
        "id": "2025.coling-main.595",
        "title": "Inductive Link Prediction in N-ary Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "N-ary Knowledge Graphs (NKGs), where a fact can involve more than two entities, have gained increasing attention. Link Prediction in NKGs (LPN) aims to predict missing elements in facts to facilitate the completion of NKGs. Current LPN methods implicitly operate under a closed-world assumption, meaning that the sets of entities and roles are fixed. These methods focus on predicting missing elements within facts composed of entities and roles seen during training. However, in reality, new facts involving unseen entities and roles frequently emerge, requiring completing these facts. Thus, this paper proposes a new task, Inductive Link Prediction in NKGs (ILPN), which aims to predict missing elements in facts involving unseen entities and roles in emerging NKGs. To address this task, we propose a Meta-learning-based N-ary knowledge Inductive Reasoner (MetaNIR), which employs a graph neural network with meta-learning mechanisms to embed unseen entities and roles adaptively. The obtained embeddings are used to predict missing elements in facts involving unseen elements. Since no existing dataset supports this task, three datasets are constructed to evaluate the effectiveness of MetaNIR. Extensive experimental results demonstrate that MetaNIR consistently outperforms representative models across all datasets.",
        "author": "Jiyao Wei; Saiping Guan; Xiaolong Jin; Jiafeng Guo; Xueqi Cheng",
        "authorids": "/j/jiyao-wei/; /s/saiping-guan/; /x/xiaolong-jin/; /j/jiafeng-guo/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{wei-etal-2025-inductive,\n    title = \"Inductive Link Prediction in N-ary Knowledge Graphs\",\n    author = \"Wei, Jiyao  and\n      Guan, Saiping  and\n      Jin, Xiaolong  and\n      Guo, Jiafeng  and\n      Cheng, Xueqi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.595/\",\n    pages = \"8885--8896\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.595.pdf",
        "site": "https://aclanthology.org/2025.coling-main.595/",
        "pdf_size": 3511278,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:7bkYpzm5RQcJ:scholar.google.com/&scioq=Inductive+Link+Prediction+in+N-ary+Knowledge+Graphs&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.553",
        "title": "Inside-Outside Algorithm for Probabilistic Product-Free Lambek Categorial Grammar",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The inside-outside algorithm is widely utilized in statistical models related to context-free grammars. It plays a key role in the EM estimation of probabilistic context-free grammars. In this work, we introduce an inside-outside algorithm for Probabilistic Lambek Categorical Grammar (PLCG)",
        "author": "Jinman Zhao; Gerald Penn",
        "authorids": "/j/jinman-zhao/; /g/gerald-penn/",
        "bibtex": "@inproceedings{zhao-penn-2025-inside,\n    title = \"Inside-Outside Algorithm for Probabilistic Product-Free {L}ambek Categorial Grammar\",\n    author = \"Zhao, Jinman  and\n      Penn, Gerald\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.553/\",\n    pages = \"8295--8303\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.553.pdf",
        "site": "https://aclanthology.org/2025.coling-main.553/",
        "pdf_size": 512546,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3355554645183985964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Dept. of Computer Science, University of Toronto; Dept. of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "https://github.com/zhaojinm/Inside-outside-plcgon",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-main.9",
        "title": "InstructGEC: Enhancing Unsupervised Grammatical Error Correction with Instruction Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent works have proposed methods of generating synthetic data automatically for unsupervised Grammatical Error Correction (GEC). Although a large amount of synthetic data is generated at a low cost, it is unrealistic and of poor quality. The copying phenomenon of synthetic data prevents GEC models from learning the semantic knowledge of contextual language. In this paper, we design an instruction format and use the masking strategy in both an erroneous sentence and the corresponding instruction consistently to alleviate the impact of the copy phenomenon. We also propose a novel approach, InstructGEC, which integrates the knowledge of grammatical detection into GEC models with instruction tuning to address the low-quality issue. Experiments are conducted on English and Chinese GEC datasets and results demonstrate that our method outperforms state-of-the-art unsupervised GEC methods.",
        "author": "Jiayi Deng; Chen Chen; Chunyan Hou; Xiaojie Yuan",
        "authorids": "/j/jiayi-deng/; /c/chen-chen/; /c/chunyan-hou/; /x/xiaojie-yuan/",
        "bibtex": "@inproceedings{deng-etal-2025-instructgec,\n    title = \"{I}nstruct{GEC}: Enhancing Unsupervised Grammatical Error Correction with Instruction Tuning\",\n    author = \"Deng, Jiayi  and\n      Chen, Chen  and\n      Hou, Chunyan  and\n      Yuan, Xiaojie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.9/\",\n    pages = \"110--122\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.9.pdf",
        "site": "https://aclanthology.org/2025.coling-main.9/",
        "pdf_size": 461613,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BNWt9ZH35eEJ:scholar.google.com/&scioq=InstructGEC:+Enhancing+Unsupervised+Grammatical+Error+Correction+with+Instruction+Tuning&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "College of Computer Science, Nankai University, Tianjin, China+MoE Key Lab of DISSec, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China+MoE Key Lab of DISSec, Nankai University, Tianjin, China; School of CSE, Tianjin University of Technology, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China+MoE Key Lab of DISSec, Nankai University, Tianjin, China",
        "aff_domain": "mail.nankai.edu.cn;nankai.edu.cn;tjut.edu.cn;nankai.edu.cn",
        "email": "mail.nankai.edu.cn;nankai.edu.cn;tjut.edu.cn;nankai.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;1;0+0",
        "aff_unique_norm": "Nankai University;Tianjin University of Technology",
        "aff_unique_dep": "College of Computer Science;School of CSE",
        "aff_unique_url": "http://www.nankai.edu.cn;",
        "aff_unique_abbr": "Nankai;",
        "aff_campus_unique_index": "0+0;0+0;0;0+0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.25",
        "title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialists, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.",
        "author": "He Cao; Zijing Liu; Xingyu Lu; Yuan Yao; Yu Li",
        "authorids": "/h/he-cao/; /z/zijing-liu/; /x/xingyu-lu/; /y/yuan-yao/; /y/yu-li/",
        "bibtex": "@inproceedings{cao-etal-2025-instructmol,\n    title = \"{I}nstruct{M}ol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery\",\n    author = \"Cao, He  and\n      Liu, Zijing  and\n      Lu, Xingyu  and\n      Yao, Yuan  and\n      Li, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.25/\",\n    pages = \"354--379\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.25.pdf",
        "site": "https://aclanthology.org/2025.coling-main.25/",
        "pdf_size": 1817080,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=833278151523708877&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "1International Digital Economy Academy (IDEA) + 2Hong Kong University of Science and Technology; 1International Digital Economy Academy (IDEA); 1International Digital Economy Academy (IDEA) + 3Tsinghua Shenzhen International Graduate School, Tsinghua University; 2Hong Kong University of Science and Technology; 1International Digital Economy Academy (IDEA)",
        "aff_domain": "connect.ust.hk;idea.edu.cn;mails.tsinghua.edu.cn;ust.hk;idea.edu.cn",
        "email": "connect.ust.hk;idea.edu.cn;mails.tsinghua.edu.cn;ust.hk;idea.edu.cn",
        "github": "https://github.com/IDEA-XL/InstructMol",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0+2;1;0",
        "aff_unique_norm": "International Digital Economy Academy;Hong Kong University of Science and Technology;Tsinghua University",
        "aff_unique_dep": ";;International Graduate School",
        "aff_unique_url": ";https://www.ust.hk;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "IDEA;HKUST;THU",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2025.coling-main.153",
        "title": "Integrating Group-based Preferences from Coarse to Fine for Cold-start Users Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent studies have demonstrated that cross-domain recommendation (CDR) effectively addresses the cold-start problem. Most approaches rely on transfer functions to generate user representations from the source to the target domain. Although these methods substantially enhance recommendation performance, they exhibit certain limitations, notably the frequent oversight of similarities in user preferences, which can offer critical insights for training transfer functions. Moreover, existing methods typically derive user preferences from historical purchase records or reviews, without considering that preferences operate at three distinct levels: category, brand, and aspect, each influencing decision-making differently. This paper proposes a model that integrates the preferences from coarse to fine levels to improve recommendations for cold-start users. The model leverages historical data from the source domain and external memory networks to generate user representations across different preference levels. A meta-network then transfers these representations to the target domain, where user-item ratings are predicted by aggregating the diverse representations. Experimental results demonstrate that our model outperforms state-of-the-art approaches in addressing the cold-start problem on three CDR tasks.",
        "author": "Siyu Wang; Jianhui Jiang; Jiangtao Qiu; Shengran Dai",
        "authorids": "/s/siyu-wang/; /j/jianhui-jiang/; /j/jiangtao-qiu/; /s/shengran-dai/",
        "bibtex": "@inproceedings{wang-etal-2025-integrating,\n    title = \"Integrating Group-based Preferences from Coarse to Fine for Cold-start Users Recommendation\",\n    author = \"Wang, Siyu  and\n      Jiang, Jianhui  and\n      Qiu, Jiangtao  and\n      Dai, Shengran\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.153/\",\n    pages = \"2236--2245\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.153.pdf",
        "site": "https://aclanthology.org/2025.coling-main.153/",
        "pdf_size": 4504019,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:OEF9LNRVdZMJ:scholar.google.com/&scioq=Integrating+Group-based+Preferences+from+Coarse+to+Fine+for+Cold-start+Users+Recommendation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Gusu Laboratory of Materials, Suzhou, China; Gusu Laboratory of Materials, Suzhou, China; Southwestern University of Finance and Economics, Chengdu, China; Gusu Laboratory of Materials, Suzhou, China",
        "aff_domain": "gusulab.ac.cn;gusulab.ac.cn;swufe.edu.cn;gusulab.ac.cn",
        "email": "gusulab.ac.cn;gusulab.ac.cn;swufe.edu.cn;gusulab.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Gusu Laboratory of Materials;Southwestern University of Finance and Economics",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.swufe.edu.cn",
        "aff_unique_abbr": ";SWUFE",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Suzhou;Chengdu",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.599",
        "title": "Integrating Visual Modalities with Large Language Models for Mental Health Support",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current work of mental health support primarily utilizes unimodal textual data and often fails to understand and respond to users\u2019 emotional states comprehensively. In this study, we introduce a novel framework that enhances Large Language Model (LLM) performance in mental health dialogue systems by integrating multimodal inputs. Our framework uses visual language models to analyze facial expressions and body movements, then combines these visual elements with dialogue context and counseling strategies. This approach allows LLMs to generate more nuanced and supportive responses. The framework comprises four components: in-context learning via computation of semantic similarity; extraction of facial expression descriptions through visual modality data; integration of external knowledge from a knowledge base; and delivery of strategic guidance through a strategy selection module. Both automatic and human evaluations confirm that our approach outperforms existing models, delivering more empathetic, coherent, and contextually relevant mental health support responses.",
        "author": "Zhouan Zhu; Shangfei Wang; Yuxin Wang; Jiaqiang Wu",
        "authorids": "/z/zhouan-zhu/; /s/shangfei-wang/; /y/yuxin-wang/; /j/jiaqiang-wu/",
        "bibtex": "@inproceedings{zhu-etal-2025-integrating,\n    title = \"Integrating Visual Modalities with Large Language Models for Mental Health Support\",\n    author = \"Zhu, Zhouan  and\n      Wang, Shangfei  and\n      Wang, Yuxin  and\n      Wu, Jiaqiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.599/\",\n    pages = \"8939--8954\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.599.pdf",
        "site": "https://aclanthology.org/2025.coling-main.599/",
        "pdf_size": 1418583,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5948075422065057457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.222",
        "title": "Intent Contrastive Learning Based on Multi-view Augmentation for Sequential Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Sequential recommendation systems play a key role in modern information retrieval. However, existing intent-related work fails to adequately capture long-term dependencies in user behavior, i.e., the influence of early user behavior on current behavior, and also fails to effectively utilize item relevance. To this end, we propose a novel sequential recommendation framework to overcome the above limitations, called ICMA. Specifically, we combine temporal variability with position encoding that has extrapolation properties to encode sequences, thereby expanding the model\u2019s view of user behavior and capturing long-term user dependencies more effectively. Additionally, we design a multi-view data augmentation method, i.e., based on random data augmentation methods (e.g., crop, mask, and reorder), and further introduce insertion and substitution operations to augment the sequence data from different views by utilizing item relevance. Within this framework, clustering is performed to learn intent distributions, and these learned intents are integrated into the sequential recommendation model via contrastive SSL, which maximizes consistency between sequence views and their corresponding intents. The training process alternates between the Expectation (E) step and the Maximization (M) step. Experiments on three real datasets show that our approach improves by 0.8% to 14.7% compared to most baselines.",
        "author": "Bo Pei; Yingzheng Zhu; Guangjin Wang; Huajuan Duan; Wenya Wu; Fuyong Xu; Yizhao Zhu; Peiyu Liu; Ran Lu",
        "authorids": "/b/bo-pei/; /y/yingzheng-zhu/; /g/guangjin-wang/; /h/huajuan-duan/; /w/wenya-wu/; /f/fuyong-xu/; /y/yizhao-zhu/; /p/peiyu-liu/; /r/ran-lu/",
        "bibtex": "@inproceedings{pei-etal-2025-intent,\n    title = \"Intent Contrastive Learning Based on Multi-view Augmentation for Sequential Recommendation\",\n    author = \"Pei, Bo  and\n      Zhu, Yingzheng  and\n      Wang, Guangjin  and\n      Duan, Huajuan  and\n      Wu, Wenya  and\n      Xu, Fuyong  and\n      Zhu, Yizhao  and\n      Liu, Peiyu  and\n      Lu, Ran\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.222/\",\n    pages = \"3300--3309\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.222.pdf",
        "site": "https://aclanthology.org/2025.coling-main.222/",
        "pdf_size": 668357,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Xk92GDj287cJ:scholar.google.com/&scioq=Intent+Contrastive+Learning+Based+on+Multi-view+Augmentation+for+Sequential+Recommendation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Science and Engineering, Shandong Normal University; School of Information Science and Engineering, Shandong Normal University; School of Information Science and Engineering, Shandong Normal University; School of Information Engineering, Shandong Management University; School of Information Science and Engineering, Shandong Normal University; School of Information Science and Engineering, Shandong Normal University; School of Information Science and Engineering, Shandong Normal University; School of Information Science and Engineering, Shandong Normal University; School of Information Science and Engineering, Shandong Normal University",
        "aff_domain": "163.com; ; ; ; ; ; ;sdnu.edu.cn;sdnu.edu.cn",
        "email": "163.com; ; ; ; ; ; ;sdnu.edu.cn;sdnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;1;0;0;0;0;0",
        "aff_unique_norm": "Shandong Normal University;Shandong Management University",
        "aff_unique_dep": "School of Information Science and Engineering;School of Information Engineering",
        "aff_unique_url": "http://www.sdu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.199",
        "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aligning large language models (LLMs) with human values, particularly when facing complex and stealthy jailbreak attacks, presents a formidable challenge. Unfortunately, existing methods often overlook this intrinsic nature of jailbreaks, which limits their effectiveness in such complex scenarios. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis (IA). IA works by triggering LLMs\u2019 inherent self-correct and improve ability through a two-stage process: 1) analyzing the essential intention of the user input, and 2) providing final policy-aligned responses based on the first round conversation. Notably,IA is an inference-only method, thus could enhance LLM safety without compromising their helpfulness. Extensive experiments on varying jailbreak benchmarks across a wide range of LLMs show that IA could consistently and significantly reduce the harmfulness in responses (averagely -48.2% attack success rate). Encouragingly, with our IA, Vicuna-7B even outperforms GPT-3.5 regarding attack success rate. We empirically demonstrate that, to some extent, IA is robust to errors in generated intentions. Further analyses reveal the underlying principle of IA: suppressing LLM\u2019s tendency to follow jailbreak prompts, thereby enhancing safety.",
        "author": "Yuqi Zhang; Liang Ding; Lefei Zhang; Dacheng Tao",
        "authorids": "/y/yuqi-zhang/; /l/liang-ding/; /l/lefei-zhang/; /d/dacheng-tao/",
        "bibtex": "@inproceedings{zhang-etal-2025-intention,\n    title = \"Intention Analysis Makes {LLM}s A Good Jailbreak Defender\",\n    author = \"Zhang, Yuqi  and\n      Ding, Liang  and\n      Zhang, Lefei  and\n      Tao, Dacheng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.199/\",\n    pages = \"2947--2968\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.199.pdf",
        "site": "https://aclanthology.org/2025.coling-main.199/",
        "pdf_size": 3333297,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5434811837007316249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Wuhan University; The University of Sydney; School of Computer Science, Wuhan University; College of Computing and Data Science at Nanyang Technological University, Singapore 639798",
        "aff_domain": "whu.edu.cn;gmail.com;whu.edu.cn;gmail.com",
        "email": "whu.edu.cn;gmail.com;whu.edu.cn;gmail.com",
        "github": "https://github.com/alphadl/SafeLLM_with_IntentionAnalysis",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Wuhan University;University of Sydney;Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science;;College of Computing and Data Science",
        "aff_unique_url": "http://www.whu.edu.cn;https://www.sydney.edu.au;https://www.ntu.edu.sg",
        "aff_unique_abbr": "WHU;USYD;NTU",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Wuhan;;Singapore",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "China;Australia;Singapore"
    },
    {
        "id": "2025.coling-main.729",
        "title": "Interaction Matters: An Evaluation Framework for Interactive Dialogue Assessment on English Second Language Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present an evaluation framework for interactive dialogue assessment in the context of English as a Second Language (ESL) speakers. Our framework collects dialogue-level interactivity labels (e.g., topic management; 4 labels in total) and micro-level span features (e.g., backchannels; 17 features in total). Given our annotated data, we study how the micro-level features influence the (higher level) interactivity quality of ESL dialogues by constructing various machine learning-based models. Our results demonstrate that certain micro-level features strongly correlate with interactivity quality, like reference words (e.g., she, her, he), revealing new insights about the interaction between higher-level dialogue quality and lower-level fundamental linguistic signals. Our framework also provides a means to assess ESL communication, which is useful for language assessment.",
        "author": "Rena Gao; Carsten Roever; Jey Han Lau",
        "authorids": "/r/rena-gao/; /c/carsten-roever/; /j/jey-han-lau/",
        "bibtex": "@inproceedings{gao-etal-2025-interaction,\n    title = \"Interaction Matters: An Evaluation Framework for Interactive Dialogue Assessment on {E}nglish Second Language Conversations\",\n    author = \"Gao, Rena  and\n      Roever, Carsten  and\n      Lau, Jey Han\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.729/\",\n    pages = \"10977--11012\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.729.pdf",
        "site": "https://aclanthology.org/2025.coling-main.729/",
        "pdf_size": 928814,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6008192720341406149&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Melbourne, Australia; University of Melbourne, Australia; University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "https://github.com/RenaGao/2024InteractiveMetrics",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Melbourne",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2025.coling-main.325",
        "title": "Interactive Evaluation for Medical LLMs via Task-oriented Dialogue System",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study focuses on evaluating proactive communication and diagnostic capabilities of medical Large Language Models (LLMs), which directly impact their effectiveness in patient consultations. In typical medical scenarios, doctors often ask a set of questions to gain a comprehensive understanding of patients\u2019 conditions. We argue that single-turn question-answering tasks such as MultiMedQA are insufficient for evaluating LLMs\u2019 medical consultation abilities. To address this limitation, we developed an evaluation benchmark called Multi-turn Medical Dialogue Evaluation (MMD-Eval), specifically designed to evaluate the proactive communication and diagnostic capabilities of medical LLMs during consultations. Considering the high cost and potential for hallucinations in LLMs, we innovatively trained a task-oriented dialogue system to simulate patients engaging in dialogues with the medical LLMs using our structured medical records dataset. This approach enabled us to generate multi-turn dialogue data. Subsequently, we evaluate the communication skills and medical expertise of the medical LLMs. All resources associated with this study will be made publicly available.",
        "author": "Ruoyu Liu; Kui Xue; Xiaofan Zhang; Shaoting Zhang",
        "authorids": "/r/ruoyu-liu/; /k/kui-xue/; /x/xiaofan-zhang/; /s/shaoting-zhang/",
        "bibtex": "@inproceedings{liu-etal-2025-interactive,\n    title = \"Interactive Evaluation for Medical {LLM}s via Task-oriented Dialogue System\",\n    author = \"Liu, Ruoyu  and\n      Xue, Kui  and\n      Zhang, Xiaofan  and\n      Zhang, Shaoting\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.325/\",\n    pages = \"4871--4896\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.325.pdf",
        "site": "https://aclanthology.org/2025.coling-main.325/",
        "pdf_size": 868006,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4422312871824668807&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai Jiao Tong University; Shanghai AI Laboratory; Shanghai Jiao Tong University+Shanghai AI Laboratory; Shanghai AI Laboratory+SenseTime Research",
        "aff_domain": "sjtu.edu.cn;pjlab.org.cn;sjtu.edu.cn;pjlab.org.cn",
        "email": "sjtu.edu.cn;pjlab.org.cn;sjtu.edu.cn;pjlab.org.cn",
        "github": "https://github.com/lry00127/MMD-Eval",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0+1;1+2",
        "aff_unique_norm": "Shanghai Jiao Tong University;Shanghai AI Laboratory;SenseTime",
        "aff_unique_dep": ";;SenseTime Research",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.shanghai-ai-lab.com;https://www.sensetime.com",
        "aff_unique_abbr": "SJTU;SAIL;SenseTime",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.629",
        "title": "InternLM-Law: An Open-Sourced Chinese Legal Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce InternLM-Law, a large language model (LLM) tailored for addressing diverse legal tasks related to Chinese laws. These tasks range from responding to standard legal questions (e.g., legal exercises in textbooks) to analyzing complex real-world legal situations. Our work contributes to Chinese Legal NLP research by (1) conducting one of the most extensive evaluations of state-of-the-art general-purpose and legal-specific LLMs to date that involves an automatic evaluation on the 20 legal NLP tasks in LawBench, a human evaluation on a challenging version of the Legal Consultation task, and an automatic evaluation of a model\u2019s ability to handle very long legal texts; (2) presenting a methodology for training a Chinese legal LLM that offers superior performance to all of its counterparts in our extensive evaluation; and (3) facilitating future research in this area by making all of our code and model publicly available at https://github.com/InternLM/InternLM-Law.",
        "author": "Zhiwei Fei; Songyang Zhang; Xiaoyu Shen; Dawei Zhu; Xiao Wang; Jidong Ge; Vincent Ng",
        "authorids": "/z/zhiwei-fei/; /s/songyang-zhang/; /x/xiaoyu-shen/; /d/dawei-zhu/; /x/xiao-wang/; /j/jidong-ge/; /v/vincent-ng/",
        "bibtex": "@inproceedings{fei-etal-2025-internlm,\n    title = \"{I}ntern{LM}-Law: An Open-Sourced {C}hinese Legal Large Language Model\",\n    author = \"Fei, Zhiwei  and\n      Zhang, Songyang  and\n      Shen, Xiaoyu  and\n      Zhu, Dawei  and\n      Wang, Xiao  and\n      Ge, Jidong  and\n      Ng, Vincent\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.629/\",\n    pages = \"9376--9392\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.629.pdf",
        "site": "https://aclanthology.org/2025.coling-main.629/",
        "pdf_size": 324979,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12724267856635742750&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Nanjing University; Shanghai AI Laboratory; Eastern Institute of Technology; Saarland University; Saarland University; Nanjing University; University of Texas at Dallas",
        "aff_domain": "pjlab.org.cn; ; ; ; ; ; ",
        "email": "pjlab.org.cn; ; ; ; ; ; ",
        "github": "https://github.com/InternLM/InternLM-Law",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;3;0;4",
        "aff_unique_norm": "Nanjing University;Shanghai AI Laboratory;Eastern Institute of Technology;Saarland University;University of Texas at Dallas",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.nju.edu.cn;https://www.shanghai-ai-lab.com;https://www.eit.ac.nz;https://www.uni-saarland.de;https://www.utdallas.edu",
        "aff_unique_abbr": "Nanjing U;SAIL;EIT;UdS;UT Dallas",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0;1;2;2;0;3",
        "aff_country_unique": "China;New Zealand;Germany;United States"
    },
    {
        "id": "2025.coling-main.720",
        "title": "Interpreting Topic Models in Byte-Pair Encoding Space",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Byte-pair encoding (BPE) is pivotal for processing text into chunksize tokens, particularly in Large Language Model (LLM). From a topic modeling perspective, as these chunksize tokens might be mere parts of valid words, evaluating and interpreting these tokens for coherence is challenging. Most, if not all, of coherence evaluation measures are incompatible as they benchmark using valid words. We propose to interpret the recovery of valid words from these tokens as a ranking problem and present a model-agnostic and training-free recovery approach from the topic-token distribution onto a selected vocabulary space, following which we could apply existing evaluation measures. Results show that topic sets recovered from BPE vocabulary space are coherent.",
        "author": "Jia Peng Lim; Hady Lauw",
        "authorids": "/j/jia-peng-lim/; /h/hady-lauw/",
        "bibtex": "@inproceedings{lim-lauw-2025-interpreting,\n    title = \"Interpreting Topic Models in Byte-Pair Encoding Space\",\n    author = \"Lim, Jia Peng  and\n      Lauw, Hady\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.720/\",\n    pages = \"10810--10838\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.720.pdf",
        "site": "https://aclanthology.org/2025.coling-main.720/",
        "pdf_size": 2467108,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ia8BNSTp-CEJ:scholar.google.com/&scioq=Interpreting+Topic+Models+in+Byte-Pair+Encoding+Space&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Singapore Management University; Singapore Management University",
        "aff_domain": "smu.edu.sg;smu.edu.sg",
        "email": "smu.edu.sg;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Singapore Management University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.smu.edu.sg",
        "aff_unique_abbr": "SMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2025.coling-main.709",
        "title": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems",
        "author": "Luyang Lin; Lingzhi Wang; Jinsong Guo; Kam-Fai Wong",
        "authorids": "/l/luyang-lin/; /l/lingzhi-wang/; /j/jinsong-guo/; /k/kam-fai-wong/",
        "bibtex": "@inproceedings{lin-etal-2025-investigating,\n    title = \"Investigating Bias in {LLM}-Based Bias Detection: Disparities between {LLM}s and Human Perception\",\n    author = \"Lin, Luyang  and\n      Wang, Lingzhi  and\n      Guo, Jinsong  and\n      Wong, Kam-Fai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.709/\",\n    pages = \"10634--10649\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.709.pdf",
        "site": "https://aclanthology.org/2025.coling-main.709/",
        "pdf_size": 1656276,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13035441418184208441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Chinese University of Hong Kong, China + MoE Key Laboratory of High Confidence Software Technologies, China; Harbin Institute of Technology, Shenzhen, China; University College London, UK; The Chinese University of Hong Kong, China + MoE Key Laboratory of High Confidence Software Technologies, China",
        "aff_domain": "se.cuhk.edu.hk;se.cuhk.edu.hk;hit.edu.cn;ucl.ac.uk",
        "email": "se.cuhk.edu.hk;se.cuhk.edu.hk;hit.edu.cn;ucl.ac.uk",
        "github": "https://github.com/lylin0/lin2024investigating",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;3;0+1",
        "aff_unique_norm": "The Chinese University of Hong Kong;MoE Key Laboratory of High Confidence Software Technologies;Harbin Institute of Technology;University College London",
        "aff_unique_dep": ";High Confidence Software Technologies;;",
        "aff_unique_url": "https://www.cuhk.edu.hk;;http://en.hhit.edu.cn/;https://www.ucl.ac.uk",
        "aff_unique_abbr": "CUHK;;HIT;UCL",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;0;1;0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2025.coling-main.95",
        "title": "Investigating the Contextualised Word Embedding Dimensions Specified for Contextual and Temporal Semantic Changes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The sense-aware contextualised word embeddings (SCWEs) encode semantic changes of words within the contextualised word embedding (CWE) spaces. Despite the superior performance of (SCWE) in contextual/temporal semantic change detection (SCD) benchmarks, it remains unclear as to how the meaning changes are encoded in the embedding space. To study this, we compare pre-trained CWEs and their fine-tuned versions on contextual and temporal semantic change benchmarks under Principal Component Analysis (PCA) and Independent Component Analysis (ICA) transformations. Our experimental results reveal (a) although there exist a smaller number of axes that are specific to semantic changes of words in the pre-trained CWE space, this information gets distributed across all dimensions when fine-tuned, and (b) in contrast to prior work studying the geometry of CWEs, we find that PCA to better represent semantic changes than ICA within the top 10% of axes. These findings encourage the development of more efficient SCD methods with a small number of SCD-aware dimensions.",
        "author": "Taichi Aida; Danushka Bollegala",
        "authorids": "/t/taichi-aida/; /d/danushka-bollegala/",
        "bibtex": "@inproceedings{aida-bollegala-2025-investigating,\n    title = \"Investigating the Contextualised Word Embedding Dimensions Specified for Contextual and Temporal Semantic Changes\",\n    author = \"Aida, Taichi  and\n      Bollegala, Danushka\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.95/\",\n    pages = \"1413--1437\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.95.pdf",
        "site": "https://aclanthology.org/2025.coling-main.95/",
        "pdf_size": 21044518,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6dq4PRNRCO4J:scholar.google.com/&scioq=Investigating+the+Contextualised+Word+Embedding+Dimensions+Specified+for+Contextual+and+Temporal+Semantic+Changes&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Tokyo Metropolitan University; University of Liverpool",
        "aff_domain": "ed.tmu.ac.jp;liverpool.ac.uk",
        "email": "ed.tmu.ac.jp;liverpool.ac.uk",
        "github": "https://github.com/LivNLP/svp-dims",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Tokyo Metropolitan University;University of Liverpool",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tmuc.ac.jp;https://www.liverpool.ac.uk",
        "aff_unique_abbr": "TMU;Liv Uni",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;United Kingdom"
    },
    {
        "id": "2025.coling-main.250",
        "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs\u2019 awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs\u2019 QA and judgemental capabilities.",
        "author": "Ruiyang Ren; Yuhao Wang; Yingqi Qu; Wayne Xin Zhao; Jing Liu; Hua Wu; Ji-Rong Wen; Haifeng Wang",
        "authorids": "/r/ruiyang-ren/; /y/yuhao-wang/; /y/yingqi-qu/; /w/wayne-xin-zhao/; /j/jing-liu/; /h/hua-wu/; /j/ji-rong-wen/; /h/haifeng-wang/",
        "bibtex": "@inproceedings{ren-etal-2025-investigating,\n    title = \"Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation\",\n    author = \"Ren, Ruiyang  and\n      Wang, Yuhao  and\n      Qu, Yingqi  and\n      Zhao, Wayne Xin  and\n      Liu, Jing  and\n      Wu, Hua  and\n      Wen, Ji-Rong  and\n      Wang, Haifeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.250/\",\n    pages = \"3697--3715\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.250.pdf",
        "site": "https://aclanthology.org/2025.coling-main.250/",
        "pdf_size": 804898,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5272648308786986933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "https://github.com/RUCAIBox/LLM-Knowledge-Boundary",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.249",
        "title": "Investigating the Impact of Incremental Processing and Voice Activity Projection on Spoken Dialogue Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The naturalness of responses in spoken dialogue systems has been significantly improved by the introduction of large language models (LLMs), although many challenges remain until human-like turn-taking can be achieved. A turn-taking model called Voice Activity Projection (VAP) is gaining attention because it can be trained in an unsupervised manner using the spoken dialogue data between two speakers. For such a turn-taking model to be fully effective, systems must initiate response generation as soon as a turn-shift is detected. This can be achieved by incremental response generation, which reduces the delay before the system responds. Incremental response generation is done using partial speech recognition results while user speech is incrementally processed. Combining incremental response generation with VAP-based turn-taking will enable spoken dialogue systems to achieve faster and more natural turn-taking. However, their effectiveness remains unclear because they have not yet been evaluated in real-world systems. In this study, we developed spoken dialogue systems that incorporate incremental response generation and VAP-based turn-taking and evaluated their impact on task success and dialogue satisfaction through user assessments.",
        "author": "Yuya Chiba; Ryuichiro Higashinaka",
        "authorids": "/y/yuya-chiba/; /r/ryuichiro-higashinaka/",
        "bibtex": "@inproceedings{chiba-higashinaka-2025-investigating,\n    title = \"Investigating the Impact of Incremental Processing and Voice Activity Projection on Spoken Dialogue Systems\",\n    author = \"Chiba, Yuya  and\n      Higashinaka, Ryuichiro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.249/\",\n    pages = \"3687--3696\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.249.pdf",
        "site": "https://aclanthology.org/2025.coling-main.249/",
        "pdf_size": 1516391,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bC-935VFhGwJ:scholar.google.com/&scioq=Investigating+the+Impact+of+Incremental+Processing+and+Voice+Activity+Projection+on+Spoken+Dialogue+Systems&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "NTT Communication Science Laboratories, Japan; Graduate School of Informatics, Nagoya University, Japan",
        "aff_domain": "ntt.com;i.nagoya-u.ac.jp",
        "email": "ntt.com;i.nagoya-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NTT Communication Science Laboratories;Nagoya University",
        "aff_unique_dep": ";Graduate School of Informatics",
        "aff_unique_url": "https://www.ntt-csl.com;https://www.nagoya-u.ac.jp",
        "aff_unique_abbr": "NTT CSL;Nagoya U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.286",
        "title": "Is Parameter Collision Hindering Continual Learning in LLMs?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) often suffer from catastrophic forgetting when learning multiple tasks sequentially, making continual learning (CL) essential for their dynamic deployment. Existing state-of-the-art (SOTA) methods, such as O-LoRA, typically focus on constructing orthogonality tasks to decouple parameter interdependence from various domains.In this paper, we reveal that building non-collision parameters is a more critical factor in addressing CL challenges. Our theoretical and experimental analyses demonstrate that non-collision parameters provide better task orthogonality, which is a sufficient but unnecessary condition. Furthermore, knowledge from multiple domains will be preserved in non-collision parameter subspaces, making it more difficult to forget previously seen data. Leveraging this insight, we propose Non-collision Low-Rank Adaptation (N-LoRA), a simple yet effective approach leveraging low collision rates to enhance CL in LLMs. Experimental results on multiple CL benchmarks indicate that N-LoRA achieves superior performance (+2.9%), higher task orthogonality (\u00d74.1times), and lower parameter collision (\u00d758.1times) than SOTA methods.",
        "author": "Shuo Yang; Kun-Peng Ning; Yu-Yang Liu; Jia-Yu Yao; Yong-Hong Tian; Yi-Bing Song; Li Yuan",
        "authorids": "/s/shuo-yang/; /k/kun-peng-ning/; /y/yu-yang-liu/; /j/jia-yu-yao/; /y/yong-hong-tian/; /y/yi-bing-song/; /l/li-yuan/",
        "bibtex": "@inproceedings{yang-etal-2025-parameter,\n    title = \"Is Parameter Collision Hindering Continual Learning in {LLM}s?\",\n    author = \"Yang, Shuo  and\n      Ning, Kun-Peng  and\n      Liu, Yu-Yang  and\n      Yao, Jia-Yu  and\n      Tian, Yong-Hong  and\n      Song, Yi-Bing  and\n      Yuan, Li\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.286/\",\n    pages = \"4243--4259\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.286.pdf",
        "site": "https://aclanthology.org/2025.coling-main.286/",
        "pdf_size": 2669441,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15043186657338439126&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Electronic and Computer Engineering, Peking University+Peng Cheng Laboratory; School of Electronic and Computer Engineering, Peking University+Peng Cheng Laboratory; School of Electronic and Computer Engineering, Peking University; School of Electronic and Computer Engineering, Peking University; School of Electronic and Computer Engineering, Peking University+Peng Cheng Laboratory; DAMO Academy, Alibaba Group+Hupan Lab; School of Electronic and Computer Engineering, Peking University+Peng Cheng Laboratory",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn; ;pku.edu.cn",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn; ;pku.edu.cn",
        "github": "https://github.com/PKU-YuanGroup/N-LoRA",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0;0;0+1;2+3;0+1",
        "aff_unique_norm": "Peking University;Peng Cheng Laboratory;Alibaba Group;Hupan Lab",
        "aff_unique_dep": "School of Electronic and Computer Engineering;;DAMO Academy;",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.pcl.ac.cn;https://www.alibaba-group.com;",
        "aff_unique_abbr": "PKU;PCL;Alibaba;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.242",
        "title": "Is Peer-Reviewing Worth the Effort?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "How effective is peer-reviewing in identifying important papers? We treat this question as a forecasting task. Can we predict which papers will be highly cited in the future based on venue and \u201cearly returns\u201d (citations soon after publication)? We show early returns are more predictive than venue. Finally, we end with a constructive suggestion to simplify reviewing.",
        "author": "Kenneth Ward Church; Raman Chandrasekar; John E. Ortega; Ibrahim Said Ahmad",
        "authorids": "/k/kenneth-church/; /r/raman-chandrasekar/; /j/john-e-ortega/; /i/ibrahim-said-ahmad/",
        "bibtex": "@inproceedings{church-etal-2025-peer,\n    title = \"Is Peer-Reviewing Worth the Effort?\",\n    author = \"Church, Kenneth Ward  and\n      Chandrasekar, Raman  and\n      Ortega, John E.  and\n      Ahmad, Ibrahim Said\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.242/\",\n    pages = \"3589--3599\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.242.pdf",
        "site": "https://aclanthology.org/2025.coling-main.242/",
        "pdf_size": 390198,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13495746884872158804&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Institute for Experiential AI, Northeastern University; Institute for Experiential AI, Northeastern University; Institute for Experiential AI, Northeastern University; Institute for Experiential AI, Northeastern University",
        "aff_domain": "northeastern.edu;northeastern.edu;northeastern.edu;northeastern.edu",
        "email": "northeastern.edu;northeastern.edu;northeastern.edu;northeastern.edu",
        "github": "https://github.com/kwchurch/is-peer-reviewing-worth-the-effort",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "Institute for Experiential AI",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-industry.48",
        "title": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "The quality of meeting summaries generated by natural language generation (NLG) systems is hard to measure automatically. Established metrics such as ROUGE and BERTScore have a relatively low correlation with human judgments and fail to capture nuanced errors. Recent studies suggest using large language models (LLMs), which have the benefit of better context understanding and adaption of error definitions without training on a large number of human preference judgments. However, current LLM-based evaluators risk masking errors and can only serve as a weak proxy, leaving human evaluation the gold standard despite being costly and hard to compare across studies. In this work, we present MESA, an LLM-based framework employing a three-step assessment of individual error types, multi-agent discussion for decision refinement, and feedback-based self-training to refine error definition understanding and alignment with human judgment. We show that MESA\u2019s components enable thorough error detection, consistent rating, and adaptability to custom error guidelines. Using GPT-4o as its backbone, MESA achieves mid to high Point-Biserial correlation with human judgment in error detection and mid Spearman and Kendall correlation in reflecting error impact on summary quality, on average 0.25 higher than previous methods. The framework\u2019s flexibility in adapting to custom error guidelines makes it suitable for various tasks with limited human-labeled data.",
        "author": "Frederic Thomas Kirstein; Terry Lima Ruas; Bela Gipp",
        "authorids": "/f/frederic-thomas-kirstein/; /t/terry-lima-ruas/; /b/bela-gipp/",
        "bibtex": "@inproceedings{kirstein-etal-2025-meeting,\n    title = \"Is my Meeting Summary Good? Estimating Quality with a Multi-{LLM} Evaluator\",\n    author = \"Kirstein, Frederic Thomas  and\n      Lima Ruas, Terry  and\n      Gipp, Bela\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.48/\",\n    pages = \"561--574\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.48.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.48/",
        "pdf_size": 531779,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8939647278061068837&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of G\u00f6ttingen, Germany; University of G\u00f6ttingen, Germany; University of G\u00f6ttingen, Germany",
        "aff_domain": "gipplab.org; ; ",
        "email": "gipplab.org; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of G\u00f6ttingen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-goettingen.de",
        "aff_unique_abbr": "Georg-August-Universit\u00e4t",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.646",
        "title": "It is not a piece of cake for GPT: Explaining Textual Entailment Recognition in the presence of Figurative Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Textual Entailment Recognition (TER) aims to predict whether a pair of premise-hypothesis sentences represents an entailment, a contradiction, or none of the above. Addressing TER in the presence of figurative language is particularly challenging because words are used in a way that deviates from the conventional order and meaning. In this work, we investigate the capabilities of Large Language Models (LLMs) to address TER and generate textual explanations of TER predictions. First, we evaluate LLM performance in Zero- and Few-Shot Learning settings, with and without using Chain-of-Thought prompting. After identifying the best prompts, we highlight the settings in which in-context learning is beneficial. The closed-source models GPT-3.5 Turbo and GPT-4o show unexpected limitations compared to significantly smaller open-source LLMs. Next, we thoroughly analyze the effect of LLM Fine-Tuning, showing substantial improvements in the quality of TER explanations compared to Zero- and Few-Shot Learning. Notably, 9 billion parameter open-source LLMs demonstrate again competitive performance against larger closed-source models. Finally, we compare our LLM-based approach with the state-of-the-art DREAM-FLUTE and Cross-Task architectures. The results show significant performance improvements, particularly in the quality of the generated explanations.",
        "author": "Giuseppe Gallipoli; Luca Cagliero",
        "authorids": "/g/giuseppe-gallipoli/; /l/luca-cagliero/",
        "bibtex": "@inproceedings{gallipoli-cagliero-2025-piece,\n    title = \"It is not a piece of cake for {GPT}: Explaining Textual Entailment Recognition in the presence of Figurative Language\",\n    author = \"Gallipoli, Giuseppe  and\n      Cagliero, Luca\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.646/\",\n    pages = \"9656--9674\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.646.pdf",
        "site": "https://aclanthology.org/2025.coling-main.646/",
        "pdf_size": 410568,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6075138234765360984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Politecnico di Torino; Politecnico di Torino",
        "aff_domain": "polito.it;polito.it",
        "email": "polito.it;polito.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Politecnico di Torino",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.polito.it",
        "aff_unique_abbr": "Polito",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2025.coling-main.440",
        "title": "Iterative Structured Knowledge Distillation: Optimizing Language Models Through Layer-by-Layer Distillation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Traditional language model compression techniques, like knowledge distillation, require a fixed architecture, limiting flexibility, while structured pruning methods often fail to preserve performance. This paper introduces Iterative Structured Knowledge Distillation (ISKD), which integrates knowledge distillation and structured pruning by progressively replacing transformer blocks with smaller, efficient versions during training. This study validates ISKD on two transformer-based language models: GPT-2 and Phi-1. ISKD outperforms L1 pruning and achieves similar performance to knowledge distillation while offering greater flexibility. ISKD reduces model parameters - 30.68% for GPT-2 and 30.16% for Phi-1 - while maintaining at least four-fifths of performance on both language modeling and commonsense reasoning tasks. These findings suggest that this method offers a promising balance between model efficiency and accuracy.",
        "author": "Malthe Have Musaeus; Rob van der Goot",
        "authorids": "/m/malthe-have-musaeus/; /r/rob-van-der-goot/",
        "bibtex": "@inproceedings{musaeus-van-der-goot-2025-iterative,\n    title = \"Iterative Structured Knowledge Distillation: Optimizing Language Models Through Layer-by-Layer Distillation\",\n    author = \"Musaeus, Malthe Have  and\n      van der Goot, Rob\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.440/\",\n    pages = \"6601--6606\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.440.pdf",
        "site": "https://aclanthology.org/2025.coling-main.440/",
        "pdf_size": 710394,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Z1QlchwZe0AJ:scholar.google.com/&scioq=Iterative+Structured+Knowledge+Distillation:+Optimizing+Language+Models+Through+Layer-by-Layer+Distillation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "IT University of Copenhagen; IT University of Copenhagen",
        "aff_domain": "musaeus.dk;itu.dk",
        "email": "musaeus.dk;itu.dk",
        "github": "https://github.com/Malthehave/ISKD-COLING-2025",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IT University of Copenhagen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://itu.dk",
        "aff_unique_abbr": "ITU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "2025.coling-main.410",
        "title": "It\u2019s What You Say and How You Say It: Investigating the Effect of Linguistic vs. Behavioral Adaptation in Task-Oriented Chatbots",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Given the conflicting expectations users have for how a dialog agent should sound and behave, there is no one-size-fits-all option for dialog system design. Therefore, adaptation is critical to ensure successful and enjoyable interactions. However, it is not yet clear what the effects of behavioral (what the agent says) vs. linguistic adaptation (how the agent says this) are in terms of dialog success and user perception. In this work, we implement three different types of task-oriented dialog agents which can each vary their level of formality. We evaluate subjective and objective metrics of dialog success as well as user perceptions through a user study, comparing the collected data to that of (CITATION), where users interacted with the same three types of agents without linguistic adaptation. From this, we draw insights into which subjective and objective aspects of success and user perception are influenced by each type of adaptation. We additionally all code, user surveys, and dialog interaction logs.",
        "author": "Lindsey Vanderlyn; Ngoc Thang Vu",
        "authorids": "/l/lindsey-vanderlyn/; /n/ngoc-thang-vu/",
        "bibtex": "@inproceedings{vanderlyn-vu-2025-say,\n    title = \"It{'}s What You Say and How You Say It: Investigating the Effect of Linguistic vs. Behavioral Adaptation in Task-Oriented Chatbots\",\n    author = \"Vanderlyn, Lindsey  and\n      Vu, Ngoc Thang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.410/\",\n    pages = \"6120--6149\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.410.pdf",
        "site": "https://aclanthology.org/2025.coling-main.410/",
        "pdf_size": 6124826,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VP5zm_dFuGYJ:scholar.google.com/&scioq=It%E2%80%99s+What+You+Say+and+How+You+Say+It:+Investigating+the+Effect+of+Linguistic+vs.+Behavioral+Adaptation+in+Task-Oriented+Chatbots&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Stuttgart; University of Stuttgart",
        "aff_domain": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "email": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-stuttgart.de",
        "aff_unique_abbr": "USTuttgart",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.395",
        "title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent developments in Japanese large language models (LLMs) primarily focus on general domains, with fewer advancements in Japanese biomedical LLMs. One obstacle is the absence of a comprehensive, large-scale benchmark for comparison. Furthermore, the resources for evaluating Japanese biomedical LLMs are insufficient. To advance this field, we propose a new benchmark including eight LLMs across four categories and 20 Japanese biomedical datasets across five tasks. Experimental results indicate that: (1) LLMs with a better understanding of Japanese and richer biomedical knowledge achieve better performance in Japanese biomedical tasks, (2) LLMs that are not mainly designed for Japanese biomedical domains can still perform unexpectedly well, and (3) there is still much room for improving the existing LLMs in certain Japanese biomedical tasks. Moreover, we offer insights that could further enhance development in this field. Our evaluation tools tailored to our benchmark as well as the datasets are publicly available to facilitate future research.",
        "author": "Junfeng Jiang; Jiahao Huang; Akiko Aizawa",
        "authorids": "/j/junfeng-jiang/; /j/jiahao-huang/; /a/akiko-aizawa/",
        "bibtex": "@inproceedings{jiang-etal-2025-jmedbench,\n    title = \"{JM}ed{B}ench: A Benchmark for Evaluating {J}apanese Biomedical Large Language Models\",\n    author = \"Jiang, Junfeng  and\n      Huang, Jiahao  and\n      Aizawa, Akiko\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.395/\",\n    pages = \"5918--5935\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.395.pdf",
        "site": "https://aclanthology.org/2025.coling-main.395/",
        "pdf_size": 905187,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15272005943249882920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The University of Tokyo; The University of Tokyo; National Institute of Informatics + The University of Tokyo",
        "aff_domain": "is.s.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;nii.ac.jp",
        "email": "is.s.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;nii.ac.jp",
        "github": "https://github.com/nii-nlp/med-eval",
        "project": "https://huggingface.co/datasets/Coldog2333/JMedBench",
        "author_num": 3,
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "University of Tokyo;National Institute of Informatics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.nii.ac.jp/",
        "aff_unique_abbr": "UTokyo;NII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.287",
        "title": "Jump To Hyperspace: Comparing Euclidean and Hyperbolic Loss Functions for Hierarchical Multi-Label Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hierarchical Multi-Label Text Classification (HMTC) is a challenging machine learning task where multiple labels from a hierarchically organized label set are assigned to a single text. In this study, we examine the effectiveness of Euclidean and hyperbolic loss functions to improve the performance of BERT models on HMTC, which very few previous studies have adopted. We critically evaluate label-aware losses as well as contrastive losses in the Euclidean and hyperbolic space, demonstrating that hyperbolic loss functions perform comparably with non-hyperbolic loss functions on four commonly used HMTC datasets in most scenarios. While hyperbolic label-aware losses perform the best on low-level labels, the overall consistency and micro-averaged performance is compromised. Additionally, we find that our contrastive losses are less effective for HMTC when deployed in the hyperbolic space than non-hyperbolic counterparts. Our research highlights that with the right metrics and training objectives, hyperbolic space does not provide any additional benefits compared to Euclidean space for HMTC, thereby prompting a reevaluation of how different geometric spaces are used in other AI applications.",
        "author": "Jens Van Nooten; Walter Daelemans",
        "authorids": "/j/jens-van-nooten/; /w/walter-daelemans/",
        "bibtex": "@inproceedings{van-nooten-daelemans-2025-jump,\n    title = \"Jump To Hyperspace: Comparing {E}uclidean and Hyperbolic Loss Functions for Hierarchical Multi-Label Text Classification\",\n    author = \"Van Nooten, Jens  and\n      Daelemans, Walter\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.287/\",\n    pages = \"4260--4273\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.287.pdf",
        "site": "https://aclanthology.org/2025.coling-main.287/",
        "pdf_size": 783705,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8745913702934415317&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/clips/jump_to_hyperspace",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2025.coling-main.422",
        "title": "Just Read the Codebook! Make Use of Quality Codebooks in Zero-Shot Classification of Multilabel Frame Datasets",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The recent development of Large Language Models lowered the barrier to entry for using Natural Language Processing methods for various tasks in the related scientific field of Computational Social Science and has led to more scrutiny of their performance on complex datasets. While in many cases the costly fine-tuning of smaller Language Models outperforms LLMs, zero and few-shot approaches on consumer hardware have the potential to deepen interdisciplinary research efforts, whilst opening up NLP research to complex, niche datasets that are hard to classify. The great effort that is coding datasets comes with the benefit of concise instructions for how to code the data at hand. We investigate, whether highly specific, instructive codebooks created by social scientists in order to code text with a multitude of complex labels can improve zero-shot performance on (quantized) LLMs. Our findings show, that using the latest LLMs, zero-shot performance can improve by providing a codebook on two complex datasets with a total of four different topics and can outperform few-shot In-Context-Learning setups. The approach is equally or more token-efficient, and requires less hands-on engineering, making it particularly compelling for practical research.",
        "author": "Mattes Ruckdeschel",
        "authorids": "/m/mattes-ruckdeschel/",
        "bibtex": "@inproceedings{ruckdeschel-2025-just,\n    title = \"Just Read the Codebook! Make Use of Quality Codebooks in Zero-Shot Classification of Multilabel Frame Datasets\",\n    author = \"Ruckdeschel, Mattes\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.422/\",\n    pages = \"6317--6337\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.422.pdf",
        "site": "https://aclanthology.org/2025.coling-main.422/",
        "pdf_size": 305881,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XByx1Lpsq3MJ:scholar.google.com/&scioq=Just+Read+the+Codebook!+Make+Use+of+Quality+Codebooks+in+Zero-Shot+Classification+of+Multilabel+Frame+Datasets&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2025.coling-industry.46",
        "title": "KARRIEREWEGE: A large scale Career Path Prediction Dataset",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Accurate career path prediction can support many stakeholders, like job seekers, recruiters, HR, and project managers. However, publicly available data and tools for career path prediction are scarce. In this work, we introduce Karrierewege, a comprehensive, publicly available dataset containing over 500k career paths, significantly surpassing the size of previously available datasets. We link the dataset to the ESCO taxonomy to offer a valuable resource for predicting career trajectories. To tackle the problem of free-text inputs typically found in resumes, we enhance it by synthesizing job titles and descriptions resulting in Karrierewege+. This allows for accurate predictions from unstructured data, closely aligning with practical application challenges. We benchmark existing state-of-the-art (SOTA) models on our dataset and a previous benchmark and see increased performance and robustness by synthesizing the data for the free-text use cases.",
        "author": "Elena Senger; Yuri Campbell; Rob van der Goot; Barbara Plank",
        "authorids": "/e/elena-senger/; /y/yuri-campbell/; /r/rob-van-der-goot/; /b/barbara-plank/",
        "bibtex": "@inproceedings{senger-etal-2025-karrierewege,\n    title = \"{KARRIEREWEGE}: A large scale Career Path Prediction Dataset\",\n    author = \"Senger, Elena  and\n      Campbell, Yuri  and\n      van der Goot, Rob  and\n      Plank, Barbara\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.46/\",\n    pages = \"533--545\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.46.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.46/",
        "pdf_size": 607809,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12066544323201682620&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "MaiNLP, Center for Information and Language Processing, LMU Munich, Germany+Fraunhofer Center for International Management and Knowledge Economy IMW, Germany; Fraunhofer Center for International Management and Knowledge Economy IMW, Germany; Department of Computer Science, IT University of Copenhagen, Denmark; MaiNLP, Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;imw.fraunhofer.de;itu.dk;lmu.de",
        "email": "cis.lmu.de;imw.fraunhofer.de;itu.dk;lmu.de",
        "github": "",
        "project": "https://huggingface.co/datasets/ElenaSenger/Karrierewege; https://huggingface.co/datasets/ElenaSenger/Karrierewege_plus",
        "author_num": 4,
        "aff_unique_index": "0+1;1;2;0",
        "aff_unique_norm": "LMU Munich;Fraunhofer Center for International Management and Knowledge Economy IMW;IT University of Copenhagen",
        "aff_unique_dep": "Center for Information and Language Processing;;Department of Computer Science",
        "aff_unique_url": "https://www.lmu.de;https://www.imw.fraunhofer.de/;https://itu.dk",
        "aff_unique_abbr": "LMU;Fraunhofer IMW;ITU Copenhagen",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+0;0;1;0",
        "aff_country_unique": "Germany;Denmark"
    },
    {
        "id": "2025.coling-main.698",
        "title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent studies have demonstrated that large language models (LLMs) are susceptible to being misled by false premise questions (FPQs), leading to errors in factual knowledge, known as factuality hallucination. Existing benchmarks that assess this vulnerability primarily rely on manual construction, resulting in limited size and lack of expandability. In this work, we introduce an automated, scalable pipeline to create FPQs based on knowledge graphs (KGs). The first step is to modify true triplets extracted from KGs to create false premises. Subsequently, utilizing the state-of-the-art capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed method, we present a comprehensive benchmark, the Knowledge Graph-based False Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three knowledge domains, at six levels of confusability, and in two task formats. Using KG-FPQ, we conduct extensive evaluations on several representative LLMs and provide valuable insights. The KG-FPQ dataset and code are available at https://github.com/yanxuzhu/KG-FPQ.",
        "author": "Yanxu Zhu; Jinlin Xiao; Yuhang Wang; Jitao Sang",
        "authorids": "/y/yanxu-zhu/; /j/jinlin-xiao/; /y/yuhang-wang/; /j/jitao-sang/",
        "bibtex": "@inproceedings{zhu-etal-2025-kg,\n    title = \"{KG}-{FPQ}: Evaluating Factuality Hallucination in {LLM}s with Knowledge Graph-based False Premise Questions\",\n    author = \"Zhu, Yanxu  and\n      Xiao, Jinlin  and\n      Wang, Yuhang  and\n      Sang, Jitao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.698/\",\n    pages = \"10472--10490\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.698.pdf",
        "site": "https://aclanthology.org/2025.coling-main.698/",
        "pdf_size": 4124437,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15836662017318371273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University + Peng Cheng Lab; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University + Peng Cheng Lab",
        "aff_domain": "bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn",
        "email": "bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn",
        "github": "https://github.com/yanxuzhu/KG-FPQ",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "Beijing Jiaotong University;Peng Cheng Lab",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining;",
        "aff_unique_url": "http://www.bjtu.edu.cn;",
        "aff_unique_abbr": "BJTU;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.611",
        "title": "KG-TRICK: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual knowledge graphs (KGs) provide high-quality relational and textual information for various NLP applications, but they are often incomplete, especially in non-English languages. Previous research has shown that combining information from KGs in different languages aids either Knowledge Graph Completion (KGC), the task of predicting missing relations between entities, or Knowledge Graph Enhancement (KGE), the task of predicting missing textual information for entities. Although previous efforts have considered KGC and KGE as independent tasks, we hypothesize that they are interdependent and mutually beneficial. To this end, we introduce KG-TRICK, a novel sequence-to-sequence framework that unifies the tasks of textual and relational information completion for multilingual KGs. KG-TRICK demonstrates that: i) it is possible to unify the tasks of KGC and KGE into a single framework, and ii) combining textual information from multiple languages is beneficial to improve the completeness of a KG. As part of our contributions, we also introduce WikiKGE10++, the largest manually-curated benchmark for textual information completion of KGs, which features over 25,000 entities across 10 diverse languages.",
        "author": "Zelin Zhou; Simone Conia; Daniel Lee; Min Li; Shenglei Huang; Umar Farooq Minhas; Saloni Potdar; Henry Xiao; Yunyao Li",
        "authorids": "/z/zelin-zhou/; /s/simone-conia/; /d/daniel-lee/; /m/min-li/; /s/shenglei-huang/; /u/umar-farooq-minhas/; /s/saloni-potdar/; /h/henry-xiao/; /y/yunyao-li/",
        "bibtex": "@inproceedings{zhou-etal-2025-kg,\n    title = \"{KG}-{TRICK}: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs\",\n    author = \"Zhou, Zelin  and\n      Conia, Simone  and\n      Lee, Daniel  and\n      Li, Min  and\n      Huang, Shenglei  and\n      Minhas, Umar Farooq  and\n      Potdar, Saloni  and\n      Xiao, Henry  and\n      Li, Yunyao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.611/\",\n    pages = \"9096--9111\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.611.pdf",
        "site": "https://aclanthology.org/2025.coling-main.611/",
        "pdf_size": 2055261,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pK3jmbm7_MUJ:scholar.google.com/&scioq=KG-TRICK:+Unifying+Textual+and+Relational+Information+Completion+of+Knowledge+for+Multilingual+Knowledge+Graphs&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Apple; Sapienza University of Rome; Adobe; Apple; Apple; Apple; Apple; Apple; Adobe",
        "aff_domain": "apple.com;uniroma1.it;adobe.com;apple.com;apple.com;apple.com;apple.com;apple.com;adobe.com",
        "email": "apple.com;uniroma1.it;adobe.com;apple.com;apple.com;apple.com;apple.com;apple.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;2;0;0;0;0;0;2",
        "aff_unique_norm": "Apple Inc.;Sapienza University of Rome;Adobe Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.apple.com;https://www.uniroma1.it;https://www.adobe.com",
        "aff_unique_abbr": "Apple;Sapienza;Adobe",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rome",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;Italy"
    },
    {
        "id": "2025.coling-main.276",
        "title": "KIA: Knowledge-Guided Implicit Vision-Language Alignment for Chest X-Ray Report Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Report generation (RG) faces challenges in understanding complex medical images and establishing cross-modal semantic alignment in radiology image-report pairs. Previous methods often overlook fine-grained cross-modal interaction, leading to insufficient understanding of detailed information. Recently, various large multimodal models have been proposed for image-text tasks. However, such models still underperform on rare domain tasks like understanding complex medical images. To address these limitations, we develop a new framework of Knowledge-guided Implicit vision-language Alignment for radiology report generation, named KIA. To better understand medical reports and images and build alignment between them, multi-task implicit alignment is creatively introduced, forming comprehensive understanding of medical images and reports. Additionally, to further meet medical refinement requirements, we design novel masking strategies guided by medical knowledge to enhance pathological observation and anatomical landm",
        "author": "Heng Yin; Shanlin Zhou; Pandong Wang; Zirui Wu; Yongtao Hao",
        "authorids": "/h/heng-yin/; /s/shanlin-zhou/; /p/pandong-wang/; /z/zirui-wu/; /y/yongtao-hao/",
        "bibtex": "@inproceedings{yin-etal-2025-kia,\n    title = \"{KIA}: Knowledge-Guided Implicit Vision-Language Alignment for Chest {X}-Ray Report Generation\",\n    author = \"Yin, Heng  and\n      Zhou, Shanlin  and\n      Wang, Pandong  and\n      Wu, Zirui  and\n      Hao, Yongtao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.276/\",\n    pages = \"4096--4108\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.276.pdf",
        "site": "https://aclanthology.org/2025.coling-main.276/",
        "pdf_size": 8478587,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4727893294087333203&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Technology, Tongji University, China; Department of Computer Science and Technology, Tongji University, China; Department of Computer Science and Technology, Tongji University, China; Department of Computer Science and Technology, Tongji University, China; Department of Computer Science and Technology, Tongji University, China",
        "aff_domain": "tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn",
        "email": "tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;tongji.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tongji University",
        "aff_unique_dep": "Department of Computer Science and Technology",
        "aff_unique_url": "https://www.tongji.edu.cn",
        "aff_unique_abbr": "Tongji",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.297",
        "title": "KVFKT: A New Horizon in Knowledge Tracing with Attention-Based Embedding and Forgetting Curve Integration",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The knowledge tracing (KT) model based on deep learning has been proven to be superior to the traditional knowledge tracing model, eliminating the need for artificial engineering features. However, there are still problems, such as insufficient interpretability of the learning and answering processes. To address these issues, we propose a new approach in knowledge tracing with attention-based embedding and forgetting curve integration, namely KVFKT. Firstly, the embedding representation module is responsible for embedding the questions and computing the attention vector of knowledge concepts (KCs) when students answer questions and when answer time stamps are collected. Secondly, the forgetting quantification module performs the pre-prediction update of the student\u2019s knowledge state matrix. This quantification involves calculating the interval time and associated forgetting rate of relevant KCs, following the forgetting curve. Thirdly, the answer prediction module generates responses based on students\u2019 knowledge status, guess coefficient, and question difficulty. Finally, the knowledge status update module further refines the students\u2019 knowledge status according to their answers to the questions and the characteristics of those questions. In the experiment, four real-world datasets are used to test the model. Experimental results show that KVFKT better traces students\u2019 knowledge state and outperforms state-of-the-art models.",
        "author": "Quanlong Guan; Xiuliang Duan; Kaiquan Bian; Guanliang Chen; Jianbo Huang; Zhiguo Gong; Liangda Fang",
        "authorids": "/q/quanlong-guan/; /x/xiuliang-duan/; /k/kaiquan-bian/; /g/guanliang-chen/; /j/jianbo-huang/; /z/zhiguo-gong/; /l/liangda-fang/",
        "bibtex": "@inproceedings{guan-etal-2025-kvfkt,\n    title = \"{KVFKT}: A New Horizon in Knowledge Tracing with Attention-Based Embedding and Forgetting Curve Integration\",\n    author = \"Guan, Quanlong  and\n      Duan, Xiuliang  and\n      Bian, Kaiquan  and\n      Chen, Guanliang  and\n      Huang, Jianbo  and\n      Gong, Zhiguo  and\n      Fang, Liangda\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.297/\",\n    pages = \"4399--4409\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.297.pdf",
        "site": "https://aclanthology.org/2025.coling-main.297/",
        "pdf_size": 1469309,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XJVK_atmu-sJ:scholar.google.com/&scioq=KVFKT:+A+New+Horizon+in+Knowledge+Tracing+with+Attention-Based+Embedding+and+Forgetting+Curve+Integration&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Jinan University, Guangzhou, China+Guangdong-Macao Advanced Intelligent Computing Joint Laboratory, Zhuhai, China; Jinan University, Guangzhou, China; Jinan University, Guangzhou, China; Monash University, Melbourne, Australia; South China University of Technology, Guangzhou, China; University of Macau, Macao, China+Guangdong-Macao Advanced Intelligent Computing Joint Laboratory, Zhuhai, China; Jinan University, Guangzhou, China+Pazhou Laboratory, Guangzhou, China+Guangdong-Macao Advanced Intelligent Computing Joint Laboratory, Zhuhai, China",
        "aff_domain": "jnu.edu.cn;stu2022.jnu.edu.cn;jnu.edu.cn;monash.edu;scut.edu.cn;umac.mo;jnu.edu.cn",
        "email": "jnu.edu.cn;stu2022.jnu.edu.cn;jnu.edu.cn;monash.edu;scut.edu.cn;umac.mo;jnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;2;3;4+1;0+5+1",
        "aff_unique_norm": "Jinan University;Guangdong-Macao Advanced Intelligent Computing Joint Laboratory;Monash University;South China University of Technology;University of Macau;Pazhou Laboratory",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.jnu.edu.cn;;https://www.monash.edu;http://www.scut.edu.cn;https://www.um.edu.mo;",
        "aff_unique_abbr": "JNU;;Monash;SCUT;UM;",
        "aff_campus_unique_index": "0+1;0;0;2;0;3+1;0+0+1",
        "aff_campus_unique": "Guangzhou;Zhuhai;Melbourne;Macao",
        "aff_country_unique_index": "0+0;0;0;1;0;0+0;0+0+0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2025.coling-main.290",
        "title": "Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hybrid search has emerged as an effective strategy to offset the limitations of different matching paradigms, especially in out-of-domain contexts where notable improvements in retrieval quality have been observed. However, existing research predominantly focuses on a limited set of retrieval methods, evaluated in pairs on domain-general datasets exclusively in English. In this work, we study the efficacy of hybrid search across a variety of prominent retrieval models within the unexplored field of law in the French language, assessing both zero-shot and in-domain scenarios. Our findings reveal that in a zero-shot context, fusing different domain-general models consistently enhances performance compared to using a standalone model, regardless of the fusion method. Surprisingly, when models are trained in-domain, we find that fusion generally diminishes performance relative to using the best single system, unless fusing scores with carefully tuned weights. These novel insights, among others, expand the applicability of prior findings across a new field and language, and contribute to a deeper understanding of hybrid search in non-English specialized domains.",
        "author": "Antoine Louis; Gijs van Dijck; Gerasimos Spanakis",
        "authorids": "/a/antoine-louis/; /g/gijs-van-dijck/; /g/gerasimos-spanakis/",
        "bibtex": "@inproceedings{louis-etal-2025-know,\n    title = \"Know When to Fuse: Investigating Non-{E}nglish Hybrid Retrieval in the Legal Domain\",\n    author = \"Louis, Antoine  and\n      van Dijck, Gijs  and\n      Spanakis, Gerasimos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.290/\",\n    pages = \"4293--4312\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.290.pdf",
        "site": "https://aclanthology.org/2025.coling-main.290/",
        "pdf_size": 1654588,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ExPPtxL9ZZUJ:scholar.google.com/&scioq=Know+When+to+Fuse:+Investigating+Non-English+Hybrid+Retrieval+in+the+Legal+Domain&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Law & Tech Lab, Maastricht University; Law & Tech Lab, Maastricht University; Law & Tech Lab, Maastricht University",
        "aff_domain": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "email": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "github": "https://github.com/maastrichtlawtech/fusion",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Maastricht University",
        "aff_unique_dep": "Law & Tech Lab",
        "aff_unique_url": "https://www.maastrichtuniversity.nl",
        "aff_unique_abbr": "MU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2025.coling-industry.4",
        "title": "Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating RAG Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Retrieval Augmented Generation (RAG) systems are a widespread application of Large Language Models (LLMs) in the industry. While many tools exist empowering developers to build their own systems, measuring their performance locally, with datasets reflective of the system\u2019s use cases, is a technological challenge. Solutions to this problem range from non-specific and cheap (most public datasets) to specific and costly (generating data from local documents). In this paper, we show that using public question and answer (Q&A) datasets to assess retrieval performance can lead to non-optimal systems design, and that common tools for RAG dataset generation can lead to unbalanced data. We propose solutions to these issues based on the characterization of RAG datasets through labels and through label-targeted data generation. Finally, we show that fine-tuned small LLMs can efficiently generate Q&A datasets. We believe that these observations are invaluable to the know-your-data step of RAG systems development.",
        "author": "Rafael Teixeira de Lima; Shubham Gupta; Cesar Berrospi Ramis; Lokesh Mishra; Michele Dolfi; Peter Staar; Panagiotis Vagenas",
        "authorids": "/r/rafael-teixeira-de-lima/; /s/shubham-gupta/; /c/cesar-berrospi-ramis/; /l/lokesh-mishra/; /m/michele-dolfi/; /p/peter-staar/; /p/panagiotis-vagenas/",
        "bibtex": "@inproceedings{teixeira-de-lima-etal-2025-know,\n    title = \"Know Your {RAG}: Dataset Taxonomy and Generation Strategies for Evaluating {RAG} Systems\",\n    author = \"Teixeira de Lima, Rafael  and\n      Gupta, Shubham  and\n      Berrospi Ramis, Cesar  and\n      Mishra, Lokesh  and\n      Dolfi, Michele  and\n      Staar, Peter  and\n      Vagenas, Panagiotis\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.4/\",\n    pages = \"39--57\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.4.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.4/",
        "pdf_size": 475238,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1856366695649245396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "IBM Research Paris-Saclay; IBM Research Paris-Saclay; IBM Research Zurich; IBM Research Zurich; IBM Research Zurich; IBM Research Zurich; IBM Research Zurich",
        "aff_domain": "ibm.com; ; ; ; ; ; ",
        "email": "ibm.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "IBM Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0;1;1;1;1;1",
        "aff_campus_unique": "Paris-Saclay;Zurich",
        "aff_country_unique_index": "0;0;1;1;1;1;1",
        "aff_country_unique": "France;Switzerland"
    },
    {
        "id": "2025.coling-main.38",
        "title": "Knowledge Graph Entity Typing with Curriculum Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The Knowledge Graph Entity Typing (KGET) task aims to predict missing type annotations for entities in knowledge graphs. Most recent studies only focus on the structural information from an entity\u2019s neighborhood or semantic information from textual representations of entities or relations. In this paper, inspired by curriculum learning and contrastive learning, we propose the CCLET model using the Curriculum Contrastive Learning strategy for KGET, which uses the Pre-trained Language Model (PLM) and the graph model to fuse the entity related semantic and the structural information of the Knowledge Graph (KG) respectively. Our CCLET model consists of two main parts. In the Knowledge Fusion part, we design an Enhanced-MLP architecture to fuse the text of the entity\u2019s description, related triplet, and tuples; In the Curriculum Contrastive Learning part, we define the difficulty of the course by controlling the level of added noise, we aim to accurately learn with curriculum contrastive learning strategy from easy to difficult. Our extensive experiments demonstrate that the CCLET model outperforms recent state-of-the-art models, verifying its effectiveness in the KGET task.",
        "author": "Hao Wang; Minghua Nuo; Shan Jiang",
        "authorids": "/h/hao-wang/; /m/minghua-nuo/; /s/shan-jiang/",
        "bibtex": "@inproceedings{wang-etal-2025-knowledge,\n    title = \"Knowledge Graph Entity Typing with Curriculum Contrastive Learning\",\n    author = \"Wang, Hao  and\n      Nuo, Minghua  and\n      Jiang, Shan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.38/\",\n    pages = \"574--583\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.38.pdf",
        "site": "https://aclanthology.org/2025.coling-main.38/",
        "pdf_size": 730324,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sHZsFfa-l18J:scholar.google.com/&scioq=Knowledge+Graph+Entity+Typing+with+Curriculum+Contrastive+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Computer Science, Inner Mongolia University; College of Computer Science, Inner Mongolia University + National and Local Joint Engineering Research Center of Intelligent Information Processing Technology for Mongolian + Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology; College of Computer Science, Inner Mongolia University",
        "aff_domain": "163.com; ; ",
        "email": "163.com; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1+0;0",
        "aff_unique_norm": "Inner Mongolia University;National and Local Joint Engineering Research Center",
        "aff_unique_dep": "College of Computer Science;Intelligent Information Processing Technology",
        "aff_unique_url": "http://www.imu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.359",
        "title": "Knowledge Graph Pooling and Unpooling for Concept Abstraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge graph embedding (KGE) aims to embed entities and relations as vectors in a continuous space and has proven to be effective for KG tasks. Recently, graph neural networks (GNN) based KGEs gain much attention due to their strong capability of encoding complex graph structures. However, most GNN-based KGEs are directly optimized based on the instance triples in KGs, ignoring the latent concepts and hierarchies of the entities. Though some works explicitly inject concepts and hierarchies into models, they are limited to predefined concepts and hierarchies, which are missing in a lot of KGs. Thus in this paper, we propose a novel framework with KG Pooling and unpooling and Contrastive Learning (KGPCL) to abstract and encode the latent concepts for better KG prediction. Specifically, with an input KG, we first construct a U-KG through KG pooling and unpooling. KG pooling abstracts the input graph to a smaller graph as a pooled graph, and KG unpooling recovers the input graph from the pooled graph. Then we model the U-KG with relational KGEs to get the representations of entities and relations for prediction. Finally, we propose the local and global contrastive loss to jointly enhance the representation of entities. Experimental results show that our models outperform the KGE baselines on link prediction task.",
        "author": "Juan Li; Wen Zhang; Zhiqiang Liu; Mingchen Tu; Mingyang Chen; Ningyu Zhang; Shijian Li",
        "authorids": "/j/juan-li/; /w/wen-zhang/; /z/zhiqiang-liu/; /m/mingchen-tu/; /m/mingyang-chen/; /n/ningyu-zhang/; /s/shijian-li/",
        "bibtex": "@inproceedings{li-etal-2025-knowledge,\n    title = \"Knowledge Graph Pooling and Unpooling for Concept Abstraction\",\n    author = \"Li, Juan  and\n      Zhang, Wen  and\n      Liu, Zhiqiang  and\n      Tu, Mingchen  and\n      Chen, Mingyang  and\n      Zhang, Ningyu  and\n      Li, Shijian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.359/\",\n    pages = \"5364--5374\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.359.pdf",
        "site": "https://aclanthology.org/2025.coling-main.359/",
        "pdf_size": 1147182,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8ku9fohDCGMJ:scholar.google.com/&scioq=Knowledge+Graph+Pooling+and+Unpooling+for+Concept+Abstraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.238",
        "title": "Knowledge Graph Unlearning with Schema",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Graph unlearning emerges as a crucial step to eliminate the impact of deleted elements from a trained model. However, unlearning on the knowledge graph (KG) has not yet been extensively studied. We remark that KG unlearning is non-trivial because KG is distinctive from general graphs. In this paper, we first propose a new unlearning method based on schema for KG. Specifically, we update the representation of the deleted element\u2019s neighborhood with an unlearning object that regulates the affinity between the affected neighborhood and the instances within the same schema. Second, we raise a new task: schema unlearning. Given a schema graph to be deleted, we remove all instances matching the pattern and make the trained model forget the removed instances. Last, we evaluate the proposed unlearning method on various KG embedding models with benchmark datasets. Our codes are available at https://github.com/NKUShaw/KGUnlearningBySchema.",
        "author": "Yang Xiao; Ruimeng Ye; Bo Hui",
        "authorids": "/y/yang-xiao/; /r/ruimeng-ye/; /b/bo-hui/",
        "bibtex": "@inproceedings{xiao-etal-2025-knowledge,\n    title = \"Knowledge Graph Unlearning with Schema\",\n    author = \"Xiao, Yang  and\n      Ye, Ruimeng  and\n      Hui, Bo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.238/\",\n    pages = \"3541--3546\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.238.pdf",
        "site": "https://aclanthology.org/2025.coling-main.238/",
        "pdf_size": 493151,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:vGUV-GrzZA8J:scholar.google.com/&scioq=Knowledge+Graph+Unlearning+with+Schema&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Tulsa; Department of Computer Science, University of Tulsa; Department of Computer Science, University of Tulsa",
        "aff_domain": "utulsa.edu;utulsa.edu;utulsa.edu",
        "email": "utulsa.edu;utulsa.edu;utulsa.edu",
        "github": "https://github.com/NKUShaw/KGUnlearningBySchema",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tulsa",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utulsa.edu",
        "aff_unique_abbr": "UTulsa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.268",
        "title": "KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like \u201cOxygen is to Gas as < blank > is to < blank >\" requires identifying the semantic relationship (e.g., \u201ctype of\u201d) between the first pair of terms (\u201cOxygen\u201d and \u201cGas\u201d) and finding a second pair that shares the same relationship (e.g., \u201cAluminum\u201d and \u201cMetal\u201d). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge. Our code and data are available at: https://github.com/Thiliniiw/KnowledgePrompts/",
        "author": "Thilini Wijesiriwardene; Ruwan Wickramarachchi; Sreeram Reddy Vennam; Vinija Jain; Aman Chadha; Amitava Das; Ponnurangam Kumaraguru; Amit Sheth",
        "authorids": "/t/thilini-wijesiriwardene/; /r/ruwan-wickramarachchi/; /s/sreeram-reddy-vennam/; /v/vinija-jain/; /a/aman-chadha/; /a/amitava-das/; /p/ponnurangam-kumaraguru/; /a/amit-sheth/",
        "bibtex": "@inproceedings{wijesiriwardene-etal-2025-knowledgeprompts,\n    title = \"{K}nowledge{P}rompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting\",\n    author = \"Wijesiriwardene, Thilini  and\n      Wickramarachchi, Ruwan  and\n      Vennam, Sreeram Reddy  and\n      Jain, Vinija  and\n      Chadha, Aman  and\n      Das, Amitava  and\n      Kumaraguru, Ponnurangam  and\n      Sheth, Amit\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.268/\",\n    pages = \"3979--3996\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.268.pdf",
        "site": "https://aclanthology.org/2025.coling-main.268/",
        "pdf_size": 1411747,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xdt0hpdEcMgJ:scholar.google.com/&scioq=KnowledgePrompts:+Exploring+the+Abilities+of+Large+Language+Models+to+Solve+Proportional+Analogies+via+Knowledge-Enhanced+Prompting&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "AI Institute, University of South Carolina, USA; AI Institute, University of South Carolina, USA; IIIT Hyderabad, India; Meta AI, USA; Amazon GenAI, USA; AI Institute, University of South Carolina, USA; IIIT Hyderabad, India; AI Institute, University of South Carolina, USA",
        "aff_domain": "sc.edu; ; ; ; ; ; ;",
        "email": "sc.edu; ; ; ; ; ; ;",
        "github": "https://github.com/Thiliniiw/KnowledgePrompts/",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;2;3;0;1;0",
        "aff_unique_norm": "University of South Carolina;International Institute of Information Technology, Hyderabad;Meta AI;Amazon GenAI",
        "aff_unique_dep": "AI Institute;;;",
        "aff_unique_url": "https://www.sc.edu;https://iiit Hyderabad.ac.in;https://meta.ai;https://genai.amazon.com",
        "aff_unique_abbr": "USC;IIIT Hyderabad;Meta AI;Amazon GenAI",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hyderabad",
        "aff_country_unique_index": "0;0;1;0;0;0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2025.coling-main.412",
        "title": "LASS: A Novel and Economical Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As more than 70% of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are hesitant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the reliance on a specific structure is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, large-scale data augmentation based on large language models faces an apparent disadvantage, the expensive costs. Therefore, in this paper, we propose LASS, a novel data augmentation framework based on both LArge and Small language models for debiaSing opinion summarization. Specifically, a small number of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on perplexity degree and sentiment classification. Experiments have proved that LASS can effectively alleviate emotional bias, similar to using only large models, but in a more economical way.",
        "author": "Yanyue Zhang; Pengfei Li; Yilong Lai; Yulan He; Deyu Zhou",
        "authorids": "/y/yanyue-zhang/; /p/pengfei-li/; /y/yilong-lai/; /y/yulan-he/; /d/deyu-zhou/",
        "bibtex": "https://aclanthology.org/2025.coling-main.412.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.412.pdf",
        "site": "https://aclanthology.org/2025.coling-main.412/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dGIDwXfg2dgJ:scholar.google.com/&scioq=LASS:+A+Novel+and+Economical+Data+Augmentation+Framework+Based+on+Language+Models+for+Debiasing+Opinion+Summarization&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-industry.50",
        "title": "LAW: Legal Agentic Workflows for Custody and Fund Services Contracts",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Legal contracts in the custody and fund services domain govern critical aspects such as key provider responsibilities, fee schedules, and indemnification rights. However, it is challenging for an off-the-shelf Large Language Model (LLM) to ingest these contracts due to the lengthy unstructured streams of text, limited LLM context windows, and complex legal jargon. To address these challenges, we introduce LAW (Legal Agentic Workflows for Custody and Fund Services Contracts). LAW features a modular design that responds to user queries by orchestrating a suite of domain-specific tools and text agents. Our experiments demonstrate that LAW, by integrating multiple specialized agents and tools, significantly outperforms the baseline. LAW excels particularly in complex tasks such as calculating a contract\u2019s termination date, surpassing the baseline by 92.9% points. Furthermore, LAW offers a cost-effective alternative to traditional fine-tuned legal LLMs by leveraging reusable, domain-specific tools.",
        "author": "William Watson; Nicole Cho; Nishan Srishankar; Zhen Zeng; Lucas Cecchi; Daniel Scott; Suchetha Siddagangappa; Rachneet Kaur; Tucker Balch; Manuela Veloso",
        "authorids": "/w/william-watson/; /n/nicole-cho/; /n/nishan-srishankar/; /z/zhen-zeng/; /l/lucas-cecchi/; /d/daniel-scott/; /s/suchetha-siddagangappa/; /r/rachneet-kaur/; /t/tucker-balch/; /m/manuela-veloso/",
        "bibtex": "@inproceedings{watson-etal-2025-law,\n    title = \"{LAW}: Legal Agentic Workflows for Custody and Fund Services Contracts\",\n    author = \"Watson, William  and\n      Cho, Nicole  and\n      Srishankar, Nishan  and\n      Zeng, Zhen  and\n      Cecchi, Lucas  and\n      Scott, Daniel  and\n      Siddagangappa, Suchetha  and\n      Kaur, Rachneet  and\n      Balch, Tucker  and\n      Veloso, Manuela\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.50/\",\n    pages = \"583--594\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.50.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.50/",
        "pdf_size": 1077952,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5763020793330139904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research; J.P. Morgan AI Research",
        "aff_domain": "jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com",
        "email": "jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com;jpmorgan.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "J.P. Morgan",
        "aff_unique_dep": "AI Research",
        "aff_unique_url": "https://www.jpmorgan.com",
        "aff_unique_abbr": "JPM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.716",
        "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "General and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, their current evaluations lack alignment with the fundamental logic of legal reasoning, the legal syllogism. This hinders trust and understanding from legal experts. To bridge this gap, we introduce LAiW, the Chinese legal LLM benchmark structured around the legal syllogism. We evaluate legal LLMs across three levels of capability, each reflecting a progressively more complex stage of legal syllogism: fundamental information retrieval, legal principles inference, and advanced legal applications, and encompassing a wide range of tasks in different legal scenarios. Our automatic evaluation reveals that LLMs, despite their ability to answer complex legal questions, lack the inherent logical processes of the legal syllogism. This limitation poses a barrier to acceptance by legal professionals. Furthermore, manual evaluation with legal experts confirms this issue and highlights the importance of pre-training on legal text to enhance the legal syllogism of LLMs. Future research may prioritize addressing this gap to unlock the full potential of LLMs in legal applications.",
        "author": "Yongfu Dai; Duanyu Feng; Jimin Huang; Haochen Jia; Qianqian Xie; Yifang Zhang; Weiguang Han; Wei Tian; Hao Wang",
        "authorids": "/y/yongfu-dai/; /d/duanyu-feng/; /j/jimin-huang/; /h/haochen-jia/; /q/qianqian-xie/; /y/yifang-zhang/; /w/weiguang-han/; /w/wei-tian/; /h/hao-wang/",
        "bibtex": "@inproceedings{dai-etal-2025-laiw,\n    title = \"{LA}i{W}: A {C}hinese Legal Large Language Models Benchmark\",\n    author = \"Dai, Yongfu  and\n      Feng, Duanyu  and\n      Huang, Jimin  and\n      Jia, Haochen  and\n      Xie, Qianqian  and\n      Zhang, Yifang  and\n      Han, Weiguang  and\n      Tian, Wei  and\n      Wang, Hao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.716/\",\n    pages = \"10738--10766\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.716.pdf",
        "site": "https://aclanthology.org/2025.coling-main.716/",
        "pdf_size": 849597,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6820726413052872011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Sichuan University, Chengdu, China; Sichuan University, Chengdu, China; The Fin AI, Singapore; Sichuan University, Chengdu, China; Wuhan University, Wuhan, China; Sichuan University, Chengdu, China; Wuhan University, Wuhan, China; Southwest Petroleum University, Chengdu, China; Sichuan University, Chengdu, China",
        "aff_domain": "gmail.com;stu.scu.edu.cn;thefin.ai;gmail.com;thefin.ai;foxmail.com;whu.edu.cn;qq.com;scu.edu.cn",
        "email": "gmail.com;stu.scu.edu.cn;thefin.ai;gmail.com;thefin.ai;foxmail.com;whu.edu.cn;qq.com;scu.edu.cn",
        "github": "https://github.com/Dai-shen/LAiW",
        "project": "https://github.com/liuchengyuan123/LegalLLMEvaluation/",
        "author_num": 9,
        "aff_unique_index": "0;0;1;0;2;0;2;3;0",
        "aff_unique_norm": "Sichuan University;The Fin AI;Wuhan University;Southwest Petroleum University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.scu.edu.cn;;http://www.whu.edu.cn/;https://www.swpu.edu.cn",
        "aff_unique_abbr": "SCU;;WHU;SWPU",
        "aff_campus_unique_index": "0;0;0;2;0;2;0;0",
        "aff_campus_unique": "Chengdu;;Wuhan",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-demos.3",
        "title": "LENS: Learning Entities from Narratives of Skin Cancer",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Learning entities from narratives of skin cancer (LENS) is an automatic entity recognition system built on colloquial writings from skin cancer-related Reddit forums. LENS encapsulates a comprehensive set of 24 labels that address clinical, demographic, and psychosocial aspects of skin cancer. Furthermore, we release LENS as a PyPI and pip package, making it easy for developers to download and install, and also provide a web application that allows users to get model predictions interactively, useful for researchers and individuals with minimal programming experience. Additionally, we publish the annotation guidelines designed specifically for spontaneous skin cancer narratives, that can be implemented to better understand and address challenges when developing corpora or systems for similar diseases. The model achieves an overall entity-level F1 score of 0.561, with notable performance for entities such as \u201cCANC_T\u201d (0.747), \u201cSTG\u201d (0.788), \u201cPOB\u201d (0.714), \u201cGENDER\u201d (0.750), \u201cA/G\u201d (0.714), and \u201cPPL\u201d (0.703). Other entities with significant results include \u201cTRT\u201d (0.625), \u201cMED\u201d (0.606), \u201cAGE\u201d (0.646), \u201cEMO\u201d (0.619), and \u201cMHD\u201d (0.5). We believe that LENS can serve as an essential tool supporting the analysis of patient discussions leading to improvements in the design and development of modern smart healthcare technologies.",
        "author": "Daisy Monika Lal; Paul Rayson; Christopher Peter; Ignatius Ezeani; Mo El-Haj; Yafei Zhu; Yufeng Liu",
        "authorids": "/d/daisy-monika-lal/; /p/paul-rayson/; /c/christopher-peter/; /i/ignatius-ezeani/; /m/mo-el-haj/; /y/yafei-zhu/; /y/yufeng-liu/",
        "bibtex": "@inproceedings{lal-etal-2025-lens,\n    title = \"{LENS}: Learning Entities from Narratives of Skin Cancer\",\n    author = \"Lal, Daisy Monika  and\n      Rayson, Paul  and\n      Peter, Christopher  and\n      Ezeani, Ignatius  and\n      El-Haj, Mo  and\n      Zhu, Yafei  and\n      Liu, Yufeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.3/\",\n    pages = \"20--27\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.3.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.3/",
        "pdf_size": 325510,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:874x3x0zUDMJ:scholar.google.com/&scioq=LENS:+Learning+Entities+from+Narratives+of+Skin+Cancer&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computing and Communications, Lancaster University, UK; School of Computing and Communications, Lancaster University, UK; National Institute of Mental Health and Neuroscience, Bengaluru, India; School of Computing and Communications, Lancaster University, UK; School of Computing and Communications, Lancaster University, UK; Linguistics and English Language, Lancaster University, UK; Shanghai International Studies University, China",
        "aff_domain": "lancaster.ac.uk; ; ; ; ; ; ",
        "email": "lancaster.ac.uk; ; ; ; ; ; ",
        "github": "https://github.com/dml2611/LENS",
        "project": "https://lens-demo.streamlit.app/",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;2",
        "aff_unique_norm": "Lancaster University;National Institute of Mental Health and Neuroscience;Shanghai International Studies University",
        "aff_unique_dep": "School of Computing and Communications;;",
        "aff_unique_url": "https://www.lancaster.ac.uk;;http://www.shisu.edu.cn",
        "aff_unique_abbr": ";;SISU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bengaluru",
        "aff_country_unique_index": "0;0;1;0;0;0;2",
        "aff_country_unique": "United Kingdom;India;China"
    },
    {
        "id": "2025.coling-industry.66",
        "title": "LLM ContextBridge: A Hybrid Approach for Intent and Dialogue Understanding in IVSR",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In-vehicle speech recognition (IVSR) systems are crucial components of modern automotive interfaces, enabling hands-free control and enhancing user safety. However, traditional IVSR systems often struggle with interpreting user intent accurately due to limitations in contextual understanding and ambiguity resolution, leading to user frustration. This paper introduces LLM ContextBridge, a novel hybrid architecture that integrates Pretrained Language Model-based intent classification with Large Language Models to enhance both command recognition and dialogue management. LLM ContextBridge serves as a seamless bridge between traditional natural language understanding techniques and LLMs, combining the precise intent recognition of conventional NLU with the contextual handling and ambiguity resolution capabilities of LLMs. This approach significantly improves recognition accuracy and user experience, particularly in complex, multi-turn dialogues. Experimental results show notable improvements in task success rates and user satisfaction, demonstrating that LLM ContextBridge can make IVSR systems more intuitive, responsive, and context-aware.",
        "author": "Changwoo Chun; Daniel Rim; Juhee Park",
        "authorids": "/c/changwoo-chun/; /d/daniel-rim/; /j/juhee-park/",
        "bibtex": "@inproceedings{chun-etal-2025-llm,\n    title = \"{LLM} {C}ontext{B}ridge: A Hybrid Approach for Intent and Dialogue Understanding in {IVSR}\",\n    author = \"Chun, Changwoo  and\n      Rim, Daniel  and\n      Park, Juhee\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.66/\",\n    pages = \"794--806\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.66.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.66/",
        "pdf_size": 647524,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qWqaSlty4bIJ:scholar.google.com/&scioq=LLM+ContextBridge:+A+Hybrid+Approach+for+Intent+and+Dialogue+Understanding+in+IVSR&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Hyundai Motor Company; Hyundai Motor Company; Hyundai Motor Company",
        "aff_domain": "hyundai.com;hyundai.com;hyundai.com",
        "email": "hyundai.com;hyundai.com;hyundai.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hyundai Motor Company",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hyundai.com",
        "aff_unique_abbr": "HMC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-industry.24",
        "title": "LLM Evaluate: An Industry-Focused Evaluation Tool for Large Language Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks in recent years. This has inspired researchers and practitioners in the real-world industrial domain to build useful products via leveraging LLMs. However, extensive evaluations of LLMs, in terms of accuracy, memory management, and inference latency, while ensuring the reproducibility of the results are crucial before deploying LLM-based solutions for real-world usage. In addition, when evaluating LLMs on internal customer data, an on-premise evaluation system is necessary to protect customer privacy rather than sending customer data to third-party APIs for evaluation. In this paper, we demonstrate how we build an on-premise system for LLM evaluation to address the challenges in the evaluation of LLMs in real-world industrial settings. We demonstrate the complexities of consolidating various datasets, models, and inference-related artifacts in complex LLM inference pipelines. For this purpose, we also present a case study in a real-world industrial setting. The demonstration of the LLM evaluation tool development would help researchers and practitioners in building on-premise systems for LLM evaluation ensuring privacy, reliability, robustness, and reproducibility.",
        "author": "Harsh Saini; Md Tahmid Rahman Laskar; Cheng Chen; Elham Mohammadi; David Rossouw",
        "authorids": "/h/harsh-saini/; /m/md-tahmid-rahman-laskar/; /c/cheng-chen/; /e/elham-mohammadi/; /d/david-rossouw/",
        "bibtex": "@inproceedings{saini-etal-2025-llm,\n    title = \"{LLM} Evaluate: An Industry-Focused Evaluation Tool for Large Language Models\",\n    author = \"Saini, Harsh  and\n      Laskar, Md Tahmid Rahman  and\n      Chen, Cheng  and\n      Mohammadi, Elham  and\n      Rossouw, David\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.24/\",\n    pages = \"286--294\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.24.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.24/",
        "pdf_size": 240603,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ATwOErHWFXYJ:scholar.google.com/&scioq=LLM+Evaluate:+An+Industry-Focused+Evaluation+Tool+for+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/openai/evals",
        "project": "https://crfm.stanford.edu/helm/",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.188",
        "title": "LLM Sensitivity Challenges in Abusive Language Detection: Instruction-Tuned vs. Human Feedback",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The capacity of large language models (LLMs) to understand and distinguish socially unacceptable texts enables them to play a promising role in abusive language detection. However, various factors can affect their sensitivity. In this work, we test whether LLMs have an unintended bias in abusive language detection, i.e., whether they predict more or less of a given abusive class than expected in zero-shot settings. Our results show that instruction-tuned LLMs tend to under-predict positive classes, since datasets used for tuning are dominated by the negative class. On the contrary, models fine-tuned with human feedback tend to be overly sensitive. In an exploratory approach to mitigate these issues, we show that label frequency in the prompt helps with the significant over-prediction.",
        "author": "Yaqi Zhang; Viktor Hangya; Alexander Fraser",
        "authorids": "/y/yaqi-zhang/; /v/viktor-hangya/; /a/alexander-fraser/",
        "bibtex": "@inproceedings{zhang-etal-2025-llm,\n    title = \"{LLM} Sensitivity Challenges in Abusive Language Detection: Instruction-Tuned vs. Human Feedback\",\n    author = \"Zhang, Yaqi  and\n      Hangya, Viktor  and\n      Fraser, Alexander\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.188/\",\n    pages = \"2765--2780\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.188.pdf",
        "site": "https://aclanthology.org/2025.coling-main.188/",
        "pdf_size": 382057,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11627284816879397832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computation, Information and Technology, Technical University of Munich + Munich Center for Machine Learning; Fraunhofer IIS, Erlangen, Germany + LMU Munich; School of Computation, Information and Technology, Technical University of Munich + Munich Center for Machine Learning",
        "aff_domain": "tum.de;iis.fraunhofer.de;tum.de",
        "email": "tum.de;iis.fraunhofer.de;tum.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2+3;0+1",
        "aff_unique_norm": "Technical University of Munich;Munich Center for Machine Learning;Fraunhofer Institute for Integrated Circuits;Ludwig Maximilian University of Munich",
        "aff_unique_dep": "School of Computation, Information and Technology;Center for Machine Learning;;",
        "aff_unique_url": "https://www.tum.de;https://www.munich-center-for-machine-learning.de;https://www.iis.fraunhofer.de/;https://www.lmu.de",
        "aff_unique_abbr": "TUM;;Fraunhofer IIS;LMU",
        "aff_campus_unique_index": ";1+2;",
        "aff_campus_unique": ";Erlangen;Munich",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.207",
        "title": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated impressive performance across various domains. However, for clinical diagnosis, higher expectations are required for LLM\u2019s reliability and sensitivity: thinking like physicians and remaining sensitive to key medical information that affects diagnostic reasoning, as subtle variations can lead to different diagnosis results. Yet, existing works focus mainly on investigating the sensitivity of LLMs to irrelevant context and overlook the importance of key information. In this paper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini, Claude3 and LLaMA2-7b, to key medical information by introducing different perturbation strategies. The evaluation results highlight the limitations of current LLMs in remaining sensitive to key medical information for diagnostic decision-making. The evolution of LLMs must focus on improving their reliability, enhancing their ability to be sensitive to key information, and effectively utilizing this information. These improvements will enhance human trust in LLMs and facilitate their practical application in real-world scenarios. Our code and dataset are available at https://github.com/chenwei23333/DiagnosisQA.",
        "author": "Chenwei Yan; Xiangling Fu; Yuxuan Xiong; Tianyi Wang; Siu Cheung Hui; Ji Wu; Xien Liu",
        "authorids": "/c/chenwei-yan/; /x/xiangling-fu/; /y/yuxuan-xiong/; /t/tianyi-wang/; /s/siu-cheung-hui/; /j/ji-wu/; /x/xien-liu/",
        "bibtex": "@inproceedings{yan-etal-2025-llm,\n    title = \"{LLM} Sensitivity Evaluation Framework for Clinical Diagnosis\",\n    author = \"Yan, Chenwei  and\n      Fu, Xiangling  and\n      Xiong, Yuxuan  and\n      Wang, Tianyi  and\n      Hui, Siu Cheung  and\n      Wu, Ji  and\n      Liu, Xien\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.207/\",\n    pages = \"3083--3094\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.207.pdf",
        "site": "https://aclanthology.org/2025.coling-main.207/",
        "pdf_size": 1782605,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:lgdHUAX_PYwJ:scholar.google.com/&scioq=LLM+Sensitivity+Evaluation+Framework+for+Clinical+Diagnosis&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Beijing University of Posts and Telecommunications+Key Laboratory of Trustworthy Distributed Computing and Service(BUPT), Ministry of Education; School of Computer Science, Beijing University of Posts and Telecommunications+Key Laboratory of Trustworthy Distributed Computing and Service(BUPT), Ministry of Education; School of Computer Science, Beijing University of Posts and Telecommunications+Key Laboratory of Trustworthy Distributed Computing and Service(BUPT), Ministry of Education; School of Computer Science, Beijing University of Posts and Telecommunications+Key Laboratory of Trustworthy Distributed Computing and Service(BUPT), Ministry of Education; Nanyang Technological University; Department of Electronic Engineering, Tsinghua University+College of AI, Tsinghua University; Department of Electronic Engineering, Tsinghua University+College of AI, Tsinghua University",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;ntu.edu.sg;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;ntu.edu.sg;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "https://github.com/chenwei23333/DiagnosisQA",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0;0+0;0+0;0+0;1;2+2;2+2",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Nanyang Technological University;Tsinghua University",
        "aff_unique_dep": "School of Computer Science;;Department of Electronic Engineering",
        "aff_unique_url": "http://www.bupt.edu.cn;https://www.ntu.edu.sg;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "BUPT;NTU;THU",
        "aff_campus_unique_index": "0;0;0;0;;",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;1;0+0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-industry.42",
        "title": "LLM-Friendly Knowledge Representation for Customer Support",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We propose a practical approach by integrating Large Language Models (LLMs) with a framework designed to navigate the complexities of Airbnb customer support operations. In this paper, our methodology employs a novel reformatting technique, the Intent, Context, and Action (ICA) format, which transforms policies and workflows into a structure more comprehensible to LLMs. Additionally, we develop a synthetic data generation strategy to create training data with minimal human intervention, enabling cost-effective fine-tuning of our model. Our internal experiments (not applied to Airbnb products) demonstrate that our approach of restructuring workflows and fine-tuning LLMs with synthetic data significantly enhances their performance, setting a new benchmark for their application in customer support. Our solution is not only cost-effective but also improves customer support, as evidenced by both accuracy and manual processing time evaluation metrics.",
        "author": "Hanchen Su; Wei Luo; Yashar Mehdad; Wei Han; Elaine Liu; Wayne Zhang; Mia Zhao; Joy Zhang",
        "authorids": "/h/hanchen-su/; /w/wei-luo/; /y/yashar-mehdad/; /w/wei-han/; /e/elaine-liu/; /w/wayne-zhang/; /m/mia-zhao/; /j/joy-zhang/",
        "bibtex": "@inproceedings{su-etal-2025-llm,\n    title = \"{LLM}-Friendly Knowledge Representation for Customer Support\",\n    author = \"Su, Hanchen  and\n      Luo, Wei  and\n      Mehdad, Yashar  and\n      Han, Wei  and\n      Liu, Elaine  and\n      Zhang, Wayne  and\n      Zhao, Mia  and\n      Zhang, Joy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.42/\",\n    pages = \"496--504\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.42.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.42/",
        "pdf_size": 1224325,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4847181705760549635&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Airbnb Inc., USA; Airbnb Inc., USA; Airbnb Inc., USA; Airbnb Inc., USA; Airbnb Inc., USA; Airbnb Inc., USA; Airbnb Inc., USA; Airbnb Inc., USA",
        "aff_domain": "airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com",
        "email": "airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com;airbnb.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Airbnb Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.airbnb.com",
        "aff_unique_abbr": "Airbnb",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.98",
        "title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to household preferences. For example, an LLM planner may find it challenging to perform tasks that require personalization, such as deciding where to place mugs in a kitchen based on specific household preferences. We introduce LLM-Personalize, a novel framework designed to personalize LLM planners for household robotics. LLM-Personalize uses an LLM planner to perform iterative planning in multi-room, partially-observable household environments, utilizing a scene graph built dynamically from local observations. To personalize the LLM planner towards user preferences, our optimization pipeline integrates imitation learning and reinforced Self-Training. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, demonstrating a more than 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences.",
        "author": "Dongge Han; Trevor McInroe; Adam Jelley; Stefano V. Albrecht; Peter Bell; Amos Storkey",
        "authorids": "/d/dongge-han/; /t/trevor-mcinroe/; /a/adam-jelley/; /s/stefano-v-albrecht/; /p/peter-bell/; /a/amos-storkey/",
        "bibtex": "@inproceedings{han-etal-2025-llm,\n    title = \"{LLM}-Personalize: Aligning {LLM} Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots\",\n    author = \"Han, Dongge  and\n      McInroe, Trevor  and\n      Jelley, Adam  and\n      Albrecht, Stefano V.  and\n      Bell, Peter  and\n      Storkey, Amos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.98/\",\n    pages = \"1465--1474\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.98.pdf",
        "site": "https://aclanthology.org/2025.coling-main.98/",
        "pdf_size": 1485111,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9451287167414788437&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft, Cambridge, UK+School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK",
        "aff_domain": "gmail.com;ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "email": "gmail.com;ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "Microsoft;University of Edinburgh",
        "aff_unique_dep": ";School of Informatics",
        "aff_unique_url": "https://www.microsoft.com;https://www.ed.ac.uk",
        "aff_unique_abbr": "MSFT;Edinburgh",
        "aff_campus_unique_index": "0+1;1;1;1;1;1",
        "aff_campus_unique": "Cambridge;Edinburgh",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.447",
        "title": "LLM4RE: A Data-centric Feasibility Study for Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Relation Extraction (RE) is a multi-task process that is a crucial part of all information extraction pipelines. With the introduction of the generative language models, Large Language Models (LLMs) have showcased significant performance boosts for complex natural language processing and understanding tasks. Recent research in RE has also started incorporating these advanced machines in their pipelines. However, the full extent of the LLM\u2019s potential for extracting relations remains unknown. Consequently, this study aims to conduct the first feasibility analysis to explore the viability of LLMs for RE by investigating their robustness to various complex RE scenarios stemming from data-specific characteristics. By conducting an exhaustive analysis of five state-of-the-art LLMs backed by more than 2100 experiments, this study posits that LLMs are not robust enough to tackle complex data characteristics for RE, and additional research efforts focusing on investigating their behaviors at extracting relationships are needed. The source code for the evaluation pipeline can be found at https://aaig.ece.ufl.edu/projects/relation-extraction .",
        "author": "Anushka Swarup; Tianyu Pan; Ronald Wilson; Avanti Bhandarkar; Damon Woodard",
        "authorids": "/a/anushka-swarup/; /t/tianyu-pan/; /r/ronald-wilson/; /a/avanti-bhandarkar/; /d/damon-woodard/",
        "bibtex": "@inproceedings{swarup-etal-2025-llm4re,\n    title = \"{LLM}4{RE}: A Data-centric Feasibility Study for Relation Extraction\",\n    author = \"Swarup, Anushka  and\n      Pan, Tianyu  and\n      Wilson, Ronald  and\n      Bhandarkar, Avanti  and\n      Woodard, Damon\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.447/\",\n    pages = \"6670--6691\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.447.pdf",
        "site": "https://aclanthology.org/2025.coling-main.447/",
        "pdf_size": 4189723,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oKGUE61iE1wJ:scholar.google.com/&scioq=LLM4RE:+A+Data-centric+Feasibility+Study+for+Relation+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Florida Institute for National Security (FINS), University of Florida, Gainesville FL 32611, USA; Florida Institute for National Security (FINS), University of Florida, Gainesville FL 32611, USA; Florida Institute for National Security (FINS), University of Florida, Gainesville FL 32611, USA; Florida Institute for National Security (FINS), University of Florida, Gainesville FL 32611, USA; Florida Institute for National Security (FINS), University of Florida, Gainesville FL 32611, USA",
        "aff_domain": "ufl.edu; ; ; ; ",
        "email": "ufl.edu; ; ; ; ",
        "github": "",
        "project": "https://aaig.ece.ufl.edu/projects/relation-extraction",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Florida Institute for National Security (FINS)",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Gainesville",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.59",
        "title": "LLMTreeRec: Unleashing the Power of Large Language Models for Cold-Start Recommendations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The lack of training data gives rise to the system cold-start problem in recommendation systems, making them struggle to provide effective recommendations. To address this problem, Large Language Models(LLMs) can model recommendation tasks as language analysis tasks and provide zero-shot results based on their vast open-world knowledge. However, the large scale of the item corpus poses a challenge to LLMs, leading to substantial token consumption that makes it impractical to deploy in real-world recommendation systems. To tackle this challenge, we introduce a tree-based LLM recommendation framework LLMTreeRec, which structures all items into an item tree to improve the efficiency of LLM\u2019s item retrieval. LLMTreeRec achieves state-of-the-art performance under the system cold-start setting in two widely used datasets, which is even competitive with conventional deep recommendation systems that use substantial training data. Furthermore, LLMTreeRec outperforms the baseline model in the A/B test on Huawei industrial system. Consequently, LLMTreeRec demonstrates its effectiveness as an industry-friendly solution that has been successfully deployed online.",
        "author": "Wenlin Zhang; Chuhan Wu; Xiangyang Li; Yuhao Wang; Kuicai Dong; Yichao Wang; Xinyi Dai; Xiangyu Zhao; Huifeng Guo; Ruiming Tang",
        "authorids": "/w/wenlin-zhang/; /c/chuhan-wu/; /x/xiangyang-li/; /y/yuhao-wang/; /k/kuicai-dong/; /y/yichao-wang/; /x/xinyi-dai/; /x/xiangyu-zhao/; /h/huifeng-guo/; /r/ruiming-tang/",
        "bibtex": "@inproceedings{zhang-etal-2025-llmtreerec,\n    title = \"{LLMT}ree{R}ec: Unleashing the Power of Large Language Models for Cold-Start Recommendations\",\n    author = \"Zhang, Wenlin  and\n      Wu, Chuhan  and\n      Li, Xiangyang  and\n      Wang, Yuhao  and\n      Dong, Kuicai  and\n      Wang, Yichao  and\n      Dai, Xinyi  and\n      Zhao, Xiangyu  and\n      Guo, Huifeng  and\n      Tang, Ruiming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.59/\",\n    pages = \"886--896\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.59.pdf",
        "site": "https://aclanthology.org/2025.coling-main.59/",
        "pdf_size": 557403,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3203915399531863589&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "City University of Hong Kong, Hong Kong, China; Noah\u2019s Ark Lab, China; Noah\u2019s Ark Lab, Singapore; City University of Hong Kong, Hong Kong, China; Noah\u2019s Ark Lab, China; Noah\u2019s Ark Lab, China; Noah\u2019s Ark Lab, China; City University of Hong Kong, Hong Kong, China; Noah\u2019s Ark Lab, China; Noah\u2019s Ark Lab, China",
        "aff_domain": "my.cityu.edu.hk;my.cityu.edu.hk;cityu.edu.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "my.cityu.edu.hk;my.cityu.edu.hk;cityu.edu.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "https://github.com/Applied-Machine-Learning-Lab/LLMTreeRec",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;0;1;1;1;0;1;1",
        "aff_unique_norm": "City University of Hong Kong;Noah\u2019s Ark Lab",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cityu.edu.hk;",
        "aff_unique_abbr": "CityU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-main.163",
        "title": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, RAG still faces several challenges in tackling complex multi-hop queries, which require LLMs to perform accurate reasoning and retrieval at each step. Inspired by the human reasoning process, where we progressively search for missing information after acquiring useful clues, it is natural to question whether LLMs have similar capabilities. In this work, we first experimentally verified the ability of LLMs to extract information from the retrieved knowledge as well as to know what is still missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content from the document, along with the information extraction capability of LLMs to extract useful information from denoised documents. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules. Code and data are released in https://github.com/AdelWang/MIGRES.",
        "author": "Keheng Wang; Feiyu Duan; Peiguang Li; Sirui Wang; Xunliang Cai",
        "authorids": "/k/keheng-wang/; /f/feiyu-duan/; /p/peiguang-li/; /s/sirui-wang/; /x/xunliang-cai/",
        "bibtex": "@inproceedings{wang-etal-2025-llms,\n    title = \"{LLM}s Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation\",\n    author = \"Wang, Keheng  and\n      Duan, Feiyu  and\n      Li, Peiguang  and\n      Wang, Sirui  and\n      Cai, Xunliang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.163/\",\n    pages = \"2379--2400\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.163.pdf",
        "site": "https://aclanthology.org/2025.coling-main.163/",
        "pdf_size": 567963,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17295569148566206203&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Meituan, Beijing, China; Meituan, Beijing, China + Beihang University, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China + Department of Automation, Tsinghua University, Beijing, China; Meituan, Beijing, China",
        "aff_domain": "meituan.com;meituan.com;meituan.com;meituan.com;meituan.com",
        "email": "meituan.com;meituan.com;meituan.com;meituan.com;meituan.com",
        "github": "https://github.com/AdelWang/MIGRES",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0+2;0",
        "aff_unique_norm": "Meituan;Beihang University;Tsinghua University",
        "aff_unique_dep": ";;Department of Automation",
        "aff_unique_url": "https://www.meituan.com;http://www.buaa.edu.cn/;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Meituan;BUAA;THU",
        "aff_campus_unique_index": "0;0+0;0;0+0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.390",
        "title": "LLMs May Perform MCQA by Selecting the Least Incorrect Option",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the field of NLP, Large Language Models (LLMs) have markedly enhanced performance across a variety of tasks. However, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the adoption of Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs has gained considerable traction. However, concerns regarding the robustness of this evaluative method persist. Building upon previous discussions on the issue of variability, we reveal an additional dimension of concern: LLMs may perform MCQA by selecting the least incorrect option rather than distinctly correct. This observation suggests that LLMs might regard multiple options as correct, which could undermine the reliability of MCQA as a metric for evaluating LLMs. To address this challenge, we introduce an enhanced dataset augmentation method for MCQA, termed MCQA+, to provide a more accurate reflection of the performance, thereby highlighting the necessity for more sophisticated evaluation mechanisms in the assessment of LLM capabilities.",
        "author": "Haochun Wang; Sendong Zhao; Zewen Qiang; Nuwa Xi; Bing Qin; Ting Liu",
        "authorids": "/h/haochun-wang/; /s/sendong-zhao/; /z/zewen-qiang/; /n/nuwa-xi/; /b/bing-qin/; /t/ting-liu/",
        "bibtex": "@inproceedings{wang-etal-2025-llms-may,\n    title = \"{LLM}s May Perform {MCQA} by Selecting the Least Incorrect Option\",\n    author = \"Wang, Haochun  and\n      Zhao, Sendong  and\n      Qiang, Zewen  and\n      Xi, Nuwa  and\n      Qin, Bing  and\n      Liu, Ting\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.390/\",\n    pages = \"5852--5862\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.390.pdf",
        "site": "https://aclanthology.org/2025.coling-main.390/",
        "pdf_size": 615490,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=158073911613388682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn; ; ; ; ",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.350",
        "title": "LLMs meet Bloom\u2019s Taxonomy: A Cognitive View on Large Language Model Evaluations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current evaluation approaches for Large Language Models (LLMs) lack a structured approach that reflects the underlying cognitive abilities required for solving the tasks. This hinders a thorough understanding of the current level of LLM capabilities. For instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in. This paper introduces a novel perspective on the evaluation of LLMs that leverages a hierarchical classification of tasks. Specifically, we explore the most widely used benchmarks for LLMs to systematically identify how well these existing evaluation methods cover the levels of Bloom\u2019s Taxonomy, a hierarchical framework for categorizing cognitive skills. This comprehensive analysis allows us to identify strengths and weaknesses in current LLM assessment strategies in terms of cognitive abilities and suggest directions for both future benchmark development as well as highlight potential avenues for LLM research. Our findings reveal that LLMs generally perform better on the lower end of Bloom\u2019s Taxonomy. Additionally, we find that there are significant gaps in the coverage of cognitive skills in the most commonly used benchmarks.",
        "author": "Thomas Huber; Christina Niklaus",
        "authorids": "/t/thomas-huber/; /c/christina-niklaus/",
        "bibtex": "@inproceedings{huber-niklaus-2025-llms,\n    title = \"{LLM}s meet Bloom{'}s Taxonomy: A Cognitive View on Large Language Model Evaluations\",\n    author = \"Huber, Thomas  and\n      Niklaus, Christina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.350/\",\n    pages = \"5211--5246\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.350.pdf",
        "site": "https://aclanthology.org/2025.coling-main.350/",
        "pdf_size": 733767,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:F6x_MuFTGiEJ:scholar.google.com/&scioq=LLMs+meet+Bloom%E2%80%99s+Taxonomy:+A+Cognitive+View+on+Large+Language+Model+Evaluations&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of St. Gallen, Switzerland; University of St. Gallen, Switzerland",
        "aff_domain": "unisg.ch;unisg.ch",
        "email": "unisg.ch;unisg.ch",
        "github": "https://github.com/ThHuberSG/coling-bloom",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of St. Gallen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unisg.ch",
        "aff_unique_abbr": "HSG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2025.coling-main.53",
        "title": "LLMs on interactive feature collections with implicit dynamic decision strategy",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In real-world contexts such as medical diagnosis and business consulting, effective problem-solving often requires gathering relevant information through interactions and targeted questioning to pinpoint the root cause of a problem. However, Large Language Models (LLMs) often struggle to efficiently narrow down the search space, leading to either missing key information or asking redundant questions when guided by implicit methods like Chain-of-Thought (CoT). Some approaches employ external engineered systems to guide reasoning paths, but these methods may not fully utilize the inherent problem-solving capabilities of LLMs and often require multiple expensive API calls. This study explores how we can implicitly guide LLMs to enhance their interactive feature collection abilities within a single prompt. Instead of employing explicit search algorithms or step-by-step external guidance, we provide high-level guidelines that allow LLMs to dynamically adjust their strategies and iteratively refine their decision-making processes independently. Evaluations on synthetic 20-Questions games and real-world scenarios, including business and medical diagnosis cases, demonstrate that LLMs guided by these strategies perform more effective interactive feature collection, asking fewer and more strategic questions and achieving better problem-solving efficiency.",
        "author": "Juyeon Heo; Vihari Piratla; Kyunghyun Lee; Hyonkeun Joh; Adrian Weller",
        "authorids": "/j/juyeon-heo/; /v/vihari-piratla/; /k/kyunghyun-lee/; /h/hyonkeun-joh/; /a/adrian-weller/",
        "bibtex": "@inproceedings{heo-etal-2025-llms,\n    title = \"{LLM}s on interactive feature collections with implicit dynamic decision strategy\",\n    author = \"Heo, Juyeon  and\n      Piratla, Vihari  and\n      Lee, Kyunghyun  and\n      Joh, Hyonkeun  and\n      Weller, Adrian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.53/\",\n    pages = \"786--811\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.53.pdf",
        "site": "https://aclanthology.org/2025.coling-main.53/",
        "pdf_size": 1187328,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11082948248791601970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Cambridge; University of Cambridge; McKinsey & Company; Yonsei University; University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;gmail.com;gmail.com;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;gmail.com;gmail.com;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Cambridge;McKinsey & Company;Yonsei University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.mckinsey.com;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Cambridge;McKinsey;Yonsei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "United Kingdom;United States;South Korea"
    },
    {
        "id": "2025.coling-main.58",
        "title": "LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "E-commerce authoring entails creating engaging, diverse, and targeted content to enhance preference elicitation and retrieval experience. While Large Language Models (LLMs) have revolutionized content generation, they often fall short in e-commerce applications due to their limited memorization of domain-specific features. This paper proposes LLaMA-E, the unified e-commerce authoring models that address the contextual preferences of customers, sellers, and platforms, the essential objects in e-commerce operation. We design the instruction set derived from tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general e-commerce Q&A. The instruction formulation ensures the interleaved cover of the presented and required object features, allowing the alignment of base models to parameterize e-commerce knowledge comprehensively. The proposed LLaMA-E models achieve state-of-the-art evaluation performance and exhibit the advantage in zero-shot practical applications. To our knowledge, this is the first LLM tailored to empower authoring applications with comprehensive scenario understanding by integrating features focused on participated objects.",
        "author": "Kaize Shi; Xueyao Sun; Dingxian Wang; Yinlin Fu; Guandong Xu; Qing Li",
        "authorids": "/k/kaize-shi/; /x/xueyao-sun/; /d/dingxian-wang/; /y/yinlin-fu/; /g/guandong-xu/; /q/qing-li/",
        "bibtex": "@inproceedings{shi-etal-2025-llama,\n    title = \"{LL}a{MA}-{E}: Empowering {E}-commerce Authoring with Object-Interleaved Instruction Following\",\n    author = \"Shi, Kaize  and\n      Sun, Xueyao  and\n      Wang, Dingxian  and\n      Fu, Yinlin  and\n      Xu, Guandong  and\n      Li, Qing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.58/\",\n    pages = \"870--885\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.58.pdf",
        "site": "https://aclanthology.org/2025.coling-main.58/",
        "pdf_size": 2310663,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8729587730836783442&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Technology Sydney; University of Technology Sydney + The Hong Kong Polytechnic University; University of Technology Sydney; Etsy; University of Technology Sydney + The Education University of Hong Kong; The Hong Kong Polytechnic University",
        "aff_domain": "uts.edu.au; ; ; ;uts.edu.au; ",
        "email": "uts.edu.au; ; ; ;uts.edu.au; ",
        "github": "https://github.com/KZ-Shi/LLaMA-E",
        "project": "https://huggingface.co/spaces/KaizeShi/LLaMA-E#/",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;2;0+3;1",
        "aff_unique_norm": "University of Technology Sydney;The Hong Kong Polytechnic University;Etsy;The Education University of Hong Kong",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uts.edu.au;https://www.polyu.edu.hk;https://www.etsy.com;https://www.eduhk.hk",
        "aff_unique_abbr": "UTS;PolyU;Etsy;EdUHK",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0+1;0;2;0+1;1",
        "aff_country_unique": "Australia;China;United States"
    },
    {
        "id": "2025.coling-main.610",
        "title": "LOG: A Local-to-Global Optimization Approach for Retrieval-based Explainable Multi-Hop Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-hop question answering (MHQA) aims to utilize multi-source intensive documents retrieved to derive the answer. However, it is very challenging to model the importance of knowledge retrieved. Previous approaches primarily emphasize single-step and multi-step iterative decomposition or retrieval, which are susceptible to failure in long-chain reasoning due to the progressive accumulation of erroneous information. To address this problem, we propose a novel Local-tO-Global optimized retrieval method (LOG) to discover more beneficial information, facilitating the MHQA. In particular, we design a pointwise conditional v-information based local information modeling to cover usable documents with reasoning knowledge. We also improve tuplet objective loss, advancing multi-examples-aware global optimization to model the relationship between scattered documents. Extensive experimental results demonstrate our proposed method outperforms prior state-of-the-art models, and it can significantly improve multi-hop reasoning, notably for long-chain reasoning.",
        "author": "Hao Xu; Yunxiao Zhao; Jiayang Zhang; Zhiqiang Wang; Ru Li",
        "authorids": "/h/hao-xu/; /y/yunxiao-zhao/; /j/jiayang-zhang/; /z/zhiqiang-wang/; /r/ru-li/",
        "bibtex": "@inproceedings{xu-etal-2025-log,\n    title = \"{LOG}: A Local-to-Global Optimization Approach for Retrieval-based Explainable Multi-Hop Question Answering\",\n    author = \"Xu, Hao  and\n      Zhao, Yunxiao  and\n      Zhang, Jiayang  and\n      Wang, Zhiqiang  and\n      Li, Ru\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.610/\",\n    pages = \"9085--9095\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.610.pdf",
        "site": "https://aclanthology.org/2025.coling-main.610/",
        "pdf_size": 1896653,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rL-GTfXbU4MJ:scholar.google.com/&scioq=LOG:+A+Local-to-Global+Optimization+Approach+for+Retrieval-based+Explainable+Multi-Hop+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China",
        "aff_domain": "163.com;163.com;sxu.edu.cn;sxu.edu.cn; ",
        "email": "163.com;163.com;sxu.edu.cn;sxu.edu.cn; ",
        "github": "https://github.com/yunxiaomr/LOG",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_unique_norm": "Shanxi University",
        "aff_unique_dep": "School of Computer and Information Technology",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_campus_unique": "Taiyuan",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.428",
        "title": "LOLA \u2013 An Open-Source Massively Multilingual Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents LOLA, a massively multilingual large language model trained on more than 160 languages using a sparse Mixture-of-Experts Transformer architecture. Our architectural and implementation choices address the challenge of harnessing linguistic diversity while maintaining efficiency and avoiding the common pitfalls of multilinguality. Our analysis of the evaluation results shows competitive performance in natural language generation and understanding tasks. Additionally, we demonstrate how the learned expert-routing mechanism exploits implicit phylogenetic linguistic patterns to potentially alleviate the curse of multilinguality. We provide an in-depth look at the training process, an analysis of the datasets, and a balanced exploration of the model\u2019s strengths and limitations. As an open-source model, LOLA promotes reproducibility and serves as a robust foundation for future research. Our findings enable the development of compute-efficient multilingual models with strong, scalable performance across languages.",
        "author": "Nikit Srivastava; Denis Kuchelev; Tatiana Moteu Ngoli; Kshitij Shetty; Michael Roeder; Hamada Zahera; Diego Moussallem; Axel-Cyrille Ngonga Ngomo",
        "authorids": "/n/nikit-srivastava/; /d/denis-kuchelev/; /t/tatiana-moteu-ngoli/; /k/kshitij-shetty/; /m/michael-roeder/; /h/hamada-zahera/; /d/diego-moussallem/; /a/axel-cyrille-ngonga-ngomo/",
        "bibtex": "@inproceedings{srivastava-etal-2025-lola,\n    title = \"{LOLA} {--} An Open-Source Massively Multilingual Large Language Model\",\n    author = \"Srivastava, Nikit  and\n      Kuchelev, Denis  and\n      Moteu Ngoli, Tatiana  and\n      Shetty, Kshitij  and\n      Roeder, Michael  and\n      Zahera, Hamada  and\n      Moussallem, Diego  and\n      Ngonga Ngomo, Axel-Cyrille\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.428/\",\n    pages = \"6420--6446\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.428.pdf",
        "site": "https://aclanthology.org/2025.coling-main.428/",
        "pdf_size": 1852802,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12735830217815672647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "github.com/dice-group/LOLA",
        "project": "huggingface.co/dice-research/lola_v1",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.132",
        "title": "LTRS: Improving Word Sense Disambiguation via Learning to Rank Senses",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Word Sense Disambiguation (WSD) is a fundamental task critical for accurate semantic understanding. Conventional training strategies usually only consider predefined senses for target words and learn each of them from relatively limited instances, neglecting the influence of similar ones. To address these problems, we propose the method of Learning to Rank Senses (LTRS) to enhance the task. This method helps a model learn to represent and disambiguate senses from a broadened range of instances via ranking an expanded list of sense definitions. By employing LTRS, our model achieves a SOTA F1 score of 79.6% in Chinese WSD and exhibits robustness in low-resource settings. Moreover, it shows excellent training efficiency, achieving faster convergence than previous methods. This provides a new technical approach to WSD and may also apply to the task for other languages.",
        "author": "Hansi Wang; Yue Wang; Qiliang Liang; Yang Liu",
        "authorids": "/h/hansi-wang/; /y/yue-wang/; /q/qiliang-liang/; /y/yang-liu/",
        "bibtex": "@inproceedings{wang-etal-2025-ltrs,\n    title = \"{LTRS}: Improving Word Sense Disambiguation via Learning to Rank Senses\",\n    author = \"Wang, Hansi  and\n      Wang, Yue  and\n      Liang, Qiliang  and\n      Liu, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.132/\",\n    pages = \"1934--1942\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.132.pdf",
        "site": "https://aclanthology.org/2025.coling-main.132/",
        "pdf_size": 870233,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:j0USn1judyIJ:scholar.google.com/&scioq=LTRS:+Improving+Word+Sense+Disambiguation+via+Learning+to+Rank+Senses&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;gmail.com;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;gmail.com;pku.edu.cn",
        "github": "https://github.com/COOLPKU/LTRS",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "National Key Laboratory for Multimedia Information Processing",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-demos.11",
        "title": "LUCE: A Dynamic Framework and Interactive Dashboard for Opinionated Text Analysis",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We introduce LUCE, an advanced dynamic framework with an interactive dashboard for analysing opinionated text aiming to understand people-centred communication. The framework features computational modules of text classification and extraction explicitly designed for analysing different elements of opinions, e.g., sentiment/emotion, suggestion, figurative language, hate/toxic speech, and topics. We designed the framework using a modular architecture, allowing scalability and extensibility with the aim of supporting other NLP tasks in subsequent versions. LUCE comprises trained models, python-based APIs, and a user-friendly dashboard, ensuring an intuitive user experience. LUCE has been validated in a relevant environment, and its capabilities and performance have been demonstrated through initial prototypes and pilot studies.",
        "author": "Omnia Zayed; Gaurav Negi; Sampritha Hassan Manjunath; Devishree Pillai; Paul Buitelaar",
        "authorids": "/o/omnia-zayed/; /g/gaurav-negi/; /s/sampritha-hassan-manjunath/; /d/devishree-pillai/; /p/paul-buitelaar/",
        "bibtex": "@inproceedings{zayed-etal-2025-luce,\n    title = \"{LUCE}: A Dynamic Framework and Interactive Dashboard for Opinionated Text Analysis\",\n    author = \"Zayed, Omnia  and\n      Negi, Gaurav  and\n      Manjunath, Sampritha Hassan  and\n      Pillai, Devishree  and\n      Buitelaar, Paul\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.11/\",\n    pages = \"104--116\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.11.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.11/",
        "pdf_size": 14573452,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:eAzfAYQKutUJ:scholar.google.com/&scioq=LUCE:+A+Dynamic+Framework+and+Interactive+Dashboard+for+Opinionated+Text+Analysis&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Insight SFI Research Centre for Data Analytics; Insight SFI Research Centre for Data Analytics; Insight SFI Research Centre for Data Analytics; Insight SFI Research Centre for Data Analytics; Insight SFI Research Centre for Data Analytics",
        "aff_domain": "universityofgalway.ie;universityofgalway.ie;universityofgalway.ie;universityofgalway.ie;universityofgalway.ie",
        "email": "universityofgalway.ie;universityofgalway.ie;universityofgalway.ie;universityofgalway.ie;universityofgalway.ie",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Insight Centre for Data Analytics",
        "aff_unique_dep": "Research Centre for Data Analytics",
        "aff_unique_url": "https://www.insight-centre.org",
        "aff_unique_abbr": "Insight Centre",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Ireland"
    },
    {
        "id": "2025.coling-main.451",
        "title": "LaERC-S: Improving LLM-based Emotion Recognition in Conversation with Speaker Characteristics",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Emotion recognition in conversation (ERC), the task of discerning human emotions for each utterance within a conversation, has garnered significant attention in human-computer interaction systems. Previous ERC studies focus on speaker-specific information that predominantly stems from relationships among utterances, which lacks sufficient information around conversations. Recent research in ERC has sought to exploit pre-trained large language models (LLMs) with speaker modelling to comprehend emotional states. Although these methods have achieved the encouraging results, the extracted speaker-specific information struggles to indicate emotional dynamics. In this paper, motivated by the fact that speaker characteristics play a crucial role and LLMs have rich world knowledge, we present LaERC-S, a novel framework that stimulates LLMs to explore speaker characteristics involving the mental state and behavior of interlocutors, for accurate emotion predictions. To endow LLMs with these knowledge information, we adopt the two-stage learning to make the models reason speaker characteristics and track the emotion of the speaker in complex conversation scenarios. Extensive experiments on three benchmark datasets demonstrate the superiority of LaERC-S, reaching the new state-of-the-art.",
        "author": "Yumeng Fu; Junjie Wu; Zhongjie Wang; Meishan Zhang; Lili Shan; Yulin Wu; Bingquan Liu",
        "authorids": "/y/yumeng-fu/; /j/junjie-wu/; /z/zhongjie-wang/; /m/meishan-zhang/; /l/lili-shan/; /y/yulin-wu/; /b/bingquan-liu/",
        "bibtex": "@inproceedings{fu-etal-2025-laerc,\n    title = \"{L}a{ERC}-{S}: Improving {LLM}-based Emotion Recognition in Conversation with Speaker Characteristics\",\n    author = \"Fu, Yumeng  and\n      Wu, Junjie  and\n      Wang, Zhongjie  and\n      Zhang, Meishan  and\n      Shan, Lili  and\n      Wu, Yulin  and\n      Liu, Bingquan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.451/\",\n    pages = \"6748--6761\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.451.pdf",
        "site": "https://aclanthology.org/2025.coling-main.451/",
        "pdf_size": 1170369,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:N6NFBFSHM9IJ:scholar.google.com/&scioq=LaERC-S:+Improving+LLM-based+Emotion+Recognition+in+Conversation+with+Speaker+Characteristics&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China",
        "aff_domain": "stu.hit.edu.cn;stu.suda.edu.cn;insun.hit.edu.cn;gmail.com;hit.edu.cn;hit.edu.cn;cs.hitsz.edu.cn",
        "email": "stu.hit.edu.cn;stu.suda.edu.cn;insun.hit.edu.cn;gmail.com;hit.edu.cn;hit.edu.cn;cs.hitsz.edu.cn",
        "github": "https://github.com/bigcat-1/LaERC-S",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science and Technology",
        "aff_unique_url": "http://www.hit.edu.cn/;http://www.soochow.edu.cn",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0;1;2;2;0;2;0",
        "aff_campus_unique": "Harbin;Suzhou;Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.480",
        "title": "Language Adaptation of Large Language Models: An Empirical Study on LLaMA2",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "There has been a surge of interest regarding language adaptation of Large Language Models (LLMs) to enhance the processing of texts in low-resource languages. While traditional language models have seen extensive research on language transfer, modern LLMs still necessitate further explorations in language adaptation. In this paper, we present a systematic review of the language adaptation process for LLMs, including vocabulary expansion, continued pre-training, and instruction fine-tuning, which focuses on empirical studies conducted on LLaMA2 and discussions on various settings affecting the model\u2019s capabilities. This study provides helpful insights covering the entire language adaptation process, and highlights the compatibility and interactions between different steps, offering researchers a practical guidebook to facilitate the effective adaptation of LLMs across different languages.",
        "author": "Shumin Wang; Yuexiang Xie; Bolin Ding; Jinyang Gao; Yanyong Zhang",
        "authorids": "/s/shumin-wang/; /y/yuexiang-xie/; /b/bolin-ding/; /j/jinyang-gao/; /y/yanyong-zhang/",
        "bibtex": "@inproceedings{wang-etal-2025-language,\n    title = \"Language Adaptation of Large Language Models: An Empirical Study on {LL}a{MA}2\",\n    author = \"Wang, Shumin  and\n      Xie, Yuexiang  and\n      Ding, Bolin  and\n      Gao, Jinyang  and\n      Zhang, Yanyong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.480/\",\n    pages = \"7195--7208\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.480.pdf",
        "site": "https://aclanthology.org/2025.coling-main.480/",
        "pdf_size": 826772,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9776482610807206023&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; Alibaba Group; Alibaba Group; Alibaba Group; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Science and Technology of China;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "USTC;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.47",
        "title": "Language Models Encode the Value of Numbers Linearly",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: how language models encode the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs encode the value of numbers linearly, offering insights for better exploring, designing, and utilizing numeric information in LLMs.",
        "author": "Fangwei Zhu; Damai Dai; Zhifang Sui",
        "authorids": "/f/fangwei-zhu/; /d/damai-dai/; /z/zhifang-sui/",
        "bibtex": "@inproceedings{zhu-etal-2025-language,\n    title = \"Language Models Encode the Value of Numbers Linearly\",\n    author = \"Zhu, Fangwei  and\n      Dai, Damai  and\n      Sui, Zhifang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.47/\",\n    pages = \"693--709\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.47.pdf",
        "site": "https://aclanthology.org/2025.coling-main.47/",
        "pdf_size": 4353711,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=400978595471319244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University; School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University; School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/solitaryzero/NumProbe",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.257",
        "title": "Language Models at the Syntax-Semantics Interface: A Case Study of the Long-Distance Binding of Chinese Reflexive Ziji",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores whether language models can effectively resolve the complex binding patterns of the Mandarin Chinese reflexive ziji, which are constrained by both syntactic and semantic factors. We construct a dataset of 320 synthetic sentences using templates and examples from syntactic literature, along with 360 natural sentences from the BCC corpus. Evaluating 21 language models against this dataset and comparing their performance to judgments from native Mandarin speakers, we find that none of the models consistently replicates human-like judgments. The results indicate that existing language models tend to rely heavily on sequential cues, though not always favoring the closest strings, and often overlooking subtle semantic and syntactic constraints. They tend to be more sensitive to noun-related than verb-related semantics.",
        "author": "Xiulin Yang",
        "authorids": "/x/xiulin-yang/",
        "bibtex": "@inproceedings{yang-2025-language,\n    title = \"Language Models at the Syntax-Semantics Interface: A Case Study of the Long-Distance Binding of {C}hinese Reflexive Ziji\",\n    author = \"Yang, Xiulin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.257/\",\n    pages = \"3808--3824\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.257.pdf",
        "site": "https://aclanthology.org/2025.coling-main.257/",
        "pdf_size": 701798,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9fjkeDd0ptYJ:scholar.google.com/&scioq=Language+Models+at+the+Syntax-Semantics+Interface:+A+Case+Study+of+the+Long-Distance+Binding+of+Chinese+Reflexive+Ziji&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "Georgetown University",
        "aff_domain": "georgetown.edu",
        "email": "georgetown.edu",
        "github": "https://github.com/xiulinyang/zh-reflexive",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Georgetown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.georgetown.edu",
        "aff_unique_abbr": "GU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.118",
        "title": "Language Models over Large-Scale Knowledge Base: on Capacity, Flexibility and Reasoning for New Facts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Advancements in language models (LMs) have sparked interest in exploring their potential as knowledge bases (KBs) due to their high capability for storing huge amounts of factual knowledge and semantic understanding. However, existing studies face challenges in quantifying the extent of large-scale knowledge packed into LMs and lack systematic studies on LMs\u2019 structured reasoning capabilities over the infused knowledge. Addressing these gaps, our research investigates whether LMs can effectively act as large-scale KBs after training over an expansive set of world knowledge triplets via addressing the following three crucial questions: (1) How do LMs of different sizes perform at storing world knowledge of different frequencies in a large-scale KB? (2) How flexible are these LMs in recalling the stored knowledge when prompted with natural language queries? (3) After training on the abundant world knowledge, can LMs additionally gain the ability to reason over such information to infer new facts? Our findings indicate that while medium-scaled LMs hold promise as world knowledge bases capable of storing and responding with flexibility, enhancements in their reasoning capabilities are necessary to fully realize their potential.",
        "author": "Qiyuan He; Yizhong Wang; Jianfei Yu; Wenya Wang",
        "authorids": "/q/qiyuan-he/; /y/yizhong-wang/; /j/jianfei-yu/; /w/wenya-wang/",
        "bibtex": "@inproceedings{he-etal-2025-language,\n    title = \"Language Models over Large-Scale Knowledge Base: on Capacity, Flexibility and Reasoning for New Facts\",\n    author = \"He, Qiyuan  and\n      Wang, Yizhong  and\n      Yu, Jianfei  and\n      Wang, Wenya\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.118/\",\n    pages = \"1736--1753\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.118.pdf",
        "site": "https://aclanthology.org/2025.coling-main.118/",
        "pdf_size": 903158,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WBbds3QswiYJ:scholar.google.com/&scioq=Language+Models+over+Large-Scale+Knowledge+Base:+on+Capacity,+Flexibility+and+Reasoning+for+New+Facts&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "College of Computing and Data Science, Nanyang Technological University; Paul G. Allen School of Computer Science & Engineering, University of Washington; School of Computer Science and Engineering, Nanjing University of Science and Technology; College of Computing and Data Science, Nanyang Technological University",
        "aff_domain": "ntu.edu.sg;cs.uw.edu;njust.edu.cn;ntu.edu.sg",
        "email": "ntu.edu.sg;cs.uw.edu;njust.edu.cn;ntu.edu.sg",
        "github": "https://github.com/hyanique/lmkb-at-scale",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Nanyang Technological University;University of Washington;Nanjing University of Science and Technology",
        "aff_unique_dep": "College of Computing and Data Science;Paul G. Allen School of Computer Science & Engineering;School of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.cs.washington.edu;http://www.nust.edu.cn",
        "aff_unique_abbr": "NTU;UW;NUST",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Seattle;Nanjing",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Singapore;United States;China"
    },
    {
        "id": "2025.coling-main.233",
        "title": "Large Language Model as a Teacher for Zero-shot Tagging at Extreme Scales",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Extreme Multi-label Text Classification (XMC) entails selecting the most relevant labels for an instance from a vast label set. Extreme Zero-shot XMC (EZ-XMC) extends this challenge by operating without annotated data, relying only on raw text instances and a predefined label set, making it particularly critical for addressing cold-start problems in large-scale recommendation and categorization systems. State-of-the-art methods, such as MACLR and RTS, leverage lightweight bi-encoders but rely on suboptimal pseudo labels for training, such as document titles (MACLR) or document segments (RTS), which may not align well with the intended tagging or categorization tasks. On the other hand, LLM-based approaches, like ICXML, achieve better label-instance alignment but are computationally expensive and impractical for real-world EZ-XMC applications due to their heavy inference costs. In this paper, we introduce LMTX (Large language Model as Teacher for eXtreme classification), a novel framework that bridges the gap between these two approaches. LMTX utilizes an LLM to identify high-quality pseudo labels during training, while employing a lightweight bi-encoder for efficient inference. This design eliminates the need for LLMs at inference time, offering the benefits of improved label alignment without sacrificing computational efficiency. Our approach achieves superior performance and efficiency over both LLM and non-LLM based approaches, establishing a new state-of-the-art in EZ-XMC.",
        "author": "Jinbin Zhang; Nasib Ullah; Rohit Babbar",
        "authorids": "/j/jinbin-zhang/; /n/nasib-ullah/; /r/rohit-babbar/",
        "bibtex": "@inproceedings{zhang-etal-2025-large,\n    title = \"Large Language Model as a Teacher for Zero-shot Tagging at Extreme Scales\",\n    author = \"Zhang, Jinbin  and\n      Ullah, Nasib  and\n      Babbar, Rohit\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.233/\",\n    pages = \"3465--3478\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.233.pdf",
        "site": "https://aclanthology.org/2025.coling-main.233/",
        "pdf_size": 1615715,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Tr_JEJeHnE8J:scholar.google.com/&scioq=Large+Language+Model+as+a+Teacher+for+Zero-shot+Tagging+at+Extreme+Scales&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/xmc-aalto/LMTX",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.500",
        "title": "Large Language Model-Based Event Relation Extraction with Rationales",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event Relation Extraction (ERE) aims to extract various types of relations between different events within texts. Although Large Language Models (LLMs) have demonstrated impressive capabilities in many natural language processing tasks, existing ERE methods based on LLMs still face three key challenges: (1) Time Inefficiency: The existing pairwise method of combining events and determining their relations is time-consuming for LLMs. (2) Low Coverage: When dealing with numerous events in a document, the limited generation length of fine-tuned LLMs restricts the coverage of their extraction results. (3) Lack of Rationale: Essential rationales concerning the results that could enhance the reasoning ability of the model are overlooked. To address these challenges, we propose LLMERE, an LLM-based approach with rationales for the ERE task. LLMERE transforms ERE into a question-and-answer task that may have multiple answers. By extracting all events related to a specified event at once, LLMERE reduces time complexity from O(n2) to O(n), compared to the pairwise method. Subsequently, LLMERE enhances the coverage of extraction results by employing a partitioning strategy that highlights only a portion of the events in the document at a time. In addition to the extracted results, LLMERE is also required to generate corresponding rationales/reasons behind them, in terms of event coreference information or transitive chains of event relations. Experimental results on three widely used datasets show that LLMERE achieves significant improvements over baseline methods.",
        "author": "Zhilei Hu; Zixuan Li; Xiaolong Jin; Long Bai; Jiafeng Guo; Xueqi Cheng",
        "authorids": "/z/zhilei-hu/; /z/zixuan-li/; /x/xiaolong-jin/; /l/long-bai/; /j/jiafeng-guo/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{hu-etal-2025-large,\n    title = \"Large Language Model-Based Event Relation Extraction with Rationales\",\n    author = \"Hu, Zhilei  and\n      Li, Zixuan  and\n      Jin, Xiaolong  and\n      Bai, Long  and\n      Guo, Jiafeng  and\n      Cheng, Xueqi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.500/\",\n    pages = \"7484--7496\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.500.pdf",
        "site": "https://aclanthology.org/2025.coling-main.500/",
        "pdf_size": 681663,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yPFiprOcNDsJ:scholar.google.com/&scioq=Large+Language+Model-Based+Event+Relation+Extraction+with+Rationales&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; School of Computer Science and Technology, University of Chinese Academy of Sciences; CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/HerbertHu/LLMERE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;School of Computer Science and Technology",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.14",
        "title": "Large Language Models are Good Annotators for Type-aware Data Augmentation in Grammatical Error Correction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have achieved outstanding performance across various NLP tasks. Grammatical Error Correction (GEC) is a task aiming at automatically correcting grammatical errors in text, but it encounters a severe shortage of annotated data. Researchers have tried to make full use of the generalization capabilities of LLMs and prompt them to correct erroneous sentences, which however results in unexpected over-correction issues. In this paper, we rethink the role of LLMs in GEC tasks and propose a method, namely TypeDA, considering LLMs as the annotators for type-aware data augmentation in GEC tasks. Different from the existing data augmentation methods, our method prevents in-distribution corruption and is able to generate sentences with multi-granularity error types. Our experiments verify that our method can generally improve the GEC performance of different backbone models with only a small amount of augmented data. Further analyses verify the high consistency and diversity of the pseudo data generated via our method.",
        "author": "Xinyuan Li; Yunshi Lan",
        "authorids": "/x/xinyuan-li/; /y/yunshi-lan/",
        "bibtex": "@inproceedings{li-lan-2025-large,\n    title = \"Large Language Models are Good Annotators for Type-aware Data Augmentation in Grammatical Error Correction\",\n    author = \"Li, Xinyuan  and\n      Lan, Yunshi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.14/\",\n    pages = \"199--213\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.14.pdf",
        "site": "https://aclanthology.org/2025.coling-main.14/",
        "pdf_size": 528526,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5020095278088174642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "East China Normal University; East China Normal University+Shanghai Engineering Research Center of Big Data Management",
        "aff_domain": "stu.ecnu.edu.cn;dase.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;dase.ecnu.edu.cn",
        "github": "https://github.com/LiXinyuan1015/TypeDA",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "East China Normal University;Shanghai Engineering Research Center of Big Data Management",
        "aff_unique_dep": ";Big Data Management",
        "aff_unique_url": "http://www.ecnu.edu.cn;",
        "aff_unique_abbr": "ECNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.300",
        "title": "Large Language Models are good multi-lingual learners : When LLMs meet cross-lingual prompts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.",
        "author": "Teng Wang; Zhenqi He; Wing-Yin Yu; Xiaojin Fu; Xiongwei Han",
        "authorids": "/t/teng-wang/; /z/zhenqi-he/; /w/wing-yin-yu/; /x/xiaojin-fu/; /x/xiongwei-han/",
        "bibtex": "@inproceedings{wang-etal-2025-large,\n    title = \"Large Language Models are good multi-lingual learners : When {LLM}s meet cross-lingual prompts\",\n    author = \"Wang, Teng  and\n      He, Zhenqi  and\n      Yu, Wing-Yin  and\n      Fu, Xiaojin  and\n      Han, Xiongwei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.300/\",\n    pages = \"4442--4456\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.300.pdf",
        "site": "https://aclanthology.org/2025.coling-main.300/",
        "pdf_size": 1387651,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14118691554472222198&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Mathematics, The University of Hong Kong, Hong Kong SAR, China; Department of Mathematics, The University of Hong Kong, Hong Kong SAR, China; Noah\u2019s Ark Lab, Huawei, Hong Kong SAR, China; Noah\u2019s Ark Lab, Huawei, Hong Kong SAR, China; Noah\u2019s Ark Lab, Huawei, Shenzhen, China",
        "aff_domain": "connect.hku.hk;connect.hku.hk;huawei.com;huawei.com;huawei.com",
        "email": "connect.hku.hk;connect.hku.hk;huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "The University of Hong Kong;Huawei",
        "aff_unique_dep": "Department of Mathematics;Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.hku.hk;https://www.huawei.com",
        "aff_unique_abbr": "HKU;Huawei",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Hong Kong SAR;;Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.337",
        "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, increasing attention has been focused on improving the ability of Large Language Models (LLMs) to perform complex reasoning. Advanced methods, such as Chain-of-Thought (CoT) and its variants, are found to enhance their reasoning skills by designing suitable prompts or breaking down complex problems into more manageable sub-problems. However, little concentration has been put on exploring the reasoning process, i.e., we discovered that most methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning (IR). This can make LLMs difficult to solve IR tasks, which are often encountered in the real world. To address this issue, we propose a Direct-Indirect Reasoning (DIR) method, which considers DR and IR as multiple parallel reasoning paths that are merged to derive the final answer. We stimulate LLMs to implement IR by crafting prompt templates incorporating the principles of contrapositive and contradiction. These templates trigger LLMs to assume the negation of the conclusion as true, combine it with the premises to deduce a conclusion, and utilize the logical equivalence of the contrapositive to enhance their comprehension of the rules used in the reasoning process. Our DIR method is simple yet effective and can be straightforwardly integrated with existing variants of CoT methods. Experimental results on four datasets related to logical reasoning and mathematic proof demonstrate that our DIR method, when combined with various baseline methods, significantly outperforms all the original methods.",
        "author": "Yanfang Zhang; Yiliu Sun; Yibing Zhan; Dapeng Tao; Dacheng Tao; Chen Gong",
        "authorids": "/y/yanfang-zhang/; /y/yiliu-sun/; /y/yibing-zhan/; /d/dapeng-tao/; /d/dacheng-tao/; /c/chen-gong/",
        "bibtex": "@inproceedings{zhang-etal-2025-large-language,\n    title = \"Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning\",\n    author = \"Zhang, Yanfang  and\n      Sun, Yiliu  and\n      Zhan, Yibing  and\n      Tao, Dapeng  and\n      Tao, Dacheng  and\n      Gong, Chen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.337/\",\n    pages = \"5040--5057\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.337.pdf",
        "site": "https://aclanthology.org/2025.coling-main.337/",
        "pdf_size": 1451293,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12858592319546167866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Nanjing University of Science and Technology; Nanjing University of Science and Technology; JD Explore Academy; Yunnan University; Nanyang Technological University; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn; ; ; ; ; ",
        "email": "sjtu.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;3;4",
        "aff_unique_norm": "Nanjing University of Science and Technology;JD Explore Academy;Yunnan University;Nanyang Technological University;Shanghai Jiao Tong University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "http://www.nust.edu.cn/;;http://www.ynu.edu.cn;https://www.ntu.edu.sg;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "NUST;;YNU;NTU;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;2;0",
        "aff_country_unique": "China;;Singapore"
    },
    {
        "id": "2025.coling-main.416",
        "title": "Large Language Models with Reinforcement Learning from Human Feedback Approach for Enhancing Explainable Sexism Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in natural language processing, driven by Large Language Models (LLMs), have significantly improved text comprehension, enabling these models to handle complex tasks with greater efficiency. A key feature of LLMs is their ability to engage in contextual learning, which allows them to understand and apply instructions given in natural language to new scenarios without requiring additional training. This capability is particularly valuable in social media, where LLMs can be crucial in addressing challenges in explainable sexism detection. We hypothesize that by leveraging contextual learning capabilities, LLMs can provide clear, explainable insights into why certain content is flagged as problematic, thus enhancing transparency in the sexism detection process. To this end, we propose a Reinforcement Learning from Human Feedback (RLHF) based fine-tuning framework for sexism detection. We studied two well-known LLMs, Mistral-7B and LLaMA-3-8B, in zero-shot, supervised fine-tuning, and RLHF scenarios to conclude the superior ability of LLMs in sexism detection. The experimental results reported in this work, based on three tasks of Explainable Detection of Online Sexism (EDOS), highlight the importance of RLHF for building explainable systems in online discourse. Furthermore, we found that the LLaMA-3-8B model achieves the best results using the RLHF approach, scoring 0.8681 on Task A (binary sexism detection), 0.6829 on Task B (category classification of sexism), and 0.4722 on Task C (fine-grained sexism vectors) test sets.",
        "author": "Ali Riahi Samani; Tianhao Wang; Kangshuo Li; Feng Chen",
        "authorids": "/a/ali-riahi-samani/; /t/tianhao-wang/; /k/kangshuo-li/; /f/feng-chen/",
        "bibtex": "@inproceedings{riahi-samani-etal-2025-large,\n    title = \"Large Language Models with Reinforcement Learning from Human Feedback Approach for Enhancing Explainable Sexism Detection\",\n    author = \"Riahi Samani, Ali  and\n      Wang, Tianhao  and\n      Li, Kangshuo  and\n      Chen, Feng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.416/\",\n    pages = \"6230--6243\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.416.pdf",
        "site": "https://aclanthology.org/2025.coling-main.416/",
        "pdf_size": 3474072,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1377394754878983268&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, The University of Texas at Dallas, Texas, USA; Department of Computer Science, The University of Texas at Dallas, Texas, USA; Department of Computer Science, The University of Texas at Dallas, Texas, USA; Department of Computer Science, The University of Texas at Dallas, Texas, USA",
        "aff_domain": "utdallas.edu;utdallas.edu;utdallas.edu;utdallas.edu",
        "email": "utdallas.edu;utdallas.edu;utdallas.edu;utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utdallas.edu",
        "aff_unique_abbr": "UT Dallas",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Dallas",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.75",
        "title": "Latent Space Interpretation for Stylistic Analysis and Explainable Authorship Attribution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent state-of-the-art authorship attribution methods learn authorship representations of text in a latent, uninterpretable space, which hinders their usability in real-world applications. We propose a novel approach for interpreting learned embeddings by identifying representative points in the latent space and leveraging large language models to generate informative natural language descriptions of the writing style associated with each point. We evaluate the alignment between our interpretable and latent spaces and demonstrate superior prediction agreement over baseline methods. Additionally, we conduct a human evaluation to assess the quality of these style descriptions and validate their utility in explaining the latent space. Finally, we show that human performance on the challenging authorship attribution task improves by +20% on average when aided with explanations from our method.",
        "author": "Milad Alshomary; Narutatsu Ri; Marianna Apidianaki; Ajay Patel; Smaranda Muresan; Kathleen McKeown",
        "authorids": "/m/milad-alshomary/; /n/narutatsu-ri/; /m/marianna-apidianaki/; /a/ajay-patel/; /s/smaranda-muresan/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{alshomary-etal-2025-latent,\n    title = \"Latent Space Interpretation for Stylistic Analysis and Explainable Authorship Attribution\",\n    author = \"Alshomary, Milad  and\n      Ri, Narutatsu  and\n      Apidianaki, Marianna  and\n      Patel, Ajay  and\n      Muresan, Smaranda  and\n      McKeown, Kathleen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.75/\",\n    pages = \"1124--1135\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.75.pdf",
        "site": "https://aclanthology.org/2025.coling-main.75/",
        "pdf_size": 900999,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CrLrJwd4e4cJ:scholar.google.com/&scioq=Latent+Space+Interpretation+for+Stylistic+Analysis+and+Explainable+Authorship+Attribution&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Columbia University; Columbia University; University of Pennsylvania; University of Pennsylvania; Columbia University; Columbia University",
        "aff_domain": "columbia.edu; ; ; ; ; ",
        "email": "columbia.edu; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Columbia University;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.upenn.edu",
        "aff_unique_abbr": "Columbia;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.593",
        "title": "Learn from Failure: Causality-guided Contrastive Learning for Generalizable Implicit Hate Speech Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Implicit hate speech presents a significant challenge for automatic detection systems due to its subtlety and ambiguity. Traditional models trained using empirical risk minimization (ERM) often rely on correlations between class labels and spurious attributes, which leads to poor performance on data lacking these correlations. In this paper, we propose a novel approach using causality-guided contrastive learning (CCL) to enhance the generalizability of implicit hate speech detection. Since ERM tends to identify spurious attributes, CCL works by aligning the representations of samples with the same class but opposite spurious attributes, identified through ERM\u2019s inference failure. This method reduces the model\u2019s reliance on spurious correlations, allowing it to learn more robust features and handle diverse, nuanced contexts better. Our extensive experiments on multiple implicit hate speech datasets show that our approach outperforms current state-of-the-art methods in cross-domain generalization.",
        "author": "Tianming Jiang",
        "authorids": "/t/tianming-jiang/",
        "bibtex": "@inproceedings{jiang-2025-learn,\n    title = \"Learn from Failure: Causality-guided Contrastive Learning for Generalizable Implicit Hate Speech Detection\",\n    author = \"Jiang, Tianming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.593/\",\n    pages = \"8858--8867\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.593.pdf",
        "site": "https://aclanthology.org/2025.coling-main.593/",
        "pdf_size": 1336362,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZT3ItXsbIcwJ:scholar.google.com/&scioq=Learn+from+Failure:+Causality-guided+Contrastive+Learning+for+Generalizable+Implicit+Hate+Speech+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Management, Central China Normal University, Wuhan, China",
        "aff_domain": "ccnu.edu.cn",
        "email": "ccnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Central China Normal University",
        "aff_unique_dep": "School of Information Management",
        "aff_unique_url": "http://www.ccnu.edu.cn",
        "aff_unique_abbr": "CCNU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.171",
        "title": "Learning Transition Patterns by Large Language Models for Sequential Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated powerful performance in sequential recommendation due to their robust language modeling and comprehension capabilities. In such paradigms, the item texts of interaction sequences are formulated as sentences and LLMs are utilized to learn language representations or directly generate target item texts by incorporating instructions. Despite their promise, these methods solely focus on modeling the mapping from sequential texts to target items, neglecting the relationship between the items in an interaction sequence. This results in a failure to learn the transition patterns between items, which reflect the dynamic change in user preferences and are crucial for predicting the next item. To tackle this issue, we propose a novel framework for mapping the sequential item texts to the sequential item IDs, named ST2SI. Specifically, we first introduce multi-query input and item linear projection (ILP) to model the conditional probability distribution of items. Then, we further propose ID alignment to address misalignment between item texts and item IDs by instruction tuning. Finally, we propose efficient ILP tuning to adapt flexibly to different scenarios, requiring only training a linear layer to achieve competitive performance. Extensive experiments on six real-world datasets show our approach outperforms the best baselines by 7.33% in NDCG@10, 4.65% in Recall@10, and 8.42% in MRR.",
        "author": "Jianyang Zhai; Zi-Feng Mai; Dongyi Zheng; Chang-Dong Wang; Xiawu Zheng; Hui Li; Feidiao Yang; Yonghong Tian",
        "authorids": "/j/jianyang-zhai/; /z/zi-feng-mai/; /d/dongyi-zheng/; /c/chang-dong-wang/; /x/xiawu-zheng/; /h/hui-li/; /f/feidiao-yang/; /y/yonghong-tian/",
        "bibtex": "@inproceedings{zhai-etal-2025-learning,\n    title = \"Learning Transition Patterns by Large Language Models for Sequential Recommendation\",\n    author = \"Zhai, Jianyang  and\n      Mai, Zi-Feng  and\n      Zheng, Dongyi  and\n      Wang, Chang-Dong  and\n      Zheng, Xiawu  and\n      Li, Hui  and\n      Yang, Feidiao  and\n      Tian, Yonghong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.171/\",\n    pages = \"2513--2525\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.171.pdf",
        "site": "https://aclanthology.org/2025.coling-main.171/",
        "pdf_size": 632572,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:spMfSQmQ4GsJ:scholar.google.com/&scioq=Learning+Transition+Patterns+by+Large+Language+Models+for+Sequential+Recommendation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Sun Yat-sen University+Pengcheng Laboratory; Sun Yat-sen University; Sun Yat-sen University+Pengcheng Laboratory; Sun Yat-sen University+Guangdong Key Laboratory of Big Data Analysis and Processing; School of Informatics, Xiamen University; School of Informatics, Xiamen University; Pengcheng Laboratory; Pengcheng Laboratory+Peking University",
        "aff_domain": "pcl.ac.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;hotmail.com;xmu.edu.cn;xmu.edu.cn;pcl.ac.cn;pku.edu.cn",
        "email": "pcl.ac.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;hotmail.com;xmu.edu.cn;xmu.edu.cn;pcl.ac.cn;pku.edu.cn",
        "github": "https://github.com/zhaijianyang/ST2SI",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0;0+1;0+2;3;3;1;1+4",
        "aff_unique_norm": "Sun Yat-sen University;Pengcheng Laboratory;Guangdong Key Laboratory of Big Data Analysis and Processing;Xiamen University;Peking University",
        "aff_unique_dep": ";;Big Data Analysis and Processing;School of Informatics;",
        "aff_unique_url": "http://www.sysu.edu.cn/;;;https://www.xmu.edu.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": "SYSU;;;XMU;Peking U",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.281",
        "title": "Learning from Impairment: Leveraging Insights from Clinical Linguistics in Language Modelling Research",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This position paper investigates the potential of integrating insights from language impairment research and its clinical treatment to develop human-inspired learning strategies and evaluation frameworks for language models (LMs). We inspect the theoretical underpinnings underlying some influential linguistically motivated training approaches derived from neurolinguistics and, particularly, aphasiology, aimed at enhancing the recovery and generalization of linguistic skills in aphasia treatment, with a primary focus on those targeting the syntactic domain. We highlight how these insights can inform the design of rigorous assessments for LMs, specifically in their handling of complex syntactic phenomena, as well as their implications for developing human-like learning strategies, aligning with efforts to create more sustainable and cognitively plausible natural language processing (NLP) models.",
        "author": "Dominique Brunato",
        "authorids": "/d/dominique-brunato/",
        "bibtex": "@inproceedings{brunato-2025-learning,\n    title = \"Learning from Impairment: Leveraging Insights from Clinical Linguistics in Language Modelling Research\",\n    author = \"Brunato, Dominique\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.281/\",\n    pages = \"4167--4174\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.281.pdf",
        "site": "https://aclanthology.org/2025.coling-main.281/",
        "pdf_size": 197916,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:7MZW_-Ms8OMJ:scholar.google.com/&scioq=Learning+from+Impairment:+Leveraging+Insights+from+Clinical+Linguistics+in+Language+Modelling+Research&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Istituto di Linguistica Computazionale 'Antonio Zampolli' (CNR-ILC) + ItaliaNLP Lab, Pisa",
        "aff_domain": "ilc.cnr.it",
        "email": "ilc.cnr.it",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Istituto di Linguistica Computazionale 'Antonio Zampolli';Universita di Pisa",
        "aff_unique_dep": "Linguistica Computazionale;ItaliaNLP Lab",
        "aff_unique_url": "http://www.ili.cnr.it;https://www.unipi.it",
        "aff_unique_abbr": "CNR-ILC;UniPi",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pisa",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2025.coling-main.203",
        "title": "Learning to Reason via Self-Iterative Process Feedback for Small Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Small language models (SLMs) are more efficient, cost-effective, and customizable than large language models (LLMs), though they often underperform in specific areas like reasoning. Past methods for enhancing SLMs\u2019 reasoning, such as supervised fine-tuning and distillation, often depend on costly external signals, resulting in SLMs being overly confident with limited supervision signals, thus limiting their abilities. Therefore, this study enables SLMs to learn to reason from self-iterative feedback. By combining odds ratio preference optimization (ORPO), we fine-tune and align SLMs using positive and negative signals generated by themselves. Additionally, we introduce process supervision for rewards in preference alignment by sampling-based inference simulation and process reward models. Compared to Supervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B by 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed method also demonstrated superior out-of-domain generalization capabilities on MMLU_Math and HumanEval.",
        "author": "Kaiyuan Chen; Jin Wang; Xuejie Zhang",
        "authorids": "/k/kaiyuan-chen/; /j/jin-wang/; /x/xuejie-zhang/",
        "bibtex": "@inproceedings{chen-etal-2025-learning,\n    title = \"Learning to Reason via Self-Iterative Process Feedback for Small Language Models\",\n    author = \"Chen, Kaiyuan  and\n      Wang, Jin  and\n      Zhang, Xuejie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.203/\",\n    pages = \"3027--3042\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.203.pdf",
        "site": "https://aclanthology.org/2025.coling-main.203/",
        "pdf_size": 1204531,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18107751139682205029&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Information Science and Engineering, Yunnan University; School of Information Science and Engineering, Yunnan University; School of Information Science and Engineering, Yunnan University",
        "aff_domain": "stu.ynu.edu.cn;ynu.edu.cn;ynu.edu.cn",
        "email": "stu.ynu.edu.cn;ynu.edu.cn;ynu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Yunnan University",
        "aff_unique_dep": "School of Information Science and Engineering",
        "aff_unique_url": "http://www.ynu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.114",
        "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) exhibit remarkable capabilities in understanding and generating natural language. However, these models can inadvertently memorize private information, posing significant privacy risks. This study addresses the challenge of enabling LLMs to protect specific individuals\u2019 private data without the need for complete retraining. We propose RETURN, a Real-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from Wikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods for protecting personal data in a realistic scenario. Additionally, we introduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection, which enables the model to learn which individuals\u2019 information should be protected without affecting its ability to answer questions related to other unrelated individuals. Our extensive experiments demonstrate that NAUF achieves a state-of-the-art average unlearning score, surpassing the best baseline method by 5.65 points, effectively protecting target individuals\u2019 personal data while maintaining the model\u2019s general capabilities.",
        "author": "Zhenhua Liu; Tong Zhu; Chuanyuan Tan; Wenliang Chen",
        "authorids": "/z/zhenhua-liu/; /t/tong-zhu/; /c/chuanyuan-tan/; /w/wenliang-chen/",
        "bibtex": "@inproceedings{liu-etal-2025-learning,\n    title = \"Learning to Refuse: Towards Mitigating Privacy Risks in {LLM}s\",\n    author = \"Liu, Zhenhua  and\n      Zhu, Tong  and\n      Tan, Chuanyuan  and\n      Chen, Wenliang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.114/\",\n    pages = \"1683--1698\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.114.pdf",
        "site": "https://aclanthology.org/2025.coling-main.114/",
        "pdf_size": 1017903,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4019593800777741136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "https://github.com/zhliu0106/learning-to-refuse",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Institute of Artificial Intelligence, School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.49",
        "title": "Learning to Rewrite Negation Queries in Product Search",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In product search, negation is frequently used to articulate unwanted product features or components. Modern search engines often struggle to comprehend negations, resulting in suboptimal user experiences. While various methods have been proposed to tackle negations in search, none of them took the vocabulary gap between query keywords and product text into consideration. In this work, we introduced a query rewriting approach to enhance the performance of product search engines when dealing with queries with negations. First, we introduced a data generation workflow that leverages large language models (LLMs) to extract query rewrites from product text. Subsequently, we trained a Seq2Seq model to generate query rewrite for unseen queries. Our experiments demonstrated that query rewriting yields a 3.17% precision@30 improvement for queries with negations. The promising results pave the way for further research on enhancing the search performance of queries with negations.",
        "author": "Mengtian Guo; Mutasem Al-Darabsah; Choon Hui Teo; Jonathan May; Tarun Agarwal; Rahul Bhagat",
        "authorids": "/m/mengtian-guo/; /m/mutasem-al-darabsah/; /c/choon-hui-teo/; /j/jonathan-may/; /t/tarun-agarwal/; /r/rahul-bhagat/",
        "bibtex": "@inproceedings{guo-etal-2025-learning,\n    title = \"Learning to Rewrite Negation Queries in Product Search\",\n    author = \"Guo, Mengtian  and\n      Al-Darabsah, Mutasem  and\n      Teo, Choon Hui  and\n      May, Jonathan  and\n      Agarwal, Tarun  and\n      Bhagat, Rahul\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.49/\",\n    pages = \"575--582\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.49.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.49/",
        "pdf_size": 300836,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:OBtkEbgVxI4J:scholar.google.com/&scioq=Learning+to+Rewrite+Negation+Queries+in+Product+Search&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "UNC at Chapel Hill; Amazon Search; Amazon Search; Amazon Search; Amazon Search; Amazon Search",
        "aff_domain": "email.unc.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "email.unc.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Amazon",
        "aff_unique_dep": ";Amazon Search",
        "aff_unique_url": "https://www.unc.edu;https://www.amazon.com",
        "aff_unique_abbr": "UNC;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.16",
        "title": "Learning to Verify Summary Facts with Fine-Grained LLM Feedback",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Training automatic summary fact verifiers often faces the challenge of a lack of human-labeled data. In this paper, we explore alternative way of leveraging Large Language Model (LLM) generated feedback to address the inherent limitation of using human-labeled data. We introduce FineSumFact, a large-scale dataset containing fine-grained factual feedback on summaries. We employ 10 distinct LLMs for diverse summary generation and Llama-3-70B-Instruct for feedback. We utilize this dataset to fine-tune the lightweight open-source model Llama-3-8B-Instruct, optimizing resource efficiency while maintaining high performance. Our experimental results reveal that the model trained on extensive LLM-generated datasets surpasses that trained on smaller human-annotated datasets when evaluated using human-generated test sets. Fine-tuning fact verification models with LLM feedback can be more effective and cost-efficient than using human feedback. The dataset is available at https://github.com/DISL-Lab/FineSumFact.",
        "author": "Jihwan Oh; Jeonghwan Choi; Nicole Hee-Yoen Kim; Taewon Yun; Hwanjun Song",
        "authorids": "/j/jihwan-oh/; /j/jeonghwan-choi/; /n/nicole-hee-yoen-kim/; /t/taewon-yun/; /h/hwanjun-song/",
        "bibtex": "@inproceedings{oh-etal-2025-learning,\n    title = \"Learning to Verify Summary Facts with Fine-Grained {LLM} Feedback\",\n    author = \"Oh, Jihwan  and\n      Choi, Jeonghwan  and\n      Kim, Nicole Hee-Yoen  and\n      Yun, Taewon  and\n      Song, Hwanjun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.16/\",\n    pages = \"230--242\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.16.pdf",
        "site": "https://aclanthology.org/2025.coling-main.16/",
        "pdf_size": 866032,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6107956494903229808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/DISL-Lab/FineSumFact",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.508",
        "title": "Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to remarkable performances across various domains. However, this progress is accompanied by a substantial surge in the resource consumption of these models. We address this pressing issue by introducing a new approach, Token Reduction using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without sacrificing their performance. Inspired by human attention patterns in Visual Question Answering (VQA) tasks, TRIM presents a fresh perspective on the selection and reduction of image tokens. The TRIM method has been extensively tested across 12 datasets, and the results demonstrate a significant reduction in computational overhead while maintaining a consistent level of performance. This research marks a critical stride in efficient MLLM development, promoting greater accessibility and sustainability of high-performing models.",
        "author": "Dingjie Song; Wenjun Wang; Shunian Chen; Xidong Wang; Michael X. Guan; Benyou Wang",
        "authorids": "/d/dingjie-song/; /w/wenjun-wang/; /s/shunian-chen/; /x/xidong-wang/; /m/michael-x-guan/; /b/benyou-wang/",
        "bibtex": "@inproceedings{song-etal-2025-less,\n    title = \"Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal {LLM}s\",\n    author = \"Song, Dingjie  and\n      Wang, Wenjun  and\n      Chen, Shunian  and\n      Wang, Xidong  and\n      Guan, Michael X.  and\n      Wang, Benyou\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.508/\",\n    pages = \"7614--7623\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.508.pdf",
        "site": "https://aclanthology.org/2025.coling-main.508/",
        "pdf_size": 656466,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8092998413652359404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The Chinese University of Hong Kong, Shenzhen\u2660Lehigh University; The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen\u2660Lehigh University",
        "aff_domain": "; ; ; ; ;cuhk.edu.cn",
        "email": "; ; ; ; ;cuhk.edu.cn",
        "github": "https://github.com/FreedomIntelligence/TRIM/",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.cn",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.695",
        "title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs\u2019 ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs\u2019 capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further study. We believe that the benchmark we provide will become one of the key data resources for improving LLMs\u2019 Chinese question-answering ability in the future.",
        "author": "Zhikun Xu; Yinghui Li; Ruixue Ding; Xinyu Wang; Boli Chen; Yong Jiang; Haitao Zheng; Wenlian Lu; Pengjun Xie; Fei Huang",
        "authorids": "/z/zhikun-xu/; /y/yinghui-li/; /r/ruixue-ding/; /x/xinyu-wang/; /b/boli-chen/; /y/yong-jiang/; /h/haitao-zheng/; /w/wenlian-lu/; /p/pengjun-xie/; /f/fei-huang/",
        "bibtex": "@inproceedings{xu-etal-2025-llms,\n    title = \"Let {LLM}s Take on the Latest Challenges! A {C}hinese Dynamic Question Answering Benchmark\",\n    author = \"Xu, Zhikun  and\n      Li, Yinghui  and\n      Ding, Ruixue  and\n      Wang, Xinyu  and\n      Chen, Boli  and\n      Jiang, Yong  and\n      Zheng, Haitao  and\n      Lu, Wenlian  and\n      Xie, Pengjun  and\n      Huang, Fei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.695/\",\n    pages = \"10435--10448\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.695.pdf",
        "site": "https://aclanthology.org/2025.coling-main.695/",
        "pdf_size": 1605713,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5527923561330592491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Fudan University+Institute for Intelligent Computing, Alibaba Group; Tsinghua University+Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; Tsinghua University; Fudan University; Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com; ; ; ; ; ; ; ; ",
        "email": "alibaba-inc.com;alibaba-inc.com; ; ; ; ; ; ; ; ",
        "github": "https://github.com/Alibaba-NLP/CDQA",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;2+1;1;1;1;1;2;0;1;1",
        "aff_unique_norm": "Fudan University;Alibaba Group;Tsinghua University",
        "aff_unique_dep": ";Institute for Intelligent Computing;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.alibabagroup.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Fudan;Alibaba;THU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.630",
        "title": "Let\u2019s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) are composed of neurons that exhibit various behaviors and roles, which become increasingly diversified as models scale. Recent studies have revealed that not all neurons are active across different datasets, and this sparsity correlates positively with the task-specific ability, leading to advancements in model pruning and training efficiency. Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary. In contrast, Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of trainable parameters, yet they still operate at a relatively macro scale (e.g., layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach that refines the granularity of parameter training down to the individual neuron, enabling a more parameter-efficient fine-tuning model. The experimental results show that NeFT not only exceeded the performance of full-parameter fine-tuning and PEFT but also provided insights into the analysis of neurons. Our code and data are available at: https://github.com/NLP2CT/NeFT.",
        "author": "Haoyun Xu; Runzhe Zhan; Yingpeng Ma; Derek F. Wong; Lidia S. Chao",
        "authorids": "/h/haoyun-xu/; /r/runzhe-zhan/; /y/yingpeng-ma/; /d/derek-f-wong/; /l/lidia-s-chao/",
        "bibtex": "@inproceedings{xu-etal-2025-lets,\n    title = \"Let{'}s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model\",\n    author = \"Xu, Haoyun  and\n      Zhan, Runzhe  and\n      Ma, Yingpeng  and\n      Wong, Derek F.  and\n      Chao, Lidia S.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.630/\",\n    pages = \"9393--9406\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.630.pdf",
        "site": "https://aclanthology.org/2025.coling-main.630/",
        "pdf_size": 2777768,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16322157558864708331&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "NLP2CT Lab, Department of Computer and Information Science, University of Macau+Tiger Research, Shanghai; NLP2CT Lab, Department of Computer and Information Science, University of Macau; NLP2CT Lab, Department of Computer and Information Science, University of Macau; NLP2CT Lab, Department of Computer and Information Science, University of Macau; NLP2CT Lab, Department of Computer and Information Science, University of Macau",
        "aff_domain": "nlp2ctgmail.com;nlp2ctgmail.com;nlp2ctgmail.com;um.edu.mo;um.edu.mo",
        "email": "nlp2ctgmail.com;nlp2ctgmail.com;nlp2ctgmail.com;um.edu.mo;um.edu.mo",
        "github": "https://github.com/NLP2CT/NeFT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "University of Macau;Tiger Research",
        "aff_unique_dep": "Department of Computer and Information Science;",
        "aff_unique_url": "https://www.um.edu.mo;",
        "aff_unique_abbr": "UM;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Macau SAR;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.152",
        "title": "Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Open-domain dialogue systems need to grasp social commonsense to understand and respond effectively to human users. Commonsense-augmented dialogue models have been proposed that aim to infer commonsense knowledge from dialogue contexts in order to improve response quality. However, existing approaches to commonsense-augmented dialogue rely on implicit reasoning to integrate commonsense inferences during response generation. In this study, we explore the impact of explicit reasoning against implicit reasoning over commonsense for dialogue response generation. Our findings demonstrate that separating commonsense reasoning into explicit steps for generating, selecting, and integrating commonsense into responses leads to better dialogue interactions, improving naturalness, engagement, specificity, and overall quality. Subsequent analyses of these findings unveil insights into the effectiveness of various types of commonsense in generating responses and the particular response traits enhanced through explicit reasoning for commonsense integration. Our work advances research in open-domain dialogue by achieving a new state-of-the-art in commonsense-augmented response generation.",
        "author": "Sarah E. Finch; Jinho D. Choi",
        "authorids": "/s/sarah-e-finch/; /j/jinho-d-choi/",
        "bibtex": "@inproceedings{finch-choi-2025-leveraging,\n    title = \"Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models\",\n    author = \"Finch, Sarah E.  and\n      Choi, Jinho D.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.152/\",\n    pages = \"2222--2235\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.152.pdf",
        "site": "https://aclanthology.org/2025.coling-main.152/",
        "pdf_size": 717576,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16195446793497803437&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Emory University, Atlanta, GA, USA; Department of Computer Science, Emory University, Atlanta, GA, USA",
        "aff_domain": "emory.edu;emory.edu",
        "email": "emory.edu;emory.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Emory University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.emory.edu",
        "aff_unique_abbr": "Emory",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.706",
        "title": "Leveraging LLM-Generated Schema Descriptions for Unanswerable Question Detection in Clinical Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have boosted research on generating SQL queries from domain-specific questions, particularly in the medical domain. A key challenge is detecting and filtering unanswerable questions. Existing methods often relying on model uncertainty, but these require extra resources and lack interpretability. We propose a lightweight model that predicts relevant database schemas to detect unanswerable questions, enhancing interpretability and addressing the data imbalance in binary classification tasks. Furthermore, we found that LLM-generated schema descriptions can significantly enhance the prediction accuracy. Our method provides a resource-efficient solution for unanswerable question detection in domain-specific question answering systems.",
        "author": "Donghee Han; Seungjae Lim; Daeyoung Roh; Sangryul Kim; Sehyun Kim; Mun Yong Yi",
        "authorids": "/d/donghee-han/; /s/seungjae-lim/; /d/daeyoung-roh/; /s/sangryul-kim/; /s/sehyun-kim/; /m/mun-yong-yi/",
        "bibtex": "@inproceedings{han-etal-2025-leveraging,\n    title = \"Leveraging {LLM}-Generated Schema Descriptions for Unanswerable Question Detection in Clinical Data\",\n    author = \"Han, Donghee  and\n      Lim, Seungjae  and\n      Roh, Daeyoung  and\n      Kim, Sangryul  and\n      Kim, Sehyun  and\n      Yi, Mun Yong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.706/\",\n    pages = \"10594--10601\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.706.pdf",
        "site": "https://aclanthology.org/2025.coling-main.706/",
        "pdf_size": 1054771,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4006337725290136781&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.182",
        "title": "Leveraging Language Models for Summarizing Mental State Examinations: A Comprehensive Evaluation and Dataset Release",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Mental health disorders affect a significant portion of the global population, with diagnoses primarily conducted through Mental State Examinations (MSEs). MSEs serve as structured assessments to evaluate behavioral and cognitive functioning across various domains, aiding mental health professionals in diagnosis and treatment monitoring. However, in developing countries, access to mental health support is limited, leading to an overwhelming demand for mental health professionals. Resident doctors often conduct initial patient assessments and create summaries for senior doctors, but their availability is constrained, resulting in extended patient wait times. This study addresses the challenge of generating concise summaries from MSEs through the evaluation of various language models. Given the scarcity of relevant mental health conversation datasets, we developed a 12-item descriptive MSE questionnaire and collected responses from 405 participants, resulting in 9720 utterances covering diverse mental health aspects. Subsequently, we assessed the performance of five well-known pre-trained summarization models, both with and without fine-tuning, for summarizing MSEs. Our comprehensive evaluation, leveraging metrics such as ROUGE, SummaC, and human evaluation, demonstrates that language models can generate automated coherent MSE summaries for doctors. With this paper, we release our collected conversational dataset and trained models publicly for the mental health research community.",
        "author": "Nilesh Kumar Sahu; Manjeet Yadav; Mudita Chaturvedi; Snehil Gupta; Haroon R. Lone",
        "authorids": "/n/nilesh-kumar-sahu/; /m/manjeet-yadav/; /m/mudita-chaturvedi/; /s/snehil-gupta/; /h/haroon-r-lone/",
        "bibtex": "@inproceedings{sahu-etal-2025-leveraging,\n    title = \"Leveraging Language Models for Summarizing Mental State Examinations: A Comprehensive Evaluation and Dataset Release\",\n    author = \"Sahu, Nilesh Kumar  and\n      Yadav, Manjeet  and\n      Chaturvedi, Mudita  and\n      Gupta, Snehil  and\n      Lone, Haroon R.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.182/\",\n    pages = \"2658--2682\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.182.pdf",
        "site": "https://aclanthology.org/2025.coling-main.182/",
        "pdf_size": 452818,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4121136692584560177&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "IISER Bhopal India; IISER Bhopal India; AIIMS Bhopal India; AIIMS Bhopal India; IISER Bhopal India",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Indian Institute of Science Education and Research;All India Institute of Medical Sciences",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iiserbhopal.ac.in;https://www.aiimsbhopal.edu.in",
        "aff_unique_abbr": "IISER Bhopal;AIIMS",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Bhopal",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.372",
        "title": "Leveraging Language-based Representations for Better Solving Symbol-related Problems with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Symbols such as numerical sequences, chemical formulas, and table delimiters exist widely, playing important roles in symbol-related tasks such as abstract reasoning, chemical property prediction, and tabular question-answering. Compared to tasks based on natural language expressions, large language models (LLMs) have limitations in understanding and reasoning on symbol-based representations, making it difficult for them to handle symbol-related problems. In this paper, we propose symbol-to-language (S2L), a method that converts symbol-based representations to language-based representations, providing valuable information for language models during reasoning. We found that, for both closed-source and open-source LLMs, the capability to solve symbol-related problems can be largely enhanced by incorporating such language-based representations. For example, by employing S2L for GPT-4, there can be substantial improvements of +21.9% and +9.5% accuracy for 1D-ARC and Dyck language tasks, respectively. There is also a consistent improvement in other six general symbol-related tasks such as table understanding and Tweet analysis. We release the GPT logs in https://github.com/THUNLP-MT/symbol2language.",
        "author": "Yile Wang; Sijie Cheng; Zixin Sun; Peng Li; Yang Liu",
        "authorids": "/y/yile-wang/; /s/sijie-cheng/; /z/zixin-sun/; /p/peng-li/; /y/yang-liu/",
        "bibtex": "@inproceedings{wang-etal-2025-leveraging,\n    title = \"Leveraging Language-based Representations for Better Solving Symbol-related Problems with Large Language Models\",\n    author = \"Wang, Yile  and\n      Cheng, Sijie  and\n      Sun, Zixin  and\n      Li, Peng  and\n      Liu, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.372/\",\n    pages = \"5544--5557\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.372.pdf",
        "site": "https://aclanthology.org/2025.coling-main.372/",
        "pdf_size": 4358609,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WTbUEGayTYoJ:scholar.google.com/&scioq=Leveraging+Language-based+Representations+for+Better+Solving+Symbol-related+Problems+with+Large+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/THUNLP-MT/symbol2language1",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.509",
        "title": "Leveraging Large Pre-trained Multilingual Models for High-Quality Speech-to-Text Translation on Industry Scenarios",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Speech-to-Text Translation (S2TT) involves converting spoken language from a source language directly into text in a target language. Traditionally, S2TT systems rely on a sequential pipeline that combines Automatic Speech Recognition (ASR) and Machine Translation (MT) models. However, these systems are prone to error propagation and demand substantial resources to develop and train each component independently. Thus, posing a major challenge in industry settings where cost-effective yet highly accurate S2TT solutions are essential. With the increasing availability of multilingual large pre-trained speech models (LPSM), we propose a parameter-efficient framework that integrates one LPSM with a multilingual MT engine. We evaluate the effectiveness of several well-established LPSMs within this framework, focusing on a real-world industry scenario that involves building a system capable of translating between French, English, and Arabic. The results show that high-quality S2TT systems can be built with minimal computational resources, offering an efficient solution for cross-lingual communication.",
        "author": "Marko Avila; Josep Crego",
        "authorids": "/m/marko-avila/; /j/josep-m-crego/",
        "bibtex": "@inproceedings{avila-crego-2025-leveraging,\n    title = \"Leveraging Large Pre-trained Multilingual Models for High-Quality Speech-to-Text Translation on Industry Scenarios\",\n    author = \"Avila, Marko  and\n      Crego, Josep\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.509/\",\n    pages = \"7624--7633\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.509.pdf",
        "site": "https://aclanthology.org/2025.coling-main.509/",
        "pdf_size": 595208,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wOgW2Kp7-lEJ:scholar.google.com/&scioq=Leveraging+Large+Pre-trained+Multilingual+Models+for+High-Quality+Speech-to-Text+Translation+on+Industry+Scenarios&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "SYSTRAN by Chapsvision; SYSTRAN by Chapsvision",
        "aff_domain": "chapsvision.com;chapsvision.com",
        "email": "chapsvision.com;chapsvision.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "SYSTRAN",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.systran.net",
        "aff_unique_abbr": "SYSTRAN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-industry.43",
        "title": "Leveraging Multilingual Models for Robust Grammatical Error Correction Across Low-Resource Languages",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Grammatical Error Correction (GEC) is a crucial task in Natural Language Processing (NLP) aimed at improving the quality of user-generated content, particularly for non-native speakers. This paper introduces a novel end-to-end architecture utilizing the M2M100 multilingual transformer model to build a unified GEC system, with a focus on low-resource languages. A synthetic data generation pipeline is proposed, tailored to address language-specific error categories. The system has been implemented for the Spanish language, showing promising results based on evaluations conducted by linguists with expertise in Spanish. Additionally, we present a user analysis that tracks user interactions, revealing an acceptance rate of 88.2%, as reflected by the actions performed by users.",
        "author": "Divesh Ramesh Kubal; Apurva Shrikant Nagvenkar",
        "authorids": "/d/divesh-ramesh-kubal/; /a/apurva-shrikant-nagvenkar/",
        "bibtex": "@inproceedings{kubal-nagvenkar-2025-leveraging,\n    title = \"Leveraging Multilingual Models for Robust Grammatical Error Correction Across Low-Resource Languages\",\n    author = \"Kubal, Divesh Ramesh  and\n      Nagvenkar, Apurva Shrikant\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.43/\",\n    pages = \"505--510\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.43.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.43/",
        "pdf_size": 266853,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ghRlweuGicwJ:scholar.google.com/&scioq=Leveraging+Multilingual+Models+for+Robust+Grammatical+Error+Correction+Across+Low-Resource+Languages&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Trinka AI; Trinka AI",
        "aff_domain": "trinka.ai;trinka.ai",
        "email": "trinka.ai;trinka.ai",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Trinka AI",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "Trinka AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Unknown"
    },
    {
        "id": "2025.coling-main.417",
        "title": "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with n independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy. Leveraging Large Language Models (LLMs), we propose novel taxonomy-embedded transitional LLM-agnostic framework for multimodality classification. The cornerstone of this advancement is the ability of models to enforce consistency across hierarchical levels. Our evaluations on the MEP-3M dataset - a Multi-modal E-commerce Product dataset with various hierarchical levels- demonstrated a significant performance improvement compared to conventional LLMs structure.",
        "author": "Shijing Chen; Mohamed Reda Bouadjenek; Usman Naseem; Basem Suleiman; Shoaib Jameel; Flora Salim; Hakim Hacid; Imran Razzak",
        "authorids": "/s/shijing-chen/; /m/mohamed-reda-bouadjenek/; /u/usman-naseem/; /b/basem-suleiman/; /s/shoaib-jameel/; /f/flora-salim/; /h/hakim-hacid/; /i/imran-razzak/",
        "bibtex": "@inproceedings{chen-etal-2025-leveraging,\n    title = \"Leveraging Taxonomy and {LLM}s for Improved Multimodal Hierarchical Classification\",\n    author = \"Chen, Shijing  and\n      Bouadjenek, Mohamed Reda  and\n      Naseem, Usman  and\n      Suleiman, Basem  and\n      Jameel, Shoaib  and\n      Salim, Flora  and\n      Hacid, Hakim  and\n      Razzak, Imran\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.417/\",\n    pages = \"6244--6254\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.417.pdf",
        "site": "https://aclanthology.org/2025.coling-main.417/",
        "pdf_size": 977987,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8163432752889997293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.213",
        "title": "Lexicography Saves Lives (LSL): Automatically Translating Suicide-Related Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent years have seen a marked increase in research that aims to identify or predict risk, intention or ideation of suicide. The majority of new tasks, datasets, language models and other resources focus on English and on suicide in the context of Western culture. However, suicide is global issue and reducing suicide rate by 2030 is one of the key goals of the UN\u2019s Sustainable Development Goals. Previous work has used English dictionaries related to suicide to translate into different target languages due to lack of other available resources. Naturally, this leads to a variety of ethical tensions (e.g.: linguistic misrepresentation), where discourse around suicide is not present in a particular culture or country. In this work, we introduce the \u2018Lexicography Saves Lives Project\u2019 to address this issue and make three distinct contributions. First, we outline ethical consideration and provide overview guidelines to mitigate harm in developing suicide-related resources. Next, we translate an existing dictionary related to suicidal ideation into 200 different languages and conduct human evaluations on a subset of translated dictionaries. Finally, we introduce a public website to make our resources available and enable community participation.",
        "author": "Annika Marie Schoene; John E. Ortega; Rodolfo Joel Zevallos; Laura Haaber Ihle",
        "authorids": "/a/annika-marie-schoene/; /j/john-e-ortega/; /r/rodolfo-joel-zevallos/; /l/laura-haaber-ihle/",
        "bibtex": "@inproceedings{schoene-etal-2025-lexicography,\n    title = \"Lexicography Saves Lives ({LSL}): Automatically Translating Suicide-Related Language\",\n    author = \"Schoene, Annika Marie  and\n      Ortega, John E.  and\n      Zevallos, Rodolfo Joel  and\n      Ihle, Laura Haaber\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.213/\",\n    pages = \"3179--3192\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.213.pdf",
        "site": "https://aclanthology.org/2025.coling-main.213/",
        "pdf_size": 279681,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16638132329348370265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Northeastern Universty, Institute for Experiential AI; Northeastern Universty, Institute for Experiential AI; Barcelona Supercomputing Center; Northeastern Universty, Institute for Experiential AI",
        "aff_domain": "gmail.com; ; ; ",
        "email": "gmail.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Northeastern University;Barcelona Supercomputing Center",
        "aff_unique_dep": "Institute for Experiential AI;",
        "aff_unique_url": "https://www.northeastern.edu;https://www.bsc.es",
        "aff_unique_abbr": "NU;BSC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Spain"
    },
    {
        "id": "2025.coling-industry.58",
        "title": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "With the recent proliferation of large language models (LLMs), enterprises have been able to rapidly develop proof-of-concepts and prototypes. As a result, there is a growing need to implement robust guardrails that monitor, quantize and control an LLM\u2019s behavior, ensuring that the use is reliable, safe, accurate and also aligned with the users\u2019 expectations. Previous approaches for filtering out inappropriate user prompts or system outputs, such as LlamaGuard and OpenAI\u2019s MOD API, have achieved significant success by fine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails introduces increased latency and higher maintenance costs, which may not be practical or scalable for cost-efficient deployments. We take a different approach, focusing on fine-tuning a lightweight architecture: Sentence-BERT. This method reduces the model size from LlamaGuard\u2019s 7 billion parameters to approximately 67 million, while maintaining comparable performance on the AEGIS safety benchmark.",
        "author": "Aaron Zheng; Mansi Rana; Andreas Stolcke",
        "authorids": "/a/aaron-zheng/; /m/mansi-rana/; /a/andreas-stolcke/",
        "bibtex": "@inproceedings{zheng-etal-2025-lightweight,\n    title = \"Lightweight Safety Guardrails Using Fine-tuned {BERT} Embeddings\",\n    author = \"Zheng, Aaron  and\n      Rana, Mansi  and\n      Stolcke, Andreas\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.58/\",\n    pages = \"689--696\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.58.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.58/",
        "pdf_size": 378250,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5763943920204224173&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.517",
        "title": "Linear Recency Bias During Training Improves Transformers\u2019 Fit to Reading Times",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent psycholinguistic research has compared human reading times to surprisal estimates from language models to study the factors shaping human sentence processing difficulty. Previous studies have shown a strong fit between surprisal values from Transformers and reading times. However, standard Transformers work with a lossless representation of the entire previous linguistic context, unlike models of human language processing that include memory decay. To bridge this gap, this paper evaluates a modification of the Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to attention scores. Surprisal estimates from a Transformer that includes ALiBi during training and inference show an improved fit to human reading times compared to a standard Transformer baseline. A subsequent analysis of attention heads suggests that ALiBi\u2019s mixture of slopes\u2014which determine the rate of memory decay in each attention head\u2014may play a role in the improvement by helping models with ALiBi to track different kinds of linguistic dependencies.",
        "author": "Christian Clark; Byung-Doh Oh; William Schuler",
        "authorids": "/c/christian-clark/; /b/byung-doh-oh/; /w/william-schuler/",
        "bibtex": "@inproceedings{clark-etal-2025-linear,\n    title = \"Linear Recency Bias During Training Improves Transformers' Fit to Reading Times\",\n    author = \"Clark, Christian  and\n      Oh, Byung-Doh  and\n      Schuler, William\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.517/\",\n    pages = \"7735--7747\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.517.pdf",
        "site": "https://aclanthology.org/2025.coling-main.517/",
        "pdf_size": 585869,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10387671762930067665&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The Ohio State University; New York University; The Ohio State University",
        "aff_domain": "osu.edu;nyu.edu;osu.edu",
        "email": "osu.edu;nyu.edu;osu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "The Ohio State University;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.osu.edu;https://www.nyu.edu",
        "aff_unique_abbr": "OSU;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.126",
        "title": "Linguistic Features Extracted by GPT-4 Improve Alzheimer\u2019s Disease Detection based on Spontaneous Speech",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Alzheimer\u2019s Disease (AD) is a significant and growing public health concern. Investigating alterations in speech and language patterns offers a promising path towards cost-effective and non-invasive early detection of AD on a large scale. Large language models (LLMs), such as GPT, have enabled powerful new possibilities for semantic text analysis. In this study, we leverage GPT-4 to extract five semantic features from transcripts of spontaneous patient speech. The features capture known symptoms of AD, but they are difficult to quantify effectively using traditional methods of computational linguistics. We demonstrate the clinical significance of these features and further validate one of them (\u201cWord-Finding Difficulties\u201d) against a proxy measure and human raters. When combined with established linguistic features and a Random Forest classifier, the GPT-derived features significantly improve the detection of AD. Our approach proves effective for both manually transcribed and automatically generated transcripts, representing a novel and impactful use of recent advancements in LLMs for AD speech analysis.",
        "author": "Jonathan Heitz; Gerold Schneider; Nicolas Langer",
        "authorids": "/j/jonathan-heitz/; /g/gerold-schneider/; /n/nicolas-langer/",
        "bibtex": "@inproceedings{heitz-etal-2025-linguistic,\n    title = \"Linguistic Features Extracted by {GPT}-4 Improve {A}lzheimer{'}s Disease Detection based on Spontaneous Speech\",\n    author = \"Heitz, Jonathan  and\n      Schneider, Gerold  and\n      Langer, Nicolas\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.126/\",\n    pages = \"1850--1864\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.126.pdf",
        "site": "https://aclanthology.org/2025.coling-main.126/",
        "pdf_size": 487109,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3g3IKsFoxH0J:scholar.google.com/&scioq=Linguistic+Features+Extracted+by+GPT-4+Improve+Alzheimer%E2%80%99s+Disease+Detection+based+on+Spontaneous+Speech&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Department of Psychology, University of Zurich, Methods of Plasticity Research, Zurich, Switzerland+Language & Medicine Competence Centre, University of Zurich, Zurich, Switzerland; Department of Computational Linguistics, University of Zurich, Zurich, Switzerland+Language & Medicine Competence Centre, University of Zurich, Zurich, Switzerland; Department of Psychology, University of Zurich, Methods of Plasticity Research, Zurich, Switzerland",
        "aff_domain": "uzh.ch;cl.uzh.ch;psychologie.uzh.ch",
        "email": "uzh.ch;cl.uzh.ch;psychologie.uzh.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0",
        "aff_unique_norm": "University of Zurich",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.unizh.ch",
        "aff_unique_abbr": "UZH",
        "aff_campus_unique_index": "0+0;0+0;0",
        "aff_campus_unique": "Zurich",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2025.coling-main.459",
        "title": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce a novel analysis that leverages linguistic minimal pairs to probe the internal linguistic representations of Large Language Models (LLMs). By measuring the similarity between LLM activation differences across minimal pairs, we quantify the linguistic similarity and gain insight into the linguistic knowledge captured by LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs in three languages, reveal properties of linguistic similarity from four key aspects: consistency across LLMs, relation to theoretical categorizations, dependency to semantic context, and cross-lingual alignment of relevant phenomena. Our findings suggest that 1) linguistic similarity is significantly influenced by training data exposure, leading to higher cross-LLM agreement in higher-resource languages. 2) Linguistic similarity strongly aligns with fine-grained theoretical linguistic categories but weakly with broader ones. 3) Linguistic similarity shows a weak correlation with semantic similarity, showing its context-dependent nature. 4) LLMs exhibit limited cross-lingual alignment in their understanding of relevant linguistic phenomena. This work demonstrates the potential of minimal pairs as a window into the neural representations of language in LLMs, shedding light on the relationship between LLMs and linguistic theory.",
        "author": "Xinyu Zhou; Delong Chen; Samuel Cahyawijaya; Xufeng Duan; Zhenguang Cai",
        "authorids": "/x/xinyu-zhou/; /d/delong-chen/; /s/samuel-cahyawijaya/; /x/xufeng-duan/; /z/zhenguang-cai/",
        "bibtex": "@inproceedings{zhou-etal-2025-linguistic,\n    title = \"Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models\",\n    author = \"Zhou, Xinyu  and\n      Chen, Delong  and\n      Cahyawijaya, Samuel  and\n      Duan, Xufeng  and\n      Cai, Zhenguang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.459/\",\n    pages = \"6866--6888\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.459.pdf",
        "site": "https://aclanthology.org/2025.coling-main.459/",
        "pdf_size": 7395459,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1408627683252622592&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Linguistics and Modern Languages, CUHK + Universit\u00e9 Paris Cit\u00e9 + Sorbonne Universit\u00e9; Department of Electronic and Computer Engineering, HKUST; Cohere + Department of Electronic and Computer Engineering, HKUST; Department of Linguistics and Modern Languages, CUHK; Department of Linguistics and Modern Languages, CUHK + Brain and Mind Institute, CUHK",
        "aff_domain": "gmail.com;connect.ust.hk; ; ; ",
        "email": "gmail.com;connect.ust.hk; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;3;4+3;0;0+0",
        "aff_unique_norm": "Chinese University of Hong Kong;Universit\u00e9 Paris Cit\u00e9;Sorbonne Universit\u00e9;Hong Kong University of Science and Technology;Cohere",
        "aff_unique_dep": "Department of Linguistics and Modern Languages;;;Department of Electronic and Computer Engineering;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.universite-paris.fr;https://www.sorbonne-universite.fr;https://www.hkust.edu.hk;https://cohere.ai",
        "aff_unique_abbr": "CUHK;UPC;Sorbonne U;HKUST;",
        "aff_campus_unique_index": "0;0;0;0;0+0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+1+1;0;2+0;0;0+0",
        "aff_country_unique": "China;France;United States"
    },
    {
        "id": "2025.coling-industry.60",
        "title": "LionGuard: A Contextualized Moderation Classifier to Tackle Localized Unsafe Content",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "As large language models (LLMs) become increasingly prevalent in a wide variety of applications, concerns about the safety of their outputs have become more significant. Most efforts at safety-tuning or moderation today take on a predominantly Western-centric view of safety, especially for toxic, hateful, or violent speech. In this paper, we describe LionGuard, a Singapore-contextualized moderation classifier that can serve as guardrails against unsafe LLM usage. When assessed on Singlish data, LionGuard outperforms existing widely-used moderation APIs, which are not finetuned for the Singapore context, by at least 14% (binary) and up to 51% (multi-label). Our work highlights the benefits of localization for moderation classifiers and presents a practical and scalable approach for low-resource languages, particularly English-based creoles.",
        "author": "Jessica Foo; Shaun Khoo",
        "authorids": "/j/jessica-foo/; /s/shaun-khoo/",
        "bibtex": "@inproceedings{foo-khoo-2025-lionguard,\n    title = \"{L}ion{G}uard: A Contextualized Moderation Classifier to Tackle Localized Unsafe Content\",\n    author = \"Foo, Jessica  and\n      Khoo, Shaun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.60/\",\n    pages = \"707--731\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.60.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.60/",
        "pdf_size": 4246600,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:FXGFTY2EpIkJ:scholar.google.com/&scioq=LionGuard:+A+Contextualized+Moderation+Classifier+to+Tackle+Localized+Unsafe+Content&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "GovTech Singapore; GovTech Singapore",
        "aff_domain": "tech.gov.sg;tech.gov.sg",
        "email": "tech.gov.sg;tech.gov.sg",
        "github": "",
        "project": "https://huggingface.co/",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "GovTech Singapore",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.govtech.gov.sg",
        "aff_unique_abbr": "GovTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2025.coling-main.751",
        "title": "LlmLink: Dual LLMs for Dynamic Entity Linking on Long Narratives with Collaborative Memorisation and Prompt Optimisation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We address the task of CoREFerence resolution (CoREF) in chunked long narratives. Existing approaches remain either focused on supervised fine-tuning or limited to one-off prediction, which poses a challenge where the context is long. We develop a dynamic approach to cope with this: by deploying dual Large Language Models (LLMs), we assign specialised LLMs to local named entity recognition and distant CoREF tasks, respectively, while ensuring their exchange of information. Utilising our novel memorisation schemes, the coreference resolution LLM would memorise characters and their associated descriptions, thereby reducing token consumption compared with storing previous messages. To alleviate hallucinations of LLMs, we employ an automatic prompt optimisation method, with the LLM ranker modified to leverage annotations. Our approach achieves performance gains over other LLM-based models and fine-tuning approaches on long narrative datasets, significantly reducing the resources required for inference and training.",
        "author": "Lixing Zhu; Jun Wang; Yulan He",
        "authorids": "/l/lixing-zhu/; /j/jun-wang/; /y/yulan-he/",
        "bibtex": "@inproceedings{zhu-etal-2025-llmlink,\n    title = \"{L}lm{L}ink: Dual {LLM}s for Dynamic Entity Linking on Long Narratives with Collaborative Memorisation and Prompt Optimisation\",\n    author = \"Zhu, Lixing  and\n      Wang, Jun  and\n      He, Yulan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.751/\",\n    pages = \"11334--11347\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.751.pdf",
        "site": "https://aclanthology.org/2025.coling-main.751/",
        "pdf_size": 895405,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0uWrBFh5GS4J:scholar.google.com/&scioq=LlmLink:+Dual+LLMs+for+Dynamic+Entity+Linking+on+Long+Narratives+with+Collaborative+Memorisation+and+Prompt+Optimisation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "King\u2019s College London; University of Warwick + King\u2019s College London; King\u2019s College London + The Alan Turing Institute",
        "aff_domain": "kcl.ac.uk;warwick.ac.uk;kcl.ac.uk",
        "email": "kcl.ac.uk;warwick.ac.uk;kcl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;0+2",
        "aff_unique_norm": "King's College London;University of Warwick;The Alan Turing Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kcl.ac.uk;https://www.warwick.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "KCL;Warwick;ATI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-industry.55",
        "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient fine-tuning of Large Language Models (LLMs). We study how different LoRA modules can be merged to achieve skill composition\u2014testing the performance of the merged model on a target task that involves combining multiple skills, each skill coming from a single LoRA. This setup is favorable when it is difficult to obtain training data for the target task and when it can be decomposed into multiple skills. First, we identify practically occurring use-cases that can be studied under the realm of skill composition, e.g. solving hard math-word problems with code, creating a bot to answer questions on proprietary manuals or about domain-specialized corpora. Our main contribution is to show that concatenation of LoRAs (CAT), which optimally weights LoRAs that were individually trained on different skills, outperforms existing model- and data- merging techniques; for instance on math-word problems, CAT beats these methods by an average of 43% and 12% respectively. Thus, this paper advocates model merging as an efficient way to solve compositional tasks and underscores CAT as a simple, compute-friendly and effective procedure.",
        "author": "Akshara Prabhakar; Yuanzhi Li; Karthik Narasimhan; Sham Kakade; Eran Malach; Samy Jelassi",
        "authorids": "/a/akshara-prabhakar/; /y/yuanzhi-li/; /k/karthik-narasimhan/; /s/sham-kakade/; /e/eran-malach/; /s/samy-jelassi/",
        "bibtex": "@inproceedings{prabhakar-etal-2025-lora,\n    title = \"{L}o{RA} Soups: Merging {L}o{RA}s for Practical Skill Composition Tasks\",\n    author = \"Prabhakar, Akshara  and\n      Li, Yuanzhi  and\n      Narasimhan, Karthik  and\n      Kakade, Sham  and\n      Malach, Eran  and\n      Jelassi, Samy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.55/\",\n    pages = \"644--655\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.55.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.55/",
        "pdf_size": 2841132,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10115845358462741824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Princeton University; Microsoft Research; Department of Computer Science, Princeton University; Kempner Institute, Harvard University; Kempner Institute, Harvard University; Center of Mathematical Sciences and Applications, Harvard University",
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "github": "https://github.com/aksh555/LoRA-Soups",
        "project": "https://arxiv.org/abs/2410.13025",
        "author_num": 6,
        "aff_unique_index": "0;1;0;2;2;2",
        "aff_unique_norm": "Princeton University;Microsoft Corporation;Harvard University",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research;Kempner Institute",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research;https://www.harvard.edu",
        "aff_unique_abbr": "Princeton;MSR;Harvard",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.371",
        "title": "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Low-Rank Adaptation (LoRA) is currently the most commonly used Parameter-efficient fine-tuning (PEFT) method. However, it still faces high computational and storage costs to models with billions of parameters. Most previous studies have tackled this issue by using pruning techniques. Nonetheless, these efforts only analyze LoRA parameter features to evaluate their importance, such as parameter count, size, and gradient. In fact, the output of LoRA directly impacts the fine-tuned model. Preliminary experiments indicate that a fraction of LoRA possesses significantly high output values, substantially influencing the layer output. Motivated by the observation, we propose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based on the LoRA output. Then we retain LoRA for important layers and the other layers share the same LoRA. We conduct abundant experiments with models of different scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can achieve performance comparable to full fine-tuning and LoRA while retaining 50% of the LoRA parameters on average.",
        "author": "Hongyun Zhou; Xiangyu Lu; Wang Xu; Conghui Zhu; Tiejun Zhao; Muyun Yang",
        "authorids": "/h/hongyun-zhou/; /x/xiangyu-lu/; /w/wang-xu/; /c/conghui-zhu/; /t/tiejun-zhao/; /m/muyun-yang/",
        "bibtex": "https://aclanthology.org/2025.coling-main.371.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.371.pdf",
        "site": "https://aclanthology.org/2025.coling-main.371/",
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9822904210450811886&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.72",
        "title": "LogiGraph: Logical Reasoning with Contrastive Learning and Lightweight Graph Networks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Logical reasoning is a crucial factor in machine reading comprehension tasks (MRC). Existing methods suffer from the balance between semantic and explicit logical relation representations, in which some emphasize contextual semantics, while others pay more attention to explicit logical features. Additionally, previous methods utilize graph convolutional networks (GCN) for node updates, still exhibiting some shortcomings. To address these challenges, in this paper, we propose a logical reasoning method with contrastive learning and lightweight graph networks (LogiGraph). Our method focuses on the lightweight aspect of the GCN, which greatly improves the shortcomings of the GCN, and employs conjunction and punctuation marks as two types of edges to construct a dual graph. Besides, we combine contrastive learning with graph reasoning, which changes the logical expression\u2019s content as the negative sample of the original context, enabling the model to capture negative logical relationships and improving generalization ability. We conduct extensive experiments on two public datasets, ReClor and LogiQA. Experimental results demonstrate that LogiGraph can achieve state-of-the-art performance on both datasets.",
        "author": "Xiang Li; Chen Shi; Yong Xu; Jun Huang",
        "authorids": "/x/xiang-li/; /c/chen-shi/; /y/yong-xu/; /j/jun-huang/",
        "bibtex": "@inproceedings{li-etal-2025-logigraph,\n    title = \"{L}ogi{G}raph: Logical Reasoning with Contrastive Learning and Lightweight Graph Networks\",\n    author = \"Li, Xiang  and\n      Shi, Chen  and\n      Xu, Yong  and\n      Huang, Jun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.72/\",\n    pages = \"1069--1079\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.72.pdf",
        "site": "https://aclanthology.org/2025.coling-main.72/",
        "pdf_size": 1370988,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5CF9r8jxe5sJ:scholar.google.com/&scioq=LogiGraph:+Logical+Reasoning+with+Contrastive+Learning+and+Lightweight+Graph+Networks&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "South China University of Technology, China+Alibaba Group, China; Alibaba Group, China; South China University of Technology, China; Alibaba Group, China",
        "aff_domain": "gmail.com; ; ; ",
        "email": "gmail.com; ; ; ",
        "github": "https://github.com/jackyideal/LogiGraph",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;1",
        "aff_unique_norm": "South China University of Technology;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.scut.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "SCUT;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-demos.4",
        "title": "Loki: An Open-Source Tool for Fact Verification",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We introduce Loki, an open-source tool designed to address the growing problem of misinformation. Loki adopts a human-centered approach, striking a balance between the quality of fact-checking and the cost of human involvement. It decomposes the fact-checking task into a five-step pipeline: breaking down long texts into individual claims, assessing their check-worthiness, generating queries, retrieving evidence, and verifying the claims. Instead of fully automating the claim verification process, provides essential information at each step to assist human judgment, especially for general users such as journalists and content moderators. Moreover, it has been optimized for latency, robustness, and cost efficiency at a commercially usable level. Loki is released under an MIT license and is available on GitHub. We also provide a video presenting the system and its capabilities.",
        "author": "Haonan Li; Xudong Han; Hao Wang; Yuxia Wang; Minghan Wang; Rui Xing; Yilin Geng; Zenan Zhai; Preslav Nakov; Timothy Baldwin",
        "authorids": "/h/haonan-li/; /x/xudong-han/; /h/hao-wang/; /y/yuxia-wang/; /m/minghan-wang/; /r/rui-xing/; /y/yilin-geng/; /z/zenan-zhai/; /p/preslav-nakov/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{li-etal-2025-loki,\n    title = \"Loki: An Open-Source Tool for Fact Verification\",\n    author = \"Li, Haonan  and\n      Han, Xudong  and\n      Wang, Hao  and\n      Wang, Yuxia  and\n      Wang, Minghan  and\n      Xing, Rui  and\n      Geng, Yilin  and\n      Zhai, Zenan  and\n      Nakov, Preslav  and\n      Baldwin, Timothy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.4/\",\n    pages = \"28--36\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.4.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.4/",
        "pdf_size": 1513553,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6748389854760177381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "LibrAI+MBZUAI; LibrAI+MBZUAI; LibrAI; LibrAI+MBZUAI; Monash University; MBZUAI+The University of Melbourne; LibrAI+The University of Melbourne; LibrAI; MBZUAI; LibrAI+MBZUAI+The University of Melbourne",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "https://github.com/Libr-AI/OpenFactVerification",
        "project": "https://www.youtube.com/watch?v=L_3Dp41Lk_k",
        "author_num": 10,
        "aff_unique_index": "0+1;0+1;0;0+1;2;1+3;0+3;0;1;0+1+3",
        "aff_unique_norm": "LibrAI;Mohamed Bin Zayed University of Artificial Intelligence;Monash University;University of Melbourne",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.mbzuai.ac.ae;https://www.monash.edu;https://www.unimelb.edu.au",
        "aff_unique_abbr": ";MBZUAI;Monash;UniMelb",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;2;1+2;2;1;1+2",
        "aff_country_unique": ";United Arab Emirates;Australia"
    },
    {
        "id": "2025.coling-main.299",
        "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image content. To mitigate hallucinations, previous studies mainly focus on retraining LVLMs with custom datasets. Although effective, they inherently come with additional computational costs. In this paper, we propose a training-free framework, MVP, that aims to reduce hallucinations by making the most of the innate capabilities of the LVLMs via Multi-View Multi-Path Reasoning. Specifically, we first devise a multi-view information-seeking strategy to thoroughly perceive the comprehensive information in the image, which enriches the general global information captured by the original vision encoder in LVLMs. Furthermore, during the answer decoding, we propose multi-path reasoning for each information view to quantify and aggregate the certainty scores for each potential answer among multiple decoding paths and finally decide the output answer. By fully grasping the information in the image and carefully considering the certainty of the potential answers when decoding, our MVP can effectively reduce hallucinations in LVLMs. The extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs.",
        "author": "Xiaoye Qu; Jiashuo Sun; Wei Wei; Daizong Liu; Jianfeng Dong; Yu Cheng",
        "authorids": "/x/xiaoye-qu/; /j/jiashuo-sun/; /w/wei-wei/; /d/daizong-liu/; /j/jianfeng-dong/; /y/yu-cheng/",
        "bibtex": "@inproceedings{qu-etal-2025-look,\n    title = \"Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning\",\n    author = \"Qu, Xiaoye  and\n      Sun, Jiashuo  and\n      Wei, Wei  and\n      Liu, Daizong  and\n      Dong, Jianfeng  and\n      Cheng, Yu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.299/\",\n    pages = \"4428--4441\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.299.pdf",
        "site": "https://aclanthology.org/2025.coling-main.299/",
        "pdf_size": 1485058,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10633359309295110366&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/GasolSun36/MVP",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.145",
        "title": "Looking at the Unseen: Effective Sampling of Non-Related Propositions for Argument Mining",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Traditionally, argument mining research has approached the task of automatic identification of argument structures by using existing definitions of what constitutes an argument, while leaving the equally important matter of what does not qualify as an argument unaddressed. With the ability to distinguish between what is and what is not a natural language argument being at the core of argument mining as a field, it is interesting that no previous work has explored approaches to effectively select non-related propositions (i.e., propositions that are not connected through an argumentative relation, such as support or attack) that improve the data for learning argument mining tasks better. In this paper, we address the question of how to effectively sample non-related propositions from six different argument mining corpora belonging to different domains and encompassing both monologue and dialogue forms of argumentation. To that end, in addition to considering undersampling baselines from previous work, we propose three new sampling strategies relying on context (i.e., short/long) and the semantic similarity between propositions. Our results indicate that using more informed sampling strategies improves the performance, not only when evaluating models on their respective test splits, but also in the case of cross-domain evaluation.",
        "author": "Ramon Ruiz-Dolz; Debela Gemechu; Zlata Kikteva; Chris Reed",
        "authorids": "/r/ramon-ruiz-dolz/; /d/debela-gemechu/; /z/zlata-kikteva/; /c/chris-reed/",
        "bibtex": "@inproceedings{ruiz-dolz-etal-2025-looking,\n    title = \"Looking at the Unseen: Effective Sampling of Non-Related Propositions for Argument Mining\",\n    author = \"Ruiz-Dolz, Ramon  and\n      Gemechu, Debela  and\n      Kikteva, Zlata  and\n      Reed, Chris\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.145/\",\n    pages = \"2131--2143\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.145.pdf",
        "site": "https://aclanthology.org/2025.coling-main.145/",
        "pdf_size": 316532,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UJgCodqyd-sJ:scholar.google.com/&scioq=Looking+at+the+Unseen:+Effective+Sampling+of+Non-Related+Propositions+for+Argument+Mining&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Centre for Argument Technology (ARG-tech), University of Dundee, United Kingdom; Centre for Argument Technology (ARG-tech), University of Dundee, United Kingdom; University of Passau, Germany; Centre for Argument Technology (ARG-tech), University of Dundee, United Kingdom",
        "aff_domain": "dundee.ac.uk; ; ; ",
        "email": "dundee.ac.uk; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Dundee;University of Passau",
        "aff_unique_dep": "Centre for Argument Technology (ARG-tech);",
        "aff_unique_url": "https://www.dundee.ac.uk;https://www.uni-passau.de",
        "aff_unique_abbr": "Dundee;UP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "2025.coling-main.15",
        "title": "Looks can be Deceptive: Distinguishing Repetition Disfluency from Reduplication",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Reduplication and repetition, though similar in form, serve distinct linguistic purposes. Reduplication is a deliberate morphological process used to express grammatical, semantic, or pragmatic nuances, while repetition is often unintentional and indicative of disfluency. This paper presents the first large-scale study of reduplication and repetition in speech using computational linguistics. We introduce IndicRedRep, a new publicly available dataset containing Hindi, Telugu, and Marathi text annotated with reduplication and repetition at the word level. We evaluate transformer-based models for multi-class reduplication and repetition token classification, utilizing the Reparandum-Interregnum-Repair structure to distinguish between the two phenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95% in Telugu, and 84.82% in Marathi for reduplication-repetition classification.",
        "author": "Arif A. Ahmad; Khyathi Gayathri Mothika; Pushpak Bhattacharyya",
        "authorids": "/a/arif-a-ahmad/; /k/khyathi-gayathri-mothika/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{ahmad-etal-2025-looks,\n    title = \"Looks can be Deceptive: Distinguishing Repetition Disfluency from Reduplication\",\n    author = \"Ahmad, Arif A.  and\n      Mothika, Khyathi Gayathri  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.15/\",\n    pages = \"214--229\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.15.pdf",
        "site": "https://aclanthology.org/2025.coling-main.15/",
        "pdf_size": 871903,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15798570685589993155&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "CFILT, Indian Institute of Technology Bombay; CFILT, Indian Institute of Technology Bombay; CFILT, Indian Institute of Technology Bombay",
        "aff_domain": "iitb.ac.in;gmail.com;cse.iitb.ac.in",
        "email": "iitb.ac.in;gmail.com;cse.iitb.ac.in",
        "github": "https://github.com/arifahmad-py/IndicRedRep/",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "CFILT",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Bombay",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bombay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.70",
        "title": "Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class Distance Calculation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, text classification methods based on neural networks and pre-trained models have gained increasing attention and demonstrated excellent performance. However, these methods still have some limitations in practical applications: (1) They typically focus only on the matching similarity between sentences. However, there exists implicit high-value information both within sentences of the same class and across different classes, which is very crucial for classification tasks. (2) Existing methods such as pre-trained language models and graph-based approaches often consume substantial memory for training and text-graph construction. (3) Although some low-resource methods can achieve good performance, they often suffer from excessively long processing times. To address these challenges, we propose a low-resource and fast text classification model called LFTC. Our approach begins by constructing a compressor list for each class to fully mine the regularity information within intra-class data. We then remove redundant information irrelevant to the target classification to reduce processing time. Finally, we compute the similarity distance between text pairs for classification. We evaluate LFTC on 9 publicly available benchmark datasets, and the results demonstrate significant improvements in performance and processing time, especially under limited computational and data resources, highlighting its superior advantages.",
        "author": "Yanxu Mao; Peipei Liu; Tiehan Cui; Congying Liu; Datao You",
        "authorids": "/y/yanxu-mao/; /p/peipei-liu/; /t/tiehan-cui/; /c/congying-liu/; /d/datao-you/",
        "bibtex": "@inproceedings{mao-etal-2025-low,\n    title = \"Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class Distance Calculation\",\n    author = \"Mao, Yanxu  and\n      Liu, Peipei  and\n      Cui, Tiehan  and\n      Liu, Congying  and\n      You, Datao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.70/\",\n    pages = \"1045--1056\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.70.pdf",
        "site": "https://aclanthology.org/2025.coling-main.70/",
        "pdf_size": 496499,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16037528728860157205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Software, Henan University, China; Institute of Information Engineering, Chinese Academy of Sciences, China+University of Chinese Academy of Sciences, China; School of Software, Henan University, China; University of Chinese Academy of Sciences, China; School of Software, Henan University, China",
        "aff_domain": "henan.edu.cn;yeah.net;henan.edu.cn;ucas.ac.cn;henan.edu.cn",
        "email": "henan.edu.cn;yeah.net;henan.edu.cn;ucas.ac.cn;henan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+2;0;2;0",
        "aff_unique_norm": "Henan University;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "School of Software;Institute of Information Engineering;",
        "aff_unique_url": ";http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": ";CAS;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.559",
        "title": "Low-Resource Language Expansion and Translation Capacity Enhancement for LLM: A Study on the Uyghur",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Although large language models have significantly advanced natural language generation, their potential in low-resource machine translation has not yet been fully explored, especially for languages that translation models have not been trained on. In this study, we provide a detailed demonstration of how to efficiently expand low-resource languages for large language models and significantly enhance the model\u2019s translation ability, using Uyghur as an example. The process involves four stages: collecting and pre-processing monolingual data, conducting continuous pre-training with extensive monolingual data, fine-tuning with less parallel corpora using translation supervision, and proposing a direct preference optimization based on translation self-evolution (DPOSE) on this basis. Extensive experiments have shown that our strategy effectively expands the low-resource languages supported by large language models and significantly enhances the model\u2019s translation ability in Uyghur with less parallel data. Our research provides detailed insights for expanding other low-resource languages into large language models.",
        "author": "Kaiwen Lu; Yating Yang; Fengyi Yang; Rui Dong; Bo Ma; Aihetamujiang Aihemaiti; Abibilla Atawulla; Lei Wang; Xi Zhou",
        "authorids": "/k/kaiwen-lu/; /y/yating-yang/; /f/fengyi-yang/; /r/rui-dong/; /b/bo-ma/; /a/aihetamujiang-aihemaiti/; /a/abibilla-atawulla/; /l/lei-wang/; /x/xi-zhou/",
        "bibtex": "@inproceedings{lu-etal-2025-low,\n    title = \"Low-Resource Language Expansion and Translation Capacity Enhancement for {LLM}: A Study on the {U}yghur\",\n    author = \"Lu, Kaiwen  and\n      Yang, Yating  and\n      Yang, Fengyi  and\n      Dong, Rui  and\n      Ma, Bo  and\n      Aihemaiti, Aihetamujiang  and\n      Atawulla, Abibilla  and\n      Wang, Lei  and\n      Zhou, Xi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.559/\",\n    pages = \"8360--8373\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.559.pdf",
        "site": "https://aclanthology.org/2025.coling-main.559/",
        "pdf_size": 2098899,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13025545508325940719&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China; 1Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, Urumqi, China+2University of Chinese Academy of Sciences, Beijing, China+3Xinjiang Laboratory of Minority Speech and Language Information Processing, Urumqi, China",
        "aff_domain": "mails.ucas.ac.cn;mails.ucas.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn",
        "email": "mails.ucas.ac.cn;mails.ucas.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2",
        "aff_unique_norm": "Xinjiang Technical Institute of Physics & Chemistry;University of Chinese Academy of Sciences;Xinjiang Laboratory of Minority Speech and Language Information Processing",
        "aff_unique_dep": "Chinese Academy of Sciences;;Laboratory of Minority Speech and Language Information Processing",
        "aff_unique_url": ";http://www.ucas.ac.cn;",
        "aff_unique_abbr": ";UCAS;",
        "aff_campus_unique_index": "0+1+0;0+1+0;0+1+0;0+1+0;0+1+0;0+1+0;0+1+0;0+1+0;0+1+0",
        "aff_campus_unique": "Urumqi;Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.34",
        "title": "Luna: A Lightweight Evaluation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Retriever-Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations - instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large encoder, fine-tuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 91% reduction in cost and latency, respectively. Luna\u2019s generalization capacity across multiple industry verticals and out-of-domain data makes it a strong candidate for guardrailing industry LLM applications.",
        "author": "Masha Belyi; Robert Friel; Shuai Shao; Atindriyo Sanyal",
        "authorids": "/m/masha-belyi/; /r/robert-friel/; /s/shuai-shao/; /a/atindriyo-sanyal/",
        "bibtex": "@inproceedings{belyi-etal-2025-luna,\n    title = \"{L}una: A Lightweight Evaluation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost\",\n    author = \"Belyi, Masha  and\n      Friel, Robert  and\n      Shao, Shuai  and\n      Sanyal, Atindriyo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.34/\",\n    pages = \"398--409\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.34.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.34/",
        "pdf_size": 485545,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3741245355529004407&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.753",
        "title": "LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Sentence embedding models play a key role in various Natural Language Processing tasks, such as in Topic Modeling, Document Clustering and Recommendation Systems. However, these models rely heavily on parallel data, which can be scarce for many low-resource languages, including Luxembourgish. This scarcity results in suboptimal performance of monolingual and cross-lingual sentence embedding models for these languages. To address this issue, we compile a relatively small but high-quality human-generated cross-lingual parallel dataset to train LuxEmbedder, an enhanced sentence embedding model for Luxembourgish with strong cross-lingual capabilities. Additionally, we present evidence suggesting that including low-resource languages in parallel training datasets can be more advantageous for other low-resource languages than relying solely on high-resource language pairs. Furthermore, recognizing the lack of sentence embedding benchmarks for low-resource languages, we create a paraphrase detection benchmark specifically for Luxembourgish, aiming to partially fill this gap and promote further research.",
        "author": "Fred Philippy; Siwen Guo; Jacques Klein; Tegawende Bissyande",
        "authorids": "/f/fred-philippy/; /s/siwen-guo/; /j/jacques-klein/; /t/tegawende-bissyande/",
        "bibtex": "@inproceedings{philippy-etal-2025-luxembedder,\n    title = \"{L}ux{E}mbedder: A Cross-Lingual Approach to Enhanced {L}uxembourgish Sentence Embeddings\",\n    author = \"Philippy, Fred  and\n      Guo, Siwen  and\n      Klein, Jacques  and\n      Bissyande, Tegawende\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.753/\",\n    pages = \"11369--11379\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.753.pdf",
        "site": "https://aclanthology.org/2025.coling-main.753/",
        "pdf_size": 2030516,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9171885222719748409&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Zortify S.A., Luxembourg+University of Luxembourg, Luxembourg; Zortify S.A., Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg",
        "aff_domain": "zortify.com;zortify.com;uni.lu;uni.lu",
        "email": "zortify.com;zortify.com;uni.lu;uni.lu",
        "github": "https://github.com/fredxlpy/LuxEmbedder",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;1;1",
        "aff_unique_norm": "Zortify S.A.;University of Luxembourg",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://wwwen.uniluxembourg.lu",
        "aff_unique_abbr": ";UniLu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Luxembourg"
    },
    {
        "id": "2025.coling-main.36",
        "title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on \u201chuge\u201d databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set.",
        "author": "Bing Wang; Changyu Ren; Jian Yang; Xinnian Liang; Jiaqi Bai; LinZheng Chai; Zhao Yan; Qian-Wen Zhang; Di Yin; Xing Sun; Zhoujun Li",
        "authorids": "/b/bing-wang/; /c/changyu-ren/; /j/jian-yang/; /x/xinnian-liang/; /j/jiaqi-bai/; /l/linzheng-chai/; /z/zhao-yan/; /q/qian-wen-zhang/; /d/di-yin/; /x/xing-sun/; /z/zhoujun-li/",
        "bibtex": "@inproceedings{wang-etal-2025-mac,\n    title = \"{MAC}-{SQL}: A Multi-Agent Collaborative Framework for Text-to-{SQL}\",\n    author = \"Wang, Bing  and\n      Ren, Changyu  and\n      Yang, Jian  and\n      Liang, Xinnian  and\n      Bai, Jiaqi  and\n      Chai, LinZheng  and\n      Yan, Zhao  and\n      Zhang, Qian-Wen  and\n      Yin, Di  and\n      Sun, Xing  and\n      Li, Zhoujun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.36/\",\n    pages = \"540--557\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.36.pdf",
        "site": "https://aclanthology.org/2025.coling-main.36/",
        "pdf_size": 905961,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14525533709148515508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Beihang University; Beihang University; Beihang University; Beihang University; Beihang University; Beihang University; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Beihang University+Tencent Youtu Lab",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;tencent.com;tencent.com;tencent.com;tencent.com;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;tencent.com;tencent.com;tencent.com;tencent.com;buaa.edu.cn",
        "github": "https://github.com/wbbeyourself/MAC-SQL",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;1;1;1;1;0+1",
        "aff_unique_norm": "Beihang University;Tencent",
        "aff_unique_dep": ";Youtu Lab",
        "aff_unique_url": "http://www.buaa.edu.cn/;https://www.tencent.com",
        "aff_unique_abbr": "BUAA;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.557",
        "title": "MAGRET: Machine-generated Text Detection with Rewritten Texts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the quick advancement in text generation ability of Large Language Mode(LLM), concerns about the misuse of machine-generated content have grown, raising potential violations of legal and ethical standards. Some existing studies concentrate on detecting machine-generated text in open-source models using in-model features, but their performance on closed-source large models is limited. This limitation occurs because, in the closed-source model detection, the only reference that can be obtained is the texts, which may differ significantly due to random sampling. In this paper, we demonstrate that texts generated by the same model can align both semantically and statistically under similar prompts, facilitating effective detection and traceability. Specifically, we fine-tune a BERT encoder through contrastive learning to achieve semantic alignment in randomly generated texts from the same model. Then, we propose a method called Machine-Generated Text Detection with Rewritten Texts, which designed several prompt refactoring methods and used them to request rewritten text from LLMs. Semantic and statistical relationships between rewritten and original texts provide a basis for detection and traceability. Finally, we expanded the text dataset with multi-parameter random sampling and verified the performance of MAGRET on three text-generated datasets. Experimental results show that previous methods struggle with closed-source model detection, while our approach significantly outperforms baseline methods in this regard. It also shows MagRet\u2019s stable performance in detection and tracing tasks across various randomly sampled texts.",
        "author": "Yifei Huang; Jiuxin Cao; Hanyu Luo; Xin Guan; Bo Liu",
        "authorids": "/y/yifei-huang/; /j/jiuxin-cao/; /h/hanyu-luo/; /x/xin-guan/; /b/bo-liu/",
        "bibtex": "@inproceedings{huang-etal-2025-magret,\n    title = \"{MAGRET}: Machine-generated Text Detection with Rewritten Texts\",\n    author = \"Huang, Yifei  and\n      Cao, Jiuxin  and\n      Luo, Hanyu  and\n      Guan, Xin  and\n      Liu, Bo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.557/\",\n    pages = \"8336--8346\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.557.pdf",
        "site": "https://aclanthology.org/2025.coling-main.557/",
        "pdf_size": 517754,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VJpmSOi0oI0J:scholar.google.com/&scioq=MAGRET:+Machine-generated+Text+Detection+with+Rewritten+Texts&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.218",
        "title": "MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval Augmented Generation (RAG) has proven to be highly effective in boosting the generative performance of language model in knowledge-intensive tasks. However, existing RAG framework either indiscriminately perform retrieval or rely on rigid single-label classifiers to select retrieval methods, leading to inefficiencies and suboptimal performance across queries of varying complexity. To address these challenges, we propose a reinforcement learning-based framework that dynamically selects the most suitable retrieval strategy based on query complexity. To address these challenges, we propose a reinforcement learning-based framework that dynamically selects the most suitable retrieval strategy based on query complexity. Our approach leverages a multi-armed bandit algorithm, which treats each retrieval method as a distinct \u201carm\u201d and adapts the selection process by balancing exploration and exploitation. Additionally, we introduce a dynamic reward function that balances accuracy and efficiency, penalizing methods that require more retrieval steps, even if they lead to a correct result. Our method achieves new state of the art results on multiple single-hop and multi-hop datasets while reducing retrieval costs. Our code are available at https://github.com/FUTUREEEEEE/MBA.",
        "author": "Xiaqiang Tang; Qiang Gao; Jian Li; Nan Du; Qi Li; Sihong Xie",
        "authorids": "/x/xiaqiang-tang/; /q/qiang-gao/; /j/jian-li/; /n/nan-du/; /q/qi-li/; /s/sihong-xie/",
        "bibtex": "@inproceedings{tang-etal-2025-mba,\n    title = \"{MBA}-{RAG}: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity\",\n    author = \"Tang, Xiaqiang  and\n      Gao, Qiang  and\n      Li, Jian  and\n      Du, Nan  and\n      Li, Qi  and\n      Xie, Sihong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.218/\",\n    pages = \"3248--3254\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.218.pdf",
        "site": "https://aclanthology.org/2025.coling-main.218/",
        "pdf_size": 432978,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14788336557570423254&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The Hong Kong University of Science and Technology (Guangzhou)+Tencent Hunyuan; Tencent Hunyuan+Wuhan University; Tencent Hunyuan; Tencent Hunyuan; Iowa State University; The Hong Kong University of Science and Technology (Guangzhou)",
        "aff_domain": "hkust-gz.edu.cn; ; ; ; ;hkust-gz.edu.cn",
        "email": "hkust-gz.edu.cn; ; ; ; ;hkust-gz.edu.cn",
        "github": "https://github.com/FUTUREEEEEE/MBA",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1+2;1;1;3;0",
        "aff_unique_norm": "The Hong Kong University of Science and Technology;Tencent;Wuhan University;Iowa State University",
        "aff_unique_dep": ";Hunyuan;;",
        "aff_unique_url": "https://www.ust.hk;https://www.tencent.com;http://www.whu.edu.cn/;https://www.iastate.edu",
        "aff_unique_abbr": "HKUST;Tencent;WHU;ISU",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Guangzhou;",
        "aff_country_unique_index": "0+0;0+0;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.24",
        "title": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks. However, their performance is still considerably lower than that of human experts on benchmarks that include complex schemas and queries, such as BIRD. This study considers the sensitivity of LLMs to the prompts and introduces a novel approach that leverages multiple prompts to explore a broader search space for possible answers and effectively aggregate them. Specifically, we robustly refine the database schema through schema linking using multiple prompts. Thereafter, we generate various candidate SQL queries based on the refined schema and diverse prompts. Finally, the candidate queries are filtered based on their confidence scores, and the optimal query is obtained through a multiple-choice selection that is presented to the LLM. When evaluated on the BIRD and Spider benchmarks, the proposed method achieved execution accuracies of 65.5% and 89.6%, respectively, significantly outperforming previous ICL-based methods.",
        "author": "Dongjun Lee; Choongwon Park; Jaehyuk Kim; Heesoo Park",
        "authorids": "/d/dongjun-lee/; /c/choongwon-park/; /j/jaehyuk-kim/; /h/heesoo-park/",
        "bibtex": "@inproceedings{lee-etal-2025-mcs,\n    title = \"{MCS}-{SQL}: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-{SQL} Generation\",\n    author = \"Lee, Dongjun  and\n      Park, Choongwon  and\n      Kim, Jaehyuk  and\n      Park, Heesoo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.24/\",\n    pages = \"337--353\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.24.pdf",
        "site": "https://aclanthology.org/2025.coling-main.24/",
        "pdf_size": 966531,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11292860639546898516&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Dunamu; Dunamu; Dunamu; Dunamu",
        "aff_domain": "dunamu.com;dunamu.com;dunamu.com;dunamu.com",
        "email": "dunamu.com;dunamu.com;dunamu.com;dunamu.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Dunamu",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.dunamu.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.711",
        "title": "MDPO: Customized Direct Preference Optimization with a Metric-based Sampler for Question and Answer Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the extensive use of large language models, automatically generating QA datasets for domain-specific fine-tuning has become crucial. However, considering the multifaceted demands for readability, diversity, and comprehensiveness of QA data, current methodologies fall short in producing high-quality QA datasets. Moreover, the dependence of existing evaluation metrics on ground truth labels further exacerbates the challenges associated with the selection of QA data. In this paper, we introduce a novel method for QA data generation, denoted as MDPO. We proposes a set of unsupervised evaluation metrics for QA data, enabling multidimensional assessment based on the relationships among context,question and answer. Furthermore, leveraging these metrics, we implement a customized direct preference optimization process that guides large language models to produce high-quality and domain-specific QA pairs. Empirical results on public datasets indicate that MDPO\u2019s performance substantially surpasses that of state-of-the-art methods.",
        "author": "Yihang Wang; Bowen Tian; Yueyang Su; Yixing Fan; Jiafeng Guo",
        "authorids": "/y/yihang-wang/; /b/bowen-tian/; /y/yueyang-su/; /y/yixing-fan/; /j/jiafeng-guo/",
        "bibtex": "@inproceedings{wang-etal-2025-mdpo,\n    title = \"{MDPO}: Customized Direct Preference Optimization with a Metric-based Sampler for Question and Answer Generation\",\n    author = \"Wang, Yihang  and\n      Tian, Bowen  and\n      Su, Yueyang  and\n      Fan, Yixing  and\n      Guo, Jiafeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.711/\",\n    pages = \"10660--10671\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.711.pdf",
        "site": "https://aclanthology.org/2025.coling-main.711/",
        "pdf_size": 1481329,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:e4DewFhGdu4J:scholar.google.com/&scioq=MDPO:+Customized+Direct+Preference+Optimization+with+a+Metric-based+Sampler+for+Question+and+Answer+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications; Hong Kong University of Science and Technology (Guangzhou); CAS Key Lab of Network Data Science and Technology, ICT, CAS + University of Chinese Academy of Sciences; CAS Key Lab of Network Data Science and Technology, ICT, CAS + University of Chinese Academy of Sciences; CAS Key Lab of Network Data Science and Technology, ICT, CAS + University of Chinese Academy of Sciences",
        "aff_domain": "gmail.com;hkust-gz.edu.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "gmail.com;hkust-gz.edu.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2+3;2+3;2+3",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Hong Kong University of Science and Technology;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": ";;Key Lab of Network Data Science and Technology;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.ust.hk;http://www.cas.cn/;http://www.ucas.ac.cn",
        "aff_unique_abbr": "BUPT;HKUST;CAS;UCAS",
        "aff_campus_unique_index": "0;1;;;",
        "aff_campus_unique": "Beijing;Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.638",
        "title": "ME2-BERT: Are Events and Emotions what you need for Moral Foundation Prediction?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Moralities, emotions, and events are complex aspects of human cognition, which are often treated separately since capturing their combined effects is challenging, especially due to the lack of annotated data. Leveraging their interrelations hence becomes crucial for advancing the understanding of human moral behaviors. In this work, we propose ME2-BERT, the first holistic framework for fine-tuning a pre-trained language model like BERT to the task of moral foundation prediction. ME2-BERT integrates events and emotions for learning domain-invariant morality-relevant text representations. Our extensive experiments show that ME2-BERT outperforms existing state-of-the-art methods for moral foundation prediction, with an average increase up to 35% in the out-of-domain scenario.",
        "author": "Lorenzo Zangari; Candida M. Greco; Davide Picca; Andrea Tagarelli",
        "authorids": "/l/lorenzo-zangari/; /c/candida-m-greco/; /d/davide-picca/; /a/andrea-tagarelli/",
        "bibtex": "@inproceedings{zangari-etal-2025-me2,\n    title = \"{ME}2-{BERT}: Are Events and Emotions what you need for Moral Foundation Prediction?\",\n    author = \"Zangari, Lorenzo  and\n      Greco, Candida M.  and\n      Picca, Davide  and\n      Tagarelli, Andrea\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.638/\",\n    pages = \"9516--9532\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.638.pdf",
        "site": "https://aclanthology.org/2025.coling-main.638/",
        "pdf_size": 1885372,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:7hPeip6MDpkJ:scholar.google.com/&scioq=ME2-BERT:+Are+Events+and+Emotions+what+you+need+for+Moral+Foundation+Prediction%3F&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "DIMES, University of Calabria, Italy; DIMES, University of Calabria, Italy; University of Lausanne, Switzerland; DIMES, University of Calabria, Italy",
        "aff_domain": "dimes.unical.it;dimes.unical.it;unil.ch;dimes.unical.it",
        "email": "dimes.unical.it;dimes.unical.it;unil.ch;dimes.unical.it",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Calabria;University of Lausanne",
        "aff_unique_dep": "DIMES;",
        "aff_unique_url": "https://www.unical.it;https://www.unil.ch",
        "aff_unique_abbr": ";UNIL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Italy;Switzerland"
    },
    {
        "id": "2025.coling-main.724",
        "title": "MESAQA: A Dataset for Multi-Span Contextual and Evidence-Grounded Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce MESAQA, a novel dataset focusing on multi-span contextual understanding question answering (QA).Unlike traditional single-span QA systems, questions in our dataset consider information from multiple spans within the context document. MESAQA supports evidence-grounded QA, demanding the model\u2019s capability of answer generation and multi-evidence identification. Our automated dataset creation method leverages the MASH-QA dataset and large language models (LLMs) to ensure that each Q/A pair requires considering all selected spans. Experimental results show that current models struggle with multi-span contextual QA, underscoring the need for new approaches. Our dataset sets a benchmark for this emerging QA paradigm, promoting research in complex information retrieval and synthesis.",
        "author": "Jui-I Wang; Hen-Hsen Huang; Hsin-Hsi Chen",
        "authorids": "/j/jui-i-wang/; /h/hen-hsen-huang/; /h/hsin-hsi-chen/",
        "bibtex": "@inproceedings{wang-etal-2025-mesaqa,\n    title = \"{MESAQA}: A Dataset for Multi-Span Contextual and Evidence-Grounded Question Answering\",\n    author = \"Wang, Jui-I  and\n      Huang, Hen-Hsen  and\n      Chen, Hsin-Hsi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.724/\",\n    pages = \"10891--10901\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.724.pdf",
        "site": "https://aclanthology.org/2025.coling-main.724/",
        "pdf_size": 755492,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:W5b1EE3SP3UJ:scholar.google.com/&scioq=MESAQA:+A+Dataset+for+Multi-Span+Contextual+and+Evidence-Grounded+Question+Answering&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Academia Sinica, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taiwan + AI Research Center (AINTU), National Taiwan University, Taiwan",
        "aff_domain": "nlg.csie.ntu.edu.tw;iis.sinica.edu.tw;ntu.edu.tw",
        "email": "nlg.csie.ntu.edu.tw;iis.sinica.edu.tw;ntu.edu.tw",
        "github": "https://github.com/reiiwang/MESAQA.git",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+0",
        "aff_unique_norm": "National Taiwan University;Academia Sinica",
        "aff_unique_dep": "Department of Computer Science and Information Engineering;",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.sinica.edu.tw",
        "aff_unique_abbr": "NTU;Academia Sinica",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.568",
        "title": "META-LORA: Memory-Efficient Sample Reweighting for Fine-Tuning Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Supervised fine-tuning (SFT) is widely adopted for tailoring large language models (LLMs) to specific downstream tasks. However, the substantial computational demands of LLMs hinder iterative exploration of fine-tuning datasets and accurate evaluation of individual sample importance. To address this challenge, we introduce Meta-LoRA, a memory-efficient method for automatic sample reweighting. Meta-LoRA learns to reweight fine-tuning samples by minimizing the loss on a small, high-quality validation set through an end-to-end bi-level optimization framework based on meta-learning. To reduce memory usage associated with computing second derivatives, we approximate the bi-level optimization using gradient similarity between training and validation datasets, replacing bi-dimensional gradient similarity with the product of one-dimensional activation states and their corresponding gradients. Further memory optimization is achieved by refining gradient computations, selectively applying them to the low-rank layers of LoRA, which results in as little as 4% additional memory usage. Comprehensive evaluations across benchmark datasets in mathematics, coding, and medical domains demonstrate Meta-LoRA\u2019s superior efficacy and efficiency. The source code is available at https://github.com/liweicheng-ai/meta-lora.",
        "author": "Weicheng Li; Lixin Zou; Min Tang; Qing Yu; Wanli Li; Chenliang Li",
        "authorids": "/w/weicheng-li/; /l/lixin-zou/; /m/min-tang/; /q/qing-yu/; /w/wanli-li/; /c/chenliang-li/",
        "bibtex": "@inproceedings{li-etal-2025-meta,\n    title = \"{META}-{LORA}: Memory-Efficient Sample Reweighting for Fine-Tuning Large Language Models\",\n    author = \"Li, Weicheng  and\n      Zou, Lixin  and\n      Tang, Min  and\n      Yu, Qing  and\n      Li, Wanli  and\n      Li, Chenliang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.568/\",\n    pages = \"8504--8517\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.568.pdf",
        "site": "https://aclanthology.org/2025.coling-main.568/",
        "pdf_size": 1029084,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1793380565516713599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Wuhan University; Wuhan University; Monash University; Wuhan University; Huazhong Agricultural University; Wuhan University",
        "aff_domain": "whu.edu.cn;whu.edu.cn;monash.edu;whu.edu.cn;mail.hzau.edu.cn;whu.edu.cn",
        "email": "whu.edu.cn;whu.edu.cn;monash.edu;whu.edu.cn;mail.hzau.edu.cn;whu.edu.cn",
        "github": "https://github.com/liweicheng-ai/meta-lora",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "Wuhan University;Monash University;Huazhong Agricultural University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.whu.edu.cn/;https://www.monash.edu;http://www.hzau.edu.cn/",
        "aff_unique_abbr": "WHU;Monash;HAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2025.coling-main.179",
        "title": "MIDLM: Multi-Intent Detection with Bidirectional Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Decoder-only Large Language Models (LLMs) have demonstrated exceptional performance in language generation, exhibiting broad capabilities across various tasks. However, the application to label-sensitive language understanding tasks remains challenging due to the limitations of their autoregressive architecture, which restricts the sharing of token information within a sentence. In this paper, we address the Multi-Intent Detection (MID) task and introduce MIDLM, a bidirectional LLM framework that incorporates intent number detection and multi-intent selection. This framework allows autoregressive LLMs to leverage bidirectional information awareness through post-training, eliminating the need for training the models from scratch. Comprehensive evaluations across 8 datasets show that MIDLM consistently outperforms both existing vanilla models and pretrained baselines, demonstrating its superior performance in the MID task.",
        "author": "Shangjian Yin; Peijie Huang; Yuhong Xu",
        "authorids": "/s/shangjian-yin/; /p/peijie-huang/; /y/yuhong-xu/",
        "bibtex": "@inproceedings{yin-etal-2025-midlm,\n    title = \"{MIDLM}: Multi-Intent Detection with Bidirectional Large Language Models\",\n    author = \"Yin, Shangjian  and\n      Huang, Peijie  and\n      Xu, Yuhong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.179/\",\n    pages = \"2616--2625\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.179.pdf",
        "site": "https://aclanthology.org/2025.coling-main.179/",
        "pdf_size": 609341,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15955824006734295292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Mathematics and Informatics, South China Agricultural University, China; College of Mathematics and Informatics, South China Agricultural University, China; College of Mathematics and Informatics, South China Agricultural University, China",
        "aff_domain": "163.com;scau.edu.cn;scau.edu.cn",
        "email": "163.com;scau.edu.cn;scau.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "South China Agricultural University",
        "aff_unique_dep": "College of Mathematics and Informatics",
        "aff_unique_url": "http://www.scau.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.617",
        "title": "MIGRATE: Cross-Lingual Adaptation of Domain-Specific LLMs through Code-Switching and Embedding Transfer",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have rapidly advanced, with domain-specific expert models emerging to handle specialized tasks across various fields. However, the predominant focus on English-centric models demands extensive data, making it challenging to develop comparable models for middle and low-resource languages. To address this limitation, we introduce Migrate, a novel method that leverages open-source static embedding models and up to 3 million tokens of code-switching data to facilitate the seamless transfer of embeddings to target languages. Migrate enables effective cross-lingual adaptation without requiring large-scale domain-specific corpora in the target language, promoting the accessibility of expert LLMs to a diverse range of linguistic communities. Our experimental results demonstrate that Migrate significantly enhances model performance in target languages, outperforming baseline and existing cross-lingual transfer methods. This approach provides a practical and efficient solution for extending the capabilities of domain-specific expert models.",
        "author": "Seongtae Hong; Seungyoon Lee; Hyeonseok Moon; Heuiseok Lim",
        "authorids": "/s/seongtae-hong/; /s/seungyoon-lee/; /h/hyeonseok-moon/; /h/heui-seok-lim/",
        "bibtex": "@inproceedings{hong-etal-2025-migrate,\n    title = \"{MIGRATE}: Cross-Lingual Adaptation of Domain-Specific {LLM}s through Code-Switching and Embedding Transfer\",\n    author = \"Hong, Seongtae  and\n      Lee, Seungyoon  and\n      Moon, Hyeonseok  and\n      Lim, Heuiseok\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.617/\",\n    pages = \"9184--9193\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.617.pdf",
        "site": "https://aclanthology.org/2025.coling-main.617/",
        "pdf_size": 346222,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kvN4nOCevnoJ:scholar.google.com/&scioq=MIGRATE:+Cross-Lingual+Adaptation+of+Domain-Specific+LLMs+through+Code-Switching+and+Embedding+Transfer&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Korea University; Department of Computer Science and Engineering, Korea University; Department of Computer Science and Engineering, Korea University; Department of Computer Science and Engineering, Korea University + Human-inspired AI Research",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Korea University;Human-inspired AI Research",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.korea.ac.kr;",
        "aff_unique_abbr": "KU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2025.coling-main.346",
        "title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 0.8M images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority.",
        "author": "Bo Li; Shaolin Zhu; Lijie Wen",
        "authorids": "/b/bo-li/; /s/shaolin-zhu/; /l/lijie-wen/",
        "bibtex": "@inproceedings{li-etal-2025-mit,\n    title = \"{MIT}-10{M}: A Large Scale Parallel Corpus of Multilingual Image Translation\",\n    author = \"Li, Bo  and\n      Zhu, Shaolin  and\n      Wen, Lijie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.346/\",\n    pages = \"5154--5167\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.346.pdf",
        "site": "https://aclanthology.org/2025.coling-main.346/",
        "pdf_size": 3131465,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4895060462372095769&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Software, Tsinghua University, Beijing, China + Baidu Inc., Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Software, Tsinghua University, Beijing, China",
        "aff_domain": "baidu.com;tju.edu.cn;tsinghua.edu.cn",
        "email": "baidu.com;tju.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "https://huggingface.co/datasets/liboaccn/MIT-10M",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0",
        "aff_unique_norm": "Tsinghua University;Baidu Inc.;Tianjin University",
        "aff_unique_dep": "School of Software;;College of Intelligence and Computing",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baidu.com;http://www.tju.edu.cn",
        "aff_unique_abbr": "THU;Baidu;Tianjin University",
        "aff_campus_unique_index": "0+0;1;0",
        "aff_campus_unique": "Beijing;Tianjin",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.129",
        "title": "MLD-EA: Check and Complete Narrative Coherence by Introducing Emotions and Actions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Narrative understanding and story generation are critical challenges in natural language processing (NLP), with much of the existing research focused on summarization and question-answering tasks. While previous studies have explored predicting plot endings and generating extended narratives, they often neglect the logical coherence within stories, leaving a significant gap in the field. To address this, we introduce the Missing Logic Detector by Emotion and Action (MLD-EA) model, which leverages large language models (LLMs) to identify narrative gaps and generate coherent sentences that integrate seamlessly with the story\u2019s emotional and logical flow. The experimental results demonstrate that the MLD-EA model enhances narrative understanding and story generation, highlighting LLMs\u2019 potential as effective logic checkers in story writing with logical coherence and emotional consistency. This work fills a gap in NLP research and advances border goals of creating more sophisticated and reliable story-generation systems.",
        "author": "Jinming Zhang; Yunfei Long",
        "authorids": "/j/jinming-zhang/; /y/yunfei-long/",
        "bibtex": "@inproceedings{zhang-long-2025-mld,\n    title = \"{MLD}-{EA}: Check and Complete Narrative Coherence by Introducing Emotions and Actions\",\n    author = \"Zhang, Jinming  and\n      Long, Yunfei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.129/\",\n    pages = \"1892--1907\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.129.pdf",
        "site": "https://aclanthology.org/2025.coling-main.129/",
        "pdf_size": 2119994,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZAqyhxnL0qIJ:scholar.google.com/&scioq=MLD-EA:+Check+and+Complete+Narrative+Coherence+by+Introducing+Emotions+and+Actions&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "University of Essex; University of Essex",
        "aff_domain": "essex.ac.uk;essex.ac.uk",
        "email": "essex.ac.uk;essex.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Essex",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.essex.ac.uk",
        "aff_unique_abbr": "Essex",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.125",
        "title": "MLLM-I2W: Harnessing Multimodal Large Language Model for Zero-Shot Composed Image Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Combined Image Retrieval (CIR) involves retrieving an image based on a reference image and a brief text description, which is widely present in various scenarios such as fashion recommendation. Existing methods can be mainly divided into two categories, respectively supervised CIR methods and Zero-Shot CIR (ZS-CIR) methods. In contrast to supervised CIR methods, which need manually annotated triples for training task-specific models, ZS-CIR models can be trained using images datasets only and performs well. However, ZS-CIR still faces the primary challenge of learning how to map pseudo-words to images within the joint image-text embedding space. Therefore, in this paper, we propose a novel image-text mapping network, named MLLM-I2W, which adaptively converts description-related image information into pseudo-word markers for precise ZS-CIR. Specifically, the image and text encoding enhancement module within the MLLM prompt selects subject headings and generates text descriptions. It then reduces the modality gap between images and text using uncertainty modeling. An adaptive weighting module and a prototype are proposed to adjust and learn the deep fusion features, which are further mapped to pseudo-word markers via well-designed MOE-based mapping network. Our model demonstrates consistent improvements across common CIR benchmarks, including COCO, CIRR, and Fashion-IQ.",
        "author": "Tong Bao; Che Liu; Derong Xu; Zhi Zheng; Tong Xu",
        "authorids": "/t/tong-bao/; /c/che-liu/; /d/derong-xu/; /z/zhi-zheng/; /t/tong-xu/",
        "bibtex": "@inproceedings{bao-etal-2025-mllm,\n    title = \"{MLLM}-{I}2{W}: Harnessing Multimodal Large Language Model for Zero-Shot Composed Image Retrieval\",\n    author = \"Bao, Tong  and\n      Liu, Che  and\n      Xu, Derong  and\n      Zheng, Zhi  and\n      Xu, Tong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.125/\",\n    pages = \"1839--1849\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.125.pdf",
        "site": "https://aclanthology.org/2025.coling-main.125/",
        "pdf_size": 7761272,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:h8UzVGiczncJ:scholar.google.com/&scioq=MLLM-I2W:+Harnessing+Multimodal+Large+Language+Model+for+Zero-Shot+Composed+Image+Retrieval&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Cognitive Intelligence; State Key Laboratory of Cognitive Intelligence; State Key Laboratory of Cognitive Intelligence; State Key Laboratory of Cognitive Intelligence; State Key Laboratory of Cognitive Intelligence",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "State Key Laboratory of Cognitive Intelligence",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.301",
        "title": "MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters. Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning. To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German. MLaKE aggregates fact chains from Wikipedia across languages and utilizes LLMs to generate questions and answer. We assessed the effectiveness of current multilingual knowledge editing methods using the MLaKE dataset. Our results show that due to considerable inconsistencies in both multilingual performance and encoding efficiency, these methods struggle to generalize effectively across languages. The accuracy of these methods when editing English is notably higher than for other languages. The experimental results further demonstrate that models encode knowledge and generation capabilities for different languages using distinct parameters, leading to poor cross-lingual transfer performance in current methods. Transfer performance is notably better within the same language family compared to across different families. These findings emphasize the urgent need to improve multilingual knowledge editing methods.",
        "author": "Zihao Wei; Jingcheng Deng; Liang Pang; Hanxing Ding; Huawei Shen; Xueqi Cheng",
        "authorids": "/z/zihao-wei/; /j/jingcheng-deng/; /l/liang-pang/; /h/hanxing-ding/; /h/huawei-shen/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{wei-etal-2025-mlake,\n    title = \"{ML}a{KE}: Multilingual Knowledge Editing Benchmark for Large Language Models\",\n    author = \"Wei, Zihao  and\n      Deng, Jingcheng  and\n      Pang, Liang  and\n      Ding, Hanxing  and\n      Shen, Huawei  and\n      Cheng, Xueqi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.301/\",\n    pages = \"4457--4473\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.301.pdf",
        "site": "https://aclanthology.org/2025.coling-main.301/",
        "pdf_size": 1293151,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17481606883491302398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/Hi-archers/MLaKE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.ict.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.460",
        "title": "MMD-ERE: Multi-Agent Multi-Sided Debate for Event Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event relation extraction (ERE) is becoming increasingly important in the era of large language models. An extensive body of research has explored how performance can be further enhanced by the emergence of exciting technologies like chain-of-thought and self-refinement. In this paper, we introduce MMD-ERE, a multi-agent multi-sided debate approach for event relation extraction, which explores the understanding of event relations among different participants before and after debate. Specifically, for organizing the debate, participants are divided into multiple groups, each assigned its own debate topic, and the process effectively integrates both cooperation and confrontation. We also regard the audience as a crucial participant, as their conclusions from an observer\u2019s perspective tend to be more objective. In the end, we explore the understanding of event relations among different participants before and after the debate. Experiments across various ERE tasks and LLMs demonstrate that MMD-ERE outperforms established baselines. Further analysis shows that debates can effectively enhance participants\u2019 understanding of event relations.",
        "author": "Yong Guan; Hao Peng; Lei Hou; Juanzi Li",
        "authorids": "/y/yong-guan/; /h/hao-peng/; /l/lei-hou/; /j/juanzi-li/",
        "bibtex": "@inproceedings{guan-etal-2025-mmd,\n    title = \"{MMD}-{ERE}: Multi-Agent Multi-Sided Debate for Event Relation Extraction\",\n    author = \"Guan, Yong  and\n      Peng, Hao  and\n      Hou, Lei  and\n      Li, Juanzi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.460/\",\n    pages = \"6889--6896\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.460.pdf",
        "site": "https://aclanthology.org/2025.coling-main.460/",
        "pdf_size": 863282,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10396067263067304320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Computer Science and Technology",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.375",
        "title": "MOPO: Multi-Objective Prompt Optimization for Affective Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "How emotions are expressed depends on the context and domain. On X (formerly Twitter), for instance, an author might simply use the hashtag #anger, while in a news headline, emotions are typically written in a more polite, indirect manner. To enable conditional text generation models to create emotionally connotated texts that fit a domain, users need to have access to a parameter that allows them to choose the appropriate way to express an emotion. To achieve this, we introduce MOPO, a Multi-Objective Prompt Optimization methodology. MOPO optimizes prompts according to multiple objectives (which correspond here to the output probabilities assigned by emotion classifiers trained for different domains). In contrast to single objective optimization, MOPO outputs a set of prompts, each with a different weighting of the multiple objectives. Users can then choose the most appropriate prompt for their context. We evaluate MOPO using three objectives, determined by various domain-specific emotion classifiers. MOPO improves performance by up to 15 pp across all objectives with a minimal loss (1\u20132 pp) for any single objective compared to single-objective optimization. These minor performance losses are offset by a broader generalization across multiple objectives \u2013 which is not possible with single-objective optimization. Additionally, MOPO reduces computational requirements by simultaneously optimizing for multiple objectives, eliminating separate optimization procedures for each objective.",
        "author": "Yarik Menchaca Resendiz; Roman Klinger",
        "authorids": "/y/yarik-menchaca-resendiz/; /r/roman-klinger/",
        "bibtex": "@inproceedings{menchaca-resendiz-klinger-2025-mopo,\n    title = \"{MOPO}: Multi-Objective Prompt Optimization for Affective Text Generation\",\n    author = \"Menchaca Resendiz, Yarik  and\n      Klinger, Roman\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.375/\",\n    pages = \"5588--5606\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.375.pdf",
        "site": "https://aclanthology.org/2025.coling-main.375/",
        "pdf_size": 4740081,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10727671893567504326&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Institut f\u00fcr Maschinelle Sprachverarbeitung, University of Stuttgart, Germany+Fundamentals of Natural Language Processing, University of Bamberg, Germany; Fundamentals of Natural Language Processing, University of Bamberg, Germany",
        "aff_domain": "uni-bamberg.de;uni-bamberg.de",
        "email": "uni-bamberg.de;uni-bamberg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "University of Stuttgart;University of Bamberg",
        "aff_unique_dep": "Institut f\u00fcr Maschinelle Sprachverarbeitung;Department of Natural Language Processing",
        "aff_unique_url": "https://www.uni-stuttgart.de;https://www.uni-bamberg.de/",
        "aff_unique_abbr": ";Uni Bamberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.291",
        "title": "MPID: A Modality-Preserving and Interaction-Driven Fusion Network for Multimodal Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The advancement of social media has intensified interest in the research direction of Multimodal Sentiment Analysis (MSA). However, current methodologies exhibit relative limitations, particularly in their fusion mechanisms that overlook nuanced differences and similarities across modalities, leading to potential biases in MSA. In addition, indiscriminate fusion across modalities can introduce unnecessary complexity and noise, undermining the effectiveness of the analysis. In this essay, a Modal-Preserving and Interaction-Driven Fusion Network is introduced to address the aforementioned challenges. The compressed representations of each modality are initially obtained through a Token Refinement Module. Subsequently, we employ a Dual Perception Fusion Module to integrate text with audio and a separate Adaptive Graded Fusion Module for text and visual data. The final step leverages text representation to enhance composite representation. Our experiments on CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets demonstrate that our model achieves state-of-the-art performance.",
        "author": "Tianyi Li; Daming Liu",
        "authorids": "/t/tianyi-li/; /d/daming-liu/",
        "bibtex": "@inproceedings{li-liu-2025-mpid,\n    title = \"{MPID}: A Modality-Preserving and Interaction-Driven Fusion Network for Multimodal Sentiment Analysis\",\n    author = \"Li, Tianyi  and\n      Liu, Daming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.291/\",\n    pages = \"4313--4322\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.291.pdf",
        "site": "https://aclanthology.org/2025.coling-main.291/",
        "pdf_size": 1089558,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5985061766936102115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai University of Electric Power; Shanghai University of Electric Power",
        "aff_domain": "mail.shiep.edu.cn;shiep.edu.cn",
        "email": "mail.shiep.edu.cn;shiep.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shanghai University of Electric Power",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.shiep.edu.cn",
        "aff_unique_abbr": "SHIEP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.104",
        "title": "MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary Negative Samples",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aligning Large Language Models (LLMs) with human feedback is crucial for their development. Existing preference optimization methods such as DPO and KTO, while improved based on Reinforcement Learning from Human Feedback (RLHF), are inherently derived from PPO, requiring a reference model that adds GPU memory resources and relies heavily on abundant preference data. Meanwhile, current preference optimization research mainly targets single-question scenarios with two replies, neglecting optimization with multiple replies, which leads to a waste of data in the application. This study introduces the MPPO algorithm, which leverages the average likelihood of model responses to fit the reward function and maximizes the utilization of preference data. Through a comparison of Point-wise, Pair-wise, and List-wise implementations, we found that the Pair-wise approach achieves the best performance, significantly enhancing the quality of model responses. Experimental results demonstrate MPPO\u2019s outstanding performance across various benchmarks. On MT-Bench, MPPO outperforms DPO, ORPO, and SimPO. Notably, on Arena-Hard, MPPO surpasses DPO and ORPO by substantial margins. These achievements underscore the remarkable advantages of MPPO in preference optimization tasks.",
        "author": "Shuo Xie; Fangzhi Zhu; Jiahui Wang; Lulu Wen; Wei Dai; Xiaowei Chen; Junxiong Zhu; Kai Zhou; Bo Zheng",
        "authorids": "/s/shuo-xie/; /f/fangzhi-zhu/; /j/jiahui-wang/; /l/lulu-wen/; /w/wei-dai/; /x/xiaowei-chen/; /j/junxiong-zhu/; /k/kai-zhou/; /b/bo-zheng/",
        "bibtex": "@inproceedings{xie-etal-2025-mppo,\n    title = \"{MPPO}: Multi Pair-wise Preference Optimization for {LLM}s with Arbitrary Negative Samples\",\n    author = \"Xie, Shuo  and\n      Zhu, Fangzhi  and\n      Wang, Jiahui  and\n      Wen, Lulu  and\n      Dai, Wei  and\n      Chen, Xiaowei  and\n      Zhu, Junxiong  and\n      Zhou, Kai  and\n      Zheng, Bo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.104/\",\n    pages = \"1545--1554\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.104.pdf",
        "site": "https://aclanthology.org/2025.coling-main.104/",
        "pdf_size": 529955,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gQgLZttIReMJ:scholar.google.com/&scioq=MPPO:+Multi+Pair-wise+Preference+Optimization+for+LLMs+with+Arbitrary+Negative+Samples&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Taobao & Tmall Group of Alibaba; Alibaba Inc.",
        "aff_domain": "gmail.com;taobao.com;taobao.com;taobao.com;taobao.com;taobao.com;taobao.com;taobao.com;alibaba-inc.com",
        "email": "gmail.com;taobao.com;taobao.com;taobao.com;taobao.com;taobao.com;taobao.com;taobao.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Alibaba Group;Alibaba Group Holding Limited",
        "aff_unique_dep": "Taobao & Tmall Group;",
        "aff_unique_url": "https://www.alibaba.com;https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.377",
        "title": "MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for Arabic Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities across numerous application domains. A key challenge is to keep these models updated with latest available information, which limits the true potential of these models for the end-applications. Although, there have been numerous attempts for LLMs\u2019 Knowledge Editing (KE), i.e., to update and/or edit the LLMs\u2019 prior knowledge and in turn test it via Multi-hop Question Answering (MQA), yet so far these studies are primarily focused and/or developed for English language. To bridge this gap, in this paper we propose: Multi-hop Questioning Answering under Knowledge Editing for Arabic Language (MQA-KEAL). MQA-KEAL stores knowledge edits as structured knowledge units in the external memory. In order to solve multi-hop question, it first uses task-decomposition to decompose the question into smaller sub-problems. Later for each sub-problem, it iteratively queries the external memory and/or target LLM in order to generate the final response. In addition, we also contribute MQUAKE-AR (Arabic translation of English benchmark MQUAKE), as well as a new benchmark MQA-AEVAL for rigorous performance evaluation of MQA under KE for Arabic language. Experimentation evaluation reveals MQA-KEAL outperforms the baseline models by a significant margin. We release the codes for MQA-KEAL at https: //github.com/asif6827/MQA-Keal.",
        "author": "Muhammad Asif Ali; Nawal Daftardar; Mutayyba Waheed; Jianbin Qin; Di Wang",
        "authorids": "/m/muhammad-asif-ali/; /n/nawal-daftardar/; /m/mutayyba-waheed/; /j/jianbin-qin/; /d/di-wang/",
        "bibtex": "@inproceedings{ali-etal-2025-mqa,\n    title = \"{MQA}-{KEAL}: Multi-hop Question Answering under Knowledge Editing for {A}rabic Language\",\n    author = \"Ali, Muhammad Asif  and\n      Daftardar, Nawal  and\n      Waheed, Mutayyba  and\n      Qin, Jianbin  and\n      Wang, Di\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.377/\",\n    pages = \"5629--5644\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.377.pdf",
        "site": "https://aclanthology.org/2025.coling-main.377/",
        "pdf_size": 2252843,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12546215931389653341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "King Abdullah University of Science and Technology, KSA; King Abdullah University of Science and Technology, KSA + King AbdulAziz University, KSA; University of Science and Technology, China; Shenzhen University, China; Center of Excellence for Generative AI, KAUST, KSA",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "https://github.com/asif6827/MQA-Keal",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;2;3;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;King Abdulaziz University;University of Science and Technology;Shenzhen University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.kaust.edu.sa;https://www.kau.edu.sa;;https://www.szu.edu.cn",
        "aff_unique_abbr": "KAUST;KAU;;SZU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;1;1;0",
        "aff_country_unique": "Saudi Arabia;China"
    },
    {
        "id": "2025.coling-main.374",
        "title": "MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown significant potential as judges for Machine Translation (MT) quality assessment, providing both scores and fine-grained feedback. Although approaches such as GEMBA-MQM have shown state-of-the-art performance on reference-free evaluation, the predicted errors do not align well with those annotated by human, limiting their interpretability as feedback signals. To enhance the quality of error annotations predicted by LLM evaluators, we introduce a universal and training-free framework, **MQM-APE**, based on the idea of filtering out non-impactful errors by Automatically Post-Editing (APE) the original translation based on each error, leaving only those errors that contribute to quality improvement. Specifically, we prompt the LLM to act as 1) *evaluator* to provide error annotations, 2) *post-editor* to determine whether errors impact quality improvement and 3) *pairwise quality verifier* as the error filter. Experiments show that our approach consistently improves both the reliability and quality of error spans against GEMBA-MQM, across eight LLMs in both high- and low-resource languages. Orthogonal to trained approaches, MQM-APE complements translation-specific evaluators such as Tower, highlighting its broad applicability. Further analysis confirms the effectiveness of each module and offers valuable insights into evaluator design and LLMs selection.",
        "author": "Qingyu Lu; Liang Ding; Kanjian Zhang; Jinxia Zhang; Dacheng Tao",
        "authorids": "/q/qingyu-lu/; /l/liang-ding/; /k/kanjian-zhang/; /j/jinxia-zhang/; /d/dacheng-tao/",
        "bibtex": "@inproceedings{lu-etal-2025-mqm,\n    title = \"{MQM}-{APE}: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in {LLM} Translation Evaluators\",\n    author = \"Lu, Qingyu  and\n      Ding, Liang  and\n      Zhang, Kanjian  and\n      Zhang, Jinxia  and\n      Tao, Dacheng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.374/\",\n    pages = \"5570--5587\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.374.pdf",
        "site": "https://aclanthology.org/2025.coling-main.374/",
        "pdf_size": 912200,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17492203168793298429&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Southeast University; The University of Sydney; Southeast University Shenzhen Research Institute+Southeast University; Southeast University; College of Computing and Data Science at Nanyang Technological University, Singapore 639798",
        "aff_domain": "seu.edu.cn;gmail.com;seu.edu.cn;seu.edu.cn;gmail.com",
        "email": "seu.edu.cn;gmail.com;seu.edu.cn;seu.edu.cn;gmail.com",
        "github": "https://github.com/Coldmist-Lu/MQM_APE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0+0;0;2",
        "aff_unique_norm": "Southeast University;University of Sydney;Nanyang Technological University",
        "aff_unique_dep": ";;College of Computing and Data Science",
        "aff_unique_url": "https://www.seu.edu.cn/;https://www.sydney.edu.au;https://www.ntu.edu.sg",
        "aff_unique_abbr": "SEU;USYD;NTU",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Shenzhen;Singapore",
        "aff_country_unique_index": "0;1;0+0;0;2",
        "aff_country_unique": "China;Australia;Singapore"
    },
    {
        "id": "2025.coling-main.221",
        "title": "MQM-Chat: Multidimensional Quality Metrics for Chat Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The complexities of chats, such as the stylized contents specific to source segments and dialogue consistency, pose significant challenges for machine translation. Recognizing the need for a precise evaluation metric to address the issues associated with chat translation, this study introduces Multidimensional Quality Metrics for Chat Translation (MQM-Chat), which encompasses seven error types, including three specifically designed for chat translations: ambiguity and disambiguation, buzzword or loanword issues, and dialogue inconsistency. In this study, human annotations were applied to the translations of chat data generated by five translation models. Based on the error distribution of MQM-Chat and the performance of relabeling errors into chat-specific types, we concluded that MQM-Chat effectively classified the errors while highlighting chat-specific issues explicitly. The results demonstrate that MQM-Chat can qualify both the lexical accuracy and semantical accuracy of translation models in chat translation tasks.",
        "author": "Yunmeng Li; Jun Suzuki; Makoto Morishita; Kaori Abe; Kentaro Inui",
        "authorids": "/y/yunmeng-li/; /j/jun-suzuki/; /m/makoto-morishita/; /k/kaori-abe/; /k/kentaro-inui/",
        "bibtex": "@inproceedings{li-etal-2025-mqm,\n    title = \"{MQM}-Chat: Multidimensional Quality Metrics for Chat Translation\",\n    author = \"Li, Yunmeng  and\n      Suzuki, Jun  and\n      Morishita, Makoto  and\n      Abe, Kaori  and\n      Inui, Kentaro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.221/\",\n    pages = \"3283--3299\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.221.pdf",
        "site": "https://aclanthology.org/2025.coling-main.221/",
        "pdf_size": 649709,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:obIroHSpqnMJ:scholar.google.com/&scioq=MQM-Chat:+Multidimensional+Quality+Metrics+for+Chat+Translation&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Tohoku University; Tohoku University+RIKEN; Future Corporation+Tohoku University; Machine Learning Solutions Inc.+Tohoku University+RIKEN; MBZUAI+Tohoku University+RIKEN",
        "aff_domain": "dc.tohoku.ac.jp; ; ; ; ",
        "email": "dc.tohoku.ac.jp; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;2+0;3+0+1;4+0+1",
        "aff_unique_norm": "Tohoku University;RIKEN;Future Corporation;Machine Learning Solutions Inc.;Mohamed Bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.riken.jp;;;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "Tohoku U;RIKEN;;;MBZUAI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;2+0+0;3+0+0",
        "aff_country_unique": "Japan;;United States;United Arab Emirates"
    },
    {
        "id": "2025.coling-main.648",
        "title": "MSG-LLM: A Multi-scale Interactive Framework for Graph-enhanced Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Graph-enhanced large language models (LLMs) leverage LLMs\u2019 remarkable ability to model language and use graph structures to capture topological relationships. Existing graph-enhanced LLMs typically retrieve similar subgraphs to augment LLMs, where the subgraphs carry the entities related to our target and relations among the entities. However, the retrieving methods mainly focus solely on accurately matching subgraphs between our target subgraph and the candidate subgraphs at the same scale, neglecting that the subgraphs with different scales may also share similar semantics or structures. To tackle this challenge, we introduce a graph-enhanced LLM with multi-scale retrieval (MSG-LLM). It captures similar graph structures and semantics across graphs at different scales and bridges the graph alignment across multiple scales. The larger scales maintain the graph\u2019s global information, while the smaller scales preserve the details of fine-grained sub-structures. Specifically, we construct a multi-scale variation to dynamically shrink the scale of graphs. Further, we employ a graph kernel search to discover subgraphs from the entire graph, which essentially achieves multi-scale graph retrieval in Hilbert space. Additionally, we propose to conduct multi-scale interactions (message passing) over graphs at various scales to integrate key information. The interaction also bridges the graph and LLMs, helping with graph retrieval and LLM generation. Finally, we employ a Chain-of-Thought-based LLM prediction to perform the downstream tasks. We evaluate our approach on two graph-based downstream tasks and the experimental results show that our method achieves state-of-the-art performance.",
        "author": "Jiayu Ding; Zhangkai Zheng; Benshuo Lin; Yun Xue; Yiping Song",
        "authorids": "/j/jiayu-ding/; /z/zhangkai-zheng/; /b/benshuo-lin/; /y/yun-xue/; /y/yiping-song/",
        "bibtex": "@inproceedings{ding-etal-2025-msg,\n    title = \"{MSG}-{LLM}: A Multi-scale Interactive Framework for Graph-enhanced Large Language Models\",\n    author = \"Ding, Jiayu  and\n      Zheng, Zhangkai  and\n      Lin, Benshuo  and\n      Xue, Yun  and\n      Song, Yiping\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.648/\",\n    pages = \"9687--9700\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.648.pdf",
        "site": "https://aclanthology.org/2025.coling-main.648/",
        "pdf_size": 890164,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12208475806621078118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University, Foshan, China; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University, Foshan, China; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University, Foshan, China; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Electronic Science and Engineering (School of Microelectronics), South China Normal University, Foshan, China; National University of Defense Technology, Changsha, China",
        "aff_domain": "m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;nudt.edu.cn",
        "email": "m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;m.scnu.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "South China Normal University;National University of Defense Technology",
        "aff_unique_dep": "School of Electronic Science and Engineering;",
        "aff_unique_url": "http://www.scnu.edu.cn;http://www.nudt.edu.cn",
        "aff_unique_abbr": "SCNU;NUDT",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Foshan;Changsha",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.386",
        "title": "MURRE: Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-SQL",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The open-domain text-to-SQL task aims to retrieve question-relevant tables from massive databases and generate SQL. However, the performance of current methods is constrained by single-hop retrieval, and existing multi-hop retrieval of open-domain question answering is not directly applicable due to the tendency to retrieve tables similar to the retrieved ones but irrelevant to the question. Since the questions in text-to-SQL usually contain all required information, while previous multi-hop retrieval supplements the questions with retrieved documents. Therefore, we propose the multi-hop table retrieval with removal (MURRE), which removes previously retrieved information from the question to guide the retriever towards unretrieved relevant tables. Our experiments on two open-domain text-to-SQL datasets demonstrate an average improvement of 5.7% over the previous state-of-the-art results.",
        "author": "Xuanliang Zhang; Dingzirui Wang; Longxu Dou; Qingfu Zhu; Wanxiang Che",
        "authorids": "/x/xuanliang-zhang/; /d/dingzirui-wang/; /l/longxu-dou/; /q/qingfu-zhu/; /w/wanxiang-che/",
        "bibtex": "@inproceedings{zhang-etal-2025-murre,\n    title = \"{MURRE}: Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-{SQL}\",\n    author = \"Zhang, Xuanliang  and\n      Wang, Dingzirui  and\n      Dou, Longxu  and\n      Zhu, Qingfu  and\n      Che, Wanxiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.386/\",\n    pages = \"5789--5806\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.386.pdf",
        "site": "https://aclanthology.org/2025.coling-main.386/",
        "pdf_size": 811065,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3309184718646408351&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Individual Researcher; Harbin Institute of Technology; Harbin Institute of Technology",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;gmail.com;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;gmail.com;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "https://github.com/zhxlia/Murre",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Individual Researcher",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.hit.edu.cn/;",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2025.coling-main.503",
        "title": "Making Large Language Models into World Models with Precondition and Effect Knowledge",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "World models, which encapsulate the dynamics of how actions affect environments, are foundational to the functioning of intelligent agents. In this work, we explore the potential of Large Language Models (LLMs) to operate as world models. Although LLMs are not inherently designed to model real-world dynamics, we show that they can be induced to perform two critical world model functions: determining the applicability of an action based on a given world state, and predicting the resulting world state upon action execution. This is achieved by fine-tuning two separate LLMs\u2014one for precondition prediction and another for effect prediction\u2014while leveraging synthetic data generation techniques. Through human-participant studies, we validate that the precondition and effect knowledge generated by our models aligns with human understanding of world dynamics. We also analyze the extent to which the world model trained on our synthetic data results in an inferred state space that supports the creation of action chains, a necessary property for planning.",
        "author": "Kaige Xie; Ian Yang; John Gunerli; Mark Riedl",
        "authorids": "/k/kaige-xie/; /i/ian-yang/; /j/john-gunerli/; /m/mark-riedl/",
        "bibtex": "@inproceedings{xie-etal-2025-making,\n    title = \"Making Large Language Models into World Models with Precondition and Effect Knowledge\",\n    author = \"Xie, Kaige  and\n      Yang, Ian  and\n      Gunerli, John  and\n      Riedl, Mark\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.503/\",\n    pages = \"7532--7545\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.503.pdf",
        "site": "https://aclanthology.org/2025.coling-main.503/",
        "pdf_size": 305802,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7257329476497719086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Interactive Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.696",
        "title": "Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Indirect User Requests (IURs), such as \u201cIt\u2019s cold in here\u201d instead of \u201cCould you please increase the temperature?\u201d are common in human-human task-oriented dialogue and require world knowledge and pragmatic reasoning from the listener. While large language models (LLMs) can handle these requests effectively, smaller models deployed on virtual assistants often struggle due to resource constraints. Moreover, existing task-oriented dialogue benchmarks lack sufficient examples of complex discourse phenomena such as indirectness. To address this, we propose a set of linguistic criteria along with an LLM-based pipeline for generating realistic IURs to test natural language understanding (NLU) and dialogue state tracking (DST) models before deployment in a new domain. We also release IndirectRequests, a dataset of IURs based on the Schema-Guided Dialogue (SGD) corpus, as a comparative testbed for evaluating the performance of smaller models in handling indirect requests.",
        "author": "Amogh Mannekote; Jinseok Nam; Ziming Li; Kristy Elizabeth Boyer; Bonnie J. Dorr",
        "authorids": "/a/amogh-mannekote/; /j/jinseok-nam/; /z/ziming-li/; /k/kristy-boyer/; /b/bonnie-dorr/",
        "bibtex": "@inproceedings{mannekote-etal-2025-making,\n    title = \"Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests\",\n    author = \"Mannekote, Amogh  and\n      Nam, Jinseok  and\n      Li, Ziming  and\n      Boyer, Kristy Elizabeth  and\n      Dorr, Bonnie J.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.696/\",\n    pages = \"10449--10459\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.696.pdf",
        "site": "https://aclanthology.org/2025.coling-main.696/",
        "pdf_size": 1009853,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14985881059590416995&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Florida + Amazon; Amazon; Amazon; University of Florida; University of Florida",
        "aff_domain": "ufl.edu;amazon.com;amazon.com;ufl.edu;ufl.edu",
        "email": "ufl.edu;amazon.com;amazon.com;ufl.edu;ufl.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;0;0",
        "aff_unique_norm": "University of Florida;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ufl.edu;https://www.amazon.com",
        "aff_unique_abbr": "UF;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.739",
        "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts and identify manipulated or inserted information. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective basic model that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and the domain and factuality of news articles, as well as revealing that manipulated sentences are more likely to encapsulate the main story or consequences of a news outlet.",
        "author": "Kung-Hsiang Huang; Hou Pong Chan; Kathleen McKeown; Heng Ji",
        "authorids": "/k/kung-hsiang-huang/; /h/hou-pong-chan/; /k/kathleen-mckeown/; /h/heng-ji/",
        "bibtex": "@inproceedings{huang-etal-2025-manitweet,\n    title = \"{M}ani{T}weet: A New Benchmark for Identifying Manipulation of News on Social Media\",\n    author = \"Huang, Kung-Hsiang  and\n      Chan, Hou Pong  and\n      McKeown, Kathleen  and\n      Ji, Heng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.739/\",\n    pages = \"11161--11180\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.739.pdf",
        "site": "https://aclanthology.org/2025.coling-main.739/",
        "pdf_size": 2501750,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17990281137490040569&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois Urbana-Champaign+Salesforce AI Research; University of Illinois Urbana-Champaign; Columbia University; University of Illinois Urbana-Champaign",
        "aff_domain": "salesforce.com;illinois.edu;cs.columbia.edu;illinois.edu",
        "email": "salesforce.com;illinois.edu;cs.columbia.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;2;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Salesforce;Columbia University",
        "aff_unique_dep": ";Salesforce AI Research;",
        "aff_unique_url": "https://illinois.edu;https://www.salesforce.com;https://www.columbia.edu",
        "aff_unique_abbr": "UIUC;Salesforce AI;Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.8",
        "title": "Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity matching (EM) is a critical step in entity resolution (ER). Recently, entity matching based on large language models (LLMs) has shown great promise. However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency among record relationships. In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives. Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios. Based on our findings, we further design a compound entity matching framework (ComEM) that leverages the composition of multiple strategies and LLMs. ComEM benefits from the advantages of different sides and achieves improvements in both effectiveness and efficiency. Experimental results on 8 ER datasets and 10 LLMs verify the superiority of incorporating record interactions through the selecting strategy, as well as the further cost-effectiveness brought by ComEM.",
        "author": "Tianshu Wang; Xiaoyang Chen; Hongyu Lin; Xuanang Chen; Xianpei Han; Le Sun; Hao Wang; Zhenyu Zeng",
        "authorids": "/t/tianshu-wang/; /x/xiaoyang-chen/; /h/hongyu-lin/; /x/xuanang-chen/; /x/xianpei-han/; /l/le-sun/; /h/hao-wang/; /z/zhenyu-zeng/",
        "bibtex": "@inproceedings{wang-etal-2025-match,\n    title = \"Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching\",\n    author = \"Wang, Tianshu  and\n      Chen, Xiaoyang  and\n      Lin, Hongyu  and\n      Chen, Xuanang  and\n      Han, Xianpei  and\n      Sun, Le  and\n      Wang, Hao  and\n      Zeng, Zhenyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.8/\",\n    pages = \"96--109\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.8.pdf",
        "site": "https://aclanthology.org/2025.coling-main.8/",
        "pdf_size": 569169,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12735772220438909660&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.540",
        "title": "Measuring Contextual Informativeness in Child-Directed Text",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "To address an important gap in creating children\u2019s stories for vocabulary enrichment, we investigate the automatic evaluation of how well stories convey the semantics of target vocabulary words, a task with substantial implications for generating educational content. We motivate this task, which we call measuring contextual informativeness in children\u2019s stories, and provide a formal task definition as well as a dataset for the task. We further propose a method for automating the task using a large language model (LLM). Our experiments show that our approach reaches a Spearman correlation of 0.4983 with human judgments of informativeness, while the strongest baseline only obtains a correlation of 0.3534. An additional analysis shows that the LLM-based approach is able to generalize to measuring contextual informativeness in adult-directed text, on which it also outperforms all baselines.",
        "author": "Maria R. Valentini; T\u00e9a Y. Wright; Ali Marashian; Jennifer M. Ellis; Eliana Colunga; Katharina von der Wense",
        "authorids": "/m/maria-r-valentini/; /t/tea-y-wright/; /a/ali-marashian/; /j/jennifer-m-ellis/; /e/eliana-colunga/; /k/katharina-von-der-wense/",
        "bibtex": "@inproceedings{valentini-etal-2025-measuring,\n    title = \"Measuring Contextual Informativeness in Child-Directed Text\",\n    author = \"Valentini, Maria R.  and\n      Wright, T{\\'e}a Y.  and\n      Marashian, Ali  and\n      Ellis, Jennifer M.  and\n      Colunga, Eliana  and\n      von der Wense, Katharina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.540/\",\n    pages = \"8109--8120\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.540.pdf",
        "site": "https://aclanthology.org/2025.coling-main.540/",
        "pdf_size": 496400,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3713372438689877082&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder + University of California Berkeley; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder + Johannes Gutenberg University Mainz",
        "aff_domain": "colorado.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0+1;0;0;0+2",
        "aff_unique_norm": "University of Colorado;University of California, Berkeley;Johannes Gutenberg University Mainz",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.colorado.edu;https://www.berkeley.edu;https://www.jgu.de",
        "aff_unique_abbr": "CU Boulder;UC Berkeley;JGU",
        "aff_campus_unique_index": "0;0;0+1;0;0;0+2",
        "aff_campus_unique": "Boulder;Berkeley;Mainz",
        "aff_country_unique_index": "0;0;0+0;0;0;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "2025.coling-main.331",
        "title": "Measuring the Robustness of Reference-Free Dialogue Evaluation Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Advancements in dialogue systems powered by large language models (LLMs) have outpaced the development of reliable evaluation metrics, particularly for diverse and creative responses. We present a benchmark for evaluating the robustness of reference-free dialogue metrics against four categories of adversarial attacks: speaker tag prefixes, static responses, ungrammatical responses, and repeated conversational context. We analyze metrics such as DialogRPT, UniEval, and PromptEval\u2014a prompt-based method leveraging LLMs\u2014across grounded and ungrounded datasets. By examining both their correlation with human judgment and susceptibility to adversarial attacks, we find that these two axes are not always aligned; metrics that appear to be equivalent when judged by traditional benchmarks may, in fact, vary in their scores of adversarial responses. These findings motivate the development of nuanced evaluation frameworks to address real-world dialogue challenges.",
        "author": "Justin Vasselli; Adam Nohejl; Taro Watanabe",
        "authorids": "/j/justin-vasselli/; /a/adam-nohejl/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{vasselli-etal-2025-measuring,\n    title = \"Measuring the Robustness of Reference-Free Dialogue Evaluation Systems\",\n    author = \"Vasselli, Justin  and\n      Nohejl, Adam  and\n      Watanabe, Taro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.331/\",\n    pages = \"4958--4972\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.331.pdf",
        "site": "https://aclanthology.org/2025.coling-main.331/",
        "pdf_size": 1390240,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0rILOstpp_oJ:scholar.google.com/&scioq=Measuring+the+Robustness+of+Reference-Free+Dialogue+Evaluation+Systems&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;is.naist.jp;is.naist.jp",
        "email": "is.naist.jp;is.naist.jp;is.naist.jp",
        "github": "https://github.com/JVasselli/dialogue-metric-robustness",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nist.go.jp",
        "aff_unique_abbr": "NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.649",
        "title": "MedEx: Enhancing Medical Question-Answering with First-Order Logic based Reasoning and Knowledge Injection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In medical question-answering, traditional knowledge triples often fail due to superfluous data and their inability to capture complex relationships between symptoms and treatments across diseases. This limits models\u2019 ability to provide accurate, contextually relevant responses. To overcome this, we introduce MedEx, which employs First-Order Logic (FOL)-based reasoning to model intricate relationships between diseases and treatments. We construct FOL-based triplets that encode the interplay of symptoms, diseases, and treatments, capturing not only surface-level data but also the logical constraints of the medical domain. MedEx encodes the discourse (questions and context) using a transformer-based unit, enhancing context comprehension. These encodings are processed by a Knowledge Injection Cell that integrates knowledge graph triples via a Graph Attention Network. The Logic Fusion Cell then combines medical-specific logical rule triples (e.g., co-occurrence, causation, diagnosis) with knowledge triples and extracts answers through a feed-forward layer. Our analysis demonstrates MedEx\u2019s effectiveness and generalization across medical question-answering tasks. By merging logical reasoning with knowledge, MedEx provides precise medical answers and adapts its logical rules based on training data nuances.",
        "author": "Aizan Zafar; Kshitij Mishra; Asif Ekbal",
        "authorids": "/a/aizan-zafar/; /k/kshitij-mishra/; /a/asif-ekbal/",
        "bibtex": "@inproceedings{zafar-etal-2025-medex,\n    title = \"{M}ed{E}x: Enhancing Medical Question-Answering with First-Order Logic based Reasoning and Knowledge Injection\",\n    author = \"Zafar, Aizan  and\n      Mishra, Kshitij  and\n      Ekbal, Asif\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.649/\",\n    pages = \"9701--9720\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.649.pdf",
        "site": "https://aclanthology.org/2025.coling-main.649/",
        "pdf_size": 3551175,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:EvwVuNBbEZMJ:scholar.google.com/&scioq=MedEx:+Enhancing+Medical+Question-Answering+with+First-Order+Logic+based+Reasoning+and+Knowledge+Injection&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; School of AI and Data Science, Indian Institute of Technology Jodhpur, India",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "https://github.com/aizanzafar/MedEx",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Indian Institute of Technology Patna;Indian Institute of Technology Jodhpur",
        "aff_unique_dep": "Department of Computer Science and Engineering;School of AI and Data Science",
        "aff_unique_url": "https://www.iitp.ac.in;https://www.iitj.ac.in",
        "aff_unique_abbr": "IIT Patna;IIT Jodhpur",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Patna;Jodhpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.173",
        "title": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a Multi-scale spreadsheet benchmark with Meta operations for Table reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion.",
        "author": "Zheng Li; Yang Du; Mao Zheng; Mingyang Song",
        "authorids": "/z/zheng-li/; /y/yang-du/; /m/mao-zheng/; /m/mingyang-song/",
        "bibtex": "@inproceedings{li-etal-2025-mimotable,\n    title = \"{M}i{M}o{T}able: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning\",\n    author = \"Li, Zheng  and\n      Du, Yang  and\n      Zheng, Mao  and\n      Song, Mingyang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.173/\",\n    pages = \"2548--2560\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.173.pdf",
        "site": "https://aclanthology.org/2025.coling-main.173/",
        "pdf_size": 1940575,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5883360296158212531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent Hunyuan; Tencent Hunyuan; Tencent Hunyuan; Tencent Hunyuan",
        "aff_domain": "tencent.com; ; ; ",
        "email": "tencent.com; ; ; ",
        "github": "https://github.com/jasonNLP/MiMoTable",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Hunyuan",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.83",
        "title": "Mining Word Boundaries from Speech-Text Parallel Data for Cross-domain Chinese Word Segmentation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Inspired by early research on exploring naturally annotated data for Chinese Word Segmentation (CWS), and also by recent research on integration of speech and text processing, this work for the first time proposes to explicitly mine word boundaries from parallel speech-text data. We employ the Montreal Forced Aligner (MFA) toolkit to perform character-level alignment on speech-text data, giving pauses as candidate word boundaries. Based on detailed analysis of collected pauses, we propose an effective probability-based strategy for filtering unreliable word boundaries. To more effectively utilize word boundaries as extra training data, we also propose a robust complete-then-train (CTT) strategy. We conduct cross-domain CWS experiments on two target domains, i.e., ZX and AISHELL2. We have annotated about 1K sentences as the evaluation data of AISHELL2. Experiments demonstrate the effectiveness of our proposed approach.",
        "author": "Xuebin Wang; Lei Zhang; Zhenghua Li; Shilin Zhou; Chen Gong; Yang Hou",
        "authorids": "/x/xuebin-wang/; /l/lei-zhang/; /z/zhenghua-li/; /s/shilin-zhou/; /c/chen-gong/; /y/yang-hou/",
        "bibtex": "@inproceedings{wang-etal-2025-mining,\n    title = \"Mining Word Boundaries from Speech-Text Parallel Data for Cross-domain {C}hinese Word Segmentation\",\n    author = \"Wang, Xuebin  and\n      Zhang, Lei  and\n      Li, Zhenghua  and\n      Zhou, Shilin  and\n      Gong, Chen  and\n      Hou, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.83/\",\n    pages = \"1247--1257\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.83.pdf",
        "site": "https://aclanthology.org/2025.coling-main.83/",
        "pdf_size": 1112059,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sNSQC4sVIXQJ:scholar.google.com/&scioq=Mining+Word+Boundaries+from+Speech-Text+Parallel+Data+for+Cross-domain+Chinese+Word+Segmentation&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;gmail.com;suda.edu.cn;outlook.com;suda.edu.cn;stu.suda.edu.cn",
        "email": "stu.suda.edu.cn;gmail.com;suda.edu.cn;outlook.com;suda.edu.cn;stu.suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.563",
        "title": "Mitigating Language Confusion through Inference-time Intervention",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Although large language models (LLMs) trained on extensive multilingual corpora exhibit impressive language transfer, they often fail to respond in the user\u2019s desired language due to corpus imbalances, an embarrassingly simple problem known as the language confusion. However, existing solutions like in-context learning and supervised fine-tuning (SFT) have drawbacks: in-context learning consumes context window space, diminishing attention as text lengthens, while SFT requires extensive, labor-intensive data collection. To overcome these limitations, we propose the language-sensitive intervention (LSI), a novel, lightweight, and label-free approach. Specifically, we analyze language confusion from a causal perspective, revealing that the training corpus\u2019s language distribution acts as a confounder, disadvantaging languages that are underrepresented in the dataset. Then, we identify a language-sensitive dimension in the LLM\u2019s residual stream, i.e., the language vector, which allows us to estimate the average causal effect of prompts on this dimension. During inference, we directly intervene on the language vector to generate responses in the desired language.To further advance research on this issue, we introduce a new benchmark that detects language confusion and assesses content quality. Experimental results demonstrate that our method effectively mitigates language confusion without additional complex mechanisms. Our code is available at https://github.com/SoseloX/LSI.",
        "author": "Xie Yunfan; Lixin Zou; Dan Luo; Min Tang; Chenliang Li; Xiangyang Luo; Liming Dong",
        "authorids": "/x/xie-yunfan/; /l/lixin-zou/; /d/dan-luo/; /m/min-tang/; /c/chenliang-li/; /x/xiangyang-luo/; /l/liming-dong/",
        "bibtex": "@inproceedings{yunfan-etal-2025-mitigating,\n    title = \"Mitigating Language Confusion through Inference-time Intervention\",\n    author = \"Yunfan, Xie  and\n      Zou, Lixin  and\n      Luo, Dan  and\n      Tang, Min  and\n      Li, Chenliang  and\n      Luo, Xiangyang  and\n      Dong, Liming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.563/\",\n    pages = \"8418--8431\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.563.pdf",
        "site": "https://aclanthology.org/2025.coling-main.563/",
        "pdf_size": 1196169,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1246858198658599684&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Wuhan University; Wuhan University; Lehigh University; Monash University; Wuhan University; National Defense University; State Key Lab of Mathematical Engineering and Advanced Computing",
        "aff_domain": "whu.edu.cn;whu.edu.cn;lehigh.edu;monash.edu;whu.edu.cn;tsinghua.org.cn;126.com",
        "email": "whu.edu.cn;whu.edu.cn;lehigh.edu;monash.edu;whu.edu.cn;tsinghua.org.cn;126.com",
        "github": "https://github.com/SoseloX/LSI",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;0;3;4",
        "aff_unique_norm": "Wuhan University;Lehigh University;Monash University;National Defense University;State Key Lab of Mathematical Engineering and Advanced Computing",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "http://www.whu.edu.cn/;https://www.lehigh.edu;https://www.monash.edu;https://www.ndu.edu;",
        "aff_unique_abbr": "WHU;Lehigh;Monash;NDU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0;1;0",
        "aff_country_unique": "China;United States;Australia"
    },
    {
        "id": "2025.coling-main.519",
        "title": "Mitigating Out-of-Entity Errors in Named Entity Recognition: A Sentence-Level Strategy",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Many previous models of named entity recognition (NER) suffer from the problem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the test samples have not appeared in the training samples, which hinders the achievement of satisfactory performance. To improve OOE-NER performance, in this paper, we propose a new framework, namely S+NER, which fully leverages sentence-level information. Our S+NER achieves better OOE-NER performance mainly due to the following two particular designs. 1) It first exploits the pre-trained language model\u2019s capability of understanding the target entity\u2019s sentence-level context with a template set. 2) Then, it refines the sentence-level representation based on the positive and negative templates, through a contrastive learning strategy and template pooling method, to obtain better NER results. Our extensive experiments on five benchmark datasets have demonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.",
        "author": "Guochao Jiang; Ziqin Luo; Chengwei Hu; Zepeng Ding; Deqing Yang",
        "authorids": "/g/guochao-jiang/; /z/ziqin-luo/; /c/chengwei-hu/; /z/zepeng-ding/; /d/deqing-yang/",
        "bibtex": "@inproceedings{jiang-etal-2025-mitigating,\n    title = \"Mitigating Out-of-Entity Errors in Named Entity Recognition: A Sentence-Level Strategy\",\n    author = \"Jiang, Guochao  and\n      Luo, Ziqin  and\n      Hu, Chengwei  and\n      Ding, Zepeng  and\n      Yang, Deqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.519/\",\n    pages = \"7754--7765\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.519.pdf",
        "site": "https://aclanthology.org/2025.coling-main.519/",
        "pdf_size": 563768,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17194009676112767414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Data Science, Fudan University, Shanghai, China + Shanghai Key Laboratory of Data Science, Shanghai, China; School of Data Science, Fudan University, Shanghai, China + Shanghai Key Laboratory of Data Science, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China + Shanghai Key Laboratory of Data Science, Shanghai, China",
        "aff_domain": "m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;0;0+1",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Data Science",
        "aff_unique_dep": "School of Data Science;Data Science",
        "aff_unique_url": "https://www.fudan.edu.cn;",
        "aff_unique_abbr": "Fudan;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.543",
        "title": "Mitigating Shortcut Learning via Smart Data Augmentation based on Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Data-driven pre-trained language models typically perform shortcut learning wherein they rely on the spurious correlations between the data and the ground truth. This reliance can undermine the robustness and generalization of the model. To address this issue, data augmentation emerges as a promising solution. By integrating anti-shortcut data to the training set, the models\u2019 shortcut-induced biases can be mitigated. However, existing methods encounter three challenges: 1) Manual definition of shortcuts is tailored to particular datasets, restricting generalization. 2) The inherent confirmation bias during model training hampers the effectiveness of data augmentation. 3) Insufficient exploration of the relationship between the model performance and the augmented data quantity may result in excessive data consumption. To tackle these challenges, we propose a method of Smart Data Augmentation based on Large Language Models (SAug-LLM). It leverages the LLMs to autonomously identify shortcuts and generate their anti-shortcut counterparts. In addition, the dual validation is employed to mitigate the confirmation bias during the model retraining. Furthermore, the data augmentation process is optimized to effectively rectify model biases while minimizing data consumption. We validate the effectiveness and generalization of our method through extensive experiments across various natural language processing tasks, demonstrating an average performance improvement of 5.61%.",
        "author": "Xinyi Sun; Hongye Tan; Yaxin Guo; Pengpeng Qiang; Ru Li; Hu Zhang",
        "authorids": "/x/xinyi-sun/; /h/hongye-tan/; /y/yaxin-guo/; /p/pengpeng-qiang/; /r/ru-li/; /h/hu-zhang/",
        "bibtex": "@inproceedings{sun-etal-2025-mitigating,\n    title = \"Mitigating Shortcut Learning via Smart Data Augmentation based on Large Language Model\",\n    author = \"Sun, Xinyi  and\n      Tan, Hongye  and\n      Guo, Yaxin  and\n      Qiang, Pengpeng  and\n      Li, Ru  and\n      Zhang, Hu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.543/\",\n    pages = \"8160--8172\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.543.pdf",
        "site": "https://aclanthology.org/2025.coling-main.543/",
        "pdf_size": 1148115,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0NLL9YxF_kYJ:scholar.google.com/&scioq=Mitigating+Shortcut+Learning+via+Smart+Data+Augmentation+based+on+Large+Language+Model&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer and Information Technology, Shanxi University, Taiyuan, China+Institute of Intelligent Information Processing, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Institute of Intelligent Information Processing, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Institute of Intelligent Information Processing, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China+Institute of Intelligent Information Processing, Shanxi University, Taiyuan, China+Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China",
        "aff_domain": "qq.com;sxu.edu.cn;163.com;163.com;sxu.edu.cn;sxu.edu.cn",
        "email": "qq.com;sxu.edu.cn;163.com;163.com;sxu.edu.cn;sxu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0+0;0+0+0;0;0;0+0+0;0+0+0",
        "aff_unique_norm": "Shanxi University",
        "aff_unique_dep": "School of Computer and Information Technology",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0+0+0;0+0+0;0;0;0+0+0;0+0+0",
        "aff_campus_unique": "Taiyuan",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.655",
        "title": "Mitigating the Discrepancy Between Video and Text Temporal Sequences: A Time-Perception Enhanced Video Grounding method for LLM",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing video LLMs typically excel at capturing the overall description of a video but lack the ability to demonstrate an understanding of temporal dynamics and a fine-grained grasp of localized content within the video. In this paper, we propose a Time-Perception Enhanced Video Grounding via Boundary Perception and Temporal Reasoning aimed at mitigating LLMs\u2019 difficulties in understanding the discrepancies between video and text temporality. Specifically, to address the inherent biases in current datasets, we design a series of boundary-perception tasks to enable LLMs to capture accurate video temporality. To tackle LLMs\u2019 insufficient understanding of temporal information, we develop specialized tasks for boundary perception and temporal relationship reasoning to deepen LLMs\u2019 perception of video temporality. Our experimental results show significant improvements across three datasets: ActivityNet, Charades, and DiDeMo (achieving up to 11.2% improvement on R@0.3), demonstrating the effectiveness of our proposed temporal awareness-enhanced data construction method.",
        "author": "Xuefen Li; Bo Wang; Ge Shi; Chong Feng; Jiahao Teng",
        "authorids": "/x/xuefen-li/; /b/bo-wang/; /g/ge-shi/; /c/chong-feng/; /j/jiahao-teng/",
        "bibtex": "@inproceedings{li-etal-2025-mitigating,\n    title = \"Mitigating the Discrepancy Between Video and Text Temporal Sequences: A Time-Perception Enhanced Video Grounding method for {LLM}\",\n    author = \"Li, Xuefen  and\n      Wang, Bo  and\n      Shi, Ge  and\n      Feng, Chong  and\n      Teng, Jiahao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.655/\",\n    pages = \"9804--9813\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.655.pdf",
        "site": "https://aclanthology.org/2025.coling-main.655/",
        "pdf_size": 831325,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cpEJFApMGZsJ:scholar.google.com/&scioq=Mitigating+the+Discrepancy+Between+Video+and+Text+Temporal+Sequences:+A+Time-Perception+Enhanced+Video+Grounding+method+for+LLM&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Beijing University of Technology, China; Beijing Institute of Technology, China + Southeast Academy of Information Technology, Beijing Institute of Technology, China; Beijing University of Technology, China; Beijing Institute of Technology, China + Southeast Academy of Information Technology, Beijing Institute of Technology, China; Beijing University of Technology, China",
        "aff_domain": "bjut.edu.cn; ; ; ; ",
        "email": "bjut.edu.cn; ; ; ; ",
        "github": "https://github.com/lixuefenfen/TPE-VLLM",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+1;0;1+1;0",
        "aff_unique_norm": "Beijing University of Technology;Beijing Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bjut.edu.cn;http://www.bit.edu.cn/",
        "aff_unique_abbr": "BJUT;BIT",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.384",
        "title": "Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Integrating information from various reference databases is a major challenge for Retrieval-Augmented Generation (RAG) systems because each knowledge source adopts a unique data structure and follows different conventions. Retrieving from multiple knowledge sources with one fixed strategy usually leads to under-exploitation of information. To mitigate this drawback, inspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that dynamically determines the optimal granularity of a knowledge source based on input queries using a router. The router is efficiently trained with a newly proposed loss function employing soft labels. We further extend MoG to MoG-Graph (MoGG), where reference documents are pre-processed as graphs, enabling the retrieval of distantly situated snippets. Experiments demonstrate that MoG and MoGG effectively predict optimal granularity levels, significantly enhancing the performance of the RAG system in downstream tasks. The code of both MoG and MoGG will be made public.",
        "author": "Zijie Zhong; Hanwen Liu; Xiaoya Cui; Xiaofan Zhang; Zengchang Qin",
        "authorids": "/z/zijie-zhong/; /h/hanwen-liu/; /x/xiaoya-cui/; /x/xiaofan-zhang/; /z/zengchang-qin/",
        "bibtex": "@inproceedings{zhong-etal-2025-mix,\n    title = \"Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation\",\n    author = \"Zhong, Zijie  and\n      Liu, Hanwen  and\n      Cui, Xiaoya  and\n      Zhang, Xiaofan  and\n      Qin, Zengchang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.384/\",\n    pages = \"5756--5774\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.384.pdf",
        "site": "https://aclanthology.org/2025.coling-main.384/",
        "pdf_size": 2624238,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5554296369574807071&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai AI Laboratory; Beihang University; Beihang University; Shanghai AI Laboratory+Beihang University; Beihang University",
        "aff_domain": "pjlab.org.cn;buaa.edu.cn;buaa.edu.cn;pjlab.org.cn;buaa.edu.cn",
        "email": "pjlab.org.cn;buaa.edu.cn;buaa.edu.cn;pjlab.org.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0+1;1",
        "aff_unique_norm": "Shanghai AI Laboratory;Beihang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.shanghai-ai-lab.com;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "SAIL;BUAA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.679",
        "title": "MoKA:Parameter Efficiency Fine-Tuning via Mixture of Kronecker Product Adaption",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid development of large language models (LLMs), traditional full-parameter fine-tuning methods have become increasingly expensive in terms of computational resources and time costs. For this reason, parameter efficient fine-tuning (PEFT) methods have emerged. Among them, Low-Rank Adaptation (LoRA) is one of the current popular PEFT methods, which is widely used in large language models. However, the low-rank update mechanism of LoRA somewhat limits its ability to approximate full-parameter fine-tuning during the training process. In this paper, we propose a novel PEFT framework, MoKA (Mixture of Kronecker Product Adaptation), which combines the Kronecker product with the Mixture-of-Experts (MoE) method. By replacing the low-rank decomposition of the weight update matrix with Kronecker products and utilizing a sparse MoE architecture, MoKA achieves parameter efficiency and better model performance. Additionally, we design an efficient routing module to further compress the parameter size. We conduct extensive experiments on the GLUE benchmark, E2E NLG Challenge, and instruction tuning tasks for LLMs. The results demonstrate that MoKA outperforms existing PEFT methods.",
        "author": "Beiming Yu; Zhenfei Yang; Xiushuang Yi",
        "authorids": "/b/beiming-yu/; /z/zhenfei-yang/; /x/xiushuang-yi/",
        "bibtex": "https://aclanthology.org/2025.coling-main.679.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.679.pdf",
        "site": "https://aclanthology.org/2025.coling-main.679/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BClLjEfXA-sJ:scholar.google.com/&scioq=MoKA:Parameter+Efficiency+Fine-Tuning+via+Mixture+of+Kronecker+Product+Adaption&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.111",
        "title": "MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, LoRA has emerged as a crucial technique for fine-tuning large pre-trained models, yet its performance in multi-task learning scenarios often falls short. In contrast, the MoE architecture presents a natural solution to this issue. However, it introduces challenges such as mutual interference of data across multiple domains and knowledge forgetting of various tasks. Additionally, MoE significantly increases the number of parameters, posing a computational cost challenge. Therefore, in this paper, we propose MoSLD, a mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these challenges by sharing the upper projection matrix in LoRA among different experts, encouraging the model to learn general knowledge across tasks, while still allowing the lower projection matrix to focus on the unique features of each task. The application of dropout alleviates the imbalanced update of parameter matrix and mitigates parameter overfitting in LoRA. Extensive experiments demonstrate that our model exhibits excellent performance in both single-task and multi-task scenarios, with robust out-of-domain generalization capabilities.",
        "author": "Lulu Zhao; Weihao Zeng; Shi Xiaofeng; Hua Zhou",
        "authorids": "/l/lulu-zhao/; /w/weihao-zeng/; /s/shi-xiaofeng/; /h/hua-zhou/",
        "bibtex": "@inproceedings{zhao-etal-2025-mosld,\n    title = \"{M}o{SLD}: An Extremely Parameter-Efficient Mixture-of-Shared {L}o{RA}s for Multi-Task Learning\",\n    author = \"Zhao, Lulu  and\n      Zeng, Weihao  and\n      Xiaofeng, Shi  and\n      Zhou, Hua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.111/\",\n    pages = \"1647--1659\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.111.pdf",
        "site": "https://aclanthology.org/2025.coling-main.111/",
        "pdf_size": 1029109,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7686525595046498391&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing Academy of Artificial Intelligence (BAAI); School of Artificial Intelligence, Beijing University of Posts and Telecommunications; Beijing Academy of Artificial Intelligence (BAAI); Beijing Academy of Artificial Intelligence (BAAI)",
        "aff_domain": "baai.ac.cn; ; ; ",
        "email": "baai.ac.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Beijing Academy of Artificial Intelligence;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": ";School of Artificial Intelligence",
        "aff_unique_url": "https://www.baai.ac.cn;http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BAAI;BUPT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.425",
        "title": "ModaFact: Multi-paradigm Evaluation for Joint Event Modality and Factuality Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Factuality and modality are two crucial aspects concerning events, since they convey the speaker\u2019s commitment to a situation in discourse as well as how this event is supposed to occur in terms of norms, wishes, necessity, duty and so on. Capturing them both is necessary to truly understand an utterance meaning and the speaker\u2019s perspective with respect to a mentioned event. Yet, NLP studies have mostly dealt with these two aspects separately, mainly devoting past efforts to the development of English datasets. In this work, we propose ModaFact, a novel resource with joint factuality and modality information for event-denoting expressions in Italian. We propose a novel annotation scheme, which however is consistent with existing ones, and compare different classification systems trained on ModaFact, as a preliminary step to the use of factuality and modality information in downstream tasks. The dataset and the best-performing model are publicly released and available under an open license.",
        "author": "Marco Rovera; Serena Cristoforetti; Sara Tonelli",
        "authorids": "/m/marco-rovera/; /s/serena-cristoforetti/; /s/sara-tonelli/",
        "bibtex": "@inproceedings{rovera-etal-2025-modafact,\n    title = \"{M}oda{F}act: Multi-paradigm Evaluation for Joint Event Modality and Factuality Detection\",\n    author = \"Rovera, Marco  and\n      Cristoforetti, Serena  and\n      Tonelli, Sara\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.425/\",\n    pages = \"6378--6396\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.425.pdf",
        "site": "https://aclanthology.org/2025.coling-main.425/",
        "pdf_size": 394151,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15436535682909377654&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy",
        "aff_domain": "fbk.eu; ;fbk.eu",
        "email": "fbk.eu; ;fbk.eu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Fondazione Bruno Kessler",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.fbk.eu",
        "aff_unique_abbr": "FBK",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Trento",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2025.coling-main.309",
        "title": "Modal Feature Optimization Network with Prompt for Multimodal Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal sentiment analysis(MSA) is mostly used to understand human emotional states through multimodal. However, due to the fact that the effective information carried by multimodal is not balanced, the modality containing less effective information cannot fully play the complementary role between modalities. Therefore, the goal of this paper is to fully explore the effective information in modalities and further optimize the under-optimized modal representation.To this end, we propose a novel Modal Feature Optimization Network (MFON) with a Modal Prompt Attention (MPA) mechanism for MSA. Specifically, we first determine which modalities are under-optimized in MSA, and then use relevant prompt information to focus the model on these features. This allows the model to focus more on the features of the modalities that need optimization, improving the utilization of each modality\u2019s feature representation and facilitating initial information aggregation across modalities. Subsequently, we design an intra-modal knowledge distillation strategy for under-optimized modalities. This approach preserves the integrity of the modal features. Furthermore, we implement inter-modal contrastive learning to better extract related features across modalities, thereby optimizing the entire network. Finally, sentiment prediction is carried out through the effective fusion of multimodal information. Extensive experimental results on public benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art models.",
        "author": "Xiangmin Zhang; Wei Wei; Shihao Zou",
        "authorids": "/x/xiangmin-zhang/; /w/wei-wei/; /s/shihao-zou/",
        "bibtex": "@inproceedings{zhang-etal-2025-modal,\n    title = \"Modal Feature Optimization Network with Prompt for Multimodal Sentiment Analysis\",\n    author = \"Zhang, Xiangmin  and\n      Wei, Wei  and\n      Zou, Shihao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.309/\",\n    pages = \"4611--4621\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.309.pdf",
        "site": "https://aclanthology.org/2025.coling-main.309/",
        "pdf_size": 513020,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15374550218552851161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL)",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.550",
        "title": "Momentum Posterior Regularization for Multi-hop Dense Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-hop question answering (QA) often requires sequential retrieval (multi-hop retrieval), where each hop retrieves missing knowledge based on information from previous hops. To facilitate more effective retrieval, we aim to distill knowledge from a posterior retrieval, which has access to posterior information like an answer, into a prior retrieval used during inference when such information is unavailable. Unfortunately, current methods for knowledge distillation in one-time retrieval are ineffective for multi-hop QA due to two issues: 1) posterior information is often defined as the response (i.e. answers), which may not clearly connect to the query without intermediate retrieval; and 2) the large knowledge gap between prior and posterior retrievals makes distillation using existing methods unstable, even resulting in performance loss. As such, we propose MoPo (Momentum Posterior Regularization) with two key innovations: 1) Posterior information of one hop is defined as a query-focus summary from the golden knowledge of the previous and current hops; 2) We develop an effective training strategy where the posterior retrieval is updated along with the prior retrieval via momentum moving average method, allowing smoother and effective distillation. Experiments on HotpotQA and StrategyQA demonstrate that MoPo outperforms existing baselines in both retrieval and downstream QA tasks.",
        "author": "Zehua Xia; Yuyang Wu; Yiyun Xia; Cam Tu Nguyen",
        "authorids": "/z/zehua-xia/; /y/yuyang-wu/; /y/yiyun-xia/; /c/cam-tu-nguyen/",
        "bibtex": "@inproceedings{xia-etal-2025-momentum,\n    title = \"Momentum Posterior Regularization for Multi-hop Dense Retrieval\",\n    author = \"Xia, Zehua  and\n      Wu, Yuyang  and\n      Xia, Yiyun  and\n      Nguyen, Cam Tu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.550/\",\n    pages = \"8255--8271\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.550.pdf",
        "site": "https://aclanthology.org/2025.coling-main.550/",
        "pdf_size": 929777,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:supLbExsSg0J:scholar.google.com/&scioq=Momentum+Posterior+Regularization+for+Multi-hop+Dense+Retrieval&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University; School of Computer Science, Nanjing University + School of Artificial Intelligence, Nanjing University; School of Artificial Intelligence, Nanjing University; State Key Laboratory for Novel Software Technology, Nanjing University + School of Artificial Intelligence, Nanjing University",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/zeaver/mopo",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+0;0;0+0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing University",
        "aff_campus_unique_index": "1+1;1;1",
        "aff_campus_unique": ";Nanjing",
        "aff_country_unique_index": "0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.370",
        "title": "Montague semantics and modifier consistency measurement in neural language models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This work proposes a novel methodology for measuring compositional behavior in contemporary language embedding models. Specifically, we focus on adjectival modifier phenomena in adjective-noun phrases. In recent years, distributional language representation models have demonstrated great practical success. At the same time, the need for interpretability has elicited questions on their intrinsic properties and capabilities. Crucially, distributional models are often inconsistent when dealing with compositional phenomena in natural language, which has significant implications for their safety and fairness. Despite this, most current research on compositionality is directed towards improving their performance on similarity tasks only. This work takes a different approach, introducing three novel tests of compositional behavior inspired by Montague semantics. Our experimental results indicate that current neural language models do not behave according to the expected linguistic theories. This indicates that current language models may lack the capability to capture the semantic properties we evaluated on limited context, or that linguistic theories from Montagovian tradition may not match the expected capabilities of distributional models.",
        "author": "Danilo Silva de Carvalho; Edoardo Manino; Julia Rozanova; Lucas Cordeiro; Andr\u00e9 Freitas",
        "authorids": "/d/danilo-silva-de-carvalho/; /e/edoardo-manino/; /j/julia-rozanova/; /l/lucas-cordeiro/; /a/andre-freitas/",
        "bibtex": "@inproceedings{silva-de-carvalho-etal-2025-montague,\n    title = \"{M}ontague semantics and modifier consistency measurement in neural language models\",\n    author = \"Silva de Carvalho, Danilo  and\n      Manino, Edoardo  and\n      Rozanova, Julia  and\n      Cordeiro, Lucas  and\n      Freitas, Andr{\\'e}\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.370/\",\n    pages = \"5515--5529\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.370.pdf",
        "site": "https://aclanthology.org/2025.coling-main.370/",
        "pdf_size": 394967,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4031802109266884495&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.71",
        "title": "Monte Carlo Tree Search Based Prompt Autogeneration for Jailbreak Attacks against LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Jailbreak attacks craft specific prompts or append adversarial suffixes to prompts, thereby inducing language models to generate harmful or unethical content and bypassing the model\u2019s safety guardrails. With the recent blossom of large language models (LLMs), there\u2019s a growing focus on jailbreak attacks to probe their safety. While current white-box attacks typically focus on meticulously identifying adversarial suffixes for specific models, their effectiveness and efficiency diminish when applied to different LLMs. In this paper, we propose a Monte Carlo Tree Search (MCTS) based Prompt Auto-generation (MPA) method to enhance the effectiveness and efficiency of attacks across various models. MPA automatically searches for and generates adversarial suffixes for valid jailbreak attacks. Specifically, we first identify a series of action candidates that could potentially trick LLMs into providing harmful responses. To streamline the exploration of adversarial suffixes, we design a prior confidence probability for each MCTS node. We then iteratively auto-generate adversarial prompts using the MCTS framework. Extensive experiments on multiple open-source models (like Llama, Gemma, and Mistral) and closed-source models (such as ChatGPT) show that our proposed MPA surpasses existing methods in search efficiency as well as attack effectiveness. The codes are available at https://github.com/KDEGroup/MPA.",
        "author": "Suhuang Wu; Huimin Wang; Yutian Zhao; Xian Wu; Yefeng Zheng; Wei Li; Hui Li; Rongrong Ji",
        "authorids": "/s/suhuang-wu/; /h/huimin-wang/; /y/yutian-zhao/; /x/xian-wu/; /y/yefeng-zheng/; /w/wei-li/; /h/hui-li/; /r/rongrong-ji/",
        "bibtex": "https://aclanthology.org/2025.coling-main.71.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.71.pdf",
        "site": "https://aclanthology.org/2025.coling-main.71/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Wf6wYZsvwLoJ:scholar.google.com/&scioq=Monte+Carlo+Tree+Search+Based+Prompt+Autogeneration+for+Jailbreak+Attacks+against+LLMs&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.647",
        "title": "MuKA: Multimodal Knowledge Augmented Visual Information-Seeking",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The visual information-seeking task aims to answer visual questions that require external knowledge, such as \u201cOn what date did this building officially open?\u201d. Existing methods using retrieval-augmented generation framework primarily rely on textual knowledge bases to assist multimodal large language models (MLLMs) in answering questions. However, the text-only knowledge can impair information retrieval for the multimodal query of image and question, and also confuse MLLMs in selecting the most relevant information during generation. In this work, we propose a novel framework MuKA which leverages a multimodal knowledge base to address these limitations. Specifically, we construct a multimodal knowledge base by automatically pairing images with text passages in existing datasets. We then design a fine-grained multimodal interaction to effectively retrieve multimodal documents and enrich MLLMs with both retrieved texts and images. MuKA outperforms state-of-the-art methods by 38.7% and 15.9% on the InfoSeek and E-VQA benchmark respectively, demonstrating the importance of multimodal knowledge in enhancing both retrieval and answer generation.",
        "author": "Lianghao Deng; Yuchong Sun; Shizhe Chen; Ning Yang; Yunfeng Wang; Ruihua Song",
        "authorids": "/l/lianghao-deng/; /y/yuchong-sun/; /s/shizhe-chen/; /n/ning-yang/; /y/yunfeng-wang/; /r/ruihua-song/",
        "bibtex": "@inproceedings{deng-etal-2025-muka,\n    title = \"{M}u{KA}: Multimodal Knowledge Augmented Visual Information-Seeking\",\n    author = \"Deng, Lianghao  and\n      Sun, Yuchong  and\n      Chen, Shizhe  and\n      Yang, Ning  and\n      Wang, Yunfeng  and\n      Song, Ruihua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.647/\",\n    pages = \"9675--9686\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.647.pdf",
        "site": "https://aclanthology.org/2025.coling-main.647/",
        "pdf_size": 1052123,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=354258220551564515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Inria, \u00c9cole normale sup\u00e9rieure, CNRS, PSL Research University; ZHI-TECH GROUP; ZHI-TECH GROUP; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn; ; ; ; ;outlook.com",
        "email": "ruc.edu.cn; ; ; ; ;outlook.com",
        "github": "https://github.com/lhdeng-gh/MuKA",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;2;0",
        "aff_unique_norm": "Renmin University of China;Inria;ZHI-TECH GROUP",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;;",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.inria.fr;",
        "aff_unique_abbr": "RUC;Inria;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "China;France;"
    },
    {
        "id": "2025.coling-main.407",
        "title": "MuPe Life Stories Dataset: Spontaneous Speech in Brazilian Portuguese with a Case Study Evaluation on ASR Bias against Speakers Groups and Topic Modeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, several public datasets for automatic speech recognition (ASR) in Brazilian Portuguese (BP) have been released, improving ASR systems performance. However, these datasets lack diversity in terms of age groups, regional accents, and education levels. In this paper, we present a new publicly available dataset consisting of 289 life story interviews (365 hours), featuring a broad range of speakers varying in age, education, and regional accents. First, we demonstrated the presence of bias in current BP ASR models concerning education levels and age groups. Second, we showed that our dataset helps mitigate these biases. Additionally, an ASR model trained on our dataset performed better during evaluation on a diverse test set. Finally, the ASR model trained with our dataset was extrinsically evaluated through a topic modeling task that utilized the automatically transcribed output.",
        "author": "Sidney Evaldo Leal; Arnaldo Candido Junior; Ricardo Marcacini; Edresson Casanova; Odilon Gon\u00e7alves; Anderson Silva Soares; Rodrigo Freitas Lima; Lucas Rafael Stefanel Gris; Sandra Alu\u00edsio",
        "authorids": "/s/sidney-evaldo-leal/; /a/arnaldo-candido-junior/; /r/ricardo-marcacini/; /e/edresson-casanova/; /o/odilon-goncalves/; /a/anderson-silva-soares/; /r/rodrigo-freitas-lima/; /l/lucas-rafael-stefanel-gris/; /s/sandra-aluisio/",
        "bibtex": "@inproceedings{evaldo-leal-etal-2025-mupe,\n    title = \"{M}u{P}e Life Stories Dataset: Spontaneous Speech in {B}razilian {P}ortuguese with a Case Study Evaluation on {ASR} Bias against Speakers Groups and Topic Modeling\",\n    author = \"Evaldo Leal, Sidney  and\n      Candido Junior, Arnaldo  and\n      Marcacini, Ricardo  and\n      Casanova, Edresson  and\n      Gon{\\c{c}}alves, Odilon  and\n      Silva Soares, Anderson  and\n      Freitas Lima, Rodrigo  and\n      Stefanel Gris, Lucas Rafael  and\n      Alu{\\'i}sio, Sandra\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.407/\",\n    pages = \"6076--6087\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.407.pdf",
        "site": "https://aclanthology.org/2025.coling-main.407/",
        "pdf_size": 641405,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:KxqbVdkncOsJ:scholar.google.com/&scioq=MuPe+Life+Stories+Dataset:+Spontaneous+Speech+in+Brazilian+Portuguese+with+a+Case+Study+Evaluation+on+ASR+Bias+against+Speakers+Groups+and+Topic+Modeling&hl=en&as_sdt=0,14",
        "gs_version_total": 2,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2025.coling-demos.13",
        "title": "MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Recent advancements in retrieval-augmented generation have demonstrated impressive performance on the question-answering task. However, most previous work predominantly focuses on text-based answers. Although some studies have explored multimodal data, they still fall short in generating comprehensive multimodal answers, especially step-by-step tutorials for accomplishing specific goals. This capability is especially valuable in application scenarios such as enterprise chatbots, customer service systems, and educational platforms. In this paper, we propose a simple and effective framework, MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR starts by generating an initial text answer based on the user\u2019s question. It then retrieves multimodal data relevant to the snippets of the initial text answer. By leveraging the retrieved multimodal data and contextual features, MuRAR refines the initial text answer to create a more comprehensive and informative response. This highly adaptable framework can be easily integrated into an enterprise chatbot to produce multimodal answers with minimal modifications. Human evaluations demonstrate that the multimodal answers generated by MuRAR are significantly more useful and readable than plain text responses. A video demo of MuRAR is available at https://youtu.be/ykGRtyVVQpU.",
        "author": "Zhengyuan Zhu; Daniel Lee; Hong Zhang; Sai Sree Harsha; Loic Feujio; Akash Maharaj; Yunyao Li",
        "authorids": "/z/zhengyuan-zhu/; /d/daniel-lee/; /h/hong-zhang/; /s/sai-sree-harsha/; /l/loic-feujio/; /a/akash-maharaj/; /y/yunyao-li/",
        "bibtex": "@inproceedings{zhu-etal-2025-murar,\n    title = \"{M}u{RAR}: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering\",\n    author = \"Zhu, Zhengyuan  and\n      Lee, Daniel  and\n      Zhang, Hong  and\n      Sree Harsha, Sai  and\n      Feujio, Loic  and\n      Maharaj, Akash  and\n      Li, Yunyao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.13/\",\n    pages = \"126--135\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.13.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.13/",
        "pdf_size": 2105376,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14826878526297929606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Texas at Arlington; Adobe; Adobe; Adobe; Adobe; Adobe; Adobe",
        "aff_domain": "mavs.uta.edu;adobe.com; ; ; ; ;adobe.com",
        "email": "mavs.uta.edu;adobe.com; ; ; ; ;adobe.com",
        "github": "",
        "project": "https://youtu.be/ykGRtyVVQpU",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "University of Texas at Arlington;Adobe Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uta.edu;https://www.adobe.com",
        "aff_unique_abbr": "UTA;Adobe",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Arlington;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.92",
        "title": "Multi-Graph Co-Training for Capturing User Intent in Session-based Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Session-based recommendation focuses on predicting the next item a user will interact with based on sequences of anonymous user sessions. A significant challenge in this field is data sparsity due to the typically short-term interactions. Most existing methods rely heavily on users\u2019 current interactions, overlooking the wealth of auxiliary information available. To address this, we propose a novel model, the Multi-Graph Co-Training model (MGCOT), which leverages not only the current session graph but also similar session graphs and a global item relation graph. This approach allows for a more comprehensive exploration of intrinsic relationships and better captures user intent from multiple views, enabling session representations to complement each other. Additionally, MGCOT employs multi-head attention mechanisms to effectively capture relevant session intent and uses contrastive learning to form accurate and robust session representations. Extensive experiments on three datasets demonstrate that MGCOT significantly enhances the performance of session-based recommendations, particularly on the Diginetica dataset, achieving improvements up to 2.00% in P@20 and 10.70% in MRR@20. Resources have been made publicly available in our GitHub repository https://github.com/liang-tian-tian/MGCOT.",
        "author": "Zhe Yang; Tiantian Liang",
        "authorids": "/z/zhe-yang/; /t/tiantian-liang/",
        "bibtex": "@inproceedings{yang-liang-2025-multi,\n    title = \"Multi-Graph Co-Training for Capturing User Intent in Session-based Recommendation\",\n    author = \"Yang, Zhe  and\n      Liang, Tiantian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.92/\",\n    pages = \"1377--1386\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.92.pdf",
        "site": "https://aclanthology.org/2025.coling-main.92/",
        "pdf_size": 585513,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qmTOgNq3qf4J:scholar.google.com/&scioq=Multi-Graph+Co-Training+for+Capturing+User+Intent+in+Session-based+Recommendation&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "suda.edu.cn;stu.suda.edu.cn",
        "email": "suda.edu.cn;stu.suda.edu.cn",
        "github": "https://github.com/liang-tian-tian/MGCOT",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.408",
        "title": "Multi-Layered Evaluation Using a Fusion of Metrics and LLMs as Judges in Open-Domain Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic evaluation of machine-generated texts, such as answers in open-domain question answering (Open-Domain QA), presents a complex challenge involving cost efficiency, hardware constraints, and high accuracy. Although various metrics exist for comparing machine-generated answers with reference (gold standard) answers, ranging from lexical metrics (e.g., exact match) to semantic ones (e.g., cosine similarity) and using large language models (LLMs) as judges, none of these approaches achieves perfect performance in terms of accuracy or cost. To address this issue, we propose two approaches to enhance evaluation. First, we summarize long answers and use the shortened versions in the evaluation process, demonstrating that this adjustment significantly improves both lexical matching and semantic-based metrics evaluation results. Second, we introduce a multi-layered evaluation methodology that combines different metrics tailored to various scenarios. This combination of simple metrics delivers performance comparable to LLMs as judges but at lower costs. Moreover, our fused approach, which integrates both lexical and semantic metrics with LLMs through our formula, outperforms previous evaluation solutions.",
        "author": "Rashin Rahnamoun; Mehrnoush Shamsfard",
        "authorids": "/r/rashin-rahnamoun/; /m/mehrnoush-shamsfard/",
        "bibtex": "@inproceedings{rahnamoun-shamsfard-2025-multi,\n    title = \"Multi-Layered Evaluation Using a Fusion of Metrics and {LLM}s as Judges in Open-Domain Question Answering\",\n    author = \"Rahnamoun, Rashin  and\n      Shamsfard, Mehrnoush\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.408/\",\n    pages = \"6088--6104\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.408.pdf",
        "site": "https://aclanthology.org/2025.coling-main.408/",
        "pdf_size": 667803,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YWEjf9CDiakJ:scholar.google.com/&scioq=Multi-Layered+Evaluation+Using+a+Fusion+of+Metrics+and+LLMs+as+Judges+in+Open-Domain+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Shahid Beheshti University, Tehran, Iran; Shahid Beheshti University, Tehran, Iran",
        "aff_domain": "gmail.com;sbu.ac.ir",
        "email": "gmail.com;sbu.ac.ir",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shahid Beheshti University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sbu.ac.ir",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tehran",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2025.coling-main.582",
        "title": "Multi-Modal Entities Matter: Benchmarking Multi-Modal Entity Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-modal entity alignment (MMEA) is a long-standing task that aims to discover identical entities between different multi-modal knowledge graphs (MMKGs). However, most of the existing MMEA datasets consider the multi-modal data as the attributes of textual entities, while neglecting the correlations among the multi-modal data and do not fit in the real-world scenarios well. In response, in this work, we establish a novel yet practical MMEA dataset, i.e. NMMEA, which models multi-modal data (e.g., images) equally as textual entities in the MMKG. Due to the introduction of multi-modal data, NMMEA poses new challenges to existing MMEA solutions, i.e., heterogeneous structural representation learning and cross-modal alignment inference. Hence, we put forward a simple yet effective solution, CrossEA, which can effectively learn the structural information of entities by considering both intra-modal and cross-modal relations, and further infer the similarity of different types of entity pairs. Extensive experiments validate the significance of NMMEA, where CrossEA can achieve superior performance in contrast to competitive methods on the proposed dataset.",
        "author": "GuanChen Xiao; WeiXin Zeng; ShiQi Zhang; MingRui Lao; Xiang Zhao",
        "authorids": "/g/guanchen-xiao/; /w/weixin-zeng/; /s/shiqi-zhang/; /m/mingrui-lao/; /x/xiang-zhao/",
        "bibtex": "@inproceedings{xiao-etal-2025-multi,\n    title = \"Multi-Modal Entities Matter: Benchmarking Multi-Modal Entity Alignment\",\n    author = \"Xiao, GuanChen  and\n      Zeng, WeiXin  and\n      Zhang, ShiQi  and\n      Lao, MingRui  and\n      Zhao, Xiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.582/\",\n    pages = \"8714--8724\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.582.pdf",
        "site": "https://aclanthology.org/2025.coling-main.582/",
        "pdf_size": 741538,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:7ChXqX4WXhwJ:scholar.google.com/&scioq=Multi-Modal+Entities+Matter:+Benchmarking+Multi-Modal+Entity+Alignment&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology",
        "aff_domain": "qq.com; ; ; ; ",
        "email": "qq.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.nudt.edu.cn/",
        "aff_unique_abbr": "NUDT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.414",
        "title": "Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slips",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study presents a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, focusing on the Chu bamboo slip (CBS) script used during the Spring and Autumn and Warring States period (771-256 BCE) in Ancient China. Considering the complex hierarchical structure of ancient Chinese scripts, where a single character may be a combination of multiple sub-characters, our tokenizer first adopts character detection to locate character boundaries. Then it conducts character recognition at both the character and sub-character levels. Moreover, to support the academic community, we assembled the first large-scale dataset of CBSs with over 100K annotated character image scans. On the part-of-speech tagging task built on our dataset, using our tokenizer gives a 5.5% relative improvement in F1-score compared to mainstream sub-word tokenizers. Our work not only aids in further investigations of the specific script but also has the potential to advance research on other forms of ancient Chinese scripts.",
        "author": "Yingfa Chen; Chenlong Hu; Cong Feng; Chenyang Song; Shi Yu; Xu Han; Zhiyuan Liu; Maosong Sun",
        "authorids": "/y/yingfa-chen/; /c/chenlong-hu/; /c/cong-feng/; /c/chenyang-song/; /s/shi-yu/; /x/xu-han/; /z/zhiyuan-liu/; /m/maosong-sun/",
        "bibtex": "@inproceedings{chen-etal-2025-multi,\n    title = \"Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slips\",\n    author = \"Chen, Yingfa  and\n      Hu, Chenlong  and\n      Feng, Cong  and\n      Song, Chenyang  and\n      Yu, Shi  and\n      Han, Xu  and\n      Liu, Zhiyuan  and\n      Sun, Maosong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.414/\",\n    pages = \"6201--6211\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.414.pdf",
        "site": "https://aclanthology.org/2025.coling-main.414/",
        "pdf_size": 4519034,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AbPQP1vN91QJ:scholar.google.com/&scioq=Multi-Modal+Multi-Granularity+Tokenizer+for+Chu+Bamboo+Slips&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Tsinghua University; Tsinghua University; Henan Polytechnic University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University",
        "aff_domain": "gmail.com; ; ; ; ; ; ;",
        "email": "gmail.com; ; ; ; ; ; ;",
        "github": "https://www.github.com/THUNLP/Chujian",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;0;0;0;0;0",
        "aff_unique_norm": "Tsinghua University;Henan Polytechnic University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;http://www.hpu.edu.cn",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.119",
        "title": "Multi-View Incongruity Learning for Multimodal Sarcasm Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal sarcasm detection (MSD) is essential for various downstream tasks. Existing MSD methods tend to rely on spurious correlations. These methods often mistakenly prioritize non-essential features yet still make correct predictions, demonstrating poor generalizability beyond training environments. Regarding this phenomenon, this paper undertakes several initiatives. Firstly, we identify two primary causes that lead to the reliance of spurious correlations. Secondly, we address these challenges by proposing a novel method that integrate Multimodal Incongruities via Contrastive Learning (MICL) for multimodal sarcasm detection. Specifically, we first leverage incongruity to drive multi-view learning from three views: token-patch, entity-object, and sentiment. Then, we introduce extensive data augmentation to mitigate the biased learning of the textual modality. Additionally, we construct a test set, SPMSD, which consists potential spurious correlations to evaluate the the model\u2019s generalizability. Experimental results demonstrate the superiority of MICL on benchmark datasets, along with the analyses showcasing MICL\u2019s advancement in mitigating the effect of spurious correlation.",
        "author": "Diandian Guo; Cong Cao; Fangfang Yuan; Yanbing Liu; Guangjie Zeng; Xiaoyan Yu; Hao Peng; Philip S. Yu",
        "authorids": "/d/diandian-guo/; /c/cong-cao/; /f/fangfang-yuan/; /y/yanbing-liu/; /g/guangjie-zeng/; /x/xiaoyan-yu/; /h/hao-peng/; /p/philip-s-yu/",
        "bibtex": "@inproceedings{guo-etal-2025-multi,\n    title = \"Multi-View Incongruity Learning for Multimodal Sarcasm Detection\",\n    author = \"Guo, Diandian  and\n      Cao, Cong  and\n      Yuan, Fangfang  and\n      Liu, Yanbing  and\n      Zeng, Guangjie  and\n      Yu, Xiaoyan  and\n      Peng, Hao  and\n      Yu, Philip S.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.119/\",\n    pages = \"1754--1766\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.119.pdf",
        "site": "https://aclanthology.org/2025.coling-main.119/",
        "pdf_size": 6813774,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12952644756350124531&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; State Key Laboratory of Software Development Environment, Beihang University; School of Computer Science and Technology, Beijing Institute of Technology; State Key Laboratory of Software Development Environment, Beihang University; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;buaa.edu.cn;bit.edu.cn;buaa.edu.cn;uic.edu",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;buaa.edu.cn;bit.edu.cn;buaa.edu.cn;uic.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0;0;0+1;2;3;2;4",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beihang University;Beijing Institute of Technology;University of Illinois at Chicago",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;State Key Laboratory of Software Development Environment;School of Computer Science and Technology;Department of Computer Science",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;http://www.buaa.edu.cn;http://www.bit.edu.cn/;https://www.uic.edu",
        "aff_unique_abbr": "CAS;UCAS;Beihang;BIT;UIC",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0+0;0;0;0+0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.113",
        "title": "Multi-perspective Preference Alignment of LLMs for Programming-Community Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Programming-Community Question Answering (PCQA) aims to tackle issues through generating functional code and guiding descriptions. It involves multiple candidates, with different users having varying preferences for them. Additionally, one may contain outdated APIs. These undoubtedly present a challenge for responsing that meet user preferences. Recently, Reinforcement Learning from Human Feedback demonstrates its ability to precisely control the behavior of large language models (LLMs) to yield human-like responses. However, applying it to LLMs in domain-specific PCQA remains unexplored. In this work, we propose Multi-perspective Preference Alignment for Programming-Community Question Answering to generate user-centric responses, called MupPCQA. It includes three stages: Preference Standardization to control content quality, Preference Integration to consider diverse user tendencies, Preference Timeliness Mitigation to alleviate outdated answers. Extensive experiments on a high-quality, real-world PCQA dataset validate its accuracy and preference. Compared to its base model, MupPCQA shows an improvement of nearly 11% in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore.",
        "author": "Hongyu Yang; Jiahui Hou; Liyang He; Rui Li",
        "authorids": "/h/hongyu-yang/; /j/jiahui-hou/; /l/liyang-he/; /r/rui-li/",
        "bibtex": "@inproceedings{yang-etal-2025-multi,\n    title = \"Multi-perspective Preference Alignment of {LLM}s for Programming-Community Question Answering\",\n    author = \"Yang, Hongyu  and\n      Hou, Jiahui  and\n      He, Liyang  and\n      Li, Rui\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.113/\",\n    pages = \"1667--1682\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.113.pdf",
        "site": "https://aclanthology.org/2025.coling-main.113/",
        "pdf_size": 2095243,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:86cbzi0Bo1oJ:scholar.google.com/&scioq=Multi-perspective+Preference+Alignment+of+LLMs+for+Programming-Community+Question+Answering&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;mail.ustc.edu.cn;mail.ustc.edu.cn",
        "github": "https://github.com/YHY0607/PETCoQA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.8",
        "title": "Multilingual Continual Learning using Attention Distillation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Query-product relevance classification is crucial for e-commerce stores like Amazon, ensuring accurate search results that match customer intent. Using a unified multilingual model across multiple languages/marketplaces tends to yield superior outcomes but also presents challenges, especially in maintaining performance across all languages when the model is updated or expanded to include a new one. To tackle this, we examine a multilingual continual learning (CL) framework focused on relevance classification tasks and address the issue of catastrophic forgetting. We propose a novel continual learning approach called attention distillation, which sequentially adds adapters for each new language and incorporates a fusion layer above language-specific adapters. This fusion layer distills attention scores from the previously trained fusion layer, focusing on the older adapters. Additionally, translating a portion of the new language data into older ones supports backward knowledge transfer. Our method reduces trainable parameters by 80%, enhancing computational efficiency and enabling frequent updates, while achieving a 1-3% ROC-AUC improvement over single marketplace baselines and outperforming SOTA CL methods on proprietary and external datasets.",
        "author": "Sanjay Agrawal; Deep Nayak; Vivek Varadarajan Sembium",
        "authorids": "/s/sanjay-agrawal/; /d/deep-nayak/; /v/vivek-varadarajan-sembium/",
        "bibtex": "@inproceedings{agrawal-etal-2025-multilingual,\n    title = \"Multilingual Continual Learning using Attention Distillation\",\n    author = \"Agrawal, Sanjay  and\n      Nayak, Deep  and\n      Sembium, Vivek Varadarajan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.8/\",\n    pages = \"91--99\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.8.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.8/",
        "pdf_size": 782406,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5259664489955902474&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon, India; Amazon, India; Amazon, India",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.in",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.385",
        "title": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously update factual knowledge across multiple languages within large language models (LLMs). Previous research indicates that the same knowledge across different languages within LLMs exhibits a degree of shareability. However, most existing MKE methods overlook the connections of the same knowledge between different languages, resulting in knowledge conflicts and limited edit performance. To address this issue, we first investigate how LLMs process multilingual factual knowledge and discover that the same factual knowledge in different languages generally activates a shared set of neurons, which we call language-agnostic factual neurons (LAFNs). These neurons represent the same factual knowledge shared across languages and imply the semantic connections among multilingual knowledge. Inspired by this finding, we propose a new MKE method by Locating and Updating Language-Agnostic Factual Neurons (LU-LAFNs) to edit multilingual knowledge simultaneously, which avoids knowledge conflicts and thus improves edit performance. Experimental results on Bi-ZsRE and MzsRE benchmarks demonstrate that our method achieves the best edit performance, indicating the effectiveness and importance of modeling the semantic connections among multilingual knowledge.",
        "author": "Xue Zhang; Yunlong Liang; Fandong Meng; Songming Zhang; Yufeng Chen; Jinan Xu; Jie Zhou",
        "authorids": "/x/xue-zhang/; /y/yunlong-liang/; /f/fandong-meng/; /s/songming-zhang/; /y/yufeng-chen/; /j/jinan-xu/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2025-multilingual,\n    title = \"Multilingual Knowledge Editing with Language-Agnostic Factual Neurons\",\n    author = \"Zhang, Xue  and\n      Liang, Yunlong  and\n      Meng, Fandong  and\n      Zhang, Songming  and\n      Chen, Yufeng  and\n      Xu, Jinan  and\n      Zhou, Jie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.385/\",\n    pages = \"5775--5788\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.385.pdf",
        "site": "https://aclanthology.org/2025.coling-main.385/",
        "pdf_size": 1921307,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9475061541936584664&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China+Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "bjtu.edu.cn;tencent.com;tencent.com;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "email": "bjtu.edu.cn;tencent.com;tencent.com;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;0;0;0;1",
        "aff_unique_norm": "Beijing Jiaotong University;Tencent Inc",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "http://www.bjtu.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "BJTU;Tencent",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.247",
        "title": "Multilingual Supervision Improves Semantic Disambiguation of Adpositions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Adpositions display a remarkable amount of ambiguity and flexibility in their meanings, and are used in different ways across languages. We conduct a systematic corpus-based cross-linguistic investigation into the lexical semantics of adpositions, utilizing SNACS (Schneider et al., 2018), an annotation framework with data available in several languages. Our investigation encompasses 5 of these languages: Chinese, English, Gujarati, Hindi, and Japanese. We find substantial distributional differences in adposition semantics, even in comparable corpora. We further train classifiers to disambiguate adpositions in each of our languages. Despite the cross-linguistic differences in adpositional usage, sharing annotated data across languages boosts overall disambiguation performance, leading to the highest published scores on this task for all 5 languages.",
        "author": "Wesley Scivetti; Lauren Levine; Nathan Schneider",
        "authorids": "/w/wesley-scivetti/; /l/lauren-levine/; /n/nathan-schneider/",
        "bibtex": "@inproceedings{scivetti-etal-2025-multilingual,\n    title = \"Multilingual Supervision Improves Semantic Disambiguation of Adpositions\",\n    author = \"Scivetti, Wesley  and\n      Levine, Lauren  and\n      Schneider, Nathan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.247/\",\n    pages = \"3655--3669\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.247.pdf",
        "site": "https://aclanthology.org/2025.coling-main.247/",
        "pdf_size": 492520,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:2bqZeZ7rH1sJ:scholar.google.com/&scioq=Multilingual+Supervision+Improves+Semantic+Disambiguation+of+Adpositions&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Georgetown University; Georgetown University; Georgetown University",
        "aff_domain": "georgetown.edu;georgetown.edu;georgetown.edu",
        "email": "georgetown.edu;georgetown.edu;georgetown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgetown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.georgetown.edu",
        "aff_unique_abbr": "GU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.535",
        "title": "Multilingual and Explainable Text Detoxification with Parallel Corpora",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Even with various regulations in place across countries and social media platforms (Government of India, 2021; European Parliament and Council of the European Union, 2022), digital abusive speech remains a significant issue. One potential approach to address this challenge is automatic text detoxification, a text style transfer (TST) approach that transforms toxic language into a more neutral or non-toxic form. To date, the availability of parallel corpora for the text detoxification task (Logacheva et al., 2022; Atwell et al., 2022; Dementieva et al., 2024a) has proven to be crucial for state-of-the-art approaches. With this work, we extend parallel text detoxification corpus to new languages\u2014German, Chinese, Arabic, Hindi, and Amharic\u2014testing in the extensive multilingual setup TST baselines. Next, we conduct the first of its kind an automated, explainable analysis of the descriptive features of both toxic and non-toxic sentences, diving deeply into the nuances, similarities, and differences of toxicity and detoxification across 9 languages. Finally, based on the obtained insights, we experiment with a novel text detoxification method inspired by the Chain-of-Thoughts reasoning approach, enhancing the prompting process through clustering on relevant descriptive attributes.",
        "author": "Daryna Dementieva; Nikolay Babakov; Amit Ronen; Abinew Ali Ayele; Naquee Rizwan; Florian Schneider; Xintong Wang; Seid Muhie Yimam; Daniil Moskovskiy; Elisei Stakovskii; Eran Kaufman; Ashraf Elnagar; Animesh Mukherjee; Alexander Panchenko",
        "authorids": "/d/daryna-dementieva/; /n/nikolay-babakov/; /a/amit-ronen/; /a/abinew-ali-ayele/; /n/naquee-rizwan/; /f/florian-schneider/; /x/xintong-wang/; /s/seid-muhie-yimam/; /d/daniil-moskovskiy/; /e/elisei-stakovskii/; /e/eran-kaufman/; /a/ashraf-elnagar/; /a/animesh-mukherjee/; /a/alexander-panchenko/",
        "bibtex": "@inproceedings{dementieva-etal-2025-multilingual,\n    title = \"Multilingual and Explainable Text Detoxification with Parallel Corpora\",\n    author = \"Dementieva, Daryna  and\n      Babakov, Nikolay  and\n      Ronen, Amit  and\n      Ayele, Abinew Ali  and\n      Rizwan, Naquee  and\n      Schneider, Florian  and\n      Wang, Xintong  and\n      Yimam, Seid Muhie  and\n      Moskovskiy, Daniil  and\n      Stakovskii, Elisei  and\n      Kaufman, Eran  and\n      Elnagar, Ashraf  and\n      Mukherjee, Animesh  and\n      Panchenko, Alexander\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.535/\",\n    pages = \"7998--8025\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.535.pdf",
        "site": "https://aclanthology.org/2025.coling-main.535/",
        "pdf_size": 3315758,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2017887299919697778&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Technical University of Munich; Universidade de Santiago de Compostela; Shenkar College; University of Hamburg+Bahir Dar University; University of Hamburg; IIT Kharagpur; University of Sharjah; Skoltech+AIRI; Skoltech+AIRI; University of North Carolina at Chapel Hill; Shenkar College; University of Sharjah; IIT Kharagpur; Skoltech+AIRI",
        "aff_domain": "tum.de; ;shenkar.ac.il;uham.de;iitkgp.ac.in;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;skoltech.ru;unc.edu;shenkar.ac.il;sharjah.ac.ae;iitkgp.ac.in;skol.tech",
        "email": "tum.de; ;shenkar.ac.il;uham.de;iitkgp.ac.in;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;skoltech.ru;unc.edu;shenkar.ac.il;sharjah.ac.ae;iitkgp.ac.in;skol.tech",
        "github": "",
        "project": "",
        "author_num": 14,
        "aff_unique_index": "0;1;2;3+4;3;5;6;7+8;7+8;9;2;6;5;7+8",
        "aff_unique_norm": "Technical University of Munich;Universidade de Santiago de Compostela;Shenkar College of Engineering and Design;University of Hamburg;Bahir Dar University;Indian Institute of Technology Kharagpur;University of Sharjah;Skolkovo Institute of Science and Technology;Artificial Intelligence Research Institute;University of North Carolina",
        "aff_unique_dep": ";;;;;;;;;",
        "aff_unique_url": "https://www.tum.de;https://www.usc.es;https://www.shenkar.ac.il;https://www.uni-hamburg.de;https://www.bdu.edu.et;https://www.iitkgp.ac.in;https://www.sharjah.ac.ae;https://www.skoltech.ru;https://www.airi.jp;https://www.unc.edu",
        "aff_unique_abbr": "TUM;USC;Shenkar;UHH;BDU;IIT KGP;UOS;Skoltech;AIRI;UNC",
        "aff_campus_unique_index": ";1;;;2;1;",
        "aff_campus_unique": ";Kharagpur;Chapel Hill",
        "aff_country_unique_index": "0;1;2;0+3;0;4;5;6+7;6+7;8;2;5;4;6+7",
        "aff_country_unique": "Germany;Spain;Israel;Ethiopia;India;United Arab Emirates;Russia;Japan;United States"
    },
    {
        "id": "2025.coling-main.22",
        "title": "Multimodal Aspect-Based Sentiment Analysis under Conditional Relation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract aspect terms from text-image pairs and identify their sentiments. Previous methods are based on the premise that the image contains the objects referred by the aspects within the text. However, this condition cannot always be met, resulting in a suboptimal performance. In this paper, we propose COnditional Relation based Sentiment Analysis framework (CORSA). Specifically, we design a conditional relation detector (CRD) to mitigate the impact of the unmet conditional image. Moreover, we design a visual object localizer (VOL) to locate the exact condition-related visual regions associated with the aspects. With CRD and VOL, our CORSA framework takes a multi-task form. In addition, to effectively learn CORSA we conduct two types of annotations. One is the conditional relation using a pretrained referring expression comprehension model; the other is the bounding boxes of visual objects by a pretrained object detection model. Experiments on our built C-MABSA dataset show that CORSA consistently outperforms existing methods. The code and data are available at https://github.com/Liuxj-Anya/CORSA.",
        "author": "Xinjing Liu; Ruifan Li; Shuqin Ye; Guangwei Zhang; Xiaojie Wang",
        "authorids": "/x/xinjing-liu/; /r/ruifan-li/; /s/shuqin-ye/; /g/guangwei-zhang/; /x/xiaojie-wang/",
        "bibtex": "@inproceedings{liu-etal-2025-multimodal,\n    title = \"Multimodal Aspect-Based Sentiment Analysis under Conditional Relation\",\n    author = \"Liu, Xinjing  and\n      Li, Ruifan  and\n      Ye, Shuqin  and\n      Zhang, Guangwei  and\n      Wang, Xiaojie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.22/\",\n    pages = \"313--323\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.22.pdf",
        "site": "https://aclanthology.org/2025.coling-main.22/",
        "pdf_size": 630817,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6592763667501754417&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications+Engineering Research Center of Information Networks, Ministry of Education, China+Key Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Computer Science, Beijing University of Posts and Telecommunications+Engineering Research Center of Information Networks, Ministry of Education, China+Key Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications+Engineering Research Center of Information Networks, Ministry of Education, China+Key Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/Liuxj-Anya/CORSA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1+2;0;0+1+2;0+1+2",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Engineering Research Center of Information Networks;Ministry of Culture and Tourism",
        "aff_unique_dep": "School of Artificial Intelligence;Ministry of Education;Key Laboratory of Interactive Technology and Experience System",
        "aff_unique_url": "http://www.bupt.edu.cn/;;",
        "aff_unique_abbr": "BUPT;;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0+0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.363",
        "title": "Multimodal Extraction and Recognition of Arabic Implicit Discourse Relations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most research on implicit discourse relation identification has focused on written language, however, it is also crucial to understand these relations in spoken discourse. We introduce a novel method for implicit discourse relation identification across both text and speech, that allows us to extract examples of semantically equivalent pairs of implicit and explicit discourse markers, based on aligning speech+transcripts with subtitles in another language variant. We apply our method to Egyptian Arabic, resulting in a novel high-quality dataset of spoken implicit discourse relations. We present a comprehensive approach to modeling implicit discourse relation classification using audio and text data with a range of different models. We find that text-based models outperform audio-based models, but combining text and audio features can lead to enhanced performance.",
        "author": "Ahmed Ruby; Christian Hardmeier; Sara Stymne",
        "authorids": "/a/ahmed-ruby/; /c/christian-hardmeier/; /s/sara-stymne/",
        "bibtex": "@inproceedings{ruby-etal-2025-multimodal,\n    title = \"Multimodal Extraction and Recognition of {A}rabic Implicit Discourse Relations\",\n    author = \"Ruby, Ahmed  and\n      Hardmeier, Christian  and\n      Stymne, Sara\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.363/\",\n    pages = \"5415--5429\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.363.pdf",
        "site": "https://aclanthology.org/2025.coling-main.363/",
        "pdf_size": 782489,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8z7aBukw-jsJ:scholar.google.com/&scioq=Multimodal+Extraction+and+Recognition+of+Arabic+Implicit+Discourse+Relations&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Uppsala University, Department of Linguistics and Philology; IT University of Copenhagen, Department of Computer Science; Uppsala University, Department of Linguistics and Philology",
        "aff_domain": "lingfil.uu.se;itu.dk;lingfil.uu.se",
        "email": "lingfil.uu.se;itu.dk;lingfil.uu.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Uppsala University;IT University of Copenhagen",
        "aff_unique_dep": "Department of Linguistics and Philology;Department of Computer Science",
        "aff_unique_url": "https://www.uu.se;https://itu.dk",
        "aff_unique_abbr": "UU;ITU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Sweden;Denmark"
    },
    {
        "id": "2025.coling-main.310",
        "title": "Multimodal Fact-Checking with Vision Language Models: A Probing Classifier based Solution with Embedding Strategies",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study evaluates the effectiveness of Vision Language Models (VLMs) in representing and utilizing multimodal content for fact-checking. To be more specific, we investigate whether incorporating multimodal content improves performance compared to text-only models and how well VLMs utilize text and image information to enhance misinformation detection. Furthermore we propose a probing classifier based solution using VLMs. Our approach extracts embeddings from the last hidden layer of selected VLMs and inputs them into a neural probing classifier for multi-class veracity classification. Through a series of experiments on two fact-checking datasets, we demonstrate that while multimodality can enhance performance, fusing separate embeddings from text and image encoders yielded superior results compared to using VLM embeddings. Furthermore, the proposed neural classifier significantly outperformed KNN and SVM baselines in leveraging extracted embeddings, highlighting its effectiveness for multimodal fact-checking.",
        "author": "Recep Firat Cekinel; Pinar Karagoz; \u00c7a\u011fr\u0131 \u00c7\u00f6ltekin",
        "authorids": "/r/recep-firat-cekinel/; /p/pinar-karagoz/; /c/cagri-coltekin/",
        "bibtex": "@inproceedings{cekinel-etal-2025-multimodal,\n    title = \"Multimodal Fact-Checking with Vision Language Models: A Probing Classifier based Solution with Embedding Strategies\",\n    author = {Cekinel, Recep Firat  and\n      Karagoz, Pinar  and\n      {\\c{C}}{\\\"o}ltekin, {\\c{C}}a{\\u{g}}r{\\i}},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.310/\",\n    pages = \"4622--4633\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.310.pdf",
        "site": "https://aclanthology.org/2025.coling-main.310/",
        "pdf_size": 4453571,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12246793663427140769&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Middle East Technical University, Turkiye; Middle East Technical University, Turkiye; University of T\u00fcbingen, Germany",
        "aff_domain": "ceng.metu.edu.tr; ; ",
        "email": "ceng.metu.edu.tr; ; ",
        "github": "",
        "project": "https://www.snopes.com/fact-check/hitler-trump-image-fake/",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Middle East Technical University;University of T\u00fcbingen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.metu.edu.tr;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "METU;Uni T\u00fcbingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Turkey;Germany"
    },
    {
        "id": "2025.coling-main.606",
        "title": "Multitask-Bench: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to their adoption across a wide range of tasks, ranging from code generation to machine translation and sentiment analysis, etc. Red teaming/Safety alignment efforts show that fine-tuning models on benign (non-harmful) data could compromise safety. However, it remains unclear to what extent this phenomenon is influenced by different variables, including fine-tuning task, model calibrations, etc. This paper explores the task-wise safety degradation due to fine-tuning on downstream tasks such as summarization, code generation, translation, and classification across various calibration. Our results reveal that: 1) Fine-tuning LLMs for code generation and translation leads to the highest degradation in safety guardrails. 2) LLMs generally have weaker guardrails for translation and classification, with 73-92% of harmful prompts answered, across baseline and other calibrations, falling into one of two concern categories. 3) Current solutions, including guards and safety tuning datasets, lack cross-task robustness. To address these issues, we developed a new multitask safety dataset effectively reducing attack success rates across a range of tasks without compromising the model\u2019s overall helpfulness. Our work underscores the need for generalized alignment measures to ensure safer and more robust models.",
        "author": "Essa Jan; Nouar Aldahoul; Moiz Ali; Faizan Ahmad; Fareed Zaffar; Yasir Zaki",
        "authorids": "/e/essa-jan/; /n/nouar-aldahoul/; /m/moiz-ali/; /f/faizan-ahmad/; /f/fareed-zaffar/; /y/yasir-zaki/",
        "bibtex": "@inproceedings{jan-etal-2025-multitask,\n    title = \"Multitask-Bench: Unveiling and Mitigating Safety Gaps in {LLM}s Fine-tuning\",\n    author = \"Jan, Essa  and\n      Aldahoul, Nouar  and\n      Ali, Moiz  and\n      Ahmad, Faizan  and\n      Zaffar, Fareed  and\n      Zaki, Yasir\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.606/\",\n    pages = \"9025--9043\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.606.pdf",
        "site": "https://aclanthology.org/2025.coling-main.606/",
        "pdf_size": 550685,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7013395064833110227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Lahore University of Management Sciences; New York University Abu Dhabi; Meta; Lahore University of Management Sciences; Lahore University of Management Sciences; New York University Abu Dhabi",
        "aff_domain": "lums.edu.pk;nyu.edu; ; ;lums.edu.pk;nyu.edu",
        "email": "lums.edu.pk;nyu.edu; ; ;lums.edu.pk;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;0;1",
        "aff_unique_norm": "Lahore University of Management Sciences;New York University;Meta Platforms, Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://lums.edu.pk;https://nyu.edu;https://meta.com",
        "aff_unique_abbr": "LUMS;NYU;Meta",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Abu Dhabi",
        "aff_country_unique_index": "0;1;2;0;0;1",
        "aff_country_unique": "Pakistan;United Arab Emirates;United States"
    },
    {
        "id": "2025.coling-main.701",
        "title": "NCRE: A Benchmark for Document-level Nominal Compound Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity and relation extraction is a conventional task in the field of information extraction. Existing work primarily focuses on detecting specific relations between entities, often constrained to particular fields and lacking general applicability. In response, we propose a novel task: nominal compound relation extraction (NCRE), which concentrates on abstract and broadly applicable relation extraction between noun phrases. This task diverges significantly from traditional entity and relation extraction in two key respects. Firstly, our task involves general nominal compounds rather than named entities, which are longer and encompass a broader scope, presenting significant challenges for extraction. Secondly, relation extraction in NCRE demands an in-depth understanding of context to detect abstract relations. We manually annotate a high-quality Chinese dataset for the NCRE task and develop a model incorporating the rotary position-enhanced word pair (RoWP) detection schema. Experimental results demonstrate the efficiency of our RoWP model over previous baselines, while the suboptimal F1 scores indicate that NCRE remains a challenging task. Our code and data are available at https://github.com/yeecjc/NCRE.",
        "author": "Jincheng Cao; Bobo Li; Jiang Liu; Donghong Ji",
        "authorids": "/j/jincheng-cao/; /b/bobo-li/; /j/jiang-liu/; /d/donghong-ji/",
        "bibtex": "@inproceedings{cao-etal-2025-ncre,\n    title = \"{NCRE}: A Benchmark for Document-level Nominal Compound Relation Extraction\",\n    author = \"Cao, Jincheng  and\n      Li, Bobo  and\n      Liu, Jiang  and\n      Ji, Donghong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.701/\",\n    pages = \"10531--10540\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.701.pdf",
        "site": "https://aclanthology.org/2025.coling-main.701/",
        "pdf_size": 690109,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uZCVpbGulYQJ:scholar.google.com/&scioq=NCRE:+A+Benchmark+for+Document-level+Nominal+Compound+Relation+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",
        "aff_domain": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "email": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "github": "https://github.com/yeecjc/NCRE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Wuhan University",
        "aff_unique_dep": "School of Cyber Science and Engineering",
        "aff_unique_url": "http://www.whu.edu.cn",
        "aff_unique_abbr": "WHU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.423",
        "title": "NLP for preserving Torlak, a vulnerable low-resource Slavic language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Torlak is an endangered, low-resource Slavic language with a high degree of areal and inter-speaker variation. In previous work, interviews were performed with Torlak speakers in Serbia, near the Bulgarian border, and the transcripts annotated with lemma and morphosyntactic descriptions at token level. As such token-level annotations facilitate cross-language comparison in the context of the Balkan Sprachbund, where multiple languages influenced Torlak over time, including Serbian and Bulgarian. Here, we aim to improve the prediction of morphosyntactic annotations for this low-resource language using the fine-tuning of large language models, comparing several predictive models. We also further fine-tuned the large language models for scoring the degree of \u2018Torlakness\u2019 of a sentence by labeling likely Torlak tokens, to facilitate the documentation of additional Torlak transcribed speech with a high degree of Torlak-style non-standard features compared to standard Serbian. Taken together, we hope that these contributions will help to document this endangered language, and improve digital access for its speakers.",
        "author": "Li Tang; Teodora Vukovi\u0107",
        "authorids": "/l/li-tang/; /t/teodora-vukovic/",
        "bibtex": "@inproceedings{tang-vukovic-2025-nlp,\n    title = \"{NLP} for preserving Torlak, a vulnerable low-resource {S}lavic language\",\n    author = \"Tang, Li  and\n      Vukovi{\\'c}, Teodora\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.423/\",\n    pages = \"6338--6347\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.423.pdf",
        "site": "https://aclanthology.org/2025.coling-main.423/",
        "pdf_size": 350018,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4pDFtYkOLQ0J:scholar.google.com/&scioq=NLP+for+preserving+Torlak,+a+vulnerable+low-resource+Slavic+language&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "University of Zurich, Linguistic Research Infrastructure, Zurich, Switzerland; University of Zurich, Linguistic Research Infrastructure, Zurich, Switzerland",
        "aff_domain": "uzh.ch;uzh.ch",
        "email": "uzh.ch;uzh.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Zurich",
        "aff_unique_dep": "Linguistic Research Infrastructure",
        "aff_unique_url": "https://www.unizh.ch",
        "aff_unique_abbr": "UZH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Zurich",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2025.coling-main.738",
        "title": "NYAYAANUMANA and INLEGALLLAMA: The Largest Indian Legal Judgment Prediction Dataset and Specialized Language Model for Enhanced Decision Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The integration of artificial intelligence (AI) in legal judgment prediction (LJP) has the potential to transform the legal landscape, particularly in jurisdictions like India, where a significant backlog of cases burdens the legal system. This paper introduces NyayaAnumana, the largest and most diverse corpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945 preprocessed cases. NyayaAnumana, which combines the words \u201cNyaya\u201d and \u201cAnumana\u201d that means \u201cjudgment\u201d and \u201cinference\u201d respectively for most major Indian languages, includes a wide range of cases from the Supreme Court, High Courts, Tribunal Courts, District Courts, and Daily Orders and, thus, provides unparalleled diversity and coverage. Our dataset surpasses existing datasets like PredEx and ILDC, offering a comprehensive foundation for advanced AI research in the legal domain. In addition to the dataset, we present INLegalLlama, a domain-specific generative large language model (LLM) tailored to the intricacies of the Indian legal system. It is developed through a two-phase training approach over a base LLaMa model. First, Indian legal documents are injected using continual pretraining. Second, task-specific supervised finetuning is done. This method allows the model to achieve a deeper understanding of legal contexts. Our experiments demonstrate that incorporating diverse court data significantly boosts model accuracy, achieving approximately 90% F1-score in prediction tasks. INLegalLlama not only improves prediction accuracy but also offers comprehensible explanations, addressing the need for explainability in AI-assisted legal decisions.",
        "author": "Shubham Kumar Nigam; Deepak Patnaik Balaramamahanthi; Shivam Mishra; Noel Shallum; Kripabandhu Ghosh; Arnab Bhattacharya",
        "authorids": "/s/shubham-kumar-nigam/; /d/deepak-patnaik-balaramamahanthi/; /s/shivam-mishra/; /n/noel-shallum/; /k/kripabandhu-ghosh/; /a/arnab-bhattacharya/",
        "bibtex": "@inproceedings{nigam-etal-2025-nyayaanumana,\n    title = \"{NYAYAANUMANA} and {INLEGALLLAMA}: The Largest {I}ndian Legal Judgment Prediction Dataset and Specialized Language Model for Enhanced Decision Analysis\",\n    author = \"Nigam, Shubham Kumar  and\n      Balaramamahanthi, Deepak Patnaik  and\n      Mishra, Shivam  and\n      Shallum, Noel  and\n      Ghosh, Kripabandhu  and\n      Bhattacharya, Arnab\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.738/\",\n    pages = \"11135--11160\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.738.pdf",
        "site": "https://aclanthology.org/2025.coling-main.738/",
        "pdf_size": 898571,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13442711041773843214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "IIT Kanpur, India; IIT Kanpur, India; IIT Kanpur, India; Symbiosis Law School Pune, India; IISER Kolkata, India; IIT Kanpur, India",
        "aff_domain": "cse.iitk.ac.in;cse.iitk.ac.in;cse.iitk.ac.in;gmail.com;iiserkol.ac.in;cse.iitk.ac.in",
        "email": "cse.iitk.ac.in;cse.iitk.ac.in;cse.iitk.ac.in;gmail.com;iiserkol.ac.in;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Symbiosis Law School;Indian Institute of Science Education and Research",
        "aff_unique_dep": ";Law;",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.sls.edu.in;https://www.iiserkol.ac.in",
        "aff_unique_abbr": "IITK;;IISER",
        "aff_campus_unique_index": "0;0;0;1;2;0",
        "aff_campus_unique": "Kanpur;Pune;Kolkata",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.134",
        "title": "NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown impressive performance on various benchmarks, yet their ability to engage in deliberate reasoning remains questionable. We present NYT-Connections, a collection of 358 simple word classification puzzles derived from the New York Times Connections game. This benchmark is designed to penalize quick, intuitive \u201cSystem 1\u201d thinking, isolating fundamental reasoning skills. We evaluated six recent LLMs, a simple machine learning heuristic, and humans across three configurations: single-attempt, multiple attempts without hints, and multiple attempts with contextual hints. Our findings reveal a significant performance gap: even top-performing LLMs like GPT-4 fall short of human performance by nearly 30%. Notably, advanced prompting techniques such as Chain-of-Thought and Self-Consistency show diminishing returns as task difficulty increases. NYT-Connections uniquely combines linguistic isolation, resistance to intuitive shortcuts, and regular updates to mitigate data leakage, offering a novel tool for assessing LLM reasoning capabilities.",
        "author": "Angel Yahir Loredo Lopez; Tyler McDonald; Ali Emami",
        "authorids": "/a/angel-yahir-loredo-lopez/; /t/tyler-mcdonald/; /a/ali-emami/",
        "bibtex": "@inproceedings{loredo-lopez-etal-2025-nyt,\n    title = \"{NYT}-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers\",\n    author = \"Loredo Lopez, Angel Yahir  and\n      McDonald, Tyler  and\n      Emami, Ali\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.134/\",\n    pages = \"1952--1963\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.134.pdf",
        "site": "https://aclanthology.org/2025.coling-main.134/",
        "pdf_size": 4258265,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9743684662304268998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Universidad Aut\u00f3noma de San Luis Potos\u00ed, San Luis Potos\u00ed, Mexico; Brock University, Saint Catharines, Canada; Brock University, Saint Catharines, Canada",
        "aff_domain": "alumnos.uaslp.mx;brocku.ca;brocku.ca",
        "email": "alumnos.uaslp.mx;brocku.ca;brocku.ca",
        "github": "",
        "project": "here",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Universidad Aut\u00f3noma de San Luis Potos\u00ed;Brock University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uaslp.mx;https://brocku.ca",
        "aff_unique_abbr": "UASLP;Brock",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "San Luis Potos\u00ed;Saint Catharines",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Mexico;Canada"
    },
    {
        "id": "2025.coling-main.657",
        "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.",
        "author": "Han Han; Tong Zhu; Xiang Zhang; MengSong Wu; Xiong Hao; Wenliang Chen",
        "authorids": "/h/han-han/; /t/tong-zhu/; /x/xiang-zhang/; /m/mengsong-wu/; /x/xiong-hao/; /w/wenliang-chen/",
        "bibtex": "@inproceedings{han-etal-2025-nestools,\n    title = \"{N}es{T}ools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models\",\n    author = \"Han, Han  and\n      Zhu, Tong  and\n      Zhang, Xiang  and\n      Wu, MengSong  and\n      Hao, Xiong  and\n      Chen, Wenliang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.657/\",\n    pages = \"9824--9844\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.657.pdf",
        "site": "https://aclanthology.org/2025.coling-main.657/",
        "pdf_size": 1063039,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9557092060037891496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "https://github.com/hhan1018/NesTools",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Institute of Artificial Intelligence, School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.67",
        "title": "Neural Document Segmentation Using Weighted Sliding Windows with Transformer Encoders",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We introduce a novel Transformer-based method for document segmentation, tailored for practical, real-world applications. This method utilizes overlapping text sequences with a unique position-aware weighting mechanism to enhance segmentation accuracy. Through comprehensive experiments on both public and proprietary datasets, we demonstrate significant improvements, establishing new state-of-the-art standards by achieving up to a 10% increase in segmentation F1 score compared to existing methods. Additionally, we explore the application of our segmentation method in downstream retrieval-augmented question answering tasks, where it improves the quality of generated responses by 5% while achieving up to four times greater efficiency. These results underscore our model\u2019s potential as a robust and scalable solution for real-world text segmentation challenges.",
        "author": "Saeed Abbasi; Aijun An; Heidar Davoudi; Ron Di Carlantonio; Gary Farmaner",
        "authorids": "/s/saeed-abbasi/; /a/aijun-an/; /h/heidar-davoudi/; /r/ron-di-carlantonio/; /g/gary-farmaner/",
        "bibtex": "@inproceedings{abbasi-etal-2025-neural,\n    title = \"Neural Document Segmentation Using Weighted Sliding Windows with Transformer Encoders\",\n    author = \"Abbasi, Saeed  and\n      An, Aijun  and\n      Davoudi, Heidar  and\n      Di Carlantonio, Ron  and\n      Farmaner, Gary\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.67/\",\n    pages = \"807--816\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.67.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.67/",
        "pdf_size": 947049,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:F552yIvLi-oJ:scholar.google.com/&scioq=Neural+Document+Segmentation+Using+Weighted+Sliding+Windows+with+Transformer+Encoders&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "York University, Toronto, Canada; York University, Toronto, Canada; Ontario Tech University, Oshawa, Canada; iNAGO Corporation, Toronto, Canada; iNAGO Corporation, Toronto, Canada",
        "aff_domain": "yorku.ca;yorku.ca;ontariotechu.ca;inago.com;inago.com",
        "email": "yorku.ca;yorku.ca;ontariotechu.ca;inago.com;inago.com",
        "github": "https://github.com/saeedabc/WeSW",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;2",
        "aff_unique_norm": "York University;Ontario Tech University;iNAGO Corporation",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://yorku.ca;https://www.ontariotechu.ca;",
        "aff_unique_abbr": "York U;OntTechU;",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Toronto;Oshawa;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-industry.16",
        "title": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with Company Size",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large language models (LLMs) are playing a pivotal role in deploying strategic use cases across a range of organizations, from large pan-continental companies to emerging startups. The issues and challenges involved in the successful utilization of LLMs can vary significantly depending on the size of the organization. It is important to study and discuss these pertinent issues of LLM adaptation with a focus on the scale of the industrial concerns and brainstorm possible solutions and prospective directions. Such a study has not been prominently featured in the current research literature. In this study, we adopt a threefold strategy: first, we conduct a case study with industry practitioners to formulate the key research questions; second, we examine existing industrial publications to address these questions; and finally, we provide a practical guide for industries to utilize LLMs more efficiently. We release the GitHub repository with the most recent papers in the field.",
        "author": "Ashok Urlana; Charaka Vinayak Kumar; Bala Mallikarjunarao Garlapati; Ajeet Kumar Singh; Rahul Mishra",
        "authorids": "/a/ashok-urlana/; /c/charaka-vinayak-kumar/; /b/bala-mallikarjunarao-garlapati/; /a/ajeet-kumar-singh/; /r/rahul-mishra/",
        "bibtex": "@inproceedings{urlana-etal-2025-size,\n    title = \"No Size Fits All: The Perils and Pitfalls of Leveraging {LLM}s Vary with Company Size\",\n    author = \"Urlana, Ashok  and\n      Vinayak Kumar, Charaka  and\n      Garlapati, Bala Mallikarjunarao  and\n      Singh, Ajeet Kumar  and\n      Mishra, Rahul\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.16/\",\n    pages = \"187--203\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.16.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.16/",
        "pdf_size": 525731,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14274108563067337861&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "TCS Research, Hyderabad, India; TCS Research, Hyderabad, India; TCS Research, Hyderabad, India; TCS Research, Hyderabad, India; IIIT Hyderabad",
        "aff_domain": "tcs.com;tcs.com;tcs.com;tcs.com;iiit.ac.in",
        "email": "tcs.com;tcs.com;tcs.com;tcs.com;iiit.ac.in",
        "github": "https://github.com/vinayakcse/IndustrialLLMsPapers",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Tata Consultancy Services;International Institute of Information Technology, Hyderabad",
        "aff_unique_dep": "Research;",
        "aff_unique_url": "https://www.tcs.com;https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "TCS;IIIT-H",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.737",
        "title": "No Train but Gain: Language Arithmetic for training-free Language Adapters enhancement",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Modular deep learning is the state-of-the-art solution for lifting the curse of multilinguality, preventing the impact of negative interference and enabling cross-lingual performance in Multilingual Pre-trained Language Models. However, a trade-off of this approach is the reduction in positive transfer learning from closely related languages. In response, we introduce a novel method called language arithmetic, which enables training-free post-processing to address this limitation. Extending the task arithmetic framework, we apply learning via addition to the language adapters, transitioning the framework from a multi-task to a multilingual setup. The effectiveness of the proposed solution is demonstrated on three downstream tasks in a MAD-X-based set of cross-lingual schemes, acting as a post-processing procedure. Language arithmetic consistently improves the baselines with significant gains, especially in the most challenging case of zero-shot application. Our code and models are available at https://github.com/mklimasz/language-arithmetic.",
        "author": "Mateusz Klimaszewski; Piotr Andruszkiewicz; Alexandra Birch",
        "authorids": "/m/mateusz-klimaszewski/; /p/piotr-andruszkiewicz/; /a/alexandra-birch/",
        "bibtex": "@inproceedings{klimaszewski-etal-2025-train,\n    title = \"No Train but Gain: Language Arithmetic for training-free Language Adapters enhancement\",\n    author = \"Klimaszewski, Mateusz  and\n      Andruszkiewicz, Piotr  and\n      Birch, Alexandra\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.737/\",\n    pages = \"11121--11134\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.737.pdf",
        "site": "https://aclanthology.org/2025.coling-main.737/",
        "pdf_size": 2353551,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15745358579397484215&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Warsaw University of Technology+University of Edinburgh; Warsaw University of Technology; University of Edinburgh",
        "aff_domain": "pw.edu.pl; ; ",
        "email": "pw.edu.pl; ; ",
        "github": "https://github.com/mklimasz/language-arithmetic",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Warsaw University of Technology;University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.pw.edu.pl;https://www.ed.ac.uk",
        "aff_unique_abbr": "WUT;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "Poland;United Kingdom"
    },
    {
        "id": "2025.coling-main.11",
        "title": "Noise-powered Multi-modal Knowledge Graph Representation Framework",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rise of Multi-modal Pre-training highlights the necessity for a unified Multi-Modal Knowledge Graph (MMKG) representation learning framework. Such a framework is essential for embedding structured knowledge into multi-modal Large Language Models effectively, alleviating issues like knowledge misconceptions and multi-modal hallucinations. In this work, we explore the efficacy of models in accurately embedding entities within MMKGs through two pivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking to robustly integrate multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets, demonstrating its versatility. Moreover, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Code and data are available at https://github.com/zjukg/SNAG.",
        "author": "Zhuo Chen; Yin Fang; Yichi Zhang; Lingbing Guo; Jiaoyan Chen; Jeff Z. Pan; Huajun Chen; Wen Zhang",
        "authorids": "/z/zhuo-chen/; /y/yin-fang/; /y/yichi-zhang/; /l/lingbing-guo/; /j/jiaoyan-chen/; /j/jeff-z-pan/; /h/huajun-chen/; /w/wen-zhang/",
        "bibtex": "@inproceedings{chen-etal-2025-noise,\n    title = \"Noise-powered Multi-modal Knowledge Graph Representation Framework\",\n    author = \"Chen, Zhuo  and\n      Fang, Yin  and\n      Zhang, Yichi  and\n      Guo, Lingbing  and\n      Chen, Jiaoyan  and\n      Pan, Jeff Z.  and\n      Chen, Huajun  and\n      Zhang, Wen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.11/\",\n    pages = \"141--155\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.11.pdf",
        "site": "https://aclanthology.org/2025.coling-main.11/",
        "pdf_size": 910796,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17433790475323874631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University + ZJU-Ant Group Joint Lab of Knowledge Graph; Zhejiang University + ZJU-Ant Group Joint Lab of Knowledge Graph; Zhejiang University + ZJU-Ant Group Joint Lab of Knowledge Graph; Zhejiang University + ZJU-Ant Group Joint Lab of Knowledge Graph; The University of Manchester; The University of Edinburgh; Zhejiang University + ZJU-Ant Group Joint Lab of Knowledge Graph; Zhejiang University + ZJU-Ant Group Joint Lab of Knowledge Graph",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn; ;ed.ac.uk;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn; ;ed.ac.uk;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/zjukg/SNAG",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+0;0+0;0+0;0+0;1;2;0+0;0+0",
        "aff_unique_norm": "Zhejiang University;The University of Manchester;University of Edinburgh",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.manchester.ac.uk;https://www.ed.ac.uk",
        "aff_unique_abbr": "ZJU;UoM;Edinburgh",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;1;1;0+0;0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2025.coling-main.66",
        "title": "Non-Emotion-Centric Empathetic Dialogue Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous work on empathetic response generation mainly focused on utilizing the speaker\u2019s emotions to generate responses. However, the performance of identifying fine-grained emotions is limited, introducing cascading errors to empathetic response generation. Moreover, due to the conflict between the information in the dialogue history and the recognized emotions, previous work often generated general and uninformative responses. To address the above issues, we propose a novel framework NEC (Non-Emotion-Centric empathetic dialogue generation) based on contrastive learning and context-sensitive entity and social commonsense, in which the frequent replies and sentences with incorrect emotions are punished through contrastive learning, thereby improving the empathy, diversity and information of the responses. The experimental results demonstrate that our NEC enhances the quality of empathetic generation and generates more diverse responses in comparison with the state-of-the-art baselines.The code will be available at https://github.com/huangfu170/NEC-empchat",
        "author": "Yuanxiang Huangfu; Peifeng Li; Yaxin Fan; Qiaoming Zhu",
        "authorids": "/y/yuanxiang-huangfu/; /p/peifeng-li/; /y/yaxin-fan/; /q/qiaoming-zhu/",
        "bibtex": "@inproceedings{huangfu-etal-2025-non,\n    title = \"Non-Emotion-Centric Empathetic Dialogue Generation\",\n    author = \"Huangfu, Yuanxiang  and\n      Li, Peifeng  and\n      Fan, Yaxin  and\n      Zhu, Qiaoming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.66/\",\n    pages = \"989--999\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.66.pdf",
        "site": "https://aclanthology.org/2025.coling-main.66/",
        "pdf_size": 595479,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3Su2eZ0JjlMJ:scholar.google.com/&scioq=Non-Emotion-Centric+Empathetic+Dialogue+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "https://github.com/huangfu170/NEC-empchat",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.521",
        "title": "Norm of Mean Contextualized Embeddings Determines their Variance",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Contextualized embeddings vary by context, even for the same token, and form a distribution in the embedding space. To analyze this distribution, we focus on the norm of the mean embedding and the variance of the embeddings. In this study, we first demonstrate that these values follow the well-known formula for variance in statistics and provide an efficient sequential computation method. Then, by observing embeddings from intermediate layers of several Transformer models, we found a strong trade-off relationship between the norm and the variance: as the mean embedding becomes closer to the origin, the variance increases. Furthermore, when the sets of token embeddings are treated as clusters, we show that the variance of the entire embedding set can theoretically be decomposed into the within-cluster variance and the between-cluster variance. We found experimentally that as the layers of Transformer models deepen, the embeddings move farther from the origin, the between-cluster variance relatively decreases, and the within-cluster variance relatively increases. These results are consistent with existing studies on the anisotropy of the embedding spaces across layers.",
        "author": "Hiroaki Yamagiwa; Hidetoshi Shimodaira",
        "authorids": "/h/hiroaki-yamagiwa/; /h/hidetoshi-shimodaira/",
        "bibtex": "@inproceedings{yamagiwa-shimodaira-2025-norm,\n    title = \"Norm of Mean Contextualized Embeddings Determines their Variance\",\n    author = \"Yamagiwa, Hiroaki  and\n      Shimodaira, Hidetoshi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.521/\",\n    pages = \"7778--7808\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.521.pdf",
        "site": "https://aclanthology.org/2025.coling-main.521/",
        "pdf_size": 10839676,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nb2hXyeSpb8J:scholar.google.com/&scioq=Norm+of+Mean+Contextualized+Embeddings+Determines+their+Variance&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Kyoto University+RIKEN; Kyoto University+RIKEN",
        "aff_domain": "sys.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "email": "sys.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "github": "https://github.com/ymgw55/Norm-and-Variance",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Kyoto University;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "Kyoto U;RIKEN",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.246",
        "title": "Not Every Metric is Equal: Cognitive Models for Predicting N400 and P600 Components During Reading Comprehension",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, numerous studies have sought to understand the cognitive dynamics underlying language processing by modeling reading times and ERP amplitudes using computational metrics like surprisal. In the present paper, we examine the predictive power of surprisal, entropy, and a novel metric based on semantic similarity for N400 and P600. Our experiments, conducted with Mandarin Chinese materials, revealed three key findings: 1) expectancy plays a primary role for N400; 2) P600 also reflects the cognitive effort required to evaluate linguistic input semantically; and 3) during the time window of interest, information uncertainty influences the language processing the most. Our findings show how computational metrics that capture distinct cognitive dimensions can effectively address psycholinguistic questions.",
        "author": "Lavinia Salicchi; Yu-Yin Hsu",
        "authorids": "/l/lavinia-salicchi/; /y/yu-yin-hsu/",
        "bibtex": "@inproceedings{salicchi-hsu-2025-every,\n    title = \"Not Every Metric is Equal: Cognitive Models for Predicting N400 and P600 Components During Reading Comprehension\",\n    author = \"Salicchi, Lavinia  and\n      Hsu, Yu-Yin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.246/\",\n    pages = \"3648--3654\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.246.pdf",
        "site": "https://aclanthology.org/2025.coling-main.246/",
        "pdf_size": 342018,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16714485402363714209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Hong Kong Polytechnic University; The Hong Kong Polytechnic University",
        "aff_domain": "connect.polyu.hk;polyu.edu.hk",
        "email": "connect.polyu.hk;polyu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Hong Kong Polytechnic University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.polyu.edu.hk",
        "aff_unique_abbr": "PolyU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.234",
        "title": "NovAScore: A New Automated Metric for Evaluating Document Level Novelty",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid expansion of online content has intensified the issue of information redundancy, underscoring the need for solutions that can identify genuinely new information. Despite this challenge, the research community has seen a decline in focus on novelty detection, particularly with the rise of large language models (LLMs). Additionally, previous approaches have relied heavily on human annotation, which is time-consuming, costly, and particularly challenging when annotators must compare a target document against a vast number of historical documents. In this work, we introduce NovAScore (Novelty Evaluation in Atomicity Score), an automated metric for evaluating document-level novelty. NovAScore aggregates the novelty and salience scores of atomic information, providing high interpretability and a detailed analysis of a document\u2019s novelty. With its dynamic weight adjustment scheme, NovAScore offers enhanced flexibility and an additional dimension to assess both the novelty level and the importance of information within a document. Our experiments show that NovAScore strongly correlates with human judgments of novelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0 dataset and a 0.920 Pearson correlation on an internal human-annotated dataset.",
        "author": "Lin Ai; Ziwei Gong; Harshsaiprasad Deshpande; Alexander Johnson; Emmy Phung; Ahmad Emami; Julia Hirschberg",
        "authorids": "/l/lin-ai/; /z/ziwei-gong/; /h/harshsaiprasad-deshpande/; /a/alexander-johnson/; /e/emmy-phung/; /a/ahmad-emami/; /j/julia-hirschberg/",
        "bibtex": "@inproceedings{ai-etal-2025-novascore,\n    title = \"{N}ov{AS}core: A New Automated Metric for Evaluating Document Level Novelty\",\n    author = \"Ai, Lin  and\n      Gong, Ziwei  and\n      Deshpande, Harshsaiprasad  and\n      Johnson, Alexander  and\n      Phung, Emmy  and\n      Emami, Ahmad  and\n      Hirschberg, Julia\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.234/\",\n    pages = \"3479--3494\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.234.pdf",
        "site": "https://aclanthology.org/2025.coling-main.234/",
        "pdf_size": 2336296,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8167145773940622468&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Machine Learning Center of Excellence, JPMorgan Chase & Co. + Department of Computer Science, Columbia University; Machine Learning Center of Excellence, JPMorgan Chase & Co. + Department of Computer Science, Columbia University; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu; ; ; ; ;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu; ; ; ; ;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0;0;0;0;1",
        "aff_unique_norm": "JPMorgan Chase & Co.;Columbia University",
        "aff_unique_dep": "Machine Learning Center of Excellence;Department of Computer Science",
        "aff_unique_url": "https://www.jpmorganchase.com;https://www.columbia.edu",
        "aff_unique_abbr": "JPMorgan;Columbia",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.468",
        "title": "N\u00fcshuRescue: Reviving the Endangered N\u00fcshu Language with AI",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The preservation and revitalization of endangered and extinct languages is a meaningful endeavor, conserving cultural heritage while enriching fields like linguistics and anthropology. However, these languages are typically low-resource, making their reconstruction labor-intensive and costly. This challenge is exemplified by N\u00fcshu, a rare script historically used by Yao women in China for self-expression within a patriarchal society. To address this challenge, we introduce N\u00fcshuRescue, an AI-driven framework designed to train large language models (LLMs) on endangered languages with minimal data. N\u00fcshuRescue automates evaluation and expands target corpora to accelerate linguistic revitalization. As a foundational component, we developed NCGold, a 500-sentence N\u00fcshu-Chinese parallel corpus, the first publicly available dataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to N\u00fcshu and only 35 short examples from NCGold, N\u00fcshuRescue achieved 48.69% translation accuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly translated modern Chinese sentences of varying lengths. In addition, we developed FastText-based and Seq2Seq models to further support research on N\u00fcshu. N\u00fcshuRescue provides a versatile and scalable tool for the revitalization of endangered languages, minimizing the need for extensive human input. All datasets and code have been made publicly available at https://github.com/ivoryayang/NushuRescue.",
        "author": "Ivory Yang; Weicheng Ma; Soroush Vosoughi",
        "authorids": "/i/ivory-yang/; /w/weicheng-ma/; /s/soroush-vosoughi/",
        "bibtex": "@inproceedings{yang-etal-2025-nushurescue,\n    title = {{N}{\\\"u}shu{R}escue: Reviving the Endangered N{\\\"u}shu Language with {AI}},\n    author = \"Yang, Ivory  and\n      Ma, Weicheng  and\n      Vosoughi, Soroush\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.468/\",\n    pages = \"7020--7034\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.468.pdf",
        "site": "https://aclanthology.org/2025.coling-main.468/",
        "pdf_size": 1801587,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5570513995580645322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, Dartmouth College; Department of Computer Science, Dartmouth College; Department of Computer Science, Dartmouth College",
        "aff_domain": "dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "email": "dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "github": "https://github.com/ivoryayang/NushuRescue",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Dartmouth College",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://dartmouth.edu",
        "aff_unique_abbr": "Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-industry.10",
        "title": "OKG: On-the-Fly Keyword Generation in Sponsored Search Advertising",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Current keyword decision-making in sponsored search advertising relies on large static datasets, limiting automatic keyword setup and failing to adapt to real-time KPI metrics and product updates essential for effective advertising. In this paper, we propose On-the-fly Keyword Generation (OKG), an LLM agent-based method that dynamically monitors KPI changes and adapts keyword generation in real-time, realizing the strategy recommended by advertising platforms. Additionally, we introduce the first publicly accessible dataset containing real keyword data with its KPIs across diverse domains, providing a valuable resource for future research. Experimental results and ablation studies demonstrate the effectiveness of OKG, showing significant improvements across various metrics and emphasizing the importance of each component. We believe OKG not only pioneers the use of LLM agents in this research field but also offers practical value for thousands of advertisers to automate keyword generation in real-world applications.",
        "author": "Zhao Wang; Briti Gangopadhyay; Mengjie Zhao; Shingo Takamatsu",
        "authorids": "/z/zhao-wang/; /b/briti-gangopadhyay/; /m/mengjie-zhao/; /s/shingo-takamatsu/",
        "bibtex": "@inproceedings{wang-etal-2025-okg,\n    title = \"{OKG}: On-the-Fly Keyword Generation in Sponsored Search Advertising\",\n    author = \"Wang, Zhao  and\n      Gangopadhyay, Briti  and\n      Zhao, Mengjie  and\n      Takamatsu, Shingo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.10/\",\n    pages = \"115--127\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.10.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.10/",
        "pdf_size": 946042,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3131524040033433529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Sony Group Corporation; Sony Group Corporation; Sony Group Corporation; Sony Group Corporation",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/sony/okg",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Sony Group Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sony.com",
        "aff_unique_abbr": "Sony",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.602",
        "title": "OVEL: Online Video Entity Linking",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, Multi-modal Entity Linking (MEL) has attracted increasing attention in the research community due to its significance in numerous multi-modal applications. Video, as a popular means of information transmission, has become prevalent in people\u2019s daily lives. However, most existing MEL methods primarily focus on linking textual and visual mentions or offline videos\u2019 mentions to entities in multi-modal knowledge bases, with limited efforts devoted to linking mentions within online video content. In this paper, we propose a task called Online Video Entity Linking (OVEL), aiming to establish connections between mentions in online videos and a knowledge base with high accuracy and timeliness. To facilitate the research works of (OVEL), we specifically concentrate on live delivery scenarios and construct a live delivery entity linking dataset called (LIVE). Besides, we propose an evaluation metric that considers robustness, timelessness, and accuracy. Furthermore, to effectively handle (OVEL) task, we leverage a memory block managed by a Large Language Model and retrieve entity candidates from the knowledge base to augment LLM performance on memory management. The experimental results prove the effectiveness and efficiency of our method.",
        "author": "Haiquan Zhao; Xuwu Wang; Shisong Chen; Zhixu Li; Xin Zheng; Yanghua Xiao",
        "authorids": "/h/haiquan-zhao/; /x/xuwu-wang/; /s/shisong-chen/; /z/zhixu-li/; /x/xin-zheng/; /y/yanghua-xiao/",
        "bibtex": "@inproceedings{zhao-etal-2025-ovel,\n    title = \"{OVEL}: Online Video Entity Linking\",\n    author = \"Zhao, Haiquan  and\n      Wang, Xuwu  and\n      Chen, Shisong  and\n      Li, Zhixu  and\n      Zheng, Xin  and\n      Xiao, Yanghua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.602/\",\n    pages = \"8979--8991\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.602.pdf",
        "site": "https://aclanthology.org/2025.coling-main.602/",
        "pdf_size": 2342337,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Cuh0sAQJ90gJ:scholar.google.com/&scioq=OVEL:+Online+Video+Entity+Linking&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; Shanghai Institute of AI for Education, East China Normal University; School of Information, Renmin University of China + International College (Suzhou Research Institute), Renmin University of China; iFLYTEK CO., LTD, Suzhou, China; School of Computer Science, Fudan University + International College (Suzhou Research Institute), Renmin University of China",
        "aff_domain": "m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;ruc.edu.cn;stu.ecnu.edu.cn;iflytek.com",
        "email": "m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;ruc.edu.cn;stu.ecnu.edu.cn;iflytek.com",
        "github": "https://github.com/haidequanbu/OVEL",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2+2;3;0+2",
        "aff_unique_norm": "Fudan University;East China Normal University;Renmin University of China;iFLYTEK CO., LTD",
        "aff_unique_dep": "School of Computer Science;Shanghai Institute of AI for Education;School of Information;",
        "aff_unique_url": "https://www.fudan.edu.cn;http://www.ecnu.edu.cn;http://www.ruc.edu.cn;https://www.iflytek.com",
        "aff_unique_abbr": "Fudan;ECNU;RUC;iFLYTEK",
        "aff_campus_unique_index": "1;2;2;2",
        "aff_campus_unique": ";Shanghai;Suzhou",
        "aff_country_unique_index": "0;0;0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.183",
        "title": "Oddballness: universal anomaly detection with language models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a new method to detect anomalies in texts (in general: in sequences of any data), using language models, in a totally unsupervised manner. The method considers probabilities (likelihoods) generated by a language model, but instead of focusing on low-likelihood tokens, it considers a new metric defined in this paper: oddballness. Oddballness measures how \u201cstrange\u201d a given token is according to the language model. We demonstrate in grammatical error detection tasks (a specific case of text anomaly detection) that oddballness is better than just considering low-likelihood events, if a totally unsupervised setup is assumed.",
        "author": "Filip Gralinski; Ryszard Staruch; Krzysztof Jurkiewicz",
        "authorids": "/f/filip-gralinski/; /r/ryszard-staruch/; /k/krzysztof-jurkiewicz/",
        "bibtex": "@inproceedings{gralinski-etal-2025-oddballness,\n    title = \"Oddballness: universal anomaly detection with language models\",\n    author = \"Gralinski, Filip  and\n      Staruch, Ryszard  and\n      Jurkiewicz, Krzysztof\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.183/\",\n    pages = \"2683--2689\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.183.pdf",
        "site": "https://aclanthology.org/2025.coling-main.183/",
        "pdf_size": 501629,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11051237014129911420&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Adam Mickiewicz University + Snowflake; Adam Mickiewicz University + Center for Artificial Intelligence AMU; Adam Mickiewicz University*",
        "aff_domain": "amu.edu.pl;snowflake.com;amu.edu.pl",
        "email": "amu.edu.pl;snowflake.com;amu.edu.pl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+2;0",
        "aff_unique_norm": "Adam Mickiewicz University;Snowflake Inc.;Aligarh Muslim University",
        "aff_unique_dep": ";;Center for Artificial Intelligence",
        "aff_unique_url": "https://www.amu.edu.pl;https://www.snowflake.com;https://www.amu.ac.in",
        "aff_unique_abbr": "AMU;Snowflake;AMU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+2;0",
        "aff_country_unique": "Poland;United States;India"
    },
    {
        "id": "2025.coling-main.388",
        "title": "On Evaluating LLMs\u2019 Capabilities as Functional Approximators: A Bayesian Evaluation Framework",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent works have successfully applied Large Language Models (LLMs) to function modeling tasks. However, the reasons behind this success remain unclear. In this work, we propose a new evaluation framework to comprehensively assess LLMs\u2019 function modeling abilities. By adopting a Bayesian perspective of function modeling, we discover that LLMs are relatively weak in understanding patterns in raw data, but excel at utilizing prior knowledge about the domain to develop a strong understanding of the underlying function. Our findings offer new insights about the strengths and limitations of LLMs in the context of function modeling.",
        "author": "Shoaib Ahmed Siddiqui; Yanzhi Chen; Juyeon Heo; Menglin Xia; Adrian Weller",
        "authorids": "/s/shoaib-ahmed-siddiqui/; /y/yanzhi-chen/; /j/juyeon-heo/; /m/menglin-xia/; /a/adrian-weller/",
        "bibtex": "@inproceedings{siddiqui-etal-2025-evaluating,\n    title = \"On Evaluating {LLM}s' Capabilities as Functional Approximators: A {B}ayesian Evaluation Framework\",\n    author = \"Siddiqui, Shoaib Ahmed  and\n      Chen, Yanzhi  and\n      Heo, Juyeon  and\n      Xia, Menglin  and\n      Weller, Adrian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.388/\",\n    pages = \"5826--5835\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.388.pdf",
        "site": "https://aclanthology.org/2025.coling-main.388/",
        "pdf_size": 567205,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qCVv7N_A9Y0J:scholar.google.com/&scioq=On+Evaluating+LLMs%E2%80%99+Capabilities+as+Functional+Approximators:+A+Bayesian+Evaluation+Framework&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge; Microsoft; University of Cambridge+The Alan Turing Institute",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0+2",
        "aff_unique_norm": "University of Cambridge;Microsoft Corporation;The Alan Turing Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.microsoft.com;https://www.turing.ac.uk",
        "aff_unique_abbr": "Cambridge;Microsoft;ATI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;1;0+0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2025.coling-main.231",
        "title": "On Evaluation Protocols for Data Augmentation in a Limited Data Scenario",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation (which modify sentences) is simply a way of performing better fine-tuning, and that spending more time doing so before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely : which DA technique performs best (all of them as long as they generate data close enough to the training set, as to not impair training) and why did DA show positive results (facilitates training of network). We further show that zero- and few-shot DA via conversational agents such as ChatGPT or LLama2 can increase performances, confirming that this form of data augmentation is preferable to classical methods.",
        "author": "Fr\u00e9d\u00e9ric Piedboeuf; Philippe Langlais",
        "authorids": "/f/frederic-piedboeuf/; /p/philippe-langlais/",
        "bibtex": "@inproceedings{piedboeuf-langlais-2025-evaluation,\n    title = \"On Evaluation Protocols for Data Augmentation in a Limited Data Scenario\",\n    author = \"Piedboeuf, Fr{\\'e}d{\\'e}ric  and\n      Langlais, Philippe\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.231/\",\n    pages = \"3428--3443\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.231.pdf",
        "site": "https://aclanthology.org/2025.coling-main.231/",
        "pdf_size": 348042,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4565326759134730854&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Universit\u00e9 de Montr\u00e9al, RALI; Universit\u00e9 de Montr\u00e9al, RALI",
        "aff_domain": "umontreal.ca; ",
        "email": "umontreal.ca; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "RALI",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-main.687",
        "title": "On Weaponization-Resistant Large Language Models with Prospect Theoretic Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have made significant advancements, but their increasing capabilities present serious risks of misuse, particularly in open-weight models where direct access to the model\u2019s parameters is possible. Current safeguards, designed for closed-weight API models, are inadequate for open-weight models, as minimal fine-tuning can bypass these protections. Preserving the integrity of open-weight LLMs before deployment has thus become a critical challenge. We argue that these vulnerabilities stem from the overemphasis on maximizing the LLM\u2019s log-likelihood during training, which amplifies data biases, especially with large datasets. To address these issues, we introduce Kahneman and Tversky\u2019s Prospect Theoretic Integrity Preserving Alignment (KT-IPA), a framework that prioritizes maximizing generative utility rather than a singular optimization metric. This approach strengthens LLMs against misuse and weaponization while maintaining high performance, even after extensive fine-tuning. Our results demonstrate that integrating prospect theory into LLM training enhances robustness, security, and responsible innovation in this rapidly evolving field. Our codes are available on https://anonymous.4open.science/r/KT-IPA-40B7",
        "author": "Zehua Cheng; Manying Zhang; Jiahao Sun; Wei Dai",
        "authorids": "/z/zehua-cheng/; /m/manying-zhang/; /j/jiahao-sun/; /w/wei-dai/",
        "bibtex": "@inproceedings{cheng-etal-2025-weaponization,\n    title = \"On Weaponization-Resistant Large Language Models with Prospect Theoretic Alignment\",\n    author = \"Cheng, Zehua  and\n      Zhang, Manying  and\n      Sun, Jiahao  and\n      Dai, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.687/\",\n    pages = \"10309--10324\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.687.pdf",
        "site": "https://aclanthology.org/2025.coling-main.687/",
        "pdf_size": 2391757,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5837310402857158750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Oxford; Institut National des Langues et Civilisations Orientales; FLock.io; FLock.io",
        "aff_domain": "cs.ox.ac.uk;inalco.fr; ;flock.io",
        "email": "cs.ox.ac.uk;inalco.fr; ;flock.io",
        "github": "",
        "project": "https://anonymous.4open.science/r/KT-IPA-40B7",
        "author_num": 4,
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "University of Oxford;Institut National des Langues et Civilisations Orientales;FLock.io",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.inalco.fr;https://www.flock.io",
        "aff_unique_abbr": "Oxford;INALCO;FLock",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;2",
        "aff_country_unique": "United Kingdom;France;United States"
    },
    {
        "id": "2025.coling-main.445",
        "title": "On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text-based reinforcement learning involves an agent interacting with a fictional environment using observed text and admissible actions in natural language to complete a task. Previous works have shown that agents can succeed in text-based interactive environments even in the complete absence of semantic understanding or other linguistic capabilities. The success of these agents in playing such games suggests that semantic understanding may not be important for the task. This raises an important question about the benefits of LMs in guiding the agents through the game states. In this work, we show that rich semantic understanding leads to efficient training of text-based RL agents. Moreover, we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning (TBRL). Specifically, we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games. These results may help develop better strategies to fine-tune agents in text-based RL scenarios.",
        "author": "Mauricio Gruppi; Soham Dan; Keerthiram Murugesan; Subhajit Chaudhury",
        "authorids": "/m/mauricio-gruppi/; /s/soham-dan/; /k/keerthiram-murugesan/; /s/subhajit-chaudhury/",
        "bibtex": "@inproceedings{gruppi-etal-2025-effects,\n    title = \"On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning\",\n    author = \"Gruppi, Mauricio  and\n      Dan, Soham  and\n      Murugesan, Keerthiram  and\n      Chaudhury, Subhajit\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.445/\",\n    pages = \"6649--6658\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.445.pdf",
        "site": "https://aclanthology.org/2025.coling-main.445/",
        "pdf_size": 2155995,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13081075298288285261&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Villanova University; IBM Research; IBM Research; IBM Research",
        "aff_domain": "villanova.edu;ibm.com;ibm.com;ibm.com",
        "email": "villanova.edu;ibm.com;ibm.com;ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Villanova University;IBM",
        "aff_unique_dep": ";IBM Research",
        "aff_unique_url": "https://www.villanova.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "Villanova;IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.277",
        "title": "On the Human-level Performance of Visual Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Visual7W has been widely used in assessing multiple-choice visual question-answering (VQA) systems. This paper reports on a replicated human experiment on Visual7W with the aim of understanding the human-level performance of VQA. The replication was not entirely successful because human participants performed significantly worse when answering \u201cwhere\u201d, \u201cwhen\u201d, and \u201chow\u201d questions in compared to other question types. An error analysis discovered that the failure was a consequence of the non-deterministic distractors in Visual7W. GPT-4V was then evaluated using and was compared to the human-level performance. The results embody that, when evaluating models\u2019 capacity on Visual7W, the performance is not necessarily the higher, the better.",
        "author": "Chenlian Zhou; Guanyi Chen; Xin Bai; Ming Dong",
        "authorids": "/c/chenlian-zhou/; /g/guanyi-chen/; /x/xin-bai/; /m/ming-dong/",
        "bibtex": "@inproceedings{zhou-etal-2025-human,\n    title = \"On the Human-level Performance of Visual Question Answering\",\n    author = \"Zhou, Chenlian  and\n      Chen, Guanyi  and\n      Bai, Xin  and\n      Dong, Ming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.277/\",\n    pages = \"4109--4113\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.277.pdf",
        "site": "https://aclanthology.org/2025.coling-main.277/",
        "pdf_size": 2333976,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yM-sse7_BM4J:scholar.google.com/&scioq=On+the+Human-level+Performance+of+Visual+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Faculty of Artificial Intelligence in Education; School of Computer Science; Faculty of Artificial Intelligence in Education; School of Computer Science",
        "aff_domain": "mails.ccnu.edu.cn;ccnu.edu.cn;mails.ccnu.edu.cn;ccnu.edu.cn",
        "email": "mails.ccnu.edu.cn;ccnu.edu.cn;mails.ccnu.edu.cn;ccnu.edu.cn",
        "github": "",
        "project": "https://ai.stanford.edu/~yukez/visual7w/",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Faculty of Artificial Intelligence in Education;School of Computer Science",
        "aff_unique_dep": "Artificial Intelligence in Education;Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2025.coling-industry.39",
        "title": "On the effective transfer of knowledge from English to Hindi Wikipedia",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Although Wikipedia is the largest multilingual encyclopedia, it remains inherently incomplete. There is a significant disparity in the quality of content between high-resource languages (HRLs, e.g., English) and low-resource languages (LRLs, e.g., Hindi), with many LRL articles lacking adequate information. To bridge these content gaps, we propose a lightweight framework to enhance knowledge equity between English and Hindi. In case the English Wikipedia page is not up-to-date, our framework extracts relevant information from external resources readily available (such as English books), and adapts it to align with Wikipedia\u2019s distinctive style, including its neutral point of view (NPOV) policy, using in-context learning capabilities of large language models. The adapted content is then machine-translated into Hindi for integration into the corresponding Wikipedia articles. On the other hand, if the English version is comprehensive and up-to-date, the framework directly transfers knowledge from English to Hindi. Our framework effectively generates new content for Hindi Wikipedia sections, enhancing Hindi Wikipedia articles respectively by 65% and 62% according to automatic and human judgment-based evaluations.",
        "author": "Paramita Das; Amartya Roy; Ritabrata Chakraborty; Animesh Mukherjee",
        "authorids": "/p/paramita-das/; /a/amartya-roy/; /r/ritabrata-chakraborty/; /a/animesh-mukherjee/",
        "bibtex": "@inproceedings{das-etal-2025-effective,\n    title = \"On the effective transfer of knowledge from {E}nglish to {H}indi {W}ikipedia\",\n    author = \"Das, Paramita  and\n      Roy, Amartya  and\n      Chakraborty, Ritabrata  and\n      Mukherjee, Animesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.39/\",\n    pages = \"453--465\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.39.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.39/",
        "pdf_size": 626827,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:PtnWRVvssHcJ:scholar.google.com/&scioq=On+the+effective+transfer+of+knowledge+from+English+to+Hindi+Wikipedia&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "IIT Kharagpur; Bosch; Osmania University; IIT Kharagpur",
        "aff_domain": "gmail.com;in.bosch.com;gmail.com;cse.iitkgp.ac.in",
        "email": "gmail.com;in.bosch.com;gmail.com;cse.iitkgp.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur;Robert Bosch GmbH;Osmania University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iitkgp.ac.in;https://www.bosch.com;https://www.osmania.ac.in",
        "aff_unique_abbr": "IIT KGP;Bosch;OU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kharagpur;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "India;Germany"
    },
    {
        "id": "2025.coling-main.755",
        "title": "OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the fac- tual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different pa- pers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified framework for building customized automatic fact-checking systems, benchmarking their accuracy, evaluating factuality of LLMs, and verifying claims in a document. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM\u2019s factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers\u2019 verification results using human-annotated datasets. Data and code are publicly available at https: //github.com/yuxiaw/openfactcheck.",
        "author": "Yuxia Wang; Minghan Wang; Hasan Iqbal; Georgi N. Georgiev; Jiahui Geng; Iryna Gurevych; Preslav Nakov",
        "authorids": "/y/yuxia-wang/; /m/minghan-wang/; /h/hasan-iqbal/; /g/georgi-n-georgiev/; /j/jiahui-geng/; /i/iryna-gurevych/; /p/preslav-nakov/",
        "bibtex": "@inproceedings{wang-etal-2025-openfactcheck,\n    title = \"{O}pen{F}act{C}heck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and {LLM}s\",\n    author = \"Wang, Yuxia  and\n      Wang, Minghan  and\n      Iqbal, Hasan  and\n      Georgiev, Georgi N.  and\n      Geng, Jiahui  and\n      Gurevych, Iryna  and\n      Nakov, Preslav\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.755/\",\n    pages = \"11399--11421\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.755.pdf",
        "site": "https://aclanthology.org/2025.coling-main.755/",
        "pdf_size": 2073599,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1487829205338685298&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "MBZUAI; Monash University; MBZUAI; Sofia University; MBZUAI; MBZUAI; MBZUAI",
        "aff_domain": "mbzuai.ac.ae; ;mbzuai.ac.ae; ; ;mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae; ;mbzuai.ac.ae; ; ;mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "https://github.com/yuxiaw/openfactcheck",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;2;0;0;0",
        "aff_unique_norm": "Mohamed Bin Zayed University of Artificial Intelligence;Monash University;Sofia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.monash.edu;https://www.sofiauni.bg/en/",
        "aff_unique_abbr": "MBZUAI;Monash;Sofia U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;0;0;0",
        "aff_country_unique": "United Arab Emirates;Australia;Bulgaria"
    },
    {
        "id": "2025.coling-main.353",
        "title": "OpenForecast: A Large-Scale Open-Ended Event Forecasting Dataset",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Complex events generally exhibit unforeseen, multifaceted, and multi-step developments, and cannot be well handled by existing closed-ended event forecasting methods, which are constrained by a limited answer space. In order to accelerate the research on complex event forecasting, we introduce OpenForecast, a large-scale open-ended dataset with two features: (1) OpenForecast defines three open-ended event forecasting tasks, enabling unforeseen, multifaceted, and multi-step forecasting. (2) OpenForecast collects and annotates a large-scale dataset from Wikipedia and news, including 43,419 complex events spanning from 1950 to 2024. Particularly, this annotation can be completed automatically without any manual annotation cost. Meanwhile, we introduce an automatic LLM-based Retrieval-Augmented Evaluation method (LRAE) for complex events, enabling OpenForecast to evaluate the ability of complex event forecasting of large language models. Finally, we conduct comprehensive human evaluations to verify the quality and challenges of OpenForecast, and the consistency between LEAE metric and human evaluation. OpenForecast and related codes will be publicly released.",
        "author": "Zhen Wang; Xi Zhou; Yating Yang; Bo Ma; Lei Wang; Rui Dong; Azmat Anwar",
        "authorids": "/z/zhen-wang/; /x/xi-zhou/; /y/yating-yang/; /b/bo-ma/; /l/lei-wang/; /r/rui-dong/; /a/azmat-anwar/",
        "bibtex": "@inproceedings{wang-etal-2025-openforecast,\n    title = \"{O}pen{F}orecast: A Large-Scale Open-Ended Event Forecasting Dataset\",\n    author = \"Wang, Zhen  and\n      Zhou, Xi  and\n      Yang, Yating  and\n      Ma, Bo  and\n      Wang, Lei  and\n      Dong, Rui  and\n      Anwar, Azmat\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.353/\",\n    pages = \"5273--5294\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.353.pdf",
        "site": "https://aclanthology.org/2025.coling-main.353/",
        "pdf_size": 813510,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9739572085610067673&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.243",
        "title": "OptiPrune: Effective Pruning Approach for Every Target Sparsity",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have achieved notable success across various tasks but are hindered by their large size and high computational demands. Post-training pruning (PTP) offers a promising solution by reducing model size through parameter removal while preserving performance. However, current PTP methods perform optimally only within specific sparsity ranges. This paper presents two key findings: (1) Layerwise uniform sparsity is effective at low sparsity, while non-uniform sparsity excels at high levels; (2) Relative importance-based pruning works best at low sparsity, whereas Hessian-based weight reconstruction is superior at high sparsity. We design and conduct experiments to validate these findings. Based on these insights, we introduce OptiPrune, a robust pruning method effective across all sparsity levels. OptiPrune adapts non-uniform sparsity with adaptive deviation and employs a threshold to select the optimal pruning strategy. Empirical results across diverse datasets, architectures, and languages validate its performance and robustness. These findings provide valuable directions for future LLM pruning research. Our code and data are publicly available.",
        "author": "Khang Nguyen Le; Ryo Sato; Dai Nakashima; Takeshi Suzuki; Minh Le Nguyen",
        "authorids": "/k/khang-nguyen-le/; /r/ryo-sato/; /d/dai-nakashima/; /t/takeshi-suzuki/; /m/minh-le-nguyen/",
        "bibtex": "@inproceedings{le-etal-2025-optiprune,\n    title = \"{O}pti{P}rune: Effective Pruning Approach for Every Target Sparsity\",\n    author = \"Le, Khang Nguyen  and\n      Sato, Ryo  and\n      Nakashima, Dai  and\n      Suzuki, Takeshi  and\n      Nguyen, Minh Le\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.243/\",\n    pages = \"3600--3612\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.243.pdf",
        "site": "https://aclanthology.org/2025.coling-main.243/",
        "pdf_size": 645985,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uEEjqRPN6h4J:scholar.google.com/&scioq=OptiPrune:+Effective+Pruning+Approach+for+Every+Target+Sparsity&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Japan Advanced Institute of Science and Technology; RICOH; RICOH; RICOH; Japan Advanced Institute of Science and Technology",
        "aff_domain": "jaist.ac.jp; ; ; ;jaist.ac.jp",
        "email": "jaist.ac.jp; ; ; ;jaist.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Japan Advanced Institute of Science and Technology;RICOH Company, Ltd.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jaist.ac.jp;https://www.ricoh.com",
        "aff_unique_abbr": "JAIST;RICOH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.746",
        "title": "Optimizing Lifelong Fine-Tuning for Multiple Tasks via Dataless Distribution Replay",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The recent emergence of various large language models, which can be fine-tuned with minimal instruction data, has demonstrated impressive performance across various tasks. However, a phenomenon of forgetting occurs during life- long fine-tuning because training on new tasks interferes with the previously acquired knowl- edge. To mitigate catastrophic forgetting, con- ventional data replay methods achieve high per- formance, but at the cost of compromising data privacy and security. This paper introduces a dataless distribution replay approach for life- long fine-tuning. Concretely, the distribution distillation is applied to replay the output dis- tribution of the linear layers at previous task stages. The optimal solution for this distri- bution replay can be directly computed using the retained inner product matrix of the input data, thereby eliminating the need for previ- ous data. Additionally, Singular Value Decom- position (SVD) and module accumulation are employed to further enhance the performance of dataless distribution replay method. Finally, the evaluation is conducted in a lifelong fine- tuning scenario involving multiple tasks. The experimental results and analysis show that the proposed method achieves significant improve- ments compared to several strong lifelong fine- tuning methods.",
        "author": "Zhenxing Wang",
        "authorids": "/z/zhenxing-wang/",
        "bibtex": "@inproceedings{wang-2025-optimizing,\n    title = \"Optimizing Lifelong Fine-Tuning for Multiple Tasks via Dataless Distribution Replay\",\n    author = \"Wang, Zhenxing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.746/\",\n    pages = \"11261--11273\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.746.pdf",
        "site": "https://aclanthology.org/2025.coling-main.746/",
        "pdf_size": 3660627,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11053571144819162357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Peng Cheng Laboratory",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Peng Cheng Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.pcl.ac.cn",
        "aff_unique_abbr": "PCL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.382",
        "title": "PADO: Personality-induced multi-Agents for Detecting OCEAN in human-generated texts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As personality can be useful in many cases, such as better understanding people\u2019s underlying contexts or providing personalized services, research has long focused on modeling personality from data. However, the development of personality detection models faces challenges due to the inherent latent and relative characteristics of personality, as well as the lack of annotated datasets. To address these challenges, our research focuses on methods that effectively exploit the inherent knowledge of Large Language Models (LLMs). We propose a novel approach that compares contrasting perspectives to better capture the relative nature of personality traits. In this paper, we introduce PADO (Personality-induced multi-Agent framework for Detecting OCEAN of the Big Five personality traits), the first LLM-based multi-agent personality detection framework. PADO employs personality-induced agents to analyze text from multiple perspectives, followed by a comparative judgment process to determine personality trait levels. Our experiments with various LLM models, from GPT-4o to LLaMA3-8B, demonstrate PADO\u2019s effectiveness and generalizability, especially with smaller parameter models. This approach offers a more nuanced, context-aware method for personality detection, potentially improving personalized services and insights into digital behavior. We will release our codes.",
        "author": "Haein Yeo; Taehyeong Noh; Seungwan Jin; Kyungsik Han",
        "authorids": "/h/haein-yeo/; /t/taehyeong-noh/; /s/seungwan-jin/; /k/kyungsik-han/",
        "bibtex": "@inproceedings{yeo-etal-2025-pado,\n    title = \"{PADO}: Personality-induced multi-Agents for Detecting {OCEAN} in human-generated texts\",\n    author = \"Yeo, Haein  and\n      Noh, Taehyeong  and\n      Jin, Seungwan  and\n      Han, Kyungsik\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.382/\",\n    pages = \"5719--5736\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.382.pdf",
        "site": "https://aclanthology.org/2025.coling-main.382/",
        "pdf_size": 2491184,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4779232762304128876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Artificial Intelligence, Hanyang University, Seoul, Republic of Korea; Department of Artificial Intelligence, Hanyang University, Seoul, Republic of Korea; Department of Data Science, Hanyang University, Seoul, Republic of Korea; Department of Artificial Intelligence, Hanyang University, Seoul, Republic of Korea + Department of Data Science, Hanyang University, Seoul, Republic of Korea",
        "aff_domain": "hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr",
        "email": "hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr",
        "github": "https://github.com/haaaein/PADO",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+0",
        "aff_unique_norm": "Hanyang University",
        "aff_unique_dep": "Department of Artificial Intelligence",
        "aff_unique_url": "http://www.hanyang.ac.kr",
        "aff_unique_abbr": "HYU",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.585",
        "title": "PARAPHRASUS: A Comprehensive Benchmark for Evaluating Paraphrase Detection Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of determining whether two texts are paraphrases has long been a challenge in NLP. However, the prevailing notion of paraphrase is often quite simplistic, offering only a limited view of the vast spectrum of paraphrase phenomena. Indeed, we find that evaluating models in a paraphrase dataset can leave uncertainty about their true semantic understanding. To alleviate this, we create PARAPHRASUS, a benchmark designed for multi-dimensional assessment, benchmarking and selection of paraphrase detection models. We find that paraphrase detection models under our fine-grained evaluation lens exhibit trade-offs that cannot be captured through a single classification dataset. Furthermore, PARAPHRASUS allows prompt calibration for different use cases, tailoring LLM models to specific strictness levels. PARAPHRASUS includes 3 challenges spanning over 10 datasets, including 8 repurposed and 2 newly annotated; we release it along with a benchmarking library at https://github.com/impresso/paraphrasus",
        "author": "Andrianos Michail; Simon Clematide; Juri Opitz",
        "authorids": "/a/andrianos-michail/; /s/simon-clematide/; /j/juri-opitz/",
        "bibtex": "@inproceedings{michail-etal-2025-paraphrasus,\n    title = \"{PARAPHRASUS}: A Comprehensive Benchmark for Evaluating Paraphrase Detection Models\",\n    author = \"Michail, Andrianos  and\n      Clematide, Simon  and\n      Opitz, Juri\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.585/\",\n    pages = \"8749--8762\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.585.pdf",
        "site": "https://aclanthology.org/2025.coling-main.585/",
        "pdf_size": 490627,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14791960570471977731&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Zurich; University of Zurich; University of Zurich",
        "aff_domain": "cl.uzh.ch;cl.uzh.ch;cl.uzh.ch",
        "email": "cl.uzh.ch;cl.uzh.ch;cl.uzh.ch",
        "github": "https://github.com/impresso/paraphrasus",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uzh.ch",
        "aff_unique_abbr": "UZH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2025.coling-industry.7",
        "title": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM\u2019s SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the \u201cdisorientation\u201d in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.",
        "author": "Yiwen Duan; Yonghong Yu; Xiaoming Zhao; Yichang Wu; Wenbo Liu",
        "authorids": "/y/yiwen-duan/; /y/yonghong-yu/; /x/xiaoming-zhao/; /y/yichang-wu/; /w/wenbo-liu/",
        "bibtex": "@inproceedings{duan-etal-2025-pdc,\n    title = \"{PDC} {\\&} {DM}-{SFT}: A Road for {LLM} {SQL} Bug-Fix Enhancing\",\n    author = \"Duan, Yiwen  and\n      Yu, Yonghong  and\n      Zhao, Xiaoming  and\n      Wu, Yichang  and\n      Liu, Wenbo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.7/\",\n    pages = \"76--90\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.7.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.7/",
        "pdf_size": 1994261,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ddNqfqTBrPkJ:scholar.google.com/&scioq=PDC+%26+DM-SFT:+A+Road+for+LLM+SQL+Bug-Fix+Enhancing&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Bytedance Inc.; Bytedance Inc.; Bytedance Inc.; Bytedance Inc.; Bytedance Inc.",
        "aff_domain": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "email": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Bytedance Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bytedance.com",
        "aff_unique_abbr": "Bytedance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.534",
        "title": "PERC: Plan-As-Query Example Retrieval for Underrepresented Code Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Code generation with large language models has shown significant promise, especially when employing retrieval-augmented generation (RAG) with few-shot examples. However, selecting effective examples that enhance generation quality remains a challenging task, particularly when the target programming language (PL) is underrepresented. In this study, we present two key findings: (1) retrieving examples whose presented algorithmic plans can be referenced for generating the desired behavior significantly improves generation accuracy, and (2) converting code into pseudocode effectively captures such algorithmic plans, enhancing retrieval quality even when the source and the target PLs are different. Based on these findings, we propose Plan-as-query Example Retrieval for few-shot prompting in Code generation (PERC), a novel framework that utilizes algorithmic plans to identify and retrieve effective examples. We validate the effectiveness of PERC through extensive experiments on the CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms the state-of-the-art RAG methods in code generation, both when the source and target programming languages match or differ, highlighting its adaptability and robustness in diverse coding environments.",
        "author": "Jaeseok Yoo; Hojae Han; Youngwon Lee; Jaejin Kim; Seung-won Hwang",
        "authorids": "/j/jaeseok-yoo/; /h/hojae-han/; /y/youngwon-lee/; /j/jaejin-kim/; /s/seung-won-hwang/",
        "bibtex": "@inproceedings{yoo-etal-2025-perc,\n    title = \"{PERC}: Plan-As-Query Example Retrieval for Underrepresented Code Generation\",\n    author = \"Yoo, Jaeseok  and\n      Han, Hojae  and\n      Lee, Youngwon  and\n      Kim, Jaejin  and\n      Hwang, Seung-won\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.534/\",\n    pages = \"7982--7997\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.534.pdf",
        "site": "https://aclanthology.org/2025.coling-main.534/",
        "pdf_size": 396482,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:r7M0e21Sq38J:scholar.google.com/&scioq=PERC:+Plan-As-Query+Example+Retrieval+for+Underrepresented+Code+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Seoul National University; Seoul National University; Seoul National University; Seoul National University+Interdisciplinary Program in Artificial Intelligence, Seoul National University; Seoul National University+Interdisciplinary Program in Artificial Intelligence, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0+0;0+0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;0;0+0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.752",
        "title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid advancement of language models (LMs) necessitates robust alignment with diverse user values. However, current preference optimization approaches often fail to capture the plurality of user opinions, instead reinforcing majority viewpoints and marginalizing minority perspectives. We introduce PERSONA, a reproducible test bed designed to evaluate and improve pluralistic alignment of LMs. We procedurally generate diverse user profiles from US census data, resulting in 1,586 synthetic personas with varied demographic and idiosyncratic attributes. We then generate a large-scale evaluation dataset containing 3,868 prompts and 317,200 feedback pairs obtained from our synthetic personas. Leveraging this dataset, we systematically evaluate LM capabilities in role-playing diverse users, verified through human judges, and the establishment of both a benchmark, PERSONA Bench, for pluralistic alignment approaches as well as an extensive dataset to create new and future benchmarks.",
        "author": "Louis Castricato; Nathan Lile; Rafael Rafailov; Jan-Philipp Fr\u00e4nken; Chelsea Finn",
        "authorids": "/l/louis-castricato/; /n/nathan-lile/; /r/rafael-rafailov/; /j/jan-philipp-franken/; /c/chelsea-finn/",
        "bibtex": "@inproceedings{castricato-etal-2025-persona,\n    title = \"{PERSONA}: A Reproducible Testbed for Pluralistic Alignment\",\n    author = {Castricato, Louis  and\n      Lile, Nathan  and\n      Rafailov, Rafael  and\n      Fr{\\\"a}nken, Jan-Philipp  and\n      Finn, Chelsea},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.752/\",\n    pages = \"11348--11368\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.752.pdf",
        "site": "https://aclanthology.org/2025.coling-main.752/",
        "pdf_size": 3121955,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9416302317201664153&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "SynthLabs.ai; SynthLabs.ai; Stanford University; Stanford University; Stanford University",
        "aff_domain": "synthlabs.ai;synthlabs.ai; ; ; ",
        "email": "synthlabs.ai;synthlabs.ai; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "SynthLabs.ai;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.synthlabs.ai;https://www.stanford.edu",
        "aff_unique_abbr": "SynthLabs.ai;Stanford",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.401",
        "title": "PIRsuader: A Persuasive Chatbot for Mitigating Psychological Insulin Resistance in Type-2 Diabetic Patients",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Psychological Insulin Resistance (PIR) is described as the reluctance towards initiation and adherence of insulin-based treatments due to psychological barriers in diabetic patients. Though studies have shown that timely initiation with lifestyle changes are known to be crucial in sugar control and prevention of chronic conditions in Type 2 Diabetes (T2D) patients, many patients often have deep-rooted fears and misgivings related to insulin which hinder them from adapting to an insulin-based treatment regimen when recommended by healthcare specialists. Therefore, it is vitally important to address and allay these fallacious beliefs in T2D patients and persuade them to consider insulin as a treatment option. In this paper, we describe the design of PIRsuader, a persuasive chatbot for mitigating PIR in T2D patients. In PIRsuader, we effectively harness the conversation generation capabilities of state-of-the-art Large Language Models via a context-specific persuasive dialog act schema. We design reward functions that capture dialog act preferences for persuading reluctant patients and apply reinforcement learning to learn a dialog act prediction model. Our experiments using a collection of real doctor-diabetic patient conversations indicate that PIRsuader is able to improve the willingness in patients to try insulin as well as address specific concerns they have in an empathetic manner.",
        "author": "Sujatha Das Gollapalli; See-Kiong Ng",
        "authorids": "/s/sujatha-das-gollapalli/; /s/see-kiong-ng/",
        "bibtex": "@inproceedings{gollapalli-ng-2025-pirsuader,\n    title = \"{PIR}suader: A Persuasive Chatbot for Mitigating Psychological Insulin Resistance in Type-2 Diabetic Patients\",\n    author = \"Gollapalli, Sujatha Das  and\n      Ng, See-Kiong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.401/\",\n    pages = \"5997--6013\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.401.pdf",
        "site": "https://aclanthology.org/2025.coling-main.401/",
        "pdf_size": 369425,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18107980755934994188&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Data Science, National University of Singapore; Institute of Data Science, National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Institute of Data Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2025.coling-main.592",
        "title": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Low-rank adaptation (LoRA) and its variants have recently gained much interest due to their ability to avoid excessive inference costs. However, LoRA still encounters the following challenges: (1) Limitation of low-rank assumption; and (2) Its initialization method may be suboptimal. To this end, we propose PMSS(Pre-trained Matrices Skeleton Selection), which enables high-rank updates with low costs while leveraging semantic and linguistic information inherent in pre-trained weight. It achieves this by selecting skeletons from the pre-trained weight matrix and only learning a small matrix instead. Experiments demonstrate that PMSS outperforms LoRA and other fine-tuning methods across tasks with much less trainable parameters. We demonstrate its effectiveness, especially in handling complex tasks such as DROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math reasoning (+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of GSM8K).The code and model will be released soon.",
        "author": "Qibin Wang; Xiaolin Hu; Weikai Xu; Wei Liu; Jian Luan; Bin Wang",
        "authorids": "/q/qibin-wang/; /x/xiaolin-hu/; /w/weikai-xu/; /w/wei-liu/; /j/jian-luan/; /b/bin-wang/",
        "bibtex": "@inproceedings{wang-etal-2025-pmss,\n    title = \"{PMSS}: Pretrained Matrices Skeleton Selection for {LLM} Fine-tuning\",\n    author = \"Wang, Qibin  and\n      Hu, Xiaolin  and\n      Xu, Weikai  and\n      Liu, Wei  and\n      Luan, Jian  and\n      Wang, Bin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.592/\",\n    pages = \"8841--8857\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.592.pdf",
        "site": "https://aclanthology.org/2025.coling-main.592/",
        "pdf_size": 443921,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9720676514723158929&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Peking University+XiaoMi AI Lab; Gaoling School of Artificial Intelligence, Renmin University of China+XiaoMi AI Lab; University of Electronic Science and Technology of China+XiaoMi AI Lab; XiaoMi AI Lab; XiaoMi AI Lab; XiaoMi AI Lab",
        "aff_domain": "stu.pku.edu.cn; ; ; ; ;xiaomi.com",
        "email": "stu.pku.edu.cn; ; ; ; ;xiaomi.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2+1;3+1;1;1;1",
        "aff_unique_norm": "Peking University;XiaoMi Corporation;Renmin University of China;University of Electronic Science and Technology of China",
        "aff_unique_dep": ";XiaoMi AI Lab;Gaoling School of Artificial Intelligence;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.xiaomi.com;http://www.ruc.edu.cn;https://www.uestc.edu.cn",
        "aff_unique_abbr": "Peking U;Xiaomi;RUC;UESTC",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.556",
        "title": "PToco: Prefix-based Token-level Collaboration Enhances Reasoning for Multi-LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Collaboration between multiple Large Language Models (LLMs) has attracted significant attention for its potential to mitigate hallucinations and enhance reasoning capabilities. Previous approaches, such as multi-agent debate and decoding-time integration, either rely on highly capable models with strong self-reflection abilities or are limited to models sharing the same tokenizer. To address these limitations, we introduce PToco (Prefix-based Token-level Collaboration), a novel mechanism that enables effective collaboration among less capable LLMs, independent of tokenizer differences. PToco uses a prefix-grouping method to extract consensus among tokens with varying levels of granularity, ensuring coherent and robust token generation across multiple models. Experimental results on a series of reasoning tasks demonstrate that PToco significantly improves performance over individual models. Furthermore, this approach generalizes well across different quantities and sizes of participating models, providing a more flexible and efficient solution for multi-LLM ensembles.",
        "author": "Yuang Bian; Yupian Lin; Jingping Liu; Tong Ruan",
        "authorids": "/y/yuang-bian/; /y/yupian-lin/; /j/jingping-liu/; /t/tong-ruan/",
        "bibtex": "@inproceedings{bian-etal-2025-ptoco,\n    title = \"{PT}oco: Prefix-based Token-level Collaboration Enhances Reasoning for Multi-{LLM}s\",\n    author = \"Bian, Yuang  and\n      Lin, Yupian  and\n      Liu, Jingping  and\n      Ruan, Tong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.556/\",\n    pages = \"8326--8335\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.556.pdf",
        "site": "https://aclanthology.org/2025.coling-main.556/",
        "pdf_size": 938385,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0HJC7LrmLOQJ:scholar.google.com/&scioq=PToco:+Prefix-based+Token-level+Collaboration+Enhances+Reasoning+for+Multi-LLMs&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China",
        "aff_domain": "gmail.com;aliyun.com;ecust.edu.cn;ecust.edu.cn",
        "email": "gmail.com;aliyun.com;ecust.edu.cn;ecust.edu.cn",
        "github": "https://github.com/BianYuang/PToco",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "East China University of Science and Technology",
        "aff_unique_dep": "School of Information Science and Engineering",
        "aff_unique_url": "http://www.ecust.edu.cn",
        "aff_unique_abbr": "ECUST",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.26",
        "title": "Page Stream Segmentation with LLMs: Challenges and Applications in Insurance Document Automation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Page Stream Segmentation (PSS) is critical for automating document processing in industries like insurance, where unstructured document collections are common. This paper explores the use of large language models (LLMs) for PSS, applying parameter-efficient fine-tuning to real-world insurance data. Our experiments show that LLMs outperform baseline models in page- and stream-level segmentation accuracy. However, stream-level calibration remains challenging, especially for high-stakes applications. We evaluate post-hoc calibration and Monte Carlo dropout, finding limited improvement. Future work will integrate active learning to enhance model calibration and support deployment in practical settings.",
        "author": "Hunter Heidenreich; Ratish Dalvi; Nikhil Verma; Yosheb Getachew",
        "authorids": "/h/hunter-heidenreich/; /r/ratish-dalvi/; /n/nikhil-verma/; /y/yosheb-getachew/",
        "bibtex": "@inproceedings{heidenreich-etal-2025-page,\n    title = \"Page Stream Segmentation with {LLM}s: Challenges and Applications in Insurance Document Automation\",\n    author = \"Heidenreich, Hunter  and\n      Dalvi, Ratish  and\n      Verma, Nikhil  and\n      Getachew, Yosheb\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.26/\",\n    pages = \"305--317\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.26.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.26/",
        "pdf_size": 662057,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3iZqeQjY_GUJ:scholar.google.com/&scioq=Page+Stream+Segmentation+with+LLMs:+Challenges+and+Applications+in+Insurance+Document+Automation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Roots Automation, New York, NY; Roots Automation, New York, NY; Roots Automation, New York, NY; Roots Automation, New York, NY",
        "aff_domain": "rootsautomation.com; ; ; ",
        "email": "rootsautomation.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Roots Automation",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.265",
        "title": "Parameter-Efficient Fine-Tuning of Large Language Models via Deconvolution in Subspace",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper proposes a novel parameter-efficient fine-tuning method that combines the knowledge completion capability of deconvolution with the subspace learning ability, reducing the number of parameters required for fine-tuning by 8 times . Experimental results demonstrate that our method achieves superior training efficiency and performance compared to existing models.",
        "author": "Jia-Chen Zhang; Yu-Jie Xiong; Chun-Ming Xia; Dong-Hai Zhu; Xi-He Qiu",
        "authorids": "/j/jia-chen-zhang/; /y/yu-jie-xiong/; /c/chun-ming-xia/; /d/dong-hai-zhu/; /x/xi-he-qiu/",
        "bibtex": "@inproceedings{zhang-etal-2025-parameter,\n    title = \"Parameter-Efficient Fine-Tuning of Large Language Models via Deconvolution in Subspace\",\n    author = \"Zhang, Jia-Chen  and\n      Xiong, Yu-Jie  and\n      Xia, Chun-Ming  and\n      Zhu, Dong-Hai  and\n      Qiu, Xi-He\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.265/\",\n    pages = \"3924--3935\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.265.pdf",
        "site": "https://aclanthology.org/2025.coling-main.265/",
        "pdf_size": 640788,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1494460659570425983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Electronic and Electrical Engineering, Shanghai University of Engineering Science; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science",
        "aff_domain": "sues.edu.cn; ; ; ; ",
        "email": "sues.edu.cn; ; ; ; ",
        "github": "https://github.com/Godz-z/DCFT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Shanghai University of Engineering Science",
        "aff_unique_dep": "School of Electronic and Electrical Engineering",
        "aff_unique_url": "http://www.sues.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.538",
        "title": "Paraphrase Generation Evaluation Powered by an LLM: A Semantic Metric, Not a Lexical One",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Evaluating automatic paraphrase production systems is a difficult task as it involves, among other things, assessing the semantic proximity between two sentences. Usual measures are based on lexical distances, or at least on semantic embedding alignments. The rise of Large Language Models (LLM) has provided tools to model relationships within a text thanks to the attention mechanism. In this article, we introduce ParaPLUIE, a new measure based on a log likelihood ratio from an LLM, to assess the quality of a potential paraphrase. This measure is compared with usual measures on two known by the NLP community datasets prior to this study. Three new small datasets have been built to allow metrics to be compared in different scenario and to avoid data contamination bias. According to evaluations, the proposed measure is better for sorting pairs of sentences by semantic proximity. In particular, it is much more independent to lexical distance and provides an interpretable classification threshold between paraphrases and non-paraphrases.",
        "author": "Quentin Lemesle; Jonathan Chevelu; Philippe Martin; Damien Lolive; Arnaud Delhay; Nelly Barbot",
        "authorids": "/q/quentin-lemesle/; /j/jonathan-chevelu/; /p/philippe-martin/; /d/damien-lolive/; /a/arnaud-delhay/; /n/nelly-barbot/",
        "bibtex": "@inproceedings{lemesle-etal-2025-paraphrase,\n    title = \"Paraphrase Generation Evaluation Powered by an {LLM}: A Semantic Metric, Not a Lexical One\",\n    author = \"Lemesle, Quentin  and\n      Chevelu, Jonathan  and\n      Martin, Philippe  and\n      Lolive, Damien  and\n      Delhay, Arnaud  and\n      Barbot, Nelly\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.538/\",\n    pages = \"8057--8087\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.538.pdf",
        "site": "https://aclanthology.org/2025.coling-main.538/",
        "pdf_size": 6983901,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:OTiklJM21lsJ:scholar.google.com/&scioq=Paraphrase+Generation+Evaluation+Powered+by+an+LLM:+A+Semantic+Metric,+Not+a+Lexical+One&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Univ Rennes, CNRS, IRISA, Expression; Univ Rennes, CNRS, IRISA, Expression; Univ Rennes, CNRS, IRISA, Expression; Univ Rennes, CNRS, IRISA, Expression + Univ of South Brittany, CNRS, IRISA, Expression; Univ Rennes, CNRS, IRISA, Expression; Univ Rennes, CNRS, IRISA, Expression",
        "aff_domain": "irisa.fr; ; ; ; ; ",
        "email": "irisa.fr; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0+1;0;0",
        "aff_unique_norm": "University of Rennes;University of South Brittany",
        "aff_unique_dep": "CNRS, IRISA;",
        "aff_unique_url": "https://www.univ-rennes1.fr;https://www.univ-ubs.fr",
        "aff_unique_abbr": "Univ Rennes;Univ of South Brittany",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.245",
        "title": "Paraphrase Makes Perfect: Leveraging Expression Paraphrase to Improve Implicit Sentiment Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing implicit sentiment learning methods mainly focus on capturing implicit sentiment knowledge individually, without paying more attention to the potential connection between implicit and explicit sentiment. From a linguistic perspective, implicit and explicit sentiment expressions are essentially similar when conveying the same sentiment polarity for a specific aspect. In this paper, we present an expression paraphrase strategy and a novel sentiment-consistent contrastive learning mechanism to learn the intrinsic connections between implicit and explicit sentiment expressions and integrate them into the model to enhance implicit sentiment learning. We perform extensive experiments on public datasets, and the results show the significant efficacy of our method on implicit sentiment analysis.",
        "author": "Xia Li; Junlang Wang; Yongqiang Zheng; Yuan Chen; Yangjia Zheng",
        "authorids": "/x/xia-li/; /j/junlang-wang/; /y/yongqiang-zheng/; /y/yuan-chen/; /y/yangjia-zheng/",
        "bibtex": "@inproceedings{li-etal-2025-paraphrase,\n    title = \"Paraphrase Makes Perfect: Leveraging Expression Paraphrase to Improve Implicit Sentiment Learning\",\n    author = \"Li, Xia  and\n      Wang, Junlang  and\n      Zheng, Yongqiang  and\n      Chen, Yuan  and\n      Zheng, Yangjia\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.245/\",\n    pages = \"3631--3647\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.245.pdf",
        "site": "https://aclanthology.org/2025.coling-main.245/",
        "pdf_size": 1619063,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J8PkNbbrLusJ:scholar.google.com/&scioq=Paraphrase+Makes+Perfect:+Leveraging+Expression+Paraphrase+to+Improve+Implicit+Sentiment+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Science and Technology, Guangdong University of Foreign Studies; School of Information Science and Technology, Guangdong University of Foreign Studies; School of Information Science and Technology, Guangdong University of Foreign Studies; School of Information Science and Technology, Guangdong University of Foreign Studies; School of Information Science and Technology, Guangdong University of Foreign Studies",
        "aff_domain": "gdufs.edu.cn;gdufs.edu.cn;gdufs.edu.cn;gdufs.edu.cn;gdufs.edu.cn",
        "email": "gdufs.edu.cn;gdufs.edu.cn;gdufs.edu.cn;gdufs.edu.cn;gdufs.edu.cn",
        "github": "https://github.com/gdufsnlp/SECP",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Guangdong University of Foreign Studies",
        "aff_unique_dep": "School of Information Science and Technology",
        "aff_unique_url": "http://www.gdufs.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.431",
        "title": "Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study investigates the behavior of model-integrated routers in Mixture of Experts (MoE) models, focusing on how tokens are routed based on their linguistic features, specifically Part-of-Speech (POS) tags. The goal is to explore across different MoE architectures whether experts specialize in processing tokens with similar linguistic traits. By analyzing token trajectories across experts and layers, we aim to uncover how MoE models handle linguistic information. Findings from six popular MoE models reveal expert specialization for specific POS categories, with routing paths showing high predictive accuracy for POS, highlighting the value of routing paths in characterizing tokens.",
        "author": "Elie Antoine; Frederic Bechet; Phillippe Langlais",
        "authorids": "/e/elie-antoine/; /f/frederic-bechet/; /p/philippe-langlais/",
        "bibtex": "@inproceedings{antoine-etal-2025-part,\n    title = \"Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models\",\n    author = \"Antoine, Elie  and\n      Bechet, Frederic  and\n      Langlais, Phillippe\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.431/\",\n    pages = \"6467--6474\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.431.pdf",
        "site": "https://aclanthology.org/2025.coling-main.431/",
        "pdf_size": 3321023,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9701708280127159472&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "CNRS, LIS, Aix-Marseille Universit\u00e9, France+International Laboratory on Learning Systems (ILLS - IRL CNRS), Montreal; CNRS, LIS, Aix-Marseille Universit\u00e9, France+International Laboratory on Learning Systems (ILLS - IRL CNRS), Montreal; RALI, DIRO, Universit\u00e9 de Montr\u00e9al, Canada",
        "aff_domain": "lis-lab.fr;lis-lab.fr;iro.umontreal.ca",
        "email": "lis-lab.fr;lis-lab.fr;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Aix-Marseille Universit\u00e9;International Laboratory on Learning Systems;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "LIS;ILL CNRS;RALI, DIRO",
        "aff_unique_url": "https://www.univ-amu.fr;;https://www.umontreal.ca",
        "aff_unique_abbr": "AMU;ILLS;UdeM",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "0+1;0+1;1",
        "aff_country_unique": "France;Canada"
    },
    {
        "id": "2025.coling-main.101",
        "title": "Partial Order-centered Hyperbolic Representation Learning for Few-shot Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prototype network-based methods have made substantial progress in few-shot relation extraction (FSRE) by enhancing relation prototypes with relation descriptions. However, the distribution of relations and instances in distinct representation spaces isolates the constraints of relations on instances, making relation prototypes biased. In this paper, we propose an end-to-end partial order-centered hyperbolic representation learning (PO-HRL) framework, which imposes the constraints of relations on instances by modeling partial order in hyperbolic space, so as to effectively learn the distribution of instance representations. Specifically, we develop the hyperbolic supervised contrastive learning based on Lorentzian cosine similarity to align representations of relations and instances, and model the partial order by constraining instances to reside within the Lorentzian entailment cone of their respective relation. Experiments on three benchmark datasets show that PO-HRL outperforms the strong baselines, especially in 1-shot settings lacking relation descriptions.",
        "author": "Biao Hu; Zhen Huang; Minghao Hu; Pinglv Yang; Peng Qiao; Yong Dou; Zhilin Wang",
        "authorids": "/b/biao-hu/; /z/zhen-huang/; /m/minghao-hu/; /p/pinglv-yang/; /p/peng-qiao/; /y/yong-dou/; /z/zhilin-wang/",
        "bibtex": "@inproceedings{hu-etal-2025-partial,\n    title = \"Partial Order-centered Hyperbolic Representation Learning for Few-shot Relation Extraction\",\n    author = \"Hu, Biao  and\n      Huang, Zhen  and\n      Hu, Minghao  and\n      Yang, Pinglv  and\n      Qiao, Peng  and\n      Dou, Yong  and\n      Wang, Zhilin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.101/\",\n    pages = \"1503--1519\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.101.pdf",
        "site": "https://aclanthology.org/2025.coling-main.101/",
        "pdf_size": 913850,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:f3Y1epdhq0kJ:scholar.google.com/&scioq=Partial+Order-centered+Hyperbolic+Representation+Learning+for+Few-shot+Relation+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "National Key Laboratory of Parallel and Distributed Computing, National University of Defense Technology; National Key Laboratory of Parallel and Distributed Computing, National University of Defense Technology; Center of Information Research, Academy of Military Science; College of Meteorology and Oceanology, National University of Defense Technology; National Key Laboratory of Parallel and Distributed Computing, National University of Defense Technology; National Key Laboratory of Parallel and Distributed Computing, National University of Defense Technology; National Key Laboratory of Parallel and Distributed Computing, National University of Defense Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;gmail.com; ; ; ; ",
        "email": "nudt.edu.cn;nudt.edu.cn;gmail.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology;Academy of Military Science",
        "aff_unique_dep": "National Key Laboratory of Parallel and Distributed Computing;Center of Information Research",
        "aff_unique_url": "http://www.nudt.edu.cn/;",
        "aff_unique_abbr": "NUDT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.554",
        "title": "Perceive the Passage of Time: A Systematic Evaluation of Large Language Model in Temporal Relativity",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Temporal perception is crucial for Large Language Models(LLMs) to effectively understand the world. However, current benchmarks primarily focus on temporal reasoning, falling short in understanding the temporal characteristics involving temporal perception, particularly in understanding temporal relativity. In this paper, we introduce TempBench, a comprehensive benchmark designed to evaluate the temporal-relative ability of LLMs. TempBench encompasses 4 distinct scenarios: Physiology, Psychology, Cognition and Mixture. We conduct an extensive experiments on GPT-4, a series of Llama and other popular LLMs. The experiment results demonstrate a significant performance gap between LLMs and humans in temporal-relative capability. Furthermore, the error types of temporal-relative ability in LLMs are proposed to thoroughly analyze the impact of multiple aspects and emphasize the associated challenges. We anticipate that TempBench will drive further advancements in enhancing the temporal-perceiving capabilities of L",
        "author": "Shuang Chen; Yining Zheng; Shimin Li; Qinyuan Cheng; Xipeng Qiu",
        "authorids": "/s/shuang-chen/; /y/yining-zheng/; /s/shimin-li/; /q/qinyuan-cheng/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{chen-etal-2025-perceive,\n    title = \"Perceive the Passage of Time: A Systematic Evaluation of Large Language Model in Temporal Relativity\",\n    author = \"Chen, Shuang  and\n      Zheng, Yining  and\n      Li, Shimin  and\n      Cheng, Qinyuan  and\n      Qiu, Xipeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.554/\",\n    pages = \"8304--8313\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.554.pdf",
        "site": "https://aclanthology.org/2025.coling-main.554/",
        "pdf_size": 1001414,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LP8NwbnO6mUJ:scholar.google.com/&scioq=Perceive+the+Passage+of+Time:+A+Systematic+Evaluation+of+Large+Language+Model+in+Temporal+Relativity&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",
        "aff_domain": "fudan.edu.cn; ; ; ;fudan.edu.cn",
        "email": "fudan.edu.cn; ; ; ;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.369",
        "title": "Persona-Consistent Dialogue Generation via Pseudo Preference Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a simple yet effective method for enhancing persona consistency in dialogue response generation using Direct Preference Optimization (DPO). In our method, we generate responses from the response generation model using persona information that has been randomly swapped with data from other dialogues, treating these responses as pseudo-negative samples. The reference responses serve as positive samples, allowing us to create pseudo-preference data. Experimental results demonstrate that our model, fine-tuned with DPO on the pseudo preference data, produces more consistent and natural responses compared to models trained using supervised fine-tuning or reinforcement learning approaches based on entailment relations between personas and utterances.",
        "author": "Junya Takayama; Masaya Ohagi; Tomoya Mizumoto; Katsumasa Yoshikawa",
        "authorids": "/j/junya-takayama/; /m/masaya-ohagi/; /t/tomoya-mizumoto/; /k/katsumasa-yoshikawa/",
        "bibtex": "@inproceedings{takayama-etal-2025-persona,\n    title = \"Persona-Consistent Dialogue Generation via Pseudo Preference Tuning\",\n    author = \"Takayama, Junya  and\n      Ohagi, Masaya  and\n      Mizumoto, Tomoya  and\n      Yoshikawa, Katsumasa\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.369/\",\n    pages = \"5507--5514\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.369.pdf",
        "site": "https://aclanthology.org/2025.coling-main.369/",
        "pdf_size": 872722,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7833994393748861101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "SB Intuitions Corp.; SB Intuitions Corp.; SB Intuitions Corp.; SB Intuitions Corp.",
        "aff_domain": "sbintuitions.co.jp;sbintuitions.co.jp;sbintuitions.co.jp;sbintuitions.co.jp",
        "email": "sbintuitions.co.jp;sbintuitions.co.jp;sbintuitions.co.jp;sbintuitions.co.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "SB Intuitions Corp.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2025.coling-main.20",
        "title": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the evaluation of response prediction, Persona-DB demonstrates superior context efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 10% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands.",
        "author": "Chenkai Sun; Ke Yang; Revanth Gangi Reddy; Yi Fung; Hou Pong Chan; Kevin Small; ChengXiang Zhai; Heng Ji",
        "authorids": "/c/chenkai-sun/; /k/ke-yang/; /r/revanth-gangi-reddy/; /y/yi-fung/; /h/hou-pong-chan/; /k/kevin-small/; /c/chengxiang-zhai/; /h/heng-ji/",
        "bibtex": "@inproceedings{sun-etal-2025-persona,\n    title = \"Persona-{DB}: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement\",\n    author = \"Sun, Chenkai  and\n      Yang, Ke  and\n      Gangi Reddy, Revanth  and\n      Fung, Yi  and\n      Chan, Hou Pong  and\n      Small, Kevin  and\n      Zhai, ChengXiang  and\n      Ji, Heng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.20/\",\n    pages = \"281--296\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.20.pdf",
        "site": "https://aclanthology.org/2025.coling-main.20/",
        "pdf_size": 2864056,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16389150797278699713&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; Amazon; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu; ; ; ; ; ;illinois.edu;illinois.edu",
        "email": "illinois.edu; ; ; ; ; ;illinois.edu;illinois.edu",
        "github": "https://github.com/chenkaisun/Persona-DB",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;1;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.amazon.com",
        "aff_unique_abbr": "UIUC;Amazon",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.254",
        "title": "Personalized Large Language Model Assistant with Evolving Conditional Memory",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid development of large language models, AI assistants like ChatGPT have become increasingly integrated into people\u2019s works and lives but are limited in personalized services. In this paper, we present a plug-and-play framework that could facilitate personalized large language model assistants with evolving conditional memory. The personalized assistant focuses on intelligently preserving the knowledge and experience from the history dialogue with the user, which can be applied to future tailored responses that better align with the user\u2019s preferences. Generally, the assistant generates a set of records from the dialogue, stores them in a memory bank, and retrieves related memory to improve the quality of the response. For the crucial memory design, we explore different ways of constructing the memory and propose a new memorizing mechanism named conditional memory to enhance the memory management of the framework. We also investigate the retrieval and usage of memory in the generation process. To better evaluate the personalized assistants\u2019 abilities, we build the first evaluation benchmark from three critical aspects: continuing previous dialogue, learning personalized knowledge and learning from user feedback. The experimental results illustrate the effectiveness of our method.",
        "author": "Ruifeng Yuan; Shichao Sun; Yongqi Li; Zili Wang; Ziqiang Cao; Wenjie Li",
        "authorids": "/r/ruifeng-yuan/; /s/shichao-sun/; /y/yongqi-li-hk/; /z/zili-wang/; /z/ziqiang-cao/; /w/wenjie-li/",
        "bibtex": "@inproceedings{yuan-etal-2025-personalized,\n    title = \"Personalized Large Language Model Assistant with Evolving Conditional Memory\",\n    author = \"Yuan, Ruifeng  and\n      Sun, Shichao  and\n      Li, Yongqi  and\n      Wang, Zili  and\n      Cao, Ziqiang  and\n      Li, Wenjie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.254/\",\n    pages = \"3764--3777\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.254.pdf",
        "site": "https://aclanthology.org/2025.coling-main.254/",
        "pdf_size": 548402,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13941480009703971391&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; ; Soochow University; The Hong Kong Polytechnic University",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "The Hong Kong Polytechnic University;Soochow University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.polyu.edu.hk;https://www.soochow.edu.cn",
        "aff_unique_abbr": "PolyU;Soochow U",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.44",
        "title": "Perturbation-driven Dual Auxiliary Contrastive Learning for Collaborative Filtering Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Graph collaborative filtering has made great progress in the recommender systems, but these methods often struggle with the data sparsity issue in real-world recommendation scenarios. To mitigate the effect of data sparsity, graph collaborative filtering incorporates contrastive learning as an auxiliary task to improve model performance. However, existing contrastive learning-based methods generally use a single data augmentation graph to construct the auxiliary contrastive learning task, which has problems such as loss of key information and low robustness. To address these problems, this paper proposes a Perturbation-driven Dual Auxiliary Contrastive Learning for Collaborative Filtering Recommendation (PDACL). PDACL designs structure perturbation and weight perturbation to construct two data augmentation graphs. The Structure Perturbation Augmentation (SPA) graph perturbs the topology of the user-item interaction graph, while the Weight Perturbation Augmentation (WPA) graph reconstructs the implicit feedback unweighted graph into a weighted graph similar to the explicit feedback. These two data augmentation graphs are combined with the user-item interaction graph to construct the dual auxiliary contrastive learning task to extract the self-supervised signals without losing key information and jointly optimize it together with the supervised recommendation task, to alleviate the data sparsity problem and improve the performance. Experimental results on multiple public datasets show that PDACL outperforms numerous benchmark models, demonstrating that the dual-perturbation data augmentation graph in PDACL can overcome the shortcomings of a single data augmentation graph, leading to superior recommendation results. The implementation of our work will be found at https://github.com/zky77/PDACL.",
        "author": "Caihong Mu; Keyang Zhang; Jialiang Zhou; Yi Liu",
        "authorids": "/c/caihong-mu/; /k/keyang-zhang/; /j/jialiang-zhou/; /y/yi-liu/",
        "bibtex": "@inproceedings{mu-etal-2025-perturbation,\n    title = \"Perturbation-driven Dual Auxiliary Contrastive Learning for Collaborative Filtering Recommendation\",\n    author = \"Mu, Caihong  and\n      Zhang, Keyang  and\n      Zhou, Jialiang  and\n      Liu, Yi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.44/\",\n    pages = \"647--657\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.44.pdf",
        "site": "https://aclanthology.org/2025.coling-main.44/",
        "pdf_size": 2121417,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xh82zP5xr7MJ:scholar.google.com/&scioq=Perturbation-driven+Dual+Auxiliary+Contrastive+Learning+for+Collaborative+Filtering+Recommendation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, Xidian University; School of Artificial Intelligence, Xidian University; Guangzhou Institute of Technology, Xidian University; School of Electronic Engineering, Xidian University",
        "aff_domain": "foxmail.com;stu.xidian.edu.cn;stu.xidian.edu.cn;foxmail.com",
        "email": "foxmail.com;stu.xidian.edu.cn;stu.xidian.edu.cn;foxmail.com",
        "github": "https://github.com/zky77/PDACL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xidian University",
        "aff_unique_dep": "School of Artificial Intelligence",
        "aff_unique_url": "http://www.xidian.edu.cn/",
        "aff_unique_abbr": "Xidian",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Guangzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.747",
        "title": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Physics problems constitute a significant aspect of reasoning, necessitating complicated reasoning ability and abundant physics knowledge. However, existing large language models (LLMs) frequently fail due to a lack of knowledge or incorrect knowledge application. To mitigate these issues, we propose Physics Reasoner, a knowledge-augmented framework to solve physics problems with LLMs. Specifically, the proposed framework constructs a comprehensive formula set to provide explicit physics knowledge and utilizes checklists containing detailed instructions to guide effective knowledge application. Namely, given a physics problem, Physics Reasoner solves it through three stages: problem analysis, formula retrieval, and guided reasoning. During the process, checklists are employed to enhance LLMs\u2019 self-improvement in the analysis and reasoning stages. Empirically, Physics Reasoner mitigates the issues of insufficient knowledge and incorrect application, achieving state-of-the-art performance on SciBench with an average accuracy improvement of 5.8%.",
        "author": "Xinyu Pang; Ruixin Hong; Zhanke Zhou; Fangrui Lv; Xinwei Yang; Zhilong Liang; Bo Han; Changshui Zhang",
        "authorids": "/x/xinyu-pang/; /r/ruixin-hong/; /z/zhanke-zhou/; /f/fangrui-lv/; /x/xinwei-yang/; /z/zhilong-liang/; /b/bo-han/; /c/changshui-zhang/",
        "bibtex": "@inproceedings{pang-etal-2025-physics,\n    title = \"Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models\",\n    author = \"Pang, Xinyu  and\n      Hong, Ruixin  and\n      Zhou, Zhanke  and\n      Lv, Fangrui  and\n      Yang, Xinwei  and\n      Liang, Zhilong  and\n      Han, Bo  and\n      Zhang, Changshui\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.747/\",\n    pages = \"11274--11289\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.747.pdf",
        "site": "https://aclanthology.org/2025.coling-main.747/",
        "pdf_size": 969284,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16963658621405866009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute for Artificial Intelligence, Tsinghua University (THUAI)+Beijing National Research Center for Information Science and Technology (BNRist)+Department of Automation, Tsinghua University, Beijing, P.R.China; Institute for Artificial Intelligence, Tsinghua University (THUAI)+Beijing National Research Center for Information Science and Technology (BNRist)+Department of Automation, Tsinghua University, Beijing, P.R.China; TMLR Group, Hong Kong Baptist University; Institute for Artificial Intelligence, Tsinghua University (THUAI)+Beijing National Research Center for Information Science and Technology (BNRist)+Department of Automation, Tsinghua University, Beijing, P.R.China; Institute for Artificial Intelligence, Tsinghua University (THUAI)+Beijing National Research Center for Information Science and Technology (BNRist)+Department of Automation, Tsinghua University, Beijing, P.R.China; Institute for Artificial Intelligence, Tsinghua University (THUAI)+Beijing National Research Center for Information Science and Technology (BNRist)+Department of Automation, Tsinghua University, Beijing, P.R.China; TMLR Group, Hong Kong Baptist University; Institute for Artificial Intelligence, Tsinghua University (THUAI)+Beijing National Research Center for Information Science and Technology (BNRist)+Department of Automation, Tsinghua University, Beijing, P.R.China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;comp.hkbu.edu.hk;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;comp.hkbu.edu.hk;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;comp.hkbu.edu.hk;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;comp.hkbu.edu.hk;mail.tsinghua.edu.cn",
        "github": "https://github.com/Xinyu-Pang/Physics_Reasoner",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1+0;0+1+0;2;0+1+0;0+1+0;0+1+0;2;0+1+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Hong Kong Baptist University",
        "aff_unique_dep": "Institute for Artificial Intelligence;;TMLR Group",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.hkbu.edu.hk",
        "aff_unique_abbr": "THU;BNRist;",
        "aff_campus_unique_index": "1;1;2;1;1;1;2;1",
        "aff_campus_unique": ";Beijing;Hong Kong SAR",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0;0+0+0;0+0+0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.498",
        "title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing claim verification datasets often do not require systems to perform complex reasoning or effectively interpret multimodal evidence. To address this, we introduce a new task: multi-hop multimodal claim verification. This task challenges models to reason over multiple pieces of evidence from diverse sources, including text, images, and tables, and determine whether the combined multimodal evidence supports or refutes a given claim. To study this task, we construct MMCV, a large-scale dataset comprising 15k multi-hop claims paired with multimodal evidence, generated and refined using large language models, with additional input from human feedback. We show that MMCV is challenging even for the latest state-of-the-art multimodal large language models, especially as the number of reasoning hops increases. Additionally, we establish a human performance benchmark on a subset of MMCV. We hope this dataset and its evaluation task will encourage future research in multimodal multi-hop claim verification.",
        "author": "Haoran Wang; Aman Rangapur; Xiongxiao Xu; Yueqing Liang; Haroon Gharwi; Carl Yang; Kai Shu",
        "authorids": "/h/haoran-wang/; /a/aman-rangapur/; /x/xiongxiao-xu/; /y/yueqing-liang/; /h/haroon-gharwi/; /c/carl-yang/; /k/kai-shu/",
        "bibtex": "@inproceedings{wang-etal-2025-piecing,\n    title = \"Piecing It All Together: Verifying Multi-Hop Multimodal Claims\",\n    author = \"Wang, Haoran  and\n      Rangapur, Aman  and\n      Xu, Xiongxiao  and\n      Liang, Yueqing  and\n      Gharwi, Haroon  and\n      Yang, Carl  and\n      Shu, Kai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.498/\",\n    pages = \"7453--7469\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.498.pdf",
        "site": "https://aclanthology.org/2025.coling-main.498/",
        "pdf_size": 2803642,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3842173606015302024&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Illinois Institute of Technology; Illinois Institute of Technology; Illinois Institute of Technology; Illinois Institute of Technology; Illinois Institute of Technology; Emory University; Emory University",
        "aff_domain": "hawk.iit.edu;hawk.iit.edu;hawk.iit.edu;hawk.iit.edu;hawk.iit.edu;emory.edu;emory.edu",
        "email": "hawk.iit.edu;hawk.iit.edu;hawk.iit.edu;hawk.iit.edu;hawk.iit.edu;emory.edu;emory.edu",
        "github": "",
        "project": "https://mmcv-dataset.github.io/",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;1",
        "aff_unique_norm": "Illinois Institute of Technology;Emory University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iit.edu;https://www.emory.edu",
        "aff_unique_abbr": "IIT;Emory",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.672",
        "title": "Planning with Multi-Constraints via Collaborative Language Agents",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks with multiple constraints at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Planning with Multi-Constraints (PMC), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning with constraints by decomposing it into a hierarchy of subordinate tasks. Each subtask is then mapped into executable actions. PMC was assessed on two constraint-intensive benchmarks, TravelPlanner and API-Bank. Notably, PMC achieved an average 42.68% success rate on TravelPlanner, significantly higher than GPT-4 (2.92%), and outperforming GPT-4 with ReAct on API-Bank by 13.64%, showing the immense potential of integrating LLM with multi-agent systems. We also show that PMC works with small LLM as the planning core, e.g., LLaMA-3.1-8B.",
        "author": "Cong Zhang; Xin Deik Goh; Dexun Li; Hao Zhang; Yong Liu",
        "authorids": "/c/cong-zhang/; /x/xin-deik-goh/; /d/dexun-li/; /h/hao-zhang/; /y/yong-liu/",
        "bibtex": "@inproceedings{zhang-etal-2025-planning,\n    title = \"Planning with Multi-Constraints via Collaborative Language Agents\",\n    author = \"Zhang, Cong  and\n      Goh, Xin Deik  and\n      Li, Dexun  and\n      Zhang, Hao  and\n      Liu, Yong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.672/\",\n    pages = \"10054--10082\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.672.pdf",
        "site": "https://aclanthology.org/2025.coling-main.672/",
        "pdf_size": 1424114,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9178290831940866445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "gmail.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "gmail.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "https://github.com/zcaicaros/PMC",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.huawei.com",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.5",
        "title": "PoemBERT: A Dynamic Masking Content and Ratio Based Semantic Language Model For Chinese Poem Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Ancient Chinese poetry stands as a crucial treasure in Chinese culture. To address the absence of pre-trained models for ancient poetry, we introduced PoemBERT, a BERT-based model utilizing a corpus of classical Chinese poetry. Recognizing the unique emotional depth and linguistic precision of poetry, we incorporated sentiment and pinyin embeddings into the model, enhancing its sensitivity to emotional information and addressing challenges posed by the phenomenon of multiple pronunciations for the same Chinese character. Additionally, we proposed Character Importance-based masking and dynamic masking strategies, significantly augmenting the model\u2019s capability to extract imagery-related features and handle poetry-specific information. Fine-tuning our PoemBERT model on various downstream tasks, including poem generation and sentiment classification, resulted in state-of-the-art performance in both automatic and manual evaluations. We provided explanations for the selection of the dynamic masking rate strategy and proposed a solution to the issue of a small dataset size.",
        "author": "Chihan Huang; Xiaobo Shen",
        "authorids": "/c/chihan-huang/; /x/xiaobo-shen/",
        "bibtex": "@inproceedings{huang-shen-2025-poembert,\n    title = \"{P}oem{BERT}: A Dynamic Masking Content and Ratio Based Semantic Language Model For {C}hinese Poem Generation\",\n    author = \"Huang, Chihan  and\n      Shen, Xiaobo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.5/\",\n    pages = \"50--60\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.5.pdf",
        "site": "https://aclanthology.org/2025.coling-main.5/",
        "pdf_size": 1906905,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wfdyuIA6zH4J:scholar.google.com/&scioq=PoemBERT:+A+Dynamic+Masking+Content+and+Ratio+Based+Semantic+Language+Model+For+Chinese+Poem+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China",
        "aff_domain": "njust.edu.cn;gmail.com",
        "email": "njust.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanjing University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.nust.edu.cn/",
        "aff_unique_abbr": "NUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.620",
        "title": "Poetry in Pixels: Prompt Tuning for Poem Image Generation via Diffusion Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of text-to-image generation has encountered significant challenges when applied to literary works, especially poetry. Poems are a distinct form of literature, with meanings that frequently transcend beyond the literal words. To address this shortcoming, we propose a PoemToPixel framework designed to generate images that visually represent the inherent meanings of poems. Our approach incorporates the concept of prompt tuning in our image generation framework to ensure that the resulting images closely align with the poetic content. In addition, we propose the PoeKey algorithm, which extracts three key elements in the form of emotions, visual elements, and themes from poems to form instructions which are subsequently provided to a diffusion model for generating corresponding images. Furthermore, to expand the diversity of the poetry dataset across different genres and ages, we introduce MiniPo, a novel multimodal dataset comprising 1001 children\u2019s poems and images. Leveraging this dataset alongside PoemSum, we conducted both quantitative and qualitative evaluations of image generation using our PoemToPixel framework. This paper demonstrates the effectiveness of our approach and offers a fresh perspective on generating images from literary sources. The code and dataset used in this work are publicly available.",
        "author": "Sofia Jamil; Bollampalli Areen Reddy; Raghvendra Kumar; Sriparna Saha; Joseph K. J; Koustava Goswami",
        "authorids": "/s/sofia-jamil/; /b/bollampalli-areen-reddy/; /r/raghvendra-kumar/; /s/sriparna-saha/; /j/joseph-k-j/; /k/koustava-goswami/",
        "bibtex": "@inproceedings{jamil-etal-2025-poetry,\n    title = \"Poetry in Pixels: Prompt Tuning for Poem Image Generation via Diffusion Models\",\n    author = \"Jamil, Sofia  and\n      Reddy, Bollampalli Areen  and\n      Kumar, Raghvendra  and\n      Saha, Sriparna  and\n      J, Joseph K.  and\n      Goswami, Koustava\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.620/\",\n    pages = \"9224--9237\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.620.pdf",
        "site": "https://aclanthology.org/2025.coling-main.620/",
        "pdf_size": 2694026,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UBwdLqe-bLIJ:scholar.google.com/&scioq=Poetry+in+Pixels:+Prompt+Tuning+for+Poem+Image+Generation+via+Diffusion+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Department of Computer Science & Engineering, Indian Institute of Technology Patna, India; Department of Computer Science & Engineering, Indian Institute of Technology Patna, India; Department of Computer Science & Engineering, Indian Institute of Technology Patna, India; Department of Computer Science & Engineering, Indian Institute of Technology Patna, India; Adobe Research, India; Adobe Research, India",
        "aff_domain": "iitp.ac.in;iitp.ac.in;iitp.ac.in;iitp.ac.in;adobe.com;adobe.com",
        "email": "iitp.ac.in;iitp.ac.in;iitp.ac.in;iitp.ac.in;adobe.com;adobe.com",
        "github": "https://github.com/SofeeyaJ/Poetry-In-Pixels-Coling2025",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Indian Institute of Technology Patna;Adobe Research",
        "aff_unique_dep": "Department of Computer Science & Engineering;",
        "aff_unique_url": "https://www.iitp.ac.in;https://research.adobe.com",
        "aff_unique_abbr": "IIT Patna;Adobe",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Patna;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-demos.1",
        "title": "PolyMinder: A Support System for Entity Annotation and Relation Extraction in Polymer Science Documents",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The growing volume of scientific literature in polymer science presents a significant challenge for researchers attempting to extract and annotate domain-specific entities, such as polymer names, material properties, and related information. Manual annotation of these documents is both time-consuming and prone to error due to the complexity of scientific language. To address this, we introduce PolyMinder, an automated support system designed to assist polymer scientists in extracting and annotating polymer-related entities and their relationships from scientific documents. The system utilizes recent advanced Named Entity Recognition (NER) and Relation Extraction (RE) models tailored to the polymer domain. PolyMinder streamlines the annotation process by providing a web-based interface where users can visualize, verify, and refine the extracted information before finalizing the annotations. The system\u2019s source code is made publicly available to facilitate further research and development in this field. Our system can be accessed through the following URL: https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/polyminder",
        "author": "Truong Dinh Do; An Hoang Trieu; Van-Thuy Phi; Minh Le Nguyen; Yuji Matsumoto",
        "authorids": "/t/truong-dinh-do/; /a/an-hoang-trieu/; /v/van-thuy-phi/; /m/minh-le-nguyen/; /y/yuji-matsumoto/",
        "bibtex": "@inproceedings{do-etal-2025-polyminder,\n    title = \"{P}oly{M}inder: A Support System for Entity Annotation and Relation Extraction in Polymer Science Documents\",\n    author = \"Do, Truong Dinh  and\n      Trieu, An Hoang  and\n      Phi, Van-Thuy  and\n      Nguyen, Minh Le  and\n      Matsumoto, Yuji\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.1/\",\n    pages = \"1--8\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.1.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.1/",
        "pdf_size": 1712083,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Pj39G9t3UqkJ:scholar.google.com/&scioq=PolyMinder:+A+Support+System+for+Entity+Annotation+and+Relation+Extraction+in+Polymer+Science+Documents&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Japan Advanced Institute of Science and Technology, Ishikawa, Japan+RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan+RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan",
        "aff_domain": "jaist.ac.jp;jaist.ac.jp;riken.jp;jaist.ac.jp;riken.jp",
        "email": "jaist.ac.jp;jaist.ac.jp;riken.jp;jaist.ac.jp;riken.jp",
        "github": "",
        "project": "https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/polyminder",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;0;1",
        "aff_unique_norm": "Japan Advanced Institute of Science and Technology;RIKEN Center for Advanced Intelligence Project",
        "aff_unique_dep": ";Center for Advanced Intelligence Project",
        "aff_unique_url": "https://www.jaist.ac.jp;https://www.riken.jp/en/c-aip/",
        "aff_unique_abbr": "JAIST;RIKEN C-AIP",
        "aff_campus_unique_index": "0+1;0+1;1;0;1",
        "aff_campus_unique": "Ishikawa;Tokyo",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.105",
        "title": "Polysemy Interpretation and Transformer Language Models: A Case of Korean Adverbial Postposition -(u)lo",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study examines how Transformer language models utilise lexico-phrasal information to interpret the polysemy of the Korean adverbial postposition -(u)lo. We analysed the attention weights of both a Korean pre-trained BERT model and a fine-tuned version. Results show a general reduction in attention weights following fine-tuning, alongside changes in the lexico-phrasal information used, depending on the specific function of -(u)lo. These findings suggest that, while fine-tuning broadly affects a model\u2019s syntactic sensitivity, it may also alter its capacity to leverage lexico-phrasal features according to the function of the target word.",
        "author": "Seongmin Mun; Gyu-Ho Shin",
        "authorids": "/s/seongmin-mun/; /g/gyu-ho-shin/",
        "bibtex": "@inproceedings{mun-shin-2025-polysemy,\n    title = \"Polysemy Interpretation and Transformer Language Models: A Case of {K}orean Adverbial Postposition -(u)lo\",\n    author = \"Mun, Seongmin  and\n      Shin, Gyu-Ho\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.105/\",\n    pages = \"1555--1561\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.105.pdf",
        "site": "https://aclanthology.org/2025.coling-main.105/",
        "pdf_size": 907532,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AG88a-UsYbEJ:scholar.google.com/&scioq=Polysemy+Interpretation+and+Transformer+Language+Models:+A+Case+of+Korean+Adverbial+Postposition+-(u)lo&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Humanities Research Institute, Ajou University, Suwon-si, Gyeonggi-do, South Korea; Department of Linguistics, University of Illinois Chicago, Chicago, IL, USA",
        "aff_domain": "ajou.ac.kr;uic.edu",
        "email": "ajou.ac.kr;uic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ajou University;University of Illinois Chicago",
        "aff_unique_dep": "Humanities Research Institute;Department of Linguistics",
        "aff_unique_url": "http://www.ajou.ac.kr;https://www.uic.edu",
        "aff_unique_abbr": "Ajou;UIC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Suwon-si;Chicago",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2025.coling-main.632",
        "title": "Position Information Emerges in Causal Transformers Without Positional Encodings via Similarity of Nearby Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Transformers with causal attention can solve tasks that require positional information without using positional encodings. In this work, we propose and investigate a new hypothesis about how positional information can be stored without using explicit positional encoding. We observe that nearby embeddings are more similar to each other than faraway embeddings, allowing the transformer to potentially reconstruct the positions of tokens. We show that this pattern can occur in both the trained and the randomly initialized Transformer models with causal attention and no positional encodings over a common range of hyperparameters.",
        "author": "Chunsheng Zuo; Pavel Guerzhoy; Michael Guerzhoy",
        "authorids": "/c/chunsheng-zuo/; /p/pavel-guerzhoy/; /m/michael-guerzhoy/",
        "bibtex": "@inproceedings{zuo-etal-2025-position,\n    title = \"Position Information Emerges in Causal Transformers Without Positional Encodings via Similarity of Nearby Embeddings\",\n    author = \"Zuo, Chunsheng  and\n      Guerzhoy, Pavel  and\n      Guerzhoy, Michael\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.632/\",\n    pages = \"9418--9430\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.632.pdf",
        "site": "https://aclanthology.org/2025.coling-main.632/",
        "pdf_size": 1991261,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3588527873113427915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of Computer Science, Johns Hopkins University; Dept. of Mathematics, University of Hawai \u2018i at M \u00afanoa; Division of Engineering Science, University of Toronto",
        "aff_domain": "jh.edu;math.hawaii.edu;cs.toronto.edu",
        "email": "jh.edu;math.hawaii.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Johns Hopkins University;University of Hawai\u2018i at M\u0101noa;University of Toronto",
        "aff_unique_dep": "Dept. of Computer Science;Department of Mathematics;Division of Engineering Science",
        "aff_unique_url": "https://www.jhu.edu;https://www.hawaii.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "JHU;UH M\u0101noa;U of T",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";M\u0101noa",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2025.coling-main.29",
        "title": "Positive Text Reframing under Multi-strategy Optimization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Differing from sentiment transfer, positive reframing seeks to substitute negative perspectives with positive expressions while preserving the original meaning. With the emergence of pre-trained language models (PLMs), it is possible to achieve acceptable results by fine-tuning PLMs. Nevertheless, generating fluent, diverse and task-constrained reframing text remains a significant challenge. To tackle this issue, a **m**ulti-**s**trategy **o**ptimization **f**ramework (MSOF) is proposed in this paper. Starting from the objective of positive reframing, we first design positive sentiment reward and content preservation reward to encourage the model to transform the negative expressions of the original text while ensuring the integrity and consistency of the semantics. Then, different decoding optimization approaches are introduced to improve the quality of text generation. Finally, based on the modeling formula of positive reframing, we propose a multi-dimensional re-ranking method that further selects candidate sentences from three dimensions: strategy consistency, text similarity and fluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate our framework achieves significant improvements on unconstrained and controlled positive reframing tasks.",
        "author": "Shutong Jia; Biwei Cao; Qingqing Gao; Jiuxin Cao; Bo Liu",
        "authorids": "/s/shutong-jia/; /b/biwei-cao/; /q/qingqing-gao/; /j/jiuxin-cao/; /b/bo-liu/",
        "bibtex": "@inproceedings{jia-etal-2025-positive,\n    title = \"Positive Text Reframing under Multi-strategy Optimization\",\n    author = \"Jia, Shutong  and\n      Cao, Biwei  and\n      Gao, Qingqing  and\n      Cao, Jiuxin  and\n      Liu, Bo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.29/\",\n    pages = \"429--447\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.29.pdf",
        "site": "https://aclanthology.org/2025.coling-main.29/",
        "pdf_size": 1741989,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16363791201274544042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Southeast University+State Grid Tianjin Power Dongli Power Supply Branch; Southeast University; Southeast University; Southeast University; Southeast University",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "Southeast University;State Grid Corporation of China",
        "aff_unique_dep": ";Tianjin Power Dongli Power Supply Branch",
        "aff_unique_url": "https://www.seu.edu.cn/;http://www.sgcc.com.cn",
        "aff_unique_abbr": "SEU;SGCC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.364",
        "title": "Post-Hoc Watermarking for Robust Detection in Text Generated by Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Research on text simplification has been ongoing for many years, yet document simplification remains a significant challenge due to the need to address complex factors such as technical terminology, metaphors, and overall coherence. In this work, we introduce a novel multi-agent framework AgentSimp for document simplification, based on large language models. This framework simulates the collaborative efforts of a team of human experts through the roles played by multiple agents, effectively meeting the intricate demands of document simplification. We investigate two communication strategies among agents (pipeline-style and synchronous) and two document reconstruction strategies (Direct and Iterative). According to both automatic evaluation metrics and human evaluation results, AgentSimp produces simplified documents that are more thoroughly simplified and more coherent across various articles and styles.",
        "author": "Jifei Hao; Jipeng Qiang; Yi Zhu; Yun Li; Yunhao Yuan; Xiaoye Ouyang",
        "authorids": "/j/jifei-hao/; /j/jipeng-qiang/; /y/yi-zhu/; /y/yun-li/; /y/yunhao-yuan/; /x/xiaoye-ouyang/",
        "bibtex": "@inproceedings{hao-etal-2025-post,\n    title = \"Post-Hoc Watermarking for Robust Detection in Text Generated by Large Language Models\",\n    author = \"Hao, Jifei  and\n      Qiang, Jipeng  and\n      Zhu, Yi  and\n      Li, Yun  and\n      Yuan, Yunhao  and\n      Ouyang, Xiaoye\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.364/\",\n    pages = \"5430--5442\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.364.pdf",
        "site": "https://aclanthology.org/2025.coling-main.364/",
        "pdf_size": 2273222,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13096464349663655791&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; School of Information and Engineering, Yangzhou University, Jiangsu, China; China Academy of Electronic and Information Technology, Beijing, China",
        "aff_domain": "gmail.com;yzu.edu.cn;yzu.edu.cn;yzu.edu.cn;yzu.edu.cn;cetc.com.cn",
        "email": "gmail.com;yzu.edu.cn;yzu.edu.cn;yzu.edu.cn;yzu.edu.cn;cetc.com.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Yangzhou University;China Academy of Electronic and Information Technology",
        "aff_unique_dep": "School of Information and Engineering;",
        "aff_unique_url": "http://www.yzu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.87",
        "title": "PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This work introduces PrahokBART, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.",
        "author": "Hour Kaing; Raj Dabre; Haiyue Song; Van-Hien Tran; Hideki Tanaka; Masao Utiyama",
        "authorids": "/h/hour-kaing/; /r/raj-dabre/; /h/haiyue-song/; /v/van-hien-tran/; /h/hideki-tanaka/; /m/masao-utiyama/",
        "bibtex": "@inproceedings{kaing-etal-2025-prahokbart,\n    title = \"{P}rahok{BART}: A Pre-trained Sequence-to-Sequence Model for {K}hmer Natural Language Generation\",\n    author = \"Kaing, Hour  and\n      Dabre, Raj  and\n      Song, Haiyue  and\n      Tran, Van-Hien  and\n      Tanaka, Hideki  and\n      Utiyama, Masao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.87/\",\n    pages = \"1309--1322\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.87.pdf",
        "site": "https://aclanthology.org/2025.coling-main.87/",
        "pdf_size": 1543042,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:H7TQdMqk2aAJ:scholar.google.com/&scioq=PrahokBART:+A+Pre-trained+Sequence-to-Sequence+Model+for+Khmer+Natural+Language+Generation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan",
        "aff_domain": "nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp",
        "email": "nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp",
        "github": "https://github.com/hour/prahokbart",
        "project": "https://huggingface.co/prajdabre/prahokbart",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "National Institute of Information and Communications Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nict.go.jp/",
        "aff_unique_abbr": "NICT",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.54",
        "title": "Pre-trained Semantic Interaction based Inductive Graph Neural Networks for Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nowadays, research of Text Classification (TC) based on graph neural networks (GNNs) is on the rise. Both inductive methods and transductive methods have made significant progress. For transductive methods, the semantic interaction between texts plays a crucial role in the learning of effective text representations. However, it is difficult to perform inductive learning while modeling interactions between texts on the graph. To give a universal solution, we propose the graph neural network based on pre-trained semantic interaction called PaSIG. Firstly, we construct a text-word heterogeneity graph and design an asymmetric structure to ensure one-way message passing from words to the test texts. Meanwhile, we use the context representation capability of the pre-trained language model to construct node features that contain classification semantic information. Afterward, we explore the adaptative aggregation methods with a gated fusion mechanism. Extensive experiments on five datasets have shown the effectiveness of PaSIG, with the accuracy exceeding the baseline by 2.7% on average. While achieving state-of-the-art performance, we have also taken measures of subgraph sampling and intermediate state preservation to achieve fast inference.",
        "author": "Shiyu Wang; Gang Zhou; Jicang Lu; Jing Chen; Ningbo Huang",
        "authorids": "/s/shiyu-wang/; /g/gang-zhou/; /j/jicang-lu/; /j/jing-chen/; /n/ningbo-huang/",
        "bibtex": "@inproceedings{wang-etal-2025-pre,\n    title = \"Pre-trained Semantic Interaction based Inductive Graph Neural Networks for Text Classification\",\n    author = \"Wang, Shiyu  and\n      Zhou, Gang  and\n      Lu, Jicang  and\n      Chen, Jing  and\n      Huang, Ningbo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.54/\",\n    pages = \"812--827\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.54.pdf",
        "site": "https://aclanthology.org/2025.coling-main.54/",
        "pdf_size": 1482115,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DY6Qp8Ao4SIJ:scholar.google.com/&scioq=Pre-trained+Semantic+Interaction+based+Inductive+Graph+Neural+Networks+for+Text+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Mathematical Engineering and Advanced Computing, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, China",
        "aff_domain": "163.com;126.com;sina.com;126.com;163.com",
        "email": "163.com;126.com;sina.com;126.com;163.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "State Key Laboratory of Mathematical Engineering and Advanced Computing",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.1",
        "title": "PreAct: Prediction Enhances Agent\u2019s Planning Ability",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Addressing the disparity between predictions and actual results can enable individuals to expand their thought processes and stimulate self-reflection, thus promoting accurate planning. In this research, we present **PreAct**, an agent framework that integrates **pre**diction, **rea**soning, and **act**ion. By utilizing the information derived from predictions, the large language model (LLM) agent can provide a wider range and more strategically focused reasoning. This leads to more efficient actions that aid the agent in accomplishing intricate tasks. Our experimental results show that PreAct surpasses the ReAct method in completing complex tasks and that PreAct\u2019s performance can be further improved when paired with other memory or selection strategy techniques. We presented the model with varying quantities of historical predictions and discovered that these predictions consistently enhance LLM planning. The variances in single-step reasoning between PreAct and ReAct indicate that PreAct indeed has benefits in terms of diversity and strategic orientation over ReAct.",
        "author": "Dayuan Fu; Jianzhao Huang; Siyuan Lu; Guanting Dong; Yejie Wang; Keqing He; Weiran Xu",
        "authorids": "/d/dayuan-fu/; /j/jianzhao-huang/; /s/siyuan-lu/; /g/guanting-dong/; /y/yejie-wang/; /k/keqing-he/; /w/weiran-xu/",
        "bibtex": "@inproceedings{fu-etal-2025-preact,\n    title = \"{P}re{A}ct: Prediction Enhances Agent{'}s Planning Ability\",\n    author = \"Fu, Dayuan  and\n      Huang, Jianzhao  and\n      Lu, Siyuan  and\n      Dong, Guanting  and\n      Wang, Yejie  and\n      He, Keqing  and\n      Xu, Weiran\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.1/\",\n    pages = \"1--16\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.1.pdf",
        "site": "https://aclanthology.org/2025.coling-main.1/",
        "pdf_size": 5106525,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14288438389230874694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Meituan, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn",
        "github": "https://github.com/Fu-Dayuan/PreAct",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BUPT;Meituan",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.17",
        "title": "Predicting Fine-tuned Performance on Larger Datasets Before Creating Them",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "This paper proposes a method to estimate the performance of pretrained models fine-tuned with a larger dataset from the result with a smaller dataset. Specifically, we demonstrate that when a pretrained model is fine-tuned, its classification performance increases at the same overall rate, regardless of the original dataset size, as the number of epochs increases. Subsequently, we verify that an approximate formula based on this trend can be used to predict the performance when the model is trained with ten times or more training data, even when the initial training dataset is limited. Our results show that this approach can help resource-limited companies develop machine-learning models.",
        "author": "Toshiki Kuramoto; Jun Suzuki",
        "authorids": "/t/toshiki-kuramoto/; /j/jun-suzuki/",
        "bibtex": "@inproceedings{kuramoto-suzuki-2025-predicting,\n    title = \"Predicting Fine-tuned Performance on Larger Datasets Before Creating Them\",\n    author = \"Kuramoto, Toshiki  and\n      Suzuki, Jun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.17/\",\n    pages = \"204--212\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.17.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.17/",
        "pdf_size": 472434,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4dJ9oLQDu9wJ:scholar.google.com/&scioq=Predicting+Fine-tuned+Performance+on+Larger+Datasets+Before+Creating+Them&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Bridgestone Corporation / Tohoku University; Tohoku University",
        "aff_domain": "dc.tohoku.ac.jp;tohoku.ac.jp",
        "email": "dc.tohoku.ac.jp;tohoku.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tohoku University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tohoku.ac.jp",
        "aff_unique_abbr": "Tohoku U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.180",
        "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs, serving as a promising paradigm for accelerating model inference. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to pursue activation sparsity and acceleration, but few can simultaneously obtain high activation sparsity and comparable model performance. This paper introduces a simple and effective method named \u201cProSparse\u201d to sparsify LLMs while achieving both targets. Specifically, after introducing ReLU activation, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing for multiple stages. This can enhance activation sparsity and mitigate performance degradation by avoiding radical shifts in activation distributions. With ProSparse, we obtain high sparsity of 89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size MiniCPM-1B, respectively, with comparable performance to their original Swish-activated versions. These present the most sparsely activated models among open-source LLaMA versions and competitive end-size models. Inference acceleration experiments further demonstrate the significant practical acceleration potential of LLMs with higher activation sparsity, obtaining up to 4.52x inference speedup.",
        "author": "Chenyang Song; Xu Han; Zhengyan Zhang; Shengding Hu; Xiyu Shi; Kuai Li; Chen Chen; Zhiyuan Liu; Guangli Li; Tao Yang; Maosong Sun",
        "authorids": "/c/chenyang-song/; /x/xu-han/; /z/zhengyan-zhang/; /s/shengding-hu/; /x/xiyu-shi/; /k/kuai-li/; /c/chen-chen/; /z/zhiyuan-liu/; /g/guangli-li/; /t/tao-yang/; /m/maosong-sun/",
        "bibtex": "@inproceedings{song-etal-2025-prosparse,\n    title = \"{P}ro{S}parse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models\",\n    author = \"Song, Chenyang  and\n      Han, Xu  and\n      Zhang, Zhengyan  and\n      Hu, Shengding  and\n      Shi, Xiyu  and\n      Li, Kuai  and\n      Chen, Chen  and\n      Liu, Zhiyuan  and\n      Li, Guangli  and\n      Yang, Tao  and\n      Sun, Maosong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.180/\",\n    pages = \"2626--2644\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.180.pdf",
        "site": "https://aclanthology.org/2025.coling-main.180/",
        "pdf_size": 724593,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2882991176355987686&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 11
    },
    {
        "id": "2025.coling-main.614",
        "title": "ProTOD: Proactive Task-oriented Dialogue System Based on Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Model (LLM)-based Task-Oriented Dialogue (TOD) systems show promising performance in helping users achieve specific goals in a zero-shot setting. However, existing systems engage with users in a reactive manner, relying on a basic single-query mechanism with the knowledge base and employing passive policy planning. The proactive TOD systems, which can provide potentially helpful information and plan cross-domain multi-task dialogue policies, have not been well studied. In addition, effective evaluation methods are also lacking. To address these issues, we propose ProTOD, a novel LLM-based proactive TOD framework designed to improve system proactivity and goal completion. First, we design an adaptive exploratory retrieval mechanism to dynamically navigate domain knowledge. Second, we introduce a two-stage passive-to-proactive policy planner that effectively organizes knowledge and actions relationship. Finally, we develop two distinct user simulators with different personalities to simulate real-world interactions and propose a new error measure called Human-targeted Policy Edit Rate (HPER) for evaluation. Experimental results show that ProTOD achieves state-of-the-art (SOTA) performance, improving goal completion rates by 10% while significantly enhancing the proactive engagement.",
        "author": "Wenjie Dong; Sirong Chen; Yan Yang",
        "authorids": "/w/wenjie-dong/; /s/sirong-chen/; /y/yan-yang/",
        "bibtex": "@inproceedings{dong-etal-2025-protod,\n    title = \"{P}ro{TOD}: Proactive Task-oriented Dialogue System Based on Large Language Model\",\n    author = \"Dong, Wenjie  and\n      Chen, Sirong  and\n      Yang, Yan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.614/\",\n    pages = \"9147--9164\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.614.pdf",
        "site": "https://aclanthology.org/2025.coling-main.614/",
        "pdf_size": 993124,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17920079034475547857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, East China Normal University, China+NPPA Key Laboratory of Publishing Integration Development, China; School of Computing and Communications, Lancaster University, UK; School of Computer Science and Technology, East China Normal University, China",
        "aff_domain": "cs.ecnu.edu.cn;lancaster.ac.uk;cs.ecnu.edu.cn",
        "email": "cs.ecnu.edu.cn;lancaster.ac.uk;cs.ecnu.edu.cn",
        "github": "https://github.com/melonhh/ProTOD",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0",
        "aff_unique_norm": "East China Normal University;NPPA Key Laboratory of Publishing Integration Development;Lancaster University",
        "aff_unique_dep": "School of Computer Science and Technology;Key Laboratory of Publishing Integration Development;School of Computing and Communications",
        "aff_unique_url": "http://www.ecnu.edu.cn;;https://www.lancaster.ac.uk",
        "aff_unique_abbr": "ECNU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2025.coling-main.502",
        "title": "Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.",
        "author": "Bowen Li; Wenhan Wu; Ziwei Tang; Lin Shi; John Yang; Jinyang Li; Shunyu Yao; Chen Qian; Binyuan Hui; Qicheng Zhang; Zhiyin Yu; He Du; Ping Yang; Dahua Lin; Chao Peng; Kai Chen",
        "authorids": "/b/bowen-li/; /w/wenhan-wu/; /z/ziwei-tang/; /l/lin-shi/; /j/john-yang/; /j/jinyang-li/; /s/shunyu-yao/; /c/chen-qian/; /b/binyuan-hui/; /q/qicheng-zhang/; /z/zhiyin-yu/; /h/he-du/; /p/ping-yang/; /d/dahua-lin/; /c/chao-peng/; /k/kai-chen/",
        "bibtex": "@inproceedings{li-etal-2025-prompting,\n    title = \"Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study\",\n    author = \"Li, Bowen  and\n      Wu, Wenhan  and\n      Tang, Ziwei  and\n      Shi, Lin  and\n      Yang, John  and\n      Li, Jinyang  and\n      Yao, Shunyu  and\n      Qian, Chen  and\n      Hui, Binyuan  and\n      Zhang, Qicheng  and\n      Yu, Zhiyin  and\n      Du, He  and\n      Yang, Ping  and\n      Lin, Dahua  and\n      Peng, Chao  and\n      Chen, Kai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.502/\",\n    pages = \"7511--7531\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.502.pdf",
        "site": "https://aclanthology.org/2025.coling-main.502/",
        "pdf_size": 1143923,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6786900156608602412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai AI Laboratory; Nanjing University; BUPT; Dartmouth College; Princeton University; The University of Hong Kong; Tsinghua University; Alibaba Group; ByteDance; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; ByteDance; Shanghai AI Laboratory; ByteDance; Shanghai AI Laboratory",
        "aff_domain": "pjlab.org.cn;bytedance.com; ; ; ; ; ; ; ; ; ; ; ; ; ;",
        "email": "pjlab.org.cn;bytedance.com; ; ; ; ; ; ; ; ; ; ; ; ; ;",
        "github": "https://github.com/open-compass/DevEval",
        "project": "",
        "author_num": 16,
        "aff_unique_index": "0;1;2;3;4;5;6;7;8;0;0;0;8;0;8;0",
        "aff_unique_norm": "Shanghai AI Laboratory;Nanjing University;Beijing University of Posts and Telecommunications;Dartmouth College;Princeton University;The University of Hong Kong;Tsinghua University;Alibaba Group;ByteDance",
        "aff_unique_dep": ";;;;;;;;",
        "aff_unique_url": "https://www.shanghai-ai-lab.com;https://www.nju.edu.cn;http://www.bupt.edu.cn/;https://www.dartmouth.edu;https://www.princeton.edu;https://www.hku.hk;https://www.tsinghua.edu.cn;https://www.alibaba.com;https://www.bytedance.com",
        "aff_unique_abbr": "SAIL;Nanjing U;BUPT;Dartmouth;Princeton;HKU;THU;Alibaba;ByteDance",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;1;1;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.376",
        "title": "PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce PropaInsight, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. PropaInsight offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present PropaGaze, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline. Our experiments show that off-the-shelf LLMs struggle with propaganda analysis, but PropaGaze significantly improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, PropaGaze complements limited human-annotated data in data-sparse and cross-domain scenarios, demonstrating its potential for comprehensive and generalizable propaganda analysis.",
        "author": "Jiateng Liu; Lin Ai; Zizhou Liu; Payam Karisani; Zheng Hui; Yi Fung; Preslav Nakov; Julia Hirschberg; Heng Ji",
        "authorids": "/j/jiateng-liu/; /l/lin-ai/; /z/zizhou-liu/; /p/payam-karisani/; /z/zheng-hui/; /y/yi-fung/; /p/preslav-nakov/; /j/julia-hirschberg/; /h/heng-ji/",
        "bibtex": "@inproceedings{liu-etal-2025-propainsight,\n    title = \"{P}ropa{I}nsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent\",\n    author = \"Liu, Jiateng  and\n      Ai, Lin  and\n      Liu, Zizhou  and\n      Karisani, Payam  and\n      Hui, Zheng  and\n      Fung, Yi  and\n      Nakov, Preslav  and\n      Hirschberg, Julia  and\n      Ji, Heng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.376/\",\n    pages = \"5607--5628\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.376.pdf",
        "site": "https://aclanthology.org/2025.coling-main.376/",
        "pdf_size": 2737341,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3306422109367812878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Illinois Urbana-Champaign; Columbia University; Columbia University; University of Illinois Urbana-Champaign; Columbia University; University of Illinois Urbana-Champaign; Mohamed bin Zayed University of Artificial Intelligence; Columbia University; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;cs.columbia.edu; ;illinois.edu;cs.columbia.edu;illinois.edu;mbzuai.ac.ae;cs.columbia.edu;illinois.edu",
        "email": "illinois.edu;cs.columbia.edu; ;illinois.edu;cs.columbia.edu;illinois.edu;mbzuai.ac.ae;cs.columbia.edu;illinois.edu",
        "github": "https://github.com/Lumos-Jiateng/PropaInsight",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;0;1;0;2;1;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Columbia University;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://illinois.edu;https://www.columbia.edu;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UIUC;Columbia;MBZUAI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0;0",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "2025.coling-main.506",
        "title": "Propulsion: Steering LLM with Tiny Fine-Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid advancements in Large Language Models (LLMs) have revolutionized natural language processing (NLP) and adjacent fields, yet fine-tuning these models for specific tasks remains computationally expensive and risks degrading pre-learned features. To address these challenges, we propose Propulsion, a novel parameter-efficient fine-tuning (PEFT) method designed to optimize task-specific performance while drastically reducing computational overhead. Inspired by the concept of controlled adjustments in physical motion, Propulsion selectively re-scales specific dimensions of a pre-trained model, guiding output predictions toward task objectives without modifying the model\u2019s parameters. By introducing lightweight, trainable Propulsion parameters at the pre-trained layer, we minimize the number of parameters updated during fine-tuning, thus preventing the overfitting or overwriting of existing knowledge. Our theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows that Propulsion approximates the performance of full fine-tuning with far fewer trainable parameters. Empirically, Propulsion reduces the parameter count from 355.3 million to a mere 0.086 million\u2014achieving over a 10x reduction compared to standard approaches like LoRA\u2014while maintaining competitive performance across benchmarks.",
        "author": "Md Kowsher; Nusrat Jahan Prottasha; Prakash Bhat",
        "authorids": "/m/md-kowsher/; /n/nusrat-jahan-prottasha/; /p/prakash-bhat/",
        "bibtex": "@inproceedings{kowsher-etal-2025-propulsion,\n    title = \"Propulsion: Steering {LLM} with Tiny Fine-Tuning\",\n    author = \"Kowsher, Md  and\n      Prottasha, Nusrat Jahan  and\n      Bhat, Prakash\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.506/\",\n    pages = \"7569--7597\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.506.pdf",
        "site": "https://aclanthology.org/2025.coling-main.506/",
        "pdf_size": 11069535,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11719016505552921723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Central Florida, FL, USA; University of Central Florida, FL, USA; Amazon, USA",
        "aff_domain": "ucf.edu;ucf.edu;amazon.com",
        "email": "ucf.edu;ucf.edu;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Central Florida;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucf.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCF;Amazon",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "FL;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.518",
        "title": "ProsodyFlow: High-fidelity Text-to-Speech through Conditional Flow Matching and Prosody Modeling with Large Speech Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text-to-speech (TTS) has seen significant advancements in high-quality, expressive speech synthesis. However, achieving diverse and natural prosody in synthesized speech remains challenging. In this paper, we propose ProsodyFlow, an end-to-end TTS model that integrates large self-supervised speech models and conditional flow matching to model prosodic features effectively. Our approach involves using a speech LLM to extract acoustic features, mapping these features into a prosody latent space, and then employing conditional flow matching to generate prosodic vectors conditioned on the input text. Experiments on the LJSpeech dataset show that ProsodyFlow improves synthesis quality and efficiency compared to existing models, achieving more prosodic and expressive speech synthesizing.",
        "author": "Haoyu Wang; Sizhe Shan; Yinlin Guo; Yuehai Wang",
        "authorids": "/h/haoyu-wang/; /s/sizhe-shan/; /y/yinlin-guo/; /y/yuehai-wang/",
        "bibtex": "@inproceedings{wang-etal-2025-prosodyflow,\n    title = \"{P}rosody{F}low: High-fidelity Text-to-Speech through Conditional Flow Matching and Prosody Modeling with Large Speech Language Models\",\n    author = \"Wang, Haoyu  and\n      Shan, Sizhe  and\n      Guo, Yinlin  and\n      Wang, Yuehai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.518/\",\n    pages = \"7748--7753\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.518.pdf",
        "site": "https://aclanthology.org/2025.coling-main.518/",
        "pdf_size": 513067,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dZMU_yq6HZkJ:scholar.google.com/&scioq=ProsodyFlow:+High-fidelity+Text-to-Speech+through+Conditional+Flow+Matching+and+Prosody+Modeling+with+Large+Speech+Language+Models&hl=en&as_sdt=0,24",
        "gs_version_total": 0,
        "aff": "College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science Electronic Engineering, Zhejiang University, Hangzhou, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "https://szczesnys.github.io/prosodyflow/",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Information Science Electronic Engineering",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.361",
        "title": "Pseudo-label Data Construction Method and Syntax-enhanced Model for Chinese Semantic Error Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chinese Semantic Error Recognition (CSER) has always been a weak link in Chinese language processing due to the complexity and obscureness of Chinese semantics. Existing research has gradually focused on leveraging pre-trained models to perform CSER. Although some researchers have attempted to integrate syntax information into the pre-trained language model, it requires training the models from scratch, which is time-consuming and laborious. Furthermore, despite the existence of datasets for CSER, the constrained size of these datasets impairs the performance of the models. Thus, in order to address the difficulty posed by a limited sample set and the need of annotating samples with semantic-level errors, we propose a Pseudo-label Data Construction method for CSER (PDC-CSER), generating pseudo-labels for augmented samples based on perplexity and model respectively, which overcomes the difficulty of constructing pseudo-label data containing semantic-level errors and ensures the quality of pseudo-labels. Moreover, we propose a CSER method with the Dependency Syntactic Attention mechanism (CSER-DSA) to explicitly infuse dependency syntactic information only in the fine-tuning stage, achieving robust performance, and simultaneously reducing substantial computing power and time cost. Results demonstrate that the pseudo-label technology PDC-CSER and the semantic error recognition method CSER-DSA surpass the existing models",
        "author": "Hongyan Wu; Nankai Lin; Shengyi Jiang; Lianxi Wang; Aimin Yang",
        "authorids": "/h/hongyan-wu/; /n/nankai-lin/; /s/shengyi-jiang/; /l/lianxi-wang/; /a/aimin-yang/",
        "bibtex": "@inproceedings{wu-etal-2025-pseudo,\n    title = \"Pseudo-label Data Construction Method and Syntax-enhanced Model for {C}hinese Semantic Error Recognition\",\n    author = \"Wu, Hongyan  and\n      Lin, Nankai  and\n      Jiang, Shengyi  and\n      Wang, Lianxi  and\n      Yang, Aimin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.361/\",\n    pages = \"5391--5402\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.361.pdf",
        "site": "https://aclanthology.org/2025.coling-main.361/",
        "pdf_size": 869567,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:T6OVvN_IGeYJ:scholar.google.com/&scioq=Pseudo-label+Data+Construction+Method+and+Syntax-enhanced+Model+for+Chinese+Semantic+Error+Recognition&hl=en&as_sdt=0,44",
        "gs_version_total": 0,
        "aff": "College of Computer, National University of Defense Technology; School of Information Science and Technology, Guangdong University of Foreign Studies; School of Information Technology and Engineering, Guangzhou College of Commerce; School of Information Science and Technology, Guangdong University of Foreign Studies; School of Computer Science and Technology, Guangdong University of Technology+School of Computer Science and Intelligence Education, Lingnan Normal University",
        "aff_domain": "outlook.com; ; ; ; ",
        "email": "outlook.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;1;3+4",
        "aff_unique_norm": "National University of Defense Technology;Guangdong University of Foreign Studies;Guangzhou College of Commerce;Guangdong University of Technology;Lingnan Normal University",
        "aff_unique_dep": "College of Computer;School of Information Science and Technology;School of Information Technology and Engineering;School of Computer Science and Technology;School of Computer Science and Intelligence Education",
        "aff_unique_url": "http://www.nudt.edu.cn/;http://www.gdufs.edu.cn;http://www.gzcc.edu.cn;http://www.gdut.edu.cn;http://www.lnu.edu.cn/",
        "aff_unique_abbr": "NUDT;;;;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.100",
        "title": "QABISAR: Query-Article Bipartite Interactions for Statutory Article Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we introduce QABISAR, a novel framework for statutory article retrieval, to overcome the semantic mismatch problem when modeling each query-article pair in isolation, making it hard to learn representation that can effectively capture multi-faceted information. QABISAR leverages bipartite interactions between queries and articles to capture diverse aspects inherent in them. Further, we employ knowledge distillation to transfer enriched query representations from the graph network into the query bi-encoder, to capture the rich semantics present in the graph representations, despite absence of graph-based supervision for unseen queries during inference. Our experiments on a real-world expert-annotated dataset demonstrate its effectiveness.",
        "author": "Santosh T.y.s.s; Hassan Sarwat; Matthias Grabmair",
        "authorids": "/s/santosh-t-y-s-s/; /h/hassan-sarwat/; /m/matthias-grabmair/",
        "bibtex": "@inproceedings{t-y-s-s-etal-2025-qabisar,\n    title = \"{QABISAR}: Query-Article Bipartite Interactions for Statutory Article Retrieval\",\n    author = \"T.y.s.s, Santosh  and\n      Sarwat, Hassan  and\n      Grabmair, Matthias\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.100/\",\n    pages = \"1496--1502\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.100.pdf",
        "site": "https://aclanthology.org/2025.coling-main.100/",
        "pdf_size": 314957,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VSMeQcyzAZwJ:scholar.google.com/&scioq=QABISAR:+Query-Article+Bipartite+Interactions+for+Statutory+Article+Retrieval&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.303",
        "title": "QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rise of large language models (LLMs) has created a need for advanced benchmarking systems beyond traditional setups. To this end, we introduce QUENCH, a novel text-based English Quizzing Benchmark manually curated and transcribed from YouTube quiz videos. QUENCH possesses masked entities and rationales for the LLMs to predict via generation. At the intersection of world knowledge, geographical context, and common sense reasoning, QUENCH helps assess world knowledge and deduction capabilities of LLMs via a zero-shot, open-domain quizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics, investigating the influence of model size, prompting style, geographical context, and gold-labeled rationale generation. The benchmarking concludes with an error analysis of various types of generative errors to which the LLMs are prone.",
        "author": "Mohammad Aflah Khan; Neemesh Yadav; Sarah Masud; Md. Shad Akhtar",
        "authorids": "/m/mohammad-aflah-khan/; /n/neemesh-yadav/; /s/sarah-masud/; /m/md-shad-akhtar/",
        "bibtex": "@inproceedings{khan-etal-2025-quench,\n    title = \"{QUENCH}: Measuring the gap between {I}ndic and Non-{I}ndic Contextual General Reasoning in {LLM}s\",\n    author = \"Khan, Mohammad Aflah  and\n      Yadav, Neemesh  and\n      Masud, Sarah  and\n      Akhtar, Md. Shad\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.303/\",\n    pages = \"4493--4509\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.303.pdf",
        "site": "https://aclanthology.org/2025.coling-main.303/",
        "pdf_size": 796389,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9vcJ1VXp87UJ:scholar.google.com/&scioq=QUENCH:+Measuring+the+gap+between+Indic+and+Non-Indic+Contextual+General+Reasoning+in+LLMs&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "IIIT Delhi, India; IIIT Delhi, India; IIIT Delhi, India; IIIT Delhi, India",
        "aff_domain": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "email": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IIIT Delhi",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iiitdelhi.ac.in",
        "aff_unique_abbr": "IIITD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.124",
        "title": "Quality Beyond A Glance: Revealing Large Quality Differences Between Web-Crawled Parallel Corpora",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Parallel corpora play a vital role in advanced multilingual natural language processing tasks, notably in machine translation (MT). The recent emergence of numerous large parallel corpora, often extracted from multilingual documents on the Internet, has expanded the available resources. Nevertheless, the quality of these corpora remains largely unexplored, while there are large differences in how the corpora are constructed. Moreover, how the potential differences affect the performance of neural MT (NMT) systems has also received limited attention. This study addresses this gap by manually and automatically evaluating four well-known publicly available parallel corpora across eleven language pairs. Our findings are quite concerning: all corpora contain a substantial amount of noisy sentence pairs, with CCMatrix and CCAligned having well below of 50% reasonably clean pairs. MaCoCu and ParaCrawl generally have higher quality texts, though around a third of the texts still have clear issues. While corpus size impacts NMT models\u2019 performance, our study highlights the critical role of quality: higher-quality corpora consistently yield better-performing NMT models when controlling for size.",
        "author": "Rik van Noord; Miquel Espl\u00e0-Gomis; Malina Chichirau; Gema Ram\u00edrez-S\u00e1nchez; Antonio Toral",
        "authorids": "/r/rik-van-noord/; /m/miquel-espla-gomis/; /m/malina-chichirau/; /g/gema-ramirez-sanchez/; /a/antonio-toral/",
        "bibtex": "@inproceedings{van-noord-etal-2025-quality,\n    title = \"Quality Beyond A Glance: Revealing Large Quality Differences Between Web-Crawled Parallel Corpora\",\n    author = \"van Noord, Rik  and\n      Espl{\\`a}-Gomis, Miquel  and\n      Chichirau, Malina  and\n      Ram{\\'i}rez-S{\\'a}nchez, Gema  and\n      Toral, Antonio\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.124/\",\n    pages = \"1824--1838\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.124.pdf",
        "site": "https://aclanthology.org/2025.coling-main.124/",
        "pdf_size": 298492,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Aqbtun2DCvYJ:scholar.google.com/&scioq=Quality+Beyond+A+Glance:+Revealing+Large+Quality+Differences+Between+Web-Crawled+Parallel+Corpora&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "University of Groningen\u2666; Universitat d\u2019Alacant\u22c6; University of Groningen\u2666; Prompsit\u2020; University of Groningen\u2666",
        "aff_domain": "gmail.com; ; ; ; ",
        "email": "gmail.com; ; ; ; ",
        "github": "https://github.com/RikVN/MaCoCu_Parallel",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "University of Groningen;Universitat d'Alacant;Prompsit",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.rug.nl;https://www.ua.es;",
        "aff_unique_abbr": "RUG;UA;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Netherlands;Spain;"
    },
    {
        "id": "2025.coling-main.588",
        "title": "Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Evaluating the outputs of large language models (LLMs) on long-form generative tasks remains challenging. While fine-grained, aspect-wise evaluations provide valuable diagnostic information, they are difficult to design exhaustively, and each aspect\u2019s contribution to the overall acceptability of an answer is unclear. In this study, we propose a method to compute an overall quality score as a weighted average of three key aspects: factuality, informative- ness, and formality. This approach achieves stronger correlations with human judgments compared to previous metrics. Our analysis identifies factuality as the most predictive aspect of overall quality. Additionally, we release a dataset of 1.2k long-form QA answers annotated with both absolute judgments and relative preferences in overall and aspect-wise schemes to aid future research in evaluation practices.",
        "author": "Go Kamoda; Akari Asai; Ana Brassard; Keisuke Sakaguchi",
        "authorids": "/g/go-kamoda/; /a/akari-asai/; /a/ana-brassard/; /k/keisuke-sakaguchi/",
        "bibtex": "@inproceedings{kamoda-etal-2025-quantifying,\n    title = \"Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment\",\n    author = \"Kamoda, Go  and\n      Asai, Akari  and\n      Brassard, Ana  and\n      Sakaguchi, Keisuke\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.588/\",\n    pages = \"8787--8808\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.588.pdf",
        "site": "https://aclanthology.org/2025.coling-main.588/",
        "pdf_size": 7270601,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-whEW4dZL-cJ:scholar.google.com/&scioq=Quantifying+the+Influence+of+Evaluation+Aspects+on+Long-Form+Response+Assessment&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Tohoku University; University of Washington; RIKEN + Tohoku University; RIKEN",
        "aff_domain": "dc.tohoku.ac.jp; ; ; ",
        "email": "dc.tohoku.ac.jp; ; ; ",
        "github": "github.com/gokamoda/lfqa-weighted-eval",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2+0;2",
        "aff_unique_norm": "Tohoku University;University of Washington;RIKEN",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.washington.edu;https://www.riken.jp",
        "aff_unique_abbr": "Tohoku U;UW;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+0;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2025.coling-industry.2",
        "title": "Query-LIFE: Query-aware Language Image Fusion Embedding for E-Commerce Relevance",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Relevance module plays a fundamental role in e-commerce search as they are responsible for selecting relevant products from thousands of items based on user queries, thereby enhancing users experience and efficiency. The traditional method calculates the relevance score based on product titles and user queries, but the information in title alone maybe insufficient to describe the product completely. A more general method is to further leverage product image information. In recent years, vision-language pre-training model has achieved impressive results in many scenarios, which leverage contrastive learning to map both textual and visual features into a joint embedding space. In e-commerce, a common practice is to further fine-tune the model using e-commerce data on the basis of pre-trained model. However, the performance is sub-optimal because the vision-language pre-training models lack of alignment specifically designed for queries. In this paper, we propose Query-aware Language Image Fusion Embedding to address these challenges. Query-LIFE utilizes a query-based multimodal fusion to effectively incorporate the image and title based on the product types. Additionally, it employs query-aware modal alignment to enhance the accuracy of the comprehensive representation of products. Furthermore, we design GenFilt, which utilizes the generation capability of large models to filter out false negative samples and further improve the overall performance of the contrastive learning task in the model. Experiments have demonstrated that Query-LIFE outperforms existing baselines. We have conducted ablation studies and human evaluations to validate the effectiveness of each module within Query-LIFE. Moreover, Query-LIFE has been deployed on Miravia Search. resulting in improved both relevance and conversion efficiency.",
        "author": "Hai Zhu; Yuankai Guo; Ronggang Dou; Kai Liu",
        "authorids": "/h/hai-zhu/; /y/yuankai-guo/; /r/ronggang-dou/; /k/kai-liu/",
        "bibtex": "@inproceedings{zhu-etal-2025-query,\n    title = \"Query-{LIFE}: Query-aware Language Image Fusion Embedding for {E}-Commerce Relevance\",\n    author = \"Zhu, Hai  and\n      Guo, Yuankai  and\n      Dou, Ronggang  and\n      Liu, Kai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.2/\",\n    pages = \"21--28\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.2.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.2/",
        "pdf_size": 3189122,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17303560822571536985&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "https://www.miravia.es/",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.34",
        "title": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The capacity of Large Language Models (LLMs) to comprehend and reason over long contexts is pivotal for advancements in diverse fields. Yet, they still stuggle with capturing long-distance dependencies within sequences to deeply understand semantics. To address this issue, we introduce Query-aware Inference for LLMs (Q-LLM), a system designed to process extensive sequences akin to human cognition. By focusing on memory data relevant to a given query, Q-LLM can accurately capture pertinent information within a fixed window size and provide precise answers to queries. It doesn\u2019t require extra training and can be seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can read Harry Potter within 30s and accurately answer the questions. On widely recognized benchmarks, Q-LLM improved by 7.17% compared to the current state-of-the-art on LLaMA3, and by 3.26% on Mistral on the \u221e-bench. In the Needle-in-a-Haystack and BABILong task, Q-LLM improved upon the current SOTA by 7.0% and 6.1%. Our code is in https://github.com/dvlab-research/Q-LLM.",
        "author": "Jingyao Li; Han Shi; Sitong Wu; Chuanyang Zheng; Zhenguo Li; Xin Jiang; Hong Xu; Jiaya Jia",
        "authorids": "/j/jingyao-li/; /h/han-shi/; /s/sitong-wu/; /c/chuanyang-zheng/; /z/zhenguo-li/; /x/xin-jiang/; /h/hong-xu/; /j/jiaya-jia/",
        "bibtex": "https://aclanthology.org/2025.coling-main.34.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.34.pdf",
        "site": "https://aclanthology.org/2025.coling-main.34/",
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5843906356398808945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.365",
        "title": "RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for Inspirational Quote Extraction from Long Documents",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Inspirational quotes from famous individuals are often used to convey thoughts in news articles, essays, and everyday conversations. In this paper, we propose a novel context-based quote extraction system that aims to predict the most relevant quote from a long text. We formulate this quote extraction as an open domain question answering problem first by employing a vector-store based retriever and then applying a multi-task reader. We curate three context-based quote extraction dataset and introduce a novel multi-task framework RA-MTR that improves the state-of-the-art performance, achieving a maximum improvement of 5.08% in BoW F1-score.",
        "author": "Sayantan Adak; Animesh Mukherjee",
        "authorids": "/s/sayantan-adak/; /a/animesh-mukherjee/",
        "bibtex": "@inproceedings{adak-mukherjee-2025-ra,\n    title = \"{RA}-{MTR}: A Retrieval Augmented Multi-Task Reader based Approach for Inspirational Quote Extraction from Long Documents\",\n    author = \"Adak, Sayantan  and\n      Mukherjee, Animesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.365/\",\n    pages = \"5443--5462\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.365.pdf",
        "site": "https://aclanthology.org/2025.coling-main.365/",
        "pdf_size": 1615772,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VMALiGQIhqcJ:scholar.google.com/&scioq=RA-MTR:+A+Retrieval+Augmented+Multi-Task+Reader+based+Approach+for+Inspirational+Quote+Extraction+from+Long+Documents&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Indian Institute of Technology Kharagpur, West Bengal \u2013 721302; Indian Institute of Technology Kharagpur, West Bengal \u2013 721302",
        "aff_domain": "kgpian.iitkgp.ac.in;cse.iitkgp.ac.in",
        "email": "kgpian.iitkgp.ac.in;cse.iitkgp.ac.in",
        "github": "https://github.com/sayantan11995/Context_based_Quote_Extraction",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitkgp.ac.in",
        "aff_unique_abbr": "IIT Kharagpur",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kharagpur",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-demos.12",
        "title": "RAGthoven: A Configurable Toolkit for RAG-enabled LLM Experimentation",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Large Language Models (LLMs) have significantly altered the landscape of Natural Language Processing (NLP), having topped the benchmarks of many standard tasks and problems, particularly when used in combination with Retrieval Augmented Generation (RAG). Despite their impressive performance and relative simplicity, its use as a baseline method has not been extensive. One of the reasons might be that adapting and optimizing RAG-based pipelines for specific NLP tasks generally requires custom development which is difficult to scale. In this work we introduce RAGthoven, a tool for automatic evaluation of RAG-based pipelines. It provides a simple yet powerful abstraction, which allows the user to start the evaluation process with nothing more than a single configuration file. To demonstrate its usefulness we conduct three case studies spanning text classification, question answering and code generation usecases. We release the code, as well as the documentation and tutorials, at https://github.com/ragthoven-dev/ragthoven",
        "author": "Gregor Karetka; Demetris Skottis; Lucia Dutkov\u00e1; Peter Hra\u0161ka; Marek Suppa",
        "authorids": "/g/gregor-karetka/; /d/demetris-skottis/; /l/lucia-dutkova/; /p/peter-hraska/; /m/marek-suppa/",
        "bibtex": "@inproceedings{karetka-etal-2025-ragthoven,\n    title = \"{RAG}thoven: A Configurable Toolkit for {RAG}-enabled {LLM} Experimentation\",\n    author = \"Karetka, Gregor  and\n      Skottis, Demetris  and\n      Dutkov{\\'a}, Lucia  and\n      Hra{\\v{s}}ka, Peter  and\n      Suppa, Marek\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.12/\",\n    pages = \"117--125\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.12.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.12/",
        "pdf_size": 314947,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6weNAXFV2WoJ:scholar.google.com/&scioq=RAGthoven:+A+Configurable+Toolkit+for+RAG-enabled+LLM+Experimentation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Brno University of Technology + Cisco Systems; Cisco Systems; Cisco Systems + Linnaeus University; Cisco Systems; Cisco Systems + Comenius University in Bratislava",
        "aff_domain": "but.cz;cisco.com;linnaeus.se;cisco.com;suppa.sk",
        "email": "but.cz;cisco.com;linnaeus.se;cisco.com;suppa.sk",
        "github": "https://github.com/ragthoven-dev/ragthoven1",
        "project": "https://ragthoven-dev.github.io/walkthrough.mp4",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1+2;1;1+3",
        "aff_unique_norm": "Brno University of Technology;Cisco Systems;Linnaeus University;Comenius University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.vutbr.cz;https://www.cisco.com;https://lnu.se/en/;https://www.uniba.sk",
        "aff_unique_abbr": "Brno UoT;Cisco;LNU;CU",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Bratislava",
        "aff_country_unique_index": "0+1;1;1+2;1;1+3",
        "aff_country_unique": "Czech Republic;United States;Sweden;Slovakia"
    },
    {
        "id": "2025.coling-main.735",
        "title": "RAIDEN Benchmark: Evaluating Role-playing Conversational Agents with Measurement-Driven Custom Dialogues",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As Large-scale Language Models (LLMs) advance, the development of engaging Role-Playing Conversational Agents (RPCAs) has gained prominence. Despite this progress, there is a notable absence of benchmarks designed around dialogues, rather than question-answering formats, to assess the effectiveness of RPCA interactions. This paper introduces the RAIDEN benchmark, containing a comprehensive dataset specifically developed for RPCA evaluation, comprising over 40,000 multi-turn utterances across 135 characters. The benchmark focuses on assessing particular dimensions at different stages of a conversation, facilitated through interactions conducted by annotators. This approach allows the evaluation phase to concentrate on specific response dimensions, and thus subjectivity in dialogue evaluation is reduced. To further enhance objectivity, evaluators compare responses from two different models rather than assessing a single response in isolation. Besides, we introduce RPCAJudger, a specialized judging LLM tailored for automatic RPCA evaluation. The evaluations conducted by RPCAJudger closely mirror human judgments, and its API-free methodology serves to prevent potential data leakage. All the models and all non-private leaderboard data will be made publicly available.",
        "author": "Bowen Wu; Kaili Sun; Ziwei Bai; Ying Li; Baoxun Wang",
        "authorids": "/b/bowen-wu/; /k/kaili-sun/; /z/ziwei-bai/; /y/ying-li/; /b/baoxun-wang/",
        "bibtex": "@inproceedings{wu-etal-2025-raiden,\n    title = \"{RAIDEN} Benchmark: Evaluating Role-playing Conversational Agents with Measurement-Driven Custom Dialogues\",\n    author = \"Wu, Bowen  and\n      Sun, Kaili  and\n      Bai, Ziwei  and\n      Li, Ying  and\n      Wang, Baoxun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.735/\",\n    pages = \"11086--11106\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.735.pdf",
        "site": "https://aclanthology.org/2025.coling-main.735/",
        "pdf_size": 4467793,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11814542121516010684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Software & Microelectronics, Peking University, Beijing, China + Platform and Content Group, Tencent; Platform and Content Group, Tencent; Platform and Content Group, Tencent; School of Software & Microelectronics, Peking University, Beijing, China; Platform and Content Group, Tencent",
        "aff_domain": "pku.edu.cn;tencent.com;tencent.com;pku.edu.cn;tencent.com",
        "email": "pku.edu.cn;tencent.com;tencent.com;pku.edu.cn;tencent.com",
        "github": "https://github.com/FrontierLabs/RAIDEN",
        "project": "https://character.ai/",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "Peking University;Tencent",
        "aff_unique_dep": "School of Software & Microelectronics;Platform and Content Group",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "PKU;Tencent",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.30",
        "title": "RAM2C: A Liberal Arts Educational Chatbot based on Retrieval-augmented Multi-role Multi-expert Collaboration",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, many studies focus on utilizing large language models (LLMs) into educational dialogues. Especially, within liberal arts dialogues, educators must balance Humanized communication, Teaching expertise, and Safety-ethics (HTS), besides the subject knowledge itself. However, due to collecting massive amounts of HTS-compliant teaching dialogues from real world as training corpus is expensive, the outputs of existing LLMs in teaching dialogues fall short of human standards. To address this, we design a Retrieval-augmented Multi-role Multi-expert Collaboration (RAM2C) framework to automatically generate such dialogues data. Specifically, we first establish HTS-guided knowledge bases, encompassing three domain knowledge in teaching skills, psychology, and safety ethics. Then, RAM2C organizes LLMs, which are retrieval-augmented by the above different knowledge bases, into multi-experts groups with distinct roles to generate the HTS-compliant educational dialogues dataset. We then fine-tuned the LLMs using this dataset. Empirical evaluations indicate that RAM2C-empowered LLMs excel in Chinese reading teaching, offering more personalized, and ethically safe teaching response, demonstrating RAM2C\u2019s practicality and high quality. We release the experiments at https://github.com/ram2c/ram2c.",
        "author": "Haoyu Huang; Tong Niu; Rui Yang; Luping Shi",
        "authorids": "/h/haoyu-huang/; /t/tong-niu/; /r/rui-yang/; /l/luping-shi/",
        "bibtex": "@inproceedings{huang-etal-2025-ram2c,\n    title = \"{RAM}2{C}: A Liberal Arts Educational Chatbot based on Retrieval-augmented Multi-role Multi-expert Collaboration\",\n    author = \"Huang, Haoyu  and\n      Niu, Tong  and\n      Yang, Rui  and\n      Shi, Luping\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.30/\",\n    pages = \"448--458\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.30.pdf",
        "site": "https://aclanthology.org/2025.coling-main.30/",
        "pdf_size": 2920002,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YbWp6DboU8IJ:scholar.google.com/&scioq=RAM2C:+A+Liberal+Arts+Educational+Chatbot+based+on+Retrieval-augmented+Multi-role+Multi-expert+Collaboration&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Center for Brain Inspired Computing Research (CBICR), Tsinghua University, Beijing, China+Optical Memory National Engineering Research Center, Tsinghua University, Beijing, China+Department of Precision Instrument, Tsinghua University, Beijing 100084, China+IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China; Center for Brain Inspired Computing Research (CBICR), Tsinghua University, Beijing, China+Optical Memory National Engineering Research Center, Tsinghua University, Beijing, China+Department of Precision Instrument, Tsinghua University, Beijing 100084, China+IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China; Center for Brain Inspired Computing Research (CBICR), Tsinghua University, Beijing, China+Optical Memory National Engineering Research Center, Tsinghua University, Beijing, China+Department of Precision Instrument, Tsinghua University, Beijing 100084, China+IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China; Center for Brain Inspired Computing Research (CBICR), Tsinghua University, Beijing, China+Optical Memory National Engineering Research Center, Tsinghua University, Beijing, China+Department of Precision Instrument, Tsinghua University, Beijing 100084, China+IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China",
        "aff_domain": "; ; ;mail.tsinghua.edu.cn",
        "email": "; ; ;mail.tsinghua.edu.cn",
        "github": "https://github.com/ram2c/ram2c",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Center for Brain Inspired Computing Research (CBICR)",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Tsinghua",
        "aff_campus_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.62",
        "title": "RE-FIN: Retrieval-based Enrichment for Financial data",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Enriching sentences with knowledge from qualitative sources benefits various NLP tasks and enhances the use of labeled data in model training. This is crucial for Financial Sentiment Analysis (FSA), where texts are often brief and contain implied information. We introduce RE-FIN (Retrieval-based Enrichment for FINancial data), an automated system designed to retrieve information from a knowledge base to enrich financial sentences, making them more knowledge-dense and explicit. RE-FIN generates propositions from the knowledge base and employs Retrieval-Augmented Generation (RAG) to augment the original text with relevant information. A large language model (LLM) rewrites the original sentence, incorporating this data. Since the LLM does not create new content, the risk of hallucinations is significantly reduced. The LLM generates multiple new sentences using different relevant information from the knowledge base; we developed an algorithm to select one that best preserves the meaning of the original sentence while avoiding excessive syntactic similarity. Results show that enhanced sentences present lower perplexity than the original ones and improve performances on FSA.",
        "author": "Lorenzo Malandri; Fabio Mercorio; Mario Mezzanzanica; Filippo Pallucchini",
        "authorids": "/l/lorenzo-malandri/; /f/fabio-mercorio/; /m/mario-mezzanzanica/; /f/filippo-pallucchini/",
        "bibtex": "@inproceedings{malandri-etal-2025-fin,\n    title = \"{RE}-{FIN}: Retrieval-based Enrichment for Financial data\",\n    author = \"Malandri, Lorenzo  and\n      Mercorio, Fabio  and\n      Mezzanzanica, Mario  and\n      Pallucchini, Filippo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.62/\",\n    pages = \"751--759\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.62.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.62/",
        "pdf_size": 1788480,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11323832697582240734&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Dept of Statistics and Quantitative Methods, University of Milano-Bicocca, Italy; CRISP Research Centre, University of Milano-Bicocca, Italy; Dept of Statistics and Quantitative Methods, University of Milano-Bicocca, Italy; CRISP Research Centre, University of Milano-Bicocca, Italy",
        "aff_domain": "unimib.it;unimib.it;unimib.it;unimib.it",
        "email": "unimib.it;unimib.it;unimib.it;unimib.it",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Milano-Bicocca",
        "aff_unique_dep": "Dept of Statistics and Quantitative Methods",
        "aff_unique_url": "https://www.unimib.it",
        "aff_unique_abbr": "UniMiB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2025.coling-industry.5",
        "title": "RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Linguistic Classifiers",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data. However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into industry processes. In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing system intervention measures aimed at improving classification performance. Our methodology outperforms LLM-generated labels in six of eight tests and base classifiers in all tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases.",
        "author": "David Farr; Nico Manzonelli; Iain Cruickshank; Jevin West",
        "authorids": "/d/david-farr/; /n/nico-manzonelli/; /i/iain-cruickshank/; /j/jevin-west/",
        "bibtex": "@inproceedings{farr-etal-2025-red,\n    title = \"{RED}-{CT}: A Systems Design Methodology for Using {LLM}-labeled Data to Train and Deploy Edge Linguistic Classifiers\",\n    author = \"Farr, David  and\n      Manzonelli, Nico  and\n      Cruickshank, Iain  and\n      West, Jevin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.5/\",\n    pages = \"58--67\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.5.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.5/",
        "pdf_size": 746942,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GK_sL6oNTN8J:scholar.google.com/&scioq=RED-CT:+A+Systems+Design+Methodology+for+Using+LLM-labeled+Data+to+Train+and+Deploy+Edge+Linguistic+Classifiers&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Washington; Army Cyber Technology and Innovation Center; Carnegie Mellon University; University of Washington",
        "aff_domain": "uw.edu; ; ; ",
        "email": "uw.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Washington;Army Cyber Technology and Innovation Center;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;;https://www.cmu.edu",
        "aff_unique_abbr": "UW;;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-industry.61",
        "title": "REVerSum: A Multi-staged Retrieval-Augmented Generation Method to Enhance Wikipedia Tail Biographies through Personal Narratives",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Wikipedia is an invaluable resource for factual information about a wide range of entities. However, the quality of articles on less-known entities often lags behind that of the well-known ones. This study proposes a novel approach to enhancing Wikipedia\u2019s B and C category biography articles by leveraging personal narratives such as autobiographies and biographies. By utilizing a multi-staged retrieval-augmented generation technique \u2013 REVerSum \u2013 we aim to enrich the informational content of these lesser-known articles. Our study reveals that personal narratives can significantly improve the quality of Wikipedia articles, providing a rich source of reliable information that has been underutilized in previous studies. Based on crowd-based evaluation, REVerSum generated content outperforms the best performing baseline by 17% in terms of integrability to the original Wikipedia article and 28.5% in terms of informativeness.",
        "author": "Sayantan Adak; Pauras Mangesh Meher; Paramita Das; Animesh Mukherjee",
        "authorids": "/s/sayantan-adak/; /p/pauras-mangesh-meher/; /p/paramita-das/; /a/animesh-mukherjee/",
        "bibtex": "@inproceedings{adak-etal-2025-reversum,\n    title = \"{REV}er{S}um: A Multi-staged Retrieval-Augmented Generation Method to Enhance {W}ikipedia Tail Biographies through Personal Narratives\",\n    author = \"Adak, Sayantan  and\n      Meher, Pauras Mangesh  and\n      Das, Paramita  and\n      Mukherjee, Animesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.61/\",\n    pages = \"732--750\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.61.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.61/",
        "pdf_size": 1695665,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14216082760211568680&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "IIT, Kharagpur; IIT, Kharagpur; IIT, Kharagpur; IIT, Kharagpur",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/sayantan11995/wikipedia_enrichment",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitkgp.ac.in",
        "aff_unique_abbr": "IIT Kharagpur",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kharagpur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.205",
        "title": "RGR-KBQA: Generating Logical Forms for Question Answering Using Knowledge-Graph-Enhanced Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the field of natural language processing, Knowledge Base Question Answering (KBQA) is a challenging task that involves accurately retrieving answers from structured knowledge. Existing methods often face issues when generating query statements using LLMs, as the knowledge introduced may be imprecise and the models themselves may exhibit hallucination problems, leading to low accuracy, particularly when dealing with complex questions. To address these challenges, we introduce a novel semantic parsing approach called RGR-KBQA, which adopts a Retrieve-Generate-Retrieve framework. The first retrieval step introduces factual knowledge from a knowledge graph to enhance the semantic understanding capabilities of LLMs, thereby improving generation accuracy of logical form. The second step uses a fine-tuned model to generate the logical form, and the final step involves unsupervised relation and entity retrieval to further enhance generation accuracy. These two retrieval steps help alleviate the hallucination problems inherent in LLMs. Experimental results show that RGR-KBQA demonstrate promising performance on CWQ and WebQSP datasets.",
        "author": "Tengfei Feng; Liang He",
        "authorids": "/t/tengfei-feng/; /l/liang-he/",
        "bibtex": "@inproceedings{feng-he-2025-rgr,\n    title = \"{RGR}-{KBQA}: Generating Logical Forms for Question Answering Using Knowledge-Graph-Enhanced Large Language Model\",\n    author = \"Feng, Tengfei  and\n      He, Liang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.205/\",\n    pages = \"3057--3070\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.205.pdf",
        "site": "https://aclanthology.org/2025.coling-main.205/",
        "pdf_size": 827098,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:SjEM1vEepXIJ:scholar.google.com/&scioq=RGR-KBQA:+Generating+Logical+Forms+for+Question+Answering+Using+Knowledge-Graph-Enhanced+Large+Language+Model&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Xinjiang University, Urumqi 830017, China + Xinjiang Key Laboratory of Signal Detection and Processing, Urumqi 830017, China; School of Computer Science and Technology, Xinjiang University, Urumqi 830017, China + Xinjiang Key Laboratory of Signal Detection and Processing, Urumqi 830017, China + Department of Electronic Engineering, and Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China",
        "aff_domain": "gmail.com;mail.tsinghua.edu.cn",
        "email": "gmail.com;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1+2",
        "aff_unique_norm": "Xinjiang University;Xinjiang Key Laboratory of Signal Detection and Processing;Tsinghua University",
        "aff_unique_dep": "School of Computer Science and Technology;Signal Detection and Processing;Department of Electronic Engineering",
        "aff_unique_url": ";;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": ";;THU",
        "aff_campus_unique_index": "0;0+2",
        "aff_campus_unique": "Urumqi;;Beijing",
        "aff_country_unique_index": "0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.633",
        "title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Riddle-solving requires advanced reasoning skills, pushing Large Language Models (LLMs) to engage in abstract thinking and creative problem-solving, often revealing limitations in their cognitive abilities. In this paper, we examine the riddle-solving capabilities of LLMs using a multiple-choice format, exploring how different prompting techniques impact performance on riddles that demand diverse reasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with COntext REcontruciton) a novel fully automated prompting method that generates and utilizes contextually reconstructed sentence-based puzzles in conjunction with the original examples to create few-shot exemplars. Our experiments demonstrate that RISCORE significantly improves the performance of language models in both vertical and lateral thinking tasks, surpassing traditional exemplar selection strategies across a variety of few-shot settings.",
        "author": "Ioannis Panagiotopoulos; George Filandrianos; Maria Lymperaiou; Giorgos Stamou",
        "authorids": "/i/ioannis-panagiotopoulos/; /g/george-filandrianos/; /m/maria-lymperaiou/; /g/giorgos-stamou/",
        "bibtex": "@inproceedings{panagiotopoulos-etal-2025-riscore,\n    title = \"{RISCORE}: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation\",\n    author = \"Panagiotopoulos, Ioannis  and\n      Filandrianos, George  and\n      Lymperaiou, Maria  and\n      Stamou, Giorgos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.633/\",\n    pages = \"9431--9455\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.633.pdf",
        "site": "https://aclanthology.org/2025.coling-main.633/",
        "pdf_size": 1559537,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1348143330088255650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Electrical and Computer Engineering, AILS Laboratory, National Technical University of Athens; School of Electrical and Computer Engineering, AILS Laboratory, National Technical University of Athens; School of Electrical and Computer Engineering, AILS Laboratory, National Technical University of Athens; School of Electrical and Computer Engineering, AILS Laboratory, National Technical University of Athens",
        "aff_domain": "gmail.com;islab.ntua.gr;islab.ntua.gr;cs.ntua.gr",
        "email": "gmail.com;islab.ntua.gr;islab.ntua.gr;cs.ntua.gr",
        "github": "https://github.com/GiannisPana/RISCORE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National Technical University of Athens",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ntua.gr",
        "aff_unique_abbr": "NTUA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Greece"
    },
    {
        "id": "2025.coling-main.149",
        "title": "ROUGE-SciQFS: A ROUGE-based Method to Automatically Create Datasets for Scientific Query-Focused Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "So far, the task of Scientific Query-Focused Summarization (Sci-QFS) has lagged in development when compared to other areas of Scientific Natural Language Processing because of the lack of data. In this work, we propose a methodology to take advantage of existing collections of academic papers to obtain large-scale datasets for this task automatically. After applying it to the papers from our reading group, we introduce a novel dataset for Sci-QFS composed of 8,695 examples, each one with a query, the sentences of the full text from a paper and the relevance labels for each. After testing several classical and state-of-the-art embedding models on this data, we found that the task of Sci-QFS is far from being solved, although it is relatively straightforward for humans. Surprisingly, we found that classical methods outperformed modern pre-trained Deep Language Models (sometimes by a large margin), showing the need for large datasets to better fine-tune the latter. We share our experiments, data and models at https://github.com/jarobyte91/rouge_sciqfs.",
        "author": "Juan Ramirez-Orta; Ana Maguitman; Axel J. Soto; Evangelos Milios",
        "authorids": "/j/juan-ramirez-orta/; /a/ana-maguitman/; /a/axel-j-soto/; /e/evangelos-milios/",
        "bibtex": "@inproceedings{ramirez-orta-etal-2025-rouge,\n    title = \"{ROUGE}-{S}ci{QFS}: A {ROUGE}-based Method to Automatically Create Datasets for Scientific Query-Focused Summarization\",\n    author = \"Ramirez-Orta, Juan  and\n      Maguitman, Ana  and\n      Soto, Axel J.  and\n      Milios, Evangelos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.149/\",\n    pages = \"2187--2197\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.149.pdf",
        "site": "https://aclanthology.org/2025.coling-main.149/",
        "pdf_size": 347352,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:HfwZipScfOoJ:scholar.google.com/&scioq=ROUGE-SciQFS:+A+ROUGE-based+Method+to+Automatically+Create+Datasets+for+Scientific+Query-Focused+Summarization&hl=en&as_sdt=0,14",
        "gs_version_total": 2,
        "aff": "Faculty of Computer Science, Dalhousie University; Faculty of Computer Science, Dalhousie University; Institute for Computer Science and Engineering, UNS - CONICET; Department of Computer Science and Engineering, Universidad Nacional del Sur",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/jarobyte91/rouge_sciqfs",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Dalhousie University;University of Buenos Aires - National Scientific and Technical Research Council;Universidad Nacional del Sur",
        "aff_unique_dep": "Faculty of Computer Science;Institute for Computer Science and Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.dal.ca;https://www.ungs.edu.ar;https://www.uns.edu.ar",
        "aff_unique_abbr": "Dal;UNS-CONICET;UNS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "Canada;Argentina"
    },
    {
        "id": "2025.coling-main.454",
        "title": "RRHF-V: Ranking Responses to Mitigate Hallucinations in Multimodal Large Language Models with Human Feedback",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal large language models (MLLMs) demonstrate strong capabilities in multimodal understanding, reasoning, and interaction but still face the fundamental limitation of hallucinations, where they generate erroneous or fabricated information. To mitigate hallucinations, existing methods annotate pair-responses (one non-hallucination vs one hallucination) using manual methods or GPT-4V, and train alignment algorithms to improve the correspondence between images and text. More critically, an image description often involve multiple dimensions (e.g., object attributes, posture, and spatial relationships), making it challenging for the model to comprehensively learn multidimensional information from pair-responses. To this end, in this paper, we propose RRHFV, which is the first using rank-responses (one non-hallucination vs multiple ranking hallucinations) to mitigate multimodal hallucinations. Instead of using pair-responses to train the model, RRHF-V expands the number of hallucinatory responses, so that the responses with different scores in a rank-response enable the model to learn rich semantic information across various dimensions of the image. Further, we propose a scene graph-based approach to automatically construct rank-responses in a cost-effective and automatic manner. We also design a novel training objective based on rank loss and margin loss to balance the differences between hallucinatory responses within a rankresponse, thereby improving the model\u2019s image comprehension. Experiments on two MLLMs of different sizes and four widely used benchmarks demonstrate that RRHF-V is effective in mitigating hallucinations and outperforms the DPO method based on pair-responses.",
        "author": "Guoqing Chen; Fu Zhang; Jinghao Lin; Chenglong Lu; Jingwei Cheng",
        "authorids": "/g/guoqing-chen/; /f/fu-zhang/; /j/jinghao-lin/; /c/chenglong-lu/; /j/jingwei-cheng/",
        "bibtex": "@inproceedings{chen-etal-2025-rrhf,\n    title = \"{RRHF}-{V}: Ranking Responses to Mitigate Hallucinations in Multimodal Large Language Models with Human Feedback\",\n    author = \"Chen, Guoqing  and\n      Zhang, Fu  and\n      Lin, Jinghao  and\n      Lu, Chenglong  and\n      Cheng, Jingwei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.454/\",\n    pages = \"6798--6815\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.454.pdf",
        "site": "https://aclanthology.org/2025.coling-main.454/",
        "pdf_size": 1731308,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:SD6IapvB7uYJ:scholar.google.com/&scioq=RRHF-V:+Ranking+Responses+to+Mitigate+Hallucinations+in+Multimodal+Large+Language+Models+with+Human+Feedback&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China",
        "aff_domain": "gmail.com;mail.neu.edu.cn;mail.neu.edu.cn; ; ",
        "email": "gmail.com;mail.neu.edu.cn;mail.neu.edu.cn; ; ",
        "github": "https://github.com/chengq1001/RRHF-V",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.neu.edu.cn/",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.444",
        "title": "RUAccent: Advanced System for Stress Placement in Russian with Homograph Resolution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a novel approach to the problem of stress placement in Russian text, with a particular focus on resolving homographs. We introduce a comprehensive system that combines morphological analysis, context-aware neural models, and a specialized \u201c\u0401-fikator\u201d to accurately place stress in Russian words, including those with ambiguous pronunciations. Our system outperforms existing solutions, achieving a 0.96 accuracy on homographs and 0.97 accuracy on non-homograph words.",
        "author": "Denis Andreevich Petrov",
        "authorids": "/d/denis-andreevich-petrov/",
        "bibtex": "@inproceedings{petrov-2025-ruaccent,\n    title = \"{RUA}ccent: Advanced System for Stress Placement in {R}ussian with Homograph Resolution\",\n    author = \"Petrov, Denis Andreevich\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.444/\",\n    pages = \"6642--6648\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.444.pdf",
        "site": "https://aclanthology.org/2025.coling-main.444/",
        "pdf_size": 346350,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fJ6ms1WoSKsJ:scholar.google.com/&scioq=RUAccent:+Advanced+System+for+Stress+Placement+in+Russian+with+Homograph+Resolution&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2025.coling-main.645",
        "title": "RUIE: Retrieval-based Unified Information Extraction using Large Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Unified information extraction (UIE) aims to extract diverse structured information from unstructured text. While large language models (LLMs) have shown promise for UIE, they require significant computational resources and often struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based Unified Information Extraction), a framework that leverages in-context learning for efficient task generalization. RUIE introduces a novel demonstration selection mechanism combining LLM preferences with a keyword-enhanced reward model, and employs a bi-encoder retriever trained through contrastive learning and knowledge distillation. As the first trainable retrieval framework for UIE, RUIE serves as a universal plugin for various LLMs. Experimental results on eight held-out datasets demonstrate RUIE\u2019s effectiveness, with average F1-score improvements of 19.22 and 3.22 compared to instruction-tuning methods and other retrievers, respectively.",
        "author": "Xincheng Liao; Junwen Duan; Yixi Huang; Jianxin Wang",
        "authorids": "/x/xincheng-liao/; /j/junwen-duan/; /y/yixi-huang/; /j/jianxin-wang/",
        "bibtex": "@inproceedings{liao-etal-2025-ruie,\n    title = \"{RUIE}: Retrieval-based Unified Information Extraction using Large Language Model\",\n    author = \"Liao, Xincheng  and\n      Duan, Junwen  and\n      Huang, Yixi  and\n      Wang, Jianxin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.645/\",\n    pages = \"9640--9655\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.645.pdf",
        "site": "https://aclanthology.org/2025.coling-main.645/",
        "pdf_size": 656965,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4714654240868736764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, Hunan, China",
        "aff_domain": "csu.edu.cn;csu.edu.cn;csu.edu.cn;mail.csu.edu.cn",
        "email": "csu.edu.cn;csu.edu.cn;csu.edu.cn;mail.csu.edu.cn",
        "github": "https://github.com/OStars/RUIE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Central South University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.csu.edu.cn",
        "aff_unique_abbr": "CSU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Changsha",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.634",
        "title": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Evaluating LLM-generated text has become a key challenge, especially in domain-specific contexts like the medical field. This work introduces a novel evaluation methodology for LLM-generated medical explanatory arguments, relying on Proxy Tasks and rankings to closely align results with human evaluation criteria, overcoming the biases typically seen in LLMs used as judges. We demonstrate that the proposed evaluators are robust against adversarial attacks, including the assessment of non-argumentative text. Additionally, the human-crafted arguments needed to train the evaluators are minimized to just one example per Proxy Task. By examining multiple LLM-generated arguments, we establish a methodology for determining whether a Proxy Task is suitable for evaluating LLM-generated medical explanatory arguments, requiring only five examples and two human experts.",
        "author": "Iker De la Iglesia; Iakes Goenaga; Johanna Ramirez-Romero; Jose Maria Villa-Gonzalez; Josu Goikoetxea; Ander Barrena",
        "authorids": "/i/iker-de-la-iglesia/; /i/iakes-goenaga/; /j/johanna-ramirez-romero/; /j/jose-maria-villa-gonzalez/; /j/josu-goikoetxea/; /a/ander-barrena/",
        "bibtex": "@inproceedings{de-la-iglesia-etal-2025-ranking,\n    title = \"Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of {LLM}-Generated Medical Explanatory Arguments\",\n    author = \"De la Iglesia, Iker  and\n      Goenaga, Iakes  and\n      Ramirez-Romero, Johanna  and\n      Villa-Gonzalez, Jose Maria  and\n      Goikoetxea, Josu  and\n      Barrena, Ander\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.634/\",\n    pages = \"9456--9471\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.634.pdf",
        "site": "https://aclanthology.org/2025.coling-main.634/",
        "pdf_size": 2362767,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:SqGwCkYcRq0J:scholar.google.com/&scioq=Ranking+Over+Scoring:+Towards+Reliable+and+Robust+Automated+Evaluation+of+LLM-Generated+Medical+Explanatory+Arguments&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "HiTZ Center - Ixa, University of the Basque Country UPV/EHU; HiTZ Center - Ixa, University of the Basque Country UPV/EHU; Cruces University Hospital, (Barakaldo, Biscay, Spain); Cruces University Hospital, (Barakaldo, Biscay, Spain); HiTZ Center - Ixa, University of the Basque Country UPV/EHU; HiTZ Center - Ixa, University of the Basque Country UPV/EHU",
        "aff_domain": "ehu.eus;ehu.eus; ; ;ehu.eus;ehu.eus",
        "email": "ehu.eus;ehu.eus; ; ;ehu.eus;ehu.eus",
        "github": "https://github.com/hitz-zentroa/Ranking-Over-Scoring-COLING-2025",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "University of the Basque Country;Cruces University Hospital",
        "aff_unique_dep": "HiTZ Center - Ixa;",
        "aff_unique_url": "https://www.ehu.eus/en;",
        "aff_unique_abbr": "UPV/EHU;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Barakaldo",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-industry.12",
        "title": "Rationale-Guided Distillation for E-Commerce Relevance Classification: Bridging Large Language Models and Lightweight Cross-Encoders",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Accurately classifying the relevance of Query-Product pairs is critical in online retail stores such as Amazon, as displaying irrelevant products can harm user experience and reduce engagement. While Large Language Models (LLMs) excel at this task due to their broad knowledge and strong reasoning abilities. However, their high computational demands constrain their practical deployment in real-world applications. In this paper, we propose a novel distillation approach for e-commerce relevance classification that uses \u201crationales\u201d generated by LLMs to guide smaller cross-encoder models. These rationales capture key decision-making insights from LLMs, enhancing training efficiency and enabling the distillation to smaller cross-encoder models deployable in production without requiring the LLM. Our method achieves average ROC-AUC improvements of 1.4% on 9 multilingual e-commerce datasets, 2.4% on 3 ESCI datasets, and 6% on GLUE datasets over vanilla cross-encoders. Our 110M parameter BERT model matches 7B parameter LLMs in performance (< 1% ROC-AUC difference) while being 50 times faster per sample.",
        "author": "Sanjay Agrawal; Faizan Ahemad; Vivek Varadarajan Sembium",
        "authorids": "/s/sanjay-agrawal/; /f/faizan-ahemad/; /v/vivek-varadarajan-sembium/",
        "bibtex": "@inproceedings{agrawal-etal-2025-rationale,\n    title = \"Rationale-Guided Distillation for {E}-Commerce Relevance Classification: Bridging Large Language Models and Lightweight Cross-Encoders\",\n    author = \"Agrawal, Sanjay  and\n      Ahemad, Faizan  and\n      Sembium, Vivek Varadarajan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.12/\",\n    pages = \"136--148\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.12.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.12/",
        "pdf_size": 2052392,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10401686719556422032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon, India; Amazon, India; Amazon, India",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.in",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.491",
        "title": "Re-Cent: A Relation-Centric Framework for Joint Zero-Shot Relation Triplet Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Zero-shot Relation Triplet Extraction (ZSRTE) aims to extract triplets from the context where the relation patterns are unseen during training. Due to the inherent challenges of the ZSRTE task, existing extractive ZSRTE methods often decompose it into named entity recognition and relation classification, which overlooks the interdependence of two tasks and may introduce error propagation. Motivated by the intuition that crucial entity attributes might be implicit in the relation labels, we propose a Relation-Centric joint ZSRTE method named Re-Cent. This approach uses minimal information, specifically unseen relation labels, to extract triplets in one go through a unified model. We develop two span-based extractors to identify the subjects and objects corresponding to relation labels, forming span-pairs. Additionally, we introduce a relation-based correction mechanism that further refines the triplets by calculating the relevance between span-pairs and relation labels. Experiments demonstrate that Re-Cent achieves state-of-the-art performance with fewer parameters and does not rely on synthetic data or manual labor.",
        "author": "Zehan Li; Fu Zhang; Kailun Lyu; Jingwei Cheng; Tianyue Peng",
        "authorids": "/z/zehan-li/; /f/fu-zhang/; /k/kailun-lyu/; /j/jingwei-cheng/; /t/tianyue-peng/",
        "bibtex": "@inproceedings{li-etal-2025-cent,\n    title = \"Re-Cent: A Relation-Centric Framework for Joint Zero-Shot Relation Triplet Extraction\",\n    author = \"Li, Zehan  and\n      Zhang, Fu  and\n      Lyu, Kailun  and\n      Cheng, Jingwei  and\n      Peng, Tianyue\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.491/\",\n    pages = \"7344--7354\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.491.pdf",
        "site": "https://aclanthology.org/2025.coling-main.491/",
        "pdf_size": 702023,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aYBHzfLRIpAJ:scholar.google.com/&scioq=Re-Cent:+A+Relation-Centric+Framework+for+Joint+Zero-Shot+Relation+Triplet+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China; School of Computer Science and Engineering, Northeastern University, China+Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China",
        "aff_domain": "163.com;mail.neu.edu.cn;163.com;mail.neu.edu.cn;163.com",
        "email": "163.com;mail.neu.edu.cn;163.com;mail.neu.edu.cn;163.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.neu.edu.cn/",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.727",
        "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Distantly-Supervised Named Entity Recognition (DS-NER) uses knowledge bases or dictionaries for annotations, reducing manual efforts but rely on large human labeled validation set. In this paper, we introduce a real-life DS-NER dataset, QTL, where the training data is annotated using domain dictionaries and the test data is annotated by domain experts. This dataset has a small validation set, reflecting real-life scenarios. Existing DS-NER approaches fail when applied to QTL, which motivate us to re-examine existing DS-NER approaches. We found that many of them rely on large validation sets and some used test set for tuning inappropriately. To solve this issue, we proposed a new approach, token-level Curriculum-based Positive-Unlabeled Learning (CuPUL), which uses curriculum learning to order training samples from easy to hard. This method stabilizes training, making it robust and effective on small validation sets. CuPUL also addresses false negative issues using the Positive-Unlabeled learning paradigm, demonstrating improved performance in real-life applications.",
        "author": "Yuepei Li; Kang Zhou; Qiao Qiao; Qing Wang; Qi Li",
        "authorids": "/y/yuepei-li/; /k/kang-zhou/; /q/qiao-qiao/; /q/qing-wang/; /q/qi-li/",
        "bibtex": "@inproceedings{li-etal-2025-examine,\n    title = \"Re-Examine Distantly Supervised {NER}: A New Benchmark and a Simple Approach\",\n    author = \"Li, Yuepei  and\n      Zhou, Kang  and\n      Qiao, Qiao  and\n      Wang, Qing  and\n      Li, Qi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.727/\",\n    pages = \"10940--10959\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.727.pdf",
        "site": "https://aclanthology.org/2025.coling-main.727/",
        "pdf_size": 1964861,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5NaCZWJcGXEJ:scholar.google.com/&scioq=Re-Examine+Distantly+Supervised+NER:+A+New+Benchmark+and+a+Simple+Approach&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA",
        "aff_domain": "iastate.edu;iastate.edu;iastate.edu;iastate.edu;iastate.edu",
        "email": "iastate.edu;iastate.edu;iastate.edu;iastate.edu;iastate.edu",
        "github": "https://github.com/liyp0095/CuPUL",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Iowa State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iastate.edu",
        "aff_unique_abbr": "ISU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Ames",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.255",
        "title": "ReLayout: Towards Real-World Document Understanding via Layout-enhanced Pre-training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent approaches for visually-rich document understanding (VrDU) uses manually annotated semantic groups, where a semantic group encompasses all semantically relevant but not obviously grouped words. As OCR tools are unable to automatically identify such grouping, we argue that current VrDU approaches are unrealistic. We thus introduce a new variant of the VrDU task, real-world visually-rich document understanding (ReVrDU), that does not allow for using manually annotated semantic groups. We also propose a new method, ReLayout, compliant with the ReVrDU scenario, which learns to capture semantic grouping through arranging words and bringing the representations of words that belong to the potential same semantic group closer together. Our experimental results demonstrate the performance of existing methods is deteriorated with the ReVrDU task, while ReLayout shows superiour performance.",
        "author": "Zhouqiang Jiang; Bowen Wang; Junhao Chen; Yuta Nakashima",
        "authorids": "/z/zhouqiang-jiang/; /b/bowen-wang/; /j/junhao-chen/; /y/yuta-nakashima/",
        "bibtex": "@inproceedings{jiang-etal-2025-relayout,\n    title = \"{R}e{L}ayout: Towards Real-World Document Understanding via Layout-enhanced Pre-training\",\n    author = \"Jiang, Zhouqiang  and\n      Wang, Bowen  and\n      Chen, Junhao  and\n      Nakashima, Yuta\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.255/\",\n    pages = \"3778--3793\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.255.pdf",
        "site": "https://aclanthology.org/2025.coling-main.255/",
        "pdf_size": 3612442,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14775889909885672249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Osaka University, Japan; Osaka University, Japan; Osaka University, Japan; Osaka University, Japan",
        "aff_domain": "is.ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;is.ids.osaka-u.ac.jp;ids.osaka-u.ac.jp",
        "email": "is.ids.osaka-u.ac.jp;ids.osaka-u.ac.jp;is.ids.osaka-u.ac.jp;ids.osaka-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Osaka University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.76",
        "title": "Read Before Grounding: Scene Knowledge Visual Grounding via Multi-step Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Visual grounding (VG) is an important task in vision and language that involves understanding the mutual relationship between query terms and images. However, existing VG datasets typically use simple and intuitive textual descriptions, with limited attribute and spatial information between images and text. Recently, the Scene Knowledge Visual Grounding (SK-VG) task has been introduced, which constructs VG datasets using visual knowledge and relational referential expressions. Due to the length of textual visual knowledge and the complexity of the referential relationships between entities, previous models have struggled with this task. Therefore, we propose ReadVG, a zero-shot, plug-and-play method that leverages the robust language understanding capabilities of Large Language Models (LLMs) to transform long visual knowledge texts into concise, information-dense visual descriptions. To improve the accuracy of target localisation, we employ a multi-step parsing algorithm that can progressively extract the query targets and their features from the visual knowledge and relational referencing expressions, thereby assisting multimodal models to more accurately localise the target for grounding purposes. Extensive experiments and case studies show that our approach can significantly improve the performance of multimodal grounding models.",
        "author": "HaiXiang Zhu; Lixian Su; ShuangMing Mao; Jing Ye",
        "authorids": "/h/haixiang-zhu/; /l/lixian-su/; /s/shuangming-mao/; /j/jing-ye/",
        "bibtex": "@inproceedings{zhu-etal-2025-read,\n    title = \"Read Before Grounding: Scene Knowledge Visual Grounding via Multi-step Parsing\",\n    author = \"Zhu, HaiXiang  and\n      Su, Lixian  and\n      Mao, ShuangMing  and\n      Ye, Jing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.76/\",\n    pages = \"1136--1149\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.76.pdf",
        "site": "https://aclanthology.org/2025.coling-main.76/",
        "pdf_size": 3293095,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5552388319635550589&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Independent Researcher; Independent Researcher; Independent Researcher; Independent Researcher",
        "aff_domain": "gmail.com;gmail.com;suou.waseda.jp;gmail.com",
        "email": "gmail.com;gmail.com;suou.waseda.jp;gmail.com",
        "github": "https://github.com/xiang-xiang-zhu/ReadVG",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Independent Researcher",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2025.coling-main.642",
        "title": "RealSafe: Quantifying Safety Risks of Language Agents in Real-World",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present RealSafe, an innovative evaluation framework that aims to rigorously assess the safety and reliability of large language model (LLM) agents in real application scenarios. RealSafe tracks the behavior of LLM agents in fourteen different application scenarios utilizing three contexts - standard operations, ambiguous interactions, and malicious behaviors. For standard operations and ambiguous interactions, possible risks based on the agents\u2019 decision-making are categorized into high, medium and low levels to reveal safety problems arising even from non-malicious user instructions. In assessing malicious behavior, we evaluate six types of malicious attacks to test the LLM agents\u2019 ability to recognize and defend against clearly malicious intent. After evaluating over 1000 queries involving multiple LLMs, we concluded that GPT-4 performed best among all evaluated models. However, it still has several deficiencies. This discovery highlights the need to enhance sensitivity and response to different security threats when designing and developing LLM agents. RealSafe offers an empirical time frame for researchers and developers to better understand the security problems LLM agents might face in real deployment and offers specific directions and ideas for building safer and smarter LLM agents down the road.",
        "author": "Yingning Ma",
        "authorids": "/y/yingning-ma/",
        "bibtex": "@inproceedings{ma-2025-realsafe,\n    title = \"{R}eal{S}afe: Quantifying Safety Risks of Language Agents in Real-World\",\n    author = \"Ma, Yingning\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.642/\",\n    pages = \"9586--9617\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.642.pdf",
        "site": "https://aclanthology.org/2025.coling-main.642/",
        "pdf_size": 1172784,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12774116401842577764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "",
        "aff_domain": "gwu.edu",
        "email": "gwu.edu",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2025.coling-main.651",
        "title": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have exhibited remarkable few-shot learning capabilities and unified the paradigm of NLP tasks through the in-context learning (ICL) technique. Despite the success of ICL, the quality of the exemplar demonstrations can significantly influence the LLM\u2019s performance. Existing exemplar selection methods mainly focus on the semantic similarity between queries and candidate exemplars. On the other hand, the logical connections between reasoning steps can also be beneficial to depict the problem-solving process. This paper proposes a novel method named Reasoning Graph-enhanced Exemplar Retrieval (RGER). RGER first queries LLM to generate an initial response and then expresses intermediate problem-solving steps to a graph structure. After that, it employs a graph kernel to select exemplars with semantic and structural similarity. Extensive experiments demonstrate the structural relationship is helpful to the alignment of queries and candidate exemplars. The efficacy of RGER on mathematics and logical reasoning tasks showcases its superiority over state-of-the-art retrieval-based approaches.",
        "author": "Yukang Lin; Bingchen Zhong; Shuoran Jiang; Joanna Siebert; Qingcai Chen",
        "authorids": "/y/yukang-lin/; /b/bingchen-zhong/; /s/shuoran-jiang/; /j/joanna-siebert/; /q/qingcai-chen/",
        "bibtex": "@inproceedings{lin-etal-2025-reasoning,\n    title = \"Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning\",\n    author = \"Lin, Yukang  and\n      Zhong, Bingchen  and\n      Jiang, Shuoran  and\n      Siebert, Joanna  and\n      Chen, Qingcai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.651/\",\n    pages = \"9737--9759\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.651.pdf",
        "site": "https://aclanthology.org/2025.coling-main.651/",
        "pdf_size": 1165819,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11686956456145818468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Harbin Institute of Technology (Shenzhen), Shenzhen, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China + Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "gmail.com;stu.hit.edu.cn;gmail.com;yahoo.com;hit.edu.cn",
        "email": "gmail.com;stu.hit.edu.cn;gmail.com;yahoo.com;hit.edu.cn",
        "github": "https://github.com/Yukang-Lin/RGER",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Harbin Institute of Technology;Peng Cheng Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://en.hhit.edu.cn/;",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0;0;0;0;0+0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.211",
        "title": "Reasoning with Trees: Faithful Question Answering over Knowledge Graph",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have shown remarkable progress in reasoning capabilities, yet they still face challenges in complex, multi-step reasoning tasks. This study introduces Reasoning with Trees (RwT), a novel framework that synergistically integrates LLMs with knowledge graphs (KGs) to enhance reasoning performance and interpretability. RwT reformulates knowledge graph question answering (KGQA) as a discrete decision-making problem, leveraging Monte Carlo Tree Search (MCTS) to iteratively refine reasoning paths. This approach mirrors human-like reasoning by dynamically integrating the LLM\u2019s internal knowledge with external KG information. We propose a real-data guided iteration technique to train an evaluation model that assesses action values, improving the efficiency of the MCTS process. Experimental results on two benchmark KGQA datasets demonstrate that RwT significantly outperforms existing state-of-the-art methods, with an average performance improvement of 9.81%. Notably, RwT achieves these improvements without requiring complete retraining of the LLM, offering a more efficient and adaptable approach to enhancing LLM reasoning capabilities.",
        "author": "Tiesunlong Shen; Jin Wang; Xuejie Zhang; Erik Cambria",
        "authorids": "/t/tiesunlong-shen/; /j/jin-wang/; /x/xuejie-zhang/; /e/erik-cambria/",
        "bibtex": "@inproceedings{shen-etal-2025-reasoning,\n    title = \"Reasoning with Trees: Faithful Question Answering over Knowledge Graph\",\n    author = \"Shen, Tiesunlong  and\n      Wang, Jin  and\n      Zhang, Xuejie  and\n      Cambria, Erik\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.211/\",\n    pages = \"3138--3157\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.211.pdf",
        "site": "https://aclanthology.org/2025.coling-main.211/",
        "pdf_size": 1288182,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qhvoqQyqj1oJ:scholar.google.com/&scioq=Reasoning+with+Trees:+Faithful+Question+Answering+over+Knowledge+Graph&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China; College of Computing and Data Science, Nanyang Technological University, Singapore",
        "aff_domain": "mail.ynu.edu.cn;ynu.edu.cn;ynu.edu.cn;ntu.edu.sg",
        "email": "mail.ynu.edu.cn;ynu.edu.cn;ynu.edu.cn;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Yunnan University;Nanyang Technological University",
        "aff_unique_dep": "School of Information Science and Engineering;College of Computing and Data Science",
        "aff_unique_url": "http://www.ynu.edu.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "YNU;NTU",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Kunming;Singapore",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-main.181",
        "title": "Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Zero-shot event-relational reasoning is an important task in natural language processing, and existing methods jointly learn a variety of event-relational prefixes and inference-form prefixes to achieve such tasks. However, training prefixes consumes large computational resources and lacks interpretability. Additionally, learning various relational and inferential knowledge inefficiently exploits the connections between tasks. Therefore, we first propose a method for Reasoning-Oriented Locating and Editing (ROLE), which locates and edits the key modules of the language model for reasoning about event relations, enhancing interpretability and also resource-efficiently optimizing the reasoning ability. Subsequently, we propose a method for Analogy-Based Locating and Editing (ABLE), which efficiently exploits the similarities and differences between tasks to optimize the zero-shot reasoning capability. Experimental results show that ROLE improves interpretability and reasoning performance with reduced computational cost. ABLE achieves SOTA results in zero-shot reasoning.",
        "author": "Jingyao Tang; Lishuang Li; Liteng Mi; Haiming Wu; Hongbin Lu",
        "authorids": "/j/jingyao-tang/; /l/lishuang-li/; /l/liteng-mi/; /h/haiming-wu/; /h/hongbin-lu/",
        "bibtex": "@inproceedings{tang-etal-2025-reasoning,\n    title = \"Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning\",\n    author = \"Tang, Jingyao  and\n      Li, Lishuang  and\n      Mi, Liteng  and\n      Wu, Haiming  and\n      Lu, Hongbin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.181/\",\n    pages = \"2645--2657\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.181.pdf",
        "site": "https://aclanthology.org/2025.coling-main.181/",
        "pdf_size": 1275330,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:z9UY2qug39MJ:scholar.google.com/&scioq=Reasoning-Oriented+and+Analogy-Based+Methods+for+Locating+and+Editing+in+Zero-Shot+Event-Relational+Reasoning&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Artificial Intelligence, Liaoning Normal University",
        "aff_domain": "mail.dlut.edu.cn;dlut.edu.cn;mail.dlut.edu.cn;bit.edu.cn;163.com",
        "email": "mail.dlut.edu.cn;dlut.edu.cn;mail.dlut.edu.cn;bit.edu.cn;163.com",
        "github": "https://github.com/manderous/ROLE_ABLE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Dalian University of Technology;Beijing Institute of Technology;Liaoning Normal University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science and Technology;School of Computer Science and Artificial Intelligence",
        "aff_unique_url": "http://en.dlut.edu.cn/;http://www.bit.edu.cn/;http://www.lnu.edu.cn/",
        "aff_unique_abbr": "DUT;BIT;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Dalian;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.68",
        "title": "RecStream: Graph-aware Stream Management for Concurrent Recommendation Model Online Serving",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recommendation Models (RMs) are crucial for predicting user preferences and enhancing personalized experiences on large-scale platforms. As the application of recommendation models grows, optimizing their online serving performance has become a significant challenge. However, current serving systems perform poorly under highly concurrent scenarios. To address this, we introduce RecStream, a system designed to optimize stream configurations based on model characteristics for handling high concurrency requests. We employ a hybrid Graph Neural Network architecture to determine the best configurations for various RMs. Experimental results demonstrate that RecStream achieves significant performance improvements, reducing latency by up to 74%.",
        "author": "Shuxi Guo; Qi Qi; Haifeng Sun; Jianxin Liao; Jingyu Wang",
        "authorids": "/s/shuxi-guo/; /q/qi-qi/; /h/haifeng-sun/; /j/jianxin-liao/; /j/jingyu-wang/",
        "bibtex": "@inproceedings{guo-etal-2025-recstream,\n    title = \"{R}ec{S}tream: Graph-aware Stream Management for Concurrent Recommendation Model Online Serving\",\n    author = \"Guo, Shuxi  and\n      Qi, Qi  and\n      Sun, Haifeng  and\n      Liao, Jianxin  and\n      Wang, Jingyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.68/\",\n    pages = \"817--826\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.68.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.68/",
        "pdf_size": 401701,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wPAUcGXTepcJ:scholar.google.com/&scioq=RecStream:+Graph-aware+Stream+Management+for+Concurrent+Recommendation+Model+Online+Serving&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;gmail.com;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;gmail.com;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.344",
        "title": "Refer to the Reference: Reference-focused Synthetic Automatic Post-Editing Data Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A prevalent approach to synthetic APE data generation uses source (src) sentences in a parallel corpus to obtain translations (mt) through an MT system and treats corresponding reference (ref) sentences as post-edits (pe). While effective, due to independence between \u2018mt\u2019 and \u2018pe,\u2019 these translations do not adequately reflect errors to be corrected by a human post-editor. Thus, we introduce a novel and simple yet effective reference-focused synthetic APE data generation technique that uses \u2018ref\u2019 instead of src\u2019 sentences to obtain corrupted translations (mt_new). The experimental results across English-German, English-Russian, English-Marathi, English-Hindi, and English-Tamil language pairs demonstrate the superior performance of APE systems trained using the newly generated synthetic data compared to those trained using existing synthetic data. Further, APE models trained using a balanced mix of existing and newly generated synthetic data achieve improvements of 0.37, 0.19, 1.01, 2.42, and 2.60 TER points, respectively. We will release the generated synthetic APE data.",
        "author": "Sourabh Deoghare; Diptesh Kanojia; Pushpak Bhattacharyya",
        "authorids": "/s/sourabh-deoghare/; /d/diptesh-kanojia/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{deoghare-etal-2025-refer,\n    title = \"Refer to the Reference: Reference-focused Synthetic Automatic Post-Editing Data Generation\",\n    author = \"Deoghare, Sourabh  and\n      Kanojia, Diptesh  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.344/\",\n    pages = \"5123--5135\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.344.pdf",
        "site": "https://aclanthology.org/2025.coling-main.344/",
        "pdf_size": 945466,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mLar4D4xIA4J:scholar.google.com/&scioq=Refer+to+the+Reference:+Reference-focused+Synthetic+Automatic+Post-Editing+Data+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "CFILT, Indian Institute of Technology Bombay, Mumbai, India; Institute for People-Centred AI, University of Surrey, United Kingdom; CFILT, Indian Institute of Technology Bombay, Mumbai, India",
        "aff_domain": "cse.iitb.ac.in;surrey.ac.uk;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;surrey.ac.uk;cse.iitb.ac.in",
        "github": "1Github Repository",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay;University of Surrey",
        "aff_unique_dep": "CFILT;Institute for People-Centred AI",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.surrey.ac.uk",
        "aff_unique_abbr": "IIT Bombay;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mumbai;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "India;United Kingdom"
    },
    {
        "id": "2025.coling-main.52",
        "title": "Refined Evaluation for End-to-End Grammatical Error Correction Using an Alignment-Based Approach",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a refined alignment-based method to assess end-to-end grammatical error correction (GEC) systems, aiming to reproduce and improve results from existing evaluation tools, such as errant, even when applied to raw text input\u2014reflecting real-world language learners\u2019 writing scenarios. Our approach addresses challenges arising from sentence boundary detection deviations in text preprocessing, a factor overlooked by current GEC evaluation metrics. We demonstrate its effectiveness by replicating results through a re-implementation of errant, utilizing stanza for error annotation and simulating end-to-end evaluation from raw text. Additionally, we propose a potential multilingual errant, presenting Chinese and Korean GEC results. Previously, Chinese and Korean errant were implemented independently for each language, with different annotation formats. Our approach generates consistent error annotations across languages, establishing a basis for standardized grammatical error annotation and evaluation in multilingual GEC contexts.",
        "author": "Junrui Wang; Mengyang Qiu; Yang Gu; Zihao Huang; Jungyeul Park",
        "authorids": "/j/junrui-wang/; /m/mengyang-qiu/; /y/yang-gu/; /z/zihao-huang/; /j/jungyeul-park/",
        "bibtex": "@inproceedings{wang-etal-2025-refined,\n    title = \"Refined Evaluation for End-to-End Grammatical Error Correction Using an Alignment-Based Approach\",\n    author = \"Wang, Junrui  and\n      Qiu, Mengyang  and\n      Gu, Yang  and\n      Huang, Zihao  and\n      Park, Jungyeul\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.52/\",\n    pages = \"774--785\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.52.pdf",
        "site": "https://aclanthology.org/2025.coling-main.52/",
        "pdf_size": 514689,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13843496638155368951&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Linguistics, The University of British Columbia, Canada + TikTok, Canada; Department of Psychology, Trent University, Canada + Open Writing Evaluation, France; Open Writing Evaluation, France; Open Writing Evaluation, France; Department of Linguistics, The University of British Columbia, Canada + Open Writing Evaluation, France",
        "aff_domain": "ubc.ca;trentu.ca;open-writing-evaluation.fr;open-writing-evaluation.fr;ubc.ca",
        "email": "ubc.ca;trentu.ca;open-writing-evaluation.fr;open-writing-evaluation.fr;ubc.ca",
        "github": "",
        "project": "http://open-writing-evaluation.github.io",
        "author_num": 5,
        "aff_unique_index": "0+1;2+3;3;3;0+3",
        "aff_unique_norm": "The University of British Columbia;TikTok;Trent University;Open Writing Evaluation",
        "aff_unique_dep": "Department of Linguistics;;Department of Psychology;",
        "aff_unique_url": "https://www.ubc.ca;https://www.tiktok.com;https://www.trentu.ca;",
        "aff_unique_abbr": "UBC;TikTok;Trent;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+1;1;1;0+1",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "2025.coling-main.88",
        "title": "Relation Logical Reasoning and Relation-aware Entity Encoding for Temporal Knowledge Graph Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Temporal Knowledge Graph Reasoning (TKGR) aims to predict future facts based on historical data. Current mainstream models primarily use embedding techniques, which predict missing facts by representing entities and relations as low-dimensional vectors. However, these models often consider only the structural information of individual entities and relations, overlooking the broader structure of the entire TKG. To address these limitations, we propose a novel model called Relation Logical Reasoning and Relation-aware Entity Encoding (RLEE), drawing inspiration from attention mechanisms and logical rule-based techniques. RLEE introduces a two-layer representation of the TKG: an entity layer and a relation layer. At the relation layer, we extract relation paths to mine potential logical correlations between different relations, learning relation embeddings through a process of relation logical reasoning. At the entity layer, we use the relation-aware attention mechanism to learn the entity embeddings specific to the predicted query relations. These learned relation and entity embeddings are then used to predict facts at future timestamps. When evaluated on five commonly used public datasets, RLEE consistently outperforms state-of-the-art baselines.",
        "author": "Longzhou Liu; Chenglong Xiao; Shanshan Wang; Tingwen Liu",
        "authorids": "/l/longzhou-liu/; /c/chenglong-xiao/; /s/shanshan-wang/; /t/tingwen-liu/",
        "bibtex": "https://aclanthology.org/2025.coling-main.88.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.88.pdf",
        "site": "https://aclanthology.org/2025.coling-main.88/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ACEoTWkB56EJ:scholar.google.com/&scioq=Relation+Logical+Reasoning+and+Relation-aware+Entity+Encoding+for+Temporal+Knowledge+Graph+Reasoning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-demos.6",
        "title": "Reliable, Reproducible, and Really Fast Leaderboards with Evalica",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The rapid advancement of natural language processing (NLP) technologies, such as instruction-tuned large language models (LLMs), urges the development of modern evaluation protocols with human and machine feedback. We introduce Evalica, an open-source toolkit that facilitates the creation of reliable and reproducible model leaderboards. This paper presents its design, evaluates its performance, and demonstrates its usability through its Web interface, command-line interface, and Python API.",
        "author": "Dmitry Ustalov",
        "authorids": "/d/dmitry-ustalov/",
        "bibtex": "@inproceedings{ustalov-2025-reliable,\n    title = \"Reliable, Reproducible, and Really Fast Leaderboards with Evalica\",\n    author = \"Ustalov, Dmitry\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.6/\",\n    pages = \"46--53\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.6.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.6/",
        "pdf_size": 891168,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6aaBnKBJoyEJ:scholar.google.com/&scioq=Reliable,+Reproducible,+and+Really+Fast+Leaderboards+with+Evalica&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "JetBrains / Belgrade, Serbia",
        "aff_domain": "jetbrains.com",
        "email": "jetbrains.com",
        "github": "https://github.com/dustalov/evalica",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "JetBrains",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jetbrains.com",
        "aff_unique_abbr": "JetBrains",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Belgrade",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Serbia"
    },
    {
        "id": "2025.coling-main.418",
        "title": "Representation Purification for End-to-End Speech Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Speech-to-text translation (ST) is a cross-modal task that involves converting spoken language into text in a different language. Previous research primarily focused on enhancing speech translation by facilitating knowledge transfer from machine translation, exploring various methods to bridge the gap between speech and text modalities. Despite substantial progress made, factors in speech that are not relevant to translation content, such as timbre and rhythm, often limit the efficiency of knowledge transfer. In this paper, we conceptualize speech representation as a combination of content-agnostic and content-relevant factors. We examine the impact of content-agnostic factors on translation performance through preliminary experiments and observe a significant performance deterioration when content-agnostic perturbations are introduced to speech signals. To address this issue, we propose a **S**peech **R**epresentation **P**urification with **S**upervision **E**nhancement (SRPSE) framework, which excludes the content-agnostic components within speech representations to mitigate their negative impact on ST. Experiments on MuST-C and CoVoST-2 datasets demonstrate that SRPSE significantly improves translation performance across all translation directions in three settings and achieves preeminent performance under a *transcript-free* setting.",
        "author": "Chengwei Zhang; Yue Zhou; Rui Zhao; Yidong Chen; Xiaodong Shi",
        "authorids": "/c/chengwei-zhang/; /y/yue-zhou/; /r/rui-zhao/; /y/yidong-chen/; /x/xiaodong-shi/",
        "bibtex": "@inproceedings{zhang-etal-2025-representation,\n    title = \"Representation Purification for End-to-End Speech Translation\",\n    author = \"Zhang, Chengwei  and\n      Zhou, Yue  and\n      Zhao, Rui  and\n      Chen, Yidong  and\n      Shi, Xiaodong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.418/\",\n    pages = \"6255--6269\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.418.pdf",
        "site": "https://aclanthology.org/2025.coling-main.418/",
        "pdf_size": 3115818,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9gC35H6Rw_kJ:scholar.google.com/&scioq=Representation+Purification+for+End-to-End+Speech+Translation&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "School of Informatics, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, China+Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, China+Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, China+Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, China+Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, China",
        "aff_domain": "stu.xmu.edu.cn;stu.xmu.edu.cn;stu.xmu.edu.cn;xmu.edu.cn;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;stu.xmu.edu.cn;stu.xmu.edu.cn;xmu.edu.cn;xmu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+0;0+1+0;0+1+0;0+1+0;0+1",
        "aff_unique_norm": "Xiamen University;Ministry of Culture and Tourism",
        "aff_unique_dep": "School of Informatics;Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan",
        "aff_unique_url": "https://www.xmu.edu.cn;",
        "aff_unique_abbr": "XMU;",
        "aff_campus_unique_index": "1;1;1;1;",
        "aff_campus_unique": ";Xiamen",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.278",
        "title": "Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing Thai Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid advancement of large language models (LLMs) has highlighted the need for robust evaluation frameworks that assess their core capabilities, such as reasoning, knowledge, and commonsense, leading to the inception of certain widely-used benchmark suites such as the H6 benchmark. However, these benchmark suites are primarily built for the English language, and there exists a lack thereof for under-represented languages, in terms of LLM development, such as Thai. On the other hand, developing LLMs for Thai should also include enhancing the cultural understanding as well as core capabilities. To address these dual challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough evaluation of various LLMs with multi-lingual capabilities, we provide a comprehensive analysis of the proposed benchmarks and how they contribute to Thai LLM development. Furthermore, we have made both the datasets and evaluation code publicly available to encourage further research and development for Thai LLMs.",
        "author": "Dahyun Kim; Sukyung Lee; Yungi Kim; Attapol Rutherford; Chanjun Park",
        "authorids": "/d/dahyun-kim/; /s/sukyung-lee/; /y/yungi-kim/; /a/attapol-rutherford/; /c/chanjun-park/",
        "bibtex": "@inproceedings{kim-etal-2025-representing,\n    title = \"Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing {T}hai Large Language Models\",\n    author = \"Kim, Dahyun  and\n      Lee, Sukyung  and\n      Kim, Yungi  and\n      Rutherford, Attapol  and\n      Park, Chanjun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.278/\",\n    pages = \"4114--4129\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.278.pdf",
        "site": "https://aclanthology.org/2025.coling-main.278/",
        "pdf_size": 2763801,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:vHjrG5eNfTEJ:scholar.google.com/&scioq=Representing+the+Under-Represented:+Cultural+and+Core+Capability+Benchmarks+for+Developing+Thai+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Twelve Labs; Upstage AI; Upstage AI; Chulalongkorn University; Korea University",
        "aff_domain": "twelvelabs.io;upstage.ai;upstage.ai;chula.ac.th;korea.ac.kr",
        "email": "twelvelabs.io;upstage.ai;upstage.ai;chula.ac.th;korea.ac.kr",
        "github": "https://github.com/UpstageAI/ThaiCLI_H6",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;3",
        "aff_unique_norm": "Twelve Labs;Upstage AI;Chulalongkorn University;Korea University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://twelvelabs.com;;http://www.chula.ac.th;https://www.korea.ac.kr",
        "aff_unique_abbr": ";;CU;KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;3",
        "aff_country_unique": "United States;;Thailand;South Korea"
    },
    {
        "id": "2025.coling-industry.20",
        "title": "Resource-Efficient Anonymization of Textual Data via Knowledge Distillation from Large Language Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Protecting personal and sensitive information in textual data is increasingly crucial, especially when leveraging large language models (LLMs) that may pose privacy risks due to their API-based access. We introduce a novel approach and pipeline for anonymizing text across arbitrary domains without the need for manually labeled data or extensive computational resources. Our method employs knowledge distillation from LLMs into smaller encoder-only models via named entity recognition (NER) coupled with regular expressions to create a lightweight model capable of effective anonymization while preserving the semantic and contextual integrity of the data. This reduces computational overhead, enabling deployment on less powerful servers or even personal computing devices. Our findings suggest that knowledge distillation offers a scalable, resource-efficient pathway for anonymization, balancing privacy preservation with model performance and computational efficiency.",
        "author": "Tobias Deu\u00dfer; Max Hahnb\u00fcck; Tobias Uelwer; Cong Zhao; Christian Bauckhage; Rafet Sifa",
        "authorids": "/t/tobias-deusser/; /m/max-hahnbuck/; /t/tobias-uelwer/; /c/cong-zhao/; /c/christian-bauckhage/; /r/rafet-sifa/",
        "bibtex": "@inproceedings{deusser-etal-2025-resource,\n    title = \"Resource-Efficient Anonymization of Textual Data via Knowledge Distillation from Large Language Models\",\n    author = {Deu{\\ss}er, Tobias  and\n      Hahnb{\\\"u}ck, Max  and\n      Uelwer, Tobias  and\n      Zhao, Cong  and\n      Bauckhage, Christian  and\n      Sifa, Rafet},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.20/\",\n    pages = \"243--250\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.20.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.20/",
        "pdf_size": 307349,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MYsEtSPPlFgJ:scholar.google.com/&scioq=Resource-Efficient+Anonymization+of+Textual+Data+via+Knowledge+Distillation+from+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Bonn, Bonn, Germany + Fraunhofer IAIS, Sankt Augustin, Germany; University of Bonn, Bonn, Germany + Fraunhofer IAIS, Sankt Augustin, Germany; Fraunhofer IAIS, Sankt Augustin, Germany; University of Bonn, Bonn, Germany + Fraunhofer IAIS, Sankt Augustin, Germany; University of Bonn, Bonn, Germany + Fraunhofer IAIS, Sankt Augustin, Germany; University of Bonn, Bonn, Germany + Fraunhofer IAIS, Sankt Augustin, Germany",
        "aff_domain": "uni-bonn.de; ; ; ; ; ",
        "email": "uni-bonn.de; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;1;0+1;0+1;0+1",
        "aff_unique_norm": "University of Bonn;Fraunhofer Institute for Applied Information Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de;https://www.iais.fraunhofer.de/",
        "aff_unique_abbr": "UBonn;Fraunhofer IAIS",
        "aff_campus_unique_index": "0+1;0+1;1;0+1;0+1;0+1",
        "aff_campus_unique": "Bonn;Sankt Augustin",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.383",
        "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.",
        "author": "Taiqiang Wu; Chaofan Tao; Jiahao Wang; Runming Yang; Zhe Zhao; Ngai Wong",
        "authorids": "/t/taiqiang-wu/; /c/chaofan-tao/; /j/jiahao-wang/; /r/runming-yang/; /z/zhe-zhao/; /n/ngai-wong/",
        "bibtex": "@inproceedings{wu-etal-2025-rethinking,\n    title = \"Rethinking {K}ullback-{L}eibler Divergence in Knowledge Distillation for Large Language Models\",\n    author = \"Wu, Taiqiang  and\n      Tao, Chaofan  and\n      Wang, Jiahao  and\n      Yang, Runming  and\n      Zhao, Zhe  and\n      Wong, Ngai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.383/\",\n    pages = \"5737--5755\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.383.pdf",
        "site": "https://aclanthology.org/2025.coling-main.383/",
        "pdf_size": 509685,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2590095351150796117&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.131",
        "title": "Rethinking Long Context Generation from the Continual Learning Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Due to the limited context window, Large Language Models (LLMs) struggle with processing long contexts. Although fine-tuning can extend the context window, it incurs substantial computation costs. In contrast, recent tuning-free approaches reallocate the attention mechanism or incorporate temporary trainable parameters. In this work, by jointly modeling instance-level generation with a limited context window and learning over sequential data, we rethink the long context generation of LLMs from a continual learning perspective. In practice, we inspect existing representative approaches and analyze their synergy with continual learning strategies. Moreover, we integrate these strategies into current approaches to further boost LLMs\u2019 efficiency in processing long contexts. Comprehensive experiments and analysis confirm the feasibility of continual learning insights for improving long-context processing.",
        "author": "Zeyuan Yang; Fangzhou Xiong; Peng Li; Yang Liu",
        "authorids": "/z/zeyuan-yang/; /f/fangzhou-xiong/; /p/peng-li/; /y/yang-liu/",
        "bibtex": "@inproceedings{yang-etal-2025-rethinking,\n    title = \"Rethinking Long Context Generation from the Continual Learning Perspective\",\n    author = \"Yang, Zeyuan  and\n      Xiong, Fangzhou  and\n      Li, Peng  and\n      Liu, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.131/\",\n    pages = \"1922--1933\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.131.pdf",
        "site": "https://aclanthology.org/2025.coling-main.131/",
        "pdf_size": 1637627,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12147258144014850664&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.197",
        "title": "Rethinking Vocabulary Augmentation: Addressing the Challenges of Low-Resource Languages in Multilingual Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The performance of multilingual language models (MLLMs) is notably inferior for low-resource languages (LRL) compared to high-resource ones, primarily due to the limited available corpus during the pre-training phase. This inadequacy stems from the under-representation of low-resource language words in the subword vocabularies of MLLMs, leading to their misidentification as unknown or incorrectly concatenated subwords. Previous approaches are based on frequency sorting to select words for augmenting vocabularies. However, these methods overlook the fundamental disparities between model representation distributions and frequency distributions. To address this gap, we introduce a novel Entropy-Consistency Word Selection (ECWS) method, which integrates semantic and frequency metrics for vocabulary augmentation. Our results indicate an improvement in performance, supporting our approach as a viable means to enrich vocabularies inadequately represented in current MLLMs.",
        "author": "Nankai Lin; Peijian Zeng; Weixiong Zheng; Shengyi Jiang; Dong Zhou; Aimin Yang",
        "authorids": "/n/nankai-lin/; /p/peijian-zeng/; /w/weixiong-zheng/; /s/shengyi-jiang/; /d/dong-zhou/; /a/aimin-yang/",
        "bibtex": "@inproceedings{lin-etal-2025-rethinking,\n    title = \"Rethinking Vocabulary Augmentation: Addressing the Challenges of Low-Resource Languages in Multilingual Models\",\n    author = \"Lin, Nankai  and\n      Zeng, Peijian  and\n      Zheng, Weixiong  and\n      Jiang, Shengyi  and\n      Zhou, Dong  and\n      Yang, Aimin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.197/\",\n    pages = \"2919--2934\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.197.pdf",
        "site": "https://aclanthology.org/2025.coling-main.197/",
        "pdf_size": 1648295,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bIA1aCe9PRUJ:scholar.google.com/&scioq=Rethinking+Vocabulary+Augmentation:+Addressing+the+Challenges+of+Low-Resource+Languages+in+Multilingual+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Science and Technology, Guangdong University of Foreign Studies; School of Computer Science and Technology, Guangdong University of Technology; School of Information Technology and Engineering, Guangzhou College of Commerce; School of Information Technology and Engineering, Guangzhou College of Commerce; School of Information Science and Technology, Guangdong University of Foreign Studies + School of Computer Science and Intelligence Education, Lingnan Normal University; School of Computer Science and Technology, Guangdong University of Technology + School of Computer Science and Intelligence Education, Lingnan Normal University",
        "aff_domain": "gdufs.edu.cn; ; ; ;163.com; ",
        "email": "gdufs.edu.cn; ; ; ;163.com; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;2;0+3;1+3",
        "aff_unique_norm": "Guangdong University of Foreign Studies;Guangdong University of Technology;Guangzhou College of Commerce;Lingnan Normal University",
        "aff_unique_dep": "School of Information Science and Technology;School of Computer Science and Technology;School of Information Technology and Engineering;School of Computer Science and Intelligence Education",
        "aff_unique_url": "http://www.gdufs.edu.cn;http://www.gdut.edu.cn;http://www.gzcc.edu.cn;http://www.lnu.edu.cn/",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "1;1;;",
        "aff_campus_unique": ";Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.136",
        "title": "Rethinking the Alignment of Psychotherapy Dialogue Generation with Motivational Interviewing Strategies",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have shown promise in generating psychotherapeutic dialogues, particularly in the context of motivational interviewing (MI). However, the inherent lack of transparency in LLM outputs presents significant challenges given the sensitive nature of psychotherapy. Applying MI strategies, a set of MI skills, to generate more controllable therapeutic-adherent conversations with explainability provides a possible solution. In this work, we explore the alignment of LLMs with MI strategies by first prompting the LLMs to predict the appropriate strategies as reasoning and then utilizing these strategies to guide the subsequent dialogue generation. We seek to investigate whether such alignment leads to more controllable and explainable generations. Multiple experiments including automatic and human evaluations are conducted to validate the effectiveness of MI strategies in aligning psychotherapy dialogue generation. Our findings demonstrate the potential of LLMs in producing strategically aligned dialogues and suggest directions for practical applications in psychotherapeutic settings.",
        "author": "Xin Sun; Xiao Tang; Abdallah El Ali; Zhuying Li; Pengjie Ren; Jan de Wit; Jiahuan Pei; Jos A.Bosch",
        "authorids": "/x/xin-sun/; /x/xiao-tang/; /a/abdallah-el-ali/; /z/zhuying-li/; /p/pengjie-ren/; /j/jan-de-wit/; /j/jiahuan-pei/; /j/jos-a-bosch/",
        "bibtex": "@inproceedings{sun-etal-2025-rethinking,\n    title = \"Rethinking the Alignment of Psychotherapy Dialogue Generation with Motivational Interviewing Strategies\",\n    author = \"Sun, Xin  and\n      Tang, Xiao  and\n      El Ali, Abdallah  and\n      Li, Zhuying  and\n      Ren, Pengjie  and\n      de Wit, Jan  and\n      Pei, Jiahuan  and\n      A.Bosch, Jos\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.136/\",\n    pages = \"1983--2002\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.136.pdf",
        "site": "https://aclanthology.org/2025.coling-main.136/",
        "pdf_size": 2286446,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12224842724293276824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Amsterdam, the Netherlands + Centrum Wiskunde & Informatica (CWI), the Netherlands; Southeast University, China; Centrum Wiskunde & Informatica (CWI), the Netherlands + Utrecht University, the Netherlands; Southeast University, China; Shandong University, China; Tilburg University, the Netherlands; Vrije Universiteit Amsterdam, the Netherlands; University of Amsterdam, the Netherlands",
        "aff_domain": "uva.nl; ;cwi.nl; ; ; ; ;uva.nl",
        "email": "uva.nl; ;cwi.nl; ; ; ; ;uva.nl",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;1+3;2;4;5;6;0",
        "aff_unique_norm": "University of Amsterdam;Centrum Wiskunde & Informatica;Southeast University;Utrecht University;Shandong University;Tilburg University;Vrije Universiteit Amsterdam",
        "aff_unique_dep": ";;;;;;",
        "aff_unique_url": "https://www.uva.nl;https://www.cwi.nl/;https://www.seu.edu.cn/;https://www.uu.nl;http://www.sdu.edu.cn;https://www.tilburguniversity.edu;https://www.vu.nl",
        "aff_unique_abbr": "UvA;CWI;SEU;UU;SDU;Tilburg U;VU Amsterdam",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;1;1;0;0;0",
        "aff_country_unique": "Netherlands;China"
    },
    {
        "id": "2025.coling-main.204",
        "title": "Rethinking-based Code Summarization with Chain of Comments",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic code summarization aims to generate concise natural language descriptions (summary) for source code, which can free software developers from the heavy burden of manual commenting and software maintenance. Existing methods focus on learning a direct mapping from pure code to summaries, overlooking the significant heterogeneity gap between code and summary. Moreover, existing methods lack a human-like re-check process to evaluate whether the generated summaries match well with the code. To address these two limitations, we introduce RBCoSum, a novel framework that incorporates the generated Chain Of Comments (COC) as auxiliary intermediate information for the model to bridge the gap between code and summaries. Also, we propose a rethinking process where a learned ranker trained on our constructed ranking dataset scores the extent of matching between the generated summary and the code, selecting the highest-scoring summary to achieve a re-check process. We conduct extensive experiments to evaluate our approach and compare it with other automatic code summarization models as well as multiple code Large Language Models (LLMs). The experimental results show that RBCoSum is effective and outperforms baselines by a large margin. The human evaluation also proves the summaries generated with RBCoSum are more natural, informative, useful, and truthful.",
        "author": "Liuwen Cao; Hongkui He; Hailin Huang; Jiexin Wang; Yi Cai",
        "authorids": "/l/liuwen-cao/; /h/hongkui-he/; /h/hailin-huang/; /j/jiexin-wang/; /y/yi-cai/",
        "bibtex": "@inproceedings{cao-etal-2025-rethinking,\n    title = \"Rethinking-based Code Summarization with Chain of Comments\",\n    author = \"Cao, Liuwen  and\n      He, Hongkui  and\n      Huang, Hailin  and\n      Wang, Jiexin  and\n      Cai, Yi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.204/\",\n    pages = \"3043--3056\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.204.pdf",
        "site": "https://aclanthology.org/2025.coling-main.204/",
        "pdf_size": 1053203,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NF8VDsx6lUcJ:scholar.google.com/&scioq=Rethinking-based+Code+Summarization+with+Chain+of+Comments&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (South China University of Technology) Ministry of Education; School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (South China University of Technology) Ministry of Education; School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (South China University of Technology) Ministry of Education; School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (South China University of Technology) Ministry of Education; School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (South China University of Technology) Ministry of Education",
        "aff_domain": "scut.edu.cn; ; ; ;scut.edu.cn",
        "email": "scut.edu.cn; ; ; ;scut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "South China University of Technology",
        "aff_unique_dep": "School of Software Engineering",
        "aff_unique_url": "https://www.scut.edu.cn",
        "aff_unique_abbr": "SCUT",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.196",
        "title": "Retrieval Augmented Instruction Tuning for Open NER with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The strong capability of large language models (LLMs) has been applied to information extraction (IE) through either retrieval augmented prompting or instruction tuning (IT). However, the best way to incorporate information with LLMs for IE remains an open question. In this paper, we explore Retrieval Augmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named entity recognition (NER). Specifically, for each training sample, we retrieve semantically similar examples from the training dataset as the context and prepend them to the input of the original instruction. To evaluate our RA-IT approach more thoroughly, we construct a Chinese IT dataset for open NER and evaluate RA-IT in both English and Chinese scenarios. Experimental results verify the effectiveness of RA-IT across various data sizes and in both English and Chinese scenarios. We also conduct thorough studies to explore the impacts of various retrieval strategies in the proposed RA-IT framework.",
        "author": "Tingyu Xie; Jian Zhang; Yan Zhang; Yuanyuan Liang; Qi Li; Hongwei Wang",
        "authorids": "/t/tingyu-xie/; /j/jian-zhang/; /y/yan-zhang/; /y/yuanyuan-liang/; /q/qi-li/; /h/hongwei-wang/",
        "bibtex": "@inproceedings{xie-etal-2025-retrieval,\n    title = \"Retrieval Augmented Instruction Tuning for Open {NER} with Large Language Models\",\n    author = \"Xie, Tingyu  and\n      Zhang, Jian  and\n      Zhang, Yan  and\n      Liang, Yuanyuan  and\n      Li, Qi  and\n      Wang, Hongwei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.196/\",\n    pages = \"2904--2918\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.196.pdf",
        "site": "https://aclanthology.org/2025.coling-main.196/",
        "pdf_size": 1741030,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6670413230182402674&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Zhejiang University; Zhejiang University; National University of Singapore; East China Normal University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;gmail.com; ;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;gmail.com; ;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Zhejiang University;National University of Singapore;East China Normal University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.nus.edu.sg;http://www.ecnu.edu.cn",
        "aff_unique_abbr": "ZJU;NUS;ECNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-main.717",
        "title": "Retrieval-Augmented Generation for Large Language Model based Few-shot Chinese Spell Checking",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) are naturally suitable for Chinese spelling check (CSC) task in few-shot scenarios due to their powerful semantic understanding and few-shot learning capabilities. Recent CSC research has begun to use LLMs as foundational models. However, most current datasets are primarily focused on errors generated during the text generation process, with little attention given to errors occurring in the modal conversion process. Furthermore, existing LLM-based CSC methods often rely on fixed prompt samples, which limits the performance of LLMs. Therefore, we propose a framework named RagID (Retrieval-Augment Generation and Iterative Discriminator Strategy). By utilizing semantic-based similarity search and an iterative discriminator mechanism, RagID can provide well-chosen prompt samples and reduce over-correction issues in LLM-based CSC. RagID demonstrates excellent effectiveness in few-shot scenarios. We conducted comprehensive experiments, and the results show that RagID achieves the best performance on dataset that include data from multiple domains and dataset containing modal conversion spelling errors. The dataset and method are available online.",
        "author": "Ming Dong; Zhiwei Cheng; Changyin Luo; Tingting He",
        "authorids": "/m/ming-dong/; /z/zhiwei-cheng/; /c/changyin-luo/; /t/tingting-he/",
        "bibtex": "https://aclanthology.org/2025.coling-main.717.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.717.pdf",
        "site": "https://aclanthology.org/2025.coling-main.717/",
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16185317957378639233&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.743",
        "title": "Return of EM: Entity-driven Answer Set Expansion for QA Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, directly using large language models (LLMs) has been shown to be the most reliable method to evaluate QA models. However, it suffers from limited interpretability, high cost, and environmental harm. To address these, we propose to use soft exact match (EM) with entity-driven answer set expansion. Our approach expands the gold answer set to include diverse surface forms, based on the observation that the surface forms often follow particular patterns depending on the entity type. The experimental results show that our method outperforms traditional evaluation methods by a large margin. Moreover, the reliability of our evaluation method is comparable to that of LLM-based ones, while offering the benefits of high interpretability and reduced environmental harm.",
        "author": "Dongryeol Lee; Minwoo Lee; Kyungmin Min; Joonsuk Park; Kyomin Jung",
        "authorids": "/d/dongryeol-lee/; /m/minwoo-lee/; /k/kyungmin-min/; /j/joonsuk-park/; /k/kyomin-jung/",
        "bibtex": "@inproceedings{lee-etal-2025-return,\n    title = \"Return of {EM}: Entity-driven Answer Set Expansion for {QA} Evaluation\",\n    author = \"Lee, Dongryeol  and\n      Lee, Minwoo  and\n      Min, Kyungmin  and\n      Park, Joonsuk  and\n      Jung, Kyomin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.743/\",\n    pages = \"11218--11234\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.743.pdf",
        "site": "https://aclanthology.org/2025.coling-main.743/",
        "pdf_size": 1651748,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5370397986980705372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of ECE, Seoul National University; LG AI Research; IPAI, Seoul National University; NA VER AI Lab + NA VER Cloud + University of Richmond; Dept. of ECE, Seoul National University",
        "aff_domain": "snu.ac.kr;lgresearch.ai;snu.ac.kr;joonsuk.org;snu.ac.kr",
        "email": "snu.ac.kr;lgresearch.ai;snu.ac.kr;joonsuk.org;snu.ac.kr",
        "github": "https://github.com/DongryeolLee96/ENTQA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2+3+4;0",
        "aff_unique_norm": "Seoul National University;LG AI Research;NAVER Corporation;NAVER Cloud;University of Richmond",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering;;AI Lab;;",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.lgaires.com;https://www.naver.com;https://www.naver.com;https://www.richmond.edu",
        "aff_unique_abbr": "SNU;LG AI;NAVER;NAVER;UR",
        "aff_campus_unique_index": "0;0;;0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0;0;0;0+0+1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2025.coling-main.497",
        "title": "Revisiting Cosine Similarity via Normalized ICA-transformed Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cosine similarity is widely used to measure the similarity between two embeddings, while interpretations based on angle and correlation coefficient are common. In this study, we focus on the interpretable axes of embeddings transformed by Independent Component Analysis (ICA), and propose a novel interpretation of cosine similarity as the sum of semantic similarities over axes. The normalized ICA-transformed embeddings exhibit sparsity, enhancing the interpretability of each axis, and the semantic similarity defined by the product of the components represents the shared meaning between the two embeddings along each axis. The effectiveness of this approach is demonstrated through intuitive numerical examples and thorough numerical experiments. By deriving the probability distributions that govern each component and the product of components, we propose a method for selecting statistically significant axes.",
        "author": "Hiroaki Yamagiwa; Momose Oyama; Hidetoshi Shimodaira",
        "authorids": "/h/hiroaki-yamagiwa/; /m/momose-oyama/; /h/hidetoshi-shimodaira/",
        "bibtex": "@inproceedings{yamagiwa-etal-2025-revisiting,\n    title = \"Revisiting Cosine Similarity via Normalized {ICA}-transformed Embeddings\",\n    author = \"Yamagiwa, Hiroaki  and\n      Oyama, Momose  and\n      Shimodaira, Hidetoshi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.497/\",\n    pages = \"7423--7452\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.497.pdf",
        "site": "https://aclanthology.org/2025.coling-main.497/",
        "pdf_size": 11837106,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8303726671990147649&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Kyoto University; Kyoto University+RIKEN; Kyoto University+RIKEN",
        "aff_domain": "sys.i.kyoto-u.ac.jp;sys.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "email": "sys.i.kyoto-u.ac.jp;sys.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0+1",
        "aff_unique_norm": "Kyoto University;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "Kyoto U;RIKEN",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.262",
        "title": "Revisiting Implicitly Abusive Language Detection: Evaluating LLMs in Zero-Shot and Few-Shot Settings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Implicitly abusive language (IAL), unlike its explicit counterpart, lacks overt slurs or unambiguously offensive keywords, such as \u201cbimbo\u201d or \u201cscum\u201d, making it challenging to detect and mitigate. While current research predominantly focuses on explicitly abusive language, the subtler and more covert forms of IAL remain insufficiently studied. The rapid advancement and widespread adoption of large language models (LLMs) have opened new possibilities for various NLP tasks, but their application to IAL detection has been limited. We revisit three very recent challenging datasets of IAL and investigate the potential of LLMs to enhance the detection of IAL in English through zero-shot and few-shot prompting approaches. We evaluate the models\u2019 capabilities in classifying sentences directly as either IAL or benign, and in extracting linguistic features associated with IAL. Our results indicate that classifiers trained on features extracted by advanced LLMs outperform the best previously reported results, achieving near-human performance.",
        "author": "Julia Jaremko; Dagmar Gromann; Michael Wiegand",
        "authorids": "/j/julia-jaremko/; /d/dagmar-gromann/; /m/michael-wiegand/",
        "bibtex": "@inproceedings{jaremko-etal-2025-revisiting,\n    title = \"Revisiting Implicitly Abusive Language Detection: Evaluating {LLM}s in Zero-Shot and Few-Shot Settings\",\n    author = \"Jaremko, Julia  and\n      Gromann, Dagmar  and\n      Wiegand, Michael\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.262/\",\n    pages = \"3879--3898\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.262.pdf",
        "site": "https://aclanthology.org/2025.coling-main.262/",
        "pdf_size": 387161,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pM9R_z5kKM4J:scholar.google.com/&scioq=Revisiting+Implicitly+Abusive+Language+Detection:+Evaluating+LLMs+in+Zero-Shot+and+Few-Shot+Settings&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Digital Age Research Center (D!ARC), Alpen-Adria-Universit\u00e4t Klagenfurt, Austria; Center for Translation Studies, University of Vienna, Austria; Digital Philology, Faculty of Philological and Cultural Studies, University of Vienna, Austria",
        "aff_domain": "aau.at;univie.ac.at;univie.ac.at",
        "email": "aau.at;univie.ac.at;univie.ac.at",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Alpen-Adria-Universit\u00e4t Klagenfurt;University of Vienna",
        "aff_unique_dep": "Digital Age Research Center (D!ARC);Center for Translation Studies",
        "aff_unique_url": "https://www.aau.at;https://www.univie.ac.at",
        "aff_unique_abbr": ";Uni Vienna",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "2025.coling-main.212",
        "title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The recent surge in jailbreaking attacks has revealed significant vulnerabilities in Large Language Models (LLMs) when exposed to malicious inputs. While various defense strategies have been proposed to mitigate these threats, there has been limited research into the underlying mechanisms that make LLMs vulnerable to such attacks. In this study, we suggest that the self-safeguarding capability of LLMs is linked to specific activity patterns within their representation space. Although these patterns have little impact on the semantic content of the generated text, they play a crucial role in shaping LLM behavior under jailbreaking attacks. Our findings demonstrate that these patterns can be detected with just a few pairs of contrastive queries. Extensive experimentation shows that the robustness of LLMs against jailbreaking can be manipulated by weakening or strengthening these patterns. Further visual analysis provides additional evidence for our conclusions, providing new insights into the jailbreaking phenomenon. These findings highlight the importance of addressing the potential misuse of open-source LLMs within the community.",
        "author": "Tianlong Li; Zhenghua Wang; Wenhao Liu; Muling Wu; Shihan Dou; Changze Lv; Xiaohua Wang; Xiaoqing Zheng; Xuanjing Huang",
        "authorids": "/t/tianlong-li/; /z/zhenghua-wang/; /w/wenhao-liu/; /m/muling-wu/; /s/shihan-dou/; /c/changze-lv/; /x/xiaohua-wang/; /x/xiaoqing-zheng/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{li-etal-2025-revisiting,\n    title = \"Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective\",\n    author = \"Li, Tianlong  and\n      Wang, Zhenghua  and\n      Liu, Wenhao  and\n      Wu, Muling  and\n      Dou, Shihan  and\n      Lv, Changze  and\n      Wang, Xiaohua  and\n      Zheng, Xiaoqing  and\n      Huang, Xuanjing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.212/\",\n    pages = \"3158--3178\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.212.pdf",
        "site": "https://aclanthology.org/2025.coling-main.212/",
        "pdf_size": 2332258,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2043124793644834195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2025.coling-main.750",
        "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator\u2019s preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM\u2019s preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.",
        "author": "Shuting Wang; Xin Yu; Mang Wang; Weipeng Chen; Yutao Zhu; Zhicheng Dou",
        "authorids": "/s/shuting-wang/; /x/xin-yu/; /m/mang-wang/; /w/weipeng-chen/; /y/yutao-zhu/; /z/zhicheng-dou/",
        "bibtex": "@inproceedings{wang-etal-2025-richrag,\n    title = \"{R}ich{RAG}: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation\",\n    author = \"Wang, Shuting  and\n      Yu, Xin  and\n      Wang, Mang  and\n      Chen, Weipeng  and\n      Zhu, Yutao  and\n      Dou, Zhicheng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.750/\",\n    pages = \"11317--11333\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.750.pdf",
        "site": "https://aclanthology.org/2025.coling-main.750/",
        "pdf_size": 1183300,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15501592783972312275&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Baichuan Intelligent Technology; Baichuan Intelligent Technology; Baichuan Intelligent Technology; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "ruc.edu.cn; ; ; ;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn; ; ; ;ruc.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;0",
        "aff_unique_norm": "Renmin University of China;Baichuan Intelligent Technology",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ruc.edu.cn;",
        "aff_unique_abbr": "RUC;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.84",
        "title": "RoBGuard: Enhancing LLMs to Assess Risk of Bias in Clinical Trial Documents",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Randomized Controlled Trials (RCTs) are rigorous clinical studies crucial for reliable decision-making, but their credibility can be compromised by bias. The Cochrane Risk of Bias tool (RoB 2) assesses this risk, yet manual assessments are time-consuming and labor-intensive. Previous approaches have employed Large Language Models (LLMs) to automate this process. However, they typically focus on manually crafted prompts and a restricted set of simple questions, limiting their accuracy and generalizability. Inspired by the human bias assessment process, we propose RoBGuard, a novel framework for enhancing LLMs to assess the risk of bias in RCTs. Specifically, RoBGuard integrates medical knowledge-enhanced question reformulation, multimodal document parsing, and multi-expert collaboration to ensure both completeness and accuracy. Additionally, to address the lack of suitable datasets, we introduce two new datasets: RoB-Item and RoB-Domain. Experimental results demonstrate RoBGuard\u2019s effectiveness on the RoB-Item dataset, outperforming existing methods.",
        "author": "Changkai Ji; Bowen Zhao; Zhuoyao Wang; Yingwen Wang; Yuejie Zhang; Ying Cheng; Rui Feng; Xiaobo Zhang",
        "authorids": "/c/changkai-ji/; /b/bowen-zhao/; /z/zhuoyao-wang/; /y/yingwen-wang/; /y/yuejie-zhang/; /y/ying-cheng/; /r/rui-feng/; /x/xiaobo-zhang/",
        "bibtex": "@inproceedings{ji-etal-2025-robguard,\n    title = \"{R}o{BG}uard: Enhancing {LLM}s to Assess Risk of Bias in Clinical Trial Documents\",\n    author = \"Ji, Changkai  and\n      Zhao, Bowen  and\n      Wang, Zhuoyao  and\n      Wang, Yingwen  and\n      Zhang, Yuejie  and\n      Cheng, Ying  and\n      Feng, Rui  and\n      Zhang, Xiaobo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.84/\",\n    pages = \"1258--1277\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.84.pdf",
        "site": "https://aclanthology.org/2025.coling-main.84/",
        "pdf_size": 4379714,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YFpOZdJ5t-EJ:scholar.google.com/&scioq=RoBGuard:+Enhancing+LLMs+to+Assess+Risk+of+Bias+in+Clinical+Trial+Documents&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.140",
        "title": "RoLargeSum: A Large Dialect-Aware Romanian News Dataset for Summary, Headline, and Keyword Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Using supervised automatic summarisation methods requires sufficient corpora that include pairs of documents and their summaries. Similarly to many tasks in natural language processing, most of the datasets available for summarization are in English, posing challenges for developing summarization models in other languages. Thus, in this work, we introduce RoLargeSum, a novel large-scale summarization dataset for the Romanian language crawled from various publicly available news websites from Romania and the Republic of Moldova that were thoroughly cleaned to ensure a high-quality standard. RoLargeSum contains more than 615K news articles, together with their summaries, as well as their headlines, keywords, dialect, and other metadata that we found on the targeted websites. We further evaluated the performance of several BART variants and open-source large language models on RoLargeSum for benchmarking purposes. We manually evaluated the results of the best-performing system to gain insight into the potential pitfalls of this data set and future development.",
        "author": "Andrei-Marius Avram; Mircea Timpuriu; Andreea Iuga; Vlad-Cristian Matei; Iulian-Marius Taiatu; Tudor G\u0103in\u0103; Dumitru-Clementin Cercel; Mihaela-Claudia Cercel; Florin Pop",
        "authorids": "/a/andrei-marius-avram/; /m/mircea-timpuriu/; /a/andreea-iuga/; /v/vlad-cristian-matei/; /i/iulian-marius-taiatu/; /t/tudor-gaina/; /d/dumitru-clementin-cercel/; /m/mihaela-claudia-cercel/; /f/florin-pop/",
        "bibtex": "@inproceedings{avram-etal-2025-rolargesum,\n    title = \"{R}o{L}arge{S}um: A Large Dialect-Aware {R}omanian News Dataset for Summary, Headline, and Keyword Generation\",\n    author = \"Avram, Andrei-Marius  and\n      Timpuriu, Mircea  and\n      Iuga, Andreea  and\n      Matei, Vlad-Cristian  and\n      Taiatu, Iulian-Marius  and\n      G{\\u{a}}in{\\u{a}}, Tudor  and\n      Cercel, Dumitru-Clementin  and\n      Cercel, Mihaela-Claudia  and\n      Pop, Florin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.140/\",\n    pages = \"2049--2066\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.140.pdf",
        "site": "https://aclanthology.org/2025.coling-main.140/",
        "pdf_size": 579440,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1302846843465319088&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "National University of Science and Technology POLITEHNICA Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania + National Institute for Research and Development in Informatics - ICI Bucharest, Romania; National University of Science and Technology POLITEHNICA Bucharest, Romania + National Institute for Research and Development in Informatics - ICI Bucharest, Romania; Paris 1 Panth\u00e9on-Sorbonne University, Paris, France + University of Bucharest, Bucharest, Romania",
        "aff_domain": "upb.ro; ; ; ; ; ; ; ; ",
        "email": "upb.ro; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0+1;0+1;2+3",
        "aff_unique_norm": "National University of Science and Technology POLITEHNICA;National Institute for Research and Development in Informatics;Paris 1 Panth\u00e9on-Sorbonne University;University of Bucharest",
        "aff_unique_dep": ";ICI;;",
        "aff_unique_url": "https://www.upb.ro;;https://www.universite-paris1.fr;https://www.unibuc.ro",
        "aff_unique_abbr": "UPB;ICI;Paris 1;Unibuc",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;2+0",
        "aff_campus_unique": "Bucharest;;Paris",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0+0;1+0",
        "aff_country_unique": "Romania;France"
    },
    {
        "id": "2025.coling-main.121",
        "title": "Robustness Evaluation of the German Extractive Question Answering Task",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "To ensure reliable performance of Question Answering (QA) systems, evaluation of robustness is crucial. Common evaluation benchmarks commonly only include performance metrics, such as Exact Match (EM) and the F1 score. However, these benchmarks overlook critical factors for the deployment of QA systems. This oversight can result in systems vulnerable to minor perturbations in the input such as typographical errors. While several methods have been proposed to test the robustness of QA models, there has been minimal exploration of these approaches for languages other than English. This study focuses on the robustness evaluation of German language QA models, extending methodologies previously applied primarily to English. The objective is to nurture the development of robust models by defining an evaluation method specifically tailored to the German language. We assess the applicability of perturbations used in English QA models for German and perform a comprehensive experimental evaluation with eight models. The results show that all models are vulnerable to character-level perturbations. Additionally, the comparison of monolingual and multilingual models suggest that the former are less affected by character and word-level perturbations.",
        "author": "Shalaka Satheesh; Katharina Beckh; Katrin Klug; H\u00e9ctor Allende-Cid; Sebastian Houben; Teena Hassan",
        "authorids": "/s/shalaka-satheesh/; /k/katharina-beckh/; /k/katrin-klug/; /h/hector-allende-cid/; /s/sebastian-houben/; /t/teena-hassan/",
        "bibtex": "@inproceedings{satheesh-etal-2025-robustness,\n    title = \"Robustness Evaluation of the {G}erman Extractive Question Answering Task\",\n    author = \"Satheesh, Shalaka  and\n      Beckh, Katharina  and\n      Klug, Katrin  and\n      Allende-Cid, H{\\'e}ctor  and\n      Houben, Sebastian  and\n      Hassan, Teena\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.121/\",\n    pages = \"1785--1801\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.121.pdf",
        "site": "https://aclanthology.org/2025.coling-main.121/",
        "pdf_size": 674799,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9Et7p4lrel0J:scholar.google.com/&scioq=Robustness+Evaluation+of+the+German+Extractive+Question+Answering+Task&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS + Lamarr Institute for Machine Learning and Artificial Intelligence; Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS + Lamarr Institute for Machine Learning and Artificial Intelligence; Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS + Lamarr Institute for Machine Learning and Artificial Intelligence; Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS + Lamarr Institute for Machine Learning and Artificial Intelligence; Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS + Bonn-Rhein-Sieg University of Applied Sciences; Bonn-Rhein-Sieg University of Applied Sciences",
        "aff_domain": "iais.fraunhofer.de; ; ; ; ; ",
        "email": "iais.fraunhofer.de; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+2;2",
        "aff_unique_norm": "Fraunhofer Institute for Intelligent Analysis and Information Systems;Lamarr Institute for Machine Learning and Artificial Intelligence;Bonn-Rhein-Sieg University of Applied Sciences",
        "aff_unique_dep": "Intelligent Analysis and Information Systems;Machine Learning and Artificial Intelligence;",
        "aff_unique_url": "https://www.iais.fraunhofer.de/;;https://www.fh-bonn-rhein-sieg.de",
        "aff_unique_abbr": "Fraunhofer IAIS;;FH Bonn-Rhein-Sieg",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0+1;0+0;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2025.coling-main.494",
        "title": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing Systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Role-playing systems powered by large language models (LLMs) have become increasingly influential in emotional communication applications. However, these systems are susceptible to character hallucinations, where the model deviates from predefined character roles and generates responses that are inconsistent with the intended persona. This paper presents the first systematic analysis of character hallucination from an attack perspective, introducing the RoleBreak framework. Our framework identifies two core mechanisms\u2014query sparsity and role-query conflict\u2014as key factors driving character hallucination. Leveraging these insights, we construct a novel dataset, RoleBreakEval, to evaluate existing hallucination mitigation techniques. Our experiments reveal that even enhanced models trained to minimize hallucination remain vulnerable to attacks. To address these vulnerabilities, we propose a novel defence strategy, the Narrator Mode, which generates supplemental context through narration to mitigate role-query conflicts and improve query generalization. Experimental results demonstrate that Narrator Mode significantly outperforms traditional refusal-based strategies by reducing hallucinations, enhancing fidelity to character roles and queries, and improving overall narrative coherence.",
        "author": "Yihong Tang; Bo Wang; Xu Wang; Dongming Zhao; Jing Liu; Ruifang He; Yuexian Hou",
        "authorids": "/y/yihong-tang/; /b/bo-wang/; /x/xu-wang/; /d/dongming-zhao/; /j/jing-liu/; /r/ruifang-he/; /y/yuexian-hou/",
        "bibtex": "@inproceedings{tang-etal-2025-rolebreak,\n    title = \"{R}ole{B}reak: Character Hallucination as a Jailbreak Attack in Role-Playing Systems\",\n    author = \"Tang, Yihong  and\n      Wang, Bo  and\n      Wang, Xu  and\n      Zhao, Dongming  and\n      Liu, Jing  and\n      He, Ruifang  and\n      Hou, Yuexian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.494/\",\n    pages = \"7386--7402\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.494.pdf",
        "site": "https://aclanthology.org/2025.coling-main.494/",
        "pdf_size": 1394209,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=947720686410681852&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of New Media and Communication, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; AI Lab, China Mobile Communication Group Tianjin Co., Ltd.; AI Lab, China Mobile Communication Group Tianjin Co., Ltd.; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn; ; ; ; ; ",
        "email": "tju.edu.cn;tju.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;1;0;0",
        "aff_unique_norm": "Tianjin University;China Mobile Communication Group Tianjin Co., Ltd.",
        "aff_unique_dep": "School of New Media and Communication;AI Lab",
        "aff_unique_url": "http://www.tju.edu.cn;http://www.chinamobileltd.com/",
        "aff_unique_abbr": "Tianjin University;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Tianjin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.562",
        "title": "Rule-KBQA: Rule-Guided Reasoning for Complex Knowledge Base Question Answering with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge base question answering (KBQA) is recognized as a challenging task, especially when parsing complex questions into executable logical forms. Traditional semantic parsing (SP)-based approaches exhibit inconsistent performance in handling various complex questions. As large language models (LLMs) have exhibited exceptional reasoning ability and language comprehension, recent studies have employed LLMs for semantic parsing to directly generate logical forms that can be executed on knowledge bases (KBs) to achieve the desired results. However, these methods of relying exclusively on LLMs to ensure grammaticality, faithfulness, and controllability may diminish their effectiveness due to hallucinations in the reasoning process. In this paper, we introduce Rule-KBQA, a framework that employs learned rules to guide the generation of logical forms. The proposed method contains two phases, an induction phase and a deduction phase. In the induction phase, we initially extract rules from the existing data and then employ the Rule-Following Fine-Tuned (RFFT) LLM to generate additional rules, ultimately constructing a comprehensive rule library. In the deduction phase, a symbolic agent, guided by learned rules, explores the environment KB to incrementally construct executable logical forms. Meanwhile, we leverage the discriminative capability of LLMs to evaluate the plausibility of candidate decisions. Extensive experiments indicate that our method achieves competitive results on standard KBQA datasets, clearly demonstrating its effectiveness.",
        "author": "Zhiqiang Zhang; Liqiang Wen; Wen Zhao",
        "authorids": "/z/zhiqiang-zhang/; /l/liqiang-wen/; /w/wen-zhao/",
        "bibtex": "@inproceedings{zhang-etal-2025-rule,\n    title = \"Rule-{KBQA}: Rule-Guided Reasoning for Complex Knowledge Base Question Answering with Large Language Models\",\n    author = \"Zhang, Zhiqiang  and\n      Wen, Liqiang  and\n      Zhao, Wen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.562/\",\n    pages = \"8399--8417\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.562.pdf",
        "site": "https://aclanthology.org/2025.coling-main.562/",
        "pdf_size": 701005,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-SsdFjYDHDsJ:scholar.google.com/&scioq=Rule-KBQA:+Rule-Guided+Reasoning+for+Complex+Knowledge+Base+Question+Answering+with+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Peking University; Peking University; Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.261",
        "title": "Rumor Detection on Social Media with Temporal Propagation Structure Optimization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Traditional methods for detecting rumors on social media primarily focus on analyzing textual content, often struggling to capture the complexity of online interactions. Recent research has shifted towards leveraging graph neural networks to model the hierarchical conversation structure that emerges during rumor propagation. However, these methods tend to overlook the temporal aspect of rumor propagation and may disregard potential noise within the propagation structure. In this paper, we propose a novel approach that incorporates temporal information by constructing a weighted propagation tree, where the weight of each edge represents the time interval between connected posts. Drawing upon the theory of structural entropy, we transform this tree into a coding tree. This transformation aims to preserve the essential structure of rumor propagation while reducing noise. Finally, we introduce a recursive neural network to learn from the coding tree for rumor veracity prediction. Experimental results on two common datasets demonstrate the superiority of our approach.",
        "author": "Xingyu Peng; Junran Wu; Ruomei Liu; Ke Xu",
        "authorids": "/x/xingyu-peng/; /j/junran-wu/; /r/ruomei-liu/; /k/ke-xu/",
        "bibtex": "@inproceedings{peng-etal-2025-rumor,\n    title = \"Rumor Detection on Social Media with Temporal Propagation Structure Optimization\",\n    author = \"Peng, Xingyu  and\n      Wu, Junran  and\n      Liu, Ruomei  and\n      Xu, Ke\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.261/\",\n    pages = \"3865--3878\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.261.pdf",
        "site": "https://aclanthology.org/2025.coling-main.261/",
        "pdf_size": 725593,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:g7GLSs-lld4J:scholar.google.com/&scioq=Rumor+Detection+on+Social+Media+with+Temporal+Propagation+Structure+Optimization&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "State Key Laboratory of Complex & Critical Software Environment, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, Beihang University, Beijing, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "State Key Laboratory of Complex & Critical Software Environment",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "BUAA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.510",
        "title": "SA-DETR:Span Aware Detection Transformer for Moment Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Moment Retrieval aims to locate specific video segments related to the given text. Recently, DETR-based methods, originating from Object Detection, have emerged as effective solutions for Moment Retrieval. These approaches focus on multimodal feature fusion and refining Queries composed of span anchor and content embedding. Despite the success, they often overlook the video-text instance related information in Query Initialization and the crucial guidance role of span anchors in Query Refinement, leading to inaccurate predictions. To address this, we propose a novel Span Aware DEtection TRansformer (SA-DETR) that leverages the importance of instance related span anchors. To fully leverage the instance related information, we generate span anchors based on video-text pair rather than using learnable parameters, as is common in conventional DETR-based methods, and supervise them with GT labels. To effectively exploit the correspondence between span anchors and video clips, we enhance content embedding guided by textual features and generate Gaussian mask to modulate the interaction between content embedding and fusion features. Furthermore, we explore the feature alignment across various stages and granularities and apply denoise learning to boost the span awareness of the model. Extensive experiments on QVHighlights, Charades-STA, and TACoS demonstrate the effectiveness of our approach.",
        "author": "Tianheng Xiong; Wei Wei; Kaihe Xu; Dangyang Chen",
        "authorids": "/t/tianheng-xiong/; /w/wei-wei/; /k/kaihe-xu/; /d/dangyang-chen/",
        "bibtex": "@inproceedings{xiong-etal-2025-sa,\n    title = \"{SA}-{DETR}:Span Aware Detection Transformer for Moment Retrieval\",\n    author = \"Xiong, Tianheng  and\n      Wei, Wei  and\n      Xu, Kaihe  and\n      Chen, Dangyang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.510/\",\n    pages = \"7634--7647\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.510.pdf",
        "site": "https://aclanthology.org/2025.coling-main.510/",
        "pdf_size": 9098024,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-6FMiGP07QkJ:scholar.google.com/&scioq=SA-DETR:Span+Aware+Detection+Transformer+for+Moment+Retrieval&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology + Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL) + Ping An Property & Casualty Insurance company of China, Ltd.; Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL) + Ping An Property & Casualty Insurance company of China, Ltd.",
        "aff_domain": "gmail.com;hust.edu.cn;gmail.com;pingan.com.cn",
        "email": "gmail.com;hust.edu.cn;gmail.com;pingan.com.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+1;0+1",
        "aff_unique_norm": "Huazhong University of Science and Technology;Ping An Property & Casualty Insurance Company of China, Ltd.",
        "aff_unique_dep": "School of Computer Science and Technology;",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.pingan.com",
        "aff_unique_abbr": "HUST;Ping An",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.202",
        "title": "SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The development of unbiased large language models is widely recognized as crucial, yet existing benchmarks fall short in detecting biases due to limited scope, contamination, and lack of a fairness baseline. SAGED(bias) is the first holistic benchmarking pipeline to address these problems. The pipeline encompasses five core stages: scraping materials, assembling benchmarks, generating responses, extracting numeric features, and diagnosing with disparity metrics. SAGED includes metrics for max disparity, such as impact ratio, and bias concentration, such as Max Z-scores. Noticing that metric tool bias and contextual bias in prompts can distort evaluation, SAGED implements counterfactual branching and baseline calibration for mitigation. For demonstration, we use SAGED on G20 Countries with popular 8b-level models including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we find that while Mistral and Qwen2 show lower max disparity and higher bias concentration than Gemma2 and Llama3.1, all models are notably biased against countries like Russia and (except for Qwen2) China. With further experiments to have models role-playing U.S. presidents, we see bias amplifies and shifts in heterogeneous directions. Moreover, we see Qwen2 and Mistral not engage in role-playing, while Llama3.1 and Gemma2 role-play Trump notably more intensively than Biden and Harris, indicating role-playing performance bias in these models.",
        "author": "Xin Guan; Nate Demchak; Saloni Gupta; Ze Wang; Ediz Ertekin Jr.; Adriano Koshiyama; Emre Kazim; Zekun Wu",
        "authorids": "/x/xin-guan/; /n/nate-demchak/; /s/saloni-gupta/; /z/ze-wang/; /e/ediz-ertekin-jr/; /a/adriano-koshiyama/; /e/emre-kazim/; /z/zekun-wu/",
        "bibtex": "@inproceedings{guan-etal-2025-saged,\n    title = \"{SAGED}: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration\",\n    author = \"Guan, Xin  and\n      Demchak, Nate  and\n      Gupta, Saloni  and\n      Wang, Ze  and\n      Ertekin Jr., Ediz  and\n      Koshiyama, Adriano  and\n      Kazim, Emre  and\n      Wu, Zekun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.202/\",\n    pages = \"3002--3026\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.202.pdf",
        "site": "https://aclanthology.org/2025.coling-main.202/",
        "pdf_size": 9330202,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8947121142441788508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Holistic AI+Center for Long-term AI; University College London; Stanford University+University College London; University of Maryland, College Park; University of California, Berkeley; Holistic AI; Holistic AI; Holistic AI+University College London",
        "aff_domain": "holistic-ai.com;ucl.ac.uk;stanford.edu;umd.edu;berkeley.edu;holistic-ai.com;holistic-ai.com;ucl.ac.uk",
        "email": "holistic-ai.com;ucl.ac.uk;stanford.edu;umd.edu;berkeley.edu;holistic-ai.com;holistic-ai.com;ucl.ac.uk",
        "github": "https://github.com/holistic-ai/SAGED-Bias",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;3+2;4;5;0;0;0+2",
        "aff_unique_norm": "Holistic AI;Center for Long-term AI;University College London;Stanford University;University of Maryland;University of California, Berkeley",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": ";;https://www.ucl.ac.uk;https://www.stanford.edu;https://www/umd.edu;https://www.berkeley.edu",
        "aff_unique_abbr": ";;UCL;Stanford;UMD;UC Berkeley",
        "aff_campus_unique_index": ";1;2;3;",
        "aff_campus_unique": ";Stanford;College Park;Berkeley",
        "aff_country_unique_index": "1;2;1+2;1;1;2",
        "aff_country_unique": ";United States;United Kingdom"
    },
    {
        "id": "2025.coling-main.639",
        "title": "SCCD: A Session-based Dataset for Chinese Cyberbullying Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rampant spread of cyberbullying content poses a growing threat to societal well-being. However, research on cyberbullying detection in Chinese remains underdeveloped, primarily due to the lack of comprehensive and reliable datasets. Notably, no existing Chinese dataset is specifically tailored for cyberbullying detection. Moreover, while comments play a crucial role within sessions, current session-based datasets often lack detailed, fine-grained annotations at the comment level. To address these limitations, we present a novel Chinese cyberbullying dataset, termed SCCD, which consists of 677 session-level samples sourced from a major social media platform Weibo. Moreover, each comment within the sessions is annotated with fine-grained labels rather than conventional binary class labels. Empirically, we evaluate the performance of various baseline methods on SCCD, highlighting the challenges for effective Chinese cyberbullying detection.",
        "author": "Qingpo Yang; Yakai Chen; Zihui Xu; Yu-ming Shang; Sanchuan Guo; Xi Zhang",
        "authorids": "/q/qingpo-yang/; /y/yakai-chen/; /z/zihui-xu/; /y/yu-ming-shang/; /s/sanchuan-guo/; /x/xi-zhang/",
        "bibtex": "@inproceedings{yang-etal-2025-sccd,\n    title = \"{SCCD}: A Session-based Dataset for {C}hinese Cyberbullying Detection\",\n    author = \"Yang, Qingpo  and\n      Chen, Yakai  and\n      Xu, Zihui  and\n      Shang, Yu-ming  and\n      Guo, Sanchuan  and\n      Zhang, Xi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.639/\",\n    pages = \"9533--9545\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.639.pdf",
        "site": "https://aclanthology.org/2025.coling-main.639/",
        "pdf_size": 2004210,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xeL6k4tU1ygJ:scholar.google.com/&scioq=SCCD:+A+Session-based+Dataset+for+Chinese+Cyberbullying+Detection&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn; ; ; ; ; ",
        "email": "bupt.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.63",
        "title": "SCV: Light and Effective Multi-Vector Retrieval with Sequence Compressive Vectors",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recent advances in language models (LMs) has driven progress in information retrieval (IR), effectively extracting semantically relevant information. However, they face challenges in balancing computational costs with deeper query-document interactions. To tackle this, we present two mechanisms: 1) a light and effective multi-vector retrieval with sequence compression vectors, dubbed SCV and 2) coarse-to-fine vector search. The strengths of SCV stems from its application of span compressive vectors for scoring. By employing a non-linear operation to examine every token in the document, we abstract these into a span-level representation. These vectors effectively reduce the document\u2019s dimensional representation, enabling the model to engage comprehensively with tokens across the entire collection of documents, rather than the subset retrieved by Approximate Nearest Neighbor. Therefore, our framework performs a coarse single vector search during the inference stage and conducts a fine-grained multi-vector search end-to-end. This approach effectively reduces the cost required for search. We empirically show that SCV achieves the fastest latency compared to other state-of-the-art models and can obtain competitive performance on both in-domain and out-of-domain benchmark datasets.",
        "author": "Cheoneum Park; Seohyeong Jeong; Minsang Kim; KyungTae Lim; Yong-Hun Lee",
        "authorids": "/c/cheoneum-park/; /s/seohyeong-jeong/; /m/minsang-kim/; /k/kyungtae-lim/; /y/yong-hun-lee/",
        "bibtex": "@inproceedings{park-etal-2025-scv,\n    title = \"{SCV}: Light and Effective Multi-Vector Retrieval with Sequence Compressive Vectors\",\n    author = \"Park, Cheoneum  and\n      Jeong, Seohyeong  and\n      Kim, Minsang  and\n      Lim, KyungTae  and\n      Lee, Yong-Hun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.63/\",\n    pages = \"760--770\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.63.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.63/",
        "pdf_size": 546985,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8925147895408674685&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Hanbat National University; SK Telecom; SK Telecom; Seoul National University of Science and Technology; SK Telecom",
        "aff_domain": "hanbat.ac.kr;gmail.com;sktelecom.com;seoultech.ac.kr;sktelecom.com",
        "email": "hanbat.ac.kr;gmail.com;sktelecom.com;seoultech.ac.kr;sktelecom.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "Hanbat National University;SK Telecom;Seoul National University of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.hanbat.ac.kr;https://www.sktelecom.com;https://www.snust.ac.kr",
        "aff_unique_abbr": "HNU;SKT;SNUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.328",
        "title": "SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) demonstrate remarkable emergent abilities across various tasks, yet fall short of complex reasoning and planning tasks. The tree-search-based reasoning methods address this by encouraging the exploration of intermediate steps, surpassing the capabilities of chain-of-thought prompting. However, significant inference latency is introduced due to the systematic exploration and evaluation of multiple thought paths. This paper introduces SEED, a novel and efficient inference framework to improve both runtime speed and GPU memory management concurrently. Based on a scheduled speculative execution, SEED efficiently handles multiple iterations for thought generation and state evaluation, leveraging a rounds-scheduled strategy to manage draft model dispatching. Extensive experimental evaluations on three reasoning datasets demonstrate the superior speedup performance of SEED.",
        "author": "Zhenglin Wang; Jialong Wu; Yilong Lai; Congzhi Zhang; Deyu Zhou",
        "authorids": "/z/zhenglin-wang/; /j/jialong-wu/; /y/yilong-lai/; /c/congzhi-zhang/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{wang-etal-2025-seed,\n    title = \"{SEED}: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding\",\n    author = \"Wang, Zhenglin  and\n      Wu, Jialong  and\n      Lai, Yilong  and\n      Zhang, Congzhi  and\n      Zhou, Deyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.328/\",\n    pages = \"4920--4937\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.328.pdf",
        "site": "https://aclanthology.org/2025.coling-main.328/",
        "pdf_size": 6341979,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3778734799875292099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "https://github.com/Linking-ai/SEED",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.525",
        "title": "SGMEA: Structure-Guided Multimodal Entity Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different multimodal knowledge graphs (MMKGs) by integrating structural information, entity attributes, and visual data, thereby promoting knowledge sharing and deep multimodal data integration. However, existing methods often overlook the deeper connections between multimodal data. They primarily focus on the interactions between neighboring entities in the structural modality while neglecting the interactions between entities in the visual and attribute modalities. To address this, we propose a structure-guided multimodal entity alignment method (SGMEA), which prioritizes structural information from knowledge graphs to enhance the visual and attribute modalities. By fusing multimodal representations, SGMEA improves the accuracy of entity alignment. Experimental results demonstrate that SGMEA achieves stateof-the-art performance across multiple datasets, validating its effectiveness and superiority in practical applications.",
        "author": "Jingwei Cheng; Mingxiao Guo; Fu Zhang",
        "authorids": "/j/jingwei-cheng/; /m/mingxiao-guo/; /f/fu-zhang/",
        "bibtex": "https://aclanthology.org/2025.coling-main.525.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.525.pdf",
        "site": "https://aclanthology.org/2025.coling-main.525/",
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:a3p4sk5b_8gJ:scholar.google.com/&scioq=SGMEA:+Structure-Guided+Multimodal+Entity+Alignment&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.333",
        "title": "SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, fine-grained sentiment analysis in finance has gained significant attention, but the scarcity of entity-level datasets remains a key challenge. To address this, we have constructed the largest English and Chinese financial entity-level sentiment analysis datasets to date. Building on this foundation, we propose a novel two-stage sentiment analysis approach called Self-aware In-context Learning Correction (SILC). The first stage involves fine-tuning a base large language model to generate pseudo-labeled data specific to our task. In the second stage, we train a correction model using a GNN-based example retriever, which is informed by the pseudo-labeled data. This two-stage strategy has allowed us to achieve state-of-the-art performance on the newly constructed datasets, advancing the field of financial sentiment analysis. In a case study, we demonstrate the enhanced practical utility of our data and methods in monitoring the cryptocurrency market. Our datasets and code are available at https://github.com/NLP-Bin/SILC-EFSA.",
        "author": "Senbin Zhu; ChenYuan He; Hongde Liu; Pengcheng Dong; Hanjie Zhao; Yuchen Yan; Yuxiang Jia; Hongying Zan; Min Peng",
        "authorids": "/s/senbin-zhu/; /c/chenyuan-he/; /h/hongde-liu/; /p/pengcheng-dong/; /h/hanjie-zhao/; /y/yuchen-yan/; /y/yuxiang-jia/; /h/hongying-zan/; /m/min-peng/",
        "bibtex": "@inproceedings{zhu-etal-2025-silc,\n    title = \"{SILC}-{EFSA}: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis\",\n    author = \"Zhu, Senbin  and\n      He, ChenYuan  and\n      Liu, Hongde  and\n      Dong, Pengcheng  and\n      Zhao, Hanjie  and\n      Yan, Yuchen  and\n      Jia, Yuxiang  and\n      Zan, Hongying  and\n      Peng, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.333/\",\n    pages = \"4980--4992\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.333.pdf",
        "site": "https://aclanthology.org/2025.coling-main.333/",
        "pdf_size": 3518599,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:zxZPtMcxrlMJ:scholar.google.com/&scioq=SILC-EFSA:+Self-aware+In-context+Learning+Correction+for+Entity-level+Financial+Sentiment+Analysis&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China + School of Computer Science, Wuhan University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Computer Science, Wuhan University, China",
        "aff_domain": "gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn; ; ;zzu.edu.cn; ;whu.edu.cn",
        "email": "gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn;gs.zzu.edu.cn; ; ;zzu.edu.cn; ;whu.edu.cn",
        "github": "https://github.com/NLP-Bin/SILC-EFSA",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0+1;0;1",
        "aff_unique_norm": "Zhengzhou University;Wuhan University",
        "aff_unique_dep": "School of Computer and Artificial Intelligence;School of Computer Science",
        "aff_unique_url": "http://www.zzu.edu.cn;http://www.whu.edu.cn",
        "aff_unique_abbr": ";WHU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.215",
        "title": "SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Small Language Models (SLMs) are attracting attention due to the high computational demands and privacy concerns of Large Language Models (LLMs). Some studies fine-tune SLMs using Chains of Thought (CoT) data distilled from LLMs, aiming to enhance their reasoning ability. Furthermore, Some CoT distillation methods introduce external symbolic knowledge into the generation process to improve the limited knowledge memory, reasoning ability and out-of-domain (OOD) generalization of SLMs. However, the introduction of symbolic knowledge increases computational overhead and introduces potential noise. In this paper, we introduce SKIntern, an innovative approach that empowers SLMs to internalize symbolic knowledge and few-shot examples gradually through a progressive fine-tuning process, guided by a predefined linear decay schedule under curriculum learning. By efficiently internalizing knowledge, SKIntern reduces computational overhead and speeds up the reasoning process by focusing solely on the question during inference. It outperforms state-of-the-art baselines by over 5%, while reducing inference costs (measured in FLOPs) by up to 4\u00d7 across a wide range of SLMs in both in-domain (ID) and out-of-domain (OOD) tasks. Our code will be available at https://github.com/Xnhyacinth/SKIntern.",
        "author": "Huanxuan Liao; Shizhu He; Yupu Hao; Xiang Li; Yuanzhe Zhang; Jun Zhao; Kang Liu",
        "authorids": "/h/huanxuan-liao/; /s/shizhu-he/; /y/yupu-hao/; /x/xiang-li/; /y/yuanzhe-zhang/; /j/jun-zhao/; /k/kang-liu/",
        "bibtex": "@inproceedings{liao-etal-2025-skintern,\n    title = \"{SKI}ntern: Internalizing Symbolic Knowledge for Distilling Better {C}o{T} Capabilities into Small Language Models\",\n    author = \"Liao, Huanxuan  and\n      He, Shizhu  and\n      Hao, Yupu  and\n      Li, Xiang  and\n      Zhang, Yuanzhe  and\n      Zhao, Jun  and\n      Liu, Kang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.215/\",\n    pages = \"3203--3221\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.215.pdf",
        "site": "https://aclanthology.org/2025.coling-main.215/",
        "pdf_size": 1088824,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7499737544615081993&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China + 2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China + 2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China + 2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China + 2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; 4National Science Library, Chinese Academy of Sciences, Beijing, China; 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China + 2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + 3Shanghai Artificial Intelligence Laboratory, Shanghai, China; 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China + 2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + 3Shanghai Artificial Intelligence Laboratory, Shanghai, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;ia.ac.cn;ia.ac.cn; ;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;ia.ac.cn;ia.ac.cn; ;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/Xnhyacinth/SKIntern",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0;0+1+2;0+1+2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0;0+0+1;0+0+1",
        "aff_campus_unique": "Beijing;Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.637",
        "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the significant improvements achieved by large language models (LLMs) in English reasoning tasks, these models continue to struggle with multilingual reasoning. Recent studies leverage a full-parameter and two-stage training paradigm to teach models to first understand non-English questions and then reason. However, this method suffers from both substantial computational resource computing and catastrophic forgetting. The fundamental cause is that, with the primary goal of enhancing multilingual comprehension, an excessive number of irrelevant layers and parameters are tuned during the first stage. Given our findings that the representation learning of languages is merely conducted in lower-level layers, we propose an efficient multilingual reasoning alignment approach that precisely identifies and fine-tunes the layers responsible for handling multilingualism. Experimental results show that our method, SLAM, only tunes 6 layers\u2019 feed-forward sub-layers including 6.5-8% of all parameters within 7B and 13B LLMs, achieving superior average performance than all strong baselines across 10 languages. Meanwhile, SLAM only involves one training stage, reducing training time by 4.1-11.9\u00d7 compared to the two-stage method.",
        "author": "Yuchun Fan; Yongyu Mu; YiLin Wang; Lei Huang; Junhao Ruan; Bei Li; Tong Xiao; Shujian Huang; Xiaocheng Feng; Jingbo Zhu",
        "authorids": "/y/yuchun-fan/; /y/yongyu-mu/; /y/yilin-wang/; /l/lei-huang/; /j/junhao-ruan/; /b/bei-li/; /t/tong-xiao/; /s/shujian-huang/; /x/xiaocheng-feng/; /j/jingbo-zhu/",
        "bibtex": "@inproceedings{fan-etal-2025-slam,\n    title = \"{SLAM}: Towards Efficient Multilingual Reasoning via Selective Language Alignment\",\n    author = \"Fan, Yuchun  and\n      Mu, Yongyu  and\n      Wang, YiLin  and\n      Huang, Lei  and\n      Ruan, Junhao  and\n      Li, Bei  and\n      Xiao, Tong  and\n      Huang, Shujian  and\n      Feng, Xiaocheng  and\n      Zhu, Jingbo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.637/\",\n    pages = \"9499--9515\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.637.pdf",
        "site": "https://aclanthology.org/2025.coling-main.637/",
        "pdf_size": 3354216,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:7s9DwCEwq5UJ:scholar.google.com/&scioq=SLAM:+Towards+Efficient+Multilingual+Reasoning+via+Selective+Language+Alignment&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China; NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China; NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China; Harbin Institute of Technology, Harbin, China; NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China; Meituan Inc.; NiuTrans Research, Shenyang, China; National Key Laboratory for Novel Software Technology, Nanjing University; Harbin Institute of Technology, Harbin, China; NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China+NiuTrans Research, Shenyang, China",
        "aff_domain": "outlook.com; ; ; ; ; ;mail.neu.edu.cn; ; ;mail.neu.edu.cn",
        "email": "outlook.com; ; ; ; ; ;mail.neu.edu.cn; ; ;mail.neu.edu.cn",
        "github": "https://github.com/fmm170/SLAM",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;1;0;2;3;4;1;0+3",
        "aff_unique_norm": "Northeastern University;Harbin Institute of Technology;Meituan Inc.;NiuTrans Research;Nanjing University",
        "aff_unique_dep": "School of Computer Science and Engineering;;;;National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.neu.edu.cn/;http://www.hit.edu.cn/;https://www.meituan.com;;http://www.nju.edu.cn",
        "aff_unique_abbr": "NEU;HIT;Meituan;;Nanjing University",
        "aff_campus_unique_index": "0;0;0;1;0;1;0",
        "aff_campus_unique": "Shenyang;Harbin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.50",
        "title": "SLARD: A Chinese Superior Legal Article Retrieval Dataset",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieving superior legal articles involves identifying relevant legal articles that hold higher legal effectiveness. This process is crucial in legislative work because superior legal articles form the legal basis for drafting new laws. However, most existing legal information retrieval research focuses on retrieving legal documents, with limited research on retrieving superior legal articles. This gap restricts the digitization of legislative work. To advance research in this area, we propose SLARD: A Chinese Superior Legal Article Retrieval Dataset, which filters 2,627 queries and 9,184 candidates from over 4.3 million effective Chinese regulations, covering 32 categories, such as environment, agriculture, and water resources. Each query is manually annotated, and the candidates include superior articles at both the provincial and national levels. We conducted detailed experiments and analyses on the dataset and found that existing retrieval methods struggle to achieve ideal results. The best method achieved a R@1 of only 0.4719. Additionally, we found that existing large language models (LLMs) lack prior knowledge of the content of superior legal articles. This indicates the necessity for further exploration and research in this field.",
        "author": "Zhe Chen; Pengjie Ren; Fuhui Sun; Xiaoyan Wang; Yujun Li; Siwen Zhao; Tengyi Yang",
        "authorids": "/z/zhe-chen/; /p/pengjie-ren/; /f/fuhui-sun/; /x/xiaoyan-wang/; /y/yujun-li/; /s/siwen-zhao/; /t/tengyi-yang/",
        "bibtex": "@inproceedings{chen-etal-2025-slard,\n    title = \"{SLARD}: A {C}hinese Superior Legal Article Retrieval Dataset\",\n    author = \"Chen, Zhe  and\n      Ren, Pengjie  and\n      Sun, Fuhui  and\n      Wang, Xiaoyan  and\n      Li, Yujun  and\n      Zhao, Siwen  and\n      Yang, Tengyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.50/\",\n    pages = \"740--754\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.50.pdf",
        "site": "https://aclanthology.org/2025.coling-main.50/",
        "pdf_size": 735123,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cDo3LxfcTA8J:scholar.google.com/&scioq=SLARD:+A+Chinese+Superior+Legal+Article+Retrieval+Dataset&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "School of information Science and Engineering, Shandong University; School of Computer Science and Technology, Shandong University; Information Technology Service Center of People\u2019s Court; Information Technology Service Center of People\u2019s Court; School of information Science and Engineering, Shandong University; School of information Science and Engineering, Shandong University; School of information Science and Engineering, Shandong University",
        "aff_domain": "mail.sdu.edu.cn; ; ;139.com;sdu.edu.cn; ; ",
        "email": "mail.sdu.edu.cn; ; ;139.com;sdu.edu.cn; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;1;0;0;0",
        "aff_unique_norm": "Shandong University;People's Court Information Technology Service Center",
        "aff_unique_dep": "School of Information Science and Engineering;Information Technology Service",
        "aff_unique_url": "http://www.sdu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.1",
        "title": "STAND-Guard: A Small Task-Adaptive Content Moderation Model",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Content moderation, the process of reviewing and monitoring the safety of generated content, is important for development of welcoming online platforms and responsible large language models. Content moderation contains various tasks, each with its unique requirements tailored to specific scenarios. Therefore, it is crucial to develop a model that can be easily adapted to novel or customized content moderation tasks accurately without extensive model tuning. This paper presents STAND-Guard, a Small Task-Adaptive coNtent moDeration model. The basic motivation is: by performing instruct tuning on various content moderation tasks, we can unleash the power of small language models (SLMs) on unseen (out-of-distribution) content moderation tasks. We also carefully study the effects of training tasks and model size on the efficacy of cross-task fine-tuning mechanism. Experiments demonstrate STAND-Guard is comparable to GPT-3.5-Turbo across over 40 public datasets, as well as proprietary datasets derived from real-world business scenarios. Remarkably, STAND-Guard achieved nearly equivalent results to GPT-4-Turbo on unseen English binary classification tasks.",
        "author": "Minjia Wang; Pingping Lin; Siqi Cai; Shengnan An; Shengjie Ma; Zeqi Lin; Congrui Huang; Bixiong Xu",
        "authorids": "/m/minjia-wang/; /p/pingping-lin/; /s/siqi-cai/; /s/shengnan-an/; /s/shengjie-ma/; /z/zeqi-lin/; /c/congrui-huang/; /b/bixiong-xu/",
        "bibtex": "@inproceedings{wang-etal-2025-stand,\n    title = \"{STAND}-Guard: A Small Task-Adaptive Content Moderation Model\",\n    author = \"Wang, Minjia  and\n      Lin, Pingping  and\n      Cai, Siqi  and\n      An, Shengnan  and\n      Ma, Shengjie  and\n      Lin, Zeqi  and\n      Huang, Congrui  and\n      Xu, Bixiong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.1/\",\n    pages = \"1--20\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.1.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.1/",
        "pdf_size": 391493,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5662514087676277915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "https://arxiv.org/abs/2411.05214",
        "author_num": 8
    },
    {
        "id": "2025.coling-main.721",
        "title": "SUMIE: A Synthetic Benchmark for Incremental Entity Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "No existing dataset adequately tests how well language models can incrementally update entity summaries \u2013 a crucial ability as these models rapidly advance. The Incremental Entity Summarization (IES) task is vital for maintaining accurate, up-to-date knowledge. To address this, we introduce , a fully synthetic dataset designed to expose real-world IES challenges. This dataset addresses issues like incorrect entity association and incomplete information, capturing real-world complexity by generating diverse attributes, summaries, and unstructured paragraphs with 99% alignment accuracy between generated summaries and paragraphs. Extensive experiments demonstrate the dataset\u2019s difficulty \u2013 state-of-the-art LLMs struggle to update summaries with an F1 higher than 80.4%. We will open-source the benchmark and the evaluation metrics to help the community make progress on IES tasks.",
        "author": "Eunjeong Hwang; Yichao Zhou; Beliz Gunel; James Bradley Wendt; Sandeep Tata",
        "authorids": "/e/eunjeong-hwang/; /y/yichao-zhou/; /b/beliz-gunel/; /j/james-bradley-wendt/; /s/sandeep-tata/",
        "bibtex": "@inproceedings{hwang-etal-2025-sumie,\n    title = \"{SUMIE}: A Synthetic Benchmark for Incremental Entity Summarization\",\n    author = \"Hwang, Eunjeong  and\n      Zhou, Yichao  and\n      Gunel, Beliz  and\n      Wendt, James Bradley  and\n      Tata, Sandeep\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.721/\",\n    pages = \"10839--10864\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.721.pdf",
        "site": "https://aclanthology.org/2025.coling-main.721/",
        "pdf_size": 5980404,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13882896463495435787&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of British Columbia+Google Deepmind; Google Deepmind; Google Deepmind; Google Deepmind; Google Deepmind",
        "aff_domain": "cs.ubc.ca;google.com;google.com;google.com;google.com",
        "email": "cs.ubc.ca;google.com;google.com;google.com;google.com",
        "github": "https://github.com/google-deepmind/sumie",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "University of British Columbia;DeepMind",
        "aff_unique_dep": ";DeepMind",
        "aff_unique_url": "https://www.ubc.ca;https://deepmind.com",
        "aff_unique_abbr": "UBC;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "2025.coling-main.31",
        "title": "SURE: Mutually Visible Objects and Self-generated Candidate Labels For Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Joint relation extraction models effectively mitigate the error propagation problem inherently present in pipeline models. Nevertheless, joint models face challenges including high computational complexity, complex network architectures, difficult parameter tuning, and notably, limited interpretability. In contrast, recent advances in pipeline relation extraction models (PURE, PL-Marker) have attracted considerable attention due to their lightweight design and high extraction accuracy. A key advancement is the introduction of a marker mechanism, which enhances relation extraction (RE) process by highlighting entities. However, these models primarily focus on generating correct labels. In doing so, they neglect the label selection process. Moreover, they fail to adequately capture the intricate interactions between entity pairs. To overcome these limitations, we develop a Candidate Label Markers (CLMs) mechanism that prioritizes strategic label selection over simple label generation. Furthermore, we facilitate interactions among diverse relation pairs, enabling the identification of more intricate relational patterns. Experimental results show that we achieve a new SOTA performance. Specifically, based on the same Named Entity Recognition (NER) results as theirs, we improve the SOTA methods by 2.5%, 1.9%, 1.2% in terms of strict F1 scores on SciERC, ACE05 and ACE04.",
        "author": "Yuxuan Feng; Qian Chen; Qianyou Wu; Xin Guo; Suge Wang",
        "authorids": "/y/yuxuan-feng/; /q/qian-chen/; /q/qianyou-wu/; /x/xin-guo/; /s/suge-wang/",
        "bibtex": "@inproceedings{feng-etal-2025-sure,\n    title = \"{SURE}: Mutually Visible Objects and Self-generated Candidate Labels For Relation Extraction\",\n    author = \"Feng, Yuxuan  and\n      Chen, Qian  and\n      Wu, Qianyou  and\n      Guo, Xin  and\n      Wang, Suge\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.31/\",\n    pages = \"459--468\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.31.pdf",
        "site": "https://aclanthology.org/2025.coling-main.31/",
        "pdf_size": 648912,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:it3p2n5GUIYJ:scholar.google.com/&scioq=SURE:+Mutually+Visible+Objects+and+Self-generated+Candidate+Labels+For+Relation+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer and Information Technology, Shanxi University+Key Laboratory of Computational Intelligence and Chinese Information Processing; School of Computer and Information Technology, Shanxi University+Key Laboratory of Computational Intelligence and Chinese Information Processing; School of Computer and Information Technology, Shanxi University; School of Computer and Information Technology, Shanxi University; School of Computer and Information Technology, Shanxi University+Key Laboratory of Computational Intelligence and Chinese Information Processing",
        "aff_domain": "gmail.com;sxu.edu.cn;gmail.com;sxu.edu.cn;sxu.edu.cn",
        "email": "gmail.com;sxu.edu.cn;gmail.com;sxu.edu.cn;sxu.edu.cn",
        "github": "https://github.com/Hiaa1/SURE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;0;0+1",
        "aff_unique_norm": "Shanxi University;Key Laboratory of Computational Intelligence and Chinese Information Processing",
        "aff_unique_dep": "School of Computer and Information Technology;",
        "aff_unique_url": "http://www.sxu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.35",
        "title": "SVD-GCL: A Noise-Augmented Hybrid Graph Contrastive Learning Framework for Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, deep graph neural networks (GNNs) have emerged as the predominant architecture for recommender systems based on collaborative filtering. Nevertheless, numerous GNN-based approaches confront challenges such as complex computations and skewed feature distributions, especially with high-dimensional, sparse, and noisy data, making it difficult to accurately capture user preferences. To tackle these issues, we introduce SVD-GCL, a streamlined graph contrastive learning recommendation model based on noise augmentation that integrates truncated singular value decomposition in the feature engineering stage. This hybrid optimization approach reduces the dimensionality and denoises the original data. Through extracting self-supervised signals and gradually adding noise to embeddings in the training phase to enrich data samples, the data sparsity is effectively alleviated. Experimental outcomes on three large public benchmark datasets illustrate that SVD-GCL effectively manages high-dimensional sparse data, remains stable in the presence of noise, and provides significant advantages in computational efficiency, recommendation performance, and robustness.",
        "author": "Liping Wang; Shichao Li; Hui Wang; Yuyan Gao; Mingyao Wei",
        "authorids": "/l/liping-wang/; /s/shichao-li/; /h/hui-wang/; /y/yuyan-gao/; /m/mingyao-wei/",
        "bibtex": "@inproceedings{wang-etal-2025-svd,\n    title = \"{SVD}-{GCL}: A Noise-Augmented Hybrid Graph Contrastive Learning Framework for Recommendation\",\n    author = \"Wang, Liping  and\n      Li, Shichao  and\n      Wang, Hui  and\n      Gao, Yuyan  and\n      Wei, Mingyao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.35/\",\n    pages = \"529--539\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.35.pdf",
        "site": "https://aclanthology.org/2025.coling-main.35/",
        "pdf_size": 2377256,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:uw6j5nhrW8kJ:scholar.google.com/&scioq=SVD-GCL:+A+Noise-Augmented+Hybrid+Graph+Contrastive+Learning+Framework+for+Recommendation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",
        "aff_domain": "zjut.edu.cn;zjut.edu.cn;zjut.edu.cn;zjut.edu.cn;zjut.edu.cn",
        "email": "zjut.edu.cn;zjut.edu.cn;zjut.edu.cn;zjut.edu.cn;zjut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University of Technology",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "https://www.zjut.edu.cn",
        "aff_unique_abbr": "ZJUT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.195",
        "title": "Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose SCAFFOLD prompting that scaffolds coordinates to promote vision-language coordination. Specifically, SCAFFOLD overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of SCAFFOLD over the textual Chain-of-Thought prompting.",
        "author": "Xuanyu Lei; Zonghan Yang; Xinrui Chen; Peng Li; Yang Liu",
        "authorids": "/x/xuanyu-lei/; /z/zonghan-yang/; /x/xinrui-chen/; /p/peng-li/; /y/yang-liu/",
        "bibtex": "@inproceedings{lei-etal-2025-scaffolding,\n    title = \"Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models\",\n    author = \"Lei, Xuanyu  and\n      Yang, Zonghan  and\n      Chen, Xinrui  and\n      Li, Peng  and\n      Liu, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.195/\",\n    pages = \"2886--2903\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.195.pdf",
        "site": "https://aclanthology.org/2025.coling-main.195/",
        "pdf_size": 15063085,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1863564332918590645&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/THUNLP-MT/Scaffold",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.713",
        "title": "Scalability of Bayesian Network Structure Elicitation with Large Language Models: a Novel Methodology and Comparative Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this work, we propose a novel method for Bayesian Networks (BNs) structure elicitation that is based on the initialization of several LLMs with different experiences, independently querying them to create a structure of the BN, and further obtaining the final structure by majority voting. We compare the method with one alternative method on various widely and not widely known BNs of different sizes and study the scalability of both methods on them. We also propose an approach to check the contamination of BNs in LLM, which shows that some widely known BNs are inapplicable for testing the LLM usage for BNs structure elicitation. We also show that some BNs may be inapplicable for such experiments because their node names are indistinguishable. The experiments on the other BNs show that our method performs better than the existing method with one of the three studied LLMs; however, the performance of both methods significantly decreases with the increase in BN size.",
        "author": "Nikolay Babakov; Ehud Reiter; Alberto Bugar\u00edn-Diz",
        "authorids": "/n/nikolay-babakov/; /e/ehud-reiter/; /a/alberto-bugarin-diz/",
        "bibtex": "@inproceedings{babakov-etal-2025-scalability,\n    title = \"Scalability of {B}ayesian Network Structure Elicitation with Large Language Models: a Novel Methodology and Comparative Analysis\",\n    author = \"Babakov, Nikolay  and\n      Reiter, Ehud  and\n      Bugar{\\'i}n-Diz, Alberto\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.713/\",\n    pages = \"10685--10711\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.713.pdf",
        "site": "https://aclanthology.org/2025.coling-main.713/",
        "pdf_size": 583611,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13586655604326924289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "CiTIUS, Universidade de Santiago de Compostela; University of Aberdeen; CiTIUS, Universidade de Santiago de Compostela",
        "aff_domain": "usc.es;abdn.ac.uk;usc.es",
        "email": "usc.es;abdn.ac.uk;usc.es",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Universidade de Santiago de Compostela;University of Aberdeen",
        "aff_unique_dep": "CiTIUS;",
        "aff_unique_url": "https://www.usc.es;https://www.abdn.ac.uk",
        "aff_unique_abbr": "USC;Aberdeen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Spain;United Kingdom"
    },
    {
        "id": "2025.coling-main.144",
        "title": "Scene Graph and Dependency Grammar Enhanced Remote Sensing Change Caption Network (SGD-RSCCN)",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the continuous advancement of remote sensing technology, it is easier to obtain high-resolution, multi-temporal and multi-spectral images. The images carry rich information of ground objects. However, how to effectively extract useful information from the complex image data and convert it into understandable semantic descriptions remains a challenge. To deal with the challenges, we propose a Scene Graph and Dependency Grammar Enhanced Remote Sensing Change Caption Network (SGD-RSCCN) to improve the accuracy and naturalness of extracting and describing change information from remote sensing images. By combining advanced visual analysis technology and natural language processing technology, the network not only optimizes the problem of insufficient understanding of complex scenes, but also enhances the ability to capture dynamic changes, thereby generating more accurate and smooth natural language description. In addition, we also proposes the decoder based on prior knowledge, which further improves the readability and comprehensibility of the description. Extensive experiments on LEVIR-CC and Dubai-CC datasets verify the advantages of the proposed method in generating accurate and true descriptions.",
        "author": "Qiaoli Sun; Yan Wang; Xiaoyu Song",
        "authorids": "/q/qiaoli-sun/; /y/yan-wang/; /x/xiaoyu-song/",
        "bibtex": "@inproceedings{sun-etal-2025-scene,\n    title = \"Scene Graph and Dependency Grammar Enhanced Remote {S}ensing Change Caption Network ({SGD}-{RSCCN})\",\n    author = \"Sun, Qiaoli  and\n      Wang, Yan  and\n      Song, Xiaoyu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.144/\",\n    pages = \"2121--2130\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.144.pdf",
        "site": "https://aclanthology.org/2025.coling-main.144/",
        "pdf_size": 705758,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fGmciHL3WFgJ:scholar.google.com/&scioq=Scene+Graph+and+Dependency+Grammar+Enhanced+Remote+Sensing+Change+Caption+Network+(SGD-RSCCN)&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "College of Computer Science, Inner Mongolia University; College of Computer Science, Inner Mongolia University; Computer Engineering, Portland State University",
        "aff_domain": "mail.imu.edu.cn;imu.edu.cn;pdx.edu",
        "email": "mail.imu.edu.cn;imu.edu.cn;pdx.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Inner Mongolia University;Portland State University",
        "aff_unique_dep": "College of Computer Science;Computer Engineering",
        "aff_unique_url": "http://www.imu.edu.cn;https://www.pdx.edu",
        "aff_unique_abbr": ";PSU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Portland",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-main.667",
        "title": "Searching for Structure: Investigating Emergent Communication with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.",
        "author": "Tom Kouwenhoven; Max Peeperkorn; Tessa Verhoef",
        "authorids": "/t/tom-kouwenhoven/; /m/max-peeperkorn/; /t/tessa-verhoef/",
        "bibtex": "@inproceedings{kouwenhoven-etal-2025-searching,\n    title = \"Searching for Structure: Investigating Emergent Communication with Large Language Models\",\n    author = \"Kouwenhoven, Tom  and\n      Peeperkorn, Max  and\n      Verhoef, Tessa\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.667/\",\n    pages = \"9977--9991\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.667.pdf",
        "site": "https://aclanthology.org/2025.coling-main.667/",
        "pdf_size": 1275251,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18378648130873347345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Leiden Institute of Advanced Computer Science, Leiden University, Netherlands; School of Computing, University of Kent, United Kingdom; Leiden Institute of Advanced Computer Science, Leiden University, Netherlands",
        "aff_domain": "liacs.leidenuniv.nl; ;liacs.leidenuniv.nl",
        "email": "liacs.leidenuniv.nl; ;liacs.leidenuniv.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Leiden University;University of Kent",
        "aff_unique_dep": "Leiden Institute of Advanced Computer Science;School of Computing",
        "aff_unique_url": "https://www.universiteitleiden.nl;https://www.kent.ac.uk",
        "aff_unique_abbr": "LU;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Leiden;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "id": "2025.coling-industry.35",
        "title": "Seeing Beyond: Enhancing Visual Question Answering with Multi-Modal Retrieval",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Multi-modal Large language models (MLLMs) have made significant strides in complex content understanding and reasoning. However, they still suffer from model hallucination and lack of specific knowledge when facing challenging questions. To address these limitations, retrieval augmented generation (RAG) has emerged as an effective solution. While incorporating knowledge has led to improvements, it also highlights the need for a more robust knowledge selection strategy. For multi-modal tasks, such as visual question answering (VQA), integrating all modalities is crucial in providing comprehensive information for accurate answers. Therefore, we propose to construct an encoder model for extracting joint embedding from all modalities, enabling alignment between the corresponding query and knowledge through contrastive learning. To further improve performance, we introduce an additional MLLM re-selection step, which selects the best matching knowledge from the top-k retrieved results of our alignment model. We evaluated our method, SeBe-VQA, on the Encyclopedic VQA dataset. Our knowledge retrieval results demonstrate the benefit of our multi-modal framework. By incorporating the retrieved knowledge along with the question, we achieve a significant performance improvement compared with the previous method and scenarios without knowledge provision.",
        "author": "Boqi Chen; Anuj Khare; Gaurav Kumar; Arjun Akula; Pradyumna Narayana",
        "authorids": "/b/boqi-chen/; /a/anuj-khare/; /g/gaurav-kumar/; /a/arjun-akula/; /p/pradyumna-narayana/",
        "bibtex": "@inproceedings{chen-etal-2025-seeing,\n    title = \"Seeing Beyond: Enhancing Visual Question Answering with Multi-Modal Retrieval\",\n    author = \"Chen, Boqi  and\n      Khare, Anuj  and\n      Kumar, Gaurav  and\n      Akula, Arjun  and\n      Narayana, Pradyumna\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.35/\",\n    pages = \"410--421\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.35.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.35/",
        "pdf_size": 3999675,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-eeThLQucVYJ:scholar.google.com/&scioq=Seeing+Beyond:+Enhancing+Visual+Question+Answering+with+Multi-Modal+Retrieval&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of North Carolina at Chapel Hill; Google; Google; Google; Google",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of North Carolina;Google",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://www.google.com",
        "aff_unique_abbr": "UNC;Google",
        "aff_campus_unique_index": "0;1;1;1;1",
        "aff_campus_unique": "Chapel Hill;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.601",
        "title": "Selected Languages are All You Need for Cross-lingual Truthfulness Transfer",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Truthfulness stands out as an essential challenge for Large Language Models (LLMs). Although many works have developed various ways for truthfulness enhancement, they seldom focus on truthfulness in multilingual scenarios. Meanwhile, contemporary multilingual aligning technologies struggle to balance massive languages and often exhibit serious truthfulness gaps across different languages, especially those that differ greatly from English. In our work, we extend truthfulness evaluation to multilingual contexts and propose a practical method for cross-lingual truthfulness transfer called Fact-aware Multilingual Selective Synergy (FaMSS). FaMSS is able to select an optimal subset of all tested languages by language bias and transfer contributions, and then employ translation instruction tuning for cross-lingual truthfulness transfer. Experimental results demonstrate that our approach can effectively reduce the multilingual representation disparity and boost cross-lingual truthfulness transfer of LLMs.",
        "author": "Weihao Liu; Ning Wu; Wenbiao Ding; Shining Liang; Ming Gong; Dongmei Zhang",
        "authorids": "/w/weihao-liu/; /n/ning-wu/; /w/wenbiao-ding/; /s/shining-liang/; /m/ming-gong/; /d/dongmei-zhang/",
        "bibtex": "@inproceedings{liu-etal-2025-selected,\n    title = \"Selected Languages are All You Need for Cross-lingual Truthfulness Transfer\",\n    author = \"Liu, Weihao  and\n      Wu, Ning  and\n      Ding, Wenbiao  and\n      Liang, Shining  and\n      Gong, Ming  and\n      Zhang, Dongmei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.601/\",\n    pages = \"8963--8978\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.601.pdf",
        "site": "https://aclanthology.org/2025.coling-main.601/",
        "pdf_size": 1416261,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fsw2a7xB-w8J:scholar.google.com/&scioq=Selected+Languages+are+All+You+Need+for+Cross-lingual+Truthfulness+Transfer&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Microsoft STC Asia, Beijing; Microsoft STC Asia, Beijing; Microsoft STC Asia, Beijing; Microsoft STC Asia, Beijing; Microsoft STC Asia, Beijing; Microsoft STC Asia, Beijing",
        "aff_domain": "outlook.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "outlook.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/FaMSS",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "STC Asia",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.686",
        "title": "Self-Evolution Knowledge Distillation for LLM-based Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge distillation (KD) has shown great promise in transferring knowledge from larger teacher models to smaller student models. However, existing KD strategies for large language models often minimize output distributions between student and teacher models indiscriminately for each token. This overlooks the imbalanced nature of tokens and their varying transfer difficulties. In response, we propose a distillation strategy called Self-Evolution KD. The core of this approach involves dynamically integrating teacher distribution and one-hot distribution of ground truth into the student distribution as prior knowledge, which promotes the distillation process. It adjusts the ratio of prior knowledge based on token learning difficulty, fully leveraging the teacher model\u2019s potential. Experimental results show our method brings an average improvement of approximately 1.4 SacreBLEU points across four translation directions in the WMT22 test sets. Further analysis indicates that the improvement comes from better knowledge transfer from teachers, confirming our hypothesis.",
        "author": "Yuncheng Song; Liang Ding; Changtong Zan; Shujian Huang",
        "authorids": "/y/yuncheng-song/; /l/liang-ding/; /c/changtong-zan/; /s/shujian-huang/",
        "bibtex": "@inproceedings{song-etal-2025-self,\n    title = \"Self-Evolution Knowledge Distillation for {LLM}-based Machine Translation\",\n    author = \"Song, Yuncheng  and\n      Ding, Liang  and\n      Zan, Changtong  and\n      Huang, Shujian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.686/\",\n    pages = \"10298--10308\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.686.pdf",
        "site": "https://aclanthology.org/2025.coling-main.686/",
        "pdf_size": 1291438,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aejqbxQMoNEJ:scholar.google.com/&scioq=Self-Evolution+Knowledge+Distillation+for+LLM-based+Machine+Translation&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Nanjing University\u2661; The University of Sydney\u211c; China University of Petroleum (East China)\u2660; Nanjing University\u2661",
        "aff_domain": "smail.nju.edu.cn;gmail.com;s.upc.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;gmail.com;s.upc.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Nanjing University;The University of Sydney;China University of Petroleum",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.sydney.edu.au;http://www.cup.edu.cn",
        "aff_unique_abbr": "Nanjing U;USYD;CUP",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";East China",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2025.coling-main.457",
        "title": "SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Traditional methods for evaluating the robustness of large language models (LLMs) often rely on standardized benchmarks, which can escalate costs and limit evaluations across varied domains. This paper introduces a novel framework designed to autonomously evaluate the robustness of LLMs by incorporating refined adversarial prompts and domain-constrained knowledge guidelines in the form of knowledge graphs. Our method systematically generates descriptive sentences from domain-constrained knowledge graph triplets to formulate adversarial prompts, enhancing the relevance and challenge of the evaluation. These prompts, generated by the LLM itself and tailored to evaluate its own robustness, undergo a rigorous filtering and refinement process, ensuring that only those with high textual fluency and semantic fidelity are used. This self-evaluation mechanism allows the LLM to evaluate its robustness without the need for external benchmarks. We assess the effectiveness of our framework through extensive testing on both proprietary models like ChatGPT and open-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that our approach not only reduces dependency on conventional data but also provides a targeted and efficient means of evaluating LLM robustness in constrained domains.",
        "author": "Aihua Pei; Zehua Yang; Shunan Zhu; Ruoxi Cheng; Ju Jia",
        "authorids": "/a/aihua-pei/; /z/zehua-yang/; /s/shunan-zhu/; /r/ruoxi-cheng/; /j/ju-jia/",
        "bibtex": "@inproceedings{pei-etal-2025-selfprompt,\n    title = \"{S}elf{P}rompt: Autonomously Evaluating {LLM} Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts\",\n    author = \"Pei, Aihua  and\n      Yang, Zehua  and\n      Zhu, Shunan  and\n      Cheng, Ruoxi  and\n      Jia, Ju\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.457/\",\n    pages = \"6840--6854\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.457.pdf",
        "site": "https://aclanthology.org/2025.coling-main.457/",
        "pdf_size": 768880,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1360353220795080063&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Waseda University; Waseda University; Waseda University; Southeast University; Southeast University",
        "aff_domain": "fuji.waseda.jp;akane.waseda.jp;ruri.waseda.jp;seu.edu.cn;seu.edu.cn",
        "email": "fuji.waseda.jp;akane.waseda.jp;ruri.waseda.jp;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Waseda University;Southeast University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.waseda.jp/top;https://www.seu.edu.cn/",
        "aff_unique_abbr": "Waseda;SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "Japan;China"
    },
    {
        "id": "2025.coling-main.536",
        "title": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which translates natural language into formal code representations. However, the reverse operation, translating code into natural language, termed semantic captioning, has received less attention. This task is increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQ2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose semantic parsing datasets for semantic captioning, specifically SQL2Text. To overcome the limited robustness of Text2SQL datasets for the reverse task, we introduce an iterative ICL prompt leveraging GPT-4o to generate multiple additional utterances. We conduct experiments across multiple in-context learning (ICL) methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for few-shot ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative approaches. Dataset and codes are accessible.",
        "author": "Ali Al Lawati; Jason Lucas; Prasenjit Mitra",
        "authorids": "/a/ali-al-lawati/; /j/jason-lucas/; /p/prasenjit-mitra/",
        "bibtex": "@inproceedings{al-lawati-etal-2025-semantic,\n    title = \"Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for {SQL}2{T}ext\",\n    author = \"Al Lawati, Ali  and\n      Lucas, Jason  and\n      Mitra, Prasenjit\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.536/\",\n    pages = \"8026--8042\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.536.pdf",
        "site": "https://aclanthology.org/2025.coling-main.536/",
        "pdf_size": 802027,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YPGBkCYrARkJ:scholar.google.com/&scioq=Semantic+Captioning:+Benchmark+Dataset+and+Graph-Aware+Few-Shot+In-Context+Learning+for+SQL2Text&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/aliwister/ast-icl",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.572",
        "title": "Semantic Reshuffling with LLM and Heterogeneous Graph Auto-Encoder for Enhanced Rumor Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Social media is crucial for information spread, necessitating effective rumor detection to curb misinformation\u2019s societal effects. Current methods struggle against complex propagation influenced by bots, coordinated accounts, and echo chambers, which fragment information and increase risks of misjudgments and model vulnerability. To counteract these issues, we introduce a new rumor detection framework, the Narrative-Integrated Metapath Graph Auto-Encoder (NIMGA). This model consists of two core components: (1) Metapath-based Heterogeneous Graph Reconstruction. (2) Narrative Reordering and Perspective Fusion. The first component dynamically reconstructs propagation structures to capture complex interactions and hidden pathways within social networks, enhancing accuracy and robustness. The second implements a dual-agent mechanism for viewpoint distillation and comment narrative reordering, using LLMs to refine diverse perspectives and semantic evolution, revealing patterns of information propagation and latent semantic correlations among comments. Extensive testing confirms our model outperforms existing methods, demonstrating its effectiveness and robustness in enhancing rumor representation through graph reconstruction and narrative reordering.",
        "author": "Guoyi Li; Die Hu; Zongzhen Liu; Xiaodan Zhang; Honglei Lyu",
        "authorids": "/g/guoyi-li/; /d/die-hu/; /z/zongzhen-liu/; /x/xiaodan-zhang/; /h/honglei-lyu/",
        "bibtex": "@inproceedings{li-etal-2025-semantic,\n    title = \"Semantic Reshuffling with {LLM} and Heterogeneous Graph Auto-Encoder for Enhanced Rumor Detection\",\n    author = \"Li, Guoyi  and\n      Hu, Die  and\n      Liu, Zongzhen  and\n      Zhang, Xiaodan  and\n      Lyu, Honglei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.572/\",\n    pages = \"8557--8572\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.572.pdf",
        "site": "https://aclanthology.org/2025.coling-main.572/",
        "pdf_size": 2940004,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12660791933071000042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.23",
        "title": "Semantic Role Labeling of NomBank Partitives",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using \u201cgold\u201d parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development.",
        "author": "Adam Meyers; Advait Pravin Savant; John E. Ortega",
        "authorids": "/a/adam-meyers/; /a/advait-pravin-savant/; /j/john-e-ortega/",
        "bibtex": "@inproceedings{meyers-etal-2025-semantic,\n    title = \"Semantic Role Labeling of {N}om{B}ank Partitives\",\n    author = \"Meyers, Adam  and\n      Savant, Advait Pravin  and\n      Ortega, John E.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.23/\",\n    pages = \"324--336\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.23.pdf",
        "site": "https://aclanthology.org/2025.coling-main.23/",
        "pdf_size": 511494,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AGbodYqs1WcJ:scholar.google.com/&scioq=Semantic+Role+Labeling+of+NomBank+Partitives&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "New York University; New York University; Northeastern University",
        "aff_domain": "cs.nyu.edu;nyu.edu;northeastern.edu",
        "email": "cs.nyu.edu;nyu.edu;northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "New York University;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nyu.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "NYU;NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.622",
        "title": "Semantic and Sentiment Dual-Enhanced Generative Model for Script Event Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Script Event Prediction (SEP) aims to forecast the next event in a sequence from a list of candidates. Traditional methods often use pre-trained language models to model event associations but struggle with semantic ambiguity and embedding bias. Semantic ambiguity arises from the multiple meanings of identical words and insufficient consideration of event arguments, while embedding bias results from assigning similar word embeddings to event pairs with similar lexical features, despite their different meanings. To address above issues, we propose a the Semantic and Sentiment Dual-enhanced Generative Model (SSD-GM). SSD-GM leverages two types of script event information to enhance the generative model. Specifically, it employs a GNN-based semantic structure aggregator to integrate the event-centric structure information, thereby mitigating the impact of semantic ambiguity. Furthermore, we find that local sentiment variability effectively reduces biases in event embeddings, while maintaining global sentiment consistency enhances predictive accuracy. As a result, SSD-GM adeptly captures both global and local sentiment of events through its sentiment information awareness mechanism. Extensive experiments on the Multi-Choice Narrative Cloze (MCNC) task demonstrate that our approach achieves better results than other state-of-the-art baselines.",
        "author": "Feiyang Wu; Peixin Huang; Yanli Hu; Zhen Tan; Xiang Zhao",
        "authorids": "/f/feiyang-wu/; /p/peixin-huang/; /y/yanli-hu/; /z/zhen-tan/; /x/xiang-zhao/",
        "bibtex": "@inproceedings{wu-etal-2025-semantic,\n    title = \"Semantic and Sentiment Dual-Enhanced Generative Model for Script Event Prediction\",\n    author = \"Wu, Feiyang  and\n      Huang, Peixin  and\n      Hu, Yanli  and\n      Tan, Zhen  and\n      Zhao, Xiang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.622/\",\n    pages = \"9250--9259\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.622.pdf",
        "site": "https://aclanthology.org/2025.coling-main.622/",
        "pdf_size": 519579,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1oxsS5ffQU8J:scholar.google.com/&scioq=Semantic+and+Sentiment+Dual-Enhanced+Generative+Model+for+Script+Event+Prediction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Laboratory for Big Data and Decision, National University of Defense Technology, China; National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; National Key Laboratory of Information Systems Engineering, National University of Defense Technology, China; Laboratory for Big Data and Decision, National University of Defense Technology, China",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "Laboratory for Big Data and Decision",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.419",
        "title": "Semi-Automated Construction of Sense-Annotated Datasets for Practically Any Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "High-quality sense-annotated datasets are vital for evaluating and comparing WSD systems. We present a novel approach to creating parallel sense-annotated datasets, which can be applied to any language that English can be translated into. The method incorporates machine translation, word alignment, sense projection, and sense filtering to produce silver annotations, which can then be revised manually to obtain gold datasets. By applying our method to Farsi, Chinese, and Bengali, we produce new parallel benchmark datasets, which are vetted by native speakers of each language. Our automatically-generated silver datasets are of higher quality than the annotations obtained with recent multilingual WSD systems, particularly on non-European languages.",
        "author": "Jai Riley; Bradley M. Hauer; Nafisa Sadaf Hriti; Guoqing Luo; Amir Reza Mirzaei; Ali Rafiei; Hadi Sheikhi; Mahvash Siavashpour; Mohammad Tavakoli; Ning Shi; Grzegorz Kondrak",
        "authorids": "/j/jai-riley/; /b/bradley-m-hauer/; /n/nafisa-sadaf-hriti/; /g/guoqing-luo/; /a/amir-reza-mirzaei/; /a/ali-rafiei/; /h/hadi-sheikhi/; /m/mahvash-siavashpour/; /m/mohammad-tavakoli/; /n/ning-shi/; /g/grzegorz-kondrak/",
        "bibtex": "@inproceedings{riley-etal-2025-semi,\n    title = \"Semi-Automated Construction of Sense-Annotated Datasets for Practically Any Language\",\n    author = \"Riley, Jai  and\n      Hauer, Bradley M.  and\n      Hriti, Nafisa Sadaf  and\n      Luo, Guoqing  and\n      Mirzaei, Amir Reza  and\n      Rafiei, Ali  and\n      Sheikhi, Hadi  and\n      Siavashpour, Mahvash  and\n      Tavakoli, Mohammad  and\n      Shi, Ning  and\n      Kondrak, Grzegorz\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.419/\",\n    pages = \"6270--6284\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.419.pdf",
        "site": "https://aclanthology.org/2025.coling-main.419/",
        "pdf_size": 474101,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5lIV0D7HZW4J:scholar.google.com/&scioq=Semi-Automated+Construction+of+Sense-Annotated+Datasets+for+Practically+Any+Language&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute",
        "aff_domain": "ualberta.ca; ; ; ; ; ; ; ; ; ;ualberta.ca",
        "email": "ualberta.ca; ; ; ; ; ; ; ; ; ;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Alberta Machine Intelligence Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ami.ualberta.ca/",
        "aff_unique_abbr": "AMII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2025.coling-main.4",
        "title": "Sequential Fusion of Text-close and Text-far Representations for Multimodal Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal Sentiment Analysis (MSA) aims to identify human attitudes from diverse modalities such as visual, audio and text modalities. Recent studies suggest that the text modality tends to be the most effective, which has encouraged models to consider text as its core modality. However, previous methods primarily concentrate on projecting modalities other than text into a space close to the text modality and learning an identical representation, which does not fully make use of the auxiliary information provided by audio and visual modalities. In this paper, we propose a framework, Sequential Fusion of Text-close and Text-far Representations (SFTTR), aiming to refine multimodal representations from multimodal data which should contain both representations close to and far from the text modality. Specifically, we employ contrastive learning to sufficiently explore the information similarities and differences between text and audio/visual modalities. Moreover, to fuse the extracted representations more effectively, we design a sequential cross-modal encoder to sequentially fuse representations that are close to and far from the text modality.",
        "author": "Kaiwei Sun; Mi Tian",
        "authorids": "/k/kaiwei-sun/; /m/mi-tian/",
        "bibtex": "@inproceedings{sun-tian-2025-sequential,\n    title = \"Sequential Fusion of Text-close and Text-far Representations for Multimodal Sentiment Analysis\",\n    author = \"Sun, Kaiwei  and\n      Tian, Mi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.4/\",\n    pages = \"40--49\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.4.pdf",
        "site": "https://aclanthology.org/2025.coling-main.4/",
        "pdf_size": 1013559,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8448291415726398256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Key Laboratory of Data Engineering and Visual Computing, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Data Engineering and Visual Computing, Chongqing University of Posts and Telecommunications, Chongqing, China",
        "aff_domain": "cqupt.edu.cn;stu.cqupt.edu.cn",
        "email": "cqupt.edu.cn;stu.cqupt.edu.cn",
        "github": "https://github.com/Mi7914/SFTTR",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chongqing University of Posts and Telecommunications",
        "aff_unique_dep": "Key Laboratory of Data Engineering and Visual Computing",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chongqing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.604",
        "title": "Should We Use a Fixed Embedding Size? Customized Dimension Sizes for Knowledge Graph Embedding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge Graph Embedding (KGE) aims to project entities and relations into a low-dimensional space, so as to enable Knowledge Graphs (KGs) to be effectively used by downstream AI tasks. Most existing KGs (e.g. Wikidata) suffer from the data imbalance issue, i.e., the occurrence frequencies vary significantly among different entities. Current KGE models use a fixed embedding size, leading to overfitting for low-frequency entities and underfitting for high-frequency ones. A simple method is to manually set embedding sizes based on frequency, but this is not feasible due to the complexity and the large number of entities. To this end, we propose CustomizE, which customizes embedding sizes in a data-driven way, assigning larger sizes for high-frequency entities and smaller sizes for low-frequency ones. We use bilevel optimization for stable learning of representations and sizes. It is noteworthy that our framework is universal and flexible, which is suitable for various KGE models. Experiments on link prediction tasks show its superiority over state-of-the-art baselines.",
        "author": "Zhanpeng Guan; Zhao Zhang; Yiqing Wu; Fuwei Zhang; Yongjun Xu",
        "authorids": "/z/zhanpeng-guan/; /z/zhao-zhang/; /y/yiqing-wu/; /f/fuwei-zhang/; /y/yongjun-xu/",
        "bibtex": "@inproceedings{guan-etal-2025-use,\n    title = \"Should We Use a Fixed Embedding Size? Customized Dimension Sizes for Knowledge Graph Embedding\",\n    author = \"Guan, Zhanpeng  and\n      Zhang, Zhao  and\n      Wu, Yiqing  and\n      Zhang, Fuwei  and\n      Xu, Yongjun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.604/\",\n    pages = \"9006--9012\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.604.pdf",
        "site": "https://aclanthology.org/2025.coling-main.604/",
        "pdf_size": 454288,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:j-CWElY9xFUJ:scholar.google.com/&scioq=Should+We+Use+a+Fixed+Embedding+Size%3F+Customized+Dimension+Sizes+for+Knowledge+Graph+Embedding&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Artificial Intelligence, Beihang University; Institute of Computing Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;buaa.edu.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;buaa.edu.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beihang University",
        "aff_unique_dep": "Institute of Computing Technology;;Institute of Artificial Intelligence",
        "aff_unique_url": "http://www.ict.ac.cn;http://www.ucas.ac.cn;http://www.buaa.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;Beihang",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.10",
        "title": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language Models via Sensible and Visionary Commonsense Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, there has been a heightened interest in building chatbots based on Large Language Models (LLMs) to emulate human-like qualities in multi-turn conversations. Despite having access to commonsense knowledge to better understand the psychological aspects and causality of dialogue context, even these powerful LLMs struggle to achieve the goals of empathy and emotional support. Current commonsense knowledge derived from dialogue contexts is inherently limited and often fails to adequately anticipate the future course of a dialogue. This lack of foresight can mislead LLMs and hinder their ability to provide effective support. In response to this challenge, we present an innovative framework named Sensible and Visionary Commonsense Knowledge (Sibyl). Designed to concentrate on the immediately succeeding dialogue, this paradigm equips LLMs with the capability to uncover the implicit requirements of the conversation, aiming to elicit more empathetic responses. Experimental results demonstrate that incorporating our paradigm for acquiring commonsense knowledge into LLMs comprehensively enhances the quality of their responses.",
        "author": "Lanrui Wang; Jiangnan Li; Chenxu Yang; Zheng Lin; Hongyin Tang; Huan Liu; Yanan Cao; Jingang Wang; Weiping Wang",
        "authorids": "/l/lanrui-wang/; /j/jiangnan-li/; /c/chenxu-yang/; /z/zheng-lin/; /h/hongyin-tang/; /h/huan-liu/; /y/yanan-cao/; /j/jingang-wang/; /w/weiping-wang/",
        "bibtex": "@inproceedings{wang-etal-2025-sibyl,\n    title = \"Sibyl: Empowering Empathetic Dialogue Generation in Large Language Models via Sensible and Visionary Commonsense Inference\",\n    author = \"Wang, Lanrui  and\n      Li, Jiangnan  and\n      Yang, Chenxu  and\n      Lin, Zheng  and\n      Tang, Hongyin  and\n      Liu, Huan  and\n      Cao, Yanan  and\n      Wang, Jingang  and\n      Wang, Weiping\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.10/\",\n    pages = \"123--140\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.10.pdf",
        "site": "https://aclanthology.org/2025.coling-main.10/",
        "pdf_size": 1740422,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pk88--aOstQJ:scholar.google.com/&scioq=Sibyl:+Empowering+Empathetic+Dialogue+Generation+in+Large+Language+Models+via+Sensible+and+Visionary+Commonsense+Inference&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/wlr737/Sibyl",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2025.coling-main.177",
        "title": "Simulating Dual-Process Thinking in Dialogue Topic Shift Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous work on dialogue topic shift detection has primarily focused on shallow local reasoning, overlooking the importance of considering the global historical structure and local details to elucidate the underlying causes of topic shift. To address the above two issues, we introduce the dual-process theory to this task and design a novel Dual-Module Framework DMF (i.e., intuition and reasoning module) for dialogue topic shift detection to emulate this cognitive process. Specifically, the intuition module employs Large Language Models (LLMs) to extract and store the global topic structure of historical dialogue, while the reasoning module introduces a LLM to generate reasoning samples between the response and the most recent topic of historical dialogue, thereby providing local detail explanations for topic shift. Moreover, we distill the dual-module framework into a small generative model to facilitate more precise reasoning. The experimental results on three public datasets show that our DMF outperforms the state-of-the-art baselines.",
        "author": "Huiyao Wang; Peifeng Li; Yaxin Fan; Qiaoming Zhu",
        "authorids": "/h/huiyao-wang/; /p/peifeng-li/; /y/yaxin-fan/; /q/qiaoming-zhu/",
        "bibtex": "@inproceedings{wang-etal-2025-simulating,\n    title = \"Simulating Dual-Process Thinking in Dialogue Topic Shift Detection\",\n    author = \"Wang, Huiyao  and\n      Li, Peifeng  and\n      Fan, Yaxin  and\n      Zhu, Qiaoming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.177/\",\n    pages = \"2592--2602\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.177.pdf",
        "site": "https://aclanthology.org/2025.coling-main.177/",
        "pdf_size": 969847,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:M160Ev_1is0J:scholar.google.com/&scioq=Simulating+Dual-Process+Thinking+in+Dialogue+Topic+Shift+Detection&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.316",
        "title": "Slender-Mamba: Fully Quantized Mamba in 1.58 Bits From Head to Toe",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have achieved significant performance improvements in natural language processing (NLP) domain. However, these models often require large computational resources for training and inference. Recently, Mamba, a language model architecture based on State-Space Models (SSMs), has achieved comparable performance to Transformer models while significantly reducing costs by compressing context windows during inference. We focused on the potential of the lightweight Mamba architecture by applying BitNet quantization method to the model architecture. In addition, while prior BitNet methods generally quantized only linear layers in the main body, we extensively quantized the embedding and projection layers considering their significant proportion of model parameters. In our experiments, we applied ternary quantization to the Mamba-2 (170M) architecture and pre-trained the model with 150 B tokens from scratch. Our method achieves approximately 90.0% reduction in the bits used by all parameters, achieving a significant improvement compared with a 48.4% reduction by the conventional BitNet quantization method. In addition, our method experienced minimal performance degradation in both the pre-training perplexity and downstream tasks. These findings demonstrate the potential of incorporating lightweight language models into edge devices, which will become more demanding in the future.",
        "author": "Zhenxuan Yu; Takeshi Kojima; Yutaka Matsuo; Yusuke Iwasawa",
        "authorids": "/z/zhenxuan-yu/; /t/takeshi-kojima/; /y/yutaka-matsuo/; /y/yusuke-iwasawa/",
        "bibtex": "@inproceedings{yu-etal-2025-slender,\n    title = \"Slender-Mamba: Fully Quantized Mamba in 1.58 Bits From Head to Toe\",\n    author = \"Yu, Zhenxuan  and\n      Kojima, Takeshi  and\n      Matsuo, Yutaka  and\n      Iwasawa, Yusuke\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.316/\",\n    pages = \"4715--4724\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.316.pdf",
        "site": "https://aclanthology.org/2025.coling-main.316/",
        "pdf_size": 2640651,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14769429841053890110&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Rikkyo University; The University of Tokyo; The University of Tokyo; The University of Tokyo",
        "aff_domain": "rikkyo.ac.jp;weblab.t.u-tokyo.ac.jp;weblab.t.u-tokyo.ac.jp;weblab.t.u-tokyo.ac.jp",
        "email": "rikkyo.ac.jp;weblab.t.u-tokyo.ac.jp;weblab.t.u-tokyo.ac.jp;weblab.t.u-tokyo.ac.jp",
        "github": "https://github.com/YU-ZHENXUAN-ucllm/Slender-Mamba",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Rikkyo University;University of Tokyo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rikkyo.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Rikkyo;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.404",
        "title": "Small Language Models Also Work With Small Vocabularies: Probing the Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent work investigates whether LMs learn human-like linguistic generalizations and representations from developmentally plausible amounts of data. Yet, the basic linguistic units processed in these LMs are determined by subword-based tokenization, which limits their validity as models of learning at and below the word level. In this paper, we explore the potential of tokenization-free, phoneme- and grapheme-based language models. We demonstrate that small models based on the Llama architecture can achieve strong linguistic performance on standard syntactic and novel lexical/phonetic benchmarks when trained with character-level vocabularies. We further show that phoneme-based models almost match grapheme-based models in standard tasks and novel evaluations. Our findings suggest a promising direction for creating more linguistically plausible language models that are better suited for computational studies of language acquisition and processing.",
        "author": "Bastian Bunzeck; Daniel Duran; Leonie Schade; Sina Zarrie\u00df",
        "authorids": "/b/bastian-bunzeck/; /d/daniel-duran/; /l/leonie-schade/; /s/sina-zarriess/",
        "bibtex": "@inproceedings{bunzeck-etal-2025-small,\n    title = \"Small Language Models Also Work With Small Vocabularies: Probing the Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas\",\n    author = \"Bunzeck, Bastian  and\n      Duran, Daniel  and\n      Schade, Leonie  and\n      Zarrie{\\ss}, Sina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.404/\",\n    pages = \"6039--6048\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.404.pdf",
        "site": "https://aclanthology.org/2025.coling-main.404/",
        "pdf_size": 280442,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5446301033000691614&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "CRC 1646 \u2013 Linguistic Creativity in Communication, Department of Linguistics, Bielefeld University, Germany; CRC 1646 \u2013 Linguistic Creativity in Communication, Department of Linguistics, Bielefeld University, Germany; CRC 1646 \u2013 Linguistic Creativity in Communication, Department of Linguistics, Bielefeld University, Germany; CRC 1646 \u2013 Linguistic Creativity in Communication, Department of Linguistics, Bielefeld University, Germany",
        "aff_domain": "uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de",
        "email": "uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Bielefeld University",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.uni-bielefeld.de",
        "aff_unique_abbr": "Uni Bielefeld",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.437",
        "title": "Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we evaluate the creative fiction writing abilities of a fine-tuned small language model (SLM), BART-large, and compare its performance to human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our evaluation consists of two experiments: (i) a human study in which 68 participants rated short stories from humans and the SLM on grammaticality, relevance, creativity, and attractiveness, and (ii) a qualitative linguistic analysis examining the textual characteristics of stories produced by each model. In the first experiment, BART-large outscored average human writers overall (2.11 vs. 1.85), a 14% relative improvement, though the slight human advantage in creativity was not statistically significant. In the second experiment, qualitative analysis showed that while GPT-4o demonstrated near-perfect coherence and used less cliche phrases, it tended to produce more predictable language, with only 3% of its synopses featuring surprising associations (compared to 15% for BART). These findings highlight how model size and fine-tuning influence the balance between creativity, fluency, and coherence in creative writing tasks, and demonstrate that smaller models can, in certain contexts, rival both humans and larger models.",
        "author": "Guillermo Marco; Luz Rello; Julio Gonzalo",
        "authorids": "/g/guillermo-marco/; /l/luz-rello/; /j/julio-gonzalo/",
        "bibtex": "@inproceedings{marco-etal-2025-small,\n    title = \"Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing {SLM}s with Humans and {LLM}s\",\n    author = \"Marco, Guillermo  and\n      Rello, Luz  and\n      Gonzalo, Julio\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.437/\",\n    pages = \"6552--6570\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.437.pdf",
        "site": "https://aclanthology.org/2025.coling-main.437/",
        "pdf_size": 310045,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11708999936443533551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "UNED, Madrid, Spain; IE University, Madrid, Spain; UNED, Madrid, Spain",
        "aff_domain": "lsi.uned.es; ; ",
        "email": "lsi.uned.es; ; ",
        "github": "https://github.com/grmarco/slm-creativity",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Universidad Nacional de Educacion a Distancia;IE University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uned.es;https://www.ie.edu",
        "aff_unique_abbr": "UNED;IE",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madrid",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-main.654",
        "title": "Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, large language models (LLMs) have significantly improved the performance of text-to-SQL systems. Nevertheless, many state-of-the-art (SOTA) approaches have overlooked the critical aspect of system robustness. Our experiments reveal that while LLM-driven methods excel on standard datasets, their accuracy is notably compromised when faced with adversarial perturbations. To address this challenge, we propose a robust text-to-SQL solution, called Solid-SQL, designed to integrate with various LLMs. We focus on the pre-processing stage, training a robust schema-linking model enhanced by LLM-based data augmentation. Additionally, we design a two-round, structural similarity-based example retrieval strategy for in-context learning. Our method achieves SOTA SQL execution accuracy levels of 82.1% and 58.9% on the general Spider and Bird benchmarks, respectively. Furthermore, experimental results show that Solid-SQL delivers an average improvement of 11.6% compared to baselines on the perturbed Spider-Syn, Spider-Realistic, and Dr. Spider benchmarks.",
        "author": "Geling Liu; Yunzhi Tan; Ruichao Zhong; Yuanzhen Xie; Lingchen Zhao; Qian Wang; Bo Hu; Zang Li",
        "authorids": "/g/geling-liu/; /y/yunzhi-tan/; /r/ruichao-zhong/; /y/yuanzhen-xie/; /l/lingchen-zhao/; /q/qian-wang/; /b/bo-hu/; /z/zang-li/",
        "bibtex": "@inproceedings{liu-etal-2025-solid,\n    title = \"Solid-{SQL}: Enhanced Schema-linking based In-context Learning for Robust Text-to-{SQL}\",\n    author = \"Liu, Geling  and\n      Tan, Yunzhi  and\n      Zhong, Ruichao  and\n      Xie, Yuanzhen  and\n      Zhao, Lingchen  and\n      Wang, Qian  and\n      Hu, Bo  and\n      Li, Zang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.654/\",\n    pages = \"9793--9803\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.654.pdf",
        "site": "https://aclanthology.org/2025.coling-main.654/",
        "pdf_size": 497398,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10869412899486993070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Cyber Science and Engineering, Wuhan University, China+Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education; Big Data and AI Platform Department, Tencent, China; Big Data and AI Platform Department, Tencent, China; Big Data and AI Platform Department, Tencent, China; School of Cyber Science and Engineering, Wuhan University, China+Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education; School of Cyber Science and Engineering, Wuhan University, China+Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education; Big Data and AI Platform Department, Tencent, China; Big Data and AI Platform Department, Tencent, China",
        "aff_domain": "whu.edu.cn;tencent.com;tencent.com;tencent.com;whu.edu.cn;whu.edu.cn;tencent.com;tencent.com",
        "email": "whu.edu.cn;tencent.com;tencent.com;tencent.com;whu.edu.cn;whu.edu.cn;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;2;2;0+1;0+1;2;2",
        "aff_unique_norm": "Wuhan University;Ministry of Education;Tencent",
        "aff_unique_dep": "School of Cyber Science and Engineering;Key Laboratory of Aerospace Information Security and Trusted Computing;Big Data and AI Platform Department",
        "aff_unique_url": "http://www.whu.edu.cn/;;https://www.tencent.com",
        "aff_unique_abbr": "WHU;;Tencent",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Wuhan;",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.455",
        "title": "Speech Foundation Models and Crowdsourcing for Efficient, High-Quality Data Collection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While crowdsourcing is an established solution for facilitating and scaling the collection of speech data, the involvement of non-experts necessitates protocols to ensure final data quality. To reduce the costs of these essential controls, this paper investigates the use of Speech Foundation Models (SFMs) to automate the validation process, examining for the first time the cost/quality trade-off in data acquisition. Experiments conducted on French, German, and Korean data demonstrate that SFM-based validation has the potential to reduce reliance on human validation, resulting in an estimated cost saving of over 40.0% without degrading final data quality. These findings open new opportunities for more efficient, cost-effective, and scalable speech data acquisition.",
        "author": "Beomseok Lee; Marco Gaido; Ioan Calapodescu; Laurent Besacier; Matteo Negri",
        "authorids": "/b/beomseok-lee/; /m/marco-gaido/; /i/ioan-calapodescu/; /l/laurent-besacier/; /m/matteo-negri/",
        "bibtex": "@inproceedings{lee-etal-2025-speech,\n    title = \"Speech Foundation Models and Crowdsourcing for Efficient, High-Quality Data Collection\",\n    author = \"Lee, Beomseok  and\n      Gaido, Marco  and\n      Calapodescu, Ioan  and\n      Besacier, Laurent  and\n      Negri, Matteo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.455/\",\n    pages = \"6816--6826\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.455.pdf",
        "site": "https://aclanthology.org/2025.coling-main.455/",
        "pdf_size": 1727788,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pWGqxzaDL1MJ:scholar.google.com/&scioq=Speech+Foundation+Models+and+Crowdsourcing+for+Efficient,+High-Quality+Data+Collection&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Trento, Italy+ Fondazione Bruno Kessler, Italy+ NA VER LABS Europe, France; Fondazione Bruno Kessler, Italy; NA VER LABS Europe, France; NA VER LABS Europe, France; Fondazione Bruno Kessler, Italy",
        "aff_domain": "unitn.it; ; ; ;",
        "email": "unitn.it; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;1;2;2;1",
        "aff_unique_norm": "University of Trento;Fondazione Bruno Kessler;NA VER LABS",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unitn.it;https://www.fbk.eu;",
        "aff_unique_abbr": "UniTN;FBK;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+1;0;1;1;0",
        "aff_country_unique": "Italy;France"
    },
    {
        "id": "2025.coling-main.266",
        "title": "StoryLLaVA: Enhancing Visual Storytelling with Multi-Modal Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid development of multimodal large language models (MLLMs) has positioned visual storytelling as a crucial area in content creation. However, existing models often struggle to maintain temporal, spatial, and narrative coherence across image sequences, and they frequently lack the depth and engagement of human-authored stories. To address these challenges, we propose Story with Large Language-and-Vision Alignment (StoryLLaVA), a novel framework for enhancing visual storytelling. Our approach introduces a topic-driven narrative optimizer that improves both the training data and MLLM models by integrating image descriptions, topic generation, and GPT-4-based refinements. Furthermore, we employ a preference-based ranked story sampling method that aligns model outputs with human storytelling preferences through positive-negative pairing. These two phases of the framework differ in their training methods: the former uses supervised fine-tuning, while the latter incorporates reinforcement learning with positive and negative sample pairs. Experimental results demonstrate that StoryLLaVA outperforms current models in visual relevance, coherence, and fluency, with LLM-based evaluations confirming the generation of richer and more engaging narratives. The enhanced dataset and model will be made publicly available soon.",
        "author": "Li Yang; Zhiding Xiao; Wenxin Huang; Xian Zhong",
        "authorids": "/l/li-yang/; /z/zhiding-xiao/; /w/wenxin-huang/; /x/xian-zhong/",
        "bibtex": "@inproceedings{yang-etal-2025-storyllava,\n    title = \"{S}tory{LL}a{VA}: Enhancing Visual Storytelling with Multi-Modal Large Language Models\",\n    author = \"Yang, Li  and\n      Xiao, Zhiding  and\n      Huang, Wenxin  and\n      Zhong, Xian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.266/\",\n    pages = \"3936--3951\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.266.pdf",
        "site": "https://aclanthology.org/2025.coling-main.266/",
        "pdf_size": 1805486,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13055957132357919456&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Hubei Key Laboratory of Big Data Intelligent Analysis and Application, School of Computer Science and Information Engineering, Hubei University; Hubei Key Laboratory of Big Data Intelligent Analysis and Application, School of Computer Science and Information Engineering, Hubei University; Hubei Key Laboratory of Big Data Intelligent Analysis and Application, School of Computer Science and Information Engineering, Hubei University; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Artificial Intelligence, Wuhan University of Technology",
        "aff_domain": "hubu.edu.cn;stu.hubu.edu.cn;163.com;whut.edu.cn",
        "email": "hubu.edu.cn;stu.hubu.edu.cn;163.com;whut.edu.cn",
        "github": "https://github.com/XxxZzD/StoryLLaVA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Hubei University;Wuhan University of Technology",
        "aff_unique_dep": "School of Computer Science and Information Engineering;School of Computer Science and Artificial Intelligence",
        "aff_unique_url": "http://www.hubu.edu.cn/;http://www.whut.edu.cn",
        "aff_unique_abbr": ";WUT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-demos.2",
        "title": "Streamlining Biomedical Research with Specialized LLMs",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "In this paper, we propose a novel system that integrates state-of-the-art, domain-specific large language models with advanced information retrieval techniques to deliver comprehensive and context-aware responses. Our approach facilitates seamless interaction among diverse components, enabling cross-validation of outputs to produce accurate, high-quality responses enriched with relevant data, images, tables, and other modalities. We demonstrate the system\u2019s capability to enhance response precision by leveraging a robust question-answering model, significantly improving the quality of dialogue generation.The system provides an accessible platform for real-time, high-fidelity interactions, allowing users to benefit from efficient human-computer interaction, precise retrieval, and simultaneous access to a wide range of literature and data. This dramatically improves the research efficiency of professionals in the biomedical and pharmaceutical domains and facilitates faster, more informed decision-making throughout the R&D process. Furthermore, the system proposed in this paper is available at https://synapse-chat.patsnap.com.",
        "author": "Linqing Chen",
        "authorids": "/l/linqing-chen/",
        "bibtex": "@inproceedings{chen-2025-streamlining,\n    title = \"Streamlining Biomedical Research with Specialized {LLM}s\",\n    author = \"Chen, Linqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.2/\",\n    pages = \"9--19\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.2.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.2/",
        "pdf_size": 3169200,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2158603644484493318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "https://synapse-chat.patsnap.com",
        "author_num": 1
    },
    {
        "id": "2025.coling-main.558",
        "title": "Structured List-Grounded Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-grounded dialogue systems aim to answer user queries by leveraging external information. Previous studies have mainly focused on handling free-form documents, often overlooking structured data such as lists, which can represent a range of nuanced semantic relations. Motivated by the observation that even advanced language models like GPT-3.5 often miss semantic cues from lists, this paper aims to enhance question answering (QA) systems for better interpretation and use of structured lists. To this end, we introduce the LIST2QA dataset, a novel benchmark to evaluate the ability of QA systems to respond effectively using list information. This dataset is created from unlabeled customer service documents using language models and model-based filtering processes to enhance data quality, and can be used to fine-tune and evaluate QA models. Apart from directly generating responses through fine-tuned models, we further explore the explicit use of Intermediate Steps for Lists (ISL), aligning list items with user backgrounds to better reflect how humans interpret list items before generating responses. Our experimental results demonstrate that models trained on LIST2QA with our ISL approach outperform baselines across various metrics. Specifically, our fine-tuned Flan-T5-XL model shows increases of 3.1% in ROUGE-L, 4.6% in correctness, 4.5% in faithfulness, and 20.6% in completeness compared to models without applying filtering and the proposed ISL method.",
        "author": "Mujeen Sung; Song Feng; James Gung; Raphael Shu; Yi Zhang; Saab Mansour",
        "authorids": "/m/mujeen-sung/; /s/song-feng/; /j/james-gung/; /r/raphael-shu/; /y/yi-zhang/; /s/saab-mansour/",
        "bibtex": "@inproceedings{sung-etal-2025-structured,\n    title = \"Structured List-Grounded Question Answering\",\n    author = \"Sung, Mujeen  and\n      Feng, Song  and\n      Gung, James  and\n      Shu, Raphael  and\n      Zhang, Yi  and\n      Mansour, Saab\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.558/\",\n    pages = \"8347--8359\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.558.pdf",
        "site": "https://aclanthology.org/2025.coling-main.558/",
        "pdf_size": 1123118,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:IX_4aukiGFAJ:scholar.google.com/&scioq=Structured+List-Grounded+Question+Answering&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Kyung Hee University; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs",
        "aff_domain": "khu.ac.kr;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "khu.ac.kr;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Kyung Hee University;Amazon Web Services",
        "aff_unique_dep": ";AWS AI Labs",
        "aff_unique_url": "http://www.khu.ac.kr;https://aws.amazon.com",
        "aff_unique_abbr": "KHU;AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2025.coling-main.21",
        "title": "Style Over Substance: Evaluation Biases for Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human or LLM judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed, machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced evaluations, indicating the need for further investigation.",
        "author": "Minghao Wu; Alham Fikri Aji",
        "authorids": "/m/minghao-wu/; /a/alham-fikri-aji/",
        "bibtex": "@inproceedings{wu-aji-2025-style,\n    title = \"Style Over Substance: Evaluation Biases for Large Language Models\",\n    author = \"Wu, Minghao  and\n      Aji, Alham Fikri\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.21/\",\n    pages = \"297--312\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.21.pdf",
        "site": "https://aclanthology.org/2025.coling-main.21/",
        "pdf_size": 1006855,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16182249669223104194&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Mohamed bin Zayed University of Artificial Intelligence+Monash University; Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "",
        "project": "https://arena.lmsys.org/",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;Monash University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.monash.edu",
        "aff_unique_abbr": "MBZUAI;Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "United Arab Emirates;Australia"
    },
    {
        "id": "2025.coling-main.130",
        "title": "SubRegWeigh: Effective and Efficient Annotation Weighing with Subword Regularization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "NLP datasets may still contain annotation errors, even when they are manually annotated. Researchers have attempted to develop methods to automatically reduce the adverse effect of errors in datasets. However, existing methods are time-consuming because they require many trained models to detect errors. This paper proposes a time-saving method that utilizes a tokenization technique called subword regularization to simulate multiple error detection models for detecting errors. Our proposed method, SubRegWeigh, can perform annotation weighting four to five times faster than the existing method. Additionally, SubRegWeigh improved performance in document classification and named entity recognition tasks. In experiments with pseudo-incorrect labels, SubRegWeigh clearly identifies pseudo-incorrect labels as annotation errors. Our code is available at https://github.com/4ldk/SubRegWeigh.",
        "author": "Kohei Tsuji; Tatsuya Hiraoka; Yuchang Cheng; Tomoya Iwakura",
        "authorids": "/k/kohei-tsuji/; /t/tatsuya-hiraoka/; /y/yuchang-cheng/; /t/tomoya-iwakura/",
        "bibtex": "@inproceedings{tsuji-etal-2025-subregweigh,\n    title = \"{S}ub{R}eg{W}eigh: Effective and Efficient Annotation Weighing with Subword Regularization\",\n    author = \"Tsuji, Kohei  and\n      Hiraoka, Tatsuya  and\n      Cheng, Yuchang  and\n      Iwakura, Tomoya\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.130/\",\n    pages = \"1908--1921\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.130.pdf",
        "site": "https://aclanthology.org/2025.coling-main.130/",
        "pdf_size": 1819896,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=449907722227423784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "NAIST; MBZUAI; NAIST + Fujitsu Ltd.; NAIST + Fujitsu Ltd.",
        "aff_domain": "naist.ac.jp;mbzuai.ac.ae;fujitsu.com;fujitsu.com",
        "email": "naist.ac.jp;mbzuai.ac.ae;fujitsu.com;fujitsu.com",
        "github": "https://github.com/4ldk/SubRegWeigh",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0+2;0+2",
        "aff_unique_norm": "Nara Institute of Science and Technology;Mohamed Bin Zayed University of Artificial Intelligence;Fujitsu Limited",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.naist.jp;https://www.mbzuai.ac.ae;https://www.fujitsu.com/",
        "aff_unique_abbr": "NAIST;MBZUAI;Fujitsu",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+0;0+0",
        "aff_country_unique": "Japan;United Arab Emirates"
    },
    {
        "id": "2025.coling-main.539",
        "title": "Summarization of Opinionated Political Documents with Varied Perspectives",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Global partisan hostility and polarization has increased, and this polarization is heightened around presidential elections. Models capable of generating accurate summaries of diverse perspectives can help reduce such polarization by exposing users to alternative perspectives. In this work, we introduce a novel dataset and task for independently summarizing each political perspective in a set of passages from opinionated news articles. For this task, we propose a framework for evaluating different dimensions of perspective summary performance. We benchmark 11 summarization models and LLMs of varying sizes and architectures through both automatic and human evaluation. While recent models like GPT-4o perform well on this task, we find that all models struggle to generate summaries that are faithful to the intended perspective. Our analysis of summaries focuses on how extraction behavior is impacted by features of the input documents.",
        "author": "Nicholas Deas; Kathleen McKeown",
        "authorids": "/n/nicholas-deas/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{deas-mckeown-2025-summarization,\n    title = \"Summarization of Opinionated Political Documents with Varied Perspectives\",\n    author = \"Deas, Nicholas  and\n      McKeown, Kathleen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.539/\",\n    pages = \"8088--8108\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.539.pdf",
        "site": "https://aclanthology.org/2025.coling-main.539/",
        "pdf_size": 6310903,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5898165944510074002&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "github": "https://github.com/NickDeas/PoliSum",
        "project": "https://www.allsides.com/",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.312",
        "title": "SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse Scenarios Handling Emotional Support Agent",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated promising potential in providing empathetic support during interactions. However, their responses often become verbose or overly formulaic, failing to adequately address the diverse emotional support needs of real-world scenarios. To tackle this challenge, we propose an innovative strategy-enhanced role-playing framework, designed to simulate authentic emotional support conversations. Specifically, our approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing Interactions, which involve three pivotal roles\u2014Seeker, Strategy Counselor, and Supporter\u2014engaging in diverse scenarios to emulate real-world interactions and promote a broader range of dialogues; and (2) Emotional Support Agent Training, achieved through fine-tuning LLMs using our specially constructed dataset. Within this framework, we develop the ServeForEmo dataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and 62.8K+ utterances. We further present SweetieChat, an emotional support agent capable of handling diverse open-domain scenarios. Extensive experiments and human evaluations confirm the framework\u2019s effectiveness in enhancing emotional support, highlighting its unique ability to provide more nuanced and tailored assistance.",
        "author": "Jing Ye; Lu Xiang; Yaping Zhang; Chengqing Zong",
        "authorids": "/j/jing-ye/; /l/lu-xiang/; /y/yaping-zhang/; /c/chengqing-zong/",
        "bibtex": "https://aclanthology.org/2025.coling-main.312.bib",
        "pdf": "https://aclanthology.org/2025.coling-main.312.pdf",
        "site": "https://aclanthology.org/2025.coling-main.312/",
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17080377815172467540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2025.coling-main.49",
        "title": "Swift Cross-Dataset Pruning: Enhancing Fine-Tuning Efficiency in Natural Language Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dataset pruning aims to select a subset of a dataset for efficient model training. While data efficiency in natural language processing has primarily focused on cross-corpus scenarios during model pre-training, efficient dataset pruning for task-specific fine-tuning across diverse datasets remains challenging due to variability in dataset sizes, data distributions, class imbalance and label spaces. Current cross-dataset pruning techniques for fine-tuning often rely on computationally expensive sample ranking processes, typically requiring full dataset training or reference models. We address this gap by proposing Swift Cross-Dataset Pruning (SCDP). Specifically, our approach uses TF-IDF embeddings with geometric median to rapidly evaluate sample importance. We then apply dataset size-adaptive pruning to ensure diversity: for smaller datasets, we retain examples far from the geometric median, while for larger ones, we employ distance-based stratified pruning. Experimental results on six diverse datasets demonstrate the effectiveness of our method, spanning various tasks and scales while significantly reducing computational resources.",
        "author": "Binh-Nguyen Nguyen; Yang He",
        "authorids": "/b/binh-nguyen-nguyen/; /y/yang-he/",
        "bibtex": "@inproceedings{nguyen-he-2025-swift,\n    title = \"Swift Cross-Dataset Pruning: Enhancing Fine-Tuning Efficiency in Natural Language Understanding\",\n    author = \"Nguyen, Binh-Nguyen  and\n      He, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.49/\",\n    pages = \"726--739\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.49.pdf",
        "site": "https://aclanthology.org/2025.coling-main.49/",
        "pdf_size": 941475,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:F4eKf6Q5FF4J:scholar.google.com/&scioq=Swift+Cross-Dataset+Pruning:+Enhancing+Fine-Tuning+Efficiency+in+Natural+Language+Understanding&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "CFAR, Agency for Science, Technology and Research, Singapore + VNU University of Engineering and Technology, Hanoi, Vietnam; IHPC, Agency for Science, Technology and Research, Singapore",
        "aff_domain": "vnu.edu.vn;cfar.a-star.edu.sg",
        "email": "vnu.edu.vn;cfar.a-star.edu.sg",
        "github": "https://github.com/he-y/NLP-Dataset-Pruning",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Agency for Science, Technology and Research;VNU University of Engineering and Technology",
        "aff_unique_dep": "CFAR;",
        "aff_unique_url": "https://www.a-star.edu.sg;",
        "aff_unique_abbr": "A*STAR;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hanoi",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Singapore;Vietnam"
    },
    {
        "id": "2025.coling-main.430",
        "title": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question Answering (QA) datasets have been instrumental in developing and evaluating Large Language Model (LLM) capabilities. However, such datasets are scarce for languages other than English due to the cost and difficulties of collection and manual annotation. This means that producing novel models and measuring the performance of multilingual LLMs in low-resource languages is challenging. To mitigate this, we propose SynDARin, a method for generating and validating QA datasets for low-resoucre languages. We utilize parallel content mining to obtain human-curated paragraphs between English and the target language. We use the English data as context to generate synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality. Combining these with their designated non-English human-curated paragraphs form the final QA dataset. The method allows to maintain content quality, reduces the likelihood of factual errors, and circumvents the need for costly annotation. To test the method, we created a QA dataset with 1.2K samples for the Armenian language. The human evaluation shows that 98% of the generated English data maintains quality and diversity in the question types and topics, while the translation validation pipeline can filter out ~70% of data with poor quality. We use the dataset to benchmark state-of-the-art LLMs, showing their inability to achieve human accuracy with some model performances closer to random chance. This shows that the generated dataset is non-trivial and can be used to evaluate reasoning capabilities in low-resource language.",
        "author": "Gayane Ghazaryan; Erik Arakelyan; Isabelle Augenstein; Pasquale Minervini",
        "authorids": "/g/gayane-ghazaryan/; /e/erik-arakelyan/; /i/isabelle-augenstein/; /p/pasquale-minervini/",
        "bibtex": "@inproceedings{ghazaryan-etal-2025-syndarin,\n    title = \"{S}yn{DAR}in: Synthesising Datasets for Automated Reasoning in Low-Resource Languages\",\n    author = \"Ghazaryan, Gayane  and\n      Arakelyan, Erik  and\n      Augenstein, Isabelle  and\n      Minervini, Pasquale\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.430/\",\n    pages = \"6459--6466\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.430.pdf",
        "site": "https://aclanthology.org/2025.coling-main.430/",
        "pdf_size": 1999838,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6706615078261155372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "American University of Armenia; University of Copenhagen; University of Edinburgh; University of Copenhagen",
        "aff_domain": "alumni.aua.am;di.ku.dk;ed.ac.uk;di.ku.dk",
        "email": "alumni.aua.am;di.ku.dk;ed.ac.uk;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "American University of Armenia;University of Copenhagen;University of Edinburgh",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://aua.am;https://www.ku.dk;https://www.ed.ac.uk",
        "aff_unique_abbr": "AUA;UCPH;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;1",
        "aff_country_unique": "Armenia;Denmark;United Kingdom"
    },
    {
        "id": "2025.coling-main.46",
        "title": "SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG) databases presents a promising avenue for enhancing LLMs\u2019 efficacy and mitigating their \u201challucinations\u201d. Given that most KGs reside in graph databases accessible solely through specialized query languages (e.g., Cypher), it is critical to connect LLMs with KG databases by automating the translation of natural language into Cypher queries (termed as \u201cText2Cypher\u201d task). Prior efforts tried to bolster LLMs\u2019 proficiency in Cypher generation through Supervised Fine-Tuning (SFT). However, these explorations are hindered by the lack of annotated datasets of Query-Cypher pairs, resulting from the labor-intensive and domain-specific nature of such annotation. In this study, we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C is applied to two medical KG databases, culminating in the creation of a synthetic dataset, MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset effectively enhances the performance of backbone LLMs on Text2Cypher task via SFT. Both the SyntheT2C codebase and the MedT2C dataset will be released.",
        "author": "Zijie Zhong; Linqing Zhong; Zhaoze Sun; Qingyun Jin; Zengchang Qin; Xiaofan Zhang",
        "authorids": "/z/zijie-zhong/; /l/linqing-zhong/; /z/zhaoze-sun/; /q/qingyun-jin/; /z/zengchang-qin/; /x/xiaofan-zhang/",
        "bibtex": "@inproceedings{zhong-etal-2025-synthet2c,\n    title = \"{S}ynthe{T}2{C}: Generating Synthetic Data for Fine-Tuning Large Language Models on the {T}ext2{C}ypher Task\",\n    author = \"Zhong, Zijie  and\n      Zhong, Linqing  and\n      Sun, Zhaoze  and\n      Jin, Qingyun  and\n      Qin, Zengchang  and\n      Zhang, Xiaofan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.46/\",\n    pages = \"672--692\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.46.pdf",
        "site": "https://aclanthology.org/2025.coling-main.46/",
        "pdf_size": 1952013,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3307592635431429816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2025.coling-main.347",
        "title": "Synthetic Paths to Integral Truth: Mitigating Hallucinations Caused by Confirmation Bias with Synthetic Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, large language models (LLMs) have made significant progress through retrieval-augmented generation (RAG) and preference learning. However, they still exhibit issues such as confirmation bias, the tendency to favor information that confirms one\u2019s beliefs, which remains largely unexplored in current research. In this paper, we propose a novel approach to mitigate confirmation bias-induced hallucination in LLMs through a synthetic data construction pipeline and Direct Preference Optimization (DPO) training. Our method enhances the integration of diverse and complementary information from multiple passages retrieved by RAG, enabling more balanced and accurate reasoning. Experimental results demonstrate significant improvements in response accuracy and reduced hallucination on benchmarks such as Natural Questions Open and HaluBench. These findings suggest that our approach effectively mitigates confirmation bias in long-context question answering, with potential applications to other NLP tasks. We release our data, and evaluation/train code for public access.3]https://github.com/OccasionallyNLP/Synthetic-Paths-to-Integral-Truth.git",
        "author": "Changwon Ok; Eunkyeong Lee; Dongsuk Oh",
        "authorids": "/c/changwon-ok/; /e/eunkyeong-lee/; /d/dongsuk-oh/",
        "bibtex": "@inproceedings{ok-etal-2025-synthetic,\n    title = \"Synthetic Paths to Integral Truth: Mitigating Hallucinations Caused by Confirmation Bias with Synthetic Data\",\n    author = \"Ok, Changwon  and\n      Lee, Eunkyeong  and\n      Oh, Dongsuk\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.347/\",\n    pages = \"5168--5180\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.347.pdf",
        "site": "https://aclanthology.org/2025.coling-main.347/",
        "pdf_size": 602931,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4377813506061227510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "KT Corporation, Republic of Korea; KT Corporation, Republic of Korea; Department of English Language and Literature, Kyungpook National University, Republic of Korea",
        "aff_domain": "kt.com;kt.com;knu.ac.kr",
        "email": "kt.com;kt.com;knu.ac.kr",
        "github": "https://github.com/OccasionallyNLP/Synthetic-Paths-to-Integral-Truth.git",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "KT Corporation;Kyungpook National University",
        "aff_unique_dep": ";Department of English Language and Literature",
        "aff_unique_url": "https://www.kt.com;http://www.knu.ac.kr",
        "aff_unique_abbr": "KT;KNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.81",
        "title": "T-MES: Trait-Aware Mix-of-Experts Representation Learning for Multi-trait Essay Scoring",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In current research on automatic essay scoring, related work tends to focus more on evaluating the overall quality or a single trait of prompt-specific essays. However, when scoring essays in an educational context, it is essential not only to consider the overall score but also to provide feedback on various aspects of the writing. This helps students clearly identify areas for improvement, enabling them to engage in targeted practice. Although many methods have been proposed to address the scoring issue, they still suffer from insufficient learning of trait representations and overlook the diversity and correlations between trait scores in the scoring process. To address this problem, we propose a novel multi-trait essay scoring method based on Trait-Aware Mix-of-Experts Representation Learning. Our method obtains trait-specific essay representations using a Mix-of-Experts scoring architecture. Furthermore, based on this scoring architecture, we propose a diversified trait-expert method to learn distinguishable expert weights. And to facilitate multi-trait scoring, we introduce two trait correlation learning strategies that achieve learning the correlations among traits. Experimental results demonstrate the effectiveness of our method, and compared to existing methods, it achieves a further improvement in computational efficiency.",
        "author": "Jiong Wang; Jie Liu",
        "authorids": "/j/jiong-wang/; /j/jie-liu/",
        "bibtex": "@inproceedings{wang-liu-2025-mes,\n    title = \"{T}-{MES}: Trait-Aware Mix-of-Experts Representation Learning for Multi-trait Essay Scoring\",\n    author = \"Wang, Jiong  and\n      Liu, Jie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.81/\",\n    pages = \"1224--1236\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.81.pdf",
        "site": "https://aclanthology.org/2025.coling-main.81/",
        "pdf_size": 1873538,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6088368768875052828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Information Science, North China University of Technology, Beijing, China + China Language Intelligence Research Center, Capital Normal University, Beijing, China",
        "aff_domain": "mail.bnu.edu.cn;ncut.edu.cn",
        "email": "mail.bnu.edu.cn;ncut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Beijing Normal University;North China University of Technology;Capital Normal University",
        "aff_unique_dep": "School of Artificial Intelligence;School of Information Science;China Language Intelligence Research Center",
        "aff_unique_url": "https://www.bnu.edu.cn;http://www.ncut.edu.cn;http://www.cnu.edu.cn",
        "aff_unique_abbr": "BNU;;CNU",
        "aff_campus_unique_index": "0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.142",
        "title": "TEEMIL : Towards Educational MCQ Difficulty Estimation in Indic Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Difficulty estimation of multiple-choice questions (MCQs) is crucial for creating effective educational assessments, yet remains underexplored in Indic languages like Hindi and Kannada due to the lack of comprehensive datasets. This paper addresses this gap by introducing two datasets, TEEMIL-H and TEEMIL-K, containing 4689 and 4215 MCQs, respectively, with manually annotated difficulty labels. We benchmark these datasets using state-of-the-art multilingual models and conduct ablation studies to analyze the effect of context, the impact of options, and the presence of the None of the Above (NOTA) option on difficulty estimation. Our findings establish baselines for difficulty estimation in Hindi and Kannada, offering valuable insights into improving model performance and guiding future research in MCQ difficulty estimation .",
        "author": "Manikandan Ravikiran; Siddharth Vohra; Rajat Verma; Rohit Saluja; Arnav Bhavsar",
        "authorids": "/m/manikandan-ravikiran/; /s/siddharth-vohra/; /r/rajat-verma/; /r/rohit-saluja/; /a/arnav-bhavsar/",
        "bibtex": "@inproceedings{ravikiran-etal-2025-teemil,\n    title = \"{TEEMIL} : Towards Educational {MCQ} Difficulty Estimation in {I}ndic Languages\",\n    author = \"Ravikiran, Manikandan  and\n      Vohra, Siddharth  and\n      Verma, Rajat  and\n      Saluja, Rohit  and\n      Bhavsar, Arnav\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.142/\",\n    pages = \"2085--2099\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.142.pdf",
        "site": "https://aclanthology.org/2025.coling-main.142/",
        "pdf_size": 1331986,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9W85zpbHVWMJ:scholar.google.com/&scioq=TEEMIL+:+Towards+Educational+MCQ+Difficulty+Estimation+in+Indic+Languages&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Indian Institute of Technology, Mandi, India; Amazon, USA; Indian Institute of Technology, Mandi, India; Indian Institute of Technology, Mandi, India; Indian Institute of Technology, Mandi, India",
        "aff_domain": "students.iitmandi.ac.in;amazon.com; ;iitmandi.ac.in;iitmandi.ac.in",
        "email": "students.iitmandi.ac.in;amazon.com; ;iitmandi.ac.in;iitmandi.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Mandi;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitmandi.ac.in;https://www.amazon.com",
        "aff_unique_abbr": "IIT Mandi;Amazon",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mandi;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2025.coling-main.552",
        "title": "TEF: Causality-Aware Taxonomy Expansion via Front-Door Criterion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Taxonomy expansion is a primary method for enriching taxonomies, involving appending a large number of additional nodes (i.e., queries) to an existing taxonomy (i.e., seed), with the crucial step being the identification of the appropriate anchor (parent node) for each query by incorporating the structural information of the seed. Despite advancements, existing research still faces an inherent challenge of spurious query-anchor matching, often due to various interference factors (e.g., the consistency of sibling nodes), resulting in biased identifications. To address the bias in taxonomy expansion caused by unobserved factors, we introduce the Structural Causal Model (SCM), known for its bias elimination capabilities, to prevent these factors from confounding the task through backdoor paths. Specifically, we employ the Front-Door Criterion, which guides the decomposition of the expansion process into a parser module and a connector. This enables the proposed causal-aware Taxonomy Expansion model to isolate confounding effects and reveal the true causal relationship between the query and the anchor. Extensive experiments on three benchmarks validate the effectiveness of TEF, with a notable 6.1% accuracy improvement over the state-of-the-art on the SemEval16-Environment dataset.",
        "author": "Yuan Meng; Songlin Zhai; Yuxin Zhang; Zhongjian Hu; Guilin Qi",
        "authorids": "/y/yuan-meng/; /s/songlin-zhai/; /y/yuxin-zhang/; /z/zhongjian-hu/; /g/guilin-qi/",
        "bibtex": "@inproceedings{meng-etal-2025-tef,\n    title = \"{TEF}: Causality-Aware Taxonomy Expansion via Front-Door Criterion\",\n    author = \"Meng, Yuan  and\n      Zhai, Songlin  and\n      Zhang, Yuxin  and\n      Hu, Zhongjian  and\n      Qi, Guilin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.552/\",\n    pages = \"8285--8294\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.552.pdf",
        "site": "https://aclanthology.org/2025.coling-main.552/",
        "pdf_size": 673530,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:jL_2W3neDb4J:scholar.google.com/&scioq=TEF:+Causality-Aware+Taxonomy+Expansion+via+Front-Door+Criterion&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Southeast University, Jiangsu Province, China+Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China; School of Computer Science and Engineering, Southeast University, Jiangsu Province, China+Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China; School of Computer Science and Engineering, Southeast University, Jiangsu Province, China+Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China; School of Computer Science and Engineering, Southeast University, Jiangsu Province, China+Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China; School of Computer Science and Engineering, Southeast University, Jiangsu Province, China+Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.296",
        "title": "TEXT-CAKE: Challenging Language Models on Local Text Coherence",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a deep investigation of encoder-based Language Models (LMs) on their abilities to detect text coherence across four languages and four text genres using a new evaluation benchmark, TEXT-CAKE. We analyze both multilingual and monolingual LMs with varying architectures and parameters in different finetuning settings. Our findings demonstrate that identifying subtle perturbations that disrupt local coherence is still a challenging task. Furthermore, our results underline the importance of using diverse text genres during pre-training and of an optimal pre-traning objective and large vocabulary size. When controlling for other parameters, deep LMs (i.e., higher number of layers) have an advantage over shallow ones, even when the total number of parameters is smaller.",
        "author": "Luca Dini; Dominique Brunato; Felice Dell\u2019Orletta; Tommaso Caselli",
        "authorids": "/l/luca-dini/; /d/dominique-brunato/; /f/felice-dellorletta/; /t/tommaso-caselli/",
        "bibtex": "@inproceedings{dini-etal-2025-text,\n    title = \"{TEXT}-{CAKE}: Challenging Language Models on Local Text Coherence\",\n    author = \"Dini, Luca  and\n      Brunato, Dominique  and\n      Dell{'}Orletta, Felice  and\n      Caselli, Tommaso\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.296/\",\n    pages = \"4384--4398\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.296.pdf",
        "site": "https://aclanthology.org/2025.coling-main.296/",
        "pdf_size": 1325296,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4698311306347058579&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d (CNR-ILC), ItaliaNLP Lab, Pisa+University of Pisa; Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d (CNR-ILC), ItaliaNLP Lab, Pisa; Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d (CNR-ILC), ItaliaNLP Lab, Pisa; Center for Language and Cognition (CLCG), University of Groningen",
        "aff_domain": "ilc.cnr.it;ilc.cnr.it;ilc.cnr.it;rug.nl",
        "email": "ilc.cnr.it;ilc.cnr.it;ilc.cnr.it;rug.nl",
        "github": "https://github.com/lucadinidue/coherence_text_cake",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;2",
        "aff_unique_norm": "Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d;University of Pisa;University of Groningen",
        "aff_unique_dep": "CNR-ILC;;Center for Language and Cognition (CLCG)",
        "aff_unique_url": ";https://www.unipi.it;https://www.rug.nl",
        "aff_unique_abbr": "ILC;UNIP;RUG",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pisa;",
        "aff_country_unique_index": "0+0;0;0;1",
        "aff_country_unique": "Italy;Netherlands"
    },
    {
        "id": "2025.coling-main.340",
        "title": "TMATH A Dataset for Evaluating Large Language Models in Generating Educational Hints for Math Word Problems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) are increasingly being applied in education, showing significant potential in personalized instruction, student feedback, and intelligent tutoring. Generating hints for Math Word Problems (MWPs) has become a critical application, particularly in helping students understand problem-solving steps and logic. However, existing models struggle to provide pedagogically sound guidance that fosters learning without offering direct answers. To address this issue, we introduce TMATH, a dataset specifically designed to evaluate LLMs\u2019 ability to generate high-quality hints for MWPs. TMATH contains diverse mathematical problems paired with carefully crafted, human-generated hints. To assess its impact, we fine-tuned a series of 7B-scale language models using TMATH. Our results, based on quantitative evaluations and expert assessments, show that while LLMs still face challenges in complex reasoning, the TMATH dataset significantly enhances their ability to generate more accurate and contextually appropriate educational hints.",
        "author": "Changyong Qi; Yuang Wei; Haoxin Xu; Longwei Zheng; Peiji Chen; Xiaoqing Gu",
        "authorids": "/c/changyong-qi/; /y/yuang-wei/; /h/haoxin-xu/; /l/longwei-zheng/; /p/peiji-chen/; /x/xiaoqing-gu/",
        "bibtex": "@inproceedings{qi-etal-2025-tmath,\n    title = \"{TMATH} A Dataset for Evaluating Large Language Models in Generating Educational Hints for Math Word Problems\",\n    author = \"Qi, Changyong  and\n      Wei, Yuang  and\n      Xu, Haoxin  and\n      Zheng, Longwei  and\n      Chen, Peiji  and\n      Gu, Xiaoqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.340/\",\n    pages = \"5082--5093\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.340.pdf",
        "site": "https://aclanthology.org/2025.coling-main.340/",
        "pdf_size": 680628,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18183762363979555216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Lab of Artificial Intelligence for Education, East China Normal University, Shanghai, China+Shanghai Institute of Artificial Intelligence for Education, East China Normal University, Shanghai, China; Lab of Artificial Intelligence for Education, East China Normal University, Shanghai, China+Shanghai Institute of Artificial Intelligence for Education, East China Normal University, Shanghai, China; Lab of Artificial Intelligence for Education, East China Normal University, Shanghai, China+Shanghai Institute of Artificial Intelligence for Education, East China Normal University, Shanghai, China; School of Education, City University of Macau, Macau, China+State Key Laboratory of Cognitive Intelligence, Hefei, China; The University of Electro-Communications, Tokyo, Japan; Department of Education Information Technology, East China Normal University, Shanghai, China",
        "aff_domain": "ecnu.edu.cn;ecnu.edu.cn;ecnu.edu.cn;cityu.edu.mo;uec.ac.jp;ecnu.edu.cn",
        "email": "ecnu.edu.cn;ecnu.edu.cn;ecnu.edu.cn;cityu.edu.mo;uec.ac.jp;ecnu.edu.cn",
        "github": "https://github.com/qi-github-ui/TMATH",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;0+0;0+0;1+2;3;0",
        "aff_unique_norm": "East China Normal University;City University of Macau;State Key Laboratory of Cognitive Intelligence;The University of Electro-Communications",
        "aff_unique_dep": "Lab of Artificial Intelligence for Education;School of Education;;",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.cityu.edu.mo;;https://www.uec.ac.jp",
        "aff_unique_abbr": "ECNU;CityU;;UEC",
        "aff_campus_unique_index": "0+0;0+0;0+0;1+2;3;0",
        "aff_campus_unique": "Shanghai;Macau;Hefei;Tokyo",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;1;0",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "2025.coling-main.355",
        "title": "TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling Capability of LLM",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Empathetic conversation is a crucial characteristic in daily conversations between individuals. Nowadays, Large Language models (LLMs) have shown outstanding performance in generating empathetic responses. Knowledge bases like COMET can assist LLMs in mitigating illusions and enhancing the understanding of users\u2019 intentions and emotions. However, models remain heavily reliant on fixed knowledge bases and unrestricted incorporation of external knowledge can introduce noise. Tool learning is a flexible end-to-end approach that assists LLMs in handling complex problems. In this paper, we propose Emotional Knowledge Tool Calling (EKTC) framework, which encapsulates the commonsense knowledge bases as empathetic tools, enabling LLMs to integrate external knowledge flexibly through tool calling. In order to adapt the models to the new task, we construct a novel dataset TOOL-ED based on the EMPATHETICDIALOGUE (ED) dataset. We validate EKTC on the ED dataset, and the experimental results demonstrate that our framework can enhance the ability of LLMs to generate empathetic responses effectively. Our code is available at https://anonymous.4open.science/r/EKTC-3FEF.",
        "author": "Huiying Cao; Yiqun Zhang; Shi Feng; Xiaocui Yang; Daling Wang; Yifei Zhang",
        "authorids": "/h/huiying-cao/; /y/yiqun-zhang/; /s/shi-feng/; /x/xiaocui-yang/; /d/daling-wang/; /y/yifei-zhang/",
        "bibtex": "@inproceedings{cao-etal-2025-tool,\n    title = \"{TOOL}-{ED}: Enhancing Empathetic Response Generation with the Tool Calling Capability of {LLM}\",\n    author = \"Cao, Huiying  and\n      Zhang, Yiqun  and\n      Feng, Shi  and\n      Yang, Xiaocui  and\n      Wang, Daling  and\n      Zhang, Yifei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.355/\",\n    pages = \"5305--5320\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.355.pdf",
        "site": "https://aclanthology.org/2025.coling-main.355/",
        "pdf_size": 1289139,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17038724389519696210&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China\u2020; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China",
        "aff_domain": "163.com;gmail.com;gmail.com;cse.neu.edu.cn;cse.neu.edu.cn;cse.neu.edu.cn",
        "email": "163.com;gmail.com;gmail.com;cse.neu.edu.cn;cse.neu.edu.cn;cse.neu.edu.cn",
        "github": "https://github.com/caohy123/EKTC",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.neu.edu.cn/",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Shenyang",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.469",
        "title": "TOP-Training: Target-Oriented Pretraining for Medical Extractive Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We study extractive question-answering in the medical domain (Medical-EQA). This problem has two main challenges: (i) domain specificity, as most AI models lack necessary domain knowledge, and (ii) extraction-based answering style, which restricts most autoregressive LLMs due to potential hallucinations. To handle those challenges, we propose TOP-Training, a target-oriented pre-training paradigm that stands out among all domain adaptation techniques with two desirable features: (i) TOP-Training moves one step further than popular domain-oriented fine-tuning since it not only moves closer to the target domain, but also familiarizes itself with the target dataset, and (ii) it does not assume the existence of a large set of unlabeled instances from the target domain. Specifically, for a target Medical-EQA dataset, we extract its entities and leverage large language models (LLMs) to generate synthetic texts containing those entities; we then demonstrate that pretraining on this synthetic text data yields better performance on the target Medical-EQA benchmarks. Overall, our contributions are threefold: (i) TOP-Training, a new pretraining technique to effectively adapt LLMs to better solve a target problem, (ii) TOP-Training has a wide application scope because it does not require the target problem to have a large set of unlabeled data, and (iii) our experiments highlight the limitations of autoregressive LLMs, emphasizing TOP-Training as a means to unlock the true potential of bidirectional LLMs.",
        "author": "Saptarshi Sengupta; Connor Heaton; Shreya Ghosh; Wenpeng Yin; Preslav Nakov; Suhang Wang",
        "authorids": "/s/saptarshi-sengupta/; /c/connor-heaton/; /s/shreya-ghosh/; /w/wenpeng-yin/; /p/preslav-nakov/; /s/suhang-wang/",
        "bibtex": "@inproceedings{sengupta-etal-2025-top,\n    title = \"{TOP}-Training: Target-Oriented Pretraining for Medical Extractive Question Answering\",\n    author = \"Sengupta, Saptarshi  and\n      Heaton, Connor  and\n      Ghosh, Shreya  and\n      Yin, Wenpeng  and\n      Nakov, Preslav  and\n      Wang, Suhang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.469/\",\n    pages = \"7035--7054\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.469.pdf",
        "site": "https://aclanthology.org/2025.coling-main.469/",
        "pdf_size": 2926396,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rGiUBzr5kpEJ:scholar.google.com/&scioq=TOP-Training:+Target-Oriented+Pretraining+for+Medical+Extractive+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Pennsylvania State University, USA; Pennsylvania State University, USA; Indian Institute of Technology (IIT) Bhubaneswar, India; Pennsylvania State University, USA; Mohamed bin Zayed University of Artificial Intelligence, UAE; Pennsylvania State University, USA",
        "aff_domain": "psu.edu;psu.edu;iitbbs.ac.in;psu.edu;mbzuai.ac.ae;psu.edu",
        "email": "psu.edu;psu.edu;iitbbs.ac.in;psu.edu;mbzuai.ac.ae;psu.edu",
        "github": "https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "Pennsylvania State University;Indian Institute of Technology Bhubaneswar;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.psu.edu;https://www.iitbbs.ac.in;https://mbzuai.ac.ae",
        "aff_unique_abbr": "PSU;IIT Bhubaneswar;MBZUAI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bhubaneswar",
        "aff_country_unique_index": "0;0;1;0;2;0",
        "aff_country_unique": "United States;India;United Arab Emirates"
    },
    {
        "id": "2025.coling-main.57",
        "title": "TaCIE: Enhancing Instruction Comprehension in Large Language Models through Task-Centred Instruction Evolution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.",
        "author": "Jiuding Yang; Shengyao Lu; Weidong Guo; Xiangyang Li; Kaitong Yang; Yu Xu; Di Niu",
        "authorids": "/j/jiuding-yang/; /s/shengyao-lu/; /w/weidong-guo/; /x/xiangyang-li/; /k/kaitong-yang/; /y/yu-xu/; /d/di-niu/",
        "bibtex": "@inproceedings{yang-etal-2025-tacie,\n    title = \"{T}a{CIE}: Enhancing Instruction Comprehension in Large Language Models through Task-Centred Instruction Evolution\",\n    author = \"Yang, Jiuding  and\n      Lu, Shengyao  and\n      Guo, Weidong  and\n      Li, Xiangyang  and\n      Yang, Kaitong  and\n      Xu, Yu  and\n      Niu, Di\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.57/\",\n    pages = \"855--869\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.57.pdf",
        "site": "https://aclanthology.org/2025.coling-main.57/",
        "pdf_size": 2082192,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kBDbYSM-0XYJ:scholar.google.com/&scioq=TaCIE:+Enhancing+Instruction+Comprehension+in+Large+Language+Models+through+Task-Centred+Instruction+Evolution&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Alberta; University of Alberta; Platform and Content Group, Tencent; Platform and Content Group, Tencent; Platform and Content Group, Tencent; Platform and Content Group, Tencent; University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;tencent.com;tencent.com;tencent.com;tencent.com;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;tencent.com;tencent.com;tencent.com;tencent.com;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;1;1;1;0",
        "aff_unique_norm": "University of Alberta;Tencent",
        "aff_unique_dep": ";Platform and Content Group",
        "aff_unique_url": "https://www.ualberta.ca;https://www.tencent.com",
        "aff_unique_abbr": "UAlberta;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;1;1;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "id": "2025.coling-main.322",
        "title": "Task-Oriented Dialog Systems for the Senegalese Wolof Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, we are seeing considerable interest in conversational agents with the rise of large language models (LLMs). Although they offer considerable advantages, LLMs also present significant risks, such as hallucination, which hinder their widespread deployment in industry. Moreover, low-resource languages such as African ones are still underrepresented in these systems limiting their performance in these languages. In this paper, we illustrate a more classical approach based on modular architectures of Task-oriented Dialog Systems (ToDS) offering better control over outputs. We propose a chatbot generation engine based on the Rasa framework and a robust methodology for projecting annotations onto the Wolof language using an in-house machine translation system. After evaluating a generated chatbot trained on the Amazon Massive dataset, our Wolof Intent Classifier performs similarly to the one obtained for French, which is a resource-rich language. We also show that this approach is extensible to other low-resource languages, thanks to the intent classifier\u2019s language-agnostic pipeline, simplifying the design of chatbots in these languages.",
        "author": "Derguene Mbaye; Moussa Diallo",
        "authorids": "/d/derguene-mbaye/; /m/moussa-diallo/",
        "bibtex": "@inproceedings{mbaye-diallo-2025-task,\n    title = \"Task-Oriented Dialog Systems for the Senegalese {W}olof Language\",\n    author = \"Mbaye, Derguene  and\n      Diallo, Moussa\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.322/\",\n    pages = \"4803--4812\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.322.pdf",
        "site": "https://aclanthology.org/2025.coling-main.322/",
        "pdf_size": 797865,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:P9TF_2OnzwsJ:scholar.google.com/&scioq=Task-Oriented+Dialog+Systems+for+the+Senegalese+Wolof+Language&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Baamtu Technologies + Universit\u00e9 Cheikh Anta Diop; Ecole Sup\u00e9rieure Polytechnique (ESP)",
        "aff_domain": "esp.sn;esp.sn",
        "email": "esp.sn;esp.sn",
        "github": "",
        "project": "https://www.masakhane.io/",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Baamtu Technologies;Universit\u00e9 Cheikh Anta Diop;Ecole Sup\u00e9rieure Polytechnique",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://ucad.edu.sn;https://www.esp.sn",
        "aff_unique_abbr": ";UCAD;ESP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Senegal"
    },
    {
        "id": "2025.coling-main.102",
        "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the emergence of large language models (LLMs) and their ability to perform a variety of tasks, their application in recommender systems (RecSys) has shown promise. However, we are facing significant challenges when deploying LLMs into RecSys, such as limited prompt length, unstructured item information, and un-constrained generation of recommendations, leading to sub-optimal performance. To address these issues, we propose a novel Taxonomy-guided Recommendation (TaxRec) framework to empower LLM with category information in a systematic approach. Specifically, TaxRec features a two-step process: one-time taxonomy categorization and LLM-based recommendation. In the one-time taxonomy categorization phase, we organize and categorize items, ensuring clarity and structure of item information. In the LLM-based recommendation phase, we feed the structured items into LLM prompts, achieving efficient token utilization and controlled feature generation. This enables more accurate, contextually relevant, and zero-shot recommendations without the need for domain-specific fine-tuning. Experimental results demonstrate that TaxRec significantly enhances recommendation quality compared to traditional zero-shot approaches, showcasing its efficacy as a personal recommender with LLMs. Code is available at: https://github.com/yueqingliang1/TaxRec.",
        "author": "Yueqing Liang; Liangwei Yang; Chen Wang; Xiongxiao Xu; Philip S. Yu; Kai Shu",
        "authorids": "/y/yueqing-liang/; /l/liangwei-yang/; /c/chen-wang/; /x/xiongxiao-xu/; /p/philip-s-yu/; /k/kai-shu/",
        "bibtex": "@inproceedings{liang-etal-2025-taxonomy,\n    title = \"Taxonomy-Guided Zero-Shot Recommendations with {LLM}s\",\n    author = \"Liang, Yueqing  and\n      Yang, Liangwei  and\n      Wang, Chen  and\n      Xu, Xiongxiao  and\n      Yu, Philip S.  and\n      Shu, Kai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.102/\",\n    pages = \"1520--1530\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.102.pdf",
        "site": "https://aclanthology.org/2025.coling-main.102/",
        "pdf_size": 1438545,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11425618392705819191&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Illinois Institute of Technology; University of Illinois at Chicago; University of Illinois at Chicago; Illinois Institute of Technology; University of Illinois at Chicago; Emory University",
        "aff_domain": "hawk.iit.edu;uic.edu;uic.edu;hawk.iit.edu;uic.edu;emory.edu",
        "email": "hawk.iit.edu;uic.edu;uic.edu;hawk.iit.edu;uic.edu;emory.edu",
        "github": "https://github.com/yueqingliang1/TaxRec",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;1;2",
        "aff_unique_norm": "Illinois Institute of Technology;University of Illinois at Chicago;Emory University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iit.edu;https://www.uic.edu;https://www.emory.edu",
        "aff_unique_abbr": "IIT;UIC;Emory",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-industry.69",
        "title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large Language Models (LLMs) exhibit impressive performance across various domains but still struggle with arithmetic reasoning tasks. Recent work shows the effectiveness of prompt design methods in enhancing reasoning capabilities. However, these approaches overlook crucial requirements for prior knowledge of specific concepts, theorems, and tricks to tackle most arithmetic reasoning problems successfully. To address this issue, we propose a novel and effective Teaching-Inspired Integrated Prompting Framework, which emulates the instructional process of a teacher guiding students. This method equips LLMs with essential concepts, relevant theorems, and similar problems with analogous solution approaches, facilitating the enhancement of reasoning abilities. Additionally, we introduce two new Chinese datasets, MathMC and MathToF, both with detailed explanations and answers. Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs. With GPT-4 and our framework, we achieve new state-of-the-art performance on four math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2% (+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%).",
        "author": "Wenting Tan; Dongxiao Chen; Jieting Xue; Zihao Wang; Taijie Chen",
        "authorids": "/w/wenting-tan/; /d/dongxiao-chen/; /j/jieting-xue/; /z/zihao-wang/; /t/taijie-chen/",
        "bibtex": "@inproceedings{tan-etal-2025-teaching,\n    title = \"Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models\",\n    author = \"Tan, Wenting  and\n      Chen, Dongxiao  and\n      Xue, Jieting  and\n      Wang, Zihao  and\n      Chen, Taijie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.69/\",\n    pages = \"827--839\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.69.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.69/",
        "pdf_size": 808534,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:QyywLzoSkIIJ:scholar.google.com/&scioq=Teaching-Inspired+Integrated+Prompting+Framework:+A+Novel+Approach+for+Enhancing+Reasoning+in+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Key Laboratory of AI Safety, Institute of Computing Technology, CAS + NetEase Youdao + The University of Hong Kong; NetEase Youdao; NetEase Youdao; NetEase Youdao; The University of Hong Kong",
        "aff_domain": "ict.ac.cn;rd.netease.com;rd.netease.com;rd.netease.com;connect.hku.hk",
        "email": "ict.ac.cn;rd.netease.com;rd.netease.com;rd.netease.com;connect.hku.hk",
        "github": "https://github.com/SallyTan13/Teaching-Inspired-Prompting",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;1;1;1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;NetEase;The University of Hong Kong",
        "aff_unique_dep": "Institute of Computing Technology;Youdao;",
        "aff_unique_url": "http://www.cas.cn/;https://www.youdao.com;https://www.hku.hk",
        "aff_unique_abbr": "CAS;Youdao;HKU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.216",
        "title": "TermDiffuSum: A Term-guided Diffusion Model for Extractive Summarization of Legal Documents",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Extractive summarization for legal documents aims to automatically extract key sentences from legal texts to form concise summaries. Recent studies have explored diffusion models for extractive summarization task, showcasing their remarkable capabilities. Despite these advancements, these models often fall short in effectively capturing and leveraging the specialized legal terminology crucial for accurate legal summarization. To address the limitation, this paper presents a novel term-guided diffusion model for extractive summarization of legal documents, named TermDiffuSum. It incorporates legal terminology into the diffusion model via a well-designed multifactor fusion noise weighting schedule, which allocates higher attention weight to sentences containing a higher concentration of legal terms during the diffusion process. Additionally, TermDiffuSum utilizes a re-ranking loss function to refine the model\u2019s selection of more relevant summaries by leveraging the relationship between the candidate summaries generated by the diffusion process and the reference summaries. Experimental results on a self-constructed legal summarization dataset reveal that TermDiffuSum outperforms existing diffusion-based summarization models, achieving improvements of 3.10 in ROUGE-1, 2.84 in ROUGE-2, and 2.89 in ROUGE-L. To further validate the generalizability of TermDiffuSum, we conduct experiments on three public datasets from news and social media domains, with results affirming the scalability of our approach.",
        "author": "Xiangyun Dong; Wei Li; Yuquan Le; Zhangyue Jiang; Junxi Zhong; Zhong Wang",
        "authorids": "/x/xiangyun-dong/; /w/wei-li/; /y/yuquan-le/; /z/zhangyue-jiang/; /j/junxi-zhong/; /z/zhong-wang/",
        "bibtex": "@inproceedings{dong-etal-2025-termdiffusum,\n    title = \"{T}erm{D}iffu{S}um: A Term-guided Diffusion Model for Extractive Summarization of Legal Documents\",\n    author = \"Dong, Xiangyun  and\n      Li, Wei  and\n      Le, Yuquan  and\n      Jiang, Zhangyue  and\n      Zhong, Junxi  and\n      Wang, Zhong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.216/\",\n    pages = \"3222--3235\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.216.pdf",
        "site": "https://aclanthology.org/2025.coling-main.216/",
        "pdf_size": 612756,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15904719053425704033&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China",
        "aff_domain": "hnu.edu.cn;hnu.edu.cn;hnu.edu.cn;hnu.edu.cn;hnu.edu.cn;hnu.edu.cn",
        "email": "hnu.edu.cn;hnu.edu.cn;hnu.edu.cn;hnu.edu.cn;hnu.edu.cn;hnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Hunan University",
        "aff_unique_dep": "College of Computer Science and Electronic Engineering",
        "aff_unique_url": "http://www.hnu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Changsha",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.722",
        "title": "Text-Attributed Graph Learning with Coupled Augmentations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Modeling text-attributed graphs is a well-known problem due to the difficulty of capturing both the text attribute and the graph structure effectively. Existing models often focus on either the text attribute or the graph structure, potentially neglecting the other aspect. This is primarily because both text learning and graph learning models require significant computational resources, making it impractical to directly connect these models in a series. However, there are situations where text-learning models correctly classify text-attributed nodes, while graph-learning models may classify them incorrectly, and vice versa. To fully leverage the potential of text-attributed graphs, we propose a Coupled Text-attributed Graph Learning (CTGL) framework that combines the strengths of both text-learning and graph-learning models in parallel and avoids the computational cost of serially connecting the two aspect models. Specifically, CTGL introduces coupled text-graph augmentation to enable coupled contrastive learning and facilitate the exchange of valuable information between text learning and graph learning. Experimental results on diverse datasets demonstrate the superior performance of our model compared to state-of-the-art text-learning and graph-learning baselines.",
        "author": "Chuang Zhou; Jiahe Du; Huachi Zhou; Hao Chen; Feiran Huang; Xiao Huang",
        "authorids": "/c/chuang-zhou/; /j/jiahe-du/; /h/huachi-zhou/; /h/hao-chen/; /f/feiran-huang/; /x/xiao-huang/",
        "bibtex": "@inproceedings{zhou-etal-2025-text,\n    title = \"Text-Attributed Graph Learning with Coupled Augmentations\",\n    author = \"Zhou, Chuang  and\n      Du, Jiahe  and\n      Zhou, Huachi  and\n      Chen, Hao  and\n      Huang, Feiran  and\n      Huang, Xiao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.722/\",\n    pages = \"10865--10876\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.722.pdf",
        "site": "https://aclanthology.org/2025.coling-main.722/",
        "pdf_size": 1417324,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:h1_ZwgocJlYJ:scholar.google.com/&scioq=Text-Attributed+Graph+Learning+with+Coupled+Augmentations&hl=en&as_sdt=0,44",
        "gs_version_total": 0,
        "aff": "The Hong Kong Polytechnic University, China; The Hong Kong Polytechnic University, China; The Hong Kong Polytechnic University, China; The Hong Kong Polytechnic University, China; Jinan University, China; The Hong Kong Polytechnic University, China",
        "aff_domain": "connect.polyu.hk;connect.polyu.hk;connect.polyu.hk;gmail.com;jnu.edu.cn;comp.polyu.hk",
        "email": "connect.polyu.hk;connect.polyu.hk;connect.polyu.hk;gmail.com;jnu.edu.cn;comp.polyu.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "The Hong Kong Polytechnic University;Jinan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.polyu.edu.hk;https://www.jnu.edu.cn",
        "aff_unique_abbr": "PolyU;JNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.39",
        "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their power comes with significant security considerations. While extensive research has been conducted on the safety of LLMs in chat mode, the security implications of their function calling feature have been largely overlooked. This paper uncovers a critical vulnerability in the function calling process of LLMs, introducing a novel \u201cjailbreak function\u201d attack method that exploits alignment discrepancies, user coercion, and the absence of rigorous safety filters. Our empirical study, conducted on six state-of-the-art LLMs including GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average success rate of over 90% for this attack. We provide a comprehensive analysis of why function calls are susceptible to such attacks and propose defensive strategies, including the use of defensive prompts. Our findings highlight the urgent need for enhanced security measures in the function calling capabilities of LLMs, contributing to the field of AI safety by identifying a previously unexplored risk, designing an effective attack method, and suggesting practical defensive measures",
        "author": "Zihui Wu; Haichang Gao; Jianping He; Ping Wang",
        "authorids": "/z/zihui-wu/; /h/haichang-gao/; /j/jianping-he/; /p/ping-wang/",
        "bibtex": "@inproceedings{wu-etal-2025-dark,\n    title = \"The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models\",\n    author = \"Wu, Zihui  and\n      Gao, Haichang  and\n      He, Jianping  and\n      Wang, Ping\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.39/\",\n    pages = \"584--592\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.39.pdf",
        "site": "https://aclanthology.org/2025.coling-main.39/",
        "pdf_size": 1528908,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2387702290294125517&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Technology, Xidian University; School of Computer Science and Technology, Xidian University; School of Computer Science and Technology, Xidian University; School of Computer Science and Technology, Xidian University",
        "aff_domain": "stu.xidian.edu.cn;xidian.edu.cn; ; ",
        "email": "stu.xidian.edu.cn;xidian.edu.cn; ; ",
        "github": "https://github.com/wooozihui/jailbreakfunction",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xidian University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.xidian.edu.cn",
        "aff_unique_abbr": "Xidian",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.187",
        "title": "The Gaps between Fine Tuning and In-context Learning in Bias Evaluation and Debiasing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The output tendencies of PLMs vary markedly before and after FT due to the updates to the model parameters. These divergences in output tendencies result in a gap in the social biases of PLMs. For example, there exits a low correlation between intrinsic bias scores of a PLM and its extrinsic bias scores under FT-based debiasing methods. Additionally, applying FT-based debiasing methods to a PLM leads to a decline in performance in downstream tasks. On the other hand, PLMs trained on large datasets can learn without parameter updates via ICL using prompts. ICL induces smaller changes to PLMs compared to FT-based debiasing methods. Therefore, we hypothesize that the gap observed in pre-trained and FT models does not hold true for debiasing methods that use ICL. In this study, we demonstrate that ICL-based debiasing methods show a higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods. Moreover, the performance degradation due to debiasing is also lower in the ICL case compared to that in the FT case.",
        "author": "Masahiro Kaneko; Danushka Bollegala; Timothy Baldwin",
        "authorids": "/m/masahiro-kaneko/; /d/danushka-bollegala/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{kaneko-etal-2025-gaps,\n    title = \"The Gaps between Fine Tuning and In-context Learning in Bias Evaluation and Debiasing\",\n    author = \"Kaneko, Masahiro  and\n      Bollegala, Danushka  and\n      Baldwin, Timothy\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.187/\",\n    pages = \"2758--2764\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.187.pdf",
        "site": "https://aclanthology.org/2025.coling-main.187/",
        "pdf_size": 691089,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gfRdd30m7EkJ:scholar.google.com/&scioq=The+Gaps+between+Fine+Tuning+and+In-context+Learning+in+Bias+Evaluation+and+Debiasing&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "MBZUAI; University of Liverpool + Amazon; MBZUAI",
        "aff_domain": "mbzuai.ac.ae;liverpool.ac.uk;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;liverpool.ac.uk;mbzuai.ac.ae",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Mohamed Bin Zayed University of Artificial Intelligence;University of Liverpool;Amazon.com, Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.liverpool.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "MBZUAI;Liv Uni;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "United Arab Emirates;United Kingdom;United States"
    },
    {
        "id": "2025.coling-main.453",
        "title": "The Invalsi Benchmarks: measuring the Linguistic and Mathematical understanding of Large Language Models in Italian",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While Italian is a high-resource language, there are few Italian-native benchmarks to evaluate generative Large Language Models (LLMs) in this language. This work presents three new benchmarks: Invalsi MATE to evaluate models performance on mathematical understanding in Italian, Invalsi ITA to evaluate language under standing in Italian and Olimpiadi MATE for more complex mathematical understanding. The first two benchmarks are based on the Invalsi tests, which are administered to students of age between 6 and 18 within the Italian school system and have been validated by several experts in teaching and pedagogy, the third one comes from the Italian highschool math Olympics. We evaluate 10 powerful language models on these benchmarks and we find that they are bound by 71% accuracy on Invalsi MATE, achieved by Llama 3.1 70b instruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we compare LLMs with the average performance of Italian students to show that Llama 3.1 is the only one to outperform them on Invalsi MATE while most models do so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than Invalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct accuracy is 45%.",
        "author": "Giovanni Puccetti; Maria Cassese; Andrea Esuli",
        "authorids": "/g/giovanni-puccetti/; /m/maria-cassese/; /a/andrea-esuli/",
        "bibtex": "@inproceedings{puccetti-etal-2025-invalsi,\n    title = \"The Invalsi Benchmarks: measuring the Linguistic and Mathematical understanding of Large Language Models in {I}talian\",\n    author = \"Puccetti, Giovanni  and\n      Cassese, Maria  and\n      Esuli, Andrea\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.453/\",\n    pages = \"6782--6797\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.453.pdf",
        "site": "https://aclanthology.org/2025.coling-main.453/",
        "pdf_size": 492800,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pBuuLF7Q4kMJ:scholar.google.com/&scioq=The+Invalsi+Benchmarks:+measuring+the+Linguistic+and+Mathematical+understanding+of+Large+Language+Models+in+Italian&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "Institute of Science and Technologies of Information \u201cA. Faedo\u201d \u2013 CNR Pisa; Institute of Science and Technologies of Information \u201cA. Faedo\u201d \u2013 CNR Pisa; Institute of Science and Technologies of Information \u201cA. Faedo\u201d \u2013 CNR Pisa",
        "aff_domain": "isti.cnr.it;isti.cnr.it;isti.cnr.it",
        "email": "isti.cnr.it;isti.cnr.it;isti.cnr.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National Research Council of Italy",
        "aff_unique_dep": "Institute of Science and Technologies of Information \"A. Faedo\"",
        "aff_unique_url": "https://www.cnr.it/en",
        "aff_unique_abbr": "CNR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pisa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2025.coling-main.603",
        "title": "The Only Way is Ethics: A Guide to Ethical Research with Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "There is a significant body of work looking at the ethical considerations of large language models (LLMs): critiquing tools to measure performance and harms; proposing toolkits to aid in ideation; discussing the risks to workers; considering legislation around privacy and security etc. As yet there is no work that integrates these resources into a single practical guide that focuses on LLMs; we attempt this ambitious goal. We introduce LLM Ethics Whitepaper, which we provide as an open and living resource for NLP practitioners, and those tasked with evaluating the ethical implications of others\u2019 work. Our goal is to translate ethics literature into concrete recommendations for computer scientists. LLM Ethics Whitepaper distils a thorough literature review into clear Do\u2019s and Don\u2019ts, which we present also in this paper. We likewise identify useful toolkits to support ethical work. We refer the interested reader to the full LLM Ethics Whitepaper, which provides a succinct discussion of ethical considerations at each stage in a project lifecycle, as well as citations for the hundreds of papers from which we drew our recommendations. The present paper can be thought of as a pocket guide to conducting ethical research with LLMs.",
        "author": "Eddie L. Ungless; Nikolas Vitsakis; Zeerak Talat; James Garforth; Bjorn Ross; Arno Onken; Atoosa Kasirzadeh; Alexandra Birch",
        "authorids": "/e/eddie-l-ungless/; /n/nikolas-vitsakis/; /z/zeerak-talat/; /j/james-garforth/; /b/bjorn-ross/; /a/arno-onken/; /a/atoosa-kasirzadeh/; /a/alexandra-birch/",
        "bibtex": "@inproceedings{ungless-etal-2025-way,\n    title = \"The Only Way is Ethics: A Guide to Ethical Research with Large Language Models\",\n    author = \"Ungless, Eddie L.  and\n      Vitsakis, Nikolas  and\n      Talat, Zeerak  and\n      Garforth, James  and\n      Ross, Bjorn  and\n      Onken, Arno  and\n      Kasirzadeh, Atoosa  and\n      Birch, Alexandra\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.603/\",\n    pages = \"8992--9005\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.603.pdf",
        "site": "https://aclanthology.org/2025.coling-main.603/",
        "pdf_size": 403246,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nXUrg2-sgs4J:scholar.google.com/&scioq=The+Only+Way+is+Ethics:+A+Guide+to+Ethical+Research+with+Large+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "University of Edinburgh; Heriot-Watt University; University of Edinburgh; University of Edinburgh; University of Edinburgh; University of Edinburgh; University of Edinburgh; University of Edinburgh",
        "aff_domain": "ed.ac.uk; ; ; ; ; ; ;ed.ac.uk",
        "email": "ed.ac.uk; ; ; ; ; ; ;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;0;0;0;0;0",
        "aff_unique_norm": "University of Edinburgh;Heriot-Watt University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.hw.ac.uk",
        "aff_unique_abbr": "Edinburgh;HWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.2",
        "title": "The PRECOM-SM Corpus: Gambling in Spanish Social Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Gambling addiction is a \u201csilent problem\u201d in society, especially among young people in recent years due to the easy access to betting and gambling sites on the Internet through smartphones and personal computers. As online communities in messaging apps, forums and other \u201cteenagers gathering\u201d sites keep growing day by day, more textual information is available for its study. This work focuses on collecting text from online Spanish-speaking communities and analysing it in order to find patterns in written language from frequent and infrequent users on the collected platforms so that an emerging gambling addiction problem can be detected. In this paper, a newly built corpus is introduced, as well as an extensive description of how it has been made. Besides, some baseline experiments on the data have been carried on, employing the generated features after the analysis of the text with different machine learning approaches like the bag of words model or deep neural network encodings.",
        "author": "Pablo \u00c1lvarez-Ojeda; Mar\u00eda Victoria Cantero-Romero; Anastasia Semikozova; Arturo Montejo-Raez",
        "authorids": "/p/pablo-alvarez-ojeda/; /m/maria-victoria-cantero-romero/; /a/anastasia-semikozova/; /a/arturo-montejo-raez/",
        "bibtex": "@inproceedings{alvarez-ojeda-etal-2025-precom,\n    title = \"The {PRECOM}-{SM} Corpus: Gambling in {S}panish Social Media\",\n    author = \"{\\'A}lvarez-Ojeda, Pablo  and\n      Cantero-Romero, Mar{\\'i}a Victoria  and\n      Semikozova, Anastasia  and\n      Montejo-Raez, Arturo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.2/\",\n    pages = \"17--28\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.2.pdf",
        "site": "https://aclanthology.org/2025.coling-main.2/",
        "pdf_size": 247808,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aYM4YgvhsOIJ:scholar.google.com/&scioq=The+PRECOM-SM+Corpus:+Gambling+in+Spanish+Social+Media&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Universidad de Ja\u00e9n, Campus Las Lagunillas, 23071, Ja\u00e9n, Spain; Universidad de Ja\u00e9n, Campus Las Lagunillas, 23071, Ja\u00e9n, Spain; Universidad de Ja\u00e9n, Campus Las Lagunillas, 23071, Ja\u00e9n, Spain; Universidad de Ja\u00e9n, Campus Las Lagunillas, 23071, Ja\u00e9n, Spain",
        "aff_domain": "ujaen.es;ujaen.es;ujaen.es;ujaen.es",
        "email": "ujaen.es;ujaen.es;ujaen.es;ujaen.es",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universidad de Ja\u00e9n",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ujaen.es",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Las Lagunillas",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-main.566",
        "title": "The Role of Natural Language Processing Tasks in Automatic Literary Character Network Construction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The automatic extraction of character networks from literary texts is generally carried out using natural language processing (NLP) cascading pipelines. While this approach is widespread, no study exists on the impact of low-level NLP tasks on their performance. In this article, we conduct such a study on a literary dataset, focusing on the role of named entity recognition (NER) and coreference resolution when extracting co-occurrence networks. To highlight the impact of these tasks\u2019 performance, we start with gold-standard annotations, progressively add uniformly distributed errors, and observe their impact in terms of character network quality. We demonstrate that NER performance depends on the tested novel and strongly affects character detection. We also show that NER-detected mentions alone miss a lot of character co-occurrences, and that coreference resolution is needed to prevent this. Finally, we present comparison points with 2 methods based on large language models (LLMs), including a fully end-to-end one, and show that these models are outperformed by traditional NLP pipelines in terms of recall.",
        "author": "Arthur Amalvy; Vincent Labatut; Richard Dufour",
        "authorids": "/a/arthur-amalvy/; /v/vincent-labatut/; /r/richard-dufour/",
        "bibtex": "@inproceedings{amalvy-etal-2025-role,\n    title = \"The Role of Natural Language Processing Tasks in Automatic Literary Character Network Construction\",\n    author = \"Amalvy, Arthur  and\n      Labatut, Vincent  and\n      Dufour, Richard\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.566/\",\n    pages = \"8462--8473\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.566.pdf",
        "site": "https://aclanthology.org/2025.coling-main.566/",
        "pdf_size": 667145,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17430768702048527459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Laboratoire Informatique d\u2019Avignon; Laboratoire Informatique d\u2019Avignon; Laboratoire des Sciences du Num\u00e9rique de Nantes",
        "aff_domain": "univ-avignon.fr;univ-avignon.fr;univ-nantes.fr",
        "email": "univ-avignon.fr;univ-avignon.fr;univ-nantes.fr",
        "github": "https://github.com/CompNet/Splice",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Laboratoire Informatique d\u2019Avignon;Laboratoire des Sciences du Num\u00e9rique de Nantes",
        "aff_unique_dep": "Informatique;Sciences du Num\u00e9rique",
        "aff_unique_url": "https://www.lia.univ-avignon.fr;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.321",
        "title": "The Shift from Logic to Dialectic in Argumentation Theory: Implications for Computational Argument Quality Assessment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the field of computational argument quality assessment, logic and dialectic are essential dimensions used to measure the quality of argumentative texts. Both of them have found their way into the field due to their importance to argumentation theory. We trace the development of core logical concepts of validity and soundness from their first use in argumentation theory to their understanding in state-of-the-art research. We show how, in the course of this development, dialectical considerations have taken center stage, at the cost of the logical perspective. Then, we take a closer look at the quality dimensions used in the field of computational argument quality assessment. Based on an analysis of prior empirical work in this field, we show how methodological considerations from argument theory can benefit state-of-the-art methods in computational argument quality assessment. We propose an even clearer separation between the two quality dimensions not only in regards to their definitions, but also in regards to the granularity at which the argumentative text is being annotated and assessed.",
        "author": "Rositsa V Ivanova; Reto Gubelmann",
        "authorids": "/r/rositsa-v-ivanova/; /r/reto-gubelmann/",
        "bibtex": "@inproceedings{ivanova-gubelmann-2025-shift,\n    title = \"The Shift from Logic to Dialectic in Argumentation Theory: Implications for Computational Argument Quality Assessment\",\n    author = \"Ivanova, Rositsa V  and\n      Gubelmann, Reto\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.321/\",\n    pages = \"4789--4802\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.321.pdf",
        "site": "https://aclanthology.org/2025.coling-main.321/",
        "pdf_size": 263772,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0A-Wyvw1PR4J:scholar.google.com/&scioq=The+Shift+from+Logic+to+Dialectic+in+Argumentation+Theory:+Implications+for+Computational+Argument+Quality+Assessment&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Institute of Computer Science, University of St. Gallen, Switzerland; Digital Society Initiative & Department of Computational Linguistics, University of Zurich, Switzerland",
        "aff_domain": "unisg.ch;uzh.ch",
        "email": "unisg.ch;uzh.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of St. Gallen;University of Zurich",
        "aff_unique_dep": "Institute of Computer Science;Department of Computational Linguistics",
        "aff_unique_url": "https://www.unisg.ch;https://www.unizh.ch",
        "aff_unique_abbr": ";UZH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2025.coling-main.206",
        "title": "To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose Hybrid Uncertainty and Diversity Sampling (HUDS), an AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection. HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them. It then clusters sentence embeddings within each stratum and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then used to select the top instances for annotation in each AL iteration. Experiments on multi-domain German-English and French-English datasets demonstrate the better performance of HUDS over other strong AL baselines. We analyze the sentence selection with HUDS and show that it prioritizes diverse instances having high model uncertainty for annotation in early AL iterations.",
        "author": "Abdul Hameed Azeemi; Ihsan Ayyub Qazi; Agha Ali Raza",
        "authorids": "/a/abdul-hameed-azeemi/; /i/ihsan-ayyub-qazi/; /a/agha-ali-raza/",
        "bibtex": "@inproceedings{azeemi-etal-2025-label,\n    title = \"To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation\",\n    author = \"Azeemi, Abdul Hameed  and\n      Qazi, Ihsan Ayyub  and\n      Raza, Agha Ali\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.206/\",\n    pages = \"3071--3082\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.206.pdf",
        "site": "https://aclanthology.org/2025.coling-main.206/",
        "pdf_size": 938313,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16347241396077729271&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2025.coling-main.79",
        "title": "Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Each new generation of English-oriented Large Language Models (LLMs) exhibits enhanced cross-lingual transfer capabilities and significantly outperforms older LLMs on low-resource languages. This prompts the question: Is there a need for LLMs dedicated to a particular low-resource language? We aim to explore this question for Bengali, a low-to-moderate resource Indo-Aryan language native to the Bengal region of South Asia. We compare the performance of open-weight and closed-source LLMs such as LLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse set of Bengali downstream tasks, including translation, summarization, paraphrasing, question-answering, and natural language inference. Our findings reveal that while LLMs generally excel in reasoning tasks, their performance in tasks requiring Bengali script generation is inconsistent. Key challenges include inefficient tokenization of Bengali script by existing LLMs, leading to increased computational costs and potential performance degradation. Additionally, we highlight biases in machine-translated datasets commonly used for Bengali NLP tasks. We conclude that there is a significant need for a Bengali-oriented LLM, but the field currently lacks the high-quality pretraining and instruction-tuning datasets necessary to develop a highly effective model.",
        "author": "Tamzeed Mahfuz; Satak Kumar Dey; Ruwad Naswan; Hasnaen Adil; Khondker Salman Sayeed; Haz Sameen Shahgir",
        "authorids": "/t/tamzeed-mahfuz/; /s/satak-kumar-dey/; /r/ruwad-naswan/; /h/hasnaen-adil/; /k/khondker-salman-sayeed/; /h/haz-sameen-shahgir/",
        "bibtex": "@inproceedings{mahfuz-etal-2025-late,\n    title = \"Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource {B}engali {LLM}s\",\n    author = \"Mahfuz, Tamzeed  and\n      Dey, Satak Kumar  and\n      Naswan, Ruwad  and\n      Adil, Hasnaen  and\n      Sayeed, Khondker Salman  and\n      Shahgir, Haz Sameen\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.79/\",\n    pages = \"1183--1200\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.79.pdf",
        "site": "https://aclanthology.org/2025.coling-main.79/",
        "pdf_size": 864575,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7876119395264192540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Bangladesh University of Engineering and Technology1; Bangladesh University of Engineering and Technology1; Bangladesh University of Engineering and Technology1; Bangladesh University of Engineering and Technology1; IQVIA2; University of California Riverside3",
        "aff_domain": "buet.ac.bd;buet.ac.bd;buet.ac.bd;buet.ac.bd;iqvia.com;ucr.edu",
        "email": "buet.ac.bd;buet.ac.bd;buet.ac.bd;buet.ac.bd;iqvia.com;ucr.edu",
        "github": "https://github.com/satak100/BanglaBench",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;2",
        "aff_unique_norm": "Bangladesh University of Engineering and Technology;IQVIA;University of California, Riverside",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.buet.ac.bd;https://www.iqvia.com;https://www.ucr.edu",
        "aff_unique_abbr": "BUET;IQVIA;UCR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Riverside",
        "aff_country_unique_index": "0;0;0;0;1;1",
        "aff_country_unique": "Bangladesh;United States"
    },
    {
        "id": "2025.coling-main.12",
        "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined. Furthermore, a sole emphasis on outcomes disregards the complex capabilities required for LLMs to effectively use tools. To tackle this issue, we propose ToolEyes, a fine-grained system tailored for the evaluation of the LLMs\u2019 tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning. The code and data are available at https://github.com/Junjie-Ye/ToolEyes.",
        "author": "Junjie Ye; Guanyu Li; SongYang Gao; Caishuang Huang; Yilong Wu; Sixian Li; Xiaoran Fan; Shihan Dou; Tao Ji; Qi Zhang; Tao Gui; Xuanjing Huang",
        "authorids": "/j/junjie-ye/; /g/guanyu-li/; /s/songyang-gao/; /c/caishuang-huang/; /y/yilong-wu/; /s/sixian-li/; /x/xiaoran-fan/; /s/shihan-dou/; /t/tao-ji/; /q/qi-zhang/; /t/tao-gui/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{ye-etal-2025-tooleyes,\n    title = \"{T}ool{E}yes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios\",\n    author = \"Ye, Junjie  and\n      Li, Guanyu  and\n      Gao, SongYang  and\n      Huang, Caishuang  and\n      Wu, Yilong  and\n      Li, Sixian  and\n      Fan, Xiaoran  and\n      Dou, Shihan  and\n      Ji, Tao  and\n      Zhang, Qi  and\n      Gui, Tao  and\n      Huang, Xuanjing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.12/\",\n    pages = \"156--187\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.12.pdf",
        "site": "https://aclanthology.org/2025.coling-main.12/",
        "pdf_size": 3011035,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12264832200676856438&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University + Shanghai Key Laboratory of Intelligent Information Processing; Institute of Modern Languages and Linguistics, Fudan University + Pengcheng Laboratory; School of Computer Science, Fudan University + Research Institute of Intelligent Complex Systems, Fudan University",
        "aff_domain": "m.fudan.edu.cn; ; ; ; ; ; ; ; ;fudan.edu.cn;fudan.edu.cn; ",
        "email": "m.fudan.edu.cn; ; ; ; ; ; ; ; ;fudan.edu.cn;fudan.edu.cn; ",
        "github": "https://github.com/Junjie-Ye/ToolEyes",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0+1;0+2;0+0",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Intelligent Information Processing;Pengcheng Laboratory",
        "aff_unique_dep": "School of Computer Science;Intelligent Information Processing;",
        "aff_unique_url": "https://www.fudan.edu.cn;;",
        "aff_unique_abbr": "Fudan;;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.191",
        "title": "Topology-of-Question-Decomposition: Enhancing Large Language Models with Information Retrieval for Knowledge-Intensive Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) are increasingly deployed for general problem-solving across various domains yet remain constrained to chaining immediate reasoning steps and depending solely on parametric knowledge. Integrating an information retrieval system directly into the reasoning process of LLMs can improve answer accuracy but might disrupt the natural reasoning sequence. Consequently, LLMs may underperform in complex, knowledge-intensive tasks requiring multiple reasoning steps, extensive real-world knowledge, or critical initial decisions. To overcome these challenges, we introduce a novel framework, Topology-of-Question-Decomposition (ToQD), which activates retrieval only when necessary. Globally, ToQD guides LLMs in constructing a topology graph from the input question, each node representing a sub-question. Locally, ToQD employs self-verify inference to determine whether a sub-question should retrieve relevant documents, necessitate further decomposition, or directly provide an answer. Experiments demonstrate that ToQD achieves superior performance and robustness in complex, knowledge-intensive tasks, significantly enhancing system response efficiency.",
        "author": "Weijie Li; Jin Wang; Liang-Chih Yu; Xuejie Zhang",
        "authorids": "/w/weijie-li/; /j/jin-wang/; /l/liang-chih-yu/; /x/xuejie-zhang/",
        "bibtex": "@inproceedings{li-etal-2025-topology,\n    title = \"Topology-of-Question-Decomposition: Enhancing Large Language Models with Information Retrieval for Knowledge-Intensive Tasks\",\n    author = \"Li, Weijie  and\n      Wang, Jin  and\n      Yu, Liang-Chih  and\n      Zhang, Xuejie\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.191/\",\n    pages = \"2814--2833\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.191.pdf",
        "site": "https://aclanthology.org/2025.coling-main.191/",
        "pdf_size": 2642913,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17619807912886970429&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China; School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China; Department of Information Management, Yuan Ze University, Taiwan; School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China",
        "aff_domain": "stu.ynu.edu.cn;ynu.edu.cn;saturn.yzu.edu.tw;ynu.edu.cn",
        "email": "stu.ynu.edu.cn;ynu.edu.cn;saturn.yzu.edu.tw;ynu.edu.cn",
        "github": "https://github.com/DCVDB/ToQD",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Yunnan University;Yuan Ze University",
        "aff_unique_dep": "School of Information Science and Engineering;Department of Information Management",
        "aff_unique_url": "http://www.ynu.edu.cn;https://www.yzu.edu.tw",
        "aff_unique_abbr": "YNU;YZU",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Yunnan;Taiwan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.432",
        "title": "Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advancements in natural language processing have highlighted the vulnerability of deep learning models to adversarial attacks. While various defence mechanisms have been proposed, there is a lack of comprehensive benchmarks that evaluate these defences across diverse datasets, models, and tasks. In this work, we address this gap by presenting an extensive benchmark for textual adversarial defence that significantly expands upon previous work. Our benchmark incorporates a wide range of datasets, evaluates state-of-the-art defence mechanisms, and extends the assessment to include critical tasks such as single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning. This work not only serves as a valuable resource for researchers and practitioners in the field of adversarial robustness but also identifies key areas for future research in textual adversarial defence. By establishing a new standard for benchmarking in this domain, we aim to accelerate progress towards more robust and reliable natural language processing systems.",
        "author": "Yang Wang; Chenghua Lin",
        "authorids": "/y/yang-wang/; /c/chenghua-lin/",
        "bibtex": "@inproceedings{wang-lin-2025-tougher,\n    title = \"Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks\",\n    author = \"Wang, Yang  and\n      Lin, Chenghua\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.432/\",\n    pages = \"6475--6491\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.432.pdf",
        "site": "https://aclanthology.org/2025.coling-main.432/",
        "pdf_size": 414938,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fJrsyTPTHDoJ:scholar.google.com/&scioq=Tougher+Text,+Smarter+Models:+Raising+the+Bar+for+Adversarial+Defence+Benchmarks&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, The University of Sheffield, UK + Automated Analytics, UK; Department of Computer Science, The University of Sheffield, UK + Department of Computer Science, The University of Manchester, UK",
        "aff_domain": "sheffield.ac.uk;manchester.ac.uk",
        "email": "sheffield.ac.uk;manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+2",
        "aff_unique_norm": "The University of Sheffield;Automated Analytics;The University of Manchester",
        "aff_unique_dep": "Department of Computer Science;;Department of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk;;https://www.manchester.ac.uk",
        "aff_unique_abbr": "Sheffield;;UoM",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.194",
        "title": "Towards Adaptive Mechanism Activation in Language Agent",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language Agent could be endowed with different mechanisms for autonomous task accomplishment. Current agents typically rely on a fixed mechanism or a set of mechanisms activated in a predefined order, limiting their adaptation to varied potential task solution structures. To this end, this paper proposes Adaptive Language Agent Mechanism Activation Learning with Self-Exploration (ALAMA), which focuses on optimizing mechanism activation adaptability without reliance on expert models. Initially, it builds a harmonized agent framework (UniAct) to Unify different mechanisms via Actions. Then it leverages a training-efficient optimization method based on self-exploration to enable the UniAct to adaptively activate the appropriate mechanisms according to the potential characteristics of the task. Experimental results demonstrate significant improvements in downstream agent tasks, affirming the effectiveness of our approach in facilitating more dynamic and context-sensitive mechanism activation.",
        "author": "Ziyang Huang; Jun Zhao; Kang Liu",
        "authorids": "/z/ziyang-huang/; /j/jun-zhao/; /k/kang-liu/",
        "bibtex": "@inproceedings{huang-etal-2025-towards,\n    title = \"Towards Adaptive Mechanism Activation in Language Agent\",\n    author = \"Huang, Ziyang  and\n      Zhao, Jun  and\n      Liu, Kang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.194/\",\n    pages = \"2867--2885\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.194.pdf",
        "site": "https://aclanthology.org/2025.coling-main.194/",
        "pdf_size": 932945,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oBC7xLQ86bAJ:scholar.google.com/&scioq=Towards+Adaptive+Mechanism+Activation+in+Language+Agent&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences; The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation",
        "aff_unique_url": "http://www.ia.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.65",
        "title": "Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "This paper studies the relevance modeling problem by integrating world knowledge stored in the parameters of LLMs with specialized domain knowledge represented by user behavior data for achieving promising performance. The novel framework ProRBP is proposed, which innovatively develops user-driven behavior neighbor retrieval module to learn domain-specific knowledge in time and introduces progressive prompting and aggregation module for considering diverse aspects of the relevance and prediction stability. We explore an industrial implementation to deploy LLMs to handle full-scale search traffics of Alipay with acceptable cost and latency. The comprehensive experiments on real-world industry data and online A/B testing validate the superiority of our proposal and the effectiveness of its main modules.",
        "author": "Zeyuan Chen; Haiyan Wu; Kaixin Wu; Wei Chen; Mingjie Zhong; Jia Xu; Zhongyi Liu; Wei Zhang",
        "authorids": "/z/zeyuan-chen/; /h/haiyan-wu/; /k/kaixin-wu/; /w/wei-chen/; /m/mingjie-zhong/; /j/jia-xu/; /z/zhongyi-liu/; /w/wei-zhang/",
        "bibtex": "@inproceedings{chen-etal-2025-towards-boosting,\n    title = \"Towards Boosting {LLM}s-driven Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting\",\n    author = \"Chen, Zeyuan  and\n      Wu, Haiyan  and\n      Wu, Kaixin  and\n      Chen, Wei  and\n      Zhong, Mingjie  and\n      Xu, Jia  and\n      Liu, Zhongyi  and\n      Zhang, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.65/\",\n    pages = \"784--793\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.65.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.65/",
        "pdf_size": 444429,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10247283878199318079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Ant Group; Alibaba Group; Ant Group; Ant Group; Ant Group; Ant Group; Ant Group; School of Computer Science and Technology, East China Normal University",
        "aff_domain": "antgroup.com;antgroup.com;antgroup.com;antgroup.com;antgroup.com;antgroup.com;taobao.com;gmail.com",
        "email": "antgroup.com;antgroup.com;antgroup.com;antgroup.com;antgroup.com;antgroup.com;taobao.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;0;0;0;0;2",
        "aff_unique_norm": "Ant Group;Alibaba Group;East China Normal University",
        "aff_unique_dep": ";;School of Computer Science and Technology",
        "aff_unique_url": "https://www.antgroup.com;https://www.alibaba.com;http://www.ecnu.edu.cn",
        "aff_unique_abbr": "Ant Group;Alibaba;ECNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.505",
        "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may explain \u201call birds can fly\u201d when answering the question \u201cCan sparrows fly?\u201d but meanwhile answer \u201cno\u201d to the related question \u201cCan penguins fly?\u201d. Explanations should be consistent across related examples so that they allow humans to simulate the LLM\u2019s decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on 4 finetuning datasets, and generalizes to 7 out-of-distribution datasets not seen during finetuning (+4.5% relative). We will make our code available for reproducibility.",
        "author": "Yanda Chen; Chandan Singh; Xiaodong Liu; Simiao Zuo; Bin Yu; He He; Jianfeng Gao",
        "authorids": "/y/yanda-chen/; /c/chandan-singh/; /x/xiaodong-liu/; /s/simiao-zuo/; /b/bin-yu/; /h/he-he/; /j/jianfeng-gao/",
        "bibtex": "@inproceedings{chen-etal-2025-towards,\n    title = \"Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning\",\n    author = \"Chen, Yanda  and\n      Singh, Chandan  and\n      Liu, Xiaodong  and\n      Zuo, Simiao  and\n      Yu, Bin  and\n      He, He  and\n      Gao, Jianfeng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.505/\",\n    pages = \"7558--7568\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.505.pdf",
        "site": "https://aclanthology.org/2025.coling-main.505/",
        "pdf_size": 346342,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1533705683306211907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Columbia University; Microsoft Research; Microsoft Research; Microsoft Research; University of California, Berkeley; NYU; Microsoft Research",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;2;3;1",
        "aff_unique_norm": "Columbia University;Microsoft Corporation;University of California, Berkeley;New York University",
        "aff_unique_dep": ";Microsoft Research;;",
        "aff_unique_url": "https://www.columbia.edu;https://www.microsoft.com/en-us/research;https://www.berkeley.edu;https://www.nyu.edu",
        "aff_unique_abbr": "Columbia;MSR;UC Berkeley;NYU",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;New York",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.373",
        "title": "Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.",
        "author": "Aditya Narayan Sankaran; Reza Farahbakhsh; Noel Crespi",
        "authorids": "/a/aditya-narayan-sankaran/; /r/reza-farahbakhsh/; /n/noel-crespi/",
        "bibtex": "@inproceedings{sankaran-etal-2025-towards,\n    title = \"Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning\",\n    author = \"Sankaran, Aditya Narayan  and\n      Farahbakhsh, Reza  and\n      Crespi, Noel\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.373/\",\n    pages = \"5558--5569\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.373.pdf",
        "site": "https://aclanthology.org/2025.coling-main.373/",
        "pdf_size": 819272,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4TeeOjyiyrEJ:scholar.google.com/&scioq=Towards+Cross-Lingual+Audio+Abuse+Detection+in+Low-Resource+Settings+with+Few-Shot+Learning&hl=en&as_sdt=0,44",
        "gs_version_total": 2,
        "aff": "SAMOV AR, T\u00e9l\u00e9com SudParis; SAMOV AR, T\u00e9l\u00e9com SudParis; SAMOV AR, T\u00e9l\u00e9com SudParis",
        "aff_domain": "ip-paris.fr; ; ",
        "email": "ip-paris.fr; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "T\u00e9l\u00e9com SudParis",
        "aff_unique_dep": "SAMOV AR",
        "aff_unique_url": "https://www.telecom-sudparis.eu",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.338",
        "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed. However, these approaches are often validated with traditional benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness when evaluating state-of-the-art LLMs on the contamination of more challenging benchmarks. To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation. Our analysis reveals that (1) Current methods have non-trivial limitations in their assumptions and practical applications; (2) Notable difficulties exist in detecting contamination introduced during instruction fine-tuning with answer augmentation; and (3) Limited consistencies between SOTA contamination detection techniques. These findings highlight the complexity of contamination detection in advanced LLMs and the urgent need for further research on robust and generalizable contamination evaluation.",
        "author": "Vinay Samuel; Yue Zhou; Henry Peng Zou",
        "authorids": "/v/vinay-samuel/; /y/yue-zhou/; /h/henry-peng-zou/",
        "bibtex": "@inproceedings{samuel-etal-2025-towards,\n    title = \"Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges\",\n    author = \"Samuel, Vinay  and\n      Zhou, Yue  and\n      Zou, Henry Peng\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.338/\",\n    pages = \"5058--5070\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.338.pdf",
        "site": "https://aclanthology.org/2025.coling-main.338/",
        "pdf_size": 553689,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9892821934536240173&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University; University of Illinois Chicago; University of Illinois Chicago",
        "aff_domain": "andrew.cmu.edu; ; ",
        "email": "andrew.cmu.edu; ; ",
        "github": "https://github.com/vsamuel2003/data-contamination",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Carnegie Mellon University;University of Illinois at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.uic.edu",
        "aff_unique_abbr": "CMU;UIC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.308",
        "title": "Towards Database-Free Text-to-SQL Evaluation: A Graph-Based Metric for Functional Correctness",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Execution Accuracy and Exact Set Match are two predominant metrics for evaluating the functional correctness of SQL queries in modern Text-to-SQL tasks. However, both metrics have notable limitations: Exact Set Match fails when queries are functionally equivalent but syntactically different, while Execution Accuracy is prone to false positives due to inadequately prepared test databases, which can be costly to create, particularly in large-scale industrial applications. To overcome these challenges, we propose a novel graph-based metric, FuncEvalGMN, that effectively overcomes the deficiencies of the aforementioned metric designs. Our method utilizes a relational operator tree (ROT), referred to as RelNode, to extract rich semantic information from the logical execution plan of SQL queries, and embed it into a graph. We then train a graph neural network (GNN) to perform graph matching on pairs of SQL queries through graph contrastive learning. FuncEvalGMN offers two highly desired advantages: (i) it requires only the database schema to derive logical execution plans, eliminating the need for extensive test database preparation, and (ii) it demonstrates strong generalization capabilities on unseen datasets. These properties highlight FuncEvalGMN\u2019s robustness as a reliable metric for assessing functional correctness across a wide range of Text-to-SQL applications.",
        "author": "Yi Zhan; Longjie Cui; Han Weng; Guifeng Wang; Yu Tian; Boyi Liu; Yingxiang Yang; Xiaoming Yin; Jiajun Xie; Yang Sun",
        "authorids": "/y/yi-zhan/; /l/longjie-cui/; /h/han-weng/; /g/guifeng-wang/; /y/yu-tian/; /b/boyi-liu/; /y/yingxiang-yang/; /x/xiaoming-yin/; /j/jiajun-xie/; /y/yang-sun/",
        "bibtex": "@inproceedings{zhan-etal-2025-towards,\n    title = \"Towards Database-Free Text-to-{SQL} Evaluation: A Graph-Based Metric for Functional Correctness\",\n    author = \"Zhan, Yi  and\n      Cui, Longjie  and\n      Weng, Han  and\n      Wang, Guifeng  and\n      Tian, Yu  and\n      Liu, Boyi  and\n      Yang, Yingxiang  and\n      Yin, Xiaoming  and\n      Xie, Jiajun  and\n      Sun, Yang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.308/\",\n    pages = \"4586--4610\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.308.pdf",
        "site": "https://aclanthology.org/2025.coling-main.308/",
        "pdf_size": 4384301,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oQIgVZH6mRwJ:scholar.google.com/&scioq=Towards+Database-Free+Text-to-SQL+Evaluation:+A+Graph-Based+Metric+for+Functional+Correctness&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Peking University, Beijing, China; Department of Computer Science, The University of Hong Kong, Hong Kong; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China; ByteDance Inc., Beijing, China",
        "aff_domain": "stu.pku.edu.cn;connect.hku.hk;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "email": "stu.pku.edu.cn;connect.hku.hk;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "github": "https://github.com/Leon0-0/FuncEvalGMN",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;2;2;2;2;2;2;2",
        "aff_unique_norm": "Peking University;The University of Hong Kong;ByteDance Inc.",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.hku.hk;https://www.bytedance.com",
        "aff_unique_abbr": "PKU;HKU;ByteDance",
        "aff_campus_unique_index": "0;1;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.292",
        "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural Language Explanation (NLE) aims to elucidate the decision-making process by providing detailed, human-friendly explanations in natural language. It helps demystify the decision-making processes of large vision-language models (LVLMs) through the use of language models. While existing methods for creating a Vision Question-Answering with Natural Language Explanation (VQA-NLE) datasets can provide explanations, they heavily rely on human annotations that are time-consuming and costly. In this study, we propose a novel approach that leverages LVLMs to efficiently generate high-quality synthetic VQA-NLE datasets. By evaluating our synthetic data samples, we showcase how advanced prompting techniques can lead to the production of high-quality VQA-NLE data. Our findings indicate that this proposed method achieves up to 20x faster than human annotation, with only a minimal decrease in qualitative metrics, achieving robust quality that is nearly equivalent to human-annotated data. Furthermore, we show that incorporating visual prompts significantly enhances the relevance of text generation. Our study paves the way for a more efficient and robust automated generation of multi-modal NLE data, offering a promising solution to the problem.",
        "author": "Patrick Amadeus Irawan; Genta Indra Winata; Samuel Cahyawijaya; Ayu Purwarianti",
        "authorids": "/p/patrick-amadeus-irawan/; /g/genta-indra-winata/; /s/samuel-cahyawijaya/; /a/ayu-purwarianti/",
        "bibtex": "@inproceedings{irawan-etal-2025-towards,\n    title = \"Towards Efficient and Robust {VQA}-{NLE} Data Generation with Large Vision-Language Models\",\n    author = \"Irawan, Patrick Amadeus  and\n      Winata, Genta Indra  and\n      Cahyawijaya, Samuel  and\n      Purwarianti, Ayu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.292/\",\n    pages = \"4323--4340\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.292.pdf",
        "site": "https://aclanthology.org/2025.coling-main.292/",
        "pdf_size": 1662498,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J9ndn9OG_eQJ:scholar.google.com/&scioq=Towards+Efficient+and+Robust+VQA-NLE+Data+Generation+with+Large+Vision-Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Institut Teknologi Bandung; Capital One + The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; Institut Teknologi Bandung",
        "aff_domain": "gmail.com;capitalone.com;connect.ust.hk;informatika.org",
        "email": "gmail.com;capitalone.com;connect.ust.hk;informatika.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;2;0",
        "aff_unique_norm": "Institut Teknologi Bandung;Capital One;Hong Kong University of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.itb.ac.id;https://www.capitalone.com;https://www.ust.hk",
        "aff_unique_abbr": "ITB;Capital One;HKUST",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1+2;2;0",
        "aff_country_unique": "Indonesia;United States;China"
    },
    {
        "id": "2025.coling-main.157",
        "title": "Towards Faithful Multi-step Reasoning through Fine-Grained Causal-aware Attribution Reasoning Distillation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the remarkable reasoning capabilities demonstrated by large language models (LLM), the substantial computational overhead limits their practices. Some efforts have been directed toward distilling multi-step reasoning capabilities into smaller models through chain-of-thought (CoT). While CoT facilitates multi-step reasoning, the dependencies between reasoning steps are not always clearly discernible, which may lead to inconsistent reasoning. In this paper, we introduce fine-grained attribution reasoning distillation (FARD), which incorporates grounded citations to consolidate the relationships between reasoning steps. Specifically, FARD distills attribution reasoning rationales from LLMs to substitute CoT reasonings, which clarifies the dependencies among reasoning steps. Besides, we regularize the model\u2019s attention pattern by leveraging the causal dependencies between reasoning steps, thereby enhancing the consistency of reasoning. Grounded attribution reasoning also enhances interpretability and verifiability, thereby facilitating faithful reasoning. We evaluate FARD on mathematical and general reasoning benchmarks. The experimental results indicate that FARD outperforms CoT distillation methods in mathematical reasoning, demonstrating its effectiveness. Furthermore, the small models trained with FARD have shown outstanding performance in out-of-distribution reasoning, proving strong generalization capabilities.",
        "author": "Zheng Chu; Jingchang Chen; Zhongjie Wang; Guo Tang; Qianglong Chen; Ming Liu; Bing Qin",
        "authorids": "/z/zheng-chu/; /j/jingchang-chen/; /z/zhongjie-wang/; /g/guo-tang/; /q/qianglong-chen/; /m/ming-liu/; /b/bing-qin/",
        "bibtex": "@inproceedings{chu-etal-2025-towards,\n    title = \"Towards Faithful Multi-step Reasoning through Fine-Grained Causal-aware Attribution Reasoning Distillation\",\n    author = \"Chu, Zheng  and\n      Chen, Jingchang  and\n      Wang, Zhongjie  and\n      Tang, Guo  and\n      Chen, Qianglong  and\n      Liu, Ming  and\n      Qin, Bing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.157/\",\n    pages = \"2291--2315\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.157.pdf",
        "site": "https://aclanthology.org/2025.coling-main.157/",
        "pdf_size": 1786379,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12492089594006600298&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Zhejiang University+Peng Cheng Laboratory; Harbin Institute of Technology+Peng Cheng Laboratory; Harbin Institute of Technology+Peng Cheng Laboratory",
        "aff_domain": "ir.hit.edu.cn; ; ; ; ; ; ",
        "email": "ir.hit.edu.cn; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1+2;0+2;0+2",
        "aff_unique_norm": "Harbin Institute of Technology;Zhejiang University;Peng Cheng Laboratory",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.zju.edu.cn;http://www.pcl.ac.cn",
        "aff_unique_abbr": "HIT;ZJU;PCL",
        "aff_campus_unique_index": "0;0;0;0;;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0;0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.421",
        "title": "Towards Human Understanding of Paraphrase Types in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Paraphrases represent a human\u2019s intuitive ability to understand expressions presented in various different ways. Current paraphrase evaluations of language models primarily use binary approaches, offering limited interpretability of specific text changes. Atomic paraphrase types (APT) decompose paraphrases into different linguistic changes and offer a granular view of the flexibility in linguistic expression (e.g., a shift in syntax or vocabulary used). In this study, we assess the human preferences towards ChatGPT in generating English paraphrases with ten APTs and five prompting techniques. We introduce APTY (Atomic Paraphrase TYpes), a dataset of 800 sentence-level and word-level annotations by 15 annotators. The dataset also provides a human preference ranking of paraphrases with different types that can be used to fine-tune models with RLHF and DPO methods. Our results reveal that ChatGPT and a DPO-trained LLama 7B model can generate simple APTs, such as additions and deletions, but struggle with complex structures (e.g., subordination changes). This study contributes to understanding which aspects of paraphrasing language models have already succeeded at understanding and what remains elusive. In addition, we show how our curated datasets can be used to develop language models with specific linguistic capabilities.",
        "author": "Dominik Meier; Jan Philip Wahle; Terry Lima Ruas; Bela Gipp",
        "authorids": "/d/dominik-meier/; /j/jan-philip-wahle/; /t/terry-lima-ruas/; /b/bela-gipp/",
        "bibtex": "@inproceedings{meier-etal-2025-towards,\n    title = \"Towards Human Understanding of Paraphrase Types in Large Language Models\",\n    author = \"Meier, Dominik  and\n      Wahle, Jan Philip  and\n      Lima Ruas, Terry  and\n      Gipp, Bela\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.421/\",\n    pages = \"6298--6316\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.421.pdf",
        "site": "https://aclanthology.org/2025.coling-main.421/",
        "pdf_size": 1832565,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:f95LSQRP4_IJ:scholar.google.com/&scioq=Towards+Human+Understanding+of+Paraphrase+Types+in+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "University of G\u00f6ttingen, Germany + LKA NRW, Germany; University of G\u00f6ttingen, Germany; University of G\u00f6ttingen, Germany; University of G\u00f6ttingen, Germany",
        "aff_domain": "gipplab.org; ; ; ",
        "email": "gipplab.org; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "University of G\u00f6ttingen;Landeskriminalamt Nordrhein-Westfalen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-goettingen.de;https://www.lka-nrw.de/",
        "aff_unique_abbr": "Georg-August-Universit\u00e4t;LKA NRW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.615",
        "title": "Towards Multilingual spoken Visual Question Answering system using Cross-Attention",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Visual question answering (VQA) poses a multi-modal translation challenge that requires the analysis of both images and questions simultaneously to generate appropriate responses. Although VQA research has mainly focused on text-based questions in English, speech-based questions in English and other languages remain largely unexplored. Incorporating speech could significantly enhance the utility of VQA systems, as speech is the primary mode of human communication. To address this gap, this work implements a speech-based VQA system and introduces the textless multilingual visual question answering (TM-VQA) dataset, featuring speech-based questions in English, German, Spanish, and French. This TM-VQA dataset contains 658,111 pairs of speech-based questions and answers based on 123,287 images. Finally, a novel, cross-attention-based unified multi-modal framework is presented to evaluate the efficacy of the TM-VQA dataset. The experimental results indicate the effectiveness of the proposed unified approach over the cascaded framework for both text and speech-based VQA systems. Dataset can be accessed at https://github.com/Synaptic-Coder/TM-VQA.",
        "author": "Amartya Roy Chowdhury; Tonmoy Rajkhowa; Sanjeev Sharma",
        "authorids": "/a/amartya-roy-chowdhury/; /t/tonmoy-rajkhowa/; /s/sanjeev-sharma/",
        "bibtex": "@inproceedings{chowdhury-etal-2025-towards,\n    title = \"Towards Multilingual spoken Visual Question Answering system using Cross-Attention\",\n    author = \"Chowdhury, Amartya Roy  and\n      Rajkhowa, Tonmoy  and\n      Sharma, Sanjeev\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.615/\",\n    pages = \"9165--9175\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.615.pdf",
        "site": "https://aclanthology.org/2025.coling-main.615/",
        "pdf_size": 1648597,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:tEGpQdh58HMJ:scholar.google.com/&scioq=Towards+Multilingual+spoken+Visual+Question+Answering+system+using+Cross-Attention&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "IIT Dharwad, India; IIT (BHU) Varanasi, India; IIT (BHU) Varanasi, India",
        "aff_domain": "iitdh.ac.in;itbhu.ac.in;iitbhu.ac.in",
        "email": "iitdh.ac.in;itbhu.ac.in;iitbhu.ac.in",
        "github": "https://github.com/Synaptic-Coder/TM-VQA",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Indian Institute of Technology Dharwad;Indian Institute of Technology (BHU) Varanasi",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitdh.ac.in;https://www.iitbhu.ac.in",
        "aff_unique_abbr": "IIT Dharwad;IIT (BHU)",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Varanasi",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.476",
        "title": "Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current rumor detection methods based on propagation structure learning predominately treat rumor detection as a class-balanced classification task on limited labeled data. However, real-world social media data exhibits an imbalanced distribution with a minority of rumors among massive regular posts. To address the data scarcity and imbalance issues, we construct two large-scale conversation datasets from Weibo and Twitter and analyze the domain distributions. We find obvious differences between rumor and non-rumor distributions, with non-rumors mostly in entertainment domains while rumors concentrate in news, indicating the conformity of rumor detection to an anomaly detection paradigm. Correspondingly, we propose the Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats unlabeled data as non-rumors and adapts graph contrastive learning for rumor detection. Extensive experiments demonstrate AD-GSCL\u2019s superiority under class-balanced, imbalanced, and few-shot conditions. Our findings provide valuable insights for real-world rumor detection featuring imbalanced data distributions.",
        "author": "Chaoqun Cui; Caiyan Jia",
        "authorids": "/c/chaoqun-cui/; /c/caiyan-jia/",
        "bibtex": "@inproceedings{cui-jia-2025-towards,\n    title = \"Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning\",\n    author = \"Cui, Chaoqun  and\n      Jia, Caiyan\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.476/\",\n    pages = \"7141--7155\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.476.pdf",
        "site": "https://aclanthology.org/2025.coling-main.476/",
        "pdf_size": 2099752,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7274882633509314579&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology & Beijing Key Lab of Traffic Data Analysis and Mining Beijing Jiaotong University, Beijing 100044, China; School of Computer Science and Technology & Beijing Key Lab of Traffic Data Analysis and Mining Beijing Jiaotong University, Beijing 100044, China",
        "aff_domain": "gmail.com;bjtu.edu.cn",
        "email": "gmail.com;bjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Beijing Jiaotong University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.bjtu.edu.cn",
        "aff_unique_abbr": "BJTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.332",
        "title": "Towards Robust Comparisons of NLP Models: A Case Study",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Comparing the test scores of different NLP models across downstream datasets to determine which model leads to the most accurate results is the ultimate step in any experimental work. Doing so via a single mean score may not accurately quantify the real capabilities of the models. Previous works have proposed diverse statistical tests to improve the comparison of NLP models; however, a key statistical phenomenon remains understudied: variability in test scores. We propose a type of regression analysis which better explains this phenomenon by isolating the effect of both nuisance factors (such as random seeds) and datasets from the effects of the models\u2019 capabilities. We showcase our approach via a case study of some of the most popular biomedical NLP models: after isolating nuisance factors and datasets, our results show that the difference between BioLinkBERT and MSR BiomedBERT is, actually, 7 times smaller than previously reported.",
        "author": "Vicente Ivan Sanchez Carmona; Shanshan Jiang; Bin Dong",
        "authorids": "/v/vicente-ivan-sanchez-carmona/; /s/shanshan-jiang/; /b/bin-dong/",
        "bibtex": "@inproceedings{sanchez-carmona-etal-2025-towards,\n    title = \"Towards Robust Comparisons of {NLP} Models: A Case Study\",\n    author = \"Sanchez Carmona, Vicente Ivan  and\n      Jiang, Shanshan  and\n      Dong, Bin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.332/\",\n    pages = \"4973--4979\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.332.pdf",
        "site": "https://aclanthology.org/2025.coling-main.332/",
        "pdf_size": 312411,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:FfUBOPvJ7qgJ:scholar.google.com/&scioq=Towards+Robust+Comparisons+of+NLP+Models:+A+Case+Study&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Ricoh Software Research Center (Beijing) Co., Ltd; Ricoh Software Research Center (Beijing) Co., Ltd; Ricoh Software Research Center (Beijing) Co., Ltd",
        "aff_domain": "cn.ricoh.com;cn.ricoh.com;cn.ricoh.com",
        "email": "cn.ricoh.com;cn.ricoh.com;cn.ricoh.com",
        "github": "",
        "project": "https://microsoft.github.io/BLURB/",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ricoh Software Research Center",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ricoh.com",
        "aff_unique_abbr": "RSRC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.200",
        "title": "Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While large language models (LLMs) have demonstrated superior multi-task capabilities, understanding the learning mechanisms behind this is still a challenging problem. In this paper, we attempt to understand such mechanisms from the perspective of neurons. Specifically, we detect task-sensitive neurons in LLMs via gradient attribution on task-specific data. Through extensive deactivation and fine-tuning experiments, we demonstrate that the detected neurons are highly correlated with the given task, which we term as task-specific neurons. With these identified task-specific neurons, we delve into two common problems in multi-task learning and continuous learning: Generalization and Catastrophic Forgetting. We find that the overlap of task-specific neurons is strongly associated with generalization and specialization across tasks. Interestingly, at certain layers of LLMs, there is a high similarity in the parameters of different task-specific neurons, and such similarity is highly correlated with the generalization performance. Inspired by these findings, we propose a neuron-level continuous fine-tuning method that only fine-tunes the current task-specific neurons during continuous learning, and extensive experiments demonstrate the effectiveness of the proposed method. Our study provides insights into the interpretability of LLMs in multi-task learning.",
        "author": "Yongqi Leng; Deyi Xiong",
        "authorids": "/y/yongqi-leng/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{leng-xiong-2025-towards,\n    title = \"Towards Understanding Multi-Task Learning (Generalization) of {LLM}s via Detecting and Exploring Task-Specific Neurons\",\n    author = \"Leng, Yongqi  and\n      Xiong, Deyi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.200/\",\n    pages = \"2969--2987\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.200.pdf",
        "site": "https://aclanthology.org/2025.coling-main.200/",
        "pdf_size": 751067,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4318410095645092273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.63",
        "title": "Towards the Machine Translation of Scientific Neologisms",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Scientific research continually discovers and invents new concepts, which are then referred to by new terms, neologisms, or neonyms in this context. As the vast majority of publications are written in English, disseminating this new knowledge to the general public often requires translating these terms. However, by definition, no parallel data exist to provide such translations. Therefore, we propose to leverage term definitions as a useful source of information for the translation process. As we discuss, Large Language Models are well suited for this task and can benefit from in-context learning with co-hyponyms and terms sharing the same derivation paradigm. These models, however, are sensitive to the superficial and morphological similarity between source and target terms. Their predictions are also impacted by subword tokenization, especially for prefixed terms.",
        "author": "Paul Lerner; Fran\u00e7ois Yvon",
        "authorids": "/p/paul-lerner/; /f/francois-yvon/",
        "bibtex": "@inproceedings{lerner-yvon-2025-towards,\n    title = \"Towards the Machine Translation of Scientific Neologisms\",\n    author = \"Lerner, Paul  and\n      Yvon, Fran{\\c{c}}ois\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.63/\",\n    pages = \"947--963\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.63.pdf",
        "site": "https://aclanthology.org/2025.coling-main.63/",
        "pdf_size": 571751,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3798712454244364051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Sorbonne Universit\u00e9, CNRS, ISIR; Sorbonne Universit\u00e9, CNRS, ISIR",
        "aff_domain": "isir.upmc.fr;isir.upmc.fr",
        "email": "isir.upmc.fr;isir.upmc.fr",
        "github": "",
        "project": "https://www.helsinki-initiative.org/",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sorbonne Universit\u00e9",
        "aff_unique_dep": "CNRS, ISIR",
        "aff_unique_url": "https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "Sorbonne U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.32",
        "title": "TransMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Transliterating related languages that use different scripts into a common script is effective for improving crosslingual transfer in downstream tasks. However, this methodology often makes pretraining a model from scratch unavoidable, as transliteration brings about new subwords not covered in existing multilingual pretrained language models (mPLMs). This is undesirable because it requires a large computation budget. A more promising way is to make full use of available mPLMs. To this end, this paper proposes a simple but effective framework: Transliterate-Merge-Initialize (TransMI). TransMI can create strong baselines for data that is transliterated into a common script by exploiting an existing mPLM and its tokenizer without any training. TransMI has three stages: (a) transliterate the vocabulary of an mPLM into a common script; (b) merge the new vocabulary with the original vocabulary; and (c) initialize the embeddings of the new subwords. We apply TransMI to three strong recent mPLMs. Our experiments demonstrate that TransMI not only preserves the mPLM\u2019s ability to handle non-transliterated data, but also enables it to effectively process transliterated data, thereby facilitating crosslingual transfer across scripts. The results show consistent improvements of 3% to 34% for different mPLMs and tasks. We make our code and models publicly available at https://github.com/cisnlp/TransMI.",
        "author": "Yihong Liu; Chunlan Ma; Haotian Ye; Hinrich Sch\u00fctze",
        "authorids": "/y/yihong-liu/; /c/chunlan-ma/; /h/haotian-ye/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{liu-etal-2025-transmi,\n    title = \"{T}rans{MI}: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data\",\n    author = {Liu, Yihong  and\n      Ma, Chunlan  and\n      Ye, Haotian  and\n      Sch{\\\"u}tze, Hinrich},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.32/\",\n    pages = \"469--495\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.32.pdf",
        "site": "https://aclanthology.org/2025.coling-main.32/",
        "pdf_size": 681030,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8062926227994400365&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Center for Information and Language Processing, LMU Munich + Munich Center for Machine Learning (MCML); Center for Information and Language Processing, LMU Munich + Munich Center for Machine Learning (MCML); Center for Information and Language Processing, LMU Munich + Munich Center for Machine Learning (MCML); Center for Information and Language Processing, LMU Munich + Munich Center for Machine Learning (MCML)",
        "aff_domain": "cis.lmu.de;cis.lmu.de;cis.lmu.de; ",
        "email": "cis.lmu.de;cis.lmu.de;cis.lmu.de; ",
        "github": "https://github.com/cisnlp/TransMI",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "LMU Munich;Munich Center for Machine Learning",
        "aff_unique_dep": "Center for Information and Language Processing;Center for Machine Learning",
        "aff_unique_url": "https://www.lmu.de;https://www.munich-center-for-machine-learning.de",
        "aff_unique_abbr": "LMU;MCML",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.528",
        "title": "Transformer-based Speech Model Learns Well as Infants and Encodes Abstractions through Exemplars in the Poverty of the Stimulus Environment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Infants are capable of learning language, predominantly through speech and associations, in impoverished environments\u2014a phenomenon known as the Poverty of the Stimulus (POS). Is this ability uniquely human, as an innate linguistic predisposition, or can it be empirically learned through potential linguistic structures from sparse and noisy exemplars? As an early exploratory work, we systematically designed a series of tasks, scenarios, and metrics to simulate the POS. We found that the emerging speech model wav2vec2.0 with pretrained weights from an English corpus can learn well in noisy and sparse Mandarin environments. We then tested various hypotheses and observed three pieces of evidence for abstraction: label correction, categorical patterns, and clustering effects. We concluded that models can encode hierarchical linguistic abstractions through exemplars in POS environments. We hope this work offers new insights into language acquisition from a speech perspective and inspires further research.",
        "author": "Yi Yang; Yiming Wang; Jiahong Yuan",
        "authorids": "/y/yi-yang/; /y/yiming-wang/; /j/jiahong-yuan/",
        "bibtex": "@inproceedings{yang-etal-2025-transformer,\n    title = \"Transformer-based Speech Model Learns Well as Infants and Encodes Abstractions through Exemplars in the Poverty of the Stimulus Environment\",\n    author = \"Yang, Yi  and\n      Wang, Yiming  and\n      Yuan, Jiahong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.528/\",\n    pages = \"7881--7890\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.528.pdf",
        "site": "https://aclanthology.org/2025.coling-main.528/",
        "pdf_size": 1749989,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZJS7lF-v5QcJ:scholar.google.com/&scioq=Transformer-based+Speech+Model+Learns+Well+as+Infants+and+Encodes+Abstractions+through+Exemplars+in+the+Poverty+of+the+Stimulus+Environment&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.47",
        "title": "Transforming Code Understanding: Clustering-Based Retrieval for Improved Summarization in Domain-Specific Languages",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "A domain-specific extension of C language known as extended Berkeley Packet Filter (eBPF) has gained widespread acceptance for various tasks, including observability, security, and network acceleration in the cloud community. Due to its recency and complexity, there is an overwhelming need for natural language summaries of existing eBPF codes (particularly open-source code) for practitioners and developers, which will go a long way in easing the understanding and development of new code. However, being a niche Domain-Specific Language (DSL), there is a scarcity of available training data. In this paper, we investigate the effectiveness of LLMs for summarizing low-resource DSLs, in the context of eBPF codes. Specifically, we propose a clustering-based technique to retrieve in-context examples that are semantically closer to the test example and propose a very simple yet powerful prompt design that yields superior-quality code summary generation. Experimental results show that our proposed retrieval approach for prompt generation improves the eBPF code summarization accuracy up to 12.9 BLEU points over other prompting techniques. The codes are available at https://github.com/babangain/ebpf_summ.",
        "author": "Baban Gain; Dibyanayan Bandyopadhyay; Samrat Mukherjee; Aryan Sahoo; Saswati Dana; Palanivel Kodeswaran; Sayandeep Sen; Asif Ekbal; Dinesh Garg",
        "authorids": "/b/baban-gain/; /d/dibyanayan-bandyopadhyay/; /s/samrat-mukherjee/; /a/aryan-sahoo/; /s/saswati-dana/; /p/palanivel-kodeswaran/; /s/sayandeep-sen/; /a/asif-ekbal/; /d/dinesh-garg/",
        "bibtex": "@inproceedings{gain-etal-2025-transforming,\n    title = \"Transforming Code Understanding: Clustering-Based Retrieval for Improved Summarization in Domain-Specific Languages\",\n    author = \"Gain, Baban  and\n      Bandyopadhyay, Dibyanayan  and\n      Mukherjee, Samrat  and\n      Sahoo, Aryan  and\n      Dana, Saswati  and\n      Kodeswaran, Palanivel  and\n      Sen, Sayandeep  and\n      Ekbal, Asif  and\n      Garg, Dinesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.47/\",\n    pages = \"546--560\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.47.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.47/",
        "pdf_size": 500059,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VxOZkRXWjr0J:scholar.google.com/&scioq=Transforming+Code+Understanding:+Clustering-Based+Retrieval+for+Improved+Summarization+in+Domain-Specific+Languages&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/babanga/in/ebpf_summ",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2025.coling-main.547",
        "title": "TriFine: A Large-Scale Dataset of Vision-Audio-Subtitle for Tri-Modal Machine Translation and Benchmark with Fine-Grained Annotated Tags",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current video-guided machine translation (VMT) approaches primarily use coarse-grained visual information, resulting in information redundancy, high computational overhead, and neglect of audio content. Our research demonstrates the significance of fine-grained visual and audio information in VMT from both data and methodological perspectives. From the data perspective, we have developed a large-scale dataset TriFine, the first vision-audio-subtitle tri-modal VMT dataset with annotated multimodal fine-grained tags. Each entry in this dataset not only includes the triples found in traditional VMT datasets but also encompasses seven fine-grained annotation tags derived from visual and audio modalities. From the methodological perspective, we propose a Fine-grained Information-enhanced Approach for Translation (FIAT). Experimental results have shown that, in comparison to traditional coarse-grained methods and text-only models, our fine-grained approach achieves superior performance with lower computational overhead. These findings underscore the pivotal role of fine-grained annotated information in advancing the field of VMT.",
        "author": "Boyu Guan; Yining Zhang; Yang Zhao; Chengqing Zong",
        "authorids": "/b/boyu-guan/; /y/yining-zhang/; /y/yang-zhao/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{guan-etal-2025-trifine,\n    title = \"{T}ri{F}ine: A Large-Scale Dataset of Vision-Audio-Subtitle for Tri-Modal Machine Translation and Benchmark with Fine-Grained Annotated Tags\",\n    author = \"Guan, Boyu  and\n      Zhang, Yining  and\n      Zhao, Yang  and\n      Zong, Chengqing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.547/\",\n    pages = \"8215--8231\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.547.pdf",
        "site": "https://aclanthology.org/2025.coling-main.547/",
        "pdf_size": 3160096,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wHFIrGoFFKEJ:scholar.google.com/&scioq=TriFine:+A+Large-Scale+Dataset+of+Vision-Audio-Subtitle+for+Tri-Modal+Machine+Translation+and+Benchmark+with+Fine-Grained+Annotated+Tags&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + Zhongguancun Academy, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ia.ac.cn;ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1+2;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Zhongguancun Academy",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.139",
        "title": "Trucidator: Document-level Event Factuality Identification via Hallucination Enhancement and Cross-Document Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level event factuality identification (DEFI) assesses the veracity degree to which an event mentioned in a document has happened, which is crucial for many natural language processing tasks. Previous work assesses event factuality by solely relying on the semantic information within a single document, which fails to identify hard cases where the document itself is hallucinative or counterfactual. There is also a pressing need for more suitable data of this kind. To tackle these issues, we construct Factualusion, a novel corpus with hallucination features that can be used not only for DEFI but can also be applied for hallucination evaluation for large language models. We further propose Trucidator, a graph-based framework that constructs intra-document and cross-document graphs and employs a multi-task learning paradigm to acquire more robust node embeddings, leveraging cross-document inference for more accurate identification. Experiments show that our proposed framework outperformed several baselines, demonstrating the effectiveness of our method.",
        "author": "Zihao Zhang; Zhong Qian; Xiaoxu Zhu; Peifeng Li; Qiaoming Zhu",
        "authorids": "/z/zihao-zhang/; /z/zhong-qian/; /x/xiaoxu-zhu/; /p/peifeng-li/; /q/qiaoming-zhu/",
        "bibtex": "@inproceedings{zhang-etal-2025-trucidator,\n    title = \"Trucidator: Document-level Event Factuality Identification via Hallucination Enhancement and Cross-Document Inference\",\n    author = \"Zhang, Zihao  and\n      Qian, Zhong  and\n      Zhu, Xiaoxu  and\n      Li, Peifeng  and\n      Zhu, Qiaoming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.139/\",\n    pages = \"2038--2048\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.139.pdf",
        "site": "https://aclanthology.org/2025.coling-main.139/",
        "pdf_size": 743158,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZQtMsvPQrAUJ:scholar.google.com/&scioq=Trucidator:+Document-level+Event+Factuality+Identification+via+Hallucination+Enhancement+and+Cross-Document+Inference&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "icloud.com;suda.edu.cn;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "icloud.com;suda.edu.cn;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.33",
        "title": "Two-stage Incomplete Utterance Rewriting on Editing Operation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused on generating rewritten utterances based solely on dialogue context, ignoring the widespread phenomenon of coreference and ellipsis in dialogues. To address this issue, we propose a novel framework called TEO (Two-stage approach on Editing Operation) for IUR, in which the first stage generates editing operations and the second stage rewrites incomplete utterances utilizing the generated editing operations and the dialogue context. Furthermore, an adversarial perturbation strategy is proposed to mitigate cascading errors and exposure bias caused by the inconsistency between training and inference in the second stage. Experimental results on three IUR datasets show that our TEO outperforms the SOTA models significantly.",
        "author": "Zhiyu Cao; Peifeng Li; Qiaoming Zhu; Yaxin Fan",
        "authorids": "/z/zhiyu-cao/; /p/peifeng-li/; /q/qiaoming-zhu/; /y/yaxin-fan/",
        "bibtex": "@inproceedings{cao-etal-2025-two,\n    title = \"Two-stage Incomplete Utterance Rewriting on Editing Operation\",\n    author = \"Cao, Zhiyu  and\n      Li, Peifeng  and\n      Zhu, Qiaoming  and\n      Fan, Yaxin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.33/\",\n    pages = \"496--507\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.33.pdf",
        "site": "https://aclanthology.org/2025.coling-main.33/",
        "pdf_size": 725556,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:x4iSA8nYqHsJ:scholar.google.com/&scioq=Two-stage+Incomplete+Utterance+Rewriting+on+Editing+Operation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn;stu.suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn;stu.suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.57",
        "title": "UCTG: A Unified Controllable Text Generation Framework for Query Auto-Completion",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In the field of natural language generation (NLG), controlling text generation (CTG) is critical, particularly in query auto-completion (QAC) where the need for personalization and diversity is paramount. However, it is essentially challenging to adapt to various control objectives and constraints, which results in existing CTG approaches meeting with mixed success. This paper presents UCTG, a unified controllable text generation framework, which introduces a novel prompt learning method for CTG. Specifically, this framework seamlessly integrates a control module, a prompt module, and a generation module. The control module leverages a fine-tuned model to distill user preference features and behavioral patterns from historical data, incorporating human feedback into the model\u2019s loss functions. These features are then transformed by the prompt module into vectors that guide the generation module. As such, the text generation can be flexibly controlled without modifying the task settings. By employing this unified approach, UCTG significantly improves query accuracy and coherence in tasks with different objectives and constraints, which is validated by extensive experiments on the Meituan and AOL real-world datasets. UCTG not only improves text generation control in QAC but also sets a new framework for flexible NLG applications.",
        "author": "Zhipeng Li; Shuang Zheng; Jiaping Xiao; Xianneng Li; Lei Wang",
        "authorids": "/z/zhipeng-li/; /s/shuang-zheng/; /j/jiaping-xiao/; /x/xianneng-li/; /l/lei-wang/",
        "bibtex": "@inproceedings{li-etal-2025-uctg,\n    title = \"{UCTG}: A Unified Controllable Text Generation Framework for Query Auto-Completion\",\n    author = \"Li, Zhipeng  and\n      Zheng, Shuang  and\n      Xiao, Jiaping  and\n      Li, Xianneng  and\n      Wang, Lei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.57/\",\n    pages = \"679--688\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.57.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.57/",
        "pdf_size": 561393,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mnJtPCWf1x8J:scholar.google.com/&scioq=UCTG:+A+Unified+Controllable+Text+Generation+Framework+for+Query+Auto-Completion&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Dalian University of Technology; Dalian University of Technology; Nanyang Technological University; Dalian University of Technology; Meituan",
        "aff_domain": "mail.dlut.edu.cn;mail.dlut.edu.cn;e.ntu.edu.sg;dlut.edu.cn;meituan.com",
        "email": "mail.dlut.edu.cn;mail.dlut.edu.cn;e.ntu.edu.sg;dlut.edu.cn;meituan.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Dalian University of Technology;Nanyang Technological University;Meituan",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.dlut.edu.cn/;https://www.ntu.edu.sg;https://www.meituan.com",
        "aff_unique_abbr": "DUT;NTU;Meituan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2025.coling-industry.51",
        "title": "UR2N: Unified Retriever and ReraNker",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "The two-stage retrieval paradigm has gained popularity, where a neural model serves as a re-ranker atop a non-neural first-stage retriever. We argue that this approach, involving two disparate models without interaction, represents a suboptimal choice. To address this, we propose a unified encoder-decoder architecture with a novel training regimen which enables the encoder representation to be used for retrieval and the decoder for re-ranking within a single unified model, facilitating end-to-end retrieval. We incorporate XTR-style retrieval on top of the trained MonoT5 reranker to specifically concentrate on addressing practical constraints to create a lightweight model. Results on the BIER benchmark demonstrate the effectiveness of our unified architecture, featuring a highly optimized index and parameters. It outperforms ColBERT, XTR, and even serves as a superior re-ranker compared to the Mono-T5 reranker. The performance gains of our proposed system in reranking become increasingly evident as model capacity grows, particularly when compared to rerankers operating over traditional first-stage retrievers like BM25. This is encouraging, as it suggests that we can integrate more advanced retrievers to further enhance final reranking performance. In contrast, BM25\u2019s static nature limits its potential for such improvements.",
        "author": "Riyaz Ahmad Bhat; Jaydeep Sen; Rudra Murthy; Vignesh P",
        "authorids": "/r/riyaz-ahmad-bhat/; /j/jaydeep-sen/; /r/rudra-murthy/; /v/vignesh-p/",
        "bibtex": "@inproceedings{bhat-etal-2025-ur2n,\n    title = \"{UR}2{N}: Unified Retriever and {R}era{N}ker\",\n    author = \"Bhat, Riyaz Ahmad  and\n      Sen, Jaydeep  and\n      Murthy, Rudra  and\n      P, Vignesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.51/\",\n    pages = \"595--602\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.51.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.51/",
        "pdf_size": 631636,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dpXkz57dahoJ:scholar.google.com/&scioq=UR2N:+Unified+Retriever+and+ReraNker&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "IBM Research, India; IBM Research, India; IBM Research, India; IBM Research, India",
        "aff_domain": "ibm.com;ibm.com;in.ibm.com;in.ibm.com",
        "email": "ibm.com;ibm.com;in.ibm.com;in.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IBM Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.463",
        "title": "URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "URIEL is a knowledge base offering geographical, phylogenetic, and typological vector representations for 7970 languages. It includes distance measures between these vectors for 4005 languages, which are accessible via the lang2vec tool. Despite being frequently cited, URIEL is limited in terms of linguistic inclusion and overall usability. To tackle these challenges, we introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses these limitations. In addition to expanding typological feature coverage for 2898 languages, URIEL+ improves the user experience with robust, customizable distance calculations to better suit the needs of users. These upgrades also offer competitive performance on downstream tasks and provide distances that better align with linguistic distance studies.",
        "author": "Aditya Khan; Mason Shipton; David Anugraha; Kaiyao Duan; Phuong H. Hoang; Eric Khiu; A. Seza Do\u011fru\u00f6z; En-Shiun Annie Lee",
        "authorids": "/a/aditya-khan/; /m/mason-shipton/; /d/david-anugraha/; /k/kaiyao-duan/; /p/phuong-h-hoang/; /e/eric-khiu/; /a/a-seza-dogruoz/; /e/en-shiun-annie-lee/",
        "bibtex": "@inproceedings{khan-etal-2025-uriel,\n    title = \"{URIEL}+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base\",\n    author = {Khan, Aditya  and\n      Shipton, Mason  and\n      Anugraha, David  and\n      Duan, Kaiyao  and\n      Hoang, Phuong H.  and\n      Khiu, Eric  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Lee, En-Shiun Annie},\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.463/\",\n    pages = \"6937--6952\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.463.pdf",
        "site": "https://aclanthology.org/2025.coling-main.463/",
        "pdf_size": 1733747,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16868567505724843406&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Toronto, Canada; Ontario Tech University, Canada; University of Toronto, Canada; University of Toronto, Canada; University of Toronto, Canada; University of Michigan, USA; LT3, IDLab, Universiteit Gent, Belgium; University of Toronto, Canada+Ontario Tech University, Canada",
        "aff_domain": "cs.toronto.edu;gmail.com;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;umich.edu;ugent.be;ontariotechu.ca",
        "email": "cs.toronto.edu;gmail.com;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;umich.edu;ugent.be;ontariotechu.ca",
        "github": "https://github.com/Masonshipton25/URIELPlus",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;0;0;2;3;0+1",
        "aff_unique_norm": "University of Toronto;Ontario Tech University;University of Michigan;Universiteit Gent",
        "aff_unique_dep": ";;;LT3, IDLab",
        "aff_unique_url": "https://www.utoronto.ca;https://www.ontariotechu.ca;https://www.umich.edu;https://www.ugent.be/en",
        "aff_unique_abbr": "U of T;OntTechU;UM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;2;0+0",
        "aff_country_unique": "Canada;United States;Belgium"
    },
    {
        "id": "2025.coling-main.387",
        "title": "Uchaguzi-2022: A Dataset of Citizen Reports on the 2022 Kenyan Election",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Online reporting platforms have enabled citizens around the world to collectively share their opinions and report in real time on events impacting their local communities. Systematically organizing (e.g., categorizing by attributes) and geotagging large amounts of crowdsourced information is crucial to ensuring that accurate and meaningful insights can be drawn from this data and used by policy makers to bring about positive change. These tasks, however, typically require extensive manual annotation efforts. In this paper we present Uchaguzi-2022, a dataset of 14k categorized and geotagged citizen reports related to the 2022 Kenyan General Election containing mentions of election-related issues such as official misconduct, vote count irregularities, and acts of violence. We use this dataset to investigate whether language models can assist in scalably categorizing and geotagging reports, thus highlighting its potential application in the AI for Social Good space.",
        "author": "Roberto Mondini; Neema Kotonya; Robert L Logan IV; Elizabeth M. Olson; Angela Oduor Lungati; Daniel Odongo; Tim Ombasa; Hemank Lamba; Aoife Cahill; Joel Tetreault; Alejandro Jaimes",
        "authorids": "/r/roberto-mondini/; /n/neema-kotonya/; /r/robert-l-logan-iv/; /e/elizabeth-m-olson/; /a/angela-oduor-lungati/; /d/daniel-odongo/; /t/tim-ombasa/; /h/hemank-lamba/; /a/aoife-cahill/; /j/joel-tetreault/; /a/alejandro-jaimes/",
        "bibtex": "@inproceedings{mondini-etal-2025-uchaguzi,\n    title = \"Uchaguzi-2022: A Dataset of Citizen Reports on the 2022 Kenyan Election\",\n    author = \"Mondini, Roberto  and\n      Kotonya, Neema  and\n      Logan IV, Robert L  and\n      Olson, Elizabeth M.  and\n      Oduor Lungati, Angela  and\n      Odongo, Daniel  and\n      Ombasa, Tim  and\n      Lamba, Hemank  and\n      Cahill, Aoife  and\n      Tetreault, Joel  and\n      Jaimes, Alejandro\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.387/\",\n    pages = \"5807--5825\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.387.pdf",
        "site": "https://aclanthology.org/2025.coling-main.387/",
        "pdf_size": 1368109,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Ba6xxaYreh0J:scholar.google.com/&scioq=Uchaguzi-2022:+A+Dataset+of+Citizen+Reports+on+the+2022+Kenyan+Election&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Dataminr Inc.; Dataminr Inc.; Dataminr Inc.; Dataminr Inc.; Ushahidi; Ushahidi; Ushahidi; Dataminr Inc.; Dataminr Inc.; Dataminr Inc.; Dataminr Inc.",
        "aff_domain": "dataminr.com;dataminr.com;dataminr.com;dataminr.com;ushahidi.com;ushahidi.com;ushahidi.com;dataminr.com;dataminr.com;dataminr.com;dataminr.com",
        "email": "dataminr.com;dataminr.com;dataminr.com;dataminr.com;ushahidi.com;ushahidi.com;ushahidi.com;dataminr.com;dataminr.com;dataminr.com;dataminr.com",
        "github": "https://github.ushahidi.org/uchaguzi-ai/",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;1;1;1;0;0;0;0",
        "aff_unique_norm": "Dataminr;Ushahidi",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dataminr.com;https://www.ushahidi.com",
        "aff_unique_abbr": "Dataminr;Ushahidi",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;1;1;0;0;0;0",
        "aff_country_unique": "United States;Kenya"
    },
    {
        "id": "2025.coling-main.96",
        "title": "Uncertainty Modelling in Under-Represented Languages with Bayesian Deep Gaussian Processes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "NLP models often face challenges with under-represented languages due to a lack of sufficient training data and language complexities. This can result in inaccurate predictions and a failure to capture the inherent uncertainties within these languages. This paper introduces a new method for modelling uncertainty in under-represented languages by employing deep Bayesian Gaussian Processes. We develop a novel framework that integrates prior knowledge and leverages kernel functions. This helps enable the quantification of uncertainty in predictions to overcome the data limitations in under-represented languages. The efficacy of our approach is validated through various experiments, and the results are benchmarked against existing methods to highlight the enhancements in prediction accuracy and measurement of uncertainty.",
        "author": "Ubaid Azam; Imran Razzak; Shelly Vishwakarma; Shoaib Jameel",
        "authorids": "/u/ubaid-azam/; /i/imran-razzak/; /s/shelly-vishwakarma/; /s/shoaib-jameel/",
        "bibtex": "@inproceedings{azam-etal-2025-uncertainty,\n    title = \"Uncertainty Modelling in Under-Represented Languages with {B}ayesian Deep {G}aussian Processes\",\n    author = \"Azam, Ubaid  and\n      Razzak, Imran  and\n      Vishwakarma, Shelly  and\n      Jameel, Shoaib\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.96/\",\n    pages = \"1438--1450\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.96.pdf",
        "site": "https://aclanthology.org/2025.coling-main.96/",
        "pdf_size": 469323,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aOiyNer7QVIJ:scholar.google.com/&scioq=Uncertainty+Modelling+in+Under-Represented+Languages+with+Bayesian+Deep+Gaussian+Processes&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Southampton, Southampton, United Kingdom; Mohamed Bin Zayed University of Arti\ufb01cial Intelligence (MBZUAI), Abu Dhabi, UAE + University of New South Wales, Sydney, Australia; University of Southampton, Southampton, United Kingdom; University of Southampton, Southampton, United Kingdom",
        "aff_domain": "soton.ac.uk;mbzuai.ac.ae;soton.ac.uk;southampton.ac.uk",
        "email": "soton.ac.uk;mbzuai.ac.ae;soton.ac.uk;southampton.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;0;0",
        "aff_unique_norm": "University of Southampton;Mohamed Bin Zayed University of Artificial Intelligence;University of New South Wales",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.southampton.ac.uk;https://www.mbzuai.ac.ae;https://www.unsw.edu.au",
        "aff_unique_abbr": "Southampton;MBZUAI;UNSW",
        "aff_campus_unique_index": "0;1+2;0;0",
        "aff_campus_unique": "Southampton;Abu Dhabi;Sydney",
        "aff_country_unique_index": "0;1+2;0;0",
        "aff_country_unique": "United Kingdom;United Arab Emirates;Australia"
    },
    {
        "id": "2025.coling-main.708",
        "title": "Understanding Token Probability Encoding in Output Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we investigate the output token probability information in the output embedding of language models. We find an approximate common log-linear encoding of output token probabilities within the output embedding vectors and empirically demonstrate that it is accurate and sparse. As a causality examination, we steer the encoding in output embedding to modify the output probability distribution accurately. Moreover, the sparsity we find in output probability encoding suggests that a large number of dimensions in the output embedding do not contribute to causal language modeling. Therefore, we attempt to delete the output-unrelated dimensions and find more than 30% of the dimensions can be deleted without significant movement in output distribution and sequence generation. Additionally, in the pre-training dynamics of language models, we find that the output embeddings capture the corpus token frequency information in early steps, even before an obvious convergence of parameters starts.",
        "author": "Hakaze Cho; Yoshihiro Sakai; Kenshiro Tanaka; Mariko Kato; Naoya Inoue",
        "authorids": "/h/hakaze-cho/; /y/yoshihiro-sakai/; /k/kenshiro-tanaka/; /m/mariko-kato/; /n/naoya-inoue/",
        "bibtex": "@inproceedings{cho-etal-2025-understanding,\n    title = \"Understanding Token Probability Encoding in Output Embeddings\",\n    author = \"Cho, Hakaze  and\n      Sakai, Yoshihiro  and\n      Tanaka, Kenshiro  and\n      Kato, Mariko  and\n      Inoue, Naoya\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.708/\",\n    pages = \"10618--10633\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.708.pdf",
        "site": "https://aclanthology.org/2025.coling-main.708/",
        "pdf_size": 18579856,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15396436479141550724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Japan Advanced Institute of Science and Technology; Japan Advanced Institute of Science and Technology; Japan Advanced Institute of Science and Technology; Japan Advanced Institute of Science and Technology; Japan Advanced Institute of Science and Technology+RIKEN",
        "aff_domain": "jaist.ac.jp; ; ; ; ",
        "email": "jaist.ac.jp; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Japan Advanced Institute of Science and Technology;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jaist.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "JAIST;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.600",
        "title": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most LLMs are built upon rotary position embedding (RoPE), a popular position encoding method. Therefore, a prominent path is to extrapolate the RoPE trained on comparably short texts to far longer texts. A heavy bunch of efforts have been dedicated to boosting the extrapolation via extending the formulations of the RoPE, however, few of them have attempted to showcase their inner workings comprehensively. In this paper, we are driven to offer a straightforward yet in-depth understanding of RoPE extensions from an attention perspective and on two benchmarking tasks. A broad array of experiments reveals several valuable findings: 1) Maintaining attention patterns to those at the pretrained length improves extrapolation; 2) Large attention uncertainty leads to retrieval errors; 3) Using longer continual pretraining lengths for RoPE extensions could reduce attention uncertainty and significantly enhance extrapolation.",
        "author": "Meizhi Zhong; Chen Zhang; Yikun Lei; Xikai Liu; Yan Gao; Yao Hu; Kehai Chen; Min Zhang",
        "authorids": "/m/meizhi-zhong/; /c/chen-zhang/; /y/yikun-lei/; /x/xikai-liu/; /y/yan-gao/; /y/yao-hu/; /k/kehai-chen/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhong-etal-2025-understanding,\n    title = \"Understanding the {R}o{PE} Extensions of Long-Context {LLM}s: An Attention Perspective\",\n    author = \"Zhong, Meizhi  and\n      Zhang, Chen  and\n      Lei, Yikun  and\n      Liu, Xikai  and\n      Gao, Yan  and\n      Hu, Yao  and\n      Chen, Kehai  and\n      Zhang, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.600/\",\n    pages = \"8955--8962\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.600.pdf",
        "site": "https://aclanthology.org/2025.coling-main.600/",
        "pdf_size": 11719359,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1457908276763398688&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China+Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China",
        "aff_domain": "gmail.com;outlook.com;hit.edu.cn;hit.edu.cn;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com",
        "email": "gmail.com;outlook.com;hit.edu.cn;hit.edu.cn;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;1;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Xiaohongshu Inc.",
        "aff_unique_dep": "Institute of Computing and Intelligence;",
        "aff_unique_url": "http://www.hhit.edu.cn;https://www.xiaohongshu.com",
        "aff_unique_abbr": "HIT;Xiaohongshu",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.269",
        "title": "Unified Grid Tagging Scheme for Aspect Sentiment Quad Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect Sentiment Quad Prediction (ASQP) aims to extract all sentiment elements in quads for a given review to explain the reason for the sentiment. Previous table-filling based methods have achieved promising results by modeling word-pair relations. However, these methods decompose the ASQP task into several subtasks without considering the association between sentiment elements. Most importantly, they fail to tackle the situation where a sentence contains multiple implicit expressions. To address these limitations, we propose a simple yet effective Unified Grid Tagging Scheme (UGTS) to extract sentiment quadruplets in one shot, with two additional special tokens from pre-trained models to represent potential implicit aspect and opinion terms. Based on this, we first introduce the adaptive graph diffusion convolution network to construct the direct connection between explicit and implicit sentiment elements from syntactic and semantic views. Next, we utilize conditional layer normalization to refine the mutual indication effect between words for matching valid aspect-opinion pairs. Finally, we employ the triaffine mechanism to integrate heterogeneous word-pair relations to capture higher-order interactions between sentiment elements. Experimental results on four benchmark datasets show the effectiveness and robustness of our model, which achieves state-of-the-art performance.",
        "author": "Guixin Su; Yongcheng Zhang; Tongguan Wang; Mingmin Wu; Ying Sha",
        "authorids": "/g/guixin-su/; /y/yongcheng-zhang/; /t/tongguan-wang/; /m/mingmin-wu/; /y/ying-sha/",
        "bibtex": "@inproceedings{su-etal-2025-unified,\n    title = \"Unified Grid Tagging Scheme for Aspect Sentiment Quad Prediction\",\n    author = \"Su, Guixin  and\n      Zhang, Yongcheng  and\n      Wang, Tongguan  and\n      Wu, Mingmin  and\n      Sha, Ying\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.269/\",\n    pages = \"3997--4010\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.269.pdf",
        "site": "https://aclanthology.org/2025.coling-main.269/",
        "pdf_size": 1564235,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6lnPXttiTTwJ:scholar.google.com/&scioq=Unified+Grid+Tagging+Scheme+for+Aspect+Sentiment+Quad+Prediction&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-demos.5",
        "title": "UnifiedGEC: Integrating Grammatical Error Correction Approaches for Multi-languages with a Unified Framework",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Grammatical Error Correction is an important research direction in NLP field. Although many models of different architectures and datasets across different languages have been developed to support the research, there is a lack of a comprehensive evaluation on these models, and different architectures make it hard for developers to implement these models on their own. To address this limitation, we present UnifiedGEC, the first open-source GEC-oriented toolkit, which consists of several core components and reusable modules. In UnifiedGEC, we integrate 5 widely-used GEC models and compare their performance on 7 datasets in different languages. Additionally, GEC-related modules such as data augmentation, prompt engineering are also deployed in it. Developers are allowed to implement new models, run and evaluate on existing benchmarks through our framework in a simple way. Code, documents and detailed results of UnifiedGEC are available at https://github.com/AnKate/UnifiedGEC.",
        "author": "Yike Zhao; Xiaoman Wang; Yunshi Lan; Weining Qian",
        "authorids": "/y/yike-zhao/; /x/xiaoman-wang/; /y/yunshi-lan/; /w/weining-qian/",
        "bibtex": "@inproceedings{zhao-etal-2025-unifiedgec,\n    title = \"{U}nified{GEC}: Integrating Grammatical Error Correction Approaches for Multi-languages with a Unified Framework\",\n    author = \"Zhao, Yike  and\n      Wang, Xiaoman  and\n      Lan, Yunshi  and\n      Qian, Weining\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.5/\",\n    pages = \"37--45\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.5.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.5/",
        "pdf_size": 431729,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZEGkti4FiskJ:scholar.google.com/&scioq=UnifiedGEC:+Integrating+Grammatical+Error+Correction+Approaches+for+Multi-languages+with+a+Unified+Framework&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Data Science and Engineering, East China Normal University; Shanghai Engineering Research Center of Big Data Management; School of Data Science and Engineering, East China Normal University; Shanghai Engineering Research Center of Big Data Management",
        "aff_domain": "stu.ecnu.edu.cn;stu.ecnu.edu.cn;dase.ecnu.edu.cn;dase.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;stu.ecnu.edu.cn;dase.ecnu.edu.cn;dase.ecnu.edu.cn",
        "github": "https://github.com/AnKate/UnifiedGEC",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "East China Normal University;Shanghai Engineering Research Center of Big Data Management",
        "aff_unique_dep": "School of Data Science and Engineering;Big Data Management",
        "aff_unique_url": "http://www.ecnu.edu.cn;",
        "aff_unique_abbr": "ECNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.209",
        "title": "Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity alignment (EA) aims to match identical entities across different knowledge graphs (KGs). Graph neural network-based entity alignment methods have achieved promising results in Euclidean space. However, KGs often contain complex local and hierarchical structures, which are hard to represent in a single space. In this paper, we propose a novel method named as UniEA, which unifies dual-space embedding to preserve the intrinsic structure of KGs. Specifically, we simultaneously learn graph structure embeddings in both Euclidean and hyperbolic spaces to maximize the consistency between embeddings in the two spaces. Moreover, we employ contrastive learning to mitigate the misalignment issues caused by similar entities, where embeddings of similar neighboring entities become too close. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in structure-based EA. Our code is available at https://github.com/wonderCS1213/UniEA.",
        "author": "Cunda Wang; Weihua Wang; Qiuyu Liang; Feilong Bao; Guanglai Gao",
        "authorids": "/c/cunda-wang/; /w/weihua-wang/; /q/qiuyu-liang/; /f/feilong-bao/; /g/guanglai-gao/",
        "bibtex": "@inproceedings{wang-etal-2025-unifying,\n    title = \"Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning\",\n    author = \"Wang, Cunda  and\n      Wang, Weihua  and\n      Liang, Qiuyu  and\n      Bao, Feilong  and\n      Gao, Guanglai\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.209/\",\n    pages = \"3110--3122\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.209.pdf",
        "site": "https://aclanthology.org/2025.coling-main.209/",
        "pdf_size": 1442730,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6645938478881191545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2025.coling-main.348",
        "title": "Unlike \u201cLikely\u201d, \u201cUnlike\u201d is Unlikely: BPE-based Segmentation hurts Morphological Derivations in LLMs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) rely on subword vocabularies to process and generate text. However, because subwords are marked as initial- or intra-word, we find that LLMs perform poorly at handling some types of affixations, which hinders their ability to generate novel (unobserved) word forms. The largest models trained on enough data can mitigate this tendency because their initial- and intra-word embeddings are aligned; in-context learning also helps when all examples are selected in a consistent way; but only morphological segmentation can achieve a near-perfect accuracy.",
        "author": "Paul Lerner; Fran\u00e7ois Yvon",
        "authorids": "/p/paul-lerner/; /f/francois-yvon/",
        "bibtex": "@inproceedings{lerner-yvon-2025-unlike,\n    title = \"Unlike {\\textquotedblleft}Likely{\\textquotedblright}, {\\textquotedblleft}Unlike{\\textquotedblright} is Unlikely: {BPE}-based Segmentation hurts Morphological Derivations in {LLM}s\",\n    author = \"Lerner, Paul  and\n      Yvon, Fran{\\c{c}}ois\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.348/\",\n    pages = \"5181--5190\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.348.pdf",
        "site": "https://aclanthology.org/2025.coling-main.348/",
        "pdf_size": 391126,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5358014564323537498&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Sorbonne Universit\u00e9, CNRS, ISIR; Sorbonne Universit\u00e9, CNRS, ISIR",
        "aff_domain": "isir.upmc.fr;isir.upmc.fr",
        "email": "isir.upmc.fr;isir.upmc.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sorbonne Universit\u00e9",
        "aff_unique_dep": "CNRS, ISIR",
        "aff_unique_url": "https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "Sorbonne U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2025.coling-main.607",
        "title": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the Detection of Machine-Generated Tweets",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The rapid development of large language models (LLMs) has significantly improved the generation of fluent and convincing text, raising concerns about their potential misuse on social media platforms. We present a comprehensive methodology for creating nine Twitter datasets to examine the generative capabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These datasets encompass four censored and five uncensored model configurations, including 7B and 8B parameter base-instruction models of the three open-source LLMs. Additionally, we perform a data quality analysis to assess the characteristics of textual outputs from human, \u201ccensored,\u201d and \u201cuncensored models,\u201d employing semantic meaning, lexical richness, structural patterns, content characteristics, and detector performance metrics to identify differences and similarities. Our evaluation demonstrates that \u201cuncensored\u201d models significantly undermine the effectiveness of automated detection methods. This study addresses a critical gap by exploring smaller open-source models and the ramifications of \u201cuncensoring,\u201d providing valuable insights into how domain adaptation and content moderation strategies influence both the detectability and structural characteristics of machine-generated text.",
        "author": "Bryan E. Tuck; Rakesh Verma",
        "authorids": "/b/bryan-e-tuck/; /r/rakesh-verma/",
        "bibtex": "@inproceedings{tuck-verma-2025-unmasking,\n    title = \"Unmasking the Imposters: How Censorship and Domain Adaptation Affect the Detection of Machine-Generated Tweets\",\n    author = \"Tuck, Bryan E.  and\n      Verma, Rakesh\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.607/\",\n    pages = \"9044--9061\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.607.pdf",
        "site": "https://aclanthology.org/2025.coling-main.607/",
        "pdf_size": 4799663,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ts7a-ECS4ekJ:scholar.google.com/&scioq=Unmasking+the+Imposters:+How+Censorship+and+Domain+Adaptation+Affect+the+Detection+of+Machine-Generated+Tweets&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Houston; University of Houston",
        "aff_domain": "uh.edu;central.uh.edu",
        "email": "uh.edu;central.uh.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Houston",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uh.edu",
        "aff_unique_abbr": "UH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.560",
        "title": "Unraveling the Mystery: Defending Against Jailbreak Attacks Via Unearthing Real Intention",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As Large Language Models (LLMs) become more advanced, the security risks they pose also increase. Ensuring that LLM behavior aligns with human values, particularly in mitigating jailbreak attacks with elusive and implicit intentions, has become a significant challenge. To address this issue, we propose a jailbreak defense method called Real Intentions Defense (RID), which involves two phases: soft extraction and hard deletion. In the soft extraction phase, LLMs are leveraged to extract unbiased, genuine intentions, while in the hard deletion phase, a greedy gradient-based algorithm is used to remove the least important parts of a sentence, based on the insight that words with smaller gradients have less impact on its meaning. We conduct extensive experiments on Vicuna and Llama2 models using eight state-of-the-art jailbreak attacks and six benchmark datasets. Our results show a significant reduction in both Attack Success Rate (ASR) and Harmful Score of jailbreak attacks, while maintaining overall model performance. Further analysis sheds light on the underlying mechanisms of our approach.",
        "author": "Yanhao Li; Hongshen Chen; Heng Zhang; Zhiwei Ge; Tianhao Li; Sulong Xu; Guibo Luo",
        "authorids": "/y/yanhao-li/; /h/hongshen-chen/; /h/heng-zhang/; /z/zhiwei-ge/; /t/tianhao-li/; /s/sulong-xu/; /g/guibo-luo/",
        "bibtex": "@inproceedings{li-etal-2025-unraveling,\n    title = \"Unraveling the Mystery: Defending Against Jailbreak Attacks Via Unearthing Real Intention\",\n    author = \"Li, Yanhao  and\n      Chen, Hongshen  and\n      Zhang, Heng  and\n      Ge, Zhiwei  and\n      Li, Tianhao  and\n      Xu, Sulong  and\n      Luo, Guibo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.560/\",\n    pages = \"8374--8384\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.560.pdf",
        "site": "https://aclanthology.org/2025.coling-main.560/",
        "pdf_size": 1003009,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9obPCenyQHUJ:scholar.google.com/&scioq=Unraveling+the+Mystery:+Defending+Against+Jailbreak+Attacks+Via+Unearthing+Real+Intention&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.358",
        "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, existing studies have predominantly focused on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a gap in the exploration of removing an entire entity, which is critical in real-world scenarios such as copyright protection. To close this gap, we propose a novel task named Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To investigate this task, we systematically evaluate popular unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of unlearning algorithms, identifying that the knowledge coverage of the forget set and its size play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable than pre-trained entities during unlearning. We hope these findings can inspire future improvements in entity-level unlearning for LLMs.",
        "author": "Weitao Ma; Xiaocheng Feng; Weihong Zhong; Lei Huang; Yangfan Ye; Xiachong Feng; Bing Qin",
        "authorids": "/w/weitao-ma/; /x/xiaocheng-feng/; /w/weihong-zhong/; /l/lei-huang/; /y/yangfan-ye/; /x/xiachong-feng/; /b/bing-qin/",
        "bibtex": "@inproceedings{ma-etal-2025-unveiling,\n    title = \"Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis\",\n    author = \"Ma, Weitao  and\n      Feng, Xiaocheng  and\n      Zhong, Weihong  and\n      Huang, Lei  and\n      Ye, Yangfan  and\n      Feng, Xiachong  and\n      Qin, Bing\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.358/\",\n    pages = \"5345--5363\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.358.pdf",
        "site": "https://aclanthology.org/2025.coling-main.358/",
        "pdf_size": 957183,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3916455708025765441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology + Peng Cheng Laboratory; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; The University of Hong Kong; Harbin Institute of Technology + Peng Cheng Laboratory",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;hku.hk;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;hku.hk;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+1;0;0;0;2;0+1",
        "aff_unique_norm": "Harbin Institute of Technology;Peng Cheng Laboratory;The University of Hong Kong",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.hit.edu.cn/;http://www.pcl.ac.cn;https://www.hku.hk",
        "aff_unique_abbr": "HIT;PCL;HKU",
        "aff_campus_unique_index": "0;0;0;0;0;2;0",
        "aff_campus_unique": "Harbin;;Hong Kong SAR",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.526",
        "title": "Unveiling Fake News with Adversarial Arguments Generated by Multimodal Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the era of social media, the proliferation of fake news has created an urgent need for more effective detection methods, particularly for multimodal content. The task of identifying fake news is highly challenging, as it requires broad background knowledge and understanding across various domains. Existing detection methods primarily rely on neural networks to learn latent feature representations, resulting in black-box classifications with limited real-world understanding. To address these limitations, we propose a novel approach that leverages Multimodal Large Language Models (MLLMs) for fake news detection. Our method introduces adversarial reasoning through debates from opposing perspectives. By harnessing the powerful capabilities of MLLMs in text generation and cross-modal reasoning, we guide these models to engage in multimodal debates, generating adversarial arguments based on contradictory evidence from both sides of the issue. We then utilize these arguments to learn reasonable thinking patterns, enabling better multimodal fusion and fine-tuning. This process effectively positions our model as a debate referee for adversarial inference. Extensive experiments conducted on four fake news detection datasets demonstrate that our proposed method significantly outperforms state-of-the-art approaches.",
        "author": "Xiaofan Zheng; Minnan Luo; Xinghao Wang",
        "authorids": "/x/xiaofan-zheng/; /m/minnan-luo/; /x/xinghao-wang/",
        "bibtex": "@inproceedings{zheng-etal-2025-unveiling,\n    title = \"Unveiling Fake News with Adversarial Arguments Generated by Multimodal Large Language Models\",\n    author = \"Zheng, Xiaofan  and\n      Luo, Minnan  and\n      Wang, Xinghao\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.526/\",\n    pages = \"7862--7869\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.526.pdf",
        "site": "https://aclanthology.org/2025.coling-main.526/",
        "pdf_size": 774414,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4752667925671681424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University",
        "aff_domain": "stu.xjtu.edu.cn;xjtu.edu.cn;stu.xjtu.edu.cn",
        "email": "stu.xjtu.edu.cn;xjtu.edu.cn;stu.xjtu.edu.cn",
        "github": "https://github.com/qingpingwan/AAR",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Xi'an Jiaotong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.xjtu.edu.cn",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.677",
        "title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As large language models (LLMs) advance in their linguistic capacity, understanding how they capture aspects of language competence remains a significant challenge. This study therefore employs psycholinguistic paradigms, which are well-suited for probing deeper cognitive aspects of language processing, to explore neuron-level representations in language model across three tasks: sound-shape association, sound-gender association, and implicit causality. Our findings indicate that while GPT-2-XL struggles with the sound-shape task, it demonstrates human-like abilities in both sound-gender association and implicit causality. Targeted neuron ablation and activation manipulation reveal a crucial relationship: When GPT-2-XL displays a linguistic ability, specific neurons correspond to that competence; conversely, the absence of such an ability indicates a lack of specialized neurons. This study is the first to utilize psycholinguistic experiments to investigate deep language competence at the neuron level, providing a new level of granularity in model interpretability and insights into the internal mechanisms driving language ability in the transformer-based LLM.",
        "author": "Xufeng Duan; Xinyu Zhou; Bei Xiao; Zhenguang Cai",
        "authorids": "/x/xufeng-duan/; /x/xinyu-zhou/; /b/bei-xiao/; /z/zhenguang-cai/",
        "bibtex": "@inproceedings{duan-etal-2025-unveiling,\n    title = \"Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability\",\n    author = \"Duan, Xufeng  and\n      Zhou, Xinyu  and\n      Xiao, Bei  and\n      Cai, Zhenguang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.677/\",\n    pages = \"10148--10157\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.677.pdf",
        "site": "https://aclanthology.org/2025.coling-main.677/",
        "pdf_size": 710321,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8294807135354709528&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Linguistics and Modern Languages, The Chinese University of Hong Kong + Brain and Mind Institute, The Chinese University of Hong Kong; Department of Linguistics and Modern Languages, The Chinese University of Hong Kong; Department of Linguistics and Modern Languages, The Chinese University of Hong Kong; Department of Linguistics and Modern Languages, The Chinese University of Hong Kong + Brain and Mind Institute, The Chinese University of Hong Kong",
        "aff_domain": "link.cuhk.edu.hk; ; ; ",
        "email": "link.cuhk.edu.hk; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0;0;0+0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Linguistics and Modern Languages",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0+0;0;0;0+0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.485",
        "title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM\u2019s ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs in healthcare fairness and the urgent need for specialized research in this area.",
        "author": "Yue Zhou; Barbara Di Eugenio; Lu Cheng",
        "authorids": "/y/yue-zhou/; /b/barbara-di-eugenio/; /l/lu-cheng/",
        "bibtex": "@inproceedings{zhou-etal-2025-unveiling,\n    title = \"Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective\",\n    author = \"Zhou, Yue  and\n      Di Eugenio, Barbara  and\n      Cheng, Lu\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.485/\",\n    pages = \"7266--7278\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.485.pdf",
        "site": "https://aclanthology.org/2025.coling-main.485/",
        "pdf_size": 527870,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12551846879345352229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois Chicago; University of Illinois Chicago; University of Illinois Chicago",
        "aff_domain": "uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.208",
        "title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multimodal large language models (MLLMs) combine visual and textual data for tasks like image captioning and visual question answering. Proper uncertainty calibration is crucial but challenging for reliable use in areas like healthcare and autonomous driving. This paper investigates several MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight differences in uncertainty between text and the impact of the integration of these two types of information in uncertainty. To better understand MLLMs\u2019 miscalibration and their ability to self-assess uncertainty, we developed the IDK (I don\u2019t know) dataset, which is key for evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications.",
        "author": "Zijun Chen; Wenbo Hu; Guande He; Zhijie Deng; ZHeng ZHang; Richang Hong",
        "authorids": "/z/zijun-chen/; /w/wenbo-hu/; /g/guande-he/; /z/zhijie-deng/; /z/zheng-zhang/; /r/richang-hong/",
        "bibtex": "@inproceedings{chen-etal-2025-unveiling,\n    title = \"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models\",\n    author = \"Chen, Zijun  and\n      Hu, Wenbo  and\n      He, Guande  and\n      Deng, Zhijie  and\n      ZHang, ZHeng  and\n      Hong, Richang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.208/\",\n    pages = \"3095--3109\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.208.pdf",
        "site": "https://aclanthology.org/2025.coling-main.208/",
        "pdf_size": 1146522,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9472250681608034314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Hefei University of Technology; Hefei University of Technology + Data Space Research Institute; UT Austin; Shanghai Jiao Tong University; Data Space Research Institute; Hefei University of Technology",
        "aff_domain": "hfut.edu.cn;hfut.edu.cn;utexas.edu;sjtu.edu.cn;hfut.edu.cn;hfut.edu.cn",
        "email": "hfut.edu.cn;hfut.edu.cn;utexas.edu;sjtu.edu.cn;hfut.edu.cn;hfut.edu.cn",
        "github": "https://github.com/hfutml/Calibration-MLLM",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;2;3;1;0",
        "aff_unique_norm": "Hefei University of Technology;Data Space Research Institute;University of Texas at Austin;Shanghai Jiao Tong University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.hfut.edu.cn;;https://www.utexas.edu;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "HUT;;UT Austin;SJTU",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0;2;0;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2025.coling-main.742",
        "title": "User Willingness-aware Sales Talk Dataset",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "User willingness is a crucial element in the sales talk process that affects the achievement of the salesperson\u2019s or sales system\u2019s objectives. Despite the importance of user willingness, to the best of our knowledge, no previous study has addressed the development of automated sales talk dialogue systems that explicitly consider user willingness. A major barrier is the lack of sales talk datasets with reliable user willingness data. Thus, in this study, we developed a user willingness\u2013aware sales talk collection by leveraging the ecological validity concept, which is discussed in the field of human\u2013computer interaction. Our approach focused on three types of user willingness essential in real sales interactions. We created a dialogue environment that closely resembles real-world scenarios to elicit natural user willingness, with participants evaluating their willingness at the utterance level from multiple perspectives. We analyzed the collected data to gain insights into practical user willingness\u2013aware sales talk strategies. In addition, as a practical application of the constructed dataset, we developed and evaluated a sales dialogue system aimed at enhancing the user\u2019s intent to purchase.",
        "author": "Asahi Hentona; Jun Baba; Shiki Sato; Reina Akama",
        "authorids": "/a/asahi-hentona/; /j/jun-baba/; /s/shiki-sato/; /r/reina-akama/",
        "bibtex": "@inproceedings{hentona-etal-2025-user,\n    title = \"User Willingness-aware Sales Talk Dataset\",\n    author = \"Hentona, Asahi  and\n      Baba, Jun  and\n      Sato, Shiki  and\n      Akama, Reina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.742/\",\n    pages = \"11206--11217\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.742.pdf",
        "site": "https://aclanthology.org/2025.coling-main.742/",
        "pdf_size": 1395276,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4rjT2hrthSwJ:scholar.google.com/&scioq=User+Willingness-aware+Sales+Talk+Dataset&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "CyberAgent; CyberAgent; CyberAgent; Tohoku University",
        "aff_domain": "cyberagent.co.jp;cyberagent.co.jp;cyberagent.co.jp;tohoku.ac.jp",
        "email": "cyberagent.co.jp;cyberagent.co.jp;cyberagent.co.jp;tohoku.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "CyberAgent;Tohoku University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cyberagent.co.jp;https://www.tohoku.ac.jp",
        "aff_unique_abbr": "CA;Tohoku U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-main.381",
        "title": "Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While the situation has improved for text-only models, it again seems to be the case currently that multimodal (text and image) models develop faster than ways to evaluate them. In this paper, we bring a recently developed evaluation paradigm from text models to multimodal models, namely evaluation through the goal-oriented game (self) play, complementing reference-based and preference-based evaluation. Specifically, we define games that challenge a model\u2019s capability to represent a situation from visual information and align such representations through dialogue. We find that the largest closed models perform rather well on the games that we define, while even the best open-weight models struggle with them. On further analysis, we find that the exceptional deep captioning capabilities of the largest models drive some of the performance. There is still room to grow for both kinds of models, ensuring the continued relevance of the benchmark.",
        "author": "Sherzod Hakimov; Yerkezhan Abdullayeva; Kushal Koshti; Antonia Schmidt; Yan Weiser; Anne Beyer; David Schlangen",
        "authorids": "/s/sherzod-hakimov/; /y/yerkezhan-abdullayeva/; /k/kushal-koshti/; /a/antonia-schmidt/; /y/yan-weiser/; /a/anne-beyer/; /d/david-schlangen/",
        "bibtex": "@inproceedings{hakimov-etal-2025-using,\n    title = \"Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models\",\n    author = \"Hakimov, Sherzod  and\n      Abdullayeva, Yerkezhan  and\n      Koshti, Kushal  and\n      Schmidt, Antonia  and\n      Weiser, Yan  and\n      Beyer, Anne  and\n      Schlangen, David\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.381/\",\n    pages = \"5686--5718\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.381.pdf",
        "site": "https://aclanthology.org/2025.coling-main.381/",
        "pdf_size": 9025669,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9852760675574069435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany + German Research Center for Artificial Intelligence (DFKI), Berlin, Germany",
        "aff_domain": "uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de",
        "email": "uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de;uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0+1",
        "aff_unique_norm": "University of Potsdam;German Research Center for Artificial Intelligence",
        "aff_unique_dep": "Department of Linguistics;",
        "aff_unique_url": "https://www.uni-potsdam.de;https://www.dFKI.de",
        "aff_unique_abbr": ";DFKI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2025.coling-main.533",
        "title": "VEEF-Multi-LLM: Effective Vocabulary Expansion and Parameter Efficient Finetuning Towards Multilingual Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models(LLMs) have brought significant transformations to various aspects of human life and productivity. However, the heavy reliance on vast amounts of data in developing these models has resulted in a notable disadvantage for low-resource languages, such as Nuosu and others, which lack large datasets. Moreover, many LLMs exhibit significant performance discrepancies between high-and lowresource languages, thereby restricting equitable access to technological advances for all linguistic communities. To address these challenges, this paper propose a low-resource multilingual large language model, termed VEEF-Multi-LLM, constructed through effective vocabulary expansion and parameter-efficient fine-tuning. We introduce a series of innovative methods to address challenges in low-resource languages. First, we adopt Byte-level Byte-Pair Encoding to expand the vocabulary for broader multilingual support. We separate input and output embedding weights to boost performance, and apply RoPE for long-context handling, as well as RMSNorm for efficient training. To generate high-quality supervised fine-tuning (SFT) data, we use self-training and selective translation, and refine the resulting dataset with the assistance of native speakers to ensure cultural and linguistic accuracy. Our model, VEEF-Multi-LLM-8B, is trained on 600 billion tokens across 50 natural and 16 programming languages. Experimental results show that the model excels in multilingual instruction-following tasks, particularly in translation, outperforming competing models in benchmarks such as XCOPA and XStoryCloze. Although it lags slightly behind English-centric models in some tasks (e.g., m-MMLU), it prioritizes safety, reliability, and inclusivity, making it valuable for diverse linguistic communities. We open-source our models on GitHub and Huggingface.",
        "author": "Jiu Sha; Mengxiao Zhu; Chong Feng; Yuming Shang",
        "authorids": "/j/jiu-sha/; /m/mengxiao-zhu/; /c/chong-feng/; /y/yuming-shang/",
        "bibtex": "@inproceedings{sha-etal-2025-veef,\n    title = \"{VEEF}-Multi-{LLM}: Effective Vocabulary Expansion and Parameter Efficient Finetuning Towards Multilingual Large Language Models\",\n    author = \"Sha, Jiu  and\n      Zhu, Mengxiao  and\n      Feng, Chong  and\n      Shang, Yuming\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.533/\",\n    pages = \"7963--7981\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.533.pdf",
        "site": "https://aclanthology.org/2025.coling-main.533/",
        "pdf_size": 1125399,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1SkJ4A6odLQJ:scholar.google.com/&scioq=VEEF-Multi-LLM:+Effective+Vocabulary+Expansion+and+Parameter+Efficient+Finetuning+Towards+Multilingual+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Baidu Inc., Beijing, China; North China University of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "ncut.edu.cn;bupt.edu.cn; ; ",
        "email": "ncut.edu.cn;bupt.edu.cn; ; ",
        "github": "https://github.com/Shajiu/VEEF-Multi-LLM",
        "project": "https://huggingface.co/shajiu/VEEF-Multi-LLM",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Baidu Inc.;North China University of Technology;Beijing Institute of Technology;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.baidu.com;http://www.ncut.edu.cn;http://www.bit.edu.cn/;http://www.bupt.edu.cn/",
        "aff_unique_abbr": "Baidu;NCUT;BIT;BUPT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.411",
        "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose the VLR-Bench, a visual question answering (VQA) benchmark for evaluating vision language models (VLMs) based on retrieval augmented generation (RAG). Unlike existing evaluation datasets for external knowledge-based VQA, the proposed VLR-Bench includes five input passages. This allows testing of the ability to determine which passage is useful for answering a given query, a capability lacking in previous research. In this context, we constructed a dataset of 32,000 automatically generated instruction-following examples, which we denote as VLR-IF. This dataset is specifically designed to enhance the RAG capabilities of VLMs by enabling them to learn how to generate appropriate answers based on input passages. We evaluated the validity of the proposed benchmark and training data and verified its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3 model. The proposed VLR-Bench and VLR-IF datasets are publicly available online.",
        "author": "Hyeonseok Lim; Dongjae Shin; Seohyun Song; Inho Won; Minjun Kim; Junghun Yuk; Haneol Jang; KyungTae Lim",
        "authorids": "/h/hyeonseok-lim/; /d/dongjae-shin/; /s/seohyun-song/; /i/inho-won/; /m/minjun-kim/; /j/junghun-yuk/; /h/haneol-jang/; /k/kyungtae-lim/",
        "bibtex": "@inproceedings{lim-etal-2025-vlr,\n    title = \"{VLR}-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation\",\n    author = \"Lim, Hyeonseok  and\n      Shin, Dongjae  and\n      Song, Seohyun  and\n      Won, Inho  and\n      Kim, Minjun  and\n      Yuk, Junghun  and\n      Jang, Haneol  and\n      Lim, KyungTae\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.411/\",\n    pages = \"6150--6168\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.411.pdf",
        "site": "https://aclanthology.org/2025.coling-main.411/",
        "pdf_size": 5878682,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11354433520458272930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Seoul National University of Science and Technology (SeoulTech); SeoulTech & Teddysum; SeoulTech; SeoulTech & Teddysum; SeoulTech; Hanbat National University; Hanbat National University; SeoulTech",
        "aff_domain": "seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;hanbat.ac.kr;hanbat.ac.kr;seoultech.ac.kr",
        "email": "seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;hanbat.ac.kr;hanbat.ac.kr;seoultech.ac.kr",
        "github": "",
        "project": "https://huggingface.co/datasets/MLP-KTLim/VLR-Bench; https://huggingface.co/datasets/MLP-KTLim/VLR-IF",
        "author_num": 8,
        "aff_unique_index": "0;1;0;1;0;2;2;0",
        "aff_unique_norm": "Seoul National University of Science and Technology;SeoulTech;Hanbat National University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.seoultech.ac.kr;http://www.seoultech.ac.kr;https://www.hanbat.ac.kr",
        "aff_unique_abbr": "SeoulTech;SeoulTech;HNU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2025.coling-main.488",
        "title": "VaeDiff-DocRE: End-to-end Data Augmentation Framework for Document-level Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level Relation Extraction (DocRE) aims to identify relationships between entity pairs within a document. However, most existing methods assume a uniform label distribution, resulting in suboptimal performance on real-world, imbalanced datasets. To tackle this challenge, we propose a novel data augmentation approach using generative models to enhance data from the embedding space. Our method leverages the Variational Autoencoder (VAE) architecture to capture all relation-wise distributions formed by entity pair representations and augment data for underrepresented relations. To better capture the multi-label nature of DocRE, we parameterize the VAE\u2019s latent space with a Diffusion Model. Additionally, we introduce a hierarchical training framework to integrate the proposed VAE-based augmentation module into DocRE systems. Experiments on two benchmark datasets demonstrate that our method outperforms state-of-the-art models, effectively addressing the long-tail distribution problem in DocRE. Our code is released at: https://github.com/khaitran22/VaeDiff-DocRE",
        "author": "Khai Phan Tran; Wen Hua; Xue Li",
        "authorids": "/k/khai-phan-tran/; /w/wen-hua/; /x/xue-li/",
        "bibtex": "@inproceedings{tran-etal-2025-vaediff,\n    title = \"{V}ae{D}iff-{D}oc{RE}: End-to-end Data Augmentation Framework for Document-level Relation Extraction\",\n    author = \"Tran, Khai Phan  and\n      Hua, Wen  and\n      Li, Xue\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.488/\",\n    pages = \"7307--7320\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.488.pdf",
        "site": "https://aclanthology.org/2025.coling-main.488/",
        "pdf_size": 2196460,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4DphCpuK8KgJ:scholar.google.com/&scioq=VaeDiff-DocRE:+End-to-end+Data+Augmentation+Framework+for+Document-level+Relation+Extraction&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "School of Electrical Engineering and Computer Science, The University of Queensland, Australia; Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong SAR, China; School of Electrical Engineering and Computer Science, The University of Queensland, Australia",
        "aff_domain": "uq.edu.au; ; ",
        "email": "uq.edu.au; ; ",
        "github": "https://github.com/khaitran22/VaeDiff-DocRE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "The University of Queensland;The Hong Kong Polytechnic University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science;Department of Data Science and Artificial Intelligence",
        "aff_unique_url": "https://www.uq.edu.au;https://www.polyu.edu.hk",
        "aff_unique_abbr": "UQ;PolyU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Australia;China"
    },
    {
        "id": "2025.coling-main.366",
        "title": "VeritasQA: A Truthfulness Benchmark Aimed at Multilingual Transferability",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As Large Language Models (LLMs) become available in a wider range of domains and applications, evaluating the truthfulness of multilingual LLMs is an issue of increasing relevance. TruthfulQA (Lin et al., 2022) is one of few benchmarks designed to evaluate how models imitate widespread falsehoods. However, it is strongly English-centric and starting to become outdated. We present VeritasQA, a context- and time-independent truthfulness benchmark built with multilingual transferability in mind, and available in Spanish, Catalan, Galician and English. VeritasQA comprises a set of 353 questions and answers inspired by common misconceptions and falsehoods that are not tied to any particular country or recent event. We release VeritasQA under an open license and present the evaluation results of 15 models of various architectures and sizes.",
        "author": "Javier Aula-Blasco; J\u00falia Falc\u00e3o; Susana Sotelo; Silvia Paniagua; Aitor Gonzalez-Agirre; Marta Villegas",
        "authorids": "/j/javier-aula-blasco/; /j/julia-falcao/; /s/susana-sotelo/; /s/silvia-paniagua/; /a/aitor-gonzalez-agirre/; /m/marta-villegas/",
        "bibtex": "@inproceedings{aula-blasco-etal-2025-veritasqa,\n    title = \"{V}eritas{QA}: A Truthfulness Benchmark Aimed at Multilingual Transferability\",\n    author = \"Aula-Blasco, Javier  and\n      Falc{\\~a}o, J{\\'u}lia  and\n      Sotelo, Susana  and\n      Paniagua, Silvia  and\n      Gonzalez-Agirre, Aitor  and\n      Villegas, Marta\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.366/\",\n    pages = \"5463--5474\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.366.pdf",
        "site": "https://aclanthology.org/2025.coling-main.366/",
        "pdf_size": 316047,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8276901679309863339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Barcelona Supercomputing Center (BSC-CNS); Barcelona Supercomputing Center (BSC-CNS); Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes (CiTIUS-USC); Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes (CiTIUS-USC); Barcelona Supercomputing Center (BSC-CNS); Barcelona Supercomputing Center (BSC-CNS)",
        "aff_domain": "bsc.es;bsc.es;usc.gal;usc.gal;bsc.es;bsc.es",
        "email": "bsc.es;bsc.es;usc.gal;usc.gal;bsc.es;bsc.es",
        "github": "",
        "project": "hf.co/datasets/projecte-aina/veritasQA",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Barcelona Supercomputing Center;Universidade de Santiago de Compostela",
        "aff_unique_dep": "Supercomputing Center;Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes",
        "aff_unique_url": "https://www.bsc.es;https://www.usc.es",
        "aff_unique_abbr": "BSC;USC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Barcelona;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2025.coling-demos.18",
        "title": "ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical Normalization",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "ViSoLex is an open-source system designed to address the unique challenges of lexical normalization for Vietnamese social media text. The platform provides two core services: Non-Standard Word (NSW) Lookup and Lexical Normalization, enabling users to retrieve standard forms of informal language and standardize text containing NSWs. ViSoLex\u2019s architecture integrates pre-trained language models and weakly supervised learning techniques to ensure accurate and efficient normalization, overcoming the scarcity of labeled data in Vietnamese. This paper details the system\u2019s design, functionality, and its applications for researchers and non-technical users. Additionally, ViSoLex offers a flexible, customizable framework that can be adapted to various datasets and research requirements. By publishing the source code, ViSoLex aims to contribute to the development of more robust Vietnamese natural language processing tools and encourage further research in lexical normalization. Future directions include expanding the system\u2019s capabilities for additional languages and improving the handling of more complex non-standard linguistic patterns.",
        "author": "Anh Thi-Hoang Nguyen; Dung Ha Nguyen; Kiet Van Nguyen",
        "authorids": "/a/anh-thi-hoang-nguyen/; /d/dung-ha-nguyen/; /k/kiet-van-nguyen/",
        "bibtex": "@inproceedings{nguyen-etal-2025-visolex,\n    title = \"{V}i{S}o{L}ex: An Open-Source Repository for {V}ietnamese Social Media Lexical Normalization\",\n    author = \"Nguyen, Anh Thi-Hoang  and\n      Nguyen, Dung Ha  and\n      Nguyen, Kiet Van\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Mather, Brodie  and\n      Dras, Mark\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-demos.18/\",\n    pages = \"183--188\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-demos.18.pdf",
        "site": "https://aclanthology.org/2025.coling-demos.18/",
        "pdf_size": 712924,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14595283501942027690&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 2,
        "aff": "University of Information Technology, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; University of Information Technology, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; University of Information Technology, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam",
        "aff_domain": "gm.uit.edu.vn;uit.edu.vn;uit.edu.vn",
        "email": "gm.uit.edu.vn;uit.edu.vn;uit.edu.vn",
        "github": "https://github.com/HaDung2002/visolex",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of Information Technology;Vietnam National University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.vnu.edu.vn",
        "aff_unique_abbr": ";VNU",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Ho Chi Minh City",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2025.coling-main.483",
        "title": "VideoQA-TA: Temporal-Aware Multi-Modal Video Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Video question answering (VideoQA) has recently gained considerable attention in the field of computer vision, aiming to generate answers rely on both linguistic and visual reasoning. However, existing methods often align visual or textual features directly with large language models, which limits the deep semantic association between modalities and hinders a comprehensive understanding of the interactions within spatial and temporal contexts, ultimately leading to sub-optimal reasoning performance. To address this issue, we propose a novel temporal-aware framework for multi-modal video question answering, dubbed VideoQA-TA, which enhances reasoning ability and accuracy of VideoQA by aligning videos and questions at fine-grained levels. Specifically, an effective Spatial-Temporal Attention mechanism (STA) is designed for video aggregation, transforming video features into spatial and temporal representations while attending to information at different levels. Furthermore, a Temporal Object Injection strategy (TOI) is proposed to align object-level and frame-level information within videos, which further improves the accuracy by injecting explicit temporal information. Experimental results on MSVD-QA, MSRVTT-QA, and ActivityNet-QA datasets demonstrate the superior performance of our proposed method compared with the current SOTAs, meanwhile, visualization analysis further verifies the effectiveness of incorporating temporal information to videos.",
        "author": "Zhixuan Wu; Bo Cheng; Jiale Han; Jiabao Ma; Shuhao Zhang; Yuli Chen; Changbo Li",
        "authorids": "/z/zhixuan-wu/; /b/bo-cheng/; /j/jiale-han/; /j/jiabao-ma/; /s/shuhao-zhang/; /y/yuli-chen/; /c/changbo-li/",
        "bibtex": "@inproceedings{wu-etal-2025-videoqa,\n    title = \"{V}ideo{QA}-{TA}: Temporal-Aware Multi-Modal Video Question Answering\",\n    author = \"Wu, Zhixuan  and\n      Cheng, Bo  and\n      Han, Jiale  and\n      Ma, Jiabao  and\n      Zhang, Shuhao  and\n      Chen, Yuli  and\n      Li, Changbo\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.483/\",\n    pages = \"7239--7252\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.483.pdf",
        "site": "https://aclanthology.org/2025.coling-main.483/",
        "pdf_size": 5547175,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:QkdzuPgkba4J:scholar.google.com/&scioq=VideoQA-TA:+Temporal-Aware+Multi-Modal+Video+Question+Answering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; Hong Kong University of Science and Technology; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; North China Institute of Computing Technology",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;ust.hk;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;163.com",
        "email": "bupt.edu.cn;bupt.edu.cn;ust.hk;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;163.com",
        "github": "https://github.com/YALYAshley/VideoQA-TA",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;2",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Hong Kong University of Science and Technology;North China Institute of Computing Technology",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology;;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.ust.hk;",
        "aff_unique_abbr": "BUPT;HKUST;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.694",
        "title": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV has a speed advantage of 3.98 times and can save 54% of GPU memory when reaching an inference length of 24K tokens. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at the following GitHub repository: https://github.com/howard-hou/VisualRWKV.",
        "author": "Haowen Hou; Peigen Zeng; Fei Ma; Fei Richard Yu",
        "authorids": "/h/haowen-hou/; /p/peigen-zeng/; /f/fei-ma/; /f/fei-richard-yu/",
        "bibtex": "@inproceedings{hou-etal-2025-visualrwkv,\n    title = \"{V}isual{RWKV}: Exploring Recurrent Neural Networks for Visual Language Models\",\n    author = \"Hou, Haowen  and\n      Zeng, Peigen  and\n      Ma, Fei  and\n      Yu, Fei Richard\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.694/\",\n    pages = \"10423--10434\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.694.pdf",
        "site": "https://aclanthology.org/2025.coling-main.694/",
        "pdf_size": 2040251,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17133578573969097392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China+College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China+Shool of Information Technology, Carleton University, Canada; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China; Shool of Information Technology, Carleton University, Canada",
        "aff_domain": "gml.ac.cn;gml.ac.cn;gml.ac.cn;gml.ac.cn",
        "email": "gml.ac.cn;gml.ac.cn;gml.ac.cn;gml.ac.cn",
        "github": "https://github.com/howard-hou/VisualRWKV",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1+2;0;2",
        "aff_unique_norm": "Guangdong Laboratory of Artificial Intelligence and Digital Economy;Shenzhen University;Carleton University",
        "aff_unique_dep": ";College of Computer Science and Software Engineering;School of Information Technology",
        "aff_unique_url": ";https://www.szu.edu.cn;https://carleton.ca",
        "aff_unique_abbr": "GD-LAB;SZU;",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0+1;0;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2025.coling-main.643",
        "title": "Voice synthesis in Polish and English - analyzing prediction differences in speaker verification systems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Deep learning has significantly enhanced voice synthesis, yielding realistic audio capable of mimicking individual voices. This progress, however, raises security concerns due to the potential misuse of audio deepfakes. Our research examines the effects of deepfakes on speaker recognition systems across English and Polish corpora, assessing both Text-to-Speech and Voice Conversion methods. We focus on the biometric similarity\u2019s role in the effectiveness of impersonations and find that synthetic voices can maintain personal traits, posing risks of unauthorized access. The study\u2019s key contributions include analyzing voice synthesis across languages, evaluating biometric resemblance in voice conversion, and contrasting Text-to-Speech and Voice Conversion paradigms. These insights emphasize the need for improved biometric security against audio deepfake threats.",
        "author": "Joanna Gajewska; Alicja Martinek; Micha\u0142 J. O\u0142owski; Ewelina Bartuzi-Trokielewicz",
        "authorids": "/j/joanna-gajewska/; /a/alicja-martinek/; /m/michal-j-olowski/; /e/ewelina-bartuzi-trokielewicz/",
        "bibtex": "@inproceedings{gajewska-etal-2025-voice,\n    title = \"Voice synthesis in {P}olish and {E}nglish - analyzing prediction differences in speaker verification systems\",\n    author = \"Gajewska, Joanna  and\n      Martinek, Alicja  and\n      O{\\l}owski, Micha{\\l} J.  and\n      Bartuzi-Trokielewicz, Ewelina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.643/\",\n    pages = \"9618--9629\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.643.pdf",
        "site": "https://aclanthology.org/2025.coling-main.643/",
        "pdf_size": 971595,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Lx-z7EWL0HwJ:scholar.google.com/&scioq=Voice+synthesis+in+Polish+and+English+-+analyzing+prediction+differences+in+speaker+verification+systems&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "NASK National Research Institute; NASK National Research Institute + AGH University of Krak\u00f3w; NASK National Research Institute; NASK National Research Institute",
        "aff_domain": "nask.pl;nask.pl;nask.pl;nask.pl",
        "email": "nask.pl;nask.pl;nask.pl;nask.pl",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "NASK National Research Institute;AGH University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nask.pl;https://www.agh.edu.pl",
        "aff_unique_abbr": "NASK;AGH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Krak\u00f3w",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "Poland"
    },
    {
        "id": "2025.coling-main.685",
        "title": "VoxpopuliTTS: a large-scale multilingual TTS corpus for zero-shot speech generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, speech generation fields have achieved significant advancements, primarily due to improvements in large TTS (text-to-speech) systems and scalable TTS datasets. However, there is still a lack of large-scale multilingual TTS datasets, which limits the development of cross-language and multilingual TTS systems. Hence, we refine Voxpopuli dataset and propose VoxpopuliTTS dataset. This dataset comprises 30,000 hours of high-quality speech data, across 3 languages with multiple speakers and styles, suitable for various speech tasks such as TTS and ASR. To enhance the quality of speech data from Voxpopuli, we improve the existing processing pipeline by: 1) filtering out low-quality speech-text pairs based on ASR confidence scores, and 2) concatenating short transcripts by checking semantic information completeness to generate the long transcript. Experimental results demonstrate the effectiveness of the VoxpopuliTTS dataset and the proposed processing pipeline.",
        "author": "Wenrui Liu; Jionghao Bai; Xize Cheng; Jialong Zuo; Ziyue Jiang; Shengpeng Ji; Minghui Fang; Xiaoda Yang; Qian Yang; Zhou Zhao",
        "authorids": "/w/wenrui-liu/; /j/jionghao-bai/; /x/xize-cheng/; /j/jialong-zuo/; /z/ziyue-jiang/; /s/shengpeng-ji/; /m/minghui-fang/; /x/xiaoda-yang/; /q/qian-yang/; /z/zhou-zhao/",
        "bibtex": "@inproceedings{liu-etal-2025-voxpopulitts,\n    title = \"{V}oxpopuli{TTS}: a large-scale multilingual {TTS} corpus for zero-shot speech generation\",\n    author = \"Liu, Wenrui  and\n      Bai, Jionghao  and\n      Cheng, Xize  and\n      Zuo, Jialong  and\n      Jiang, Ziyue  and\n      Ji, Shengpeng  and\n      Fang, Minghui  and\n      Yang, Xiaoda  and\n      Yang, Qian  and\n      Zhao, Zhou\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.685/\",\n    pages = \"10293--10297\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.685.pdf",
        "site": "https://aclanthology.org/2025.coling-main.685/",
        "pdf_size": 598024,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MfzQQE8xwFUJ:scholar.google.com/&scioq=VoxpopuliTTS:+a+large-scale+multilingual+TTS+corpus+for+zero-shot+speech+generation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn; ; ; ; ; ; ; ; ",
        "email": "zju.edu.cn;zju.edu.cn; ; ; ; ; ; ; ; ",
        "github": "https://github.com/voxpopulitts/voxpopulitts.github.io",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.397",
        "title": "WER We Stand: Benchmarking Urdu ASR Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a comprehensive evaluation of Urdu Automatic Speech Recognition (ASR) models. We analyze the performance of three ASR model families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along with a detailed examination of the most frequent wrong words and error types including insertions, deletions, and substitutions. Our analysis is conducted using two types of datasets, read speech and conversational speech. Notably, we present the first conversational speech dataset designed for benchmarking Urdu ASR models. We find that seamless-large outperforms other ASR models on the read speech dataset, while whisper-large performs best on the conversational speech dataset. Furthermore, this evaluation highlights the complexities of assessing ASR models for low-resource languages like Urdu using quantitative metrics alone and emphasizes the need for a robust Urdu text normalization system. Our findings contribute valuable insights for developing robust ASR systems for low-resource languages like Urdu.",
        "author": "Samee Arif; Aamina Jamal Khan; Mustafa Abbas; Agha Ali Raza; Awais Athar",
        "authorids": "/s/samee-arif/; /a/aamina-jamal-khan/; /m/mustafa-abbas/; /a/agha-ali-raza/; /a/awais-athar/",
        "bibtex": "@inproceedings{arif-etal-2025-wer,\n    title = \"{WER} We Stand: Benchmarking {U}rdu {ASR} Models\",\n    author = \"Arif, Samee  and\n      Khan, Aamina Jamal  and\n      Abbas, Mustafa  and\n      Raza, Agha Ali  and\n      Athar, Awais\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.397/\",\n    pages = \"5952--5961\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.397.pdf",
        "site": "https://aclanthology.org/2025.coling-main.397/",
        "pdf_size": 1843034,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14854722847448527788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Lahore University of Management Sciences; Lahore University of Management Sciences; Lahore University of Management Sciences; Lahore University of Management Sciences; EMBL European Bioinformatics Institute",
        "aff_domain": "lums.edu.pk;lums.edu.pk;lums.edu.pk;lums.edu.pk;ebi.ac.uk",
        "email": "lums.edu.pk;lums.edu.pk;lums.edu.pk;lums.edu.pk;ebi.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Lahore University of Management Sciences;EMBL European Bioinformatics Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://lums.edu.pk;https://www.ebi.ac.uk",
        "aff_unique_abbr": "LUMS;EMBL-EBI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Pakistan;Unknown"
    },
    {
        "id": "2025.coling-main.349",
        "title": "WIKIGENBENCH:Exploring Full-length Wikipedia Generation under Real-World Scenario",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "It presents significant challenges to generate comprehensive and accurate Wikipedia articles for newly emerging events under real-world scenario. Existing attempts fall short either by focusing only on short snippets or by using metrics that are insufficient to evaluate real-world scenarios. In this paper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries, designed to align with real-world scenarios in both generation and evaluation. For generation, we explore a real-world scenario where structured, full-length Wikipedia articles with citations are generated for new events using input documents from web sources. For evaluation, we integrate systematic metrics and LLM-based metrics to assess the verifiability, organization, and other aspects aligned with real-world scenarios. Based on this benchmark, we conduct extensive experiments using various models within three commonly used frameworks: direct RAG, hierarchical structure-based RAG, and RAG with fine-tuned generation model. Experimental results show that hierarchical-based methods can generate more comprehensive content, while fine-tuned methods achieve better verifiability. However, even the best methods still show a significant gap compared to existing Wikipedia content, indicating that further research is necessary.",
        "author": "Jiebin Zhang; Eugene J. Yu; Qinyu Chen; Chenhao Xiong; Dawei Zhu; Han Qian; Mingbo Song; Weimin Xiong; Xiaoguang Li; Qun Liu; Sujian Li",
        "authorids": "/j/jiebin-zhang/; /e/eugene-j-yu/; /q/qinyu-chen/; /c/chenhao-xiong/; /d/dawei-zhu/; /h/han-qian/; /m/mingbo-song/; /w/weimin-xiong/; /x/xiaoguang-li/; /q/qun-liu/; /s/sujian-li/",
        "bibtex": "@inproceedings{zhang-etal-2025-wikigenbench,\n    title = \"{WIKIGENBENCH}:Exploring Full-length {W}ikipedia Generation under Real-World Scenario\",\n    author = \"Zhang, Jiebin  and\n      Yu, Eugene J.  and\n      Chen, Qinyu  and\n      Xiong, Chenhao  and\n      Zhu, Dawei  and\n      Qian, Han  and\n      Song, Mingbo  and\n      Xiong, Weimin  and\n      Li, Xiaoguang  and\n      Liu, Qun  and\n      Li, Sujian\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.349/\",\n    pages = \"5191--5210\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.349.pdf",
        "site": "https://aclanthology.org/2025.coling-main.349/",
        "pdf_size": 877497,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7308301173284182756&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University; Huawei Technologies; Huawei Technologies; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com;huawei.com;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com;huawei.com;pku.edu.cn",
        "github": "https://github.com/zhzihao/WikiGenBench",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;1;1;0",
        "aff_unique_norm": "Peking University;Huawei Technologies",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "PKU;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.342",
        "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cryptic crosswords are puzzles that rely on general knowledge and the solver\u2019s ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is challenging even for modern NLP models, including Large Language Models (LLMs). However, there is little to no research on the reasons for their poor performance on this task. In this paper, we establish the benchmark results for three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance on this task is still significantly below that of humans. We also investigate why these models struggle to achieve superior performance. We release our code and introduced datasets at https://github.com/bodasadallah/decrypting-crosswords.",
        "author": "Abdelrahman Sadallah; Daria Kotova; Ekaterina Kochmar",
        "authorids": "/a/abdelrahman-sadallah/; /d/daria-kotova/; /e/ekaterina-kochmar/",
        "bibtex": "@inproceedings{sadallah-etal-2025-makes,\n    title = \"What Makes Cryptic Crosswords Challenging for {LLM}s?\",\n    author = \"Sadallah, Abdelrahman  and\n      Kotova, Daria  and\n      Kochmar, Ekaterina\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.342/\",\n    pages = \"5102--5114\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.342.pdf",
        "site": "https://aclanthology.org/2025.coling-main.342/",
        "pdf_size": 527565,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Dz4EzJwMaXsJ:scholar.google.com/&scioq=What+Makes+Cryptic+Crosswords+Challenging+for+LLMs%3F&hl=en&as_sdt=0,14",
        "gs_version_total": 3,
        "aff": "Department of Natural Language Processing, MBZUAI; Department of Natural Language Processing, MBZUAI; Department of Natural Language Processing, MBZUAI",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "https://github.com/bodasadallah/decrypting-crosswords",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "MBZUAI",
        "aff_unique_dep": "Department of Natural Language Processing",
        "aff_unique_url": "https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Arab Emirates"
    },
    {
        "id": "2025.coling-main.546",
        "title": "What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Visual instruction tuning is crucial for enhancing the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). In this paper, we aim to investigate a fundamental question: \u201cwhat makes for good visual instructions\u201d. Through a comprehensive empirical study, we find that instructions focusing on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs, with results correlating to instruction complexity. Based on this insight, we develop a systematic approach to automatically create high-quality complex visual reasoning instructions. Our approach employs a synthesize-complicate-reformulate paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. Based on this approach, we create the ComVint dataset with 32K examples, and fine-tune four MLLMs on it. Experimental results consistently demonstrate the enhanced performance of all compared MLLMs, such as a 27.86% and 27.60% improvement for LLaVA on MME-Perception and MME-Cognition, respectively. Our code and data are publicly available at the link: https://github.com/RUCAIBox/ComVint.",
        "author": "Yifan Du; Hangyu Guo; Kun Zhou; Wayne Xin Zhao; Jinpeng Wang; Chuyuan Wang; Mingchen Cai; Ruihua Song; Ji-Rong Wen",
        "authorids": "/y/yifan-du/; /h/hangyu-guo/; /k/kun-zhou/; /w/wayne-xin-zhao/; /j/jinpeng-wang/; /c/chuyuan-wang/; /m/mingchen-cai/; /r/ruihua-song/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{du-etal-2025-makes,\n    title = \"What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning\",\n    author = \"Du, Yifan  and\n      Guo, Hangyu  and\n      Zhou, Kun  and\n      Zhao, Wayne Xin  and\n      Wang, Jinpeng  and\n      Wang, Chuyuan  and\n      Cai, Mingchen  and\n      Song, Ruihua  and\n      Wen, Ji-Rong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.546/\",\n    pages = \"8197--8214\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.546.pdf",
        "site": "https://aclanthology.org/2025.coling-main.546/",
        "pdf_size": 1327380,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2864297681303299536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; School of Information, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Meituan Group; Meituan Group; Meituan Group; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;163.com; ; ; ; ;",
        "email": "gmail.com;gmail.com;gmail.com;163.com; ; ; ; ;",
        "github": "https://github.com/RUCAIBox/ComVint",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;1;1;1;0;0",
        "aff_unique_norm": "Renmin University of China;Meituan Group",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.meituan.com",
        "aff_unique_abbr": "RUC;Meituan",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.143",
        "title": "What\u2019s Wrong? Refining Meeting Summaries with LLM Feedback",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Meeting summarization has become a critical task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context understanding compared to traditional methods. However, they still struggle to maintain relevance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence. This post-hoc refinement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar complex text generation tasks requiring robustness, action planning, and discussion towards a goal.",
        "author": "Frederic Thomas Kirstein; Terry Lima Ruas; Bela Gipp",
        "authorids": "/f/frederic-thomas-kirstein/; /t/terry-lima-ruas/; /b/bela-gipp/",
        "bibtex": "@inproceedings{kirstein-etal-2025-whats,\n    title = \"What{'}s Wrong? Refining Meeting Summaries with {LLM} Feedback\",\n    author = \"Kirstein, Frederic Thomas  and\n      Lima Ruas, Terry  and\n      Gipp, Bela\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.143/\",\n    pages = \"2100--2120\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.143.pdf",
        "site": "https://aclanthology.org/2025.coling-main.143/",
        "pdf_size": 632725,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=288707691864142947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Georg-August-Universit\u00e4t G\u00f6ttingen, Germany+2; Georg-August-Universit\u00e4t G\u00f6ttingen, Germany; Georg-August-Universit\u00e4t G\u00f6ttingen, Germany",
        "aff_domain": "gipplab.org; ; ",
        "email": "gipplab.org; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georg-August-Universit\u00e4t G\u00f6ttingen;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-goettingen.de;",
        "aff_unique_abbr": "GAU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "2025.coling-main.317",
        "title": "What\u2019s the most important value? INVP: INvestigating the Value Priorities of LLMs through Decision-making in Social Scenarios",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As large language models (LLMs) demonstrate impressive performance in various tasks and are increasingly integrated into the decision-making process, ensuring they align with human values has become crucial. This paper highlights that value priorities\u2014the relative importance of different value\u2014play a pivotal role in the decision-making process. To explore the value priorities in LLMs, this paper introduces INVP, a framework for INvestigating Value Priorities through decision-making in social scenarios. The framework encompasses social scenarios including binary decision-making, covering both individual and collective decision-making contexts, and is based on Schwartz\u2019s value theory for constructing value priorities. Using this framework, we construct a dataset, which contains a total of 1613 scenarios and 3226 decisions across 283 topics. We evaluate seven popular LLMs and the experimental results reveal commonalities in the value priorities across different LLMs, such as an emphasis on Universalism and Benevolence, while Power and Hedonism are typically given lower priority. This study provides fresh insights into understanding and enhancing the moral and value alignment of LLMs when making complex social decisions.",
        "author": "Xuelin Liu; Pengyuan Liu; Dong Yu",
        "authorids": "/x/xuelin-liu/; /p/pengyuan-liu/; /d/dong-yu/",
        "bibtex": "@inproceedings{liu-etal-2025-whats,\n    title = \"What{'}s the most important value? {INVP}: {IN}vestigating the Value Priorities of {LLM}s through Decision-making in Social Scenarios\",\n    author = \"Liu, Xuelin  and\n      Liu, Pengyuan  and\n      Yu, Dong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.317/\",\n    pages = \"4725--4752\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.317.pdf",
        "site": "https://aclanthology.org/2025.coling-main.317/",
        "pdf_size": 8821820,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-1Hvf9AcwhoJ:scholar.google.com/&scioq=What%E2%80%99s+the+most+important+value%3F+INVP:+INvestigating+the+Value+Priorities+of+LLMs+through+Decision-making+in+Social+Scenarios&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China + National Print Media Language Resources Monitoring & Research Center, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China",
        "aff_domain": "stu.blcu.edu.cn;pku.edu.cn;126.com",
        "email": "stu.blcu.edu.cn;pku.edu.cn;126.com",
        "github": "https://github.com/MuMu-Lily/INVP",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+0;0",
        "aff_unique_norm": "Beijing Language and Culture University",
        "aff_unique_dep": "School of Information Science",
        "aff_unique_url": "http://www.blcu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0+0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.357",
        "title": "When Evolution Strategy Meets Language Models Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models further. However, each has its limitations regarding performance enhancements and susceptibility to overfitting. In this paper, we introduce a novel on-policy approach called Evolution Strategy Optimization (ESO), which is designed by harnessing the principle of biological evolution, namely survival of the fittest. Particularly, we consider model tuning as an evolution process, and each output sentence generated by the model can provide a perturbation signal to the model parameter space. Then, the fitness of perturbation signals is quantified by the difference between its score and the averaged one offered by a reward function, which guides the optimization process. Empirically, the proposed method can achieve superior performance in various tasks and comparable performance in the human alignment task.",
        "author": "Bo Huang; Yuxin Jiang; Mingyang Chen; Yi Wang; Hongyang Chen; Wei Wang",
        "authorids": "/b/bo-huang/; /y/yuxin-jiang/; /m/mingyang-chen/; /y/yi-wang/; /h/hongyang-chen/; /w/wei-wang/",
        "bibtex": "@inproceedings{huang-etal-2025-evolution,\n    title = \"When Evolution Strategy Meets Language Models Tuning\",\n    author = \"Huang, Bo  and\n      Jiang, Yuxin  and\n      Chen, Mingyang  and\n      Wang, Yi  and\n      Chen, Hongyang  and\n      Wang, Wei\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.357/\",\n    pages = \"5333--5344\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.357.pdf",
        "site": "https://aclanthology.org/2025.coling-main.357/",
        "pdf_size": 495190,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AKY4mXY5yAIJ:scholar.google.com/&scioq=When+Evolution+Strategy+Meets+Language+Models+Tuning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China + The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China + The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China + The Hong Kong University of Science and Technology, Hong Kong SAR, China; Dongguan University of Technology, Dongguan, China; Zhejiang Lab, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China + The Hong Kong University of Science and Technology, Hong Kong SAR, China",
        "aff_domain": "connect.ust.hk;connect.ust.hk;connect.ust.hk;dgut.edu.cn;ieee.org;ust.hk",
        "email": "connect.ust.hk;connect.ust.hk;connect.ust.hk;dgut.edu.cn;ieee.org;ust.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;2;3;0+1",
        "aff_unique_norm": "The Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Dongguan University of Technology;Zhejiang Lab",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ust.hk;https://www.ust.hk;http://www.dgut.edu.cn;http://www.zhejianglab.com",
        "aff_unique_abbr": "HKUST;HKUST;DGUT;",
        "aff_campus_unique_index": "0+1;0+1;0+1;2;0+1",
        "aff_campus_unique": "Guangzhou;Hong Kong;Dongguan;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.38",
        "title": "Where do LLMs Encode the Knowledge to Assess the Ambiguity?",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recently, large language models (LLMs) have shown remarkable performance across various natural language processing tasks, thanks to their vast amount of knowledge. Nevertheless, they often generate unreliable responses. A common example is providing a single biased answer to an ambiguous question that could have multiple correct answers. To address this issue, in this study, we discuss methods to detect such ambiguous samples. More specifically, we propose a classifier that uses a representation from an intermediate layer of the LLM as input. This is based on observations from previous research that representations of ambiguous samples in intermediate layers are closer to those of relevant label samples in the embedding space, but not necessarily in higher layers. The experimental results demonstrate that using representations from intermediate layers detects ambiguous input prompts more effectively than using representations from the final layer. Furthermore, in this study, we propose a method to train such classifiers without ambiguity labels, as most datasets lack labels regarding the ambiguity of samples, and evaluate its effectiveness.",
        "author": "Hancheol Park; Geonmin Kim",
        "authorids": "/h/hancheol-park/; /g/geonmin-kim/",
        "bibtex": "@inproceedings{park-kim-2025-llms,\n    title = \"Where do {LLM}s Encode the Knowledge to Assess the Ambiguity?\",\n    author = \"Park, Hancheol  and\n      Kim, Geonmin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.38/\",\n    pages = \"445--452\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.38.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.38/",
        "pdf_size": 359409,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4044729848692684487&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2025.coling-main.684",
        "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The efficacy of detectors for texts generated by large language models (LLMs) substantially depends on the availability of large-scale training data. However, white-box zero-shot detectors, which require no such data, are limited by the accessibility of the source model of the LLM-generated text. In this paper, we propose a simple yet effective black-box zero-shot detection approach based on the observation that, from the perspective of LLMs, human-written texts typically contain more grammatical errors than LLM-generated texts. This approach involves calculating the Grammar Error Correction Score (GECScore) for the given text to differentiate between human-written and LLM-generated text. Experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62% across XSum and Writing Prompts dataset. Additionally, our approach demonstrates strong reliability in the wild, exhibiting robust generalization and resistance to paraphrasing attacks. Data and code are available at: https://github.com/NLP2CT/GECScore.",
        "author": "Junchao Wu; Runzhe Zhan; Derek F. Wong; Shu Yang; Xuebo Liu; Lidia S. Chao; Min Zhang",
        "authorids": "/j/junchao-wu/; /r/runzhe-zhan/; /d/derek-f-wong/; /s/shu-yang/; /x/xuebo-liu/; /l/lidia-s-chao/; /m/min-zhang/",
        "bibtex": "@inproceedings{wu-etal-2025-wrote,\n    title = \"Who Wrote This? The Key to Zero-Shot {LLM}-Generated Text Detection Is {GECS}core\",\n    author = \"Wu, Junchao  and\n      Zhan, Runzhe  and\n      Wong, Derek F.  and\n      Yang, Shu  and\n      Liu, Xuebo  and\n      Chao, Lidia S.  and\n      Zhang, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.684/\",\n    pages = \"10275--10292\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.684.pdf",
        "site": "https://aclanthology.org/2025.coling-main.684/",
        "pdf_size": 818365,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10252198950573123412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "NLP2CT Lab, Department of Computer and Information Science, University of Macau; NLP2CT Lab, Department of Computer and Information Science, University of Macau; NLP2CT Lab, Department of Computer and Information Science, University of Macau; NLP2CT Lab, Department of Computer and Information Science, University of Macau; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; NLP2CT Lab, Department of Computer and Information Science, University of Macau; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China",
        "aff_domain": "um.edu.mo;um.edu.mo;um.edu.mo;um.edu.mo;hit.edu.cn;um.edu.mo;hit.edu.cn",
        "email": "um.edu.mo;um.edu.mo;um.edu.mo;um.edu.mo;hit.edu.cn;um.edu.mo;hit.edu.cn",
        "github": "https://github.com/NLP2CT/GECScore",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;0;1",
        "aff_unique_norm": "University of Macau;Harbin Institute of Technology",
        "aff_unique_dep": "Department of Computer and Information Science;Institute of Computing and Intelligence",
        "aff_unique_url": "https://www.um.edu.mo;http://www.hhit.edu.cn",
        "aff_unique_abbr": "UM;HIT",
        "aff_campus_unique_index": "0;0;0;0;1;0;1",
        "aff_campus_unique": "Macau SAR;Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.426",
        "title": "Why Does ChatGPT \u201cDelve\u201d So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Scientific English is currently undergoing rapid change, with words like \u201cdelve,\u201d \u201cintricate,\u201d and \u201cunderscore\u201d appearing far more frequently than just a few years ago. It is widely assumed that scientists\u2019 use of large language models (LLMs) is responsible for such trends. We develop a formal, transferable method to characterize these linguistic changes. Application of our method yields 21 focal words whose increased occurrence in scientific abstracts is likely the result of LLM usage. We then pose \u201cthe puzzle of lexical overrepresentation\u201d: why are such words overused by LLMs? We fail to find evidence that lexical overrepresentation is caused by model architecture, algorithm choices, or training data. To assess whether reinforcement learning from human feedback (RLHF) contributes to the overuse of focal words, we undertake comparative model testing and conduct an exploratory online study. While the model testing is consistent with RLHF playing a role, our experimental results suggest that participants may be reacting differently to \u201cdelve\u201d than to other focal words. With LLMs quickly becoming a driver of global language change, investigating these potential sources of lexical overrepresentation is important. We note that while insights into the workings of LLMs are within reach, a lack of transparency surrounding model development remains an obstacle to such research.",
        "author": "Tom S Juzek; Zina B. Ward",
        "authorids": "/t/tom-s-juzek/; /z/zina-b-ward/",
        "bibtex": "@inproceedings{juzek-ward-2025-chatgpt,\n    title = \"Why Does {C}hat{GPT} {\\textquotedblleft}Delve{\\textquotedblright} So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models\",\n    author = \"Juzek, Tom S  and\n      Ward, Zina B.\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.426/\",\n    pages = \"6397--6411\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.426.pdf",
        "site": "https://aclanthology.org/2025.coling-main.426/",
        "pdf_size": 2476748,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1765533890123638584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Florida State University; Florida State University",
        "aff_domain": "fsu.edu;fsu.edu",
        "email": "fsu.edu;fsu.edu",
        "github": "github.com/tjuzek/delve",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Florida State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.fsu.edu",
        "aff_unique_abbr": "FSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.441",
        "title": "Why do language models perform worse for morphologically complex languages?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language models perform differently across languages. It has been previously suggested that morphological typology may explain some of this variability (Cotterell et al., 2018). We replicate previous analyses and find additional new evidence for a performance gap between agglutinative and fusional languages, where fusional languages, such as English, tend to have better language modeling performance than morphologically more complex languages like Turkish. We then propose and test three possible causes for this performance gap: morphological alignment of tokenizers, tokenization quality, and disparities in dataset sizes and measurement. To test the morphological alignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and supporting datasets for 22 languages. We find some evidence that tokenization quality explains the performance gap, but none for the role of morphological alignment. Instead we find that the performance gap is most reduced when training datasets are of equivalent size across language types, but only when scaled according to the so-called \u201cbyte-premium\u201d\u2014the different encoding efficiencies of different languages and orthographies. These results suggest that languages of particular morphological types are not intrinsically advantaged or disadvantaged in language modeling. Differences in performance can be attributed to disparities in dataset size. These findings bear on ongoing efforts to improve performance for low-performing and under-resourced languages.",
        "author": "Catherine Arnett; Benjamin Bergen",
        "authorids": "/c/catherine-arnett/; /b/benjamin-bergen/",
        "bibtex": "@inproceedings{arnett-bergen-2025-language,\n    title = \"Why do language models perform worse for morphologically complex languages?\",\n    author = \"Arnett, Catherine  and\n      Bergen, Benjamin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.441/\",\n    pages = \"6607--6623\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.441.pdf",
        "site": "https://aclanthology.org/2025.coling-main.441/",
        "pdf_size": 398120,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7211759600361633794&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Linguistics, University of California San Diego; Department of Cognitive Science, University of California San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "https://osf.io/jukzd/?view only=3d0d491d24074215a0ab81f72a693c16",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, San Diego;University of California San Diego",
        "aff_unique_dep": "Department of Linguistics;Department of Cognitive Science",
        "aff_unique_url": "https://ucsd.edu;https://ucsd.edu",
        "aff_unique_abbr": "UCSD;UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2025.coling-main.320",
        "title": "Why should only High-Resource-Languages have all the fun? Pivot Based Evaluation in Low Resource Setting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Evaluating machine translation (MT) systems for low-resource languages has long been a challenge due to the limited availability of evaluation metrics and resources. As a result, researchers in this space have relied primarily on lexical-based metrics like BLEU, TER, and ChrF, which lack semantic evaluation. In this first-of-its-kind work, we propose a novel pivot-based evaluation framework that addresses these limitations; after translating low-resource language outputs into a related high-resource language, we leverage advanced neural and embedding-based metrics for more meaningful evaluation. Through a series of experiments using five low-resource languages: Assamese, Manipuri, Kannada, Bhojpuri, and Nepali, we demonstrate how this method extends the coverage of both lexical-based and embedding-based metrics, even for languages not directly supported by advanced metrics. Our results show that the differences between direct and pivot-based evaluation scores are minimal, proving that this approach is a viable and effective solution for evaluating translations in endangered and low-resource languages. This work paves the way for more inclusive, accurate, and scalable MT evaluation for underrepresented languages, marking a significant step forward in this under-explored area of research. The code and data will be made available at https://github.com/AnanyaCoder/PivotBasedEvaluation.",
        "author": "Ananya Mukherjee; Saumitra Yadav; Manish Shrivastava",
        "authorids": "/a/ananya-mukherjee/; /s/saumitra-yadav/; /m/manish-shrivastava/",
        "bibtex": "@inproceedings{mukherjee-etal-2025-high,\n    title = \"Why should only High-Resource-Languages have all the fun? Pivot Based Evaluation in Low Resource Setting\",\n    author = \"Mukherjee, Ananya  and\n      Yadav, Saumitra  and\n      Shrivastava, Manish\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.320/\",\n    pages = \"4779--4788\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.320.pdf",
        "site": "https://aclanthology.org/2025.coling-main.320/",
        "pdf_size": 780989,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Db8zKxkxaBoJ:scholar.google.com/&scioq=Why+should+only+High-Resource-Languages+have+all+the+fun%3F+Pivot+Based+Evaluation+in+Low+Resource+Setting&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "MT-NLP Lab, LTRC, KCIS, IIIT Hyderabad, India; MT-NLP Lab, LTRC, KCIS, IIIT Hyderabad, India; MT-NLP Lab, LTRC, KCIS, IIIT Hyderabad, India",
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "github": "https://github.com/AnanyaCoder/PivotBasedEvaluation",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "IIIT Hyderabad",
        "aff_unique_dep": "MT-NLP Lab, LTRC, KCIS",
        "aff_unique_url": "https://www.iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT-H",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.138",
        "title": "Word-level Cross-lingual Structure in Large Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a broad spectrum of cross-lingual Natural Language Processing (NLP) tasks. However, previous methods predominantly focus on leveraging parallel corpus to conduct instruction data for continuing pre-training or fine-tuning. They ignored the state of parallel data on the hidden layers of LLMs. In this paper, we demonstrate Word-level Cross-lingual Structure (WCS) of LLM which proves that the word-level embedding on the hidden layers are isomorphic between languages. We find that the hidden states of different languages\u2019 input on the LLMs hidden layers can be aligned with an orthogonal matrix on word-level. We prove this conclusion in both mathematical and downstream task ways on two representative LLM foundations, LLaMA2 and BLOOM. Besides, we propose an Isomorphism-based Data Augmentation (IDA) method to apply the WCS on a downstream cross-lingual task, Bilingual Lexicon Induction (BLI), in both supervised and unsupervised ways. The experiment shows the significant improvement of our proposed method over all the baselines, especially on low-resource languages.",
        "author": "Zihao Feng; Hailong Cao; Wang Xu; Tiejun Zhao",
        "authorids": "/z/zihao-feng/; /h/hailong-cao/; /w/wang-xu/; /t/tiejun-zhao/",
        "bibtex": "@inproceedings{feng-etal-2025-word,\n    title = \"Word-level Cross-lingual Structure in Large Language Models\",\n    author = \"Feng, Zihao  and\n      Cao, Hailong  and\n      Xu, Wang  and\n      Zhao, Tiejun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.138/\",\n    pages = \"2026--2037\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.138.pdf",
        "site": "https://aclanthology.org/2025.coling-main.138/",
        "pdf_size": 1194667,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0JzKL0-UfLkJ:scholar.google.com/&scioq=Word-level+Cross-lingual+Structure+in+Large+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology",
        "aff_domain": "outlook.com;hit.edu.cn;126.com;hit.edu.cn",
        "email": "outlook.com;hit.edu.cn;126.com;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "Faculty of Computing",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.41",
        "title": "XFormParser: A Simple and Effective Multimodal Multilingual Semi-structured Form Parser",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the domain of Document AI, parsing semi-structured image form is a crucial Key Information Extraction (KIE) task. The advent of pre-trained multimodal models significantly empowers Document AI frameworks to extract key information from form documents in different formats such as PDF, Word, and images. Nonetheless, form parsing is still encumbered by notable challenges like subpar capabilities in multilingual parsing and diminished recall in industrial contexts in rich text and rich visuals. In this work, we introduce a simple but effective Multimodal and Multilingual semi-structured FORM PARSER (XFormParser), which is anchored on a comprehensive Transformer-based pre-trained language model and innovatively amalgamates semantic entity recognition (SER) and relation extraction (RE) into a unified framework. Combined with Bi-LSTM, the performance of multilingual parsing is significantly improved. Furthermore, we develop InDFormSFT, a pioneering supervised fine-tuning (SFT) industrial dataset that specifically addresses the parsing needs of forms in a variety of industrial contexts. Through rigorous testing on established benchmarks, XFormParser has demonstrated its unparalleled effectiveness and robustness. Compared to existing state-of-the-art (SOTA) models, XFormParser notably achieves up to 1.79% F1 score improvement on RE tasks in language-specific settings. It also exhibits exceptional improvements in cross-task performance in both multilingual and zero-shot settings.",
        "author": "Xianfu Cheng; Hang Zhang; Jian Yang; Xiang Li; Weixiao Zhou; Fei Liu; Kui Wu; Xiangyuan Guan; Tao Sun; Xianjie Wu; Tongliang Li; Zhoujun Li",
        "authorids": "/x/xianfu-cheng/; /h/hang-zhang/; /j/jian-yang/; /x/xiang-li/; /w/weixiao-zhou/; /f/fei-liu/; /k/kui-wu/; /x/xiangyuan-guan/; /t/tao-sun/; /x/xianjie-wu/; /t/tongliang-li/; /z/zhoujun-li/",
        "bibtex": "@inproceedings{cheng-etal-2025-xformparser,\n    title = \"{XF}orm{P}arser: A Simple and Effective Multimodal Multilingual Semi-structured Form Parser\",\n    author = \"Cheng, Xianfu  and\n      Zhang, Hang  and\n      Yang, Jian  and\n      Li, Xiang  and\n      Zhou, Weixiao  and\n      Liu, Fei  and\n      Wu, Kui  and\n      Guan, Xiangyuan  and\n      Sun, Tao  and\n      Wu, Xianjie  and\n      Li, Tongliang  and\n      Li, Zhoujun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.41/\",\n    pages = \"606--620\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.41.pdf",
        "site": "https://aclanthology.org/2025.coling-main.41/",
        "pdf_size": 1584830,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2312592395435291&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "CCSE, Beihang University; CCSE, Beihang University; Beihang University; Beihang University; CCSE, Beihang University; Beijing Language and Culture University; CCSE, Beihang University; CCSE, Beihang University; CCSE, Beihang University; CCSE, Beihang University; Beijing Information Science and Technology University; CCSE, Beihang University + Shenzhen Intelligent Strong Technology Co.,Ltd.",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;163.com;gmail.com;bistu.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;163.com;gmail.com;bistu.edu.cn",
        "github": "https://github.com/zhbuaa0/xformparser",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;1;0;0;0;0;2;0+3",
        "aff_unique_norm": "Beihang University;Beijing Language and Culture University;Beijing Information Science and Technology University;Shenzhen Intelligent Strong Technology Co., Ltd.",
        "aff_unique_dep": "CCSE;;;",
        "aff_unique_url": "http://www.buaa.edu.cn;http://www.blcu.edu.cn;http://www.bistu.edu.cn;",
        "aff_unique_abbr": ";BLCU;BISTU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.30",
        "title": "XTR meets ColBERTv2: Adding ColBERTv2 Optimizations to XTR",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "XTR (Lee et al., 2023) introduced an efficient multi-vector retrieval method that addresses the limitations of the ColBERT (Khattab and Zaharia, 2020model by simplifying retrieval into a single stage through a modified learning objective. While XTR eliminates the need for multistage retrieval, it doesn\u2019t incorporate the efficiency optimizations from ColBERTv2 (Santhanam et al., 2022, which improve indexing and retrieval speed. In this work, we enhance XTR by integrating ColBERTv2\u2019s optimizations, showing that the combined approach preserves the strengths of both models. This results in a more efficient and scalable solution for multi-vector retrieval, while maintaining XTR\u2019s streamlined retrieval process.",
        "author": "Riyaz Ahmad Bhat; Jaydeep Sen",
        "authorids": "/r/riyaz-ahmad-bhat/; /j/jaydeep-sen/",
        "bibtex": "@inproceedings{bhat-sen-2025-xtr,\n    title = \"{XTR} meets {C}ol{BERT}v2: Adding {C}ol{BERT}v2 Optimizations to {XTR}\",\n    author = \"Bhat, Riyaz Ahmad  and\n      Sen, Jaydeep\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.30/\",\n    pages = \"358--365\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.30.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.30/",
        "pdf_size": 506801,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AttVzbxXbnsJ:scholar.google.com/&scioq=XTR+meets+ColBERTv2:+Adding+ColBERTv2+Optimizations+to+XTR&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "IBM Research, India; IBM Research, India",
        "aff_domain": "ibm.com;in.ibm.com",
        "email": "ibm.com;in.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.230",
        "title": "You Only Query Twice: Multimodal Rumor Detection via Evidential Evaluation from Dual Perspectives",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current rumor detectors exhibit limitations in fully exploiting responses to the source tweet as essential public opinions, and in explaining and indicating the reliability of the results obtained. Additionally, the joint utilization of both responses and the multimodal source content for detection presents challenges due to the heterogeneous nature of the data points. In this work, to address the first challenge, we initially prompt the Large Language Model (LLM) with both multimodal source content and the corresponding response set to extract contrasting evidence to enable maximal utilization of informative responses. To overcome the second challenge, we introduce an uncertainty-aware evidential evaluator to assess the evidence intensity from the multimodal source content and dual-sided reasoning, from which the final prediction is derived. As we model the second-order probability, we can effectively indicate the model\u2019s uncertainty (i.e., the reliability) of the results. The reasoning from the correct perspective also serves as a natural language-based explanation. To this end, the third challenge is also addressed as we fully leverage the available resources. Extensive experiments validate the effectiveness, uncertainty awareness in predictions, helpful explainability for human judgment, and superior efficiency of our approach compared to contemporary works utilizing LLMs.",
        "author": "Junyi Chen; Leyuan Liu; Tian Lan; Fan Zhou; Xiaosong Zhang",
        "authorids": "/j/junyi-chen/; /l/leyuan-liu/; /t/tian-lan/; /f/fan-zhou/; /x/xiaosong-zhang/",
        "bibtex": "@inproceedings{chen-etal-2025-query,\n    title = \"You Only Query Twice: Multimodal Rumor Detection via Evidential Evaluation from Dual Perspectives\",\n    author = \"Chen, Junyi  and\n      Liu, Leyuan  and\n      Lan, Tian  and\n      Zhou, Fan  and\n      Zhang, Xiaosong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.230/\",\n    pages = \"3415--3427\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.230.pdf",
        "site": "https://aclanthology.org/2025.coling-main.230/",
        "pdf_size": 616205,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CT6cUjVNaS0J:scholar.google.com/&scioq=You+Only+Query+Twice:+Multimodal+Rumor+Detection+via+Evidential+Evaluation+from+Dual+Perspectives&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; University of Electronic Science and Technology of China",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Electronic Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uestc.edu.cn",
        "aff_unique_abbr": "UESTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.542",
        "title": "Zero-Shot Entailment Learning for Ontology-Based Biomedical Annotation Without Explicit Mentions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic biomedical annotation is essential for advancing medical research, diagnosis, and treatment. However, it presents significant challenges, especially when entities are not explicitly mentioned in the text, leading to difficulties in extraction of relevant information. These challenges are intensified by unclear terminology, implicit background knowledge, and the lack of labeled training data. Annotating with a specific ontology adds another layer of complexity, as it requires aligning text with a predefined set of concepts and relationships. Manual annotation is time-consuming and expensive, highlighting the need for automated systems to handle large volumes of biomedical data efficiently. In this paper, we propose an entailment-based zero-shot text classification approach to annotate biomedical text passages using the Homeostasis Imbalance Process (HOIP) ontology. Our method reformulates the annotation task as a multi-class, multi-label classification problem and uses natural language inference to classify text into related HOIP processes. Experimental results show promising performance, especially when processes are not explicitly mentioned, highlighting the effectiveness of our approach for ontological annotation of biomedical literature.",
        "author": "Rumana Ferdous Munne; Noriki Nishida; Shanshan Liu; Narumi Tokunaga; Yuki Yamagata; Kouji Kozaki; Yuji Matsumoto",
        "authorids": "/r/rumana-ferdous-munne/; /n/noriki-nishida/; /s/shanshan-liu/; /n/narumi-tokunaga/; /y/yuki-yamagata/; /k/kouji-kozaki/; /y/yuji-matsumoto/",
        "bibtex": "@inproceedings{munne-etal-2025-zero,\n    title = \"Zero-Shot Entailment Learning for Ontology-Based Biomedical Annotation Without Explicit Mentions\",\n    author = \"Munne, Rumana Ferdous  and\n      Nishida, Noriki  and\n      Liu, Shanshan  and\n      Tokunaga, Narumi  and\n      Yamagata, Yuki  and\n      Kozaki, Kouji  and\n      Matsumoto, Yuji\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.542/\",\n    pages = \"8148--8159\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.542.pdf",
        "site": "https://aclanthology.org/2025.coling-main.542/",
        "pdf_size": 2258521,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mXit7KihFwcJ:scholar.google.com/&scioq=Zero-Shot+Entailment+Learning+for+Ontology-Based+Biomedical+Annotation+Without+Explicit+Mentions&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "RIKEN AIP; RIKEN AIP; RIKEN AIP; RIKEN AIP; RIKEN R-IH+RIKEN BRC; RIKEN AIP+Osaka Electro-Communication University; RIKEN AIP",
        "aff_domain": "riken.jp;riken.jp;riken.jp;riken.jp;riken.jp;osakac.ac.jp;riken.jp",
        "email": "riken.jp;riken.jp;riken.jp;riken.jp;riken.jp;osakac.ac.jp;riken.jp",
        "github": "https://github.com/norikinishida/HOIP-dataset",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0+0;0+1;0",
        "aff_unique_norm": "RIKEN;Osaka Electro-Communication University",
        "aff_unique_dep": "Advanced Institute for Computational Science;",
        "aff_unique_url": "https://www.aip.riken.jp;https://www.oecu.ac.jp",
        "aff_unique_abbr": "RIKEN AIP;OECU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0;0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2025.coling-industry.59",
        "title": "Zero-shot Slot Filling in the Age of LLMs for Dialogue Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Zero-shot slot filling is a well-established subtask of Natural Language Understanding (NLU). However, most existing methods primarily focus on single-turn text data, overlooking the unique complexities of conversational dialogue. Conversational data is highly dynamic, often involving abrupt topic shifts, interruptions, and implicit references that make it difficult to directly apply zero-shot slot filling techniques, even with the remarkable capabilities of large language models (LLMs). This paper addresses these challenges by proposing strategies for automatic data annotation with slot induction and black-box knowledge distillation (KD) from a teacher LLM to a smaller model, outperforming vanilla LLMs on internal datasets by 26% absolute increase in F1 score. Additionally, we introduce an efficient system architecture for call center product settings that surpasses off-the-shelf extractive models by 34% relative F1 score, enabling near real-time inference on dialogue streams with higher accuracy, while preserving low latency.",
        "author": "Mansi Rana; Kadri Hacioglu; Sindhuja Gopalan; Maragathamani Boothalingam",
        "authorids": "/m/mansi-rana/; /k/kadri-hacioglu/; /s/sindhuja-gopalan/; /m/maragathamani-boothalingam/",
        "bibtex": "@inproceedings{rana-etal-2025-zero,\n    title = \"Zero-shot Slot Filling in the Age of {LLM}s for Dialogue Systems\",\n    author = \"Rana, Mansi  and\n      Hacioglu, Kadri  and\n      Gopalan, Sindhuja  and\n      Boothalingam, Maragathamani\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.59/\",\n    pages = \"697--706\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.59.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.59/",
        "pdf_size": 544574,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4171951299376902201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Uniphore; Uniphore; Uniphore; Uniphore",
        "aff_domain": "uniphore.com;uniphore.com;uniphore.com;uniphore.com",
        "email": "uniphore.com;uniphore.com;uniphore.com;uniphore.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Uniphore Software Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uniphore.com",
        "aff_unique_abbr": "Uniphore",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2025.coling-main.650",
        "title": "Zero-shot and Few-shot Learning with Instruction-following LLMs for Claim Matching in Automated Fact-checking",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The claim matching (CM) task can benefit an automated fact-checking pipeline by putting together claims that can be resolved with the same fact-check. In this work, we are the first to explore zero-shot and few-shot learning approaches to the task. We consider CM as a binary classification task and experiment with a set of instruction-following large language models (GPT-3.5-turbo, Gemini-1.5-flash, Mistral-7B-Instruct, and Llama-3-8B-Instruct), investigating prompt templates. We introduce a new CM dataset, ClaimMatch, which will be released upon acceptance. We put LLMs to the test in the CM task and find out that it can be tackled by leveraging more mature yet similar tasks such as natural language inference or paraphrase detection. We also propose a pipeline for CM, which we evaluate on texts of different lengths.",
        "author": "Dina Pisarevskaya; Arkaitz Zubiaga",
        "authorids": "/d/dina-pisarevskaya/; /a/arkaitz-zubiaga/",
        "bibtex": "@inproceedings{pisarevskaya-zubiaga-2025-zero,\n    title = \"Zero-shot and Few-shot Learning with Instruction-following {LLM}s for Claim Matching in Automated Fact-checking\",\n    author = \"Pisarevskaya, Dina  and\n      Zubiaga, Arkaitz\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.650/\",\n    pages = \"9721--9736\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.650.pdf",
        "site": "https://aclanthology.org/2025.coling-main.650/",
        "pdf_size": 278974,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Wved_cxUGi0J:scholar.google.com/&scioq=Zero-shot+and+Few-shot+Learning+with+Instruction-following+LLMs+for+Claim+Matching+in+Automated+Fact-checking&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Queen Mary University of London, UK; Queen Mary University of London, UK",
        "aff_domain": "qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Queen Mary University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.qmul.ac.uk",
        "aff_unique_abbr": "QMUL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2025.coling-main.251",
        "title": "Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance through supervised fine-tuning or in-context learning using gold labels. However, this paradigm is limited by the availability of gold labels, while in certain scenarios, LLMs may need to perform tasks that are too complex for humans to provide such labels. To tackle this challenge, this study explores whether solely utilizing unlabeled data can elicit strong model capabilities. We propose a new paradigm termed zero-to-strong generalization. We iteratively prompt LLMs to annotate unlabeled data and retain high-quality labels by filtering. Surprisingly, we obverse that this iterative process gradually unlocks LLMs\u2019 potential on downstream tasks. Our experiments on extensive classification and reasoning tasks confirm the effectiveness of our proposed framework. Our analysis indicates that this paradigm is effective for both in-context learning and fine-tuning, and for various model sizes.",
        "author": "Chaoqun Liu; Qin Chao; Wenxuan Zhang; Xiaobao Wu; Boyang Li; Anh Tuan Luu; Lidong Bing",
        "authorids": "/c/chaoqun-liu/; /q/qin-chao/; /w/wenxuan-zhang/; /x/xiaobao-wu/; /b/boyang-li/; /l/luu-anh-tuan/; /l/lidong-bing/",
        "bibtex": "@inproceedings{liu-etal-2025-zero,\n    title = \"Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels\",\n    author = \"Liu, Chaoqun  and\n      Chao, Qin  and\n      Zhang, Wenxuan  and\n      Wu, Xiaobao  and\n      Li, Boyang  and\n      Luu, Anh Tuan  and\n      Bing, Lidong\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.251/\",\n    pages = \"3716--3731\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.251.pdf",
        "site": "https://aclanthology.org/2025.coling-main.251/",
        "pdf_size": 811138,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9787167767232286935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2025.coling-main.596",
        "title": "ZigZagKV: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large Language models (LLMs) have become a research hotspot. To accelerate the inference of LLMs, storing computed caches in memory has become the standard technique. However, as the inference length increases, growing KV caches might lead to out-of-memory issues. Many existing methods address this issue through KV cache compression, primarily by preserving key tokens throughout all layers to reduce information loss. Most of them allocate a uniform budget size for each layer to retain. However, we observe that the minimum budget sizes needed to retain essential information vary across layers and models based on the perspectives of attention and hidden state output. Building on this observation, this paper proposes a simple yet effective KV cache compression method that leverages layer uncertainty to allocate budget size for each layer. Experimental results show that the proposed method can reduce memory usage of the KV caches to only ~20% when compared to full KV inference while achieving nearly lossless performance.",
        "author": "Meizhi Zhong; Xikai Liu; Chen Zhang; Yikun Lei; Yan Gao; Yao Hu; Kehai Chen; Min Zhang",
        "authorids": "/m/meizhi-zhong/; /x/xikai-liu/; /c/chen-zhang/; /y/yikun-lei/; /y/yan-gao/; /y/yao-hu/; /k/kehai-chen/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhong-etal-2025-zigzagkv,\n    title = \"{Z}ig{Z}ag{KV}: Dynamic {KV} Cache Compression for Long-context Modeling based on Layer Uncertainty\",\n    author = \"Zhong, Meizhi  and\n      Liu, Xikai  and\n      Zhang, Chen  and\n      Lei, Yikun  and\n      Gao, Yan  and\n      Hu, Yao  and\n      Chen, Kehai  and\n      Zhang, Min\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.596/\",\n    pages = \"8897--8907\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.596.pdf",
        "site": "https://aclanthology.org/2025.coling-main.596/",
        "pdf_size": 654514,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:zGN2HhDNNvEJ:scholar.google.com/&scioq=ZigZagKV:+Dynamic+KV+Cache+Compression+for+Long-context+Modeling+based+on+Layer+Uncertainty&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China+Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Xiaohongshu Inc.; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China",
        "aff_domain": "gmail.com;outlook.com;hit.edu.cn;hit.edu.cn;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com",
        "email": "gmail.com;outlook.com;hit.edu.cn;hit.edu.cn;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com;xiaohongshu.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;1;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Xiaohongshu Inc.",
        "aff_unique_dep": "Institute of Computing and Intelligence;",
        "aff_unique_url": "http://www.hhit.edu.cn;https://www.xiaohongshu.com",
        "aff_unique_abbr": "HIT;Xiaohongshu",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.435",
        "title": "data2lang2vec: Data Driven Typological Features Completion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language typology databases enhance multi-lingual Natural Language Processing (NLP) by improving model adaptability to diverse linguistic structures. The widely-used lang2vec toolkit integrates several such databases, but its coverage remains limited at 28.9%. Previous work on automatically increasing coverage predicts missing values based on features from other languages or focuses on single features, we propose to use textual data for better-informed feature prediction. To this end, we introduce a multi-lingual Part-of-Speech (POS) tagger, achieving over 70% accuracy across 1,749 languages, and experiment with external statistical features and a variety of machine learning algorithms. We also introduce a more realistic evaluation setup, focusing on likely to be missing typology features, and show that our approach outperforms previous work in both setups.",
        "author": "Hamidreza Amirzadeh; Sadegh Jafari; Anika Harju; Rob van der Goot",
        "authorids": "/h/hamidreza-amirzadeh/; /s/sadegh-jafari/; /a/anika-harju/; /r/rob-van-der-goot/",
        "bibtex": "@inproceedings{amirzadeh-etal-2025-data2lang2vec,\n    title = \"data2lang2vec: Data Driven Typological Features Completion\",\n    author = \"Amirzadeh, Hamidreza  and\n      Jafari, Sadegh  and\n      Harju, Anika  and\n      van der Goot, Rob\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.435/\",\n    pages = \"6520--6529\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.435.pdf",
        "site": "https://aclanthology.org/2025.coling-main.435/",
        "pdf_size": 563298,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:o0puWZ8CBksJ:scholar.google.com/&scioq=data2lang2vec:+Data+Driven+Typological+Features+Completion&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "IT University of Copenhagen, Denmark+Sharif University of Technology, Iran; IT University of Copenhagen, Denmark+Iran University of Science and Technology, Iran; IT University of Copenhagen, Denmark; IT University of Copenhagen, Denmark",
        "aff_domain": "sharif.edu;comp.iust.ac.ir;itu.dk;itu.dk",
        "email": "sharif.edu;comp.iust.ac.ir;itu.dk;itu.dk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+2;0;0",
        "aff_unique_norm": "IT University of Copenhagen;Sharif University of Technology;Iran University of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://itu.dk;https://www.sharif.edu;https://www.iust.ac.ir",
        "aff_unique_abbr": "ITU;SUT;IUST",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0;0",
        "aff_country_unique": "Denmark;Iran"
    },
    {
        "id": "2025.coling-main.624",
        "title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (Integrating Knowledge Graphs as Assistants of LLMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network-like approach, allowing initial diagnosis by the LLM to be merged into KG search results. Through a path-based reranking algorithm and a fill-in-the-blank style prompt template, it further refined the diagnostic process. We validated medIKAL\u2019s effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis in real-world settings.",
        "author": "Mingyi Jia; Junwen Duan; Yan Song; Jianxin Wang",
        "authorids": "/m/mingyi-jia/; /j/junwen-duan/; /y/yan-song/; /j/jianxin-wang/",
        "bibtex": "@inproceedings{jia-etal-2025-medikal,\n    title = \"med{IKAL}: Integrating Knowledge Graphs as Assistants of {LLM}s for Enhanced Clinical Diagnosis on {EMR}s\",\n    author = \"Jia, Mingyi  and\n      Duan, Junwen  and\n      Song, Yan  and\n      Wang, Jianxin\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.624/\",\n    pages = \"9278--9298\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.624.pdf",
        "site": "https://aclanthology.org/2025.coling-main.624/",
        "pdf_size": 1373622,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15018111199191734359&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University; Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University; University of Science and Technology of China; Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University",
        "aff_domain": "csu.edu.cn;csu.edu.cn;gmial.com;mail.csu.edu.cn",
        "email": "csu.edu.cn;csu.edu.cn;gmial.com;mail.csu.edu.cn",
        "github": "https://github.com/CSU-NLP-Group/mediKAL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Central South University;University of Science and Technology of China",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "http://www.csu.edu.cn;http://www.ustc.edu.cn",
        "aff_unique_abbr": "CSU;USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-industry.31",
        "title": "sDPO: Don\u2019t Use Your Data All at Once",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "As large language models (LLMs) continue to advance, aligning them with human preferences has become a critical objective. In this paper, we introduce stepwise DPO (sDPO), an innovative extension of the recently popularized Direct Preference Optimization (DPO) technique for alignment tuning. sDPO systematically partitions the available preference datasets and applies them incrementally, rather than utilizing the entire dataset simultaneously. This stepwise manner enables the integration of progressively more aligned reference models within the DPO training framework. Our empirical results demonstrate that sDPO not only enhances the alignment precision of reference models but also significantly improves the overall performance of the final model, surpassing other prominent LLMs with larger parameter counts.",
        "author": "Dahyun Kim; Yungi Kim; Wonho Song; Hyeonwoo Kim; Yunsu Kim; Sanghoon Kim; Chanjun Park",
        "authorids": "/d/dahyun-kim/; /y/yungi-kim/; /w/wonho-song/; /h/hyeonwoo-kim/; /y/yunsu-kim/; /s/sanghoon-kim/; /c/chanjun-park/",
        "bibtex": "@inproceedings{kim-etal-2025-sdpo,\n    title = \"s{DPO}: Don{'}t Use Your Data All at Once\",\n    author = \"Kim, Dahyun  and\n      Kim, Yungi  and\n      Song, Wonho  and\n      Kim, Hyeonwoo  and\n      Kim, Yunsu  and\n      Kim, Sanghoon  and\n      Park, Chanjun\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.31/\",\n    pages = \"366--373\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.31.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.31/",
        "pdf_size": 620274,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4698697190792784399&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Twelve Labs; Upstage AI; Upstage AI; Upstage AI; Upstage AI; Upstage AI; Korea University",
        "aff_domain": "twelvelabs.io;upstage.ai;upstage.ai;upstage.ai;upstage.ai;upstage.ai;korea.ac.kr",
        "email": "twelvelabs.io;upstage.ai;upstage.ai;upstage.ai;upstage.ai;upstage.ai;korea.ac.kr",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;2",
        "aff_unique_norm": "Twelve Labs;Upstage AI;Korea University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://twelvelabs.com;;https://www.korea.ac.kr",
        "aff_unique_abbr": ";;KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2",
        "aff_country_unique": "United States;;South Korea"
    },
    {
        "id": "2025.coling-main.192",
        "title": "t-HNE: A Text-guided Hierarchical Noise Eliminator for Multimodal Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the Multimodal Sentiment Analysis task, most existing approaches focus on extracting modality-consistent information from raw unimodal data and integrating it into multimodal representations for sentiment classification. However, these methods often assume that all modalities contribute equally to model performance, prioritizing the extraction and enhancement of consistent information, while overlooking the adverse effects of noise caused by modality inconsistency. In contrast to these approaches, this paper introduces a novel approach namely text-guided Hierarchical Noise Eliminator (t-HNE). This model consists of a two-stage denoising phase and a feature recovery phase. Firstly, textual information is injected into both visual and acoustic modalities using an attention mechanism, aiming to reduce intra-modality noise in the visual and acoustic representations. Secondly, it further mitigates inter-modality noise by maximizing the mutual information between textual representations and the respective visual and acoustic representations. Finally, to address the potential loss of modality-invariant information during denoising, the fused multimodal representation is refined through contrastive learning with each unimodal representation except the textual. Extensive experiments conducted on the CMU-MOSI and CMU-MOSEI datasets demonstrate the efficacy of our approach.",
        "author": "Zuocheng Li; Lishuang Li",
        "authorids": "/z/zuocheng-li/; /l/lishuang-li/",
        "bibtex": "@inproceedings{li-li-2025-hne,\n    title = \"t-{HNE}: A Text-guided Hierarchical Noise Eliminator for Multimodal Sentiment Analysis\",\n    author = \"Li, Zuocheng  and\n      Li, Lishuang\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.192/\",\n    pages = \"2834--2844\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.192.pdf",
        "site": "https://aclanthology.org/2025.coling-main.192/",
        "pdf_size": 939962,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VC-D1iRMblcJ:scholar.google.com/&scioq=t-HNE:+A+Text-guided+Hierarchical+Noise+Eliminator+for+Multimodal+Sentiment+Analysis&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China",
        "aff_domain": "mail.dlut.edu.cn;dlut.edu.cn",
        "email": "mail.dlut.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.dlut.edu.cn/",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dalian",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2025.coling-main.146",
        "title": "\u201cNot Aligned\u201d is Not \u201cMalicious\u201d: Being Careful about Hallucinations of Large Language Models\u2019 Jailbreak",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "\u201cJailbreak\u201d is a major safety concern of Large Language Models (LLMs), which occurs when malicious prompts lead LLMs to produce harmful outputs, raising issues about the reliability and safety of LLMs. Therefore, an effective evaluation of jailbreaks is very crucial to develop its mitigation strategies. However, our research reveals that many jailbreaks identified by current evaluations may actually be hallucinations\u2014erroneous outputs that are mistaken for genuine safety breaches. This finding suggests that some perceived vulnerabilities might not represent actual threats, indicating a need for more precise red teaming benchmarks. To address this problem, we propose the Benchmark for reliABilitY and jailBreak haLlUcination Evaluation (BabyBLUE). BabyBLUE introduces a specialized validation framework including various evaluators to enhance existing jailbreak benchmarks, ensuring outputs are useful malicious instructions. Additionally, BabyBLUE presents a new dataset as an augmentation to the existing red teaming benchmarks, specifically addressing hallucinations in jailbreaks, aiming to evaluate the true potential of jailbroken LLM outputs to cause harm to human society.",
        "author": "Lingrui Mei; Shenghua Liu; Yiwei Wang; Baolong Bi; Jiayi Mao; Xueqi Cheng",
        "authorids": "/l/lingrui-mei/; /s/shenghua-liu/; /y/yiwei-wang/; /b/baolong-bi/; /j/jiayi-mao/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{mei-etal-2025-aligned,\n    title = \"{\\textquotedblleft}Not Aligned{\\textquotedblright} is Not {\\textquotedblleft}Malicious{\\textquotedblright}: Being Careful about Hallucinations of Large Language Models' Jailbreak\",\n    author = \"Mei, Lingrui  and\n      Liu, Shenghua  and\n      Wang, Yiwei  and\n      Bi, Baolong  and\n      Mao, Jiayi  and\n      Cheng, Xueqi\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-main.146/\",\n    pages = \"2144--2162\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-main.146.pdf",
        "site": "https://aclanthology.org/2025.coling-main.146/",
        "pdf_size": 2054039,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5910660141557157959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS+University of Chinese Academy of Sciences; CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS+University of Chinese Academy of Sciences; UCLA+University of California, Merced; CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS+University of Chinese Academy of Sciences; Tsinghua University; CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS+University of Chinese Academy of Sciences",
        "aff_domain": "mails.ucas.ac.cn;ict.ac.cn;gmail.com;ict.ac.cn;mails.tsinghua.edu.cn;ict.ac.cn",
        "email": "mails.ucas.ac.cn;ict.ac.cn;gmail.com;ict.ac.cn;mails.tsinghua.edu.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;2+3;0+1;4;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;University of California, Los Angeles;University of California, Merced;Tsinghua University",
        "aff_unique_dep": "Institute of Computing Technology;;;;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn;https://www.ucla.edu;https://www.ucmerced.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;UCLA;UC Merced;THU",
        "aff_campus_unique_index": ";;1+2;;",
        "aff_campus_unique": ";Los Angeles;Merced",
        "aff_country_unique_index": "0+0;0+0;1+1;0+0;0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2025.coling-industry.23",
        "title": "\u201cStupid robot, I want to speak to a human!\u201d User Frustration Detection in Task-Oriented Dialog Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems is imperative for maintaining overall user satisfaction, engagement, and retention. However, most recent research is focused on sentiment and emotion detection in academic settings, thus failing to fully encapsulate implications of real-world user data. To mitigate this gap, in this work, we focus on user frustration in a deployed TOD system, assessing the feasibility of out-of-the-box solutions for user frustration detection. Specifically, we compare the performance of our deployed keyword-based approach, open-source approaches to sentiment analysis, dialog breakdown detection methods, and emerging in-context learning LLM-based detection. Our analysis highlights the limitations of open-source methods for real-world frustration detection, while demonstrating the superior performance of the LLM-based approach, achieving a 16% relative improvement in F1 score on an internal benchmark. Finally, we analyze advantages and limitations of our methods and provide an insight into user frustration detection task for industry practitioners.",
        "author": "Mireia Hernandez Caralt; Ivan Sekulic; Filip Carevic; Nghia Khau; Diana Nicoleta Popa; Bruna Guedes; Victor Guimaraes; Zeyu Yang; Andre Manso; Meghana Reddy; Paolo Rosso; Roland Mathis",
        "authorids": "/m/mireia-hernandez-caralt/; /i/ivan-sekulic/; /f/filip-carevic/; /n/nghia-khau/; /d/diana-nicoleta-popa/; /b/bruna-guedes/; /v/victor-guimaraes/; /z/zeyu-yang/; /a/andre-manso/; /m/meghana-reddy/; /p/paolo-rosso/; /r/roland-mathis/",
        "bibtex": "@inproceedings{hernandez-caralt-etal-2025-stupid,\n    title = \"{\\textquotedblleft}Stupid robot, {I} want to speak to a human!{\\textquotedblright} User Frustration Detection in Task-Oriented Dialog Systems\",\n    author = \"Hernandez Caralt, Mireia  and\n      Sekulic, Ivan  and\n      Carevic, Filip  and\n      Khau, Nghia  and\n      Popa, Diana Nicoleta  and\n      Guedes, Bruna  and\n      Guimaraes, Victor  and\n      Yang, Zeyu  and\n      Manso, Andre  and\n      Reddy, Meghana  and\n      Rosso, Paolo  and\n      Mathis, Roland\",\n    editor = \"Rambow, Owen  and\n      Wanner, Leo  and\n      Apidianaki, Marianna  and\n      Al-Khalifa, Hend  and\n      Eugenio, Barbara Di  and\n      Schockaert, Steven  and\n      Darwish, Kareem  and\n      Agarwal, Apoorv\",\n    booktitle = \"Proceedings of the 31st International Conference on Computational Linguistics: Industry Track\",\n    month = jan,\n    year = \"2025\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.coling-industry.23/\",\n    pages = \"276--285\"\n}",
        "pdf": "https://aclanthology.org/2025.coling-industry.23.pdf",
        "site": "https://aclanthology.org/2025.coling-industry.23/",
        "pdf_size": 721884,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hQGx8U0jHJ0J:scholar.google.com/&scioq=%E2%80%9CStupid+robot,+I+want+to+speak+to+a+human!%E2%80%9D+User+Frustration+Detection+in+Task-Oriented+Dialog+Systems&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland; Telepathy Labs GmbH, Z\u00fcrich, Switzerland",
        "aff_domain": "telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai",
        "email": "telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai;telepathy.ai",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Telepathy Labs GmbH",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    }
]