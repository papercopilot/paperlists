[
    {
        "id": "2022.coling-1.36",
        "title": "A Closer Look at Few-Shot Out-of-Distribution Intent Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We consider few-shot out-of-distribution (OOD) intent detection, a practical and important problem for the development of task-oriented dialogue systems. Despite its importance, this problem is seldom studied in the literature, let alone examined in a systematic way. In this work, we take a closer look at this problem and identify key issues for research. In our pilot study, we reveal the reason why existing OOD intent detection methods are not adequate in dealing with this problem. Based on the observation, we propose a promising approach to tackle this problem based on latent representation generation and self-supervision. Comprehensive experiments on three real-world intent detection benchmark datasets demonstrate the high effectiveness of our proposed approach and its great potential in improving state-of-the-art methods for few-shot OOD intent detection.",
        "author": "Li-Ming Zhan; Haowen Liang; Lu Fan; Albert Y.S. Lam; Xiao-Ming Wu",
        "authorids": "/l/li-ming-zhan/; /h/haowen-liang/; /l/lu-fan/; /a/albert-y-s-lam/; /x/xiao-ming-wu/",
        "bibtex": "@inproceedings{zhan-etal-2022-closer,\n    title = \"A Closer Look at Few-Shot Out-of-Distribution Intent Detection\",\n    author = \"Zhan, Li-Ming  and\n      Liang, Haowen  and\n      Fan, Lu  and\n      Lam, Albert Y.S.  and\n      Wu, Xiao-Ming\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.36/\",\n    pages = \"451--460\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.36.pdf",
        "site": "https://aclanthology.org/2022.coling-1.36/",
        "pdf_size": 659381,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10284250396044624520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1; Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1; Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1; Fano Labs, Hong Kong S.A.R.2; Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1",
        "aff_domain": "connect.polyu.hk;connect.polyu.hk;comp.polyu.edu.hk;fano.ai;comp.polyu.edu.hk",
        "email": "connect.polyu.hk;connect.polyu.hk;comp.polyu.edu.hk;fano.ai;comp.polyu.edu.hk",
        "github": "https://github.com/liam0949/Few-shot-Intent-OOD",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "The Hong Kong Polytechnic University;Fano Labs",
        "aff_unique_dep": "Department of Computing;",
        "aff_unique_url": "https://www.polyu.edu.hk;https://www.fanolabs.com",
        "aff_unique_abbr": "PolyU;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.424",
        "title": "A Closer Look at Parameter Contributions When Training Neural Language and Translation Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We analyze the learning dynamics of neural language and translation models using Loss Change Allocation (LCA), an indicator that enables a fine-grained analysis of parameter updates when optimizing for the loss function. In other words, we can observe the contributions of different network components at training time. In this article, we systematically study masked language modeling, causal language modeling, and machine translation. We show that the choice of training objective leads to distinctive optimization procedures, even when performed on comparable Transformer architectures. We demonstrate how the various Transformer parameters are used during training, supporting that the feed-forward components of each layer are the main contributors to the optimization procedure. Finally, we find that the learning dynamics are not affected by data size and distribution but rather determined by the learning objective.",
        "author": "Ra\u00fal V\u00e1zquez; Hande Celikkanat; Vinit Ravishankar; Mathias Creutz; J\u00f6rg Tiedemann",
        "authorids": "/r/raul-vazquez/; /h/hande-celikkanat/; /v/vinit-ravishankar/; /m/mathias-creutz/; /j/jorg-tiedemann/",
        "bibtex": "@inproceedings{vazquez-etal-2022-closer,\n    title = \"A Closer Look at Parameter Contributions When Training Neural Language and Translation Models\",\n    author = {V{\\'a}zquez, Ra{\\'u}l  and\n      Celikkanat, Hande  and\n      Ravishankar, Vinit  and\n      Creutz, Mathias  and\n      Tiedemann, J{\\\"o}rg},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.424/\",\n    pages = \"4788--4800\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.424.pdf",
        "site": "https://aclanthology.org/2022.coling-1.424/",
        "pdf_size": 1802859,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14646035666132477484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Digital Humanities, University of Helsinki; Department of Digital Humanities, University of Helsinki; Language Technology Group, Department of Informatics, University of Oslo; Department of Digital Humanities, University of Helsinki; Department of Digital Humanities, University of Helsinki",
        "aff_domain": "helsinki.fi;helsinki.fi;ifi.uio.no;helsinki.fi;helsinki.fi",
        "email": "helsinki.fi;helsinki.fi;ifi.uio.no;helsinki.fi;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Helsinki;University of Oslo",
        "aff_unique_dep": "Department of Digital Humanities;Department of Informatics",
        "aff_unique_url": "https://www.helsinki.fi;https://www.uio.no",
        "aff_unique_abbr": "UH;UiO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Finland;Norway"
    },
    {
        "id": "2022.coling-1.230",
        "title": "A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing fake news detection methods aim to classify a piece of news as true or false and provide veracity explanations, achieving remarkable performances. However, they often tailor automated solutions on manual fact-checked reports, suffering from limited news coverage and debunking delays. When a piece of news has not yet been fact-checked or debunked, certain amounts of relevant raw reports are usually disseminated on various media outlets, containing the wisdom of crowds to verify the news claim and explain its verdict. In this paper, we propose a novel Coarse-to-fine Cascaded Evidence-Distillation (CofCED) neural network for explainable fake news detection based on such raw reports, alleviating the dependency on fact-checked ones. Specifically, we first utilize a hierarchical encoder for web text representation, and then develop two cascaded selectors to select the most explainable sentences for verdicts on top of the selected top-K reports in a coarse-to-fine manner. Besides, we construct two explainable fake news datasets, which is publicly available. Experimental results demonstrate that our model significantly outperforms state-of-the-art detection baselines and generates high-quality explanations from diverse evaluation perspectives.",
        "author": "Zhiwei Yang; Jing Ma; Hechang Chen; Hongzhan Lin; Ziyang Luo; Yi Chang",
        "authorids": "/z/zhiwei-yang/; /j/jing-ma/; /h/hechang-chen/; /h/hongzhan-lin/; /z/ziyang-luo/; /y/yi-chang/",
        "bibtex": "@inproceedings{yang-etal-2022-coarse,\n    title = \"A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection\",\n    author = \"Yang, Zhiwei  and\n      Ma, Jing  and\n      Chen, Hechang  and\n      Lin, Hongzhan  and\n      Luo, Ziyang  and\n      Chang, Yi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.230/\",\n    pages = \"2608--2621\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.230.pdf",
        "site": "https://aclanthology.org/2022.coling-1.230/",
        "pdf_size": 5791158,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7088559435111706849&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "College of Computer Science and Technology, Jilin University, Changchun, China+Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education; Department of Computer Science, Hong Kong Baptist University, Hong Kong, China; School of Artificial Intelligence, Jilin University, China+Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education; Department of Computer Science, Hong Kong Baptist University, Hong Kong, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong, China; School of Artificial Intelligence, Jilin University, China+International Center of Future Science, Jilin University, China+Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education",
        "aff_domain": "mails.jlu.edu.cn;comp.hkbu.edu.hk;jlu.edu.cn;comp.hkbu.edu.hk;comp.hkbu.edu.hk;jlu.edu.cn",
        "email": "mails.jlu.edu.cn;comp.hkbu.edu.hk;jlu.edu.cn;comp.hkbu.edu.hk;comp.hkbu.edu.hk;jlu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;0+1;2;2;0+0+1",
        "aff_unique_norm": "Jilin University;Key Laboratory of Symbolic Computation and Knowledge Engineering;Hong Kong Baptist University",
        "aff_unique_dep": "College of Computer Science and Technology;Ministry of Education;Department of Computer Science",
        "aff_unique_url": "http://www.jlu.edu.cn;;https://www.hkbu.edu.hk",
        "aff_unique_abbr": "JLU;;HKBU",
        "aff_campus_unique_index": "0;2;;2;2;",
        "aff_campus_unique": "Changchun;;Hong Kong",
        "aff_country_unique_index": "0+0;0;0+0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.581",
        "title": "A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task, which focuses on detecting the sentiment polarity towards the aspect in a sentence. However, it is always sensitive to the multi-aspect challenge, where features of multiple aspects in a sentence will affect each other. To mitigate this issue, we design a novel training framework, called Contrastive Cross-Channel Data Augmentation (C3 DA), which leverages an in-domain generator to construct more multi-aspect samples and then boosts the robustness of ABSA models via contrastive learning on these generated data. In practice, given a generative pretrained language model and some limited ABSA labeled data, we first employ some parameter-efficient approaches to perform the in-domain fine-tuning. Then, the obtained in-domain generator is used to generate the synthetic sentences from two channels, i.e., Aspect Augmentation Channel and Polarity Augmentation Channel, which generate the sentence condition on a given aspect and polarity respectively. Specifically, our C3 DA performs the sentence generation in a cross-channel manner to obtain more sentences, and proposes an Entropy-Minimization Filter to filter low-quality generated samples. Extensive experiments show that our C3 DA can outperform those baselines without any augmentations by about 1% on accuracy and Macro- F1. Code and data are released in https://github.com/wangbing1416/C3DA.",
        "author": "Bing Wang; Liang Ding; Qihuang Zhong; Ximing Li; Dacheng Tao",
        "authorids": "/b/bing-wang/; /l/liang-ding/; /q/qihuang-zhong/; /x/ximing-li/; /d/dacheng-tao/",
        "bibtex": "@inproceedings{wang-etal-2022-contrastive,\n    title = \"A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis\",\n    author = \"Wang, Bing  and\n      Ding, Liang  and\n      Zhong, Qihuang  and\n      Li, Ximing  and\n      Tao, Dacheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.581/\",\n    pages = \"6691--6704\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.581.pdf",
        "site": "https://aclanthology.org/2022.coling-1.581/",
        "pdf_size": 860049,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12502490996493401670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science and Technology, Jilin University+JD Explore Academy; JD Explore Academy+The University of Sydney; Wuhan University; College of Computer Science and Technology, Jilin University; JD Explore Academy+The University of Sydney",
        "aff_domain": "jd.com;gmail.com; ; ; ",
        "email": "jd.com;gmail.com; ; ; ",
        "github": "https://github.com/wangbing1416/C3DA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1+2;3;0;1+2",
        "aff_unique_norm": "Jilin University;JD Explore Academy;University of Sydney;Wuhan University",
        "aff_unique_dep": "College of Computer Science and Technology;;;",
        "aff_unique_url": "http://www.jlu.edu.cn;;https://www.sydney.edu.au;http://www.whu.edu.cn/",
        "aff_unique_abbr": "JLU;;USYD;WHU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;0;0;2",
        "aff_country_unique": "China;;Australia"
    },
    {
        "id": "2022.coling-1.327",
        "title": "A Data-driven Approach to Named Entity Recognition for Early Modern French",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Named entity recognition has become an increasingly useful tool for digital humanities research, specially when it comes to historical texts. However, historical texts pose a wide range of challenges to both named entity recognition and natural language processing in general that are still difficult to address even with modern neural methods. In this article we focus in named entity recognition for historical French, and in particular for Early Modern French (16th-18th c.), i.e. Ancien R\u00e9gime French. However, instead of developing a specialised architecture to tackle the particularities of this state of language, we opt for a data-driven approach by developing a new corpus with fine-grained entity annotation, covering three centuries of literature corresponding to the early modern period; we try to annotate as much data as possible producing a corpus that is many times bigger than the most popular NER evaluation corpora for both Contemporary English and French. We then fine-tune existing state-of-the-art architectures for Early Modern and Contemporary French, obtaining results that are on par with those of the current state-of-the-art NER systems for Contemporary English. Both the corpus and the fine-tuned models are released.",
        "author": "Pedro Ortiz Suarez; Simon Gabay",
        "authorids": "/p/pedro-ortiz-suarez/; /s/simon-gabay/",
        "bibtex": "@inproceedings{ortiz-suarez-gabay-2022-data,\n    title = \"A Data-driven Approach to Named Entity Recognition for Early {M}odern {F}rench\",\n    author = \"Ortiz Suarez, Pedro  and\n      Gabay, Simon\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.327/\",\n    pages = \"3722--3730\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.327.pdf",
        "site": "https://aclanthology.org/2022.coling-1.327/",
        "pdf_size": 373700,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17945819757418977928&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Data and Web Science Group, University of Mannheim; Universit\u00e9 de Gen\u00e8ve",
        "aff_domain": "uni-mannheim.de;unige.ch",
        "email": "uni-mannheim.de;unige.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Mannheim;University of Geneva",
        "aff_unique_dep": "Data and Web Science Group;",
        "aff_unique_url": "https://www.uni-mannheim.de;https://www.unige.ch",
        "aff_unique_abbr": ";UNIGE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "2022.coling-1.76",
        "title": "A Distance-Aware Multi-Task Framework for Conversational Discourse Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational discourse parsing aims to construct an implicit utterance dependency tree to reflect the turn-taking in a multi-party conversation. Existing works are generally divided into two lines: graph-based and transition-based paradigms, which perform well for short-distance and long-distance dependency links, respectively. However, there is no study to consider the advantages of both paradigms to facilitate conversational discourse parsing. As a result, we propose a distance-aware multi-task framework DAMT that incorporates the strengths of transition-based paradigm to facilitate the graph-based paradigm from the encoding and decoding process. To promote multi-task learning on two paradigms, we first introduce an Encoding Interactive Module (EIM) to enhance the flow of semantic information between both two paradigms during the encoding step. And then we apply a Distance-Aware Graph Convolutional Network (DAGCN) in the decoding process, which can incorporate the different-distance dependency links predicted by the transition-based paradigm to facilitate the decoding of the graph-based paradigm. The experimental results on the datasets STAC and Molweni show that our method can significantly improve the performance of the SOTA graph-based paradigm on long-distance dependency links.",
        "author": "Yaxin Fan; Peifeng Li; Fang Kong; Qiaoming Zhu",
        "authorids": "/y/yaxin-fan/; /p/peifeng-li/; /f/fang-kong/; /q/qiaoming-zhu/",
        "bibtex": "https://aclanthology.org/2022.coling-1.76.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.76.pdf",
        "site": "https://aclanthology.org/2022.coling-1.76/",
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14506979960340361469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.85",
        "title": "A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a biomedical knowledge enhanced pre-trained language model for medicinal product vertical search. Following ELECTRA\u2019s replaced token detection (RTD) pre-training, we leverage biomedical entity masking (EM) strategy to learn better contextual word representations. Furthermore, we propose a novel pre-training task, product attribute prediction (PAP), to inject product knowledge into the pre-trained language model efficiently by leveraging medicinal product databases directly. By sharing the parameters of PAP\u2019s transformer encoder with that of RTD\u2019s main transformer, these two pre-training tasks are jointly learned. Experiments demonstrate the effectiveness of PAP task for pre-trained language model on medicinal product vertical search scenario, which includes query-title relevance, query intent classification, and named entity recognition in query.",
        "author": "Kesong Liu; Jianhui Jiang; Feifei Lyu",
        "authorids": "/k/kesong-liu/; /j/jianhui-jiang/; /f/feifei-lyu/",
        "bibtex": "@inproceedings{liu-etal-2022-domain,\n    title = \"A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products\",\n    author = \"Liu, Kesong  and\n      Jiang, Jianhui  and\n      Lyu, Feifei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.85/\",\n    pages = \"1014--1023\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.85.pdf",
        "site": "https://aclanthology.org/2022.coling-1.85/",
        "pdf_size": 740860,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6590810862392990175&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.442",
        "title": "A Generalized Method for Automated Multilingual Loanword Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Loanwords are words incorporated from one language into another without translation. Suppose two words from distantly-related or unrelated languages sound similar and have a similar meaning. In that case, this is evidence of likely borrowing. This paper presents a method to automatically detect loanwords across various language pairs, accounting for differences in script, pronunciation and phonetic transformation by the borrowing language. We incorporate edit distance, semantic similarity measures, and phonetic alignment. We evaluate on 12 language pairs and achieve performance comparable to or exceeding state of the art methods on single-pair loanword detection tasks. We also demonstrate that multilingual models perform the same or often better than models trained on single language pairs and can potentially generalize to unseen language pairs with sufficient data, and that our method can exceed human performance on loanword detection.",
        "author": "Abhijnan Nath; Sina Mahdipour Saravani; Ibrahim Khebour; Sheikh Mannan; Zihui Li; Nikhil Krishnaswamy",
        "authorids": "/a/abhijnan-nath/; /s/sina-mahdipour-saravani/; /i/ibrahim-khebour/; /s/sheikh-mannan/; /z/zihui-li/; /n/nikhil-krishnaswamy/",
        "bibtex": "@inproceedings{nath-etal-2022-generalized,\n    title = \"A Generalized Method for Automated Multilingual Loanword Detection\",\n    author = \"Nath, Abhijnan  and\n      Mahdipour Saravani, Sina  and\n      Khebour, Ibrahim  and\n      Mannan, Sheikh  and\n      Li, Zihui  and\n      Krishnaswamy, Nikhil\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.442/\",\n    pages = \"4996--5013\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.442.pdf",
        "site": "https://aclanthology.org/2022.coling-1.442/",
        "pdf_size": 927596,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=631894556108682990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University+Colorado State University; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University",
        "aff_domain": "colostate.edu;cs.utah.edu;colostate.edu;colostate.edu;colostate.edu;colostate.edu",
        "email": "colostate.edu;cs.utah.edu;colostate.edu;colostate.edu;colostate.edu;colostate.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+0;0;0;0;0",
        "aff_unique_norm": "Colorado State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colostate.edu",
        "aff_unique_abbr": "CSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.611",
        "title": "A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, some span-based methods have achieved encouraging performances for joint aspect-sentiment analysis, which first extract aspects (aspect extraction) by detecting aspect boundaries and then classify the span-level sentiments (sentiment classification). However, most existing approaches either sequentially extract task-specific features, leading to insufficient feature interactions, or they encode aspect features and sentiment features in a parallel manner, implying that feature representation in each task is largely independent of each other except for input sharing. Both of them ignore the internal correlations between the aspect extraction and sentiment classification. To solve this problem, we novelly propose a hierarchical interactive network (HI-ASA) to model two-way interactions between two tasks appropriately, where the hierarchical interactions involve two steps: shallow-level interaction and deep-level interaction. First, we utilize cross-stitch mechanism to combine the different task-specific features selectively as the input to ensure proper two-way interactions. Second, the mutual information technique is applied to mutually constrain learning between two tasks in the output layer, thus the aspect input and the sentiment input are capable of encoding features of the other task via backpropagation. Extensive experiments on three real-world datasets demonstrate HI-ASA\u2019s superiority over baselines.",
        "author": "Wei Chen; Jinglong Du; Zhao Zhang; Fuzhen Zhuang; Zhongshi He",
        "authorids": "/w/wei-chen/; /j/jinglong-du/; /z/zhao-zhang/; /f/fuzhen-zhuang/; /z/zhongshi-he/",
        "bibtex": "@inproceedings{chen-etal-2022-hierarchical,\n    title = \"A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis\",\n    author = \"Chen, Wei  and\n      Du, Jinglong  and\n      Zhang, Zhao  and\n      Zhuang, Fuzhen  and\n      He, Zhongshi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.611/\",\n    pages = \"7013--7019\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.611.pdf",
        "site": "https://aclanthology.org/2022.coling-1.611/",
        "pdf_size": 593910,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3950585632784885753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science, Chongqing University, Chongqing, China+Institute of Artificial Intelligence, Beihang University, Beijing 100191, China; College of Medical Informatics, Chongqing Medical University, Chongqing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLSDE, School of Computer Science, Beihang University, Beijing 100191, China+Institute of Artificial Intelligence, Beihang University, Beijing 100191, China; College of Computer Science, Chongqing University, Chongqing, China+Institute of Artificial Intelligence, Beihang University, Beijing 100191, China",
        "aff_domain": "cqu.edu.cn;cqu.edu.cn;buaa.edu.cn; ; ",
        "email": "cqu.edu.cn;cqu.edu.cn;buaa.edu.cn; ; ",
        "github": "https://github.com/cwei01/HI-ASA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;3;1+1;0+1",
        "aff_unique_norm": "Chongqing University;Beihang University;Chongqing Medical University;Chinese Academy of Sciences",
        "aff_unique_dep": "College of Computer Science;Institute of Artificial Intelligence;College of Medical Informatics;Institute of Computing Technology",
        "aff_unique_url": "http://en.cqu.edu.cn/;http://www.buaa.edu.cn;http://www.cqmu.edu.cn;http://www.ict.ac.cn",
        "aff_unique_abbr": "CQU;BUAA;CQMU;CAS",
        "aff_campus_unique_index": "0+1;0;1;1+1;0+1",
        "aff_campus_unique": "Chongqing;Beijing",
        "aff_country_unique_index": "0+0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.166",
        "title": "A Hybrid Model of Classification and Generation for Spatial Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Extracting spatial relations from texts is a fundamental task for natural language understanding and previous studies only regard it as a classification task, ignoring those spatial relations with null roles due to their poor information. To address the above issue, we first view spatial relation extraction as a generation task and propose a novel hybrid model HMCGR for this task. HMCGR contains a generation and a classification model, while the former can generate those null-role relations and the latter can extract those non-null-role relations to complement each other. Moreover, a reflexivity evaluation mechanism is applied to further improve the accuracy based on the reflexivity principle of spatial relation. Experimental results on SpaceEval show that HMCGR outperforms the SOTA baselines significantly.",
        "author": "Feng Wang; Peifeng Li; Qiaoming Zhu",
        "authorids": "/f/feng-wang/; /p/peifeng-li/; /q/qiaoming-zhu/",
        "bibtex": "@inproceedings{wang-etal-2022-hybrid,\n    title = \"A Hybrid Model of Classification and Generation for Spatial Relation Extraction\",\n    author = \"Wang, Feng  and\n      Li, Peifeng  and\n      Zhu, Qiaoming\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.166/\",\n    pages = \"1915--1924\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.166.pdf",
        "site": "https://aclanthology.org/2022.coling-1.166/",
        "pdf_size": 338674,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8748491828413447227&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.45",
        "title": "A Multi-Dimensional, Cross-Domain and Hierarchy-Aware Neural Architecture for ISO-Standard Dialogue Act Tagging",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dialogue Act tagging with the ISO 24617-2 standard is a difficult task that involves multi-label text classification across a diverse set of labels covering semantic, syntactic and pragmatic aspects of dialogue. The lack of an adequately sized training set annotated with this standard is a major problem when using the standard in practice. In this work we propose a neural architecture to increase classification accuracy, especially on low-frequency fine-grained tags. Our model takes advantage of the hierarchical structure of the ISO taxonomy and utilises syntactic information in the form of Part-Of-Speech and dependency tags, in addition to contextual information from previous turns. We train our architecture on an aggregated corpus of conversations from different domains, which provides a variety of dialogue interactions and linguistic registers. Our approach achieves state-of-the-art tagging results on the DialogBank benchmark data set, providing empirical evidence that this architecture can successfully generalise to different domains.",
        "author": "Stefano Mezza; Wayne Wobcke; Alan Blair",
        "authorids": "/s/stefano-mezza/; /w/wayne-wobcke/; /a/alan-blair/",
        "bibtex": "@inproceedings{mezza-etal-2022-multi,\n    title = \"A Multi-Dimensional, Cross-Domain and Hierarchy-Aware Neural Architecture for {ISO}-Standard Dialogue Act Tagging\",\n    author = \"Mezza, Stefano  and\n      Wobcke, Wayne  and\n      Blair, Alan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.45/\",\n    pages = \"542--552\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.45.pdf",
        "site": "https://aclanthology.org/2022.coling-1.45/",
        "pdf_size": 536427,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=563648011891140147&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, University of New South Wales, Sydney NSW 2052, Australia; School of Computer Science and Engineering, University of New South Wales, Sydney NSW 2052, Australia; School of Computer Science and Engineering, University of New South Wales, Sydney NSW 2052, Australia",
        "aff_domain": "unsw.edu.au;unsw.edu.au;unsw.edu.au",
        "email": "unsw.edu.au;unsw.edu.au;unsw.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of New South Wales",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.unsw.edu.au",
        "aff_unique_abbr": "UNSW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Sydney",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.coling-1.173",
        "title": "A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event argument extraction (EAE) aims to extract arguments with given roles from texts, which have been widely studied in natural language processing. Most previous works have achieved good performance in specific EAE datasets with dedicated neural architectures. Whereas, these architectures are usually difficult to adapt to new datasets/scenarios with various annotation schemas or formats. Furthermore, they rely on large-scale labeled data for training, which is unavailable due to the high labelling cost in most cases. In this paper, we propose a multi-format transfer learning model with variational information bottleneck, which makes use of the information especially the common knowledge in existing datasets for EAE in new datasets. Specifically, we introduce a shared-specific prompt framework to learn both format-shared and format-specific knowledge from datasets with different formats. In order to further absorb the common knowledge for EAE and eliminate the irrelevant noise, we integrate variational information bottleneck into our architecture to refine the shared representation. We conduct extensive experiments on three benchmark datasets, and obtain new state-of-the-art performance on EAE.",
        "author": "Jie Zhou; Qi Zhang; Qin Chen; Qi Zhang; Liang He; Xuanjing Huang",
        "authorids": "/j/jie-zhou/; /q/qi-zhang/; /q/qin-chen/; /q/qi-zhang/; /l/liang-he/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{zhou-etal-2022-multi,\n    title = \"A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck\",\n    author = \"Zhou, Jie  and\n      Zhang, Qi  and\n      Chen, Qin  and\n      Zhang, Qi  and\n      He, Liang  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.173/\",\n    pages = \"1990--2000\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.173.pdf",
        "site": "https://aclanthology.org/2022.coling-1.173/",
        "pdf_size": 897521,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1360067299908222470&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, Fudan Univerisity, Shanghai, China+School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science, Fudan Univerisity, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science, Fudan Univerisity, Shanghai, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;cs.ecnu.edu.cn;stu.ecnu.edu.cn;cs.ecnu.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;cs.ecnu.edu.cn;stu.ecnu.edu.cn;cs.ecnu.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;0;1;0",
        "aff_unique_norm": "Fudan University;East China Normal University",
        "aff_unique_dep": "School of Computer Science;School of Computer Science and Technology",
        "aff_unique_url": "https://www.fudan.edu.cn;http://www.ecnu.edu.cn",
        "aff_unique_abbr": "Fudan;ECNU",
        "aff_campus_unique_index": "0+0;0;0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.616",
        "title": "A Multi-Task Dual-Tree Network for Aspect Sentiment Triplet Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims at extracting triplets from a given sentence, where each triplet includes an aspect, its sentiment polarity, and a corresponding opinion explaining the polarity. Existing methods are poor at detecting complicated relations between aspects and opinions as well as classifying multiple sentiment polarities in a sentence. Detecting unclear boundaries of multi-word aspects and opinions is also a challenge. In this paper, we propose a Multi-Task Dual-Tree Network (MTDTN) to address these issues. We employ a constituency tree and a modified dependency tree in two sub-tasks of Aspect Opinion Co-Extraction (AOCE) and ASTE, respectively. To enhance the information interaction between the two sub-tasks, we further design a Transition-Based Inference Strategy (TBIS) that transfers the boundary information from tags of AOCE to ASTE through a transition matrix. Extensive experiments are conducted on four popular datasets, and the results show the effectiveness of our model.",
        "author": "Yichun Zhao; Kui Meng; Gongshen Liu; Jintao Du; Huijia Zhu",
        "authorids": "/y/yichun-zhao/; /k/kui-meng/; /g/gongshen-liu/; /j/jintao-du/; /h/huijia-zhu/",
        "bibtex": "@inproceedings{zhao-etal-2022-multi,\n    title = \"A Multi-Task Dual-Tree Network for Aspect Sentiment Triplet Extraction\",\n    author = \"Zhao, Yichun  and\n      Meng, Kui  and\n      Liu, Gongshen  and\n      Du, Jintao  and\n      Zhu, Huijia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.616/\",\n    pages = \"7065--7074\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.616.pdf",
        "site": "https://aclanthology.org/2022.coling-1.616/",
        "pdf_size": 568191,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7818653206451179559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Ant Group Co., Ltd.; Ant Group Co., Ltd.",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;antgroup.com;antgroup.com",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;antgroup.com;antgroup.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Ant Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.antgroup.com",
        "aff_unique_abbr": "SJTU;Ant Group",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.584",
        "title": "A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause analysis, which extracts potential emotion-cause pairs from an emotional document. Most recent studies use end-to-end methods to tackle the ECPE task. However, these methods either suffer from a label sparsity problem or fail to model complicated relations between emotions and causes. Furthermore, they all do not consider explicit semantic information of clauses. To this end, we transform the ECPE task into a document-level machine reading comprehension (MRC) task and propose a Multi-turn MRC framework with Rethink mechanism (MM-R). Our framework can model complicated relations between emotions and causes while avoiding generating the pairing matrix (the leading cause of the label sparsity problem). Besides, the multi-turn structure can fuse explicit semantic information flow between emotions and causes. Extensive experiments on the benchmark emotion cause corpus demonstrate the effectiveness of our proposed framework, which outperforms existing state-of-the-art methods.",
        "author": "Changzhi Zhou; Dandan Song; Jing Xu; Zhijing Wu",
        "authorids": "/c/changzhi-zhou/; /d/dandan-song/; /j/jing-xu/; /z/zhijing-wu/",
        "bibtex": "@inproceedings{zhou-etal-2022-multi-turn,\n    title = \"A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction\",\n    author = \"Zhou, Changzhi  and\n      Song, Dandan  and\n      Xu, Jing  and\n      Wu, Zhijing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.584/\",\n    pages = \"6726--6735\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.584.pdf",
        "site": "https://aclanthology.org/2022.coling-1.584/",
        "pdf_size": 659243,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5143092752645063388&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science and Technology, Southeast Academy of Information Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Southeast Academy of Information Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Southeast Academy of Information Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Southeast Academy of Information Technology, Beijing Institute of Technology, Beijing, China",
        "aff_domain": "163.com;bit.edu.cn;bit.edu.cn;gmail.com",
        "email": "163.com;bit.edu.cn;bit.edu.cn;gmail.com",
        "github": "https://github.com/zhoucz97/ECPE-MM-R",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.bit.edu.cn",
        "aff_unique_abbr": "BIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.326",
        "title": "A New Public Corpus for Clinical Section Identification: MedSecId",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The process by which sections in a document are demarcated and labeled is known as section identification. Such sections are helpful to the reader when searching for information and contextualizing specific topics. The goal of this work is to segment the sections of clinical medical domain documentation. The primary contribution of this work is MedSecId, a publicly available set of 2,002 fully annotated medical notes from the MIMIC-III. We include several baselines, source code, a pretrained model and analysis of the data showing a relationship between medical concepts across sections using principal component analysis.",
        "author": "Paul Landes; Kunal Patel; Sean S. Huang; Adam Webb; Barbara Di Eugenio; Cornelia Caragea",
        "authorids": "/p/paul-landes/; /k/kunal-patel/; /s/sean-s-huang/; /a/adam-webb/; /b/barbara-di-eugenio/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{landes-etal-2022-new,\n    title = \"A New Public Corpus for Clinical Section Identification: {M}ed{S}ec{I}d\",\n    author = \"Landes, Paul  and\n      Patel, Kunal  and\n      Huang, Sean S.  and\n      Webb, Adam  and\n      Di Eugenio, Barbara  and\n      Caragea, Cornelia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.326/\",\n    pages = \"3709--3721\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.326.pdf",
        "site": "https://aclanthology.org/2022.coling-1.326/",
        "pdf_size": 1311823,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1898812151463759040&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Illinois at Chicago; Department of Emergency Medicine, University of Illinois at Chicago; Department of Internal Medicine and Geriatrics, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu;uic.edu;uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu;uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.358",
        "title": "A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type Identification in Sanskrit",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The phenomenon of compounding is ubiquitous in Sanskrit. It serves for achieving brevity in expressing thoughts, while simultaneously enriching the lexical and structural formation of the language. In this work, we focus on the Sanskrit Compound Type Identification (SaCTI) task, where we consider the problem of identifying semantic relations between the components of a compound word. Earlier approaches solely rely on the lexical information obtained from the components and ignore the most crucial contextual and syntactic information useful for SaCTI. However, the SaCTI task is challenging primarily due to the implicitly encoded context-sensitive semantic relation between the compound components. Thus, we propose a novel multi-task learning architecture which incorporates the contextual information and enriches the complementary syntactic information using morphological tagging and dependency parsing as two auxiliary tasks. Experiments on the benchmark datasets for SaCTI show 6.1 points (Accuracy) and 7.7 points (F1-score) absolute gain compared to the state-of-the-art system. Further, our multi-lingual experiments demonstrate the efficacy of the proposed architecture in English and Marathi languages.",
        "author": "Jivnesh Sandhan; Ashish Gupta; Hrishikesh Terdalkar; Tushar Sandhan; Suvendu Samanta; Laxmidhar Behera; Pawan Goyal",
        "authorids": "/j/jivnesh-sandhan/; /a/ashish-gupta/; /h/hrishikesh-terdalkar/; /t/tushar-sandhan/; /s/suvendu-samanta/; /l/laxmidhar-behera/; /p/pawan-goyal/",
        "bibtex": "@inproceedings{sandhan-etal-2022-novel,\n    title = \"A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type Identification in {S}anskrit\",\n    author = \"Sandhan, Jivnesh  and\n      Gupta, Ashish  and\n      Terdalkar, Hrishikesh  and\n      Sandhan, Tushar  and\n      Samanta, Suvendu  and\n      Behera, Laxmidhar  and\n      Goyal, Pawan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.358/\",\n    pages = \"4071--4083\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.358.pdf",
        "site": "https://aclanthology.org/2022.coling-1.358/",
        "pdf_size": 833278,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17879180880066176148&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "IIT Kanpur; IIT Kharagpur; IIT Kanpur; IIT Kanpur; IIT Kanpur; IIT Kanpur + IIT Mandi; IIT Kharagpur",
        "aff_domain": "iitk.ac.in;gmail.com; ; ; ; ;cse.iitkgp.ac.in",
        "email": "iitk.ac.in;gmail.com; ; ; ; ;cse.iitkgp.ac.in",
        "github": "https://github.com/ashishgupta2598/SaCTI",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;0;0;0+2;1",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Indian Institute of Technology Kharagpur;Indian Institute of Technology Mandi",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.iitkgp.ac.in;https://www.iitmandi.ac.in",
        "aff_unique_abbr": "IITK;IIT KGP;IIT Mandi",
        "aff_campus_unique_index": "0;1;0;0;0;0+2;1",
        "aff_campus_unique": "Kanpur;Kharagpur;Mandi",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.29",
        "title": "A Personalized Dialogue Generator with Implicit User Persona Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current works in the generation of personalized dialogue primarily contribute to the agent presenting a consistent personality and driving a more informative response. However, we found that the generated responses from most previous models tend to be self-centered, with little care for the user in the dialogue. Moreover, we consider that human-like conversation is essentially built based on inferring information about the persona of the other party. Motivated by this, we propose a novel personalized dialogue generator by detecting an implicit user persona. Because it is hard to collect a large number of detailed personas for each user, we attempted to model the user\u2019s potential persona and its representation from dialogue history, with no external knowledge. The perception and fader variables were conceived using conditional variational inference. The two latent variables simulate the process of people being aware of each other\u2019s persona and producing a corresponding expression in conversation. Finally, posterior-discriminated regularization was presented to enhance the training procedure. Empirical studies demonstrate that, compared to state-of-the-art methods, our approach is more concerned with the user\u2019s persona and achieves a considerable boost across both automatic metrics and human evaluations.",
        "author": "Itsugun Cho; Dongyang Wang; Ryota Takahashi; Hiroaki Saito",
        "authorids": "/i/itsugun-cho/; /d/dongyang-wang/; /r/ryota-takahashi/; /h/hiroaki-saito/",
        "bibtex": "@inproceedings{cho-etal-2022-personalized,\n    title = \"A Personalized Dialogue Generator with Implicit User Persona Detection\",\n    author = \"Cho, Itsugun  and\n      Wang, Dongyang  and\n      Takahashi, Ryota  and\n      Saito, Hiroaki\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.29/\",\n    pages = \"367--377\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.29.pdf",
        "site": "https://aclanthology.org/2022.coling-1.29/",
        "pdf_size": 1474208,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12018441669177347160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Keio University, Japan; Keio University, Japan; Keio University, Japan; Keio University, Japan",
        "aff_domain": "keio.jp;keio.jp;keio.jp;ics.keio.ac.jp",
        "email": "keio.jp;keio.jp;keio.jp;ics.keio.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Keio University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.keio.ac.jp",
        "aff_unique_abbr": "Keio",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.242",
        "title": "A Progressive Framework for Role-Aware Rumor Resolution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing works on rumor resolution have shown great potential in recognizing word appearance and user participation. However, they ignore the intrinsic propagation mechanisms of rumors and present poor adaptive ability when unprecedented news emerges. To exploit the fine-grained rumor diffusion patterns and generalize rumor resolution methods, we formulate a predecessor task to identify triggering posts, and then exploit their characteristics to facilitate rumor verification. We design a tree-structured annotation interface and extend PHEME dataset with labels on the message level. Data analysis shows that triggers play a critical role in verifying rumors and present similar lingual patterns across irrelevant events. We propose a graph-based model considering the direction and interaction of information flow to implement role-aware rumor resolution. Experimental results demonstrate the effectiveness of our proposed model and progressive scheme.",
        "author": "Lei Chen; Guanying Li; Zhongyu Wei; Yang Yang; Baohua Zhou; Qi Zhang; Xuanjing Huang",
        "authorids": "/l/lei-chen/; /g/guanying-li/; /z/zhongyu-wei/; /y/yang-yang/; /b/baohua-zhou/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{chen-etal-2022-progressive,\n    title = \"A Progressive Framework for Role-Aware Rumor Resolution\",\n    author = \"Chen, Lei  and\n      Li, Guanying  and\n      Wei, Zhongyu  and\n      Yang, Yang  and\n      Zhou, Baohua  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.242/\",\n    pages = \"2748--2758\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.242.pdf",
        "site": "https://aclanthology.org/2022.coling-1.242/",
        "pdf_size": 780795,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12418502415601924832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.203",
        "title": "A Relation Extraction Dataset for Knowledge Extraction from Web Tables",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Relational web-tables are significant sources of structural information that are widely used for relation extraction and population of facts into knowledge graphs. To transform the web-table data into knowledge, we need to identify the relations that exist between column pairs. Currently, there are only a handful of publicly available datasets with relations annotated against natural web-tables. Most datasets are constructed using synthetic tables that lack valuable metadata information, or are limited in size to be considered as a challenging evaluation set. In this paper, we present REDTab, the largest natural-table relation extraction dataset. We have annotated ~9K tables and ~22K column pairs using crowd sourced annotators from MTurk, which has 50x larger number of column pairs than the existing human-annotated benchmark. Our test set is specially designed to be challenging as observed in our experiment results using TaBERT. We publicly release REDTab as a benchmark for the evaluation process in relation extraction.",
        "author": "Siffi Singh; Alham Fikri Aji; Gaurav Singh; Christos Christodoulopoulos",
        "authorids": "/s/siffi-singh/; /a/alham-fikri-aji/; /g/gaurav-singh-tomar/; /c/christos-christodoulopoulos/",
        "bibtex": "@inproceedings{singh-etal-2022-relation,\n    title = \"A Relation Extraction Dataset for Knowledge Extraction from Web Tables\",\n    author = \"Singh, Siffi  and\n      Aji, Alham Fikri  and\n      Singh, Gaurav  and\n      Christodoulopoulos, Christos\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.203/\",\n    pages = \"2319--2327\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.203.pdf",
        "site": "https://aclanthology.org/2022.coling-1.203/",
        "pdf_size": 2089198,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13585814876875508056&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.587",
        "title": "A Sentiment and Emotion Aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we hypothesize that humor is closely related to sentiment and emotions. Also, due to the tremendous growth in multilingual content, there is a great demand for building models and systems that support multilingual information access. To end this, we first extend the recently released Multimodal Multiparty Hindi Humor (M2H2) dataset by adding parallel English utterances corresponding to Hindi utterances and then annotating each utterance with sentiment and emotion classes. We name it Sentiment, Humor, and Emotion aware Multilingual Multimodal Multiparty Dataset (SHEMuD). Therefore, we propose a multitask framework wherein the primary task is humor detection, and the auxiliary tasks are sentiment and emotion identification. We design a multitasking framework wherein we first propose a Context Transformer to capture the deep contextual relationships with the input utterances. We then propose a Sentiment and Emotion aware Embedding (SE-Embedding) to get the overall representation of a particular emotion and sentiment w.r.t. the specific humor situation. Experimental results on the SHEMuD show the efficacy of our approach and shows that multitask learning offers an improvement over the single-task framework for both monolingual (4.86 points in Hindi and 5.9 points in English in F1-score) and multilingual (5.17 points in F1-score) setting.",
        "author": "Dushyant Singh Chauhan; Gopendra Vikram Singh; Aseem Arora; Asif Ekbal; Pushpak Bhattacharyya",
        "authorids": "/d/dushyant-singh-chauhan/; /g/gopendra-vikram-singh/; /a/aseem-arora/; /a/asif-ekbal/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{chauhan-etal-2022-sentiment,\n    title = \"A Sentiment and Emotion Aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting\",\n    author = \"Chauhan, Dushyant Singh  and\n      Singh, Gopendra Vikram  and\n      Arora, Aseem  and\n      Ekbal, Asif  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.587/\",\n    pages = \"6752--6761\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.587.pdf",
        "site": "https://aclanthology.org/2022.coling-1.587/",
        "pdf_size": 1495145,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=365677065430992064&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Indian Institute of Technology Patna; Indian Institute of Technology Patna; Indian Institute of Technology Patna; Indian Institute of Technology Patna; Indian Institute of Technology Bombay",
        "aff_domain": "iitp.ac.in;iitp.ac.in;iitp.ac.in;iitp.ac.in;cse.iitb.ac.in",
        "email": "iitp.ac.in;iitp.ac.in;iitp.ac.in;iitp.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Indian Institute of Technology Patna;Indian Institute of Technology Bombay",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitp.ac.in;https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Patna;IIT Bombay",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Patna;Bombay",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.407",
        "title": "A Simple Log-based Loss Function for Ordinal Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The cross-entropy loss function is widely used and generally considered the default loss function for text classification. When it comes to ordinal text classification where there is an ordinal relationship between labels, the cross-entropy is not optimal as it does not incorporate the ordinal character into its feedback. In this paper, we propose a new simple loss function called ordinal log-loss (OLL). We show that this loss function outperforms state-of-the-art previously introduced losses on four benchmark text classification datasets.",
        "author": "Fran\u00e7ois Castagnos; Martin Mihelich; Charles Dognin",
        "authorids": "/f/francois-castagnos/; /m/martin-mihelich/; /c/charles-dognin/",
        "bibtex": "@inproceedings{castagnos-etal-2022-simple,\n    title = \"A Simple Log-based Loss Function for Ordinal Text Classification\",\n    author = \"Castagnos, Fran{\\c{c}}ois  and\n      Mihelich, Martin  and\n      Dognin, Charles\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.407/\",\n    pages = \"4604--4609\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.407.pdf",
        "site": "https://aclanthology.org/2022.coling-1.407/",
        "pdf_size": 275327,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13158167002711379425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Glanceable; Glanceable; Glanceable",
        "aff_domain": "glanceable.io;glanceable.io;glanceable.io",
        "email": "glanceable.io;glanceable.io;glanceable.io",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Glanceable",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.coling-1.234",
        "title": "A Simple Model for Distantly Supervised Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Distantly supervised relation extraction is challenging due to the noise within data. Recent methods focus on exploiting bag representations based on deep neural networks with complex de-noising scheme to achieve remarkable performance. In this paper, we propose a simple but effective BERT-based Graph convolutional network Model (i.e., BGM). Our BGM comprises of an instance embedding module and a bag representation module. The instance embedding module uses a BERT-based pretrained language model to extract key information from each instance. The bag representaion module constructs the corresponding bag graph then apply a convolutional operation to obtain the bag representation. Our BGM model achieves a considerable improvement on two benchmark datasets, i.e., NYT10 and GDS.",
        "author": "Ziqin Rao; Fangxiang Feng; Ruifan Li; Xiaojie Wang",
        "authorids": "/z/ziqin-rao/; /f/fangxiang-feng/; /r/ruifan-li/; /x/xiaojie-wang/",
        "bibtex": "@inproceedings{rao-etal-2022-simple,\n    title = \"A Simple Model for Distantly Supervised Relation Extraction\",\n    author = \"Rao, Ziqin  and\n      Feng, Fangxiang  and\n      Li, Ruifan  and\n      Wang, Xiaojie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.234/\",\n    pages = \"2651--2657\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.234.pdf",
        "site": "https://aclanthology.org/2022.coling-1.234/",
        "pdf_size": 512151,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12005806125930780715&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/ziqinrao",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Artificial Intelligence",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.181",
        "title": "A Simple Temporal Information Matching Mechanism for Entity Alignment between Temporal Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity alignment (EA) aims to find entities in different knowledge graphs (KGs) that refer to the same object in the real world. Recent studies incorporate temporal information to augment the representations of KGs. The existing methods for EA between temporal KGs (TKGs) utilize a time-aware attention mechanisms to incorporate relational and temporal information into entity embeddings. The approaches outperform the previous methods by using temporal information. However, we believe that it is not necessary to learn the embeddings of temporal information in KGs since most TKGs have uniform temporal representations. Therefore, we propose a simple GNN model combined with a temporal information matching mechanism, which achieves better performance with less time and fewer parameters. Furthermore, since alignment seeds are difficult to label in real-world applications, we also propose a method to generate unsupervised alignment seeds via the temporal information of TKG. Extensive experiments on public datasets indicate that our supervised method significantly outperforms the previous methods and the unsupervised one has competitive performance.",
        "author": "Li Cai; Xin Mao; Meirong Ma; Hao Yuan; Jianchao Zhu; Man Lan",
        "authorids": "/l/li-cai/; /x/xinnian-mao/; /m/meirong-ma/; /h/hao-yuan/; /j/jianchao-zhu/; /m/man-lan/",
        "bibtex": "@inproceedings{cai-etal-2022-simple,\n    title = \"A Simple Temporal Information Matching Mechanism for Entity Alignment between Temporal Knowledge Graphs\",\n    author = \"Cai, Li  and\n      Mao, Xin  and\n      Ma, Meirong  and\n      Yuan, Hao  and\n      Zhu, Jianchao  and\n      Lan, Man\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.181/\",\n    pages = \"2075--2086\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.181.pdf",
        "site": "https://aclanthology.org/2022.coling-1.181/",
        "pdf_size": 312894,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3894341570027300751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, East China Normal University + College of Computer Science and Technology, Guizhou University; School of Computer Science and Technology, East China Normal University; Shanghai Transsion Co., Ltd; Shanghai Transsion Co., Ltd; Shanghai Transsion Co., Ltd; School of Computer Science and Technology, East China Normal University",
        "aff_domain": "gmail.com;stu.ecnu.edu.cn;transsion.com;transsion.com;transsion.com;cs.ecnu.edu.cn",
        "email": "gmail.com;stu.ecnu.edu.cn;transsion.com;transsion.com;transsion.com;cs.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;2;2;2;0",
        "aff_unique_norm": "East China Normal University;Guizhou University;Shanghai Transsion Co., Ltd",
        "aff_unique_dep": "School of Computer Science and Technology;College of Computer Science and Technology;",
        "aff_unique_url": "http://www.ecnu.edu.cn;http://www.gzu.edu.cn;http://www.transsion.com/",
        "aff_unique_abbr": "ECNU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.385",
        "title": "A Simple and Effective Method to Improve Zero-Shot Cross-Lingual Transfer Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing zero-shot cross-lingual transfer methods rely on parallel corpora or bilingual dictionaries, which are expensive and impractical for low-resource languages. To disengage from these dependencies, researchers have explored training multilingual models on English-only resources and transferring them to low-resource languages. However, its effect is limited by the gap between embedding clusters of different languages. To address this issue, we propose Embedding-Push, Attention-Pull, and Robust targets to transfer English embeddings to virtual multilingual embeddings without semantic loss, thereby improving cross-lingual transferability. Experimental results on mBERT and XLM-R demonstrate that our method significantly outperforms previous works on the zero-shot cross-lingual text classification task and can obtain a better multilingual alignment.",
        "author": "Kunbo Ding; Weijie Liu; Yuejian Fang; Weiquan Mao; Zhe Zhao; Tao Zhu; Haoyan Liu; Rong Tian; Yiren Chen",
        "authorids": "/k/kunbo-ding/; /w/weijie-liu/; /y/yuejian-fang/; /w/weiquan-mao/; /z/zhe-zhao/; /t/tao-zhu/; /h/haoyan-liu/; /r/rong-tian/; /y/yiren-chen/",
        "bibtex": "@inproceedings{ding-etal-2022-simple,\n    title = \"A Simple and Effective Method to Improve Zero-Shot Cross-Lingual Transfer Learning\",\n    author = \"Ding, Kunbo  and\n      Liu, Weijie  and\n      Fang, Yuejian  and\n      Mao, Weiquan  and\n      Zhao, Zhe  and\n      Zhu, Tao  and\n      Liu, Haoyan  and\n      Tian, Rong  and\n      Chen, Yiren\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.385/\",\n    pages = \"4372--4380\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.385.pdf",
        "site": "https://aclanthology.org/2022.coling-1.385/",
        "pdf_size": 741820,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3139977578746793680&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Peking University; Peking University + Tencent Research; Peking University; Tencent Research; Tencent Research; Tencent Research; Tencent Research; Tencent Research; Tencent Research",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;ss.pku.edu.cn;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "email": "stu.pku.edu.cn;pku.edu.cn;ss.pku.edu.cn;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0+1;0;1;1;1;1;1;1",
        "aff_unique_norm": "Peking University;Tencent",
        "aff_unique_dep": ";Tencent Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "Peking U;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.619",
        "title": "A Structure-Aware Argument Encoder for Literature Discourse Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing research for argument representation learning mainly treats tokens in the sentence equally and ignores the implied structure information of argumentative context. In this paper, we propose to separate tokens into two groups, namely framing tokens and topic ones, to capture structural information of arguments. In addition, we consider high-level structure by incorporating paragraph-level position information. A novel structure-aware argument encoder is proposed for literature discourse analysis. Experimental results on both a self-constructed corpus and a public corpus show the effectiveness of our model. Resources are available at https://github.com/lemuria-wchen/SAE.",
        "author": "Yinzi Li; Wei Chen; Zhongyu Wei; Yujun Huang; Chujun Wang; Siyuan Wang; Qi Zhang; Xuanjing Huang; Libo Wu",
        "authorids": "/y/yinzi-li/; /w/wei-chen/; /z/zhongyu-wei/; /y/yujun-huang/; /c/chujun-wang/; /s/siyuan-wang/; /q/qi-zhang/; /x/xuan-jing-huang/; /l/libo-wu/",
        "bibtex": "@inproceedings{li-etal-2022-structure,\n    title = \"A Structure-Aware Argument Encoder for Literature Discourse Analysis\",\n    author = \"Li, Yinzi  and\n      Chen, Wei  and\n      Wei, Zhongyu  and\n      Huang, Yujun  and\n      Wang, Chujun  and\n      Wang, Siyuan  and\n      Zhang, Qi  and\n      Huang, Xuanjing  and\n      Wu, Libo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.619/\",\n    pages = \"7093--7098\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.619.pdf",
        "site": "https://aclanthology.org/2022.coling-1.619/",
        "pdf_size": 450248,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17714320835534998056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China + Research Institute of Intelligent Complex Systems, Fudan University; School of Data Science, Fudan University, China; Department of Environmental Science and Engineering, Fudan University, China; School of Data Science, Fudan University, China; Department of Environmental Science and Engineering, Fudan University, China; School of Computer Science, Fudan University, China; School of Data Science, Fudan University, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "https://github.com/lemuria-wchen/SAE",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0+0;0;0;0;0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Data Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.113",
        "title": "A Study of Implicit Bias in Pretrained Language Models against People with Disabilities",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pretrained language models (PLMs) have been shown to exhibit sociodemographic biases, such as against gender and race, raising concerns of downstream biases in language technologies. However, PLMs\u2019 biases against people with disabilities (PWDs) have received little attention, in spite of their potential to cause similar harms. Using perturbation sensitivity analysis, we test an assortment of popular word embedding-based and transformer-based PLMs and show significant biases against PWDs in all of them. The results demonstrate how models trained on large corpora widely favor ableist language.",
        "author": "Pranav Narayanan Venkit; Mukund Srinath; Shomir Wilson",
        "authorids": "/p/pranav-narayanan-venkit/; /m/mukund-srinath/; /s/shomir-wilson/",
        "bibtex": "@inproceedings{venkit-etal-2022-study,\n    title = \"A Study of Implicit Bias in Pretrained Language Models against People with Disabilities\",\n    author = \"Venkit, Pranav Narayanan  and\n      Srinath, Mukund  and\n      Wilson, Shomir\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.113/\",\n    pages = \"1324--1332\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.113.pdf",
        "site": "https://aclanthology.org/2022.coling-1.113/",
        "pdf_size": 286111,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7872852743565630205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.69",
        "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Irony is a ubiquitous figurative language in daily communication. Previously, many researchers have approached irony from linguistic, cognitive science, and computational aspects. Recently, some progress have been witnessed in automatic irony processing due to the rapid development in deep neural models in natural language processing (NLP). In this paper, we will provide a comprehensive overview of computational irony, insights from linguisic theory and cognitive science, as well as its interactions with downstream NLP tasks and newly proposed multi-X irony processing perspectives.",
        "author": "Qingcheng Zeng; An-Ran Li",
        "authorids": "/q/qingcheng-zeng/; /a/an-ran-li/",
        "bibtex": "@inproceedings{zeng-li-2022-survey,\n    title = \"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-{X} Perspectives\",\n    author = \"Zeng, Qingcheng  and\n      Li, An-Ran\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.69/\",\n    pages = \"824--836\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.69.pdf",
        "site": "https://aclanthology.org/2022.coling-1.69/",
        "pdf_size": 292796,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7995284430528963403&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Linguistics, Northwestern University; Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University",
        "aff_domain": "outlook.com;connect.polyu.hk",
        "email": "outlook.com;connect.polyu.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Northwestern University;The Hong Kong Polytechnic University",
        "aff_unique_dep": "Department of Linguistics;Department of Chinese and Bilingual Studies",
        "aff_unique_url": "https://www.northwestern.edu;https://www.polyu.edu.hk",
        "aff_unique_abbr": "NU;PolyU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.coling-1.536",
        "title": "A Survey of Automatic Text Summarization Using Graph Neural Networks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Although automatic text summarization (ATS) has been researched for several decades, the application of graph neural networks (GNNs) to this task started relatively recently. In this survey we provide an overview on the rapidly evolving approach of using GNNs for the task of automatic text summarization. In particular we provide detailed information on the functionality of GNNs in the context of ATS, and a comprehensive overview of models utilizing this approach.",
        "author": "Marco Ferdinand Salchner; Adam Jatowt",
        "authorids": "/m/marco-ferdinand-salchner/; /a/adam-jatowt/",
        "bibtex": "@inproceedings{salchner-jatowt-2022-survey,\n    title = \"A Survey of Automatic Text Summarization Using Graph Neural Networks\",\n    author = \"Salchner, Marco Ferdinand  and\n      Jatowt, Adam\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.536/\",\n    pages = \"6139--6150\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.536.pdf",
        "site": "https://aclanthology.org/2022.coling-1.536/",
        "pdf_size": 386562,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14847479864868180942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of Innsbruck, Austria; Department of Computer Science & Digital Science Center, University of Innsbruck, Austria",
        "aff_domain": "student.uibk.ac.at;uibk.ac.at",
        "email": "student.uibk.ac.at;uibk.ac.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Innsbruck",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uibk.ac.at",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "2022.coling-1.576",
        "title": "A Survey on Multimodal Disinformation Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent years have witnessed the proliferation of offensive content online such as fake news, propaganda, misinformation, and disinformation. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract more attention, and spread further than text. As a result, researchers started leveraging different modalities and combinations thereof to tackle online multimodal offensive content. In this study, we offer a survey on the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, speech, video, social media network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality, and (ii) harmfulness \u2013, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.",
        "author": "Firoj Alam; Stefano Cresci; Tanmoy Chakraborty; Fabrizio Silvestri; Dimiter Dimitrov; Giovanni Da San Martino; Shaden Shaar; Hamed Firooz; Preslav Nakov",
        "authorids": "/f/firoj-alam/; /s/stefano-cresci/; /t/tanmoy-chakraborty/; /f/fabrizio-silvestri/; /d/dimiter-dimitrov/; /g/giovanni-da-san-martino/; /s/shaden-shaar/; /h/hamed-firooz/; /p/preslav-nakov/",
        "bibtex": "@inproceedings{alam-etal-2022-survey,\n    title = \"A Survey on Multimodal Disinformation Detection\",\n    author = \"Alam, Firoj  and\n      Cresci, Stefano  and\n      Chakraborty, Tanmoy  and\n      Silvestri, Fabrizio  and\n      Dimitrov, Dimiter  and\n      Martino, Giovanni Da San  and\n      Shaar, Shaden  and\n      Firooz, Hamed  and\n      Nakov, Preslav\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.576/\",\n    pages = \"6625--6643\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.576.pdf",
        "site": "https://aclanthology.org/2022.coling-1.576/",
        "pdf_size": 8613052,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7570034469742875717&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "Qatar Computing Research Institute, HBKU; IIT-CNR, Pisa; IIT Delhi; Sapienza University of Rome, Italy; Sofia University; University of Padova, Italy; Qatar Computing Research Institute, HBKU; Facebook AI; Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "hbku.edu.qa;hbku.edu.qa;iit.cnr.it;ee.iitd.ac.in;diag.uniroma1.it;gmail.com;math.unipd.it;fb.com;mbzuai.ac.ae",
        "email": "hbku.edu.qa;hbku.edu.qa;iit.cnr.it;ee.iitd.ac.in;diag.uniroma1.it;gmail.com;math.unipd.it;fb.com;mbzuai.ac.ae",
        "github": "",
        "project": "https://www.who.int/health-topics/infodemic",
        "author_num": 9,
        "aff_unique_index": "0;1;2;3;4;5;0;6;7",
        "aff_unique_norm": "Qatar Computing Research Institute;Istituto Italiano di Tecnologia;Indian Institute of Technology Delhi;Sapienza University of Rome;Sofia University;University of Padova;Facebook;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;;;;;Facebook AI;",
        "aff_unique_url": "https://www.qcri.org;https://www.iit.it;https://www.iitd.ac.in;https://www.uniroma1.it;https://www.sofiauni.bg/en/;https://www.unipd.it;https://www.facebook.com;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "QCRI;IIT;IITD;Sapienza;Sofia U;UNIPD;Facebook AI;MBZUAI",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Pisa;Delhi",
        "aff_country_unique_index": "0;1;2;1;3;1;0;4;5",
        "aff_country_unique": "Qatar;Italy;India;Bulgaria;United States;United Arab Emirates"
    },
    {
        "id": "2022.coling-1.629",
        "title": "A Transformer-based Threshold-Free Framework for Multi-Intent NLU",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-intent natural language understanding (NLU) has recently gained attention. It detects multiple intents in an utterance, which is better suited to real-world scenarios. However, the state-of-the-art joint NLU models mainly detect multiple intents on threshold-based strategy, resulting in one main issue: the model is extremely sensitive to the threshold settings. In this paper, we propose a transformer-based Threshold-Free Multi-intent NLU model (TFMN) with multi-task learning (MTL). Specifically, we first leverage multiple layers of a transformer-based encoder to generate multi-grain representations. Then we exploit the information of the number of multiple intents in each utterance without additional manual annotations and propose an auxiliary detection task: Intent Number detection (IND). Furthermore, we propose a threshold-free intent multi-intent classifier that utilizes the output of IND task and detects the multiple intents without depending on the threshold. Extensive experiments demonstrate that our proposed model achieves superior results on two public multi-intent datasets.",
        "author": "Lisung Chen; Nuo Chen; Yuexian Zou; Yong Wang; Xinzhong Sun",
        "authorids": "/l/lisung-chen/; /n/nuo-chen/; /y/yuexian-zou/; /y/yong-wang/; /x/xinzhong-sun/",
        "bibtex": "@inproceedings{chen-etal-2022-transformer,\n    title = \"A Transformer-based Threshold-Free Framework for Multi-Intent {NLU}\",\n    author = \"Chen, Lisung  and\n      Chen, Nuo  and\n      Zou, Yuexian  and\n      Wang, Yong  and\n      Sun, Xinzhong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.629/\",\n    pages = \"7187--7192\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.629.pdf",
        "site": "https://aclanthology.org/2022.coling-1.629/",
        "pdf_size": 399030,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2817416863144957665&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "ADSPLAB, School of ECE, Peking University, Shenzhen, China; ADSPLAB, School of ECE, Peking University, Shenzhen, China; ADSPLAB, School of ECE, Peking University, Shenzhen, China; AOTO Electronics Co., Ltd; AOTO Electronics Co., Ltd",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Peking University;AOTO Electronics Co., Ltd",
        "aff_unique_dep": "School of ECE;",
        "aff_unique_url": "http://www.pku.edu.cn;",
        "aff_unique_abbr": "PKU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.369",
        "title": "A Transition-based Method for Complex Question Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Complex Question Understanding (CQU) parses complex questions to Question Decomposition Meaning Representation (QDMR) which is a sequence of atomic operators. Existing works are based on end-to-end neural models which do not explicitly model the intermediate states and lack interpretability for the parsing process. Besides, they predict QDMR in a mismatched granularity and do not model the step-wise information which is an essential characteristic of QDMR. To alleviate the issues, we treat QDMR as a computational graph and propose a transition-based method where a decider predicts a sequence of actions to build the graph node-by-node. In this way, the partial graph at each step enables better representation of the intermediate states and better interpretability. At each step, the decider encodes the intermediate state with specially designed encoders and predicts several candidates of the next action and its confidence. For inference, a searcher seeks the optimal graph based on the predictions of the decider to alleviate the error propagation. Experimental results demonstrate the parsing accuracy of our method against several strong baselines. Moreover, our method has transparent and human-readable intermediate results, showing improved interpretability.",
        "author": "Yu Xia; Wenbin Jiang; Yajuan Lyu; Sujian Li",
        "authorids": "/y/yu-xia/; /w/wenbin-jiang/; /y/yajuan-lyu/; /s/sujian-li/",
        "bibtex": "@inproceedings{xia-etal-2022-transition,\n    title = \"A Transition-based Method for Complex Question Understanding\",\n    author = \"Xia, Yu  and\n      Jiang, Wenbin  and\n      Lyu, Yajuan  and\n      Li, Sujian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.369/\",\n    pages = \"4203--4211\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.369.pdf",
        "site": "https://aclanthology.org/2022.coling-1.369/",
        "pdf_size": 649558,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ddxWSFbgtzQJ:scholar.google.com/&scioq=A+Transition-based+Method+for+Complex+Question+Understanding&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "MOE Key Lab of Computational Linguistics, Peking University, Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; MOE Key Lab of Computational Linguistics, Peking University, Beijing, China",
        "aff_domain": "pku.edu.cn;baidu.com;baidu.com;pku.edu.cn",
        "email": "pku.edu.cn;baidu.com;baidu.com;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Peking University;Baidu Inc.",
        "aff_unique_dep": "MOE Key Lab of Computational Linguistics;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.baidu.com",
        "aff_unique_abbr": "Peking University;Baidu",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.263",
        "title": "A Two Stage Adaptation Framework for Frame Detection via Prompt Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Framing is a communication strategy to bias discussion by selecting and emphasizing. Frame detection aims to automatically analyze framing strategy. Previous works on frame detection mainly focus on a single scenario or issue, ignoring the special characteristics of frame detection that new events emerge continuously and policy agenda changes dynamically. To better deal with various context and frame typologies across different issues, we propose a two-stage adaptation framework. In the framing domain adaptation from pre-training stage, we design two tasks based on pivots and prompts to learn a transferable encoder, verbalizer, and prompts. In the downstream scenario generalization stage, the transferable components are applied to new issues and label sets. Experiment results demonstrate the effectiveness of our framework in different scenarios. Also, it shows superiority both in full-resource and low-resource conditions.",
        "author": "Xinyi Mou; Zhongyu Wei; Changjian Jiang; Jiajie Peng",
        "authorids": "/x/xinyi-mou/; /z/zhongyu-wei/; /c/changjian-jiang/; /j/jiajie-peng/",
        "bibtex": "@inproceedings{mou-etal-2022-two,\n    title = \"A Two Stage Adaptation Framework for Frame Detection via Prompt Learning\",\n    author = \"Mou, Xinyi  and\n      Wei, Zhongyu  and\n      Jiang, Changjian  and\n      Peng, Jiajie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.263/\",\n    pages = \"2968--2978\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.263.pdf",
        "site": "https://aclanthology.org/2022.coling-1.263/",
        "pdf_size": 1087970,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16054519484454796235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Data Science, Fudan University, China; School of Data Science, Fudan University, China + Research Institute of Intelligent and Complex Systems, Fudan University, China; School of International Relations & Public Affairs, Fudan University, China; School of Computer Science, Northwestern Polytechnical University, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;nwpu.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;nwpu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+0;0;1",
        "aff_unique_norm": "Fudan University;Northwestern Polytechnical University",
        "aff_unique_dep": "School of Data Science;School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.nwpu.edu.cn",
        "aff_unique_abbr": "Fudan;NPU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.244",
        "title": "A Unified Propagation Forest-based Framework for Fake News Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Fake news\u2019s quick propagation on social media brings severe social ramifications and economic damage. Previous fake news detection usually learn semantic and structural patterns within a single target propagation tree. However, they are usually limited in narrow signals since they do not consider latent information cross other propagation trees. Motivated by a common phenomenon that most fake news is published around a specific hot event/topic, this paper develops a new concept of propagation forest to naturally combine propagation trees in a semantic-aware clustering. We propose a novel Unified Propagation Forest-based framework (UniPF) to fully explore latent correlations between propagation trees to improve fake news detection. Besides, we design a root-induced training strategy, which encourages representations of propagation trees to be closer to their prototypical root nodes. Extensive experiments on four benchmarks consistently suggest the effectiveness and scalability of UniPF.",
        "author": "Lingwei Wei; Dou Hu; Yantong Lai; Wei Zhou; Songlin Hu",
        "authorids": "/l/lingwei-wei/; /d/dou-hu/; /y/yantong-lai/; /w/wei-zhou/; /s/songlin-hu/",
        "bibtex": "@inproceedings{wei-etal-2022-unified,\n    title = \"A Unified Propagation Forest-based Framework for Fake News Detection\",\n    author = \"Wei, Lingwei  and\n      Hu, Dou  and\n      Lai, Yantong  and\n      Zhou, Wei  and\n      Hu, Songlin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.244/\",\n    pages = \"2769--2779\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.244.pdf",
        "site": "https://aclanthology.org/2022.coling-1.244/",
        "pdf_size": 1863447,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14752455695108224400&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.400",
        "title": "A Weak Supervision Approach for Predicting Difficulty of Technical Interview Questions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Predicting difficulty of questions is crucial for technical interviews. However, such questions are long-form and more open-ended than factoid and multiple choice questions explored so far for question difficulty prediction. Existing models also require large volumes of candidate response data for training. We study weak-supervision and use unsupervised algorithms for both question generation and difficulty prediction. We create a dataset of interview questions with difficulty scores for deep learning and use it to evaluate SOTA models for question difficulty prediction trained using weak supervision. Our analysis brings out the task\u2019s difficulty as well as the promise of weak supervision for it.",
        "author": "Arpita Kundu; Subhasish Ghosh; Pratik Saini; Tapas Nayak; Indrajit Bhattacharya",
        "authorids": "/a/arpita-kundu/; /s/subhasish-ghosh/; /p/pratik-saini/; /t/tapas-nayak/; /i/indrajit-bhattacharya/",
        "bibtex": "@inproceedings{kundu-etal-2022-weak,\n    title = \"A Weak Supervision Approach for Predicting Difficulty of Technical Interview Questions\",\n    author = \"Kundu, Arpita  and\n      Ghosh, Subhasish  and\n      Saini, Pratik  and\n      Nayak, Tapas  and\n      Bhattacharya, Indrajit\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.400/\",\n    pages = \"4537--4543\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.400.pdf",
        "site": "https://aclanthology.org/2022.coling-1.400/",
        "pdf_size": 449301,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Uji9oji0QAIJ:scholar.google.com/&scioq=A+Weak+Supervision+Approach+for+Predicting+Difficulty+of+Technical+Interview+Questions&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "TCS Research, India; TCS Research, India; TCS Research, India; TCS Research, India; TCS Research, India",
        "aff_domain": "tcs.com;tcs.com;tcs.com;tcs.com;tcs.com",
        "email": "tcs.com;tcs.com;tcs.com;tcs.com;tcs.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tata Consultancy Services",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.tcs.com",
        "aff_unique_abbr": "TCS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.603",
        "title": "A Zero-Shot Claim Detection Framework Using Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, there has been an increasing interest in claim detection as an important building block for misinformation detection. This involves detecting more fine-grained attributes relating to the claim, such as the claimer, claim topic, claim object pertaining to the topic, etc. Yet, a notable bottleneck of existing claim detection approaches is their portability to emerging events and low-resource training data settings. In this regard, we propose a fine-grained claim detection framework that leverages zero-shot Question Answering (QA) using directed questions to solve a diverse set of sub-tasks such as topic filtering, claim object detection, and claimer detection. We show that our approach significantly outperforms various zero-shot, few-shot and task-specific baselines on the NewsClaims benchmark (Reddy et al., 2021).",
        "author": "Revanth Gangi Reddy; Sai Chetan Chinthakindi; Yi R. Fung; Kevin Small; Heng Ji",
        "authorids": "/r/revanth-gangi-reddy/; /s/sai-chetan-chinthakindi/; /y/yi-r-fung/; /k/kevin-small/; /h/heng-ji/",
        "bibtex": "@inproceedings{gangi-reddy-etal-2022-zero,\n    title = \"A Zero-Shot Claim Detection Framework Using Question Answering\",\n    author = \"Gangi Reddy, Revanth  and\n      Chinthakindi, Sai Chetan  and\n      Fung, Yi R.  and\n      Small, Kevin  and\n      Ji, Heng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.603/\",\n    pages = \"6927--6933\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.603.pdf",
        "site": "https://aclanthology.org/2022.coling-1.603/",
        "pdf_size": 521654,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7434645470201975998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Amazon Alexa AI; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;amazon.com;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;amazon.com;illinois.edu",
        "github": "https://github.com/blender-nlp/NewsClaims/tree/main/zero-shot-qa",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Amazon",
        "aff_unique_dep": ";Alexa AI",
        "aff_unique_url": "https://illinois.edu;https://www.amazon.com",
        "aff_unique_abbr": "UIUC;Amazon",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.511",
        "title": "A-TIP: Attribute-aware Text Infilling via Pre-trained Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text infilling aims to restore incomplete texts by filling in blanks, which has attracted more attention recently because of its wide application in ancient text restoration and text rewriting. However, attribute- aware text infilling is yet to be explored, and existing methods seldom focus on the infilling length of each blank or the number/location of blanks. In this paper, we propose an Attribute-aware Text Infilling method via a Pre-trained language model (A-TIP), which contains a text infilling component and a plug- and-play discriminator. Specifically, we first design a unified text infilling component with modified attention mechanisms and intra- and inter-blank positional encoding to better perceive the number of blanks and the infilling length for each blank. Then, we propose a plug-and-play discriminator to guide generation towards the direction of improving attribute relevance without decreasing text fluency. Finally, automatic and human evaluations on three open-source datasets indicate that A-TIP achieves state-of- the-art performance compared with all baselines.",
        "author": "Dongyuan Li; Jingyi You; Kotaro Funakoshi; Manabu Okumura",
        "authorids": "/d/dongyuan-li/; /j/jingyi-you/; /k/kotaro-funakoshi/; /m/manabu-okumura/",
        "bibtex": "@inproceedings{li-etal-2022-tip,\n    title = \"A-{TIP}: Attribute-aware Text Infilling via Pre-trained Language Model\",\n    author = \"Li, Dongyuan  and\n      You, Jingyi  and\n      Funakoshi, Kotaro  and\n      Okumura, Manabu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.511/\",\n    pages = \"5857--5869\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.511.pdf",
        "site": "https://aclanthology.org/2022.coling-1.511/",
        "pdf_size": 2547333,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9245146498979359461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp",
        "email": "lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.495",
        "title": "ACT-Thor: A Controlled Benchmark for Embodied Action Understanding in Simulated Environments",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Artificial agents are nowadays challenged to perform embodied AI tasks. To succeed, agents must understand the meaning of verbs and how their corresponding actions transform the surrounding world. In this work, we propose ACT-Thor, a novel controlled benchmark for embodied action understanding. We use the AI2-THOR simulated environment to produce a controlled setup in which an agent, given a before-image and an associated action command, has to determine what the correct after-image is among a set of possible candidates. First, we assess the feasibility of the task via a human evaluation that resulted in 81.4% accuracy, and very high inter-annotator agreement (84.9%). Second, we design both unimodal and multimodal baselines, using state-of-the-art visual feature extractors. Our evaluation and error analysis suggest that only models that have a very structured representation of the actions together with powerful visual features can perform well on the task. However, they still fall behind human performance in a zero-shot scenario where the model is exposed to unseen (action, object) pairs. This paves the way for a systematic way of evaluating embodied AI agents that understand grounded actions.",
        "author": "Michael Hanna; Federico Pedeni; Alessandro Suglia; Alberto Testoni; Raffaella Bernardi",
        "authorids": "/m/michael-hanna/; /f/federico-pedeni/; /a/alessandro-suglia/; /a/alberto-testoni/; /r/raffaella-bernardi/",
        "bibtex": "@inproceedings{hanna-etal-2022-act,\n    title = \"{ACT}-Thor: A Controlled Benchmark for Embodied Action Understanding in Simulated Environments\",\n    author = \"Hanna, Michael  and\n      Pedeni, Federico  and\n      Suglia, Alessandro  and\n      Testoni, Alberto  and\n      Bernardi, Raffaella\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.495/\",\n    pages = \"5597--5612\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.495.pdf",
        "site": "https://aclanthology.org/2022.coling-1.495/",
        "pdf_size": 2943702,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17329039108624843977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Amsterdam + University of Trento; University of Trento; Heriot-Watt University; University of Trento; University of Trento",
        "aff_domain": "uva.nl;studenti.unitn.it;hw.ac.uk;unitn.it;unitn.it",
        "email": "uva.nl;studenti.unitn.it;hw.ac.uk;unitn.it;unitn.it",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;2;1;1",
        "aff_unique_norm": "University of Amsterdam;University of Trento;Heriot-Watt University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uva.nl;https://www.unitn.it;https://www.hw.ac.uk",
        "aff_unique_abbr": "UvA;UniTN;HWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;2;1;1",
        "aff_country_unique": "Netherlands;Italy;United Kingdom"
    },
    {
        "id": "2022.coling-1.529",
        "title": "ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their potential substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS-ES protocol for Spanish opening exciting new avenues for cross-lingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated three models for substitute generation on this dataset, namely mBERT, XLM-R, and BERTimbau. The latter achieved the highest performance across all evaluation metrics.",
        "author": "Kai North; Marcos Zampieri; Tharindu Ranasinghe",
        "authorids": "/k/kai-north/; /m/marcos-zampieri/; /t/tharindu-ranasinghe/",
        "bibtex": "@inproceedings{north-etal-2022-alexsis,\n    title = \"{ALEXSIS}-{PT}: A New Resource for {P}ortuguese Lexical Simplification\",\n    author = \"North, Kai  and\n      Zampieri, Marcos  and\n      Ranasinghe, Tharindu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.529/\",\n    pages = \"6057--6062\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.529.pdf",
        "site": "https://aclanthology.org/2022.coling-1.529/",
        "pdf_size": 219774,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5800498224384492403&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "George Mason University, USA; George Mason University, USA; University of Wolverhampton, UK",
        "aff_domain": "gmu.edu; ; ",
        "email": "gmu.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "George Mason University;University of Wolverhampton",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gmu.edu;https://www.wolverhampton.ac.uk",
        "aff_unique_abbr": "GMU;Wolverhampton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2022.coling-1.623",
        "title": "AMOA: Global Acoustic Feature Enhanced Modal-Order-Aware Network for Multimodal Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, multimodal sentiment analysis (MSA) has attracted more and more interest, which aims to predict the sentiment polarity expressed in a video. Existing methods typically 1) treat three modal features (textual, acoustic, visual) equally, without distinguishing the importance of different modalities; and 2) split the video into frames, leading to missing the global acoustic information. In this paper, we propose a global Acoustic feature enhanced Modal-Order-Aware network (AMOA) to address these problems. Firstly, a modal-order-aware network is designed to obtain the multimodal fusion feature. This network integrates the three modalities in a certain order, which makes the modality at the core position matter more. Then, we introduce the global acoustic feature of the whole video into our model. Since the global acoustic feature and multimodal fusion feature originally reside in their own spaces, contrastive learning is further employed to align them before concatenation. Experiments on two public datasets show that our model outperforms the state-of-the-art models. In addition, we also generalize our model to the sentiment with more complex semantics, such as sarcasm detection. Our model also achieves state-of-the-art performance on a widely used sarcasm dataset.",
        "author": "Ziming Li; Yan Zhou; Weibo Zhang; Yaxin Liu; Chuanpeng Yang; Zheng Lian; Songlin Hu",
        "authorids": "/z/ziming-li/; /y/yan-zhou/; /w/weibo-zhang/; /y/yaxin-liu/; /c/chuanpeng-yang/; /z/zheng-lian/; /s/songlin-hu/",
        "bibtex": "@inproceedings{li-etal-2022-amoa,\n    title = \"{AMOA}: Global Acoustic Feature Enhanced Modal-Order-Aware Network for Multimodal Sentiment Analysis\",\n    author = \"Li, Ziming  and\n      Zhou, Yan  and\n      Zhang, Weibo  and\n      Liu, Yaxin  and\n      Yang, Chuanpeng  and\n      Lian, Zheng  and\n      Hu, Songlin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.623/\",\n    pages = \"7136--7146\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.623.pdf",
        "site": "https://aclanthology.org/2022.coling-1.623/",
        "pdf_size": 761007,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11426213707119270990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.530",
        "title": "APPDIA: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Using style-transfer models to reduce offensiveness of social media comments can help foster a more inclusive environment. However, there are no sizable datasets that contain offensive texts and their inoffensive counterparts, and fine-tuning pretrained models with limited labeled data can lead to the loss of original meaning in the style-transferred text. To address this issue, we provide two major contributions. First, we release the first publicly-available, parallel corpus of offensive Reddit comments and their style-transferred counterparts annotated by expert sociolinguists. Then, we introduce the first discourse-aware style-transfer models that can effectively reduce offensiveness in Reddit text while preserving the meaning of the original text. These models are the first to examine inferential links between the comment and the text it is replying to when transferring the style of offensive Reddit text. We propose two different methods of integrating discourse relations with pretrained transformer models and evaluate them on our dataset of offensive comments from Reddit and their inoffensive counterparts. Improvements over the baseline with respect to both automatic metrics and human evaluation indicate that our discourse-aware models are better at preserving meaning in style-transferred text when compared to the state-of-the-art discourse-agnostic models.",
        "author": "Katherine Atwell; Sabit Hassan; Malihe Alikhani",
        "authorids": "/k/katherine-atwell/; /s/sabit-hassan/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{atwell-etal-2022-appdia,\n    title = \"{APPDIA}: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations\",\n    author = \"Atwell, Katherine  and\n      Hassan, Sabit  and\n      Alikhani, Malihe\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.530/\",\n    pages = \"6063--6074\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.530.pdf",
        "site": "https://aclanthology.org/2022.coling-1.530/",
        "pdf_size": 1407996,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6910181765960791786&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA; Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA; Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA",
        "aff_domain": "pitt.edu;pitt.edu;pitt.edu",
        "email": "pitt.edu;pitt.edu;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.414",
        "title": "Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conditional computation algorithms, such as the early exiting (EE) algorithm, can be applied to accelerate the inference of pretrained language models (PLMs) while maintaining competitive performance on resource-constrained devices. However, this approach is only applied to the vertical architecture to decide which layers should be used for inference. Conversely, the operation of the horizontal perspective is ignored, and the determination of which tokens in each layer should participate in the computation fails, leading to a high redundancy for adaptive inference. To address this limitation, a unified horizontal and vertical multi-perspective early exiting (MPEE) framework is proposed in this study to accelerate the inference of transformer-based models. Specifically, the vertical architecture uses recycling EE classifier memory and weighted self-distillation to enhance the performance of the EE classifiers. Then, the horizontal perspective uses recycling class attention memory to emphasize the informative tokens. Conversely, the tokens with less information are truncated by weighted fusion and isolated from the following computation. Based on this, both horizontal and vertical EE are unified to obtain a better tradeoff between performance and efficiency. Extensive experimental results show that MPEE can achieve higher acceleration inference with competent performance than existing competitive methods.",
        "author": "Jun Kong; Jin Wang; Liang-Chih Yu; Xuejie Zhang",
        "authorids": "/j/jun-kong/; /j/jin-wang/; /l/liang-chih-yu/; /x/xuejie-zhang/",
        "bibtex": "@inproceedings{kong-etal-2022-accelerating,\n    title = \"Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting\",\n    author = \"Kong, Jun  and\n      Wang, Jin  and\n      Yu, Liang-Chih  and\n      Zhang, Xuejie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.414/\",\n    pages = \"4677--4686\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.414.pdf",
        "site": "https://aclanthology.org/2022.coling-1.414/",
        "pdf_size": 738773,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4264418526778870283&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China; School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China + Department of Information Management, Yuan Ze University, Taiwan; Department of Information Management, Yuan Ze University, Taiwan; School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China",
        "aff_domain": "ynu.edu.cn;saturn.yzu.edu.tw; ; ",
        "email": "ynu.edu.cn;saturn.yzu.edu.tw; ; ",
        "github": "https://github.com/JunKong5/MPEE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;1;0",
        "aff_unique_norm": "Yunnan University;Yuan Ze University",
        "aff_unique_dep": "School of Information Science and Engineering;Department of Information Management",
        "aff_unique_url": "http://www.ynu.edu.cn;https://www.yzu.edu.tw",
        "aff_unique_abbr": "YNU;YZU",
        "aff_campus_unique_index": "0;0+1;1;0",
        "aff_campus_unique": "Yunnan;Taiwan",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.336",
        "title": "Accounting for Language Effect in the Evaluation of Cross-lingual AMR Parsers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-lingual Abstract Meaning Representation (AMR) parsers are currently evaluated in comparison to gold English AMRs, despite parsing a language other than English, due to the lack of multilingual AMR evaluation metrics. This evaluation practice is problematic because of the established effect of source language on AMR structure. In this work, we present three multilingual adaptations of monolingual AMR evaluation metrics and compare the performance of these metrics to sentence-level human judgments. We then use our most highly correlated metric to evaluate the output of state-of-the-art cross-lingual AMR parsers, finding that Smatch may still be a useful metric in comparison to gold English AMRs, while our multilingual adaptation of S2match (XS2match) is best for comparison with gold in-language AMRs.",
        "author": "Shira Wein; Nathan Schneider",
        "authorids": "/s/shira-wein/; /n/nathan-schneider/",
        "bibtex": "@inproceedings{wein-schneider-2022-accounting,\n    title = \"Accounting for Language Effect in the Evaluation of Cross-lingual {AMR} Parsers\",\n    author = \"Wein, Shira  and\n      Schneider, Nathan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.336/\",\n    pages = \"3824--3834\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.336.pdf",
        "site": "https://aclanthology.org/2022.coling-1.336/",
        "pdf_size": 421789,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7263142070281745870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Georgetown University; Georgetown University",
        "aff_domain": "georgetown.edu;georgetown.edu",
        "email": "georgetown.edu;georgetown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgetown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.georgetown.edu",
        "aff_unique_abbr": "GU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.332",
        "title": "Accuracy meets Diversity in a News Recommender System",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "News recommender systems face certain challenges. These challenges arise due to evolving users\u2019 preferences over dynamically created news articles. The diversity is necessary for a news recommender system to expose users to a variety of information. We propose a deep neural network based on a two-tower architecture that learns news representation through a news item tower and users\u2019 representations through a query tower. We customize an augmented vector for each query and news item to introduce information interaction between the two towers. We introduce diversity in the proposed architecture by considering a category loss function that aligns items\u2019 representation of uneven news categories. Experimental results on two news datasets reveal that our proposed architecture is more effective compared to the state-of-the-art methods and achieves a balance between accuracy and diversity.",
        "author": "Shaina Raza; Syed Raza Bashir; Usman Naseem",
        "authorids": "/s/shaina-raza/; /s/syed-raza-bashir/; /u/usman-naseem/",
        "bibtex": "@inproceedings{raza-etal-2022-accuracy,\n    title = \"Accuracy meets Diversity in a News Recommender System\",\n    author = \"Raza, Shaina  and\n      Bashir, Syed Raza  and\n      Naseem, Usman\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.332/\",\n    pages = \"3778--3787\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.332.pdf",
        "site": "https://aclanthology.org/2022.coling-1.332/",
        "pdf_size": 1091358,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2057884298066983231&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Toronto, Toronto, ON, Canada; Toronto Metropolitan University, Toronto, ON, Canada; The University of Sydney, Sydney, Australia",
        "aff_domain": "utoronto.ca;hotmail.com;sydney.edu.au",
        "email": "utoronto.ca;hotmail.com;sydney.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Toronto;Toronto Metropolitan University;The University of Sydney",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.tmuh.ca;https://www.sydney.edu.au",
        "aff_unique_abbr": "U of T;TMU;USYD",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Toronto;Sydney",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Canada;Australia"
    },
    {
        "id": "2022.coling-1.382",
        "title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) \u2014 fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.",
        "author": "Jesujoba O. Alabi; David Ifeoluwa Adelani; Marius Mosbach; Dietrich Klakow",
        "authorids": "/j/jesujoba-alabi/; /d/david-ifeoluwa-adelani/; /m/marius-mosbach/; /d/dietrich-klakow/",
        "bibtex": "@inproceedings{alabi-etal-2022-adapting,\n    title = \"Adapting Pre-trained Language Models to {A}frican Languages via Multilingual Adaptive Fine-Tuning\",\n    author = \"Alabi, Jesujoba O.  and\n      Adelani, David Ifeoluwa  and\n      Mosbach, Marius  and\n      Klakow, Dietrich\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.382/\",\n    pages = \"4336--4349\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.382.pdf",
        "site": "https://aclanthology.org/2022.coling-1.382/",
        "pdf_size": 369128,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4727338399864545709&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Spoken Language Systems (LSV), Saarland University, Saarland Informatics Campus, Germany; Spoken Language Systems (LSV), Saarland University, Saarland Informatics Campus, Germany; Spoken Language Systems (LSV), Saarland University, Saarland Informatics Campus, Germany; Spoken Language Systems (LSV), Saarland University, Saarland Informatics Campus, Germany",
        "aff_domain": "lsv.uni-saarland.de;lsv.uni-saarland.de;lsv.uni-saarland.de;lsv.uni-saarland.de",
        "email": "lsv.uni-saarland.de;lsv.uni-saarland.de;lsv.uni-saarland.de;lsv.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "Spoken Language Systems (LSV)",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "Uni Saar",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Saarland Informatics Campus",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.467",
        "title": "Adapting to Non-Centered Languages for Zero-shot Multilingual Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual neural machine translation can translate unseen language pairs during training, i.e. zero-shot translation. However, the zero-shot translation is always unstable. Although prior works attributed the instability to the domination of central language, e.g. English, we supplement this viewpoint with the strict dependence of non-centered languages. In this work, we propose a simple, lightweight yet effective language-specific modeling method by adapting to non-centered languages and combining the shared information and the language-specific information to counteract the instability of zero-shot translation. Experiments with Transformer on IWSLT17, Europarl, TED talks, and OPUS-100 datasets show that our method not only performs better than strong baselines in centered data conditions but also can easily fit non-centered data conditions. By further investigating the layer attribution, we show that our proposed method can disentangle the coupled representation in the correct direction.",
        "author": "Zhi Qu; Taro Watanabe",
        "authorids": "/z/zhi-qu/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{qu-watanabe-2022-adapting,\n    title = \"Adapting to Non-Centered Languages for Zero-shot Multilingual Translation\",\n    author = \"Qu, Zhi  and\n      Watanabe, Taro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.467/\",\n    pages = \"5251--5265\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.467.pdf",
        "site": "https://aclanthology.org/2022.coling-1.467/",
        "pdf_size": 1281199,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5754004940128840204&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Nara Institute of Science and Technology; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;is.naist.jp",
        "email": "is.naist.jp;is.naist.jp",
        "github": "https://github.com/zhiqu22/AdapNonCenter",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nist.go.jp",
        "aff_unique_abbr": "NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.98",
        "title": "Adaptive Feature Discrimination and Denoising for Asymmetric Text Matching",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Asymmetric text matching has becoming increasingly indispensable for many downstream tasks (e.g., IR and NLP). Here, asymmetry means that the documents involved for matching hold different amounts of information, e.g., a short query against a relatively longer document. The existing solutions mainly focus on modeling the feature interactions between asymmetric texts, but rarely go one step further to recognize discriminative features and perform feature denoising to enhance relevance learning. In this paper, we propose a novel adaptive feature discrimination and denoising model for asymmetric text matching, called ADDAX. For each asymmetric text pair, ADDAX is devised to explicitly distinguish discriminative features and filter out irrelevant features in a context-aware fashion. Concretely, a matching-adapted gating siamese cell (MAGS) is firstly devised to identify discriminative features and produce the corresponding hybrid representations for a text pair. Afterwards, we introduce a locality-constrained hashing denoiser to perform feature-level denoising by learning a discriminative low-dimensional binary codes for redundantly longer text. Extensive experiments on four real-world datasets from different downstream tasks demostrate that the proposed ADDAX obtains substantial performance gain over 36 up-to-date state-of-the-art alternatives.",
        "author": "Yan Li; Chenliang Li; Junjun Guo",
        "authorids": "/y/yan-li/; /c/chenliang-li/; /j/junjun-guo/",
        "bibtex": "@inproceedings{li-etal-2022-adaptive,\n    title = \"Adaptive Feature Discrimination and Denoising for Asymmetric Text Matching\",\n    author = \"Li, Yan  and\n      Li, Chenliang  and\n      Guo, Junjun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.98/\",\n    pages = \"1146--1156\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.98.pdf",
        "site": "https://aclanthology.org/2022.coling-1.98/",
        "pdf_size": 1728636,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1385100572229780219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Kunming University of Science and Technology; Wuhan University; Kunming University of Science and Technology",
        "aff_domain": "163.com;whu.edu.cn;163.com",
        "email": "163.com;whu.edu.cn;163.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Kunming University of Science and Technology;Wuhan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.kmust.edu.cn;http://www.whu.edu.cn/",
        "aff_unique_abbr": "KUST;WHU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kunming;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.431",
        "title": "Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot text classification aims to classify the text under the few-shot scenario. Most of the previous methods adopt optimization-based meta learning to obtain task distribution. However, due to the neglect of matching between the few amount of samples and complicated models, as well as the distinction between useful and useless task features, these methods suffer from the overfitting issue. To address this issue, we propose a novel Adaptive Meta-learner via Gradient Similarity (AMGS) method to improve the model generalization ability to a new task. Specifically, the proposed AMGS alleviates the overfitting based on two aspects: (i) acquiring the potential semantic representation of samples and improving model generalization through the self-supervised auxiliary task in the inner loop, (ii) leveraging the adaptive meta-learner via gradient similarity to add constraints on the gradient obtained by base-learner in the outer loop. Moreover, we make a systematic analysis of the influence of regularization on the entire framework. Experimental results on several benchmarks demonstrate that the proposed AMGS consistently improves few-shot text classification performance compared with the state-of-the-art optimization-based meta-learning approaches. The code is available at: https://github.com/Tianyi-Lei.",
        "author": "Tianyi Lei; Honghui Hu; Qiaoyang Luo; Dezhong Peng; Xu Wang",
        "authorids": "/t/tianyi-lei/; /h/honghui-hu/; /q/qiaoyang-luo/; /d/dezhong-peng/; /x/xu-wang/",
        "bibtex": "@inproceedings{lei-etal-2022-adaptive,\n    title = \"Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification\",\n    author = \"Lei, Tianyi  and\n      Hu, Honghui  and\n      Luo, Qiaoyang  and\n      Peng, Dezhong  and\n      Wang, Xu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.431/\",\n    pages = \"4873--4882\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.431.pdf",
        "site": "https://aclanthology.org/2022.coling-1.431/",
        "pdf_size": 1607053,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17986064992706638895&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science, Sichuan University; College of Computer Science, Sichuan University; The University of Adelaide; College of Computer Science, Sichuan University; College of Computer Science, Sichuan University",
        "aff_domain": "gmail.com;gmail.com; ; ; ",
        "email": "gmail.com;gmail.com; ; ; ",
        "github": "https://github.com/Tianyi-Lei",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Sichuan University;University of Adelaide",
        "aff_unique_dep": "College of Computer Science;",
        "aff_unique_url": "https://www.scu.edu.cn;https://www.adelaide.edu.au",
        "aff_unique_abbr": ";Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.coling-1.19",
        "title": "Adaptive Natural Language Generation for Task-oriented Dialogue via Reinforcement Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "When a natural language generation (NLG) component is implemented in a real-world task-oriented dialogue system, it is necessary to generate not only natural utterances as learned on training data but also utterances adapted to the dialogue environment (e.g., noise from environmental sounds) and the user (e.g., users with low levels of understanding ability). Inspired by recent advances in reinforcement learning (RL) for language generation tasks, we propose ANTOR, a method for Adaptive Natural language generation for Task-Oriented dialogue via Reinforcement learning. In ANTOR, a natural language understanding (NLU) module, which corresponds to the user\u2019s understanding of system utterances, is incorporated into the objective function of RL. If the NLG\u2019s intentions are correctly conveyed to the NLU, which understands a system\u2019s utterances, the NLG is given a positive reward. We conducted experiments on the MultiWOZ dataset, and we confirmed that ANTOR could generate adaptive utterances against speech recognition errors and the different vocabulary levels of users.",
        "author": "Atsumoto Ohashi; Ryuichiro Higashinaka",
        "authorids": "/a/atsumoto-ohashi/; /r/ryuichiro-higashinaka/",
        "bibtex": "@inproceedings{ohashi-higashinaka-2022-adaptive,\n    title = \"Adaptive Natural Language Generation for Task-oriented Dialogue via Reinforcement Learning\",\n    author = \"Ohashi, Atsumoto  and\n      Higashinaka, Ryuichiro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.19/\",\n    pages = \"242--252\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.19.pdf",
        "site": "https://aclanthology.org/2022.coling-1.19/",
        "pdf_size": 558306,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17234154833164250124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Graduate School of Informatics, Nagoya University; Graduate School of Informatics, Nagoya University",
        "aff_domain": "s.mail.nagoya-u.ac.jp;i.nagoya-u.ac.jp",
        "email": "s.mail.nagoya-u.ac.jp;i.nagoya-u.ac.jp",
        "github": "https://github.com/nu-dialogue/antor",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nagoya University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.nagoya-u.ac.jp",
        "aff_unique_abbr": "Nagoya U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nagoya",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.157",
        "title": "Adaptive Threshold Selective Self-Attention for Chinese NER",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, Transformer has achieved great success in Chinese named entity recognition (NER) owing to its good parallelism and ability to model long-range dependencies, which utilizes self-attention to encode context. However, the fully connected way of self-attention may scatter the attention distribution and allow some irrelevant character information to be integrated, leading to entity boundaries being misidentified. In this paper, we propose a data-driven Adaptive Threshold Selective Self-Attention (ATSSA) mechanism that aims to dynamically select the most relevant characters to enhance the Transformer architecture for Chinese NER. In ATSSA, the attention score threshold of each query is automatically generated, and characters with attention score higher than the threshold are selected by the query while others are discarded, so as to address irrelevant attention integration. Experiments on four benchmark Chinese NER datasets show that the proposed ATSSA brings 1.68 average F1 score improvements to the baseline model and achieves state-of-the-art performance.",
        "author": "Biao Hu; Zhen Huang; Minghao Hu; Ziwen Zhang; Yong Dou",
        "authorids": "/b/biao-hu/; /z/zhen-huang/; /m/minghao-hu/; /z/ziwen-zhang/; /y/yong-dou/",
        "bibtex": "@inproceedings{hu-etal-2022-adaptive,\n    title = \"Adaptive Threshold Selective Self-Attention for {C}hinese {NER}\",\n    author = \"Hu, Biao  and\n      Huang, Zhen  and\n      Hu, Minghao  and\n      Zhang, Ziwen  and\n      Dou, Yong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.157/\",\n    pages = \"1823--1833\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.157.pdf",
        "site": "https://aclanthology.org/2022.coling-1.157/",
        "pdf_size": 1859397,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=494097757724158040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; Information Research Center of Military Science, PLA Academy of Military Science, Beijing, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;gmail.com;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;gmail.com;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "National University of Defense Technology;PLA Academy of Military Science",
        "aff_unique_dep": "College of Computer;Information Research Center of Military Science",
        "aff_unique_url": "http://www.nudt.edu.cn;",
        "aff_unique_abbr": "NUDT;",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Changsha;Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.632",
        "title": "Adaptive Unsupervised Self-training for Disfluency Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Supervised methods have achieved remarkable results in disfluency detection. However, in real-world scenarios, human-annotated data is difficult to obtain. Recent works try to handle disfluency detection with unsupervised self-training, which can exploit existing large-scale unlabeled data efficiently. However, their self-training-based methods suffer from the problems of selection bias and error accumulation. To tackle these problems, we propose an adaptive unsupervised self-training method for disfluency detection. Specifically, we re-weight the importance of each training example according to its grammatical feature and prediction confidence. Experiments on the Switchboard dataset show that our method improves 2.3 points over the current SOTA unsupervised method. Moreover, our method is competitive with the SOTA supervised method.",
        "author": "Zhongyuan Wang; Yixuan Wang; Shaolei Wang; Wanxiang Che",
        "authorids": "/z/zhongyuan-wang/; /y/yixuan-wang/; /s/shaolei-wang/; /w/wanxiang-che/",
        "bibtex": "@inproceedings{wang-etal-2022-adaptive,\n    title = \"Adaptive Unsupervised Self-training for Disfluency Detection\",\n    author = \"Wang, Zhongyuan  and\n      Wang, Yixuan  and\n      Wang, Shaolei  and\n      Che, Wanxiang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.632/\",\n    pages = \"7209--7218\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.632.pdf",
        "site": "https://aclanthology.org/2022.coling-1.632/",
        "pdf_size": 563507,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14454330067565416676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.455",
        "title": "Addressing Asymmetry in Multilingual Neural Machine Translation with Fuzzy Task Clustering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual neural machine translation (NMT) enables positive knowledge transfer among multiple translation tasks with a shared underlying model, but a unified multilingual model usually suffers from capacity bottleneck when tens or hundreds of languages are involved. A possible solution is to cluster languages and train individual model for each cluster. However, the existing clustering methods based on language similarity cannot handle the asymmetric problem in multilingual NMT, i.e., one translation task A can benefit from another translation task B but task B will be harmed by task A. To address this problem, we propose a fuzzy task clustering method for multilingual NMT. Specifically, we employ task affinity, defined as the loss change of one translation task caused by the training of another, as the clustering criterion. Next, we cluster the translation tasks based on the task affinity, such that tasks from the same cluster can benefit each other. For each cluster, we further find out a set of auxiliary translation tasks that benefit the tasks in this cluster. In this way, the model for each cluster is trained not only on the tasks in the cluster but also on the auxiliary tasks. We conduct extensive experiments for one-to-many, manyto-one, and many-to-many translation scenarios to verify the effectiveness of our method.",
        "author": "Qian Wang; Jiajun Zhang",
        "authorids": "/q/qian-wang/; /j/jiajun-zhang/",
        "bibtex": "@inproceedings{wang-zhang-2022-addressing,\n    title = \"Addressing Asymmetry in Multilingual Neural Machine Translation with Fuzzy Task Clustering\",\n    author = \"Wang, Qian  and\n      Zhang, Jiajun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.455/\",\n    pages = \"5129--5141\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.455.pdf",
        "site": "https://aclanthology.org/2022.coling-1.455/",
        "pdf_size": 680714,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bT1CbzVWrUMJ:scholar.google.com/&scioq=Addressing+Asymmetry+in+Multilingual+Neural+Machine+Translation+with+Fuzzy+Task+Clustering&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+Beijing Academy of Artificial Intelligence, Beijing, China",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1+2",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": ";http://www.ucas.ac.cn;https://www.baaic.cn",
        "aff_unique_abbr": ";UCAS;BAAI",
        "aff_campus_unique_index": "0+0;0+0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.84",
        "title": "Addressing Leakage in Self-Supervised Contextualized Code Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We address contextualized code retrieval, the search for code snippets helpful to fill gaps in a partial input program. Our approach facilitates a large-scale self-supervised contrastive training by splitting source code randomly into contexts and targets. To combat leakage between the two, we suggest a novel approach based on mutual identifier masking, dedentation, and the selection of syntax-aligned targets. Our second contribution is a new dataset for direct evaluation of contextualized code retrieval, based on a dataset of manually aligned subpassages of code clones. Our experiments demonstrate that the proposed approach improves retrieval substantially, and yields new state-of-the-art results for code clone and defect detection.",
        "author": "Johannes Villmow; Viola Campos; Adrian Ulges; Ulrich Schwanecke",
        "authorids": "/j/johannes-villmow/; /v/viola-campos/; /a/adrian-ulges/; /u/ulrich-schwanecke/",
        "bibtex": "@inproceedings{villmow-etal-2022-addressing,\n    title = \"Addressing Leakage in Self-Supervised Contextualized Code Retrieval\",\n    author = \"Villmow, Johannes  and\n      Campos, Viola  and\n      Ulges, Adrian  and\n      Schwanecke, Ulrich\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.84/\",\n    pages = \"1006--1013\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.84.pdf",
        "site": "https://aclanthology.org/2022.coling-1.84/",
        "pdf_size": 463425,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=742023894525381213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "RheinMain University of Applied Sciences; RheinMain University of Applied Sciences; RheinMain University of Applied Sciences; RheinMain University of Applied Sciences",
        "aff_domain": "hs-rm.de;hs-rm.de;hs-rm.de;hs-rm.de",
        "email": "hs-rm.de;hs-rm.de;hs-rm.de;hs-rm.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "RheinMain University of Applied Sciences",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rheinmain.de",
        "aff_unique_abbr": "RMUAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.137",
        "title": "Addressing Limitations of Encoder-Decoder Based Approach to Text-to-SQL",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most attempts on Text-to-SQL task using encoder-decoder approach show a big problem of dramatic decline in performance for new databases. For the popular Spider dataset, despite models achieving 70% accuracy on its development or test sets, the same models show a huge decline below 20% accuracy for unseen databases. The root causes for this problem are complex and they cannot be easily fixed by adding more manually created training. In this paper we address the problem and propose a solution that is a hybrid system using automated training-data augmentation technique. Our system consists of a rule-based and a deep learning components that interact to understand crucial information in a given query and produce correct SQL as a result. It achieves double-digit percentage improvement for databases that are not part of the Spider corpus.",
        "author": "Octavian Popescu; Irene Manotas; Ngoc Phuoc An Vo; Hangu Yeo; Elahe Khorashani; Vadim Sheinin",
        "authorids": "/o/octavian-popescu/; /i/irene-manotas/; /n/ngoc-phuoc-an-vo/; /h/hangu-yeo/; /e/elahe-khorashani/; /v/vadim-sheinin/",
        "bibtex": "@inproceedings{popescu-etal-2022-addressing,\n    title = \"Addressing Limitations of Encoder-Decoder Based Approach to Text-to-{SQL}\",\n    author = \"Popescu, Octavian  and\n      Manotas, Irene  and\n      Vo, Ngoc Phuoc An  and\n      Yeo, Hangu  and\n      Khorashani, Elahe  and\n      Sheinin, Vadim\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.137/\",\n    pages = \"1593--1603\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.137.pdf",
        "site": "https://aclanthology.org/2022.coling-1.137/",
        "pdf_size": 1071624,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1850823770640444854&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research",
        "aff_domain": "us.ibm.com;ibm.com;ibm.com;us.ibm.com;ibm.com;us.ibm.com",
        "email": "us.ibm.com;ibm.com;ibm.com;us.ibm.com;ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.465",
        "title": "Adversarial Training on Disentangling Meaning and Language Representations for Unsupervised Quality Estimation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a method to distill language-agnostic meaning embeddings from multilingual sentence encoders for unsupervised quality estimation of machine translation. Our method facilitates that the meaning embeddings focus on semantics by adversarial training that attempts to eliminate language-specific information. Experimental results on unsupervised quality estimation reveal that our method achieved higher correlations with human evaluations.",
        "author": "Yuto Kuroda; Tomoyuki Kajiwara; Yuki Arase; Takashi Ninomiya",
        "authorids": "/y/yuto-kuroda/; /t/tomoyuki-kajiwara/; /y/yuki-arase/; /t/takashi-ninomiya/",
        "bibtex": "@inproceedings{kuroda-etal-2022-adversarial,\n    title = \"Adversarial Training on Disentangling Meaning and Language Representations for Unsupervised Quality Estimation\",\n    author = \"Kuroda, Yuto  and\n      Kajiwara, Tomoyuki  and\n      Arase, Yuki  and\n      Ninomiya, Takashi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.465/\",\n    pages = \"5240--5245\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.465.pdf",
        "site": "https://aclanthology.org/2022.coling-1.465/",
        "pdf_size": 348958,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8630256829359946157&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Ehime University; Ehime University; Osaka University; Ehime University",
        "aff_domain": "ai.cs.ehime-u.ac.jp;cs.ehime-u.ac.jp;ist.osaka-u.ac.jp;cs.ehime-u.ac.jp",
        "email": "ai.cs.ehime-u.ac.jp;cs.ehime-u.ac.jp;ist.osaka-u.ac.jp;cs.ehime-u.ac.jp",
        "github": "https://github.com/kuro961/MEAT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Ehime University;Osaka University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ehime-u.ac.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "Ehime U;Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.269",
        "title": "AiM: Taking Answers in Mind to Correct Chinese Cloze Tests in Educational Applications",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "To automatically correct handwritten assignments, the traditional approach is to use an OCR model to recognize characters and compare them to answers. The OCR model easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference. However, teachers always have these answers in mind to review and correct assignments. In this paper, we focus on the Chinese cloze tests correction and propose a multimodal approach(named AiM). The encoded representations of answers interact with the visual information of students\u2019 handwriting. Instead of predicting \u2018right\u2019 or \u2018wrong\u2019, we perform the sequence labeling on the answer text to infer which answer character differs from the handwritten content in a fine-grained way. We take samples of OCR datasets as the positive samples for this task, and develop a negative sample augmentation method to scale up the training data. Experimental results show that AiM outperforms OCR-based methods by a large margin. Extensive studies demonstrate the effectiveness of our multimodal approach.",
        "author": "Yusen Zhang; Zhongli Li; Qingyu Zhou; Ziyi Liu; Chao Li; Mina Ma; Yunbo Cao; Hongzhi Liu",
        "authorids": "/y/yusen-zhang/; /z/zhongli-li/; /q/qingyu-zhou/; /z/ziyi-liu/; /c/chao-li/; /m/mina-ma/; /y/yunbo-cao/; /h/hongzhi-liu/",
        "bibtex": "@inproceedings{zhang-etal-2022-aim,\n    title = \"{A}i{M}: Taking Answers in Mind to Correct {C}hinese Cloze Tests in Educational Applications\",\n    author = \"Zhang, Yusen  and\n      Li, Zhongli  and\n      Zhou, Qingyu  and\n      Liu, Ziyi  and\n      Li, Chao  and\n      Ma, Mina  and\n      Cao, Yunbo  and\n      Liu, Hongzhi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.269/\",\n    pages = \"3042--3053\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.269.pdf",
        "site": "https://aclanthology.org/2022.coling-1.269/",
        "pdf_size": 1226614,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6778847234927986945&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Peking University; Tencent Cloud Xiaowei; Tencent Cloud Xiaowei; Peking University; Xiaomi Group; Tencent Cloud Xiaowei; Tencent Cloud Xiaowei; Peking University",
        "aff_domain": "stu.pku.edu.cn;tencent.com;tencent.com;stu.pku.edu.cn;xiaomi.com;tencent.com;tencent.com;ss.pku.edu.cn",
        "email": "stu.pku.edu.cn;tencent.com;tencent.com;stu.pku.edu.cn;xiaomi.com;tencent.com;tencent.com;ss.pku.edu.cn",
        "github": "https://github.com/YusenZhang826/AiM",
        "project": "http://kousuan.yuanfudao.com; https://jiazhang.zuoyebang.com; https://www.zuoye.ai",
        "author_num": 8,
        "aff_unique_index": "0;1;1;0;2;1;1;0",
        "aff_unique_norm": "Peking University;Tencent;Xiaomi Corporation",
        "aff_unique_dep": ";Tencent Cloud Xiaowei;",
        "aff_unique_url": "http://www.pku.edu.cn;https://cloud.tencent.com;https://www.xiaomi.com",
        "aff_unique_abbr": "Peking U;Tencent;Xiaomi",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.375",
        "title": "Aligning Multilingual Embeddings for Improved Code-switched Natural Language Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual pretrained models, while effective on monolingual data, need additional training to work well with code-switched text. In this work, we present a novel idea of training multilingual models with alignment objectives using parallel text so as to explicitly align word representations with the same underlying semantics across languages. Such an explicit alignment step has a positive downstream effect and improves performance on multiple code-switched NLP tasks. We explore two alignment strategies and report improvements of up to 7.32%, 0.76% and 1.9% on Hindi-English Sentiment Analysis, Named Entity Recognition and Question Answering tasks compared to a competitive baseline model.",
        "author": "Barah Fazili; Preethi Jyothi",
        "authorids": "/b/barah-fazili/; /p/preethi-jyothi/",
        "bibtex": "@inproceedings{fazili-jyothi-2022-aligning,\n    title = \"Aligning Multilingual Embeddings for Improved Code-switched Natural Language Understanding\",\n    author = \"Fazili, Barah  and\n      Jyothi, Preethi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.375/\",\n    pages = \"4268--4273\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.375.pdf",
        "site": "https://aclanthology.org/2022.coling-1.375/",
        "pdf_size": 229784,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11884219970398475474&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "IIT Bombay; IIT Bombay",
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IITB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mumbai",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.466",
        "title": "Alleviating the Inequality of Attention Heads for Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent studies show that the attention heads in Transformer are not equal. We relate this phenomenon to the imbalance training of multi-head attention and the model dependence on specific heads. To tackle this problem, we propose a simple masking method: HeadMask, in two specific ways. Experiments show that translation improvements are achieved on multiple language pairs. Subsequent empirical analyses also support our assumption and confirm the effectiveness of the method.",
        "author": "Zewei Sun; Shujian Huang; Xinyu Dai; Jiajun Chen",
        "authorids": "/z/zewei-sun/; /s/shujian-huang/; /x/xinyu-dai/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{sun-etal-2022-alleviating,\n    title = \"Alleviating the Inequality of Attention Heads for Neural Machine Translation\",\n    author = \"Sun, Zewei  and\n      Huang, Shujian  and\n      Dai, Xinyu  and\n      Chen, Jiajun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.466/\",\n    pages = \"5246--5250\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.466.pdf",
        "site": "https://aclanthology.org/2022.coling-1.466/",
        "pdf_size": 425535,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=255395579208762752&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "ByteDance AI Lab; State Key Laboratory for Novel Software Technology, Nanjing University + Peng Cheng Laboratory, China; State Key Laboratory for Novel Software Technology, Nanjing University; State Key Laboratory for Novel Software Technology, Nanjing University",
        "aff_domain": "bytedance.com;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "email": "bytedance.com;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;1;1",
        "aff_unique_norm": "ByteDance;Nanjing University;Peng Cheng Laboratory",
        "aff_unique_dep": "AI Lab;State Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "https://www.bytedance.com;http://www.nju.edu.cn;",
        "aff_unique_abbr": "ByteDance;Nanjing University;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.130",
        "title": "An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic math problem solving has attracted much attention of NLP researchers recently. However, most of the works focus on the solving of Math Word Problems (MWPs). In this paper, we study on the Geometric Problem Solving based on neural networks. Solving geometric problems requires the integration of text and diagram information as well as the knowledge of the relevant theorems. The lack of high-quality datasets and efficient neural geometric solvers impedes the development of automatic geometric problems solving. Based on GeoQA, we newly annotate 2,518 geometric problems with richer types and greater difficulty to form an augmented benchmark dataset GeoQA+, containing 6,027 problems in training set and 7,528 totally. We further perform data augmentation method to expand the training set to 12,054. Besides, we design a Dual Parallel text Encoder DPE to efficiently encode long and medium-length problem text. The experimental results validate the effectiveness of GeoQA+ and DPE module, and the accuracy of automatic geometric problem solving is improved to 66.09%.",
        "author": "Jie Cao; Jing Xiao",
        "authorids": "/j/jie-cao/; /j/jing-xiao/",
        "bibtex": "@inproceedings{cao-xiao-2022-augmented,\n    title = \"An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding\",\n    author = \"Cao, Jie  and\n      Xiao, Jing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.130/\",\n    pages = \"1511--1520\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.130.pdf",
        "site": "https://aclanthology.org/2022.coling-1.130/",
        "pdf_size": 2532377,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1432582388987393573&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, South China Normal University; School of Computer Science, South China Normal University",
        "aff_domain": "m.scnu.edu.cn;scnu.edu.cn",
        "email": "m.scnu.edu.cn;scnu.edu.cn",
        "github": "https://github.com/SCNU203/GeoQA-Plus",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "South China Normal University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.scnu.edu.cn",
        "aff_unique_abbr": "SCNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.558",
        "title": "An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization Framework Based on Semantic Blocks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Unsupervised summarization methods have achieved remarkable results by incorporating representations from pre-trained language models. However, existing methods fail to consider efficiency and effectiveness at the same time when the input document is extremely long. To tackle this problem, in this paper, we proposed an efficient Coarse-to-Fine Facet-Aware Ranking (C2F-FAR) framework for unsupervised long document summarization, which is based on the semantic block. The semantic block refers to continuous sentences in the document that describe the same facet. Specifically, we address this problem by converting the one-step ranking method into the hierarchical multi-granularity two-stage ranking. In the coarse-level stage, we proposed a new segment algorithm to split the document into facet-aware semantic blocks and then filter insignificant blocks. In the fine-level stage, we select salient sentences in each block and then extract the final summary from selected sentences. We evaluate our framework on four long document summarization datasets: Gov-Report, BillSum, arXiv, and PubMed. Our C2F-FAR can achieve new state-of-the-art unsupervised summarization results on Gov-Report and BillSum. In addition, our method speeds up 4-28 times more than previous methods.",
        "author": "Xinnian Liang; Jing Li; Shuangzhi Wu; Jiali Zeng; Yufan Jiang; Mu Li; Zhoujun Li",
        "authorids": "/x/xinnian-liang/; /j/jing-li/; /s/shuangzhi-wu/; /j/jiali-zeng/; /y/yufan-jiang/; /m/mu-li/; /z/zhoujun-li/",
        "bibtex": "@inproceedings{liang-etal-2022-efficient,\n    title = \"An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization Framework Based on Semantic Blocks\",\n    author = \"Liang, Xinnian  and\n      Li, Jing  and\n      Wu, Shuangzhi  and\n      Zeng, Jiali  and\n      Jiang, Yufan  and\n      Li, Mu  and\n      Li, Zhoujun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.558/\",\n    pages = \"6415--6425\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.558.pdf",
        "site": "https://aclanthology.org/2022.coling-1.558/",
        "pdf_size": 1021355,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11504962459811767632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/xnliang98/c2f-far",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.426",
        "title": "An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Unsupervised sentence embeddings learning has been recently dominated by contrastive learning methods (e.g., SimCSE), which keep positive pairs similar and push negative pairs apart. The contrast operation aims to keep as much information as possible by maximizing the mutual information between positive instances, which leads to redundant information in sentence embedding. To address this problem, we present an information minimization based contrastive learning InforMin-CL model to retain the useful information and discard the redundant information by maximizing the mutual information and minimizing the information entropy between positive instances meanwhile for unsupervised sentence representation learning. Specifically, we find that information minimization can be achieved by simple contrast and reconstruction objectives. The reconstruction operation reconstitutes the positive instance via the other positive instance to minimize the information entropy between positive instances. We evaluate our model on fourteen downstream tasks, including both supervised and unsupervised (semantic textual similarity) tasks. Extensive experimental results show that our InforMin-CL obtains a state-of-the-art performance.",
        "author": "Shaobin Chen; Jie Zhou; Yuling Sun; Liang He",
        "authorids": "/s/shaobin-chen/; /j/jie-zhou/; /y/yuling-sun/; /l/liang-he/",
        "bibtex": "@inproceedings{chen-etal-2022-information,\n    title = \"An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning\",\n    author = \"Chen, Shaobin  and\n      Zhou, Jie  and\n      Sun, Yuling  and\n      He, Liang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.426/\",\n    pages = \"4821--4831\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.426.pdf",
        "site": "https://aclanthology.org/2022.coling-1.426/",
        "pdf_size": 437288,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16746411527799533259&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, East China Normal University; School of Computer Science, Fudan University; School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University",
        "aff_domain": "stu.ecnu.edu.cn;fudan.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;fudan.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn",
        "github": "https://github.com/Bin199/InforMin-CL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "East China Normal University;Fudan University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.fudan.edu.cn",
        "aff_unique_abbr": "ECNU;Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.191",
        "title": "An MRC Framework for Semantic Role Labeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantic Role Labeling (SRL) aims at recognizing the predicate-argument structure of a sentence and can be decomposed into two subtasks: predicate disambiguation and argument labeling. Prior work deals with these two tasks independently, which ignores the semantic connection between the two tasks. In this paper, we propose to use the machine reading comprehension (MRC) framework to bridge this gap. We formalize predicate disambiguation as multiple-choice machine reading comprehension, where the descriptions of candidate senses of a given predicate are used as options to select the correct sense. The chosen predicate sense is then used to determine the semantic roles for that predicate, and these semantic roles are used to construct the query for another MRC model for argument labeling. In this way, we are able to leverage both the predicate semantics and the semantic role semantics for argument labeling. We also propose to select a subset of all the possible semantic roles for computational efficiency. Experiments show that the proposed framework achieves state-of-the-art or comparable results to previous work.",
        "author": "Nan Wang; Jiwei Li; Yuxian Meng; Xiaofei Sun; Han Qiu; Ziyao Wang; Guoyin Wang; Jun He",
        "authorids": "/n/nan-wang/; /j/jiwei-li/; /y/yuxian-meng/; /x/xiaofei-sun/; /h/han-qiu/; /z/ziyao-wang/; /g/guoyin-wang/; /j/jun-he/",
        "bibtex": "@inproceedings{wang-etal-2022-mrc,\n    title = \"An {MRC} Framework for Semantic Role Labeling\",\n    author = \"Wang, Nan  and\n      Li, Jiwei  and\n      Meng, Yuxian  and\n      Sun, Xiaofei  and\n      Qiu, Han  and\n      Wang, Ziyao  and\n      Wang, Guoyin  and\n      He, Jun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.191/\",\n    pages = \"2188--2198\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.191.pdf",
        "site": "https://aclanthology.org/2022.coling-1.191/",
        "pdf_size": 404739,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11607661776454113321&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Key Laboratory of Data Engineering and Knowledge Engineering of MOE, School of Information, Renmin University of China; Shannon.AI+Zhejiang University; Shannon.AI; Tsinghua University; University of Leeds; Amazon Alexa AI; Amazon Alexa AI; Key Laboratory of Data Engineering and Knowledge Engineering of MOE, School of Information, Renmin University of China",
        "aff_domain": "ruc.edu.cn;shannonai.com; ; ; ; ; ;ruc.edu.cn",
        "email": "ruc.edu.cn;shannonai.com; ; ; ; ; ;ruc.edu.cn",
        "github": "https://github.com/ShannonAI/MRC-SRL",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1+2;1;3;4;5;5;0",
        "aff_unique_norm": "Renmin University of China;Shannon.AI;Zhejiang University;Tsinghua University;University of Leeds;Amazon",
        "aff_unique_dep": "School of Information;;;;;Alexa AI",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.shannon.ai;https://www.zju.edu.cn;https://www.tsinghua.edu.cn;https://www.leeds.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "RUC;Shannon.AI;ZJU;THU;Leeds;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;1;0;2;1;1;0",
        "aff_country_unique": "China;United States;United Kingdom"
    },
    {
        "id": "2022.coling-1.257",
        "title": "Analytic Automated Essay Scoring Based on Deep Neural Networks Integrating Multidimensional Item Response Theory",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Essay exams have been attracting attention as a way of measuring the higher-order abilities of examinees, but they have two major drawbacks in that grading them is expensive and raises questions about fairness. As an approach to overcome these problems, automated essay scoring (AES) is in increasing need. Many AES models based on deep neural networks have been proposed in recent years and have achieved high accuracy, but most of these models are designed to predict only a single overall score. However, to provide detailed feedback in practical situations, we often require not only the overall score but also analytic scores corresponding to various aspects of the essay. Several neural AES models that can predict both the analytic scores and the overall score have also been proposed for this very purpose. However, conventional models are designed to have complex neural architectures for each analytic score, which makes interpreting the score prediction difficult. To improve the interpretability of the prediction while maintaining scoring accuracy, we propose a new neural model for automated analytic scoring that integrates a multidimensional item response theory model, which is a popular psychometric model.",
        "author": "Takumi Shibata; Masaki Uto",
        "authorids": "/t/takumi-shibata/; /m/masaki-uto/",
        "bibtex": "@inproceedings{shibata-uto-2022-analytic,\n    title = \"Analytic Automated Essay Scoring Based on Deep Neural Networks Integrating Multidimensional Item Response Theory\",\n    author = \"Shibata, Takumi  and\n      Uto, Masaki\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.257/\",\n    pages = \"2917--2926\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.257.pdf",
        "site": "https://aclanthology.org/2022.coling-1.257/",
        "pdf_size": 760365,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17495095754970888279&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "The University of Electro-Communications; The University of Electro-Communications",
        "aff_domain": "ai.lab.uec.ac.jp;ai.lab.uec.ac.jp",
        "email": "ai.lab.uec.ac.jp;ai.lab.uec.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Electro-Communications",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uec.ac.jp",
        "aff_unique_abbr": "UEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.600",
        "title": "Analyzing Persuasion Strategies of Debaters on Social Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing studies on the analysis of persuasion in online discussions focus on investigating the effectiveness of comments in discussions and ignore the analysis of the effectiveness of debaters over multiple discussions. In this paper, we propose to quantify debaters effectiveness in the online discussion platform: \u201cChangeMyView\u201d in order to explore diverse insights into their persuasion strategies. In particular, targeting debaters with different levels of effectiveness (e.g., good vs. bad), various behavioral characteristics (e..g, engagement) and text stylistic features (e.g., used frames) of debaters are carefully examined, leading to several outcomes that can be the backbone of writing assistants and persuasive text generation.",
        "author": "Matti Wiegmann; Khalid Al Khatib; Vishal Khanna; Benno Stein",
        "authorids": "/m/matti-wiegmann/; /k/khalid-al-khatib/; /v/vishal-khanna/; /b/benno-stein/",
        "bibtex": "@inproceedings{wiegmann-etal-2022-analyzing,\n    title = \"Analyzing Persuasion Strategies of Debaters on Social Media\",\n    author = \"Wiegmann, Matti  and\n      Al Khatib, Khalid  and\n      Khanna, Vishal  and\n      Stein, Benno\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.600/\",\n    pages = \"6897--6905\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.600.pdf",
        "site": "https://aclanthology.org/2022.coling-1.600/",
        "pdf_size": 3854161,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8163255647548695466&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Bauhaus-Universit\u00e4t Weimar; University of Groningen; Bauhaus-Universit\u00e4t Weimar; Bauhaus-Universit\u00e4t Weimar",
        "aff_domain": "uni-weimar.de;rug.nl;uni-weimar.de;uni-weimar.de",
        "email": "uni-weimar.de;rug.nl;uni-weimar.de;uni-weimar.de",
        "github": "",
        "project": "https://doi.org/10.5281/zenodo.7034173",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Bauhaus-Universit\u00e4t Weimar;University of Groningen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bauhaus-university.de;https://www.rug.nl",
        "aff_unique_abbr": "Bauhaus-Uni Weimar;RUG",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Weimar;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Germany;Netherlands"
    },
    {
        "id": "2022.coling-1.542",
        "title": "Analyzing the Dialect Diversity in Multi-document Summaries",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Social media posts provide a compelling, yet challenging source of data of diverse perspectives from many socially salient groups. Automatic text summarization algorithms make this data accessible at scale by compressing large collections of documents into short summaries that preserve salient information from the source text. In this work, we take a complementary approach to analyzing and improving the quality of summaries generated from social media data in terms of their ability to represent salient as well as diverse perspectives. We introduce a novel dataset, DivSumm, of dialect diverse tweets and human-written extractive and abstractive summaries. Then, we study the extent of dialect diversity reflected in human-written reference summaries as well as system-generated summaries. The results of our extensive experiments suggest that humans annotate fairly well-balanced dialect diverse summaries, and that cluster-based pre-processing approaches seem beneficial in improving the overall quality of the system-generated summaries without loss in diversity.",
        "author": "Olubusayo Olabisi; Aaron Hudson; Antonie Jetter; Ameeta Agrawal",
        "authorids": "/o/olubusayo-olabisi/; /a/aaron-hudson/; /a/antonie-jetter/; /a/ameeta-agrawal/",
        "bibtex": "@inproceedings{olabisi-etal-2022-analyzing,\n    title = \"Analyzing the Dialect Diversity in Multi-document Summaries\",\n    author = \"Olabisi, Olubusayo  and\n      Hudson, Aaron  and\n      Jetter, Antonie  and\n      Agrawal, Ameeta\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.542/\",\n    pages = \"6208--6221\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.542.pdf",
        "site": "https://aclanthology.org/2022.coling-1.542/",
        "pdf_size": 808057,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4558267672495747829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Portland State University; Portland State University; Portland State University; Portland State University",
        "aff_domain": "pdx.edu;pdx.edu;pdx.edu;pdx.edu",
        "email": "pdx.edu;pdx.edu;pdx.edu;pdx.edu",
        "github": "https://github.com/PortNLP/DivSumm",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Portland State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.pdx.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.394",
        "title": "Applying Natural Annotation and Curriculum Learning to Named Entity Recognition for Under-Resourced Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current practices in building new NLP models for low-resourced languages rely either on Machine Translation of training sets from better resourced languages or on cross-lingual transfer from them. Still we can see a considerable performance gap between the models originally trained within better resourced languages and the models transferred from them. In this study we test the possibility of (1) using natural annotation to build synthetic training sets from resources not initially designed for the target downstream task and (2) employing curriculum learning methods to select the most suitable examples from synthetic training sets. We test this hypothesis across seven Slavic languages and across three curriculum learning strategies on Named Entity Recognition as the downstream task. We also test the possibility of fine-tuning the synthetic resources to reflect linguistic properties, such as the grammatical case and gender, both of which are important for the Slavic languages. We demonstrate the possibility to achieve the mean F1 score of 0.78 across the three basic entities types for Belarusian starting from zero resources in comparison to the baseline of 0.63 using the zero-shot transfer from English. For comparison, the English model trained on the original set achieves the mean F1-score of 0.75. The experimental results are available from https://github.com/ValeraLobov/SlavNER",
        "author": "Valeriy Lobov; Alexandra Ivoylova; Serge Sharoff",
        "authorids": "/v/valeriy-lobov/; /a/alexandra-ivoylova/; /s/serge-sharoff/",
        "bibtex": "@inproceedings{lobov-etal-2022-applying,\n    title = \"Applying Natural Annotation and Curriculum Learning to Named Entity Recognition for Under-Resourced Languages\",\n    author = \"Lobov, Valeriy  and\n      Ivoylova, Alexandra  and\n      Sharoff, Serge\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.394/\",\n    pages = \"4468--4480\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.394.pdf",
        "site": "https://aclanthology.org/2022.coling-1.394/",
        "pdf_size": 473528,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:L3rzweWAxDgJ:scholar.google.com/&scioq=Applying+Natural+Annotation+and+Curriculum+Learning+to+Named+Entity+Recognition+for+Under-Resourced+Languages&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "MIPT; RSUH; School of Languages, University of Leeds, UK",
        "aff_domain": "phystech.edu;gmail.com;leeds.ac.uk",
        "email": "phystech.edu;gmail.com;leeds.ac.uk",
        "github": "https://github.com/ValeraLobov/SlavNER",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Moscow Institute of Physics and Technology;Ryazan State University;University of Leeds",
        "aff_unique_dep": ";;School of Languages",
        "aff_unique_url": "https://mipt.ru;http://www.rsu.ru;https://www.leeds.ac.uk",
        "aff_unique_abbr": "MIPT;RSUH;Leeds",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Russia;United Kingdom"
    },
    {
        "id": "2022.coling-1.128",
        "title": "ArT: All-round Thinker for Unsupervised Commonsense Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Without labeled question-answer pairs for necessary training, unsupervised commonsense question-answering (QA) appears to be extremely challenging due to its indispensable unique prerequisite on commonsense source like knowledge bases (KBs), which are usually highly resource consuming in construction. Recently pre-trained language models (PLMs) show effectiveness as an alternative for commonsense clues when they play a role of knowledge generator. However, existing work either relies on large-scale in-domain or out-of-domain labeled data, or fails to generate knowledge of high quality in a general way. Motivated by human thinking experience, we propose an approach of All-round Thinker (ArT) by fully taking association during knowledge generating. In detail, our model first focuses on key parts in the given context, and then generates highly related knowledge on such a basis in an association way like human thinking. Besides, for casual reasoning, a reverse thinking mechanism is especially added to further enhance bidirectional inferring between cause and effect. ArT is totally unsupervised and KBs-free. We evaluate it on three commonsense QA benchmarks: COPA, SocialIQA and SCT. On all scales of PLM backbones, ArT shows its brilliant performance and outperforms previous advanced unsupervised models.",
        "author": "Jiawei Wang; Hai Zhao",
        "authorids": "/j/jiawei-wang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{wang-zhao-2022-art,\n    title = \"{A}r{T}: All-round Thinker for Unsupervised Commonsense Question Answering\",\n    author = \"Wang, Jiawei  and\n      Zhao, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.128/\",\n    pages = \"1490--1501\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.128.pdf",
        "site": "https://aclanthology.org/2022.coling-1.128/",
        "pdf_size": 785745,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17153420362493356715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/WangJW424/commonsenseQA-ArT",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.148",
        "title": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional ranking-based KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency.",
        "author": "Yu Gu; Yu Su",
        "authorids": "/y/yu-gu/; /y/yu-su/",
        "bibtex": "@inproceedings{gu-su-2022-arcaneqa,\n    title = \"{A}rcane{QA}: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering\",\n    author = \"Gu, Yu  and\n      Su, Yu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.148/\",\n    pages = \"1718--1731\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.148.pdf",
        "site": "https://aclanthology.org/2022.coling-1.148/",
        "pdf_size": 849170,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1216591836288905725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The Ohio State University; The Ohio State University",
        "aff_domain": "osu.edu;osu.edu",
        "email": "osu.edu;osu.edu",
        "github": "",
        "project": "dki-lab/ArcaneQA",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.226",
        "title": "Are People Located in the Places They Mention in Their Tweets? A Multimodal Approach",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper introduces the problem of determining whether people are located in the places they mention in their tweets. In particular, we investigate the role of text and images to solve this challenging problem. We present a new corpus of tweets that contain both text and images. Our analyses show that this problem is multimodal at its core: human judgments depend on whether annotators have access to the text, the image, or both. Experimental results show that a neural architecture that combines both modalities yields better results. We also conduct an error analysis to provide insights into why and when each modality is beneficial.",
        "author": "Zhaomin Xiao; Eduardo Blanco",
        "authorids": "/z/zhaomin-xiao/; /e/eduardo-blanco/",
        "bibtex": "@inproceedings{xiao-blanco-2022-people,\n    title = \"Are People Located in the Places They Mention in Their Tweets? A Multimodal Approach\",\n    author = \"Xiao, Zhaomin  and\n      Blanco, Eduardo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.226/\",\n    pages = \"2561--2571\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.226.pdf",
        "site": "https://aclanthology.org/2022.coling-1.226/",
        "pdf_size": 6577750,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7789202241311325586&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of North Texas; University of Arizona",
        "aff_domain": "my.unt.edu;arizona.edu",
        "email": "my.unt.edu;arizona.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of North Texas;University of Arizona",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unt.edu;https://www.arizona.edu",
        "aff_unique_abbr": "UNT;UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.318",
        "title": "Are Pretrained Multilingual Models Equally Fair across Languages?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pretrained multilingual language models can help bridge the digital language divide, enabling high-quality NLP models for lower-resourced languages. Studies of multilingual models have so far focused on performance, consistency, and cross-lingual generalisation. However, with their wide-spread application in the wild and downstream societal impact, it is important to put multilingual models under the same scrutiny as monolingual models. This work investigates the group fairness of multilingual models, asking whether these models are equally fair across languages. To this end, we create a new four-way multilingual dataset of parallel cloze test examples (MozArt), equipped with demographic information (balanced with regard to gender and native tongue) about the test participants. We evaluate three multilingual models on MozArt \u2013mBERT, XLM-R, and mT5\u2013 and show that across the four target languages, the three models exhibit different levels of group disparity, e.g., exhibiting near-equal risk for Spanish, but high levels of disparity for German.",
        "author": "Laura Cabello Piqueras; Anders S\u00f8gaard",
        "authorids": "/l/laura-cabello-piqueras/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{cabello-piqueras-sogaard-2022-pretrained,\n    title = \"Are Pretrained Multilingual Models Equally Fair across Languages?\",\n    author = \"Cabello Piqueras, Laura  and\n      S{\\o}gaard, Anders\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.318/\",\n    pages = \"3597--3605\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.318.pdf",
        "site": "https://aclanthology.org/2022.coling-1.318/",
        "pdf_size": 795983,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11558388416933356432&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Copenhagen; University of Copenhagen",
        "aff_domain": "di.ku.dk;di.ku.dk",
        "email": "di.ku.dk;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Copenhagen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ku.dk",
        "aff_unique_abbr": "UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "2022.coling-1.491",
        "title": "Are Visual-Linguistic Models Commonsense Knowledge Bases?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the recent success of pretrained language models as on-the-fly knowledge sources for various downstream tasks, they are shown to inadequately represent trivial common facts that vision typically captures. This limits their application to natural language understanding tasks that require commonsense knowledge. We seek to determine the capability of pretrained visual-linguistic models as knowledge sources on demand. To this end, we systematically compare language-only and visual-linguistic models in a zero-shot commonsense question answering inference task. We find that visual-linguistic models are highly promising regarding their benefit for text-only tasks on certain types of commonsense knowledge associated with the visual world. Surprisingly, this knowledge can be activated even when no visual input is given during inference, suggesting an effective multimodal fusion during pretraining. However, we reveal that there is still a huge space for improvement towards better cross-modal reasoning abilities and pretraining strategies for event understanding.",
        "author": "Hsiu-Yu Yang; Carina Silberer",
        "authorids": "/h/hsiu-yu-yang/; /c/carina-silberer/",
        "bibtex": "@inproceedings{yang-silberer-2022-visual,\n    title = \"Are Visual-Linguistic Models Commonsense Knowledge Bases?\",\n    author = \"Yang, Hsiu-Yu  and\n      Silberer, Carina\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.491/\",\n    pages = \"5542--5559\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.491.pdf",
        "site": "https://aclanthology.org/2022.coling-1.491/",
        "pdf_size": 1979805,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14354295986554163847&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 0,
        "aff": "University of Stuttgart; University of Stuttgart",
        "aff_domain": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "email": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "github": "https://github.com/Mallory24/CS_Probing",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-stuttgart.de",
        "aff_unique_abbr": "USTuttgart",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.540",
        "title": "ArgLegalSumm: Improving Abstractive Summarization of Legal Documents with Argument Mining",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines.",
        "author": "Mohamed Elaraby; Diane Litman",
        "authorids": "/m/mohamed-elaraby/; /d/diane-litman/",
        "bibtex": "@inproceedings{elaraby-litman-2022-arglegalsumm,\n    title = \"{A}rg{L}egal{S}umm: Improving Abstractive Summarization of Legal Documents with Argument Mining\",\n    author = \"Elaraby, Mohamed  and\n      Litman, Diane\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.540/\",\n    pages = \"6187--6194\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.540.pdf",
        "site": "https://aclanthology.org/2022.coling-1.540/",
        "pdf_size": 370953,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17339386574084288782&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "University of Pittsburgh; University of Pittsburgh",
        "aff_domain": "pitt.edu;pitt.edu",
        "email": "pitt.edu;pitt.edu",
        "github": "https://github.com/EngSalem/arglegalsumm",
        "project": "https://www.canlii.org/en/",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.408",
        "title": "Ask Question First for Enhancing Lifelong Language Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Lifelong language learning aims to stream learning NLP tasks while retaining knowledge of previous tasks. Previous works based on the language model and following data-free constraint approaches have explored formatting all data as \u201cbegin token (B) + context (C) + question (Q) + answer (A)\u201d for different tasks. However, they still suffer from catastrophic forgetting and are exacerbated when the previous task\u2019s pseudo data is insufficient for the following reasons: (1) The model has difficulty generating task-corresponding pseudo data, and (2) A is prone to error when A and C are separated by Q because the information of the C is diminished before generating A. Therefore, we propose the Ask Question First and Replay Question (AQF-RQ), including a novel data format \u201cBQCA\u201d and a new training task to train pseudo questions of previous tasks. Experimental results demonstrate that AQF-RQ makes it easier for the model to generate more pseudo data that match corresponding tasks, and is more robust to both sufficient and insufficient pseudo-data when the task boundary is both clear and unclear. AQF-RQ can achieve only 0.36% lower performance than multi-task learning.",
        "author": "Han Wang; Ruiliu Fu; Xuejun Zhang; Jun Zhou; Qingwei Zhao",
        "authorids": "/h/han-wang/; /r/ruiliu-fu/; /x/xuejun-zhang/; /j/jun-zhou/; /q/qingwei-zhao/",
        "bibtex": "@inproceedings{wang-etal-2022-ask,\n    title = \"Ask Question First for Enhancing Lifelong Language Learning\",\n    author = \"Wang, Han  and\n      Fu, Ruiliu  and\n      Zhang, Xuejun  and\n      Zhou, Jun  and\n      Zhao, Qingwei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.408/\",\n    pages = \"4610--4621\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.408.pdf",
        "site": "https://aclanthology.org/2022.coling-1.408/",
        "pdf_size": 1431295,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VoJ8kT7HFNcJ:scholar.google.com/&scioq=Ask+Question+First+for+Enhancing+Lifelong+Language+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Institute of Acoustics, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "hccl.ioa.ac.cn;hccl.ioa.ac.cn;hccl.ioa.ac.cn;hccl.ioa.ac.cn;hccl.ioa.ac.cn",
        "email": "hccl.ioa.ac.cn;hccl.ioa.ac.cn;hccl.ioa.ac.cn;hccl.ioa.ac.cn;hccl.ioa.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Acoustics;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.217",
        "title": "Aspect-based Sentiment Analysis as Machine Reading Comprehension",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing studies typically handle aspect-based sentiment analysis by stacking multiple neural modules, which inevitably result in severe error propagation. Instead, we propose a novel end-to-end framework, MRCOOL: MRC-PrOmpt mOdeL framework, where numerous sentiment aspects are elicited by a machine reading comprehension (MRC) model and their corresponding sentiment polarities are classified in a prompt learning way. Experiments show that our end-to-end framework consistently yields promising results on widely-used benchmark datasets which significantly outperform existing state-of-the-art models or achieve comparable performance.",
        "author": "Yifei Yang; Hai Zhao",
        "authorids": "/y/yifei-yang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{yang-zhao-2022-aspect,\n    title = \"Aspect-based Sentiment Analysis as Machine Reading Comprehension\",\n    author = \"Yang, Yifei  and\n      Zhao, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.217/\",\n    pages = \"2461--2471\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.217.pdf",
        "site": "https://aclanthology.org/2022.coling-1.217/",
        "pdf_size": 515247,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2761511622886975834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/yangyifei729/MRC4absa",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.379",
        "title": "Assessing Digital Language Support on a Global Scale",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The users of endangered languages struggle to thrive in a digitally-mediated world. We have developed an automated method for assessing how well every language recognized by ISO 639 is faring in terms of digital language support. The assessment is based on scraping the names of supported languages from the websites of 143 digital tools selected to represent a full range of ways that digital technology can support languages. The method uses Mokken scale analysis to produce an explainable model for quantifying digital language support and monitoring it on a global scale.",
        "author": "Gary F. Simons; Abbey L. L. Thomas; Chad K. K. White",
        "authorids": "/g/gary-f-simons/; /a/abbey-l-l-thomas/; /c/chad-k-k-white/",
        "bibtex": "@inproceedings{simons-etal-2022-assessing,\n    title = \"Assessing Digital Language Support on a Global Scale\",\n    author = \"Simons, Gary F.  and\n      Thomas, Abbey L. L.  and\n      White, Chad K. K.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.379/\",\n    pages = \"4299--4305\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.379.pdf",
        "site": "https://aclanthology.org/2022.coling-1.379/",
        "pdf_size": 543759,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3069127682648945099&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "SIL International; The University of Texas at Dallas; SIL International",
        "aff_domain": "sil.org;utdallas.edu;sil.org",
        "email": "sil.org;utdallas.edu;sil.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "SIL International;University of Texas at Dallas",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sil.org;https://www.utdallas.edu",
        "aff_unique_abbr": "SIL;UT Dallas",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.604",
        "title": "Asymmetric Mutual Learning for Multi-source Unsupervised Sentiment Adaptation with Dynamic Feature Network",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, fine-tuning the pre-trained language model (PrLM) on labeled sentiment datasets demonstrates impressive performance. However, collecting labeled sentiment dataset is time-consuming, and fine-tuning the whole PrLM brings about much computation cost. To this end, we focus on multi-source unsupervised sentiment adaptation problem with the pre-trained features, which is more practical and challenging. We first design a dynamic feature network to fully exploit the extracted pre-trained features for efficient domain adaptation. Meanwhile, with the difference of the traditional source-target domain alignment methods, we propose a novel asymmetric mutual learning strategy, which can robustly estimate the pseudo-labels of the target domain with the knowledge from all the other source models. Experiments on multiple sentiment benchmarks show that our method outperforms the recent state-of-the-art approaches, and we also conduct extensive ablation studies to verify the effectiveness of each the proposed module.",
        "author": "Rui Li; Cheng Liu; Dazhi Jiang",
        "authorids": "/r/rui-li/; /c/cheng-liu/; /d/dazhi-jiang/",
        "bibtex": "@inproceedings{li-etal-2022-asymmetric,\n    title = \"Asymmetric Mutual Learning for Multi-source Unsupervised Sentiment Adaptation with Dynamic Feature Network\",\n    author = \"Li, Rui  and\n      Liu, Cheng  and\n      Jiang, Dazhi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.604/\",\n    pages = \"6934--6943\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.604.pdf",
        "site": "https://aclanthology.org/2022.coling-1.604/",
        "pdf_size": 491135,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14041914973258512697&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, Shantou University; Department of Computer Science, Shantou University; Department of Computer Science, Shantou University",
        "aff_domain": "stu.edu.cn;stu.edu.cn;stu.edu.cn",
        "email": "stu.edu.cn;stu.edu.cn;stu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shantou University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.422",
        "title": "Attention Networks for Augmenting Clinical Text with Support Sets for Diagnosis Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Diagnosis prediction on admission notes is a core clinical task. However, these notes may incompletely describe the patient. Also, clinical language models may suffer from idiosyncratic language or imbalanced vocabulary for describing diseases or symptoms. We tackle the task of diagnosis prediction, which consists of predicting future patient diagnoses from clinical texts at the time of admission. We improve the performance on this task by introducing an additional signal from support sets of diagnostic codes from prior admissions or as they emerge during differential diagnosis. To enhance the robustness of diagnosis prediction methods, we propose to augment clinical text with potentially complementary set data from diagnosis codes from previous patient visits or from codes that emerge from the current admission as they become available through diagnostics. We discuss novel attention network architectures and augmentation strategies to solve this problem. Our experiments reveal that support sets improve the performance drastically to predict less common diagnosis codes. Our approach clearly outperforms the previous state-of-the-art PubMedBERT baseline by up 3% points. Furthermore, we find that support sets drastically improve the performance for pregnancy- and gynecology-related diagnoses up to 32.9% points compared to the baseline.",
        "author": "Paul Grundmann; Tom Oberhauser; Felix Gers; Alexander L\u00f6ser",
        "authorids": "/p/paul-grundmann/; /t/tom-oberhauser/; /f/felix-gers/; /a/alexander-loser/",
        "bibtex": "@inproceedings{grundmann-etal-2022-attention,\n    title = \"Attention Networks for Augmenting Clinical Text with Support Sets for Diagnosis Prediction\",\n    author = {Grundmann, Paul  and\n      Oberhauser, Tom  and\n      Gers, Felix  and\n      L{\\\"o}ser, Alexander},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.422/\",\n    pages = \"4765--4775\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.422.pdf",
        "site": "https://aclanthology.org/2022.coling-1.422/",
        "pdf_size": 500580,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15261521475550539857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Berliner Hochschule f\u00fcr Technik; Berliner Hochschule f\u00fcr Technik; Berliner Hochschule f\u00fcr Technik; Berliner Hochschule f\u00fcr Technik",
        "aff_domain": "bht-berlin.de;bht-berlin.de;bht-berlin.de;bht-berlin.de",
        "email": "bht-berlin.de;bht-berlin.de;bht-berlin.de;bht-berlin.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Berliner Hochschule f\u00fcr Technik",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bht-berlin.de/",
        "aff_unique_abbr": "BHT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.88",
        "title": "Attribute Injection for Pretrained Language Models: A New Benchmark and an Efficient Method",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Metadata attributes (e.g., user and product IDs from reviews) can be incorporated as additional inputs to neural-based NLP models, by expanding the architecture of the models to improve performance. However, recent models rely on pretrained language models (PLMs), in which previously used techniques for attribute injection are either nontrivial or cost-ineffective. In this paper, we introduce a benchmark for evaluating attribute injection models, which comprises eight datasets across a diverse range of tasks and domains and six synthetically sparsified ones. We also propose a lightweight and memory-efficient method to inject attributes into PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules, to include attributes both independently of or jointly with the text. We use approximation techniques to parameterize the model efficiently for domains with large attribute vocabularies, and training mechanisms to handle multi-labeled and sparse attributes. Extensive experiments and analyses show that our method outperforms previous attribute injection methods and achieves state-of-the-art performance on all datasets.",
        "author": "Reinald Kim Amplayo; Kang Min Yoo; Sang-Woo Lee",
        "authorids": "/r/reinald-kim-amplayo/; /k/kang-min-yoo/; /s/sang-woo-lee/",
        "bibtex": "@inproceedings{amplayo-etal-2022-attribute,\n    title = \"Attribute Injection for Pretrained Language Models: A New Benchmark and an Efficient Method\",\n    author = \"Amplayo, Reinald Kim  and\n      Yoo, Kang Min  and\n      Lee, Sang-Woo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.88/\",\n    pages = \"1051--1064\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.88.pdf",
        "site": "https://aclanthology.org/2022.coling-1.88/",
        "pdf_size": 456936,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=798772139926494536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Edinburgh + NA VER AI Lab; NA VER AI Lab; NA VER AI Lab",
        "aff_domain": "ed.ac.uk;navercorp.com;navercorp.com",
        "email": "ed.ac.uk;navercorp.com;navercorp.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Edinburgh;NAVER Corporation",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.naver.com",
        "aff_unique_abbr": "Edinburgh;NAVER",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "United Kingdom;South Korea"
    },
    {
        "id": "2022.coling-1.161",
        "title": "Augmentation, Retrieval, Generation: Event Sequence Prediction with a Three-Stage Sequence-to-Sequence Approach",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Being able to infer possible events related to a specific target is critical to natural language processing. One challenging task in this line is event sequence prediction, which aims at predicting a sequence of events given a goal. Currently existing approach models this task as a statistical induction problem, to predict a sequence of events by exploring the similarity between the given goal and the known sequences of events. However, this statistical based approach is complex and predicts a limited variety of events. At the same time this approach ignores the rich knowledge of external events that is important for predicting event sequences. In this paper, in order to predict more diverse events, we first reformulate the event sequence prediction problem as a sequence generation problem. Then to leverage external event knowledge, we propose a three-stage model including augmentation, retrieval and generation. Experimental results on the event sequence prediction dataset show that our model outperforms existing methods, demonstrating the effectiveness of the proposed model.",
        "author": "Bo Zhou; Chenhao Wang; Yubo Chen; Kang Liu; Jun Zhao; Jiexin Xu; Xiaojian Jiang; Qiuxia Li",
        "authorids": "/b/bo-zhou/; /c/chenhao-wang/; /y/yubo-chen/; /k/kang-liu/; /j/jun-zhao/; /j/jiexin-xu/; /x/xiaojian-jiang/; /q/qiuxia-li/",
        "bibtex": "@inproceedings{zhou-etal-2022-augmentation,\n    title = \"Augmentation, Retrieval, Generation: Event Sequence Prediction with a Three-Stage Sequence-to-Sequence Approach\",\n    author = \"Zhou, Bo  and\n      Wang, Chenhao  and\n      Chen, Yubo  and\n      Liu, Kang  and\n      Zhao, Jun  and\n      Xu, Jiexin  and\n      Jiang, Xiaojian  and\n      Li, Qiuxia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.161/\",\n    pages = \"1865--1874\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.161.pdf",
        "site": "https://aclanthology.org/2022.coling-1.161/",
        "pdf_size": 431920,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7831944779516027661&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA;School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA;School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA;School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA+Beijing Academy of Artificial Intelligence;School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA;China Merchants Bank;China Merchants Bank;China Merchants Bank",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;cmbchina.com;cmbchina.com;cmbchina.com",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;cmbchina.com;cmbchina.com;cmbchina.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0+1;0+1+2;0+1;3;3;3",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences, Institute of Automation;Beijing Academy of Artificial Intelligence;China Merchants Bank",
        "aff_unique_dep": "School of Artificial Intelligence;National Laboratory of Pattern Recognition;;",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ia.cas.cn;https://www.baaic.cn;https://www.cmbchina.com.cn",
        "aff_unique_abbr": "UCAS;CASIA;BAAI;CMB",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.235",
        "title": "Augmenting Legal Judgment Prediction with Contrastive Case Relations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing legal judgment prediction methods usually only consider one single case fact description as input, which may not fully utilize the information in the data such as case relations and frequency. In this paper, we propose a new perspective that introduces some contrastive case relations to construct case triples as input, and a corresponding judgment prediction framework with case triples modeling (CTM). Our CTM can more effectively utilize beneficial information to refine the encoding and decoding processes through three customized modules, including the case triple module, the relational attention module, and the category decoder module. Finally, we conduct extensive experiments on two public datasets to verify the effectiveness of our CTM, including overall evaluation, compatibility analysis, ablation studies, analysis of gain source and visualization of case representations.",
        "author": "Dugang Liu; Weihao Du; Lei Li; Weike Pan; Zhong Ming",
        "authorids": "/d/dugang-liu/; /w/weihao-du/; /l/lei-li/; /w/weike-pan/; /z/zhong-ming/",
        "bibtex": "@inproceedings{liu-etal-2022-augmenting,\n    title = \"Augmenting Legal Judgment Prediction with Contrastive Case Relations\",\n    author = \"Liu, Dugang  and\n      Du, Weihao  and\n      Li, Lei  and\n      Pan, Weike  and\n      Ming, Zhong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.235/\",\n    pages = \"2658--2667\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.235.pdf",
        "site": "https://aclanthology.org/2022.coling-1.235/",
        "pdf_size": 1302025,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12202928296952472718&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Shenzhen University, Shenzhen, China+Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China; Shenzhen University, Shenzhen, China+Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China; Hong Kong Baptist University, Hong Kong, China; Shenzhen University, Shenzhen, China+Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China; Shenzhen University, Shenzhen, China+Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China",
        "aff_domain": "gmail.com; ; ;szu.edu.cn;szu.edu.cn",
        "email": "gmail.com; ; ;szu.edu.cn;szu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;2;0+1;0+1",
        "aff_unique_norm": "Shenzhen University;Guangdong Laboratory of Artificial Intelligence and Digital Economy;Hong Kong Baptist University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.szu.edu.cn;;https://www.hkbu.edu.hk",
        "aff_unique_abbr": "SZU;GD-LAB;HKBU",
        "aff_campus_unique_index": "0+0;0+0;1;0+0;0+0",
        "aff_campus_unique": "Shenzhen;Hong Kong",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.266",
        "title": "Automated Chinese Essay Scoring from Multiple Traits",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic Essay Scoring (AES) is the task of using the computer to evaluate the quality of essays automatically. Current research on AES focuses on scoring the overall quality or single trait of prompt-specific essays. However, the users not only expect to obtain the overall score but also the instant feedback from different traits to help their writing in the real world. Therefore, we first annotate a mutli-trait dataset ACEA including 1220 argumentative essays from four traits, i.e., essay organization, topic, logic, and language. And then we design a hierarchical multi-task trait scorer HMTS to evaluate the quality of writing by modeling these four traits. Moreover, we propose an inter-sequence attention mechanism to enhance information interaction between different tasks and design the trait-specific features for various tasks in AES. The experimental results on ACEA show that our HMTS can effectively score essays from multiple traits, outperforming several strong models.",
        "author": "Yaqiong He; Feng Jiang; Xiaomin Chu; Peifeng Li",
        "authorids": "/y/yaqiong-he/; /f/feng-jiang/; /x/xiaomin-chu/; /p/peifeng-li/",
        "bibtex": "@inproceedings{he-etal-2022-automated,\n    title = \"Automated {C}hinese Essay Scoring from Multiple Traits\",\n    author = \"He, Yaqiong  and\n      Jiang, Feng  and\n      Chu, Xiaomin  and\n      Li, Peifeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.266/\",\n    pages = \"3007--3016\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.266.pdf",
        "site": "https://aclanthology.org/2022.coling-1.266/",
        "pdf_size": 452161,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1044572567043117949&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.240",
        "title": "Automated Essay Scoring via Pairwise Contrastive Regression",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automated essay scoring (AES) involves the prediction of a score relating to the writing quality of an essay. Most existing works in AES utilize regression objectives or ranking objectives respectively. However, the two types of methods are highly complementary. To this end, in this paper we take inspiration from contrastive learning and propose a novel unified Neural Pairwise Contrastive Regression (NPCR) model in which both objectives are optimized simultaneously as a single loss. Specifically, we first design a neural pairwise ranking model to guarantee the global ranking order in a large list of essays, and then we further extend this pairwise ranking model to predict the relative scores between an input essay and several reference essays. Additionally, a multi-sample voting strategy is employed for inference. We use Quadratic Weighted Kappa to evaluate our model on the public Automated Student Assessment Prize (ASAP) dataset, and the experimental results demonstrate that NPCR outperforms previous methods by a large margin, achieving the state-of-the-art average performance for the AES task.",
        "author": "Jiayi Xie; Kaiwei Cai; Li Kong; Junsheng Zhou; Weiguang Qu",
        "authorids": "/j/jiayi-xie/; /k/kaiwei-cai/; /l/li-kong/; /j/junsheng-zhou/; /w/weiguang-qu/",
        "bibtex": "@inproceedings{xie-etal-2022-automated,\n    title = \"Automated Essay Scoring via Pairwise Contrastive Regression\",\n    author = \"Xie, Jiayi  and\n      Cai, Kaiwei  and\n      Kong, Li  and\n      Zhou, Junsheng  and\n      Qu, Weiguang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.240/\",\n    pages = \"2724--2733\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.240.pdf",
        "site": "https://aclanthology.org/2022.coling-1.240/",
        "pdf_size": 394854,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7798310089646675837&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "NLP Lab, Department of Computer and Electronic Information, Nanjing Normal University, China; NLP Lab, Department of Computer and Electronic Information, Nanjing Normal University, China; NLP Lab, Department of Computer and Electronic Information, Nanjing Normal University, China; NLP Lab, Department of Computer and Electronic Information, Nanjing Normal University, China; NLP Lab, Department of Computer and Electronic Information, Nanjing Normal University, China",
        "aff_domain": "gmail.com;gmail.com;nnu.edu.cn;njnu.edu.cn;njnu.edu.cn",
        "email": "gmail.com;gmail.com;nnu.edu.cn;njnu.edu.cn;njnu.edu.cn",
        "github": "https://github.com/CarryCKW/AES-NPCR",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Nanjing Normal University",
        "aff_unique_dep": "Department of Computer and Electronic Information",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "NNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.297",
        "title": "Automatic Generation of Large-scale Multi-turn Dialogues from Reddit",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents novel methods to automatically convert posts and their comments from discussion forums such as Reddit into multi-turn dialogues. Our methods are generalizable to any forums; thus, they allow us to generate a massive amount of dialogues for diverse topics that can be used to pretrain language models. Four methods are introduced, Greedy_Baseline, Greedy_Advanced, Beam Search and Threading, which are applied to posts from 10 subreddits and assessed. Each method makes a noticeable improvement over its predecessor such that the best method shows an improvement of 36.3% over the baseline for appropriateness. Our best method is applied to posts from those 10 subreddits for the creation of a corpus comprising 10,098 dialogues (3.3M tokens), 570 of which are compared against dialogues in three other datasets, Blended Skill Talk, Daily Dialogue, and Topical Chat. Our dialogues are found to be more engaging but slightly less natural than the ones in the other datasets, while it costs a fraction of human labor and money to generate our corpus compared to the others. To the best of our knowledge, it is the first work to create a large multi-turn dialogue corpus from Reddit that can advance neural dialogue systems.",
        "author": "Daniil Huryn; William M. Hutsell; Jinho D. Choi",
        "authorids": "/d/daniil-huryn/; /w/william-m-hutsell/; /j/jinho-d-choi/",
        "bibtex": "@inproceedings{huryn-etal-2022-automatic,\n    title = \"Automatic Generation of Large-scale Multi-turn Dialogues from {R}eddit\",\n    author = \"Huryn, Daniil  and\n      Hutsell, William M.  and\n      Choi, Jinho D.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.297/\",\n    pages = \"3360--3373\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.297.pdf",
        "site": "https://aclanthology.org/2022.coling-1.297/",
        "pdf_size": 465843,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10461639511338700726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Computer Science, Emory University, Atlanta, GA, USA; Computer Science, Emory University, Atlanta, GA, USA; Computer Science, Emory University, Atlanta, GA, USA",
        "aff_domain": "gmail.com;gmail.com;emory.edu",
        "email": "gmail.com;gmail.com;emory.edu",
        "github": "https://github.com/emorynlp/reddit-to-dialogue",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Emory University",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.emory.edu",
        "aff_unique_abbr": "Emory",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.254",
        "title": "Automatic ICD Coding Exploiting Discourse Structure and Reconciled Code Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The International Classification of Diseases (ICD) is the foundation of global health statistics and epidemiology. The ICD is designed to translate health conditions into alphanumeric codes. A number of approaches have been proposed for automatic ICD coding, since manual coding is labor-intensive and there is a global shortage of healthcare workers. However, existing studies did not exploit the discourse structure of clinical notes, which provides rich contextual information for code assignment. In this paper, we exploit the discourse structure by leveraging section type classification and section type embeddings. We also focus on the class-imbalanced problem and the heterogeneous writing style between clinical notes and ICD code definitions. The proposed reconciled embedding approach is able to tackle them simultaneously. Experimental results on the MIMIC dataset show that our model outperforms all previous state-of-the-art models by a large margin. The source code is available at https://github.com/discnet2022/discnet",
        "author": "Shurui Zhang; Bozheng Zhang; Fuxin Zhang; Bo Sang; Wanchun Yang",
        "authorids": "/s/shurui-zhang/; /b/bozheng-zhang/; /f/fuxin-zhang/; /b/bo-sang/; /w/wanchun-yang/",
        "bibtex": "@inproceedings{zhang-etal-2022-automatic,\n    title = \"Automatic {ICD} Coding Exploiting Discourse Structure and Reconciled Code Embeddings\",\n    author = \"Zhang, Shurui  and\n      Zhang, Bozheng  and\n      Zhang, Fuxin  and\n      Sang, Bo  and\n      Yang, Wanchun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.254/\",\n    pages = \"2883--2891\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.254.pdf",
        "site": "https://aclanthology.org/2022.coling-1.254/",
        "pdf_size": 1300754,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=772287979653372818&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "MsunHealth Co., LTD; MsunHealth Co., LTD; MsunHealth Co., LTD; MsunHealth Co., LTD; Shandong Jiaotong University",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "github": "https://github.com/discnet2022/discnet",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "MsunHealth;Shandong Jiaotong University",
        "aff_unique_dep": "Co., LTD;",
        "aff_unique_url": ";http://www.sdjtu.edu.cn",
        "aff_unique_abbr": ";SDJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2022.coling-1.204",
        "title": "Automatic Keyphrase Generation by Incorporating Dual Copy Mechanisms in Sequence-to-Sequence Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The keyphrase generation task is a challenging work that aims to generate a set of keyphrases for a piece of text. Many previous studies based on the sequence-to-sequence model were used to generate keyphrases, and they introduce a copy mechanism to achieve good results. However, we observed that most of the keyphrases are composed of some important words (seed words) in the source text, and if these words can be identified accurately and copied to create more keyphrases, the performance of the model might be improved. To address this challenge, we propose a DualCopyNet model, which introduces an additional sequence labeling layer for identifying seed words, and further copies the words for generating new keyphrases by dual copy mechanisms. Experimental results demonstrate that our model outperforms the baseline models and achieves an obvious performance improvement.",
        "author": "Siyu Wang; Jianhui Jiang; Yao Huang; Yin Wang",
        "authorids": "/s/siyu-wang/; /j/jianhui-jiang/; /y/yao-huang/; /y/yin-wang/",
        "bibtex": "@inproceedings{wang-etal-2022-automatic-keyphrase,\n    title = \"Automatic Keyphrase Generation by Incorporating Dual Copy Mechanisms in Sequence-to-Sequence Learning\",\n    author = \"Wang, Siyu  and\n      Jiang, Jianhui  and\n      Huang, Yao  and\n      Wang, Yin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.204/\",\n    pages = \"2328--2338\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.204.pdf",
        "site": "https://aclanthology.org/2022.coling-1.204/",
        "pdf_size": 1739470,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17682376651116805073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Gusu Laboratory of Materials, Suzhou, China; Gusu Laboratory of Materials, Suzhou, China; Gusu Laboratory of Materials, Suzhou, China; Gusu Laboratory of Materials, Suzhou, China+Hongzhiwei Technology (Shanghai) Co., Ltd, Shanghai, China",
        "aff_domain": "gusulab.ac.cn;gusulab.ac.cn;gusulab.ac.cn;hzwtech.com",
        "email": "gusulab.ac.cn;gusulab.ac.cn;gusulab.ac.cn;hzwtech.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Gusu Laboratory of Materials;Hongzhiwei Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Suzhou;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.440",
        "title": "Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prompting, which casts downstream applications as language modeling tasks, has shown to be sample efficient compared to standard fine-tuning with pre-trained models. However, one pitfall of prompting is the need of manually-designed patterns, whose outcome can be unintuitive and requires large validation sets to tune. To tackle the challenge, we propose AutoSeq, a fully automatic prompting method: (1) We adopt natural language prompts on sequence-to-sequence models, enabling free-form generation and larger label search space; (2) We propose label sequences \u2013 phrases with indefinite lengths to verbalize the labels \u2013 which eliminate the need of manual templates and are more expressive than single label words; (3) We use beam search to automatically generate a large amount of label sequence candidates and propose contrastive re-ranking to get the best combinations. AutoSeq significantly outperforms other no-manual-design methods, such as soft prompt tuning, adapter tuning, and automatic search on single label words; the generated label sequences are even better than curated manual ones on a variety of tasks. Our method reveals the potential of sequence-to-sequence models in few-shot learning and sheds light on a path to generic and automatic prompting. The source code of this paper can be obtained from https://github.com/thunlp/Seq2Seq-Prompt.",
        "author": "Zichun Yu; Tianyu Gao; Zhengyan Zhang; Yankai Lin; Zhiyuan Liu; Maosong Sun; Jie Zhou",
        "authorids": "/z/zichun-yu/; /t/tianyu-gao/; /z/zhengyan-zhang/; /y/yankai-lin/; /z/zhiyuan-liu/; /m/maosong-sun/; /j/jie-zhou/",
        "bibtex": "@inproceedings{yu-etal-2022-automatic-label,\n    title = \"Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models\",\n    author = \"Yu, Zichun  and\n      Gao, Tianyu  and\n      Zhang, Zhengyan  and\n      Lin, Yankai  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.440/\",\n    pages = \"4965--4975\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.440.pdf",
        "site": "https://aclanthology.org/2022.coling-1.440/",
        "pdf_size": 354883,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16533238880230454865&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/thunlp/Seq2Seq-Prompt",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.524",
        "title": "Automatic Nominalization of Clauses through Textual Entailment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nominalization re-writes a clause as a noun phrase. It requires the transformation of the head verb of the clause into a deverbal noun, and the verb\u2019s modifiers into nominal modifiers. Past research has focused on the selection of deverbal nouns, but has paid less attention to predicting the word positions and word forms for the nominal modifiers. We propose the use of a textual entailment model for clause nominalization. We obtained the best performance by fine-tuning a textual entailment model on this task, outperforming a number of unsupervised approaches using language model scores from a state-of-the-art neural language model.",
        "author": "John S. Y. Lee; Ho Hung Lim; Carol Webster; Anton Melser",
        "authorids": "/j/john-s-y-lee/; /h/ho-hung-lim/; /c/carol-webster/; /a/anton-melser/",
        "bibtex": "@inproceedings{lee-etal-2022-automatic,\n    title = \"Automatic Nominalization of Clauses through Textual Entailment\",\n    author = \"Lee, John S. Y.  and\n      Lim, Ho Hung  and\n      Webster, Carol  and\n      Melser, Anton\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.524/\",\n    pages = \"6002--6006\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.524.pdf",
        "site": "https://aclanthology.org/2022.coling-1.524/",
        "pdf_size": 216659,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:n8vmoh8_S0cJ:scholar.google.com/&scioq=Automatic+Nominalization+of+Clauses+through+Textual+Entailment&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Department of Linguistics and Translation, City University of Hong Kong; Department of Linguistics and Translation, City University of Hong Kong; College of Professional and Continuing Education, The Hong Kong Polytechnic University; Department of Linguistics and Translation, City University of Hong Kong",
        "aff_domain": "cityu.edu.hk;cityu.edu.hk;cpce-polyu.edu.hk;my.cityu.edu.hk",
        "email": "cityu.edu.hk;cityu.edu.hk;cpce-polyu.edu.hk;my.cityu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "City University of Hong Kong;The Hong Kong Polytechnic University",
        "aff_unique_dep": "Department of Linguistics and Translation;College of Professional and Continuing Education",
        "aff_unique_url": "https://www.cityu.edu.hk;https://www.polyu.edu.hk",
        "aff_unique_abbr": "CityU;PolyU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.25",
        "title": "Autoregressive Entity Generation for End-to-End Task-Oriented Dialog",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Task-oriented dialog (TOD) systems are often required to interact with an external knowledge base (KB) to retrieve necessary entity (e.g., restaurants) information to support their response generation. Most current end-to-end TOD systems either retrieve the KB information explicitly or embed it into model parameters for implicit access. While the first approach demands scanning the KB at each turn of response generation, which is inefficient when the KB scales up, the second approach shows higher flexibility and efficiency. In either approach, the response shall contain attributes of the same entity, however the systems may generate a response with conflicting entities. To address this, we propose to generate the entity autoregressively before leveraging it to guide the response generation in an end-to-end system. To ensure entity consistency, we impose a trie constraint on the decoding of an entity. We also introduce a logit concatenation strategy to facilitate gradient backpropagation for end-to-end training. Experiments on MultiWOZ 2.1 single and CAMREST show that our system can generate more high-quality and entity-consistent responses in an end-to-end manner.",
        "author": "Guanhuan Huang; Xiaojun Quan; Qifan Wang",
        "authorids": "/g/guanhuan-huang/; /x/xiaojun-quan/; /q/qifan-wang/",
        "bibtex": "@inproceedings{huang-etal-2022-autoregressive,\n    title = \"Autoregressive Entity Generation for End-to-End Task-Oriented Dialog\",\n    author = \"Huang, Guanhuan  and\n      Quan, Xiaojun  and\n      Wang, Qifan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.25/\",\n    pages = \"323--332\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.25.pdf",
        "site": "https://aclanthology.org/2022.coling-1.25/",
        "pdf_size": 542598,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13324298485848907053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Meta AI, Menlo Park, CA, USA",
        "aff_domain": "mail2.sysu.edu.cn;mail.sysu.edu.cn;fb.com",
        "email": "mail2.sysu.edu.cn;mail.sysu.edu.cn;fb.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Sun Yat-sen University;Meta AI",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "http://www.sysu.edu.cn;https://meta.ai",
        "aff_unique_abbr": "SYSU;Meta AI",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Guangzhou;Menlo Park",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.324",
        "title": "BECEL: Benchmark for Consistency Evaluation of Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Behavioural consistency is a critical condition for a language model (LM) to become trustworthy like humans. Despite its importance, however, there is little consensus on the definition of LM consistency, resulting in different definitions across many studies. In this paper, we first propose the idea of LM consistency based on behavioural consistency and establish a taxonomy that classifies previously studied consistencies into several sub-categories. Next, we create a new benchmark that allows us to evaluate a model on 19 test cases, distinguished by multiple types of consistency and diverse downstream tasks. Through extensive experiments on the new benchmark, we ascertain that none of the modern pre-trained language models (PLMs) performs well in every test case, while exhibiting high inconsistency in many cases. Our experimental results suggest that a unified benchmark that covers broad aspects (i.e., multiple consistency types and tasks) is essential for a more precise evaluation.",
        "author": "Myeongjun Jang; Deuk Sin Kwon; Thomas Lukasiewicz",
        "authorids": "/m/myeongjun-jang/; /d/deuk-sin-kwon/; /t/thomas-lukasiewicz/",
        "bibtex": "@inproceedings{jang-etal-2022-becel,\n    title = \"{BECEL}: Benchmark for Consistency Evaluation of Language Models\",\n    author = \"Jang, Myeongjun  and\n      Kwon, Deuk Sin  and\n      Lukasiewicz, Thomas\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.324/\",\n    pages = \"3680--3696\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.324.pdf",
        "site": "https://aclanthology.org/2022.coling-1.324/",
        "pdf_size": 682236,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=701762948894658740&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Oxford, UK; Language Super Intelligence Labs, SK Telecom, South Korea; Institute of Logic and Computation, TU Wien, Austria + Department of Computer Science, University of Oxford, UK",
        "aff_domain": "cs.ox.ac.uk;gmail.com;cs.ox.ac.uk",
        "email": "cs.ox.ac.uk;gmail.com;cs.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "University of Oxford;SK Telecom;TU Wien",
        "aff_unique_dep": "Department of Computer Science;Language Super Intelligence Labs;Institute of Logic and Computation",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.sktelecom.com;https://www.tuwien.ac.at",
        "aff_unique_abbr": "Oxford;SKT;TU Wien",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2+0",
        "aff_country_unique": "United Kingdom;South Korea;Austria"
    },
    {
        "id": "2022.coling-1.104",
        "title": "BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-label Text Classification (MLTC) is the task of categorizing documents into one or more topics. Considering the large volumes of data and varying domains of such tasks, fully supervised learning requires manually fully annotated datasets which is costly and time-consuming. In this paper, we propose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text Classification (WSMLTC) model that reduces the need for full supervision. This new model (1) produces BERT sentence embeddings and calibrates them using a flow model, (2) generates an initial topic-document matrix by averaging results of a seeded sparse topic model and a textual entailment model which only require surface name of topics and 4-6 seed words per topic, and (3) adopts a VAE framework to reconstruct the embeddings under the guidance of the topic-document matrix. Finally, (4) it uses the means produced by the encoder model in the VAE architecture as predictions for MLTC. Experimental results on 6 multi-label datasets show that BFV can substantially outperform other baseline WSMLTC models in key metrics and achieve approximately 84% performance of a fully-supervised model.",
        "author": "Ziwen Liu; Josep Grau-Bove; Scott Allan Orr",
        "authorids": "/z/ziwen-liu/; /j/josep-grau-bove/; /s/scott-allan-orr/",
        "bibtex": "@inproceedings{liu-etal-2022-bert,\n    title = \"{BERT}-Flow-{VAE}: A Weakly-supervised Model for Multi-Label Text Classification\",\n    author = \"Liu, Ziwen  and\n      Grau-Bove, Josep  and\n      Orr, Scott Allan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.104/\",\n    pages = \"1203--1220\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.104.pdf",
        "site": "https://aclanthology.org/2022.coling-1.104/",
        "pdf_size": 2516911,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14031292001856285998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University College London; University College London; University College London",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "email": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.389",
        "title": "BRCC and SentiBahasaRojak: The First Bahasa Rojak Corpus for Pretraining and Sentiment Analysis Dataset",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Code-mixing refers to the mixed use of multiple languages. It is prevalent in multilingual societies and is also one of the most challenging natural language processing tasks. In this paper, we study Bahasa Rojak, a dialect popular in Malaysia that consists of English, Malay, and Chinese. Aiming to establish a model to deal with the code-mixing phenomena of Bahasa Rojak, we use data augmentation to automatically construct the first Bahasa Rojak corpus for pre-training language models, which we name the Bahasa Rojak Crawled Corpus (BRCC). We also develop a new pre-trained model called \u201cMixed XLM\u201d. The model can tag the language of the input token automatically to process code-mixing input. Finally, to test the effectiveness of the Mixed XLM model pre-trained on BRCC for social media scenarios where code-mixing is found frequently, we compile a new Bahasa Rojak sentiment analysis dataset, SentiBahasaRojak, with a Kappa value of 0.77.",
        "author": "Nanda Putri Romadhona; Sin-En Lu; Bo-Han Lu; Richard Tzong-Han Tsai",
        "authorids": "/n/nanda-putri-romadhona/; /s/sin-en-lu/; /b/bo-han-lu/; /r/richard-tzong-han-tsai/",
        "bibtex": "@inproceedings{romadhona-etal-2022-brcc,\n    title = \"{BRCC} and {S}enti{B}ahasa{R}ojak: The First {B}ahasa Rojak Corpus for Pretraining and Sentiment Analysis Dataset\",\n    author = \"Romadhona, Nanda Putri  and\n      Lu, Sin-En  and\n      Lu, Bo-Han  and\n      Tsai, Richard Tzong-Han\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.389/\",\n    pages = \"4418--4428\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.389.pdf",
        "site": "https://aclanthology.org/2022.coling-1.389/",
        "pdf_size": 404841,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15596005348709258795&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Information Engineering, National Central University, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taiwan; Center for GIS, Research Center for Humanities and Social Sciences, Academia Sinica, Taiwan + Department of Computer Science and Information Engineering, National Central University, Taiwan",
        "aff_domain": "gmail.com;g.ncu.edu.tw;g.ncu.edu.tw;g.ncu.edu.tw",
        "email": "gmail.com;g.ncu.edu.tw;g.ncu.edu.tw;g.ncu.edu.tw",
        "github": "",
        "project": "https://data.depositar.io/dataset/brcc_and_sentibahasarojak",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1+0",
        "aff_unique_norm": "National Central University;Academia Sinica",
        "aff_unique_dep": "Department of Computer Science and Information Engineering;Center for GIS, Research Center for Humanities and Social Sciences",
        "aff_unique_url": "https://www.ncu.edu.tw;https://www.sinica.edu.tw",
        "aff_unique_abbr": "NCU;AS",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.487",
        "title": "Belief Revision Based Caption Re-ranker with Visual Semantic Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this work, we focus on improving the captions generated by image-caption generation systems. We propose a novel re-ranking approach that leverages visual-semantic measures to identify the ideal caption that maximally captures the visual information in the image. Our re-ranker utilizes the Belief Revision framework (Blok et al., 2003) to calibrate the original likelihood of the top-n captions by explicitly exploiting semantic relatedness between the depicted caption and the visual context. Our experiments demonstrate the utility of our approach, where we observe that our re-ranker can enhance the performance of a typical image-captioning system without necessity of any additional training or fine-tuning.",
        "author": "Ahmed Sabir; Francesc Moreno-Noguer; Pranava Madhyastha; Llu\u00eds Padr\u00f3",
        "authorids": "/a/ahmed-sabir/; /f/francesc-moreno-noguer/; /p/pranava-swaroop-madhyastha/; /l/lluis-padro/",
        "bibtex": "@inproceedings{sabir-etal-2022-belief,\n    title = \"Belief Revision Based Caption Re-ranker with Visual Semantic Information\",\n    author = \"Sabir, Ahmed  and\n      Moreno-Noguer, Francesc  and\n      Madhyastha, Pranava  and\n      Padr{\\'o}, Llu{\\'i}s\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.487/\",\n    pages = \"5488--5506\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.487.pdf",
        "site": "https://aclanthology.org/2022.coling-1.487/",
        "pdf_size": 4110187,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17621169876015712376&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Department, Universitat Polit\u00e8cnica de Catalunya, Barcelona, Spain; Institut de Rob\u00f2tica i Inform\u00e0tica Industrial, CSIC-UPC, Barcelona, Spain; City, University of London, London, UK; Computer Science Department, Universitat Polit\u00e8cnica de Catalunya, Barcelona, Spain",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "https://github.com/ahmedssabir/Belief-Revision-Score",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Catalunya;Institut de Rob\u00f2tica i Inform\u00e0tica Industrial;City, University of London",
        "aff_unique_dep": "Computer Science Department;;",
        "aff_unique_url": "https://www.upc.edu;http://www.iri.upc.edu/;https://www.city.ac.uk",
        "aff_unique_abbr": "UPC;IRI;City, University of London",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Barcelona;London",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Spain;United Kingdom"
    },
    {
        "id": "2022.coling-1.313",
        "title": "Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated MedLane dataset and the effectiveness of the proposed model DECLARE.",
        "author": "Junyu Luo; Junxian Lin; Chi Lin; Cao Xiao; Xinning Gui; Fenglong Ma",
        "authorids": "/j/junyu-luo/; /j/junxian-lin/; /c/chi-lin/; /c/cao-xiao/; /x/xinning-gui/; /f/fenglong-ma/",
        "bibtex": "@inproceedings{luo-etal-2022-benchmarking,\n    title = \"Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation\",\n    author = \"Luo, Junyu  and\n      Lin, Junxian  and\n      Lin, Chi  and\n      Xiao, Cao  and\n      Gui, Xinning  and\n      Ma, Fenglong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.313/\",\n    pages = \"3550--3562\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.313.pdf",
        "site": "https://aclanthology.org/2022.coling-1.313/",
        "pdf_size": 767091,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13334591935385573403&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/machinelearning4health/MedLane",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.525",
        "title": "Benchmarking Compositionality with Formal Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP acquire this ability while learning from data is an open question. In this paper, we look at this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties (number of states, alphabet size, number of transitions etc.) contribute to learnability of a compositional relation by a neural network. In general, we find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition.",
        "author": "Josef Valvoda; Naomi Saphra; Jonathan Rawski; Adina Williams; Ryan Cotterell",
        "authorids": "/j/josef-valvoda/; /n/naomi-saphra/; /j/jonathan-rawski/; /a/adina-williams/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{valvoda-etal-2022-benchmarking,\n    title = \"Benchmarking Compositionality with Formal Languages\",\n    author = \"Valvoda, Josef  and\n      Saphra, Naomi  and\n      Rawski, Jonathan  and\n      Williams, Adina  and\n      Cotterell, Ryan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.525/\",\n    pages = \"6007--6018\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.525.pdf",
        "site": "https://aclanthology.org/2022.coling-1.525/",
        "pdf_size": 710551,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4612062573308159666&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Cambridge; New York University; San Jos \u00b4e State University; FAIR; ETH Z \u00a8urich",
        "aff_domain": "cam.ac.uk;nyu.edu;sjsu.edu;fb.com;inf.ethz.ch",
        "email": "cam.ac.uk;nyu.edu;sjsu.edu;fb.com;inf.ethz.ch",
        "github": "https://github.com/valvoda/neuralTransducer",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "University of Cambridge;New York University;San Jose State University;Facebook AI Research;ETH Z\u00fcrich",
        "aff_unique_dep": ";;;Facebook AI Research;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.nyu.edu;https://www.sjsu.edu;https://research.facebook.com;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;NYU;SJSU;FAIR;ETHZ",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "Cambridge;;San Jose;Z\u00fcrich",
        "aff_country_unique_index": "0;1;1;1;2",
        "aff_country_unique": "United Kingdom;United States;Switzerland"
    },
    {
        "id": "2022.coling-1.485",
        "title": "BiBL: AMR Parsing and Generation with Bidirectional Bayesian Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Abstract Meaning Representation (AMR) offers a unified semantic representation for natural language sentences. Thus transformation between AMR and text yields two transition tasks in opposite directions, i.e., Text-to-AMR parsing and AMR-to-Text generation. Existing AMR studies only focus on one-side improvements despite the duality of the two tasks, and their improvements are greatly attributed to the inclusion of large extra training data or complex structure modifications which harm the inference speed. Instead, we propose data-efficient Bidirectional Bayesian learning (BiBL) to facilitate bidirectional information transition by adopting a single-stage multitasking strategy so that the resulting model may enjoy much lighter training at the same time. Evaluation on benchmark datasets shows that our proposed BiBL outperforms strong previous seq2seq refinements without the help of extra data which is indispensable in existing counterpart models. We release the codes of BiBL at: https://github.com/KHAKhazeus/BiBL.",
        "author": "Ziming Cheng; Zuchao Li; Hai Zhao",
        "authorids": "/z/ziming-cheng/; /z/zuchao-li/; /h/hai-zhao/",
        "bibtex": "@inproceedings{cheng-etal-2022-bibl,\n    title = \"{B}i{BL}: {AMR} Parsing and Generation with Bidirectional {B}ayesian Learning\",\n    author = \"Cheng, Ziming  and\n      Li, Zuchao  and\n      Zhao, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.485/\",\n    pages = \"5461--5475\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.485.pdf",
        "site": "https://aclanthology.org/2022.coling-1.485/",
        "pdf_size": 930165,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3017819231080239563&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/KHAKhazeus/BiBL",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.115",
        "title": "Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural Language Processing (NLP) has become increasingly utilized to provide adaptivity in educational applications. However, recent research has highlighted a variety of biases in pre-trained language models. While existing studies investigate bias in different domains, they are limited in addressing fine-grained analysis on educational corpora and text that is not English. In this work, we analyze bias across text and through multiple architectures on a corpus of 9,165 German peer-reviews collected from university students over five years. Notably, our corpus includes labels such as helpfulness, quality, and critical aspect ratings from the peer-review recipient as well as demographic attributes. We conduct a Word Embedding Association Test (WEAT) analysis on (1) our collected corpus in connection with the clustered labels, (2) the most common pre-trained German language models (T5, BERT, and GPT-2) and GloVe embeddings, and (3) the language models after fine-tuning on our collected data-set. In contrast to our initial expectations, we found that our collected corpus does not reveal many biases in the co-occurrence analysis or in the GloVe embeddings. However, the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data. With our research, we aim to contribute to the fourth UN sustainability goal (quality education) with a novel dataset, an understanding of biases in natural language education data, and the potential harms of not counteracting biases in language models for educational tasks.",
        "author": "Thiemo Wambsganss; Vinitra Swamy; Roman Rietsche; Tanja K\u00e4ser",
        "authorids": "/t/thiemo-wambsganss/; /v/vinitra-swamy/; /r/roman-rietsche/; /t/tanja-kaser/",
        "bibtex": "@inproceedings{wambsganss-etal-2022-bias,\n    title = \"Bias at a Second Glance: A Deep Dive into Bias for {G}erman Educational Peer-Review Data Modeling\",\n    author = {Wambsganss, Thiemo  and\n      Swamy, Vinitra  and\n      Rietsche, Roman  and\n      K{\\\"a}ser, Tanja},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.115/\",\n    pages = \"1344--1356\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.115.pdf",
        "site": "https://aclanthology.org/2022.coling-1.115/",
        "pdf_size": 692341,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2883070077319409395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "EPFL, Lausanne, Switzerland; EPFL, Lausanne, Switzerland; University of St.Gallen, St.Gallen, Switzerland; EPFL, Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;unisg.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;unisg.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne;University of St.Gallen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.unisg.ch",
        "aff_unique_abbr": "EPFL;HSG",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Lausanne;St.Gallen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.coling-1.109",
        "title": "Bigger Data or Fairer Data? Augmenting BERT via Active Sampling for Educational Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pretrained Language Models (PLMs), though popular, have been diagnosed to encode bias against protected groups in the representations they learn, which may harm the prediction fairness of downstream models. Given that such bias is believed to be related to the amount of demographic information carried in the learned representations, this study aimed to quantify the awareness that a PLM (i.e., BERT) has regarding people\u2019s protected attributes and augment BERT to improve prediction fairness of downstream models by inhibiting this awareness. Specifically, we developed a method to dynamically sample data to continue the pretraining of BERT and enable it to generate representations carrying minimal demographic information, which can be directly used as input to downstream models for fairer predictions. By experimenting on the task of classifying educational forum posts and measuring fairness between students of different gender or first-language backgrounds, we showed that, compared to a baseline without any additional pretraining, our method improved not only fairness (with a maximum improvement of 52.33%) but also accuracy (with a maximum improvement of 2.53%). Our method can be generalized to any PLM and demographic attributes. All the codes used in this study can be accessed via https://github.com/lsha49/FairBERT_deploy.",
        "author": "Lele Sha; Yuheng Li; Dragan Gasevic; Guanliang Chen",
        "authorids": "/l/lele-sha/; /y/yuheng-li/; /d/dragan-gasevic/; /g/guanliang-chen/",
        "bibtex": "@inproceedings{sha-etal-2022-bigger,\n    title = \"Bigger Data or Fairer Data? Augmenting {BERT} via Active Sampling for Educational Text Classification\",\n    author = \"Sha, Lele  and\n      Li, Yuheng  and\n      Gasevic, Dragan  and\n      Chen, Guanliang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.109/\",\n    pages = \"1275--1285\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.109.pdf",
        "site": "https://aclanthology.org/2022.coling-1.109/",
        "pdf_size": 544188,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17253818982495918870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Centre for Learning Analytics, Monash University; Centre for Learning Analytics, Monash University; Centre for Learning Analytics, Monash University; Centre for Learning Analytics, Monash University",
        "aff_domain": "monash.edu;monash.edu;monash.edu;monash.edu",
        "email": "monash.edu;monash.edu;monash.edu;monash.edu",
        "github": "https://github.com/lsha49/FairBERT_deploy",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Centre for Learning Analytics",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.coling-1.323",
        "title": "Biographically Relevant Tweets \u2013 a New Dataset, Linguistic Analysis and Classification Experiments",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a new dataset comprising tweets for the novel task of detecting biographically relevant utterances. Biographically relevant utterances are all those utterances that reveal some persistent and non-trivial information about the author of a tweet, e.g. habits, (dis)likes, family status, physical appearance, employment information, health issues etc. Unlike previous research we do not restrict biographical relevance to a small fixed set of pre-defined relations. Next to classification experiments employing state-of-the-art classifiers to establish strong baselines for future work, we carry out a linguistic analysis that compares the predictiveness of various high-level features. We also show that the task is different from established tasks, such as aspectual classification or sentiment analysis.",
        "author": "Michael Wiegand; Rebecca Wilm; Katja Markert",
        "authorids": "/m/michael-wiegand/; /r/rebecca-wilm/; /k/katja-markert/",
        "bibtex": "@inproceedings{wiegand-etal-2022-biographically,\n    title = \"Biographically Relevant Tweets {--} a New Dataset, Linguistic Analysis and Classification Experiments\",\n    author = \"Wiegand, Michael  and\n      Wilm, Rebecca  and\n      Markert, Katja\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.323/\",\n    pages = \"3669--3679\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.323.pdf",
        "site": "https://aclanthology.org/2022.coling-1.323/",
        "pdf_size": 264611,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:i1TvSnVT8wwJ:scholar.google.com/&scioq=Biographically+Relevant+Tweets+%E2%80%93+a+New+Dataset,+Linguistic+Analysis+and+Classification+Experiments&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Digital Age Research Center (D!ARC), Alpen-Adria-Universit\u00e4t Klagenfurt, AT-9020 Klagenfurt, Austria; Institute of Computational Linguistics, Heidelberg University, D-69120 Heidelberg, Germany; Institute of Computational Linguistics, Heidelberg University, D-69120 Heidelberg, Germany",
        "aff_domain": "aau.at;cl.uni-heidelberg.de;cl.uni-heidelberg.de",
        "email": "aau.at;cl.uni-heidelberg.de;cl.uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Alpen-Adria-Universit\u00e4t Klagenfurt;Heidelberg University",
        "aff_unique_dep": "Digital Age Research Center (D!ARC);Institute of Computational Linguistics",
        "aff_unique_url": "https://www.aau.at;https://www.uni-heidelberg.de",
        "aff_unique_abbr": "AAU;Uni Heidelberg",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Klagenfurt;Heidelberg",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "id": "2022.coling-1.521",
        "title": "Boosting Code Summarization by Embedding Code Structures",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent research on code summarization relies on the structural information from the abstract syntax tree (AST) of source codes. It is, however, questionable whether it is the most effective to use AST for expressing the structural information. We find that a program dependency graph (PDG) can represent the structure of a code more effectively. We propose PDG Boosting Module (PBM) that encodes PDG into graph embedding and the framework to implement the proposed PBM with the existing models. PBM achieves improvements of 6.67% (BLEU) and 7.47% (ROUGE) on average. We then analyze the experimental results, and examine how PBM helps the training of baseline models and its performance robustness. For the validation of robustness, we measure the performance of an out-of-domain benchmark dataset, and confirm its robustness. In addition, we apply a new evaluation measure, SBERT score, to evaluate the semantic performance. The models implemented with PBM improve the performance of SBERT score. This implies that they generate summaries that are semantically more similar to the reference summary.",
        "author": "Jikyoeng Son; Joonghyuk Hahn; HyeonTae Seo; Yo-Sub Han",
        "authorids": "/j/jikyoeng-son/; /j/joonghyuk-hahn/; /h/hyeontae-seo/; /y/yo-sub-han/",
        "bibtex": "@inproceedings{son-etal-2022-boosting,\n    title = \"Boosting Code Summarization by Embedding Code Structures\",\n    author = \"Son, Jikyoeng  and\n      Hahn, Joonghyuk  and\n      Seo, HyeonTae  and\n      Han, Yo-Sub\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.521/\",\n    pages = \"5966--5977\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.521.pdf",
        "site": "https://aclanthology.org/2022.coling-1.521/",
        "pdf_size": 556037,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2355218943762873573&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Yonsei University; Department of Computer Science, Yonsei University; Department of Computer Science, Yonsei University; Department of Computer Science, Yonsei University",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.249",
        "title": "Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Understanding news content is critical to improving the quality of news recommendation. To achieve this goal, recent studies have attempted to apply pre-trained language models (PLMs) such as BERT for semantic-enhanced news recommendation. Despite their great success in offline evaluation, it is still a challenge to apply such large PLMs in real-time ranking model due to the stringent requirement in inference and updating time. To bridge this gap, we propose a plug-and-play pre-trainer, namely PREC, to learn both user and news encoders through multi-task pre-training. Instead of directly leveraging sophisticated PLMs for end-to-end inference, we focus on how to use the derived user and item representations to boost the performance of conventional lightweight models for click-through-rate prediction. This enables efficient online inference as well as compatibility to conventional models, which would significantly ease the practical deployment. We validate the effectiveness of PREC through both offline evaluation on public datasets and online A/B testing in an industrial application.",
        "author": "Qijiong Liu; Jieming Zhu; Quanyu Dai; Xiao-Ming Wu",
        "authorids": "/q/qijiong-liu/; /j/jieming-zhu/; /q/quanyu-dai/; /x/xiao-ming-wu/",
        "bibtex": "@inproceedings{liu-etal-2022-boosting,\n    title = \"Boosting Deep {CTR} Prediction with a Plug-and-Play Pre-trainer for News Recommendation\",\n    author = \"Liu, Qijiong  and\n      Zhu, Jieming  and\n      Dai, Quanyu  and\n      Wu, Xiao-Ming\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.249/\",\n    pages = \"2823--2833\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.249.pdf",
        "site": "https://aclanthology.org/2022.coling-1.249/",
        "pdf_size": 1662928,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12868460545894889972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "The Hong Kong Polytechnic University; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; The Hong Kong Polytechnic University",
        "aff_domain": "connect.polyu.hk;ieee.org;connect.polyu.hk;polyu.edu.hk",
        "email": "connect.polyu.hk;ieee.org;connect.polyu.hk;polyu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "The Hong Kong Polytechnic University;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.polyu.edu.hk;https://www.huawei.com",
        "aff_unique_abbr": "PolyU;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.489",
        "title": "Building Joint Relationship Attention Network for Image-Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Attention based methods for image-text generation often focus on visual features individually, while ignoring relationship information among image features that provides important guidance for generating sentences. To alleviate this issue, in this work we propose the Joint Relationship Attention Network (JRAN) that novelly explores the relationships among the features. Specifically, different from the previous relationship based approaches that only explore the single relationship in the image, our JRAN can effectively learn two relationships, the visual relationships among region features and the visual-semantic relationships between region features and semantic features, and further make a dynamic trade-off between them during outputting the relationship representation. Moreover, we devise a new relationship based attention, which can adaptively focus on the output relationship representation when predicting different words. Extensive experiments on large-scale MSCOCO and small-scale Flickr30k datasets show that JRAN achieves state-of-the-art performance. More remarkably, JRAN achieves new 28.3% and 58.2% performance in terms of BLEU4 and CIDEr metric on Flickr30k dataset.",
        "author": "Changzhi Wang; Xiaodong Gu",
        "authorids": "/c/changzhi-wang/; /x/xiaodong-gu/",
        "bibtex": "@inproceedings{wang-gu-2022-building,\n    title = \"Building Joint Relationship Attention Network for Image-Text Generation\",\n    author = \"Wang, Changzhi  and\n      Gu, Xiaodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.489/\",\n    pages = \"5521--5531\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.489.pdf",
        "site": "https://aclanthology.org/2022.coling-1.489/",
        "pdf_size": 12316398,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oVIHA6bDXg0J:scholar.google.com/&scioq=Building+Joint+Relationship+Attention+Network+for+Image-Text+Generation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Electronic Engineering, Fudan University; Department of Electronic Engineering, Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.388",
        "title": "Byte-based Multilingual NMT for Endangered Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual neural machine translation (MNMT) jointly trains a shared model for translation with multiple language pairs. However, traditional subword-based MNMT approaches suffer from out-of-vocabulary (OOV) issues and representation bottleneck, which often degrades translation performance on certain language pairs. While byte tokenization is used to tackle the OOV problems in neural machine translation (NMT), until now its capability has not been validated in MNMT. Additionally, existing work has not studied how byte encoding can benefit endangered language translation to our knowledge. We propose a byte-based multilingual neural machine translation system (BMNMT) to alleviate the representation bottleneck and improve translation performance in endangered languages. Furthermore, we design a random byte mapping method with an ensemble prediction to enhance our model robustness. Experimental results show that our BMNMT consistently and significantly outperforms subword/word-based baselines on twelve language pairs up to +18.5 BLEU points, an 840% relative improvement.",
        "author": "Mengjiao Zhang; Jia Xu",
        "authorids": "/m/mengjiao-zhang/; /j/jia-xu/",
        "bibtex": "@inproceedings{zhang-xu-2022-byte,\n    title = \"Byte-based Multilingual {NMT} for Endangered Languages\",\n    author = \"Zhang, Mengjiao  and\n      Xu, Jia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.388/\",\n    pages = \"4407--4417\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.388.pdf",
        "site": "https://aclanthology.org/2022.coling-1.388/",
        "pdf_size": 616201,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16918690305173753058&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Stevens Institute of Technology; Stevens Institute of Technology",
        "aff_domain": "stevens.edu;stevens.edu",
        "email": "stevens.edu;stevens.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stevens Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stevens.edu",
        "aff_unique_abbr": "SIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.294",
        "title": "CCTC: A Cross-Sentence Chinese Text Correction Dataset for Native Speakers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The Chinese text correction (CTC) focuses on detecting and correcting Chinese spelling errors and grammatical errors. Most existing datasets of Chinese spelling check (CSC) and Chinese grammatical error correction (GEC) are focused on a single sentence written by Chinese-as-a-second-language (CSL) learners. We find that errors caused by native speakers differ significantly from those produced by non-native speakers. These differences make it inappropriate to use the existing test sets directly to evaluate text correction systems for native speakers. Some errors also require the cross-sentence information to be identified and corrected. In this paper, we propose a cross-sentence Chinese text correction dataset for native speakers. Concretely, we manually annotated 1,500 texts written by native speakers. The dataset consists of 30,811 sentences and more than 1,000,000 Chinese characters. It contains four types of errors: spelling errors, redundant words, missing words, and word ordering errors. We also test some state-of-the-art models on the dataset. The experimental results show that even the model with the best performance is 20 points lower than humans, which indicates that there is still much room for improvement. We hope that the new dataset can fill the gap in cross-sentence text correction for native Chinese speakers.",
        "author": "Baoxin Wang; Xingyi Duan; Dayong Wu; Wanxiang Che; Zhigang Chen; Guoping Hu",
        "authorids": "/b/baoxin-wang/; /x/xingyi-duan/; /d/dayong-wu/; /w/wanxiang-che/; /z/zhigang-chen/; /g/guoping-hu/",
        "bibtex": "@inproceedings{wang-etal-2022-cctc,\n    title = \"{CCTC}: A Cross-Sentence {C}hinese Text Correction Dataset for Native Speakers\",\n    author = \"Wang, Baoxin  and\n      Duan, Xingyi  and\n      Wu, Dayong  and\n      Che, Wanxiang  and\n      Chen, Zhigang  and\n      Hu, Guoping\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.294/\",\n    pages = \"3331--3341\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.294.pdf",
        "site": "https://aclanthology.org/2022.coling-1.294/",
        "pdf_size": 1020980,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16662654888126078295&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Research Center for SCIR, Harbin Institute of Technology, Harbin, China+State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China; State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China; State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China+iFLYTEK AI Research (Hebei), Langfang, China; Research Center for SCIR, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China+Jilin Kexun Information Technology Co., Ltd., Changchun, China; State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China",
        "aff_domain": "iflytek.com;iflytek.com;iflytek.com;ir.hit.edu.cn;iflytek.com;iflytek.com",
        "email": "iflytek.com;iflytek.com;iflytek.com;ir.hit.edu.cn;iflytek.com;iflytek.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1+2;0;1+3;1",
        "aff_unique_norm": "Harbin Institute of Technology;iFLYTEK Research;iFLYTEK AI Research;Jilin Kexun Information Technology Co., Ltd.",
        "aff_unique_dep": "Research Center for SCIR;State Key Laboratory of Cognitive Intelligence;AI Research;",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.iflytek.com;https://www.iflytek.com;",
        "aff_unique_abbr": "HIT;iFLYTEK;iFLYTEK;",
        "aff_campus_unique_index": "0;2;0;3",
        "aff_campus_unique": "Harbin;;Langfang;Changchun",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.197",
        "title": "CETA: A Consensus Enhanced Training Approach for Denoising in Distantly Supervised Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Distantly supervised relation extraction aims to extract relational facts from texts but suffers from noisy instances. Existing methods usually select reliable sentences that rely on potential noisy labels, resulting in wrongly selecting many noisy training instances or underutilizing a large amount of valuable training data. This paper proposes a sentence-level DSRE method beyond typical instance selection approaches by preventing samples from falling into the wrong classification space on the feature space. Specifically, a theorem for denoising and the corresponding implementation, named Consensus Enhanced Training Approach (CETA), are proposed in this paper. By training the model with CETA, samples of different classes are separated, and samples of the same class are closely clustered in the feature space. Thus the model can easily establish the robust classification boundary to prevent noisy labels from biasing wrongly labeled samples into the wrong classification space. This process is achieved by enhancing the classification consensus between two discrepant classifiers and does not depend on any potential noisy labels, thus avoiding the above two limitations. Extensive experiments on widely-used benchmarks have demonstrated that CETA significantly outperforms the previous methods and achieves new state-of-the-art results.",
        "author": "Ruri Liu; Shasha Mo; Jianwei Niu; Shengda Fan",
        "authorids": "/r/ruri-liu/; /s/shasha-mo/; /j/jianwei-niu/; /s/shengda-fan/",
        "bibtex": "@inproceedings{liu-etal-2022-ceta,\n    title = \"{CETA}: A Consensus Enhanced Training Approach for Denoising in Distantly Supervised Relation Extraction\",\n    author = \"Liu, Ruri  and\n      Mo, Shasha  and\n      Niu, Jianwei  and\n      Fan, Shengda\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.197/\",\n    pages = \"2247--2258\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.197.pdf",
        "site": "https://aclanthology.org/2022.coling-1.197/",
        "pdf_size": 2038278,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13945131247777182778&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Cyber Science and Technology, Beihang University; School of Cyber Science and Technology, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University; School of Cyber Science and Technology, Beihang University",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "School of Cyber Science and Technology",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "Beihang",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.37",
        "title": "CGIM: A Cycle Guided Interactive Learning Model for Consistency Identification in Task-oriented Dialogue",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Consistency identification in task-oriented dialog (CI-ToD) usually consists of three subtasks, aiming to identify inconsistency between current system response and current user response, dialog history and the corresponding knowledge base. This work aims to solve CI-ToD task by introducing an explicit interaction paradigm, Cycle Guided Interactive learning Model (CGIM), which achieves to make information exchange explicitly from all the three tasks. Specifically, CGIM relies on two core insights, referred to as guided multi-head attention module and cycle interactive mechanism, that collaborate from each other. On the one hand, each two tasks are linked with the guided multi-head attention module, aiming to explicitly model the interaction across two related tasks. On the other hand, we further introduce cycle interactive mechanism that focuses on facilitating model to exchange information among the three correlated sub-tasks via a cycle interaction manner. Experimental results on CI-ToD benchmark show that our model achieves the state-of-the-art performance, pushing the overall score to 56.3% (5.0% point absolute improvement). In addition, we find that CGIM is robust to the initial task flow order.",
        "author": "Libo Qin; Qiguang Chen; Tianbao Xie; Qian Liu; Shijue Huang; Wanxiang Che; Zhou Yu",
        "authorids": "/l/libo-qin/; /q/qiguang-chen/; /t/tianbao-xie/; /q/qian-liu/; /s/shijue-huang/; /w/wanxiang-che/; /z/zhou-yu/",
        "bibtex": "@inproceedings{qin-etal-2022-cgim,\n    title = \"{CGIM}: A Cycle Guided Interactive Learning Model for Consistency Identification in Task-oriented Dialogue\",\n    author = \"Qin, Libo  and\n      Chen, Qiguang  and\n      Xie, Tianbao  and\n      Liu, Qian  and\n      Huang, Shijue  and\n      Che, Wanxiang  and\n      Yu, Zhou\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.37/\",\n    pages = \"461--470\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.37.pdf",
        "site": "https://aclanthology.org/2022.coling-1.37/",
        "pdf_size": 679338,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14595487001600851227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Beihang University, Beijing, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Columbia University",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;buaa.edu.cn; ; ;columbia.edu",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;buaa.edu.cn; ; ;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0;0;2",
        "aff_unique_norm": "Harbin Institute of Technology;Beihang University;Columbia University",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval;;",
        "aff_unique_url": "http://www.hit.edu.cn/;http://www.buaa.edu.cn/;https://www.columbia.edu",
        "aff_unique_abbr": "HIT;BUAA;Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.559",
        "title": "CHAE: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Story generation has emerged as an interesting yet challenging NLP task in recent years. Some existing studies aim at generating fluent and coherent stories from keywords and outlines; while others attempt to control the global features of the story, such as emotion, style and topic. However, these works focus on coarse-grained control on the story, neglecting control on the details of the story, which is also crucial for the task. To fill the gap, this paper proposes a model for fine-grained control on the story, which allows the generation of customized stories with characters, corresponding actions and emotions arbitrarily assigned. Extensive experimental results on both automatic and human manual evaluations show the superiority of our method. It has strong controllability to generate stories according to the fine-grained personalized guidance, unveiling the effectiveness of our methodology. Our code is available at https://github.com/victorup/CHAE.",
        "author": "Xinpeng Wang; Han Jiang; Zhihua Wei; Shanlin Zhou",
        "authorids": "/x/xinpeng-wang/; /h/han-jiang/; /z/zhihua-wei/; /s/shanlin-zhou/",
        "bibtex": "@inproceedings{wang-etal-2022-chae,\n    title = \"{CHAE}: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions\",\n    author = \"Wang, Xinpeng  and\n      Jiang, Han  and\n      Wei, Zhihua  and\n      Zhou, Shanlin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.559/\",\n    pages = \"6426--6435\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.559.pdf",
        "site": "https://aclanthology.org/2022.coling-1.559/",
        "pdf_size": 730018,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9318150096614847951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China; School of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China",
        "aff_domain": "tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;mail.shiep.edu.cn",
        "email": "tongji.edu.cn;tongji.edu.cn;tongji.edu.cn;mail.shiep.edu.cn",
        "github": "https://github.com/victorup/CHAE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Tongji University;Shanghai University of Electric Power",
        "aff_unique_dep": "Department of Computer Science and Technology;School of Computer Science and Technology",
        "aff_unique_url": "https://www.tongji.edu.cn;http://www.suep.cn",
        "aff_unique_abbr": "Tongji;SUEP",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.417",
        "title": "CILDA: Contrastive Data Augmentation Using Intermediate Layer Knowledge Distillation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge distillation (KD) is an efficient framework for compressing large-scale pre-trained language models. Recent years have seen a surge of research aiming to improve KD by leveraging Contrastive Learning, Intermediate Layer Distillation, Data Augmentation, and Adversarial Training. In this work, we propose a learning-based data augmentation technique tailored for knowledge distillation, called CILDA. To the best of our knowledge, this is the first time that intermediate layer representations of the main task are used in improving the quality of augmented samples. More precisely, we introduce an augmentation technique for KD based on intermediate layer matching using contrastive loss to improve masked adversarial data augmentation. CILDA outperforms existing state-of-the-art KD approaches on the GLUE benchmark, as well as in an out-of-domain evaluation.",
        "author": "Md Akmal Haidar; Mehdi Rezagholizadeh; Abbas Ghaddar; Khalil Bibi; Phillippe Langlais; Pascal Poupart",
        "authorids": "/m/md-akmal-haidar/; /m/mehdi-rezagholizadeh/; /a/abbas-ghaddar/; /k/khalil-bibi/; /p/philippe-langlais/; /p/pascal-poupart/",
        "bibtex": "@inproceedings{haidar-etal-2022-cilda,\n    title = \"{CILDA}: Contrastive Data Augmentation Using Intermediate Layer Knowledge Distillation\",\n    author = \"Haidar, Md Akmal  and\n      Rezagholizadeh, Mehdi  and\n      Ghaddar, Abbas  and\n      Bibi, Khalil  and\n      Langlais, Phillippe  and\n      Poupart, Pascal\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.417/\",\n    pages = \"4707--4713\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.417.pdf",
        "site": "https://aclanthology.org/2022.coling-1.417/",
        "pdf_size": 780528,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11090851035493977616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; RALI/DIRO, Universit\u00e9 de Montr\u00e9al, Canada; David R. Cheriton School of Computer Science, University of Waterloo",
        "aff_domain": "huawei.com;huawei.com; ; ;iro.umontreal.ca;uwaterloo.ca",
        "email": "huawei.com;huawei.com; ; ;iro.umontreal.ca;uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;2",
        "aff_unique_norm": "Huawei;Universit\u00e9 de Montr\u00e9al;University of Waterloo",
        "aff_unique_dep": "Noah\u2019s Ark Lab;RALI/DIRO;David R. Cheriton School of Computer Science",
        "aff_unique_url": "https://www.huawei.com;https://www.umontreal.ca;https://uwaterloo.ca",
        "aff_unique_abbr": "Huawei;UdeM;UWaterloo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Montr\u00e9al",
        "aff_country_unique_index": "0;0;0;0;1;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2022.coling-1.362",
        "title": "CILex: An Investigation of Context Information for Lexical Substitution Methods",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Lexical substitution, which aims to generate substitutes for a target word given a context, is an important natural language processing task useful in many applications. Due to the paucity of annotated data, existing methods for lexical substitution tend to rely on manually curated lexical resources and contextual word embedding models. Methods based on lexical resources are likely to miss relevant substitutes whereas relying only on contextual word embedding models fails to provide adequate information on the impact of a substitute in the entire context and the overall meaning of the input. We proposed CILex, which uses contextual sentence embeddings along with methods that capture additional context information complimenting contextual word embeddings for lexical substitution. This ensured the semantic consistency of a substitute with the target word while maintaining the overall meaning of the sentence. Our experimental comparisons with previously proposed methods indicated that our solution is now the state-of-the-art on both the widely used LS07 and CoInCo datasets with P@1 scores of 55.96% and 57.25% for lexical substitution. The implementation of the proposed approach is available at https://github.com/sandaruSen/CILex under the MIT license.",
        "author": "Sandaru Seneviratne; Elena Daskalaki; Artem Lenskiy; Hanna Suominen",
        "authorids": "/s/sandaru-seneviratne/; /e/elena-daskalaki/; /a/artem-lenskiy/; /h/hanna-suominen/",
        "bibtex": "@inproceedings{seneviratne-etal-2022-cilex,\n    title = \"{CIL}ex: An Investigation of Context Information for Lexical Substitution Methods\",\n    author = \"Seneviratne, Sandaru  and\n      Daskalaki, Elena  and\n      Lenskiy, Artem  and\n      Suominen, Hanna\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.362/\",\n    pages = \"4124--4135\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.362.pdf",
        "site": "https://aclanthology.org/2022.coling-1.362/",
        "pdf_size": 368916,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14567300825534143456&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "The Australian National University (ANU) / Canberra, ACT, Australia; The Australian National University (ANU) / Canberra, ACT, Australia; The Australian National University (ANU) / Canberra, ACT, Australia; The Australian National University (ANU) / Canberra, ACT, Australia + University of Turku / Turku, Finland",
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "github": "https://github.com/sandaruSen/CILex",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Australian National University;University of Turku",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.utu.fi",
        "aff_unique_abbr": "ANU;UTU",
        "aff_campus_unique_index": "0;0;0;0+1",
        "aff_campus_unique": "Canberra;Turku",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Australia;Finland"
    },
    {
        "id": "2022.coling-1.346",
        "title": "CINO: A Chinese Minority Pre-trained Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multilingual pre-trained language models have shown impressive performance on cross-lingual tasks. It greatly facilitates the applications of natural language processing on low-resource languages. However, there are still some languages that the current multilingual models do not perform well on. In this paper, we propose CINO (Chinese Minority Pre-trained Language Model), a multilingual pre-trained language model for Chinese minority languages. It covers Standard Chinese, Yue Chinese, and six other ethnic minority languages. To evaluate the cross-lingual ability of the multilingual model on ethnic minority languages, we collect documents from Wikipedia and news websites, and construct two text classification datasets, WCM (Wiki-Chinese-Minority) and CMNews (Chinese-Minority-News). We show that CINO notably outperforms the baselines on various classification tasks. The CINO model and the datasets are publicly available at http://cino.hfl-rc.com.",
        "author": "Ziqing Yang; Zihang Xu; Yiming Cui; Baoxin Wang; Min Lin; Dayong Wu; Zhigang Chen",
        "authorids": "/z/ziqing-yang/; /z/zihang-xu/; /y/yiming-cui/; /b/baoxin-wang/; /m/min-lin/; /d/dayong-wu/; /z/zhigang-chen/",
        "bibtex": "@inproceedings{yang-etal-2022-cino,\n    title = \"{CINO}: A {C}hinese Minority Pre-trained Language Model\",\n    author = \"Yang, Ziqing  and\n      Xu, Zihang  and\n      Cui, Yiming  and\n      Wang, Baoxin  and\n      Lin, Min  and\n      Wu, Dayong  and\n      Chen, Zhigang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.346/\",\n    pages = \"3937--3949\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.346.pdf",
        "site": "https://aclanthology.org/2022.coling-1.346/",
        "pdf_size": 412746,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9074875039774198891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "http://cino.hfl-rc.com",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.221",
        "title": "CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Transforming the large amounts of unstructured text on the Internet into structured event knowledge is a critical, yet unsolved goal of NLP, especially when addressing document-level text. Existing methods struggle in Document-level Event Extraction (DEE) due to its two intrinsic challenges: (a) Nested arguments, which means one argument is the sub-string of another one. (b) Multiple events, which indicates we should identify multiple events and assemble the arguments for them. In this paper, we propose a role-interactive multi-event head attention network (CLIO) to solve these two challenges jointly. The key idea is to map different events to multiple subspaces (i.e. multi-event head). In each event subspace, we draw the semantic representation of each role closer to its corresponding arguments, then we determine whether the current event exists. To further optimize event representation, we propose an event representation enhancing strategy to regularize pre-trained embedding space to be more isotropic. Our experiments on two widely used DEE datasets show that CLIO achieves consistent improvements over previous methods.",
        "author": "Yubing Ren; Yanan Cao; Fang Fang; Ping Guo; Zheng Lin; Wei Ma; Yi Liu",
        "authorids": "/y/yubing-ren/; /y/yanan-cao/; /f/fang-fang/; /p/ping-guo/; /z/zheng-lin/; /w/wei-ma/; /y/yi-liu/",
        "bibtex": "@inproceedings{ren-etal-2022-clio,\n    title = \"{CLIO}: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction\",\n    author = \"Ren, Yubing  and\n      Cao, Yanan  and\n      Fang, Fang  and\n      Guo, Ping  and\n      Lin, Zheng  and\n      Ma, Wei  and\n      Liu, Yi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.221/\",\n    pages = \"2504--2514\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.221.pdf",
        "site": "https://aclanthology.org/2022.coling-1.221/",
        "pdf_size": 983495,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11698607297031274016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.274",
        "title": "CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.",
        "author": "Borun Chen; Hongyin Tang; Jiahao Bu; Kai Zhang; Jingang Wang; Qifan Wang; Hai-Tao Zheng; Wei Wu; Liqian Yu",
        "authorids": "/b/borun-chen/; /h/hongyin-tang/; /j/jiahao-bu/; /k/kai-zhang/; /j/jingang-wang/; /q/qifan-wang/; /h/hai-tao-zheng/; /w/wei-wu/; /l/liqian-yu/",
        "bibtex": "@inproceedings{chen-etal-2022-clower,\n    title = \"{CLOWER}: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations\",\n    author = \"Chen, Borun  and\n      Tang, Hongyin  and\n      Bu, Jiahao  and\n      Zhang, Kai  and\n      Wang, Jingang  and\n      Wang, Qifan  and\n      Zheng, Hai-Tao  and\n      Wu, Wei  and\n      Yu, Liqian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.274/\",\n    pages = \"3098--3108\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.274.pdf",
        "site": "https://aclanthology.org/2022.coling-1.274/",
        "pdf_size": 597419,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3318598479584103093&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tsinghua Shenzhen International Graduate School, Tsinghua University; Meituan; Meituan; Meituan; Meituan; Meta AI; Tsinghua Shenzhen International Graduate School, Tsinghua University + Peng Cheng Laboratory; Meituan; Meituan",
        "aff_domain": "mails.tsinghua.edu.cn;meituan.com;gmail.com;foxmail.com;meituan.com;fb.com;sz.tsinghua.edu.cn;meituan.com;meituan.com",
        "email": "mails.tsinghua.edu.cn;meituan.com;gmail.com;foxmail.com;meituan.com;fb.com;sz.tsinghua.edu.cn;meituan.com;meituan.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;2;0+3;1;1",
        "aff_unique_norm": "Tsinghua University;Meituan;Meta Platforms, Inc.;Peng Cheng Laboratory",
        "aff_unique_dep": "International Graduate School;;Meta AI;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.meituan.com;https://meta.com;http://www.pcl.ac.cn",
        "aff_unique_abbr": "THU;Meituan;Meta;PCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0;0;0;0;1;0+0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.245",
        "title": "CLoSE: Contrastive Learning of Subframe Embeddings for Political Bias Classification of News Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Framing is a political strategy in which journalists and politicians emphasize certain aspects of a societal issue in order to influence and sway public opinion. Frameworks for detecting framing in news articles or social media posts are critical in understanding the spread of biased information in our society. In this paper, we propose CLoSE, a multi-task BERT-based model which uses contrastive learning to embed indicators of frames from news articles in order to predict political bias. We evaluate the performance of our proposed model on subframes and political bias classification tasks. We also demonstrate the model\u2019s classification accuracy on zero-shot and few-shot learning tasks, providing a promising avenue for framing detection in unlabeled data.",
        "author": "Michelle YoungJin Kim; Kristen Marie Johnson",
        "authorids": "/m/michelle-youngjin-kim/; /k/kristen-johnson/",
        "bibtex": "@inproceedings{kim-johnson-2022-close,\n    title = \"{CL}o{SE}: Contrastive Learning of Subframe Embeddings for Political Bias Classification of News Media\",\n    author = \"Kim, Michelle YoungJin  and\n      Johnson, Kristen Marie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.245/\",\n    pages = \"2780--2793\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.245.pdf",
        "site": "https://aclanthology.org/2022.coling-1.245/",
        "pdf_size": 1859740,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16541773467057246223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Computer Science and Engineering, Michigan State University, USA; Computer Science and Engineering, Michigan State University, USA",
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.563",
        "title": "CM-Gen: A Neural Framework for Chinese Metaphor Generation with Explicit Context Modelling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nominal metaphors are frequently used in human language and have been shown to be effective in persuading, expressing emotion, and stimulating interest. This paper tackles the problem of Chinese Nominal Metaphor (NM) generation. We introduce a novel multitask framework, which jointly optimizes three tasks: NM identification, NM component identification, and NM generation. The metaphor identification module is able to perform a self-training procedure, which discovers novel metaphors from a large-scale unlabeled corpus for NM generation. The NM component identification module emphasizes components during training and conditions the generation on these NM components for more coherent results. To train the NM identification and component identification modules, we construct an annotated corpus consisting of 6.3k sentences that contain diverse metaphorical patterns. Automatic metrics show that our method can produce diverse metaphors with good readability, where 92% of them are novel metaphorical comparisons. Human evaluation shows our model significantly outperforms baselines on consistency and creativity.",
        "author": "Yucheng Li; Chenghua Lin; Frank Guerin",
        "authorids": "/y/yucheng-li/; /c/chenghua-lin/; /f/frank-guerin/",
        "bibtex": "@inproceedings{li-etal-2022-cm,\n    title = \"{CM}-Gen: A Neural Framework for {C}hinese Metaphor Generation with Explicit Context Modelling\",\n    author = \"Li, Yucheng  and\n      Lin, Chenghua  and\n      Guerin, Frank\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.563/\",\n    pages = \"6468--6479\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.563.pdf",
        "site": "https://aclanthology.org/2022.coling-1.563/",
        "pdf_size": 996811,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14297013446443330223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Surrey, UK; Department of Computer Science, University of Sheffield, UK; Department of Computer Science, University of Surrey, UK",
        "aff_domain": "surrey.ac.uk;sheffild.ac.uk;surrey.ac.uk",
        "email": "surrey.ac.uk;sheffild.ac.uk;surrey.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Surrey;University of Sheffield",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.surrey.ac.uk;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Surrey;Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.146",
        "title": "CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Forcing the answer of the Question Answering (QA) task to be a single text span might be restrictive since the answer can be multiple spans in the context. Moreover, we found that multi-span answers often appear with two characteristics when building the QA system for a real-world application. First, multi-span answers might be caused by users lacking domain knowledge and asking ambiguous questions, which makes the question need to be answered with conditions. Second, there might be hierarchical relations among multiple answer spans. Some recent span-extraction QA datasets include multi-span samples, but they only contain unconditional and parallel answers, which cannot be used to tackle this problem. To bridge the gap, we propose a new task: conditional question answering with hierarchical multi-span answers, where both the hierarchical relations and the conditions need to be extracted. Correspondingly, we introduce CMQA, a Conditional Multiple-span Chinese Question Answering dataset to study the new proposed task. The final release of CMQA consists of 7,861 QA pairs and 113,089 labels, where all samples contain multi-span answers, 50.4% of samples are conditional, and 56.6% of samples are hierarchical. CMQA can serve as a benchmark to study the new proposed task and help study building QA systems for real-world applications. The low performance of models drawn from related literature shows that the new proposed task is challenging for the community to solve.",
        "author": "Yiming Ju; Weikang Wang; Yuanzhe Zhang; Suncong Zheng; Kang Liu; Jun Zhao",
        "authorids": "/y/yiming-ju/; /w/weikang-wang/; /y/yuanzhe-zhang/; /s/suncong-zheng/; /k/kang-liu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{ju-etal-2022-cmqa,\n    title = \"{CMQA}: A Dataset of Conditional Question Answering with Multiple-Span Answers\",\n    author = \"Ju, Yiming  and\n      Wang, Weikang  and\n      Zhang, Yuanzhe  and\n      Zheng, Suncong  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.146/\",\n    pages = \"1697--1707\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.146.pdf",
        "site": "https://aclanthology.org/2022.coling-1.146/",
        "pdf_size": 2802393,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5333842265849298032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/juyiming/CMQA",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.350",
        "title": "COIN \u2013 an Inexpensive and Strong Baseline for Predicting Out of Vocabulary Word Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Social media is the ultimate challenge for many natural language processing tools. The constant emergence of linguistic constructs challenge even the most sophisticated NLP tools. Predicting word embeddings for out of vocabulary words is one of those challenges. Word embedding models only include terms that occur a sufficient number of times in their training corpora. Word embedding vector models are unable to directly provide any useful information about a word not in their vocabularies. We propose a fast method for predicting vectors for out of vocabulary terms that makes use of the surrounding terms of the unknown term and the hidden context layer of the word2vec model. We propose this method as a strong baseline in the sense that 1) while it does not surpass all state-of-the-art methods, it surpasses several techniques for vector prediction on benchmark tasks, 2) even when it underperforms, the margin is very small retaining competitive performance in downstream tasks, and 3) it is inexpensive to compute, requiring no additional training stage. We also show that our technique can be incorporated into existing methods to achieve a new state-of-the-art on the word vector prediction problem.",
        "author": "Andrew Schneider; Lihong He; Zhijia Chen; Arjun Mukherjee; Eduard Dragut",
        "authorids": "/a/andrew-schneider/; /l/lihong-he/; /z/zhijia-chen/; /a/arjun-mukherjee/; /e/eduard-dragut/",
        "bibtex": "@inproceedings{schneider-etal-2022-coin,\n    title = \"{COIN} {--} an Inexpensive and Strong Baseline for Predicting Out of Vocabulary Word Embeddings\",\n    author = \"Schneider, Andrew  and\n      He, Lihong  and\n      Chen, Zhijia  and\n      Mukherjee, Arjun  and\n      Dragut, Eduard\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.350/\",\n    pages = \"3984--3993\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.350.pdf",
        "site": "https://aclanthology.org/2022.coling-1.350/",
        "pdf_size": 373387,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4YiZOy-tcysJ:scholar.google.com/&scioq=COIN+%E2%80%93+an+Inexpensive+and+Strong+Baseline+for+Predicting+Out+of+Vocabulary+Word+Embeddings&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Near Miss Management; IBM Research, Almaden; Temple University; University of Houston; Temple University",
        "aff_domain": "nearmissmgmt.com;ibm.com;temple.edu;cs.uh.edu;temple.edu",
        "email": "nearmissmgmt.com;ibm.com;temple.edu;cs.uh.edu;temple.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;2",
        "aff_unique_norm": "Near Miss Management;IBM Research;Temple University;University of Houston",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.ibm.com/research;https://www.temple.edu;https://www.uh.edu",
        "aff_unique_abbr": ";IBM;Temple;UH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Almaden",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2022.coling-1.608",
        "title": "COMMA-DEER: COmmon-sense Aware Multimodal Multitask Approach for Detection of Emotion and Emotional Reasoning in Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Mental health is a critical component of the United Nations\u2019 Sustainable Development Goals (SDGs), particularly Goal 3, which aims to provide \u201cgood health and well-being\u201d. The present mental health treatment gap is exacerbated by stigma, lack of human resources, and lack of research capability for implementation and policy reform. We present and discuss a novel task of detecting emotional reasoning (ER) and accompanying emotions in conversations. In particular, we create a first-of-its-kind multimodal mental health conversational corpus that is manually annotated at the utterance level with emotional reasoning and related emotion. We develop a multimodal multitask framework with a novel multimodal feature fusion technique and a contextuality learning module to handle the two tasks. Leveraging multimodal sources of information, commonsense reasoning, and through a multitask framework, our proposed model produces strong results. We achieve performance gains of 6% accuracy and 4.62% F1 on the emotion detection task and 3.56% accuracy and 3.31% F1 on the ER detection task, when compared to the existing state-of-the-art model.",
        "author": "Soumitra Ghosh; Gopendra Vikram Singh; Asif Ekbal; Pushpak Bhattacharyya",
        "authorids": "/s/soumitra-ghosh/; /g/gopendra-vikram-singh/; /a/asif-ekbal/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{ghosh-etal-2022-comma,\n    title = \"{COMMA}-{DEER}: {CO}mmon-sense Aware Multimodal Multitask Approach for Detection of Emotion and Emotional Reasoning in Conversations\",\n    author = \"Ghosh, Soumitra  and\n      Singh, Gopendra Vikram  and\n      Ekbal, Asif  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.608/\",\n    pages = \"6978--6990\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.608.pdf",
        "site": "https://aclanthology.org/2022.coling-1.608/",
        "pdf_size": 1619940,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14376674941981154596&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, IIT Patna, India; Department of Computer Science and Engineering, IIT Patna, India; Department of Computer Science and Engineering, IIT Patna, India; Department of Computer Science and Engineering, IIT Bombay, India",
        "aff_domain": "gmail.com;iitp.ac.in;iitp.ac.in;cse.iitb.ac.in",
        "email": "gmail.com;iitp.ac.in;iitp.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "https://woebothealth.com/",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Indian Institute of Technology Patna;IIT Bombay",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitpatna.ac.in;https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Patna;IITB",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Patna;Bombay",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.15",
        "title": "COMMA: Modeling Relationship among Motivations, Emotions and Actions in Language-based Human Activities",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Motivations, emotions, and actions are inter-related essential factors in human activities. While motivations and emotions have long been considered at the core of exploring how people take actions in human activities, there has been relatively little research supporting analyzing the relationship between human mental states and actions. We present the first study that investigates the viability of modeling motivations, emotions, and actions in language-based human activities, named COMMA (Cognitive Framework of Human Activities). Guided by COMMA, we define three natural language processing tasks (emotion understanding, motivation understanding and conditioned action generation), and build a challenging dataset Hail through automatically extracting samples from Story Commonsense. Experimental results on NLP applications prove the effectiveness of modeling the relationship. Furthermore, our models inspired by COMMA can better reveal the essential relationship among motivations, emotions and actions than existing methods.",
        "author": "Yuqiang Xie; Yue Hu; Wei Peng; Guanqun Bi; Luxi Xing",
        "authorids": "/y/yuqiang-xie/; /y/yue-hu/; /w/wei-peng/; /g/guanqun-bi/; /l/luxi-xing/",
        "bibtex": "@inproceedings{xie-etal-2022-comma,\n    title = \"{COMMA}: Modeling Relationship among Motivations, Emotions and Actions in Language-based Human Activities\",\n    author = \"Xie, Yuqiang  and\n      Hu, Yue  and\n      Peng, Wei  and\n      Bi, Guanqun  and\n      Xing, Luxi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.15/\",\n    pages = \"163--177\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.15.pdf",
        "site": "https://aclanthology.org/2022.coling-1.15/",
        "pdf_size": 2398755,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2191515364860478490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "https://github.com/IndexFziQ/COMMA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.86",
        "title": "CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Fact-checking has gained increasing attention due to the widespread of falsified information. Most fact-checking approaches focus on claims made in English only due to the data scarcity issue in other languages. The lack of fact-checking datasets in low-resource languages calls for an effective cross-lingual transfer technique for fact-checking. Additionally, trustworthy information in different languages can be complementary and helpful in verifying facts. To this end, we present the first fact-checking framework augmented with cross-lingual retrieval that aggregates evidence retrieved from multiple languages through a cross-lingual retriever. Given the absence of cross-lingual information retrieval datasets with claim-like queries, we train the retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a self-supervised algorithm that creates training instances by translating the title of a passage. The goal for X-ICT is to learn cross-lingual retrieval in which the model learns to identify the passage corresponding to a given translated title. On the X-Fact dataset, our approach achieves 2.23% absolute F1 improvement in the zero-shot cross-lingual setup over prior systems. The source code and data are publicly available at https://github.com/khuangaf/CONCRETE.",
        "author": "Kung-Hsiang Huang; ChengXiang Zhai; Heng Ji",
        "authorids": "/k/kung-hsiang-huang/; /c/chengxiang-zhai/; /h/heng-ji/",
        "bibtex": "@inproceedings{huang-etal-2022-concrete,\n    title = \"{CONCRETE}: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval\",\n    author = \"Huang, Kung-Hsiang  and\n      Zhai, ChengXiang  and\n      Ji, Heng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.86/\",\n    pages = \"1024--1035\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.86.pdf",
        "site": "https://aclanthology.org/2022.coling-1.86/",
        "pdf_size": 1185653,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6775255463269257951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Science, University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/khuangaf/CONCRETE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.222",
        "title": "COPNER: Contrastive Learning with Prompt Guiding for Few-shot Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Distance metric learning has become a popular solution for few-shot Named Entity Recognition (NER). The typical setup aims to learn a similarity metric for measuring the semantic similarity between test samples and referents, where each referent represents an entity class. The effect of this setup may, however, be compromised for two reasons. First, there is typically a limited optimization exerted on the representations of entity tokens after initing by pre-trained language models. Second, the referents may be far from representing corresponding entity classes due to the label scarcity in the few-shot setting. To address these challenges, we propose a novel approach named COntrastive learning with Prompt guiding for few-shot NER (COPNER). We introduce a novel prompt composed of class-specific words to COPNER to serve as 1) supervision signals for conducting contrastive learning to optimize token representations; 2) metric referents for distance-metric inference on test samples. Experimental results demonstrate that COPNER outperforms state-of-the-art models with a significant margin in most cases. Moreover, COPNER shows great potential in the zero-shot setting.",
        "author": "Yucheng Huang; Kai He; Yige Wang; Xianli Zhang; Tieliang Gong; Rui Mao; Chen Li",
        "authorids": "/y/yucheng-huang/; /k/kai-he/; /y/yige-wang/; /x/xianli-zhang/; /t/tieliang-gong/; /r/rui-mao/; /c/chen-li/",
        "bibtex": "@inproceedings{huang-etal-2022-copner,\n    title = \"{COPNER}: Contrastive Learning with Prompt Guiding for Few-shot Named Entity Recognition\",\n    author = \"Huang, Yucheng  and\n      He, Kai  and\n      Wang, Yige  and\n      Zhang, Xianli  and\n      Gong, Tieliang  and\n      Mao, Rui  and\n      Li, Chen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.222/\",\n    pages = \"2515--2527\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.222.pdf",
        "site": "https://aclanthology.org/2022.coling-1.222/",
        "pdf_size": 774052,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1941291152146205624&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Xi\u2019an Jiaotong University; School of Computer Science and Technology, Xi\u2019an Jiaotong University; School of Computer Science and Technology, Xi\u2019an Jiaotong University; School of Computer Science and Technology, Xi\u2019an Jiaotong University; School of Computer Science and Technology, Xi\u2019an Jiaotong University; School of Computer Science and Engineering, Nanyang Technological University; School of Computer Science and Technology, Xi\u2019an Jiaotong University",
        "aff_domain": "stu.xjtu.edu.cn;stu.xjtu.edu.cn;stu.xjtu.edu.cn;stu.xjtu.edu.cn;xjtu.edu.cn;ntu.edu.sg;xjtu.edu.cn",
        "email": "stu.xjtu.edu.cn;stu.xjtu.edu.cn;stu.xjtu.edu.cn;stu.xjtu.edu.cn;xjtu.edu.cn;ntu.edu.sg;xjtu.edu.cn",
        "github": "https://github.com/AndrewHYC/COPNER",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Xi'an Jiaotong University;Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science and Engineering",
        "aff_unique_url": "https://www.xjtu.edu.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "XJTU;NTU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.coling-1.144",
        "title": "CORN: Co-Reasoning Network for Commonsense Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Commonsense question answering (QA) requires machines to utilize the QA content and external commonsense knowledge graph (KG) for reasoning when answering questions. Existing work uses two independent modules to model the QA contextual text representation and relationships between QA entities in KG, which prevents information sharing between modules for co-reasoning. In this paper, we propose a novel model, Co-Reasoning Network (CORN), which adopts a bidirectional multi-level connection structure based on Co-Attention Transformer. The structure builds bridges to connect each layer of the text encoder and graph encoder, which can introduce the QA entity relationship from KG to the text encoder and bring contextual text information to the graph encoder, so that these features can be deeply interactively fused to form comprehensive text and graph node representations. Meanwhile, we propose a QA-aware node based KG subgraph construction method. The QA-aware nodes aggregate the question entity nodes and the answer entity nodes, and further guide the expansion and construction process of the subgraph to enhance the connectivity and reduce the introduction of noise. We evaluate our model on QA benchmarks in the CommonsenseQA and OpenBookQA datasets, and CORN achieves state-of-the-art performance.",
        "author": "Xin Guan; Biwei Cao; Qingqing Gao; Zheng Yin; Bo Liu; Jiuxin Cao",
        "authorids": "/x/xin-guan/; /b/biwei-cao/; /q/qingqing-gao/; /z/zheng-yin/; /b/bo-liu/; /j/jiuxin-cao/",
        "bibtex": "@inproceedings{guan-etal-2022-corn,\n    title = \"{CORN}: Co-Reasoning Network for Commonsense Question Answering\",\n    author = \"Guan, Xin  and\n      Cao, Biwei  and\n      Gao, Qingqing  and\n      Yin, Zheng  and\n      Liu, Bo  and\n      Cao, Jiuxin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.144/\",\n    pages = \"1677--1686\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.144.pdf",
        "site": "https://aclanthology.org/2022.coling-1.144/",
        "pdf_size": 650099,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=145826641858548439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Southeast University; Southeast University; Southeast University; Southeast University; Southeast University; Southeast University",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.32",
        "title": "CR-GIS: Improving Conversational Recommendation via Goal-aware Interest Sequence Modeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational recommendation systems (CRS) aim to determine a goal item by sequentially tracking users\u2019 interests through multi-turn conversation. In CRS, implicit patterns of user interest sequence guide the smooth transition of dialog utterances to the goal item. However, with the convenient explicit knowledge of knowledge graph (KG), existing KG-based CRS methods over-rely on the explicit separate KG links to model the user interests but ignore the rich goal-aware implicit interest sequence patterns in a dialog. In addition, interest sequence is also not fully used to generate smooth transited utterances. We propose CR-GIS with a parallel star framework. First, an interest-level star graph is designed to model the goal-aware implicit user interest sequence. Second, a hierarchical Star Transformer is designed to guide the multi-turn utterances generation with the interest-level star graph. Extensive experiments verify the effectiveness of CR-GIS in achieving more accurate recommended items with more fluent and coherent dialog utterances.",
        "author": "Jinfeng Zhou; Bo Wang; Zhitong Yang; Dongming Zhao; Kun Huang; Ruifang He; Yuexian Hou",
        "authorids": "/j/jinfeng-zhou/; /b/bo-wang/; /z/zhitong-yang/; /d/dongming-zhao/; /k/kun-huang/; /r/ruifang-he/; /y/yuexian-hou/",
        "bibtex": "@inproceedings{zhou-etal-2022-cr,\n    title = \"{CR}-{GIS}: Improving Conversational Recommendation via Goal-aware Interest Sequence Modeling\",\n    author = \"Zhou, Jinfeng  and\n      Wang, Bo  and\n      Yang, Zhitong  and\n      Zhao, Dongming  and\n      Huang, Kun  and\n      He, Ruifang  and\n      Hou, Yuexian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.32/\",\n    pages = \"400--411\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.32.pdf",
        "site": "https://aclanthology.org/2022.coling-1.32/",
        "pdf_size": 901794,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12162110541105884377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China+State Key Laboratory of Communication Content Cognition, People\u2019s Daily Online, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China+State Key Laboratory of Communication Content Cognition, People\u2019s Daily Online, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China+State Key Laboratory of Communication Content Cognition, People\u2019s Daily Online, Beijing, China; AI Lab, China Mobile Communication Group Tianjin Co., Ltd.; AI Lab, China Mobile Communication Group Tianjin Co., Ltd.; College of Intelligence and Computing, Tianjin University, Tianjin, China+State Key Laboratory of Communication Content Cognition, People\u2019s Daily Online, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn; ; ;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn; ; ;tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;2;2;0+1;0",
        "aff_unique_norm": "Tianjin University;People\u2019s Daily Online;China Mobile Communication Group Tianjin Co., Ltd.",
        "aff_unique_dep": "College of Intelligence and Computing;State Key Laboratory of Communication Content Cognition;AI Lab",
        "aff_unique_url": "http://www.tju.edu.cn;;http://www.chinamobileltd.com/",
        "aff_unique_abbr": "Tianjin University;;",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1;0",
        "aff_campus_unique": "Tianjin;Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.344",
        "title": "CSL: A Large-scale Chinese Scientific Literature Dataset",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Scientific literature serves as a high-quality corpus, supporting a lot of Natural Language Processing (NLP) research. However, existing datasets are centered around the English language, which restricts the development of Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers. To our knowledge, CSL is the first scientific document dataset in Chinese. The CSL can serve as a Chinese corpus. Also, this semi-structured data is a natural annotation that can constitute many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the performance of models across scientific domain tasks, i.e., summarization, keyword generation and text classification. We analyze the behavior of existing text-to-text models on the evaluation tasks and reveal the challenges for Chinese scientific NLP tasks, which provides a valuable reference for future research. Data and code will be publicly available.",
        "author": "Yudong Li; Yuqing Zhang; Zhe Zhao; Linlin Shen; Weijie Liu; Weiquan Mao; Hui Zhang",
        "authorids": "/y/yudong-li/; /y/yuqing-zhang/; /z/zhe-zhao/; /l/linlin-shen/; /w/weijie-liu/; /w/weiquan-mao/; /h/hui-zhang/",
        "bibtex": "@inproceedings{li-etal-2022-csl,\n    title = \"{CSL}: A Large-scale {C}hinese Scientific Literature Dataset\",\n    author = \"Li, Yudong  and\n      Zhang, Yuqing  and\n      Zhao, Zhe  and\n      Shen, Linlin  and\n      Liu, Weijie  and\n      Mao, Weiquan  and\n      Zhang, Hui\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.344/\",\n    pages = \"3917--3923\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.344.pdf",
        "site": "https://aclanthology.org/2022.coling-1.344/",
        "pdf_size": 473128,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15295582975425043755&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "China University of Geosciences (Beijing), School of Information Engineering+Shenzhen University, School of Computer Science and Software Engineering; China University of Geosciences (Beijing), School of Information Engineering; Tencent AI Lab; Shenzhen University, School of Computer Science and Software Engineering; Tencent AI Lab; Tencent AI Lab; Information Technology Center for National Science and Technology Infrastructure, Beijing, China",
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "github": "https://github.com/ydli-ai/CSL",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;2;1;2;2;3",
        "aff_unique_norm": "China University of Geosciences;Shenzhen University;Tencent;Information Technology Center for National Science and Technology Infrastructure",
        "aff_unique_dep": "School of Information Engineering;School of Computer Science and Software Engineering;Tencent AI Lab;",
        "aff_unique_url": "http://www.cugb.edu.cn;https://www.szu.edu.cn;https://ai.tencent.com;",
        "aff_unique_abbr": "CUGB;SZU;Tencent AI Lab;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.247",
        "title": "CXR Data Annotation and Classification with Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Clinical data annotation has been one of the major obstacles for applying machine learning approaches in clinical NLP. Open-source tools such as NegBio and CheXpert are usually designed on data from specific institutions, which limit their applications to other institutions due to the differences in writing style, structure, language use as well as label definition. In this paper, we propose a new weak supervision annotation framework with two improvements compared to existing annotation frameworks: 1) we propose to select representative samples for efficient manual annotation; 2) we propose to auto-annotate the remaining samples, both leveraging on a self-trained sentence encoder. This framework also provides a function for identifying inconsistent annotation errors. The utility of our proposed weak supervision annotation framework is applicable to any given data annotation task, and it provides an efficient form of sample selection and data auto-annotation with better classification results for real applications.",
        "author": "Nina Zhou; Ai Ti Aw; Zhuo Han Liu; Cher heng Tan; Yonghan Ting; Wen Xiang Chen; Jordan sim zheng Ting",
        "authorids": "/n/nina-zhou/; /a/aiti-aw/; /z/zhuo-han-liu/; /c/cher-heng-tan/; /y/yonghan-ting/; /w/wen-xiang-chen/; /j/jordan-sim-zheng-ting/",
        "bibtex": "@inproceedings{zhou-etal-2022-cxr,\n    title = \"{CXR} Data Annotation and Classification with Pre-trained Language Models\",\n    author = \"Zhou, Nina  and\n      Aw, Ai Ti  and\n      Liu, Zhuo Han  and\n      Tan, Cher heng  and\n      Ting, Yonghan  and\n      Chen, Wen Xiang  and\n      Ting, Jordan sim zheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.247/\",\n    pages = \"2801--2811\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.247.pdf",
        "site": "https://aclanthology.org/2022.coling-1.247/",
        "pdf_size": 1062786,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14044210346292727863&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Institute for Infocomm Research(I2R), Singapore; Institute for Infocomm Research(I2R), Singapore; Institute for Infocomm Research(I2R), Singapore; Tan Toch Seng Hospital (TTSH), Singapore; Tan Toch Seng Hospital (TTSH), Singapore; Tan Toch Seng Hospital (TTSH), Singapore; MOH Holdings (MOHH), Singapore",
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg;ttsh.com.sg;ttsh.com.sg;ttsh.com.sg;mohh.com.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg;ttsh.com.sg;ttsh.com.sg;ttsh.com.sg;mohh.com.sg",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;1;1;2",
        "aff_unique_norm": "Institute for Infocomm Research;Tan Toch Seng Hospital;MOH Holdings",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.ttsh.com.sg;https://www.mohh.com.sg",
        "aff_unique_abbr": "I2R;TTSH;MOHH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.coling-1.437",
        "title": "Can Data Diversity Enhance Learning Generalization?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper introduces our Diversity Advanced Actor-Critic reinforcement learning (A2C) framework (DAAC) to improve the generalization and accuracy of Natural Language Processing (NLP). We show that the diversification of training samples alleviates overfitting and improves model generalization and accuracy. We quantify diversity on a set of samples using the max dispersion, convex hull volume, and graph entropy based on sentence embeddings in high-dimensional metric space. We also introduce A2C to select such a diversified training subset efficiently. Our experiments achieve up to +23.8 accuracy increase (38.0% relatively) in sentiment analysis, -44.7 perplexity decrease (37.9% relatively) in language modeling, and consistent improvements in named entity recognition over various domains. In particular, our method outperforms both domain adaptation and generalization baselines without using any target domain knowledge.",
        "author": "Yu Yu; Shahram Khadivi; Jia Xu",
        "authorids": "/y/yu-yu/; /s/shahram-khadivi/; /j/jia-xu/",
        "bibtex": "@inproceedings{yu-etal-2022-data,\n    title = \"Can Data Diversity Enhance Learning Generalization?\",\n    author = \"Yu, Yu  and\n      Khadivi, Shahram  and\n      Xu, Jia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.437/\",\n    pages = \"4933--4945\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.437.pdf",
        "site": "https://aclanthology.org/2022.coling-1.437/",
        "pdf_size": 3052082,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1815677117836579278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Engineering and Science, Steven Institute of Technology, NJ 07030, USA; eBay Inc., Aachen 52064, Germany; School of Engineering and Science, Steven Institute of Technology, NJ 07030, USA",
        "aff_domain": "stevens.edu;ebay.com;stevens.edu",
        "email": "stevens.edu;ebay.com;stevens.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Steven Institute of Technology;eBay Inc.",
        "aff_unique_dep": "School of Engineering and Science;",
        "aff_unique_url": "https://www.stevens.edu;https://www.ebayinc.com",
        "aff_unique_abbr": "SIT;eBay",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "NJ;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "2022.coling-1.139",
        "title": "Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "There have been many efforts to try to understand what grammatical knowledge (e.g., ability to understand the part of speech of a token) is encoded in large pre-trained language models (LM). This is done through \u2018Edge Probing\u2019 (EP) tests: supervised classification tasks to predict the grammatical properties of a span (whether it has a particular part of speech) using only the token representations coming from the LM encoder. However, most NLP applications fine-tune these LM encoders for specific tasks. Here, we ask: if an LM is fine-tuned, does the encoding of linguistic information in it change, as measured by EP tests? Specifically, we focus on the task of Question Answering (QA) and conduct experiments on multiple datasets. We find that EP test results do not change significantly when the fine-tuned model performs well or in adversarial situations where the model is forced to learn wrong correlations. From a similar finding, some recent papers conclude that fine-tuning does not change linguistic knowledge in encoders but they do not provide an explanation. We find that EP models are susceptible to exploiting spurious correlations in the EP datasets. When this dataset bias is corrected, we do see an improvement in the EP test results as expected.",
        "author": "Sagnik Ray Choudhury; Nikita Bhutani; Isabelle Augenstein",
        "authorids": "/s/sagnik-ray-choudhury/; /n/nikita-bhutani/; /i/isabelle-augenstein/",
        "bibtex": "@inproceedings{ray-choudhury-etal-2022-edge,\n    title = \"Can Edge Probing Tests Reveal Linguistic Knowledge in {QA} Models?\",\n    author = \"Ray Choudhury, Sagnik  and\n      Bhutani, Nikita  and\n      Augenstein, Isabelle\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.139/\",\n    pages = \"1620--1635\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.139.pdf",
        "site": "https://aclanthology.org/2022.coling-1.139/",
        "pdf_size": 420035,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17093298415967569218&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Michigan+University of Copenhagen; Megagon Labs; University of Copenhagen",
        "aff_domain": "gmail.com;megagon.ai;di.ku.dk",
        "email": "gmail.com;megagon.ai;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;1",
        "aff_unique_norm": "University of Michigan;University of Copenhagen;Megagon Labs",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umich.edu;https://www.ku.dk;https://www.megagonlabs.com",
        "aff_unique_abbr": "UM;UCPH;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "United States;Denmark"
    },
    {
        "id": "2022.coling-1.285",
        "title": "Can Transformers Process Recursive Nested Constructions, Like Humans?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recursive processing is considered a hallmark of human linguistic abilities. A recent study evaluated recursive processing in recurrent neural language models (RNN-LMs) and showed that such models perform below chance level on embedded dependencies within nested constructions \u2013 a prototypical example of recursion in natural language. Here, we study if state-of-the-art Transformer LMs do any better. We test eight different Transformer LMs on two different types of nested constructions, which differ in whether the embedded (inner) dependency is short or long range. We find that Transformers achieve near-perfect performance on short-range embedded dependencies, significantly better than previous results reported for RNN-LMs and humans. However, on long-range embedded dependencies, Transformers\u2019 performance sharply drops below chance level. Remarkably, the addition of only three words to the embedded dependency caused Transformers to fall from near-perfect to below-chance performance. Taken together, our results reveal how brittle syntactic processing is in Transformers, compared to humans.",
        "author": "Yair Lakretz; Th\u00e9o Desbordes; Dieuwke Hupkes; Stanislas Dehaene",
        "authorids": "/y/yair-lakretz/; /t/theo-desbordes/; /d/dieuwke-hupkes/; /s/stanislas-dehaene/",
        "bibtex": "@inproceedings{lakretz-etal-2022-transformers,\n    title = \"Can Transformers Process Recursive Nested Constructions, Like Humans?\",\n    author = \"Lakretz, Yair  and\n      Desbordes, Th{\\'e}o  and\n      Hupkes, Dieuwke  and\n      Dehaene, Stanislas\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.285/\",\n    pages = \"3226--3232\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.285.pdf",
        "site": "https://aclanthology.org/2022.coling-1.285/",
        "pdf_size": 277681,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5497670314697410149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.125",
        "title": "Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the success of state-of-the-art pre-trained language models (PLMs) on a series of multi-hop reasoning tasks, they still suffer from their limited abilities to transfer learning from simple to complex tasks and vice-versa. We argue that one step forward to overcome this limitation is to better understand the behavioral trend of PLMs at each hop over the inference chain. Our critical underlying idea is to mimic human-style reasoning: we envision the multi-hop reasoning process as a sequence of explicit single-hop reasoning steps. To endow PLMs with incremental reasoning skills, we propose a set of inference strategies on relevant facts and distractors allowing us to build automatically generated training datasets. Using the SHINRA and ConceptNet resources jointly, we empirically show the effectiveness of our proposal on multiple-choice question answering and reading comprehension, with a relative improvement in terms of accuracy of 68.4% and 16.0% w.r.t. classic PLMs, respectively.",
        "author": "Jesus Lovon-Melgarejo; Jose G. Moreno; Romaric Besan\u00e7on; Olivier Ferret; Lynda Tamine",
        "authorids": "/j/jesus-lovon-melgarejo/; /j/jose-g-moreno/; /r/romaric-besancon/; /o/olivier-ferret/; /l/lynda-tamine/",
        "bibtex": "@inproceedings{lovon-melgarejo-etal-2022-guide,\n    title = \"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?\",\n    author = \"Lovon-Melgarejo, Jesus  and\n      Moreno, Jose G.  and\n      Besan{\\c{c}}on, Romaric  and\n      Ferret, Olivier  and\n      Tamine, Lynda\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.125/\",\n    pages = \"1455--1466\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.125.pdf",
        "site": "https://aclanthology.org/2022.coling-1.125/",
        "pdf_size": 2187560,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12672918848176115063&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Universit\u00e9 Paul Sabatier, IRIT, Toulouse, France; Universit\u00e9 Paul Sabatier, IRIT, Toulouse, France; Universit\u00e9 Paris-Saclay, CEA, List, Palaiseau, France; Universit\u00e9 Paris-Saclay, CEA, List, Palaiseau, France; Universit\u00e9 Paul Sabatier, IRIT, Toulouse, France",
        "aff_domain": "irit.fr;irit.fr;cea.fr;cea.fr;irit.fr",
        "email": "irit.fr;irit.fr;cea.fr;cea.fr;irit.fr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Universit\u00e9 Paul Sabatier;Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "IRIT;CEA List",
        "aff_unique_url": "https://www.ups-tlse.fr;https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "UPS;UPS",
        "aff_campus_unique_index": "0;0;1;1;0",
        "aff_campus_unique": "Toulouse;Palaiseau",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.coling-1.134",
        "title": "Case-Based Abductive Natural Language Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most of the contemporary approaches for multi-hop Natural Language Inference (NLI) construct explanations considering each test case in isolation. However, this paradigm is known to suffer from semantic drift, a phenomenon that causes the construction of spurious explanations leading to wrong conclusions. In contrast, this paper proposes an abductive framework for multi-hop NLI exploring the retrieve-reuse-refine paradigm in Case-Based Reasoning (CBR). Specifically, we present Case-Based Abductive Natural Language Inference (CB-ANLI), a model that addresses unseen inference problems by analogical transfer of prior explanations from similar examples. We empirically evaluate the abductive framework on commonsense and scientific question answering tasks, demonstrating that CB-ANLI can be effectively integrated with sparse and dense pre-trained encoders to improve multi-hop inference, or adopted as an evidence retriever for Transformers. Moreover, an empirical analysis of semantic drift reveals that the CBR paradigm boosts the quality of the most challenging explanations, a feature that has a direct impact on robustness and accuracy in downstream inference tasks.",
        "author": "Marco Valentino; Mokanarangan Thayaparan; Andr\u00e9 Freitas",
        "authorids": "/m/marco-valentino/; /m/mokanarangan-thayaparan/; /a/andre-freitas/",
        "bibtex": "@inproceedings{valentino-etal-2022-case,\n    title = \"Case-Based Abductive Natural Language Inference\",\n    author = \"Valentino, Marco  and\n      Thayaparan, Mokanarangan  and\n      Freitas, Andr{\\'e}\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.134/\",\n    pages = \"1556--1568\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.134.pdf",
        "site": "https://aclanthology.org/2022.coling-1.134/",
        "pdf_size": 574028,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11340062937475798109&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Idiap Research Institute, Switzerland; University of Manchester, United Kingdom; University of Manchester, United Kingdom",
        "aff_domain": "idiap.ch; ; ",
        "email": "idiap.ch; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Idiap Research Institute;University of Manchester",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idiap.ch;https://www.manchester.ac.uk",
        "aff_unique_abbr": "Idiap;UoM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2022.coling-1.464",
        "title": "Categorizing Semantic Representations for Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks. However, they have recently been shown to suffer limitation in compositional generalization, failing to effectively learn the translation of atoms (e.g., words) and their semantic composition (e.g., modification) from seen compounds (e.g., phrases), and thus suffering from significantly weakened translation performance on unseen compounds during inference. We address this issue by introducing categorization to the source contextualized representations. The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding. Experiments on a dedicated MT dataset (i.e., CoGnition) show that our method reduces compositional generalization error rates by 24% error reduction. In addition, our conceptually simple method gives consistently better results than the Transformer baseline on a range of general MT datasets.",
        "author": "Yongjing Yin; Yafu Li; Fandong Meng; Jie Zhou; Yue Zhang",
        "authorids": "/y/yongjing-yin/; /y/yafu-li/; /f/fandong-meng/; /j/jie-zhou/; /y/yue-zhang/",
        "bibtex": "@inproceedings{yin-etal-2022-categorizing,\n    title = \"Categorizing Semantic Representations for Neural Machine Translation\",\n    author = \"Yin, Yongjing  and\n      Li, Yafu  and\n      Meng, Fandong  and\n      Zhou, Jie  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.464/\",\n    pages = \"5227--5239\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.464.pdf",
        "site": "https://aclanthology.org/2022.coling-1.464/",
        "pdf_size": 770908,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7151800353097854841&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Zhejiang University+School of Engineering, Westlake University; Zhejiang University+School of Engineering, Westlake University; Pattern Recognition Center, WeChat AI, Tencent Inc; Pattern Recognition Center, WeChat AI, Tencent Inc; School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;tencent.com;tencent.com;wias.org.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;tencent.com;tencent.com;wias.org.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;2;2;1+3",
        "aff_unique_norm": "Zhejiang University;Westlake University;Tencent Inc;Westlake Institute for Advanced Study",
        "aff_unique_dep": ";School of Engineering;Pattern Recognition Center, WeChat AI;Institute of Advanced Technology",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn;https://www.tencent.com;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;Tencent;WIAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.607",
        "title": "Causal Intervention Improves Implicit Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite having achieved great success for sentiment analysis, existing neural models struggle with implicit sentiment analysis. It is because they may latch onto spurious correlations (\u201cshortcuts\u201d, e.g., focusing only on explicit sentiment words), resulting in undermining the effectiveness and robustness of the learned model. In this work, we propose a CausaL intervention model for implicit sEntiment ANalysis using instrumental variable (CLEAN). We first review sentiment analysis from a causal perspective and analyze the confounders existing in this task. Then, we introduce instrumental variable to eliminate the confounding causal effects, thus extracting the pure causal effect between sentence and sentiment. We compare the proposed CLEAN with several strong baselines on both the general implicit sentiment analysis and aspect-based implicit sentiment analysis tasks. The results indicate the great advantages of our model and the efficacy of implicit sentiment reasoning.",
        "author": "Siyin Wang; Jie Zhou; Changzhi Sun; Junjie Ye; Tao Gui; Qi Zhang; Xuanjing Huang",
        "authorids": "/s/siyin-wang/; /j/jie-zhou/; /c/changzhi-sun/; /j/junjie-ye/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{wang-etal-2022-causal,\n    title = \"Causal Intervention Improves Implicit Sentiment Analysis\",\n    author = \"Wang, Siyin  and\n      Zhou, Jie  and\n      Sun, Changzhi  and\n      Ye, Junjie  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.607/\",\n    pages = \"6966--6977\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.607.pdf",
        "site": "https://aclanthology.org/2022.coling-1.607/",
        "pdf_size": 609409,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4684934222619432197&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; ByteDance AI Lab; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;bytedance.com;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;bytedance.com;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;0",
        "aff_unique_norm": "Fudan University;ByteDance",
        "aff_unique_dep": "School of Computer Science;AI Lab",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.bytedance.com",
        "aff_unique_abbr": "Fudan;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.291",
        "title": "CausalQA: A Benchmark for Causal Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "At least 5% of questions submitted to search engines ask about cause-effect relationships in some way. To support the development of tailored approaches that can answer such questions, we construct Webis-CausalQA-22, a benchmark corpus of 1.1 million causal questions with answers. We distinguish different types of causal questions using a novel typology derived from a data-driven, manual analysis of questions from ten large question answering (QA) datasets. Using high-precision lexical rules, we extract causal questions of each type from these datasets to create our corpus. As an initial baseline, the state-of-the-art QA model UnifiedQA achieves a ROUGE-L F1 score of 0.48 on our new benchmark.",
        "author": "Alexander Bondarenko; Magdalena Wolska; Stefan Heindorf; Lukas Bl\u00fcbaum; Axel-Cyrille Ngonga Ngomo; Benno Stein; Pavel Braslavski; Matthias Hagen; Martin Potthast",
        "authorids": "/a/alexander-bondarenko/; /m/magdalena-wolska/; /s/stefan-heindorf/; /l/lukas-blubaum/; /a/axel-cyrille-ngonga-ngomo/; /b/benno-stein/; /p/pavel-braslavski/; /m/matthias-hagen/; /m/martin-potthast/",
        "bibtex": "@inproceedings{bondarenko-etal-2022-causalqa,\n    title = \"{C}ausal{QA}: A Benchmark for Causal Question Answering\",\n    author = {Bondarenko, Alexander  and\n      Wolska, Magdalena  and\n      Heindorf, Stefan  and\n      Bl{\\\"u}baum, Lukas  and\n      Ngonga Ngomo, Axel-Cyrille  and\n      Stein, Benno  and\n      Braslavski, Pavel  and\n      Hagen, Matthias  and\n      Potthast, Martin},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.291/\",\n    pages = \"3296--3308\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.291.pdf",
        "site": "https://aclanthology.org/2022.coling-1.291/",
        "pdf_size": 792408,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12013918723959303563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Martin-Luther-Univerist\u00e4t Halle-Wittenberg; Bauhaus-Universit\u00e4t Weimar; Paderborn University; Paderborn University; Paderborn University; Bauhaus-Universit\u00e4t Weimar; Ural Federal University+HSE University; Martin-Luther-Univerist\u00e4t Halle-Wittenberg; Leipzig University",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/webis-de/COLING-22",
        "project": "https://causalqa.webis.de",
        "author_num": 9,
        "aff_unique_index": "0;1;2;2;2;1;3+4;0;5",
        "aff_unique_norm": "Martin Luther University Halle-Wittenberg;Bauhaus-Universit\u00e4t Weimar;Paderborn University;Ural Federal University;Higher School of Economics;Leipzig University",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.uni-halle.de;https://www.bauhaus-university.de;https://www.upb.de/;https://urfu.ru;https://hse.ru;https://www.uni-leipzig.de",
        "aff_unique_abbr": "MLU;Bauhaus-Uni Weimar;UPB;UFU;HSE;Uni Leipzig",
        "aff_campus_unique_index": "1;1;",
        "aff_campus_unique": ";Weimar",
        "aff_country_unique_index": "0;0;0;0;0;0;1+1;0;0",
        "aff_country_unique": "Germany;Russia"
    },
    {
        "id": "2022.coling-1.14",
        "title": "Character Jacobian: Modeling Chinese Character Meanings with Deep Learning Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Compounding, a prevalent word-formation process, presents an interesting challenge for computational models. Indeed, the relations between compounds and their constituents are often complicated. It is particularly so in Chinese morphology, where each character is almost simultaneously bound and free when treated as a morpheme. To model such word-formation process, we propose the Notch (NOnlinear Transformation of CHaracter embeddings) model and the character Jacobians. The Notch model first learns the non-linear relations between the constituents and words, and the character Jacobians further describes the character\u2019s role in each word. In a series of experiments, we show that the Notch model predicts the embeddings of the real words from their constituents but helps account for the behavioral data of the pseudowords. Moreover, we also demonstrated that character Jacobians reflect the characters\u2019 meanings. Taken together, the Notch model and character Jacobians may provide a new perspective on studying the word-formation process and morphology with modern deep learning.",
        "author": "Yu-Hsiang Tseng; Shu-Kai Hsieh",
        "authorids": "/y/yu-hsiang-tseng/; /s/shu-kai-hsieh/",
        "bibtex": "@inproceedings{tseng-hsieh-2022-character,\n    title = \"Character Jacobian: Modeling {C}hinese Character Meanings with Deep Learning Model\",\n    author = \"Tseng, Yu-Hsiang  and\n      Hsieh, Shu-Kai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.14/\",\n    pages = \"152--162\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.14.pdf",
        "site": "https://aclanthology.org/2022.coling-1.14/",
        "pdf_size": 1252263,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10803035640968046690&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Graduate Institute of Linguistics, National Taiwan University; Graduate Institute of Linguistics, National Taiwan University",
        "aff_domain": "gmail.com;ntu.edu.tw",
        "email": "gmail.com;ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Graduate Institute of Linguistics",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.560",
        "title": "Chinese Couplet Generation with Syntactic Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chinese couplet generation aims to generate a pair of clauses (usually generating a subsequent clause given an antecedent one) with certain rules (e.g., morphological and syntactical symmetry) adhered and has long been a challenging task with cultural background. To generate high-quality couplet (antecedent) clauses, it normally requires a model to learn the correspondences between antecedent and subsequent clauses under aforementioned rules and constraint of few characters with their concise usage. To tackle this task, previous studies normally directly adopt deep neural networks without explicitly taking into account fine-grained analysis of the clauses, in this paper, we propose to enhance Chinese couplet generation by leveraging syntactic information, i.e., part-of-speech (POS) tags and word dependencies. In doing so, we identify word boundaries in the antecedent clause and then use a special attention module to encode the syntactic information over the words for better generating the subsequent clause. Experimental results on a dataset for Chinese couplet generation illustrate the validity and effectiveness of our approach, which outperforms strong baselines with respect to automatic and manual evaluation metrics.",
        "author": "Yan Song",
        "authorids": "/y/yan-song/",
        "bibtex": "@inproceedings{song-2022-chinese,\n    title = \"{C}hinese Couplet Generation with Syntactic Information\",\n    author = \"Song, Yan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.560/\",\n    pages = \"6436--6446\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.560.pdf",
        "site": "https://aclanthology.org/2022.coling-1.560/",
        "pdf_size": 1368673,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17966686882906943999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "https://github.com/synlp/ChiCoupletGen",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.399",
        "title": "CitRet: A Hybrid Model for Cited Text Span Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The paper aims to identify cited text spans in the reference paper related to the given citance in the citing paper. We refer to it as cited text span retrieval (CTSR). Most current methods attempt this task by relying on pre-trained off-the-shelf deep learning models like SciBERT. Though these models are pre-trained on large datasets, they under-perform in out-of-domain settings. We introduce CitRet, a novel hybrid model for CTSR that leverages unique semantic and syntactic structural characteristics of scientific documents. This enables us to use significantly less data for finetuning. We use only 1040 documents for finetuning. Our model augments mildly-trained SBERT-based contextual embeddings with pre-trained non-contextual Word2Vec embeddings to calculate semantic textual similarity. We demonstrate the performance of our model on the CLSciSumm shared tasks. It improves the state-of-the-art results by over 15% on the F1 score evaluation.",
        "author": "Amit Pandey; Avani Gupta; Vikram Pudi",
        "authorids": "/a/amit-pandey/; /a/avani-gupta/; /v/vikram-pudi/",
        "bibtex": "@inproceedings{pandey-etal-2022-citret,\n    title = \"{C}it{R}et: A Hybrid Model for Cited Text Span Retrieval\",\n    author = \"Pandey, Amit  and\n      Gupta, Avani  and\n      Pudi, Vikram\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.399/\",\n    pages = \"4528--4536\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.399.pdf",
        "site": "https://aclanthology.org/2022.coling-1.399/",
        "pdf_size": 980584,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1TmBVYeUuLcJ:scholar.google.com/&scioq=CitRet:+A+Hybrid+Model+for+Cited+Text+Span+Retrieval&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Data Sciences and Analytics Center, Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India; Data Sciences and Analytics Center, Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India; Data Sciences and Analytics Center, Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India",
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International Institute of Information Technology",
        "aff_unique_dep": "Data Sciences and Analytics Center, Kohli Center on Intelligent Systems",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.419",
        "title": "Classical Sequence Match Is a Competitive Few-Shot One-Class Learner",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nowadays, transformer-based models gradually become the default choice for artificial intelligence pioneers. The models also show superiority even in the few-shot scenarios. In this paper, we revisit the classical methods and propose a new few-shot alternative. Specifically, we investigate the few-shot one-class problem, which actually takes a known sample as a reference to detect whether an unknown instance belongs to the same class. This problem can be studied from the perspective of sequence match. It is shown that with meta-learning, the classical sequence match method, i.e. Compare-Aggregate, significantly outperforms transformer ones. The classical approach requires much less training cost. Furthermore, we perform an empirical comparison between two kinds of sequence match approaches under simple fine-tuning and meta-learning. Meta-learning causes the transformer models\u2019 features to have high-correlation dimensions. The reason is closely related to the number of layers and heads of transformer models. Experimental codes and data are available at https://github.com/hmt2014/FewOne.",
        "author": "Mengting Hu; Hang Gao; Yinhao Bai; Mingming Liu",
        "authorids": "/m/mengting-hu/; /h/hang-gao/; /y/yinhao-bai/; /m/mingming-liu/",
        "bibtex": "@inproceedings{hu-etal-2022-classical,\n    title = \"Classical Sequence Match Is a Competitive Few-Shot One-Class Learner\",\n    author = \"Hu, Mengting  and\n      Gao, Hang  and\n      Bai, Yinhao  and\n      Liu, Mingming\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.419/\",\n    pages = \"4728--4740\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.419.pdf",
        "site": "https://aclanthology.org/2022.coling-1.419/",
        "pdf_size": 4836716,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:rXtRcGS6UDwJ:scholar.google.com/&scioq=Classical+Sequence+Match+Is+a+Competitive+Few-Shot+One-Class+Learner&hl=en&as_sdt=0,33",
        "gs_version_total": 6,
        "aff": "College of Software, Nankai University; Institute for Public Safety Research, Tsinghua University; College of Software, Nankai University; College of Software, Nankai University",
        "aff_domain": "nankai.edu.cn;mail.tsinghua.edu.cn;mail.nankai.edu.cn;nankai.edu.cn",
        "email": "nankai.edu.cn;mail.tsinghua.edu.cn;mail.nankai.edu.cn;nankai.edu.cn",
        "github": "https://github.com/hmt2014/FewOne",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Nankai University;Tsinghua University",
        "aff_unique_dep": "College of Software;Institute for Public Safety Research",
        "aff_unique_url": "http://www.nankai.edu.cn;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Nankai;Tsinghua",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.158",
        "title": "Cluster-aware Pseudo-Labeling for Supervised Open Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Supervised open relation extraction aims to discover novel relations by leveraging supervised data of pre-defined relations. However, most existing methods do not achieve effective knowledge transfer from pre-defined relations to novel relations, they have difficulties generating high-quality pseudo-labels for unsupervised data of novel relations and usually suffer from the error propagation issue. In this paper, we propose a Cluster-aware Pseudo-Labeling (CaPL) method to improve the pseudo-labels quality and transfer more knowledge for discovering novel relations. Specifically, the model is firstly pre-trained with the pre-defined relations to learn the relation representations. To improve the pseudo-labels quality, the distances between each instance and all cluster centers are used to generate the cluster-aware soft pseudo-labels for novel relations. To mitigate the catastrophic forgetting issue, we design the consistency regularization loss to make better use of the pseudo-labels and jointly train the model with both unsupervised and supervised data. Experimental results on two public datasets demonstrate that our proposed method achieves new state-of-the-arts performance.",
        "author": "Bin Duan; Shusen Wang; Xingxian Liu; Yajing Xu",
        "authorids": "/b/bin-duan/; /s/shusen-wang/; /x/xingxian-liu/; /y/yajing-xu/",
        "bibtex": "@inproceedings{duan-etal-2022-cluster,\n    title = \"Cluster-aware Pseudo-Labeling for Supervised Open Relation Extraction\",\n    author = \"Duan, Bin  and\n      Wang, Shusen  and\n      Liu, Xingxian  and\n      Xu, Yajing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.158/\",\n    pages = \"1834--1841\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.158.pdf",
        "site": "https://aclanthology.org/2022.coling-1.158/",
        "pdf_size": 490386,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1647832095846813186&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/BobTuan/CaPL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "Pattern Recognition & Intelligent System Laboratory",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.557",
        "title": "CoCGAN: Contrastive Learning for Adversarial Category Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of generating texts of different categories has attracted more and more attention in the area of natural language generation recently. Meanwhile, generative adversarial net (GAN) has demonstrated its effectiveness on text generation, and is further applied to category text generation in later works. Different from existing methods, which mainly consider the pairwise relations between the text embedding and the corresponding fixed one-hot class label (data-to-class relations), this paper proposes a novel Contrastive Category Generative Adversarial Net (CoCGAN) to incorporate contrastive learning into adversarial category text generation, considering more flexible data-to-class relations as well as relations between the multiple text embeddings in the same batch (data-to-data relations). The discriminator of CoCGAN discriminates the authenticity of given samples and optimizes a contrastive learning objective to capture both more flexible data-to-class relations and data-to-data relations among training samples. Accordingly, the generator tries to produce more realistic samples which can confuse the discriminator. Experimental results on both synthetic and real category text generation datasets demonstrate that CoCGAN can achieve significant improvements over the baseline category text generation models.",
        "author": "Xin Sheng; Linli Xu; Yinlong Xu; Changcun Bao; Huang Chen; Bo Ren",
        "authorids": "/x/xin-sheng/; /l/linli-xu/; /y/yinlong-xu/; /c/changcun-bao/; /h/huang-chen/; /b/bo-ren/",
        "bibtex": "@inproceedings{sheng-etal-2022-cocgan,\n    title = \"{C}o{CGAN}: Contrastive Learning for Adversarial Category Text Generation\",\n    author = \"Sheng, Xin  and\n      Xu, Linli  and\n      Xu, Yinlong  and\n      Bao, Changcun  and\n      Chen, Huang  and\n      Ren, Bo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.557/\",\n    pages = \"6403--6414\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.557.pdf",
        "site": "https://aclanthology.org/2022.coling-1.557/",
        "pdf_size": 597635,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3961973108668183398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China + State Key Laboratory of Cognitive Intelligence; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China + State Key Laboratory of Cognitive Intelligence; School of Computer Science and Technology, University of Science and Technology of China; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab",
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;tencent.com;tencent.com;tencent.com",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0;2;2;2",
        "aff_unique_norm": "University of Science and Technology of China;State Key Laboratory of Cognitive Intelligence;Tencent",
        "aff_unique_dep": "School of Computer Science and Technology;;Youtu Lab",
        "aff_unique_url": "http://www.ustc.edu.cn;;https://www.tencent.com",
        "aff_unique_abbr": "USTC;;Tencent",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.462",
        "title": "CoDoNMT: Modeling Cohesion Devices for Document-Level Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cohesion devices, e.g., reiteration, coreference, are crucial for building cohesion links across sentences. In this paper, we propose a document-level neural machine translation framework, CoDoNMT, which models cohesion devices from two perspectives: Cohesion Device Masking (CoDM) and Cohesion Attention Focusing (CoAF). In CoDM, we mask cohesion devices in the current sentence and force NMT to predict them with inter-sentential context information. A prediction task is also introduced to be jointly trained with NMT. In CoAF, we attempt to guide the model to pay exclusive attention to relevant cohesion devices in the context when translating cohesion devices in the current sentence. Such a cohesion attention focusing strategy is softly applied to the self-attention layer. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art document-level neural machine translation baselines. Further linguistic evaluation validates the effectiveness of the proposed model in producing cohesive translations.",
        "author": "Yikun Lei; Yuqi Ren; Deyi Xiong",
        "authorids": "/y/yikun-lei/; /y/yuqi-ren/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{lei-etal-2022-codonmt,\n    title = \"{C}o{D}o{NMT}: Modeling Cohesion Devices for Document-Level Neural Machine Translation\",\n    author = \"Lei, Yikun  and\n      Ren, Yuqi  and\n      Xiong, Deyi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.462/\",\n    pages = \"5205--5216\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.462.pdf",
        "site": "https://aclanthology.org/2022.coling-1.462/",
        "pdf_size": 1542548,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18017750793376863001&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.48",
        "title": "CoHS-CQG: Context and History Selection for Conversational Question Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational question generation (CQG) serves as a vital task for machines to assist humans, such as interactive reading comprehension, through conversations. Compared to traditional single-turn question generation (SQG), CQG is more challenging in the sense that the generated question is required not only to be meaningful, but also to align with the provided conversation. Previous studies mainly focus on how to model the flow and alignment of the conversation, but do not thoroughly study which parts of the context and history are necessary for the model. We believe that shortening the context and history is crucial as it can help the model to optimise more on the conversational alignment property. To this end, we propose CoHS-CQG, a two-stage CQG framework, which adopts a novel CoHS module to shorten the context and history of the input. In particular, it selects the top-p sentences and history turns by calculating the relevance scores of them. Our model achieves state-of-the-art performances on CoQA in both the answer-aware and answer-unaware settings.",
        "author": "Xuan Long Do; Bowei Zou; Liangming Pan; Nancy F. Chen; Shafiq Joty; Ai Ti Aw",
        "authorids": "/x/xuan-long-do/; /b/bowei-zou/; /l/liangming-pan/; /n/nancy-chen/; /s/shafiq-joty/; /a/aiti-aw/",
        "bibtex": "@inproceedings{do-etal-2022-cohs,\n    title = \"{C}o{HS}-{CQG}: Context and History Selection for Conversational Question Generation\",\n    author = \"Do, Xuan Long  and\n      Zou, Bowei  and\n      Pan, Liangming  and\n      Chen, Nancy F.  and\n      Joty, Shafiq  and\n      Aw, Ai Ti\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.48/\",\n    pages = \"580--591\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.48.pdf",
        "site": "https://aclanthology.org/2022.coling-1.48/",
        "pdf_size": 569377,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11350757463944162123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/dxlong2000/CoHS-CQG",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.508",
        "title": "CoLo: A Contrastive Learning Based Re-ranking Framework for One-Stage Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called CoLo. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that CoLo boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3x 8x speed-up ratio during inference while maintaining comparable results.",
        "author": "Chenxin An; Ming Zhong; Zhiyong Wu; Qin Zhu; Xuanjing Huang; Xipeng Qiu",
        "authorids": "/c/chenxin-an/; /m/ming-zhong/; /z/zhiyong-wu/; /q/qin-zhu/; /x/xuan-jing-huang/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{an-etal-2022-colo,\n    title = \"{C}o{L}o: A Contrastive Learning Based Re-ranking Framework for One-Stage Summarization\",\n    author = \"An, Chenxin  and\n      Zhong, Ming  and\n      Wu, Zhiyong  and\n      Zhu, Qin  and\n      Huang, Xuanjing  and\n      Qiu, Xipeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.508/\",\n    pages = \"5783--5793\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.508.pdf",
        "site": "https://aclanthology.org/2022.coling-1.508/",
        "pdf_size": 3134826,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=723365448313581435&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Fudan University; University of Illinois at Urbana-Champaign; Shanghai AI Lab; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",
        "aff_domain": "fudan.edu.cn;illinois.edu;pjlab.org.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;illinois.edu;pjlab.org.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;0;0",
        "aff_unique_norm": "Fudan University;University of Illinois at Urbana-Champaign;Shanghai AI Lab",
        "aff_unique_dep": "School of Computer Science;;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://illinois.edu;https://www.shanghaiailab.com",
        "aff_unique_abbr": "Fudan;UIUC;SAIL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;1;0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.595",
        "title": "CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present CoNTACT: a Dutch language model adapted to the domain of COVID-19 tweets. The model was developed by continuing the pre-training phase of RobBERT (Delobelle et al., 2020) by using 2.8M Dutch COVID-19 related tweets posted in 2021. In order to test the performance of the model and compare it to RobBERT, the two models were tested on two tasks: (1) binary vaccine hesitancy detection and (2) detection of arguments for vaccine hesitancy. For both tasks, not only Twitter but also Facebook data was used to show cross-genre performance. In our experiments, CoNTACT showed statistically significant gains over RobBERT in all experiments for task 1. For task 2, we observed substantial improvements in virtually all classes in all experiments. An error analysis indicated that the domain adaptation yielded better representations of domain-specific terminology, causing CoNTACT to make more accurate classification decisions. For task 2, we observed substantial improvements in virtually all classes in all experiments. An error analysis indicated that the domain adaptation yielded better representations of domain-specific terminology, causing CoNTACT to make more accurate classification decisions.",
        "author": "Jens Lemmens; Jens Van Nooten; Tim Kreutz; Walter Daelemans",
        "authorids": "/j/jens-lemmens/; /j/jens-van-nooten/; /t/tim-kreutz/; /w/walter-daelemans/",
        "bibtex": "@inproceedings{lemmens-etal-2022-contact,\n    title = \"{C}o{NTACT}: A {D}utch {COVID}-19 Adapted {BERT} for Vaccine Hesitancy and Argumentation Detection\",\n    author = \"Lemmens, Jens  and\n      Van Nooten, Jens  and\n      Kreutz, Tim  and\n      Daelemans, Walter\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.595/\",\n    pages = \"6837--6845\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.595.pdf",
        "site": "https://aclanthology.org/2022.coling-1.595/",
        "pdf_size": 188971,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7095262054903125000&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "https://huggingface.co/clips/contact",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.132",
        "title": "Coalescing Global and Local Information for Procedural Text Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Procedural text understanding is a challenging language reasoning task that requires models to track entity states across the development of a narrative. We identify three core aspects required for modeling this task, namely the local and global view of the inputs, as well as the global view of outputs. Prior methods have considered a subset of these aspects, which leads to either low precision or low recall. In this paper, we propose a new model Coalescing Global and Local Information (CGLI), which builds entity- and timestep-aware input representations (local input) considering the whole context (global input), and we jointly model the entity states with a structured prediction objective (global output). Thus, CGLI simultaneously optimizes for both precision and recall. Moreover, we extend CGLI with additional output layers and integrate it into a story reasoning framework. Extensive experiments on a popular procedural text understanding dataset show that our model achieves state-of-the-art results, while experiments on a story reasoning benchmark show the positive impact of our model on downstream reasoning.",
        "author": "Kaixin Ma; Filip Ilievski; Jonathan Francis; Eric Nyberg; Alessandro Oltramari",
        "authorids": "/k/kaixin-ma/; /f/filip-ilievski/; /j/jonathan-francis/; /e/eric-nyberg/; /a/alessandro-oltramari/",
        "bibtex": "@inproceedings{ma-etal-2022-coalescing,\n    title = \"Coalescing Global and Local Information for Procedural Text Understanding\",\n    author = \"Ma, Kaixin  and\n      Ilievski, Filip  and\n      Francis, Jonathan  and\n      Nyberg, Eric  and\n      Oltramari, Alessandro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.132/\",\n    pages = \"1534--1545\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.132.pdf",
        "site": "https://aclanthology.org/2022.coling-1.132/",
        "pdf_size": 1748611,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11164456748809084883&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Information Sciences Institute, University of Southern California; Language Technologies Institute, Carnegie Mellon University + Human-Machine Collaboration, Bosch Research Pittsburgh; Language Technologies Institute, Carnegie Mellon University; Human-Machine Collaboration, Bosch Research Pittsburgh",
        "aff_domain": "cs.cmu.edu;isi.edu;cs.cmu.edu;cs.cmu.edu;us.bosch.com",
        "email": "cs.cmu.edu;isi.edu;cs.cmu.edu;cs.cmu.edu;us.bosch.com",
        "github": "https://github.com/Mayer123/CGLI",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0+2;0;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Southern California;Bosch Research",
        "aff_unique_dep": "Language Technologies Institute;Information Sciences Institute;Human-Machine Collaboration",
        "aff_unique_url": "https://www.cmu.edu;https://www.usc.edu;https://research.bosch.com",
        "aff_unique_abbr": "CMU;USC;Bosch",
        "aff_campus_unique_index": "0;1;0+0;0;0",
        "aff_campus_unique": "Pittsburgh;Los Angeles",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.439",
        "title": "Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generalized text representations are the foundation of many natural language understanding tasks. To fully utilize the different corpus, it is inevitable that models need to understand the relevance among them. However, many methods ignore the relevance and adopt a single-channel model (a coarse paradigm) directly for all tasks, which lacks enough rationality and interpretation. In addition, some existing works learn downstream tasks by stitches skill block (a fine paradigm), which might cause irrational results due to its redundancy and noise. In this work, we first analyze the task correlation through three different perspectives, , data property, manual design, and model-based relevance, based on which the similar tasks are grouped together. Then, we propose a hierarchical framework with a coarse-to-fine paradigm, with the bottom level shared to all the tasks, the mid-level divided to different groups, and the top-level assigned to each of the tasks. This allows our model to learn basic language properties from all tasks, boost performance on relevant tasks, and reduce the negative impact from irrelevant tasks. Our experiments on 13 benchmark datasets across five natural language understanding tasks demonstrate the superiority of our method.",
        "author": "Zhaoye Fei; Yu Tian; Yongkang Wu; Xinyu Zhang; Yutao Zhu; Zheng Liu; Jiawen Wu; Dejiang Kong; Ruofei Lai; Zhao Cao; Zhicheng Dou; Xipeng Qiu",
        "authorids": "/z/zhaoye-fei/; /y/yu-tian/; /y/yongkang-wu/; /x/xinyu-zhang/; /y/yutao-zhu/; /z/zheng-liu/; /j/jiawen-wu/; /d/dejiang-kong/; /r/ruofei-lai/; /z/zhao-cao/; /z/zhicheng-dou/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{fei-etal-2022-coarse,\n    title = \"Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language Understanding\",\n    author = \"Fei, Zhaoye  and\n      Tian, Yu  and\n      Wu, Yongkang  and\n      Zhang, Xinyu  and\n      Zhu, Yutao  and\n      Liu, Zheng  and\n      Wu, Jiawen  and\n      Kong, Dejiang  and\n      Lai, Ruofei  and\n      Cao, Zhao  and\n      Dou, Zhicheng  and\n      Qiu, Xipeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.439/\",\n    pages = \"4952--4964\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.439.pdf",
        "site": "https://aclanthology.org/2022.coling-1.439/",
        "pdf_size": 496751,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12554861829734184876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Huawei Poisson Lab, China+School of Computer Science, Fudan University, Shanghai, China; Huawei Poisson Lab, China+School of Computer Science, Fudan University, Shanghai, China; Huawei Poisson Lab, China+School of Computer Science, Fudan University, Shanghai, China; Huawei Poisson Lab, China+School of Computer Science, Fudan University, Shanghai, China; University of Montreal, Montreal, Quebec, Canada; Huawei Poisson Lab, China; School of Computer Science, Fudan University, Shanghai, China; Huawei Poisson Lab, China; Huawei Poisson Lab, China; Huawei Poisson Lab, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; School of Computer Science, Fudan University, Shanghai, China",
        "aff_domain": "huawei.com;huawei.com;huawei.com; ; ; ; ; ; ; ; ;",
        "email": "huawei.com;huawei.com;huawei.com; ; ; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0+1;0+1;0+1;0+1;2;0;1;0;0;0;3;1",
        "aff_unique_norm": "Huawei;Fudan University;University of Montreal;Renmin University of China",
        "aff_unique_dep": "Poisson Lab;School of Computer Science;;Gaoling School of Artificial Intelligence",
        "aff_unique_url": "https://www.huawei.com;https://www.fudan.edu.cn;https://www.mcgill.ca;http://www.ruc.edu.cn",
        "aff_unique_abbr": "Huawei;Fudan;UM;RUC",
        "aff_campus_unique_index": "1;1;1;1;2;1;3;1",
        "aff_campus_unique": ";Shanghai;Montreal;Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;1;0;0;0;0;0;0;0",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2022.coling-1.215",
        "title": "CofeNet: Context and Former-Label Enhanced Net for Complicated Quotation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Quotation extraction aims to extract quotations from written text. There are three components in a quotation: source refers to the holder of the quotation, cue is the trigger word(s), and content is the main body. Existing solutions for quotation extraction mainly utilize rule-based approaches and sequence labeling models. While rule-based approaches often lead to low recalls, sequence labeling models cannot well handle quotations with complicated structures. In this paper, we propose the Context and Former-Label Enhanced Net () for quotation extraction. is able to extract complicated quotations with components of variable lengths and complicated structures. On two public datasets (and ) and one proprietary dataset (), we show that our achieves state-of-the-art performance on complicated quotation extraction.",
        "author": "Yequan Wang; Xiang Li; Aixin Sun; Xuying Meng; Huaming Liao; Jiafeng Guo",
        "authorids": "/y/yequan-wang/; /x/xiang-li/; /a/aixin-sun/; /x/xuying-meng/; /h/huaming-liao/; /j/jiafeng-guo/",
        "bibtex": "@inproceedings{wang-etal-2022-cofenet,\n    title = \"{C}ofe{N}et: Context and Former-Label Enhanced Net for Complicated Quotation Extraction\",\n    author = \"Wang, Yequan  and\n      Li, Xiang  and\n      Sun, Aixin  and\n      Meng, Xuying  and\n      Liao, Huaming  and\n      Guo, Jiafeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.215/\",\n    pages = \"2438--2449\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.215.pdf",
        "site": "https://aclanthology.org/2022.coling-1.215/",
        "pdf_size": 631236,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14812033660718346260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.284",
        "title": "CogBERT: Cognition-Guided Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We study the problem of integrating cognitive language processing signals (e.g., eye-tracking or EEG data) into pre-trained language models like BERT. Existing methods typically fine-tune pre-trained models on cognitive data, ignoring the semantic gap between the texts and cognitive signals. To fill the gap, we propose CogBERT, a framework that can induce fine-grained cognitive features from cognitive data and incorporate cognitive features into BERT by adaptively adjusting the weight of cognitive features for different NLP tasks. Extensive experiments show that: (1) Cognition-guided pre-trained models can consistently perform better than basic pre-trained models on ten NLP tasks. (2) Different cognitive features contribute differently to different NLP tasks. Based on this observation, we give a fine-grained explanation of why cognitive data is helpful for NLP. (3) Different transformer layers of pre-trained models should encode different cognitive features, with word-level cognitive features at the bottom and semantic-level cognitive features at the top. (4) Attention visualization demonstrates that CogBERT aligns with human gaze patterns and improves its natural language comprehension ability.",
        "author": "Xiao Ding; Bowen Chen; Li Du; Bing Qin; Ting Liu",
        "authorids": "/x/xiao-ding/; /b/bowen-chen/; /l/li-du/; /b/bing-qin/; /t/ting-liu/",
        "bibtex": "@inproceedings{ding-etal-2022-cogbert,\n    title = \"{C}og{BERT}: Cognition-Guided Pre-trained Language Models\",\n    author = \"Ding, Xiao  and\n      Chen, Bowen  and\n      Du, Li  and\n      Qin, Bing  and\n      Liu, Ting\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.284/\",\n    pages = \"3210--3225\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.284.pdf",
        "site": "https://aclanthology.org/2022.coling-1.284/",
        "pdf_size": 2564971,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15917457847045474125&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.252",
        "title": "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Quantization, knowledge distillation, and magnitude pruning are among the most popular methods for neural network compression in NLP. Independently, these methods reduce model size and can accelerate inference, but their relative benefit and combinatorial interactions have not been rigorously studied. For each of the eight possible subsets of these techniques, we compare accuracy vs. model size tradeoffs across six BERT architecture sizes and eight GLUE tasks. We find that quantization and distillation consistently provide greater benefit than pruning. Surprisingly, except for the pair of pruning and quantization, using multiple methods together rarely yields diminishing returns. Instead, we observe complementary and super-multiplicative reductions to model size. Our work quantitatively demonstrates that combining compression methods can synergistically reduce model size, and that practitioners should prioritize (1) quantization, (2) knowledge distillation, and (3) pruning to maximize accuracy vs. model size tradeoffs.",
        "author": "Rajiv Movva; Jinhao Lei; Shayne Longpre; Ajay Gupta; Chris DuBois",
        "authorids": "/r/rajiv-movva/; /j/jinhao-lei/; /s/shayne-longpre/; /a/ajay-gupta/; /c/chris-dubois/",
        "bibtex": "@inproceedings{movva-etal-2022-combining,\n    title = \"Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks\",\n    author = \"Movva, Rajiv  and\n      Lei, Jinhao  and\n      Longpre, Shayne  and\n      Gupta, Ajay  and\n      DuBois, Chris\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.252/\",\n    pages = \"2861--2872\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.252.pdf",
        "site": "https://aclanthology.org/2022.coling-1.252/",
        "pdf_size": 2515903,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13452330619588293527&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Cornell Tech + Apple + Massachusetts Institute of Technology; Apple; Apple + Massachusetts Institute of Technology; Apple; Apple",
        "aff_domain": "cornell.edu;apple.com;mit.edu;apple.com;apple.com",
        "email": "cornell.edu;apple.com;mit.edu;apple.com;apple.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;1;1+2;1;1",
        "aff_unique_norm": "Cornell University;Apple Inc.;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://tech.cornell.edu;https://www.apple.com;https://web.mit.edu",
        "aff_unique_abbr": "Cornell Tech;Apple;MIT",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "New York City;",
        "aff_country_unique_index": "0+0+0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.81",
        "title": "Community Topic: Topic Model Inference by Consecutive Word Community Discovery",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present our novel, hyperparameter-free topic modelling algorithm, Community Topic. Our algorithm is based on mining communities from term co-occurrence networks. We empirically evaluate and compare Community Topic with Latent Dirichlet Allocation and the recently developed top2vec algorithm. We find that Community Topic runs faster than the competitors and produces topics that achieve higher coherence scores. Community Topic can discover coherent topics at various scales. The network representation used by Community Topic results in a natural relationship between topics and a topic hierarchy. This allows sub- and super-topics to be found on demand. These features make Community Topic the ideal tool for downstream applications such as applied research and conversational agents.",
        "author": "Eric Austin; Osmar R. Za\u00efane; Christine Largeron",
        "authorids": "/e/eric-austin/; /o/osmar-r-zaiane/; /c/christine-largeron/",
        "bibtex": "@inproceedings{austin-etal-2022-community,\n    title = \"Community Topic: Topic Model Inference by Consecutive Word Community Discovery\",\n    author = {Austin, Eric  and\n      Za{\\\"i}ane, Osmar R.  and\n      Largeron, Christine},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.81/\",\n    pages = \"971--983\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.81.pdf",
        "site": "https://aclanthology.org/2022.coling-1.81/",
        "pdf_size": 2793389,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9176987361737305061&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.593",
        "title": "CommunityLM: Probing Partisan Worldviews from Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As political attitudes have diverged ideologically in the United States, political speech has diverged lingusitically. The ever-widening polarization between the US political parties is accelerated by an erosion of mutual understanding between them. We aim to make these communities more comprehensible to each other with a framework that probes community-specific responses to the same survey questions using community language models CommunityLM. In our framework we identify committed partisan members for each community on Twitter and fine-tune LMs on the tweets authored by them. We then assess the worldviews of the two groups using prompt-based probing of their corresponding LMs, with prompts that elicit opinions about public figures and groups surveyed by the American National Election Studies (ANES) 2020 Exploratory Testing Survey. We compare the responses generated by the LMs to the ANES survey results, and find a level of alignment that greatly exceeds several baseline methods. Our work aims to show that we can use community LMs to query the worldview of any group of people given a sufficiently large sample of their social media discussions or media diet.",
        "author": "Hang Jiang; Doug Beeferman; Brandon Roy; Deb Roy",
        "authorids": "/h/hang-jiang/; /d/doug-beeferman/; /b/brandon-roy/; /d/deb-roy/",
        "bibtex": "@inproceedings{jiang-etal-2022-communitylm,\n    title = \"{C}ommunity{LM}: Probing Partisan Worldviews from Language Models\",\n    author = \"Jiang, Hang  and\n      Beeferman, Doug  and\n      Roy, Brandon  and\n      Roy, Deb\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.593/\",\n    pages = \"6818--6826\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.593.pdf",
        "site": "https://aclanthology.org/2022.coling-1.593/",
        "pdf_size": 246241,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9242839682805234510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.522",
        "title": "Comparative Graph-based Summarization of Scientific Papers Guided by Comparative Citations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid growth of scientific papers, understanding the changes and trends in a research area is rather time-consuming. The first challenge is to find related and comparable articles for the research. Comparative citations compare co-cited papers in a citation sentence and can serve as good guidance for researchers to track a research area. We thus go through comparative citations to find comparable objects and build a comparative scientific summarization corpus (CSSC). And then, we propose the comparative graph-based summarization (CGSUM) method to create comparative summaries using citations as guidance. The comparative graph is constructed using sentences as nodes and three different relationships of sentences as edges. The relationship that sentences occur in the same paper is used to calculate the salience of sentences, the relationship that sentences occur in two different papers is used to calculate the difference between sentences, and the relationship that sentences are related to citations is used to calculate the commonality of sentences. Experiments show that CGSUM outperforms comparative baselines on CSSC and performs well on DUC2006 and DUC2007.",
        "author": "Jingqiang Chen; Chaoxiang Cai; Xiaorui Jiang; Kejia Chen",
        "authorids": "/j/jingqiang-chen/; /c/chaoxiang-cai/; /x/xiaorui-jiang/; /k/kejia-chen/",
        "bibtex": "@inproceedings{chen-etal-2022-comparative-graph,\n    title = \"Comparative Graph-based Summarization of Scientific Papers Guided by Comparative Citations\",\n    author = \"Chen, Jingqiang  and\n      Cai, Chaoxiang  and\n      Jiang, Xiaorui  and\n      Chen, Kejia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.522/\",\n    pages = \"5978--5988\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.522.pdf",
        "site": "https://aclanthology.org/2022.coling-1.522/",
        "pdf_size": 1149337,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6483294010226828380&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Nanjing University of Posts and Telecommunications, China; School of Computer Science, Nanjing University of Posts and Telecommunications, China; Centre for Computational Science and Mathematical Modelling, Coventry University, UK; School of Computer Science, Nanjing University of Posts and Telecommunications, China",
        "aff_domain": "njupt.edu.cn;njupt.edu.cn;coventry.ac.uk;njupt.edu.cn",
        "email": "njupt.edu.cn;njupt.edu.cn;coventry.ac.uk;njupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Nanjing University of Posts and Telecommunications;Coventry University",
        "aff_unique_dep": "School of Computer Science;Centre for Computational Science and Mathematical Modelling",
        "aff_unique_url": "http://www.njupt.edu.cn;https://www.coventry.ac.uk",
        "aff_unique_abbr": "NUPT;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nanjing;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.coling-1.131",
        "title": "Competence-based Question Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Models of natural language understanding often rely on question answering and logical inference benchmark challenges to evaluate the performance of a system. While informative, such task-oriented evaluations do not assess the broader semantic abilities that humans have as part of their linguistic competence when speaking and interpreting language. We define competence-based (CB) question generation, and focus on queries over lexical semantic knowledge involving implicit argument and subevent structure of verbs. We present a method to generate such questions and a dataset of English cooking recipes we use for implementing the generation method. Our primary experiment shows that even large pretrained language models perform poorly on CB questions until they are provided with additional contextualized semantic information. The data and the source code is available at: https: //github.com/brandeis-llc/CompQG.",
        "author": "Jingxuan Tu; Kyeongmin Rim; James Pustejovsky",
        "authorids": "/j/jingxuan-tu/; /k/kyeongmin-rim/; /j/james-pustejovsky/",
        "bibtex": "@inproceedings{tu-etal-2022-competence,\n    title = \"Competence-based Question Generation\",\n    author = \"Tu, Jingxuan  and\n      Rim, Kyeongmin  and\n      Pustejovsky, James\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.131/\",\n    pages = \"1521--1533\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.131.pdf",
        "site": "https://aclanthology.org/2022.coling-1.131/",
        "pdf_size": 767460,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17183063583828053347&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Brandeis University; Department of Computer Science, Brandeis University; Department of Computer Science, Brandeis University",
        "aff_domain": "brandeis.edu;brandeis.edu;brandeis.edu",
        "email": "brandeis.edu;brandeis.edu;brandeis.edu",
        "github": "https://github.com/brandeis-llc/CompQG",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brandeis University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brandeis.edu",
        "aff_unique_abbr": "Brandeis",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.97",
        "title": "Complicate Then Simplify: A Novel Way to Explore Pre-trained Models for Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the development of pre-trained models (PTMs), the performance of text classification has been continuously improved by directly employing the features generated by PTMs. However such way might not fully explore the knowledge in PTMs as it is constrained by the difficulty of the task. Compared to difficult task, the learning algorithms tend to saturate early on the simple task. Moreover, the native sentence representations derived from BERT are prone to be collapsed and directly employing such representation for text classification might fail to fully capture discriminative features. In order to address these issues, in this paper we propose a novel framework for text classification which implements a two-stage training strategy. In the pre-training stage, auxiliary labels are introduced to increase the task difficulties and to fully exploit the knowledge in the pre-trained model. In the fine-tuning stage, the textual representation learned in the pre-training stage is employed and the classifier is fine-tuned to obtain better classification performance. Experiments were conducted on six text classification corpora and the results showed that the proposed framework outperformed several state-of-the-art baselines.",
        "author": "Xu Zhang; Zejie Liu; Yanzheng Xiang; Deyu Zhou",
        "authorids": "/x/xu-zhang/; /z/zejie-liu/; /y/yanzheng-xiang/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2022-complicate,\n    title = \"Complicate Then Simplify: A Novel Way to Explore Pre-trained Models for Text Classification\",\n    author = \"Zhang, Xu  and\n      Liu, Zejie  and\n      Xiang, Yanzheng  and\n      Zhou, Deyu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.97/\",\n    pages = \"1136--1145\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.97.pdf",
        "site": "https://aclanthology.org/2022.coling-1.97/",
        "pdf_size": 376983,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15509770008423502505&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.594",
        "title": "Composition-based Heterogeneous Graph Multi-channel Attention Network for Multi-aspect Multi-sentiment Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) has drawn more and more attention because of its extensive applications. However, towards the sentence carried with more than one aspect, most existing works generate an aspect-specific sentence representation for each aspect term to predict sentiment polarity, which neglects the sentiment relationship among aspect terms. Besides, most current ABSA methods focus on sentences containing only one aspect term or multiple aspect terms with the same sentiment polarity, which makes ABSA degenerate into sentence-level sentiment analysis. In this paper, to deal with this problem, we construct a heterogeneous graph to model inter-aspect relationships and aspect-context relationships simultaneously and propose a novel Composition-based Heterogeneous Graph Multi-channel Attention Network (CHGMAN) to encode the constructed heterogeneous graph. Meanwhile, we conduct extensive experiments on three datasets: MAMSATSA, Rest14, and Laptop14, experimental results show the effectiveness of our method.",
        "author": "Hao Niu; Yun Xiong; Jian Gao; Zhongchen Miao; Xiaosu Wang; Hongrun Ren; Yao Zhang; Yangyong Zhu",
        "authorids": "/h/hao-niu/; /y/yun-xiong/; /j/jian-gao/; /z/zhongchen-miao/; /x/xiaosu-wang/; /h/hongrun-ren/; /y/yao-zhang/; /y/yangyong-zhu/",
        "bibtex": "@inproceedings{niu-etal-2022-composition,\n    title = \"Composition-based Heterogeneous Graph Multi-channel Attention Network for Multi-aspect Multi-sentiment Classification\",\n    author = \"Niu, Hao  and\n      Xiong, Yun  and\n      Gao, Jian  and\n      Miao, Zhongchen  and\n      Wang, Xiaosu  and\n      Ren, Hongrun  and\n      Zhang, Yao  and\n      Zhu, Yangyong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.594/\",\n    pages = \"6827--6836\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.594.pdf",
        "site": "https://aclanthology.org/2022.coling-1.594/",
        "pdf_size": 1312387,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4217949014781927107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2022.coling-1.298",
        "title": "ConFiguRe: Exploring Discourse-level Chinese Figures of Speech",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Figures of speech, such as metaphor and irony, are ubiquitous in literature works and colloquial conversations. This poses great challenge for natural language understanding since figures of speech usually deviate from their ostensible meanings to express deeper semantic implications. Previous research lays emphasis on the literary aspect of figures and seldom provide a comprehensive exploration from a view of computational linguistics. In this paper, we first propose the concept of figurative unit, which is the carrier of a figure. Then we select 12 types of figures commonly used in Chinese, and build a Chinese corpus for Contextualized Figure Recognition (ConFiguRe). Different from previous token-level or sentence-level counterparts, ConFiguRe aims at extracting a figurative unit from discourse-level context, and classifying the figurative unit into the right figure type. On ConFiguRe, three tasks, i.e., figure extraction, figure type classification and figure recognition, are designed and the state-of-the-art techniques are utilized to implement the benchmarks. We conduct thorough experiments and show that all three tasks are challenging for existing models, thus requiring further research. Our dataset and code are publicly available at https://github.com/pku-tangent/ConFiguRe.",
        "author": "Dawei Zhu; Qiusi Zhan; Zhejian Zhou; Yifan Song; Jiebin Zhang; Sujian Li",
        "authorids": "/d/dawei-zhu/; /q/qiusi-zhan/; /z/zhejian-zhou/; /y/yifan-song/; /j/jiebin-zhang/; /s/sujian-li/",
        "bibtex": "@inproceedings{zhu-etal-2022-configure,\n    title = \"{C}on{F}igu{R}e: Exploring Discourse-level {C}hinese Figures of Speech\",\n    author = \"Zhu, Dawei  and\n      Zhan, Qiusi  and\n      Zhou, Zhejian  and\n      Song, Yifan  and\n      Zhang, Jiebin  and\n      Li, Sujian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.298/\",\n    pages = \"3374--3385\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.298.pdf",
        "site": "https://aclanthology.org/2022.coling-1.298/",
        "pdf_size": 665066,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5736387411301495158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University + Key Laboratory of Computational Linguistics, MOE, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/pku-tangent/ConFiguRe",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.100",
        "title": "ConTextING: Granting Document-Wise Contextual Embeddings to Graph Neural Networks for Inductive Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Graph neural networks (GNNs) have been recently applied in natural language processing. Various GNN research studies are proposed to learn node interactions within the local graph of each document that contains words, sentences, or topics for inductive text classification. However, most inductive GNNs that are built on a word graph generally take global word embeddings as node features, without referring to document-wise contextual information. Consequently, we find that BERT models can perform better than inductive GNNs. An intuitive follow-up approach is used to enrich GNNs with contextual embeddings from BERT, yet there is a lack of related research. In this work, we propose a simple yet effective unified model, coined ConTextING, with a joint training mechanism to learn from both document embeddings and contextual word interactions simultaneously. Our experiments show that ConTextING outperforms pure inductive GNNs and BERT-style models. The analyses also highlight the benefits of the sub-word graph and joint training with separated classifiers.",
        "author": "Yen-Hao Huang; Yi-Hsin Chen; Yi-Shin Chen",
        "authorids": "/y/yen-hao-huang/; /y/yi-hsin-chen/; /y/yi-shin-chen/",
        "bibtex": "@inproceedings{huang-etal-2022-contexting,\n    title = \"{C}on{T}ext{ING}: Granting Document-Wise Contextual Embeddings to Graph Neural Networks for Inductive Text Classification\",\n    author = \"Huang, Yen-Hao  and\n      Chen, Yi-Hsin  and\n      Chen, Yi-Shin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.100/\",\n    pages = \"1163--1168\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.100.pdf",
        "site": "https://aclanthology.org/2022.coling-1.100/",
        "pdf_size": 491643,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8750611964732574818&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Systems and Applications\u2020+National Tsing Hua University, Hsinchu, Taiwan; Dcard, Taipei, Taiwan\u2021; Institute of Information Systems and Applications\u2020+National Tsing Hua University, Hsinchu, Taiwan",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Institute of Information Systems and Applications;National Tsing Hua University;Dcard",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.nthu.edu.tw;https://www.dcard.com.tw",
        "aff_unique_abbr": ";NTHU;",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2022.coling-1.75",
        "title": "ConnPrompt: Connective-cloze Prompt Learning for Implicit Discourse Relation Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Implicit Discourse Relation Recognition (IDRR) is to detect and classify relation sense between two text segments without an explicit connective. Vanilla pre-train and fine-tuning paradigm builds upon a Pre-trained Language Model (PLM) with a task-specific neural network. However, the task objective functions are often not in accordance with that of the PLM. Furthermore, this paradigm cannot well exploit some linguistic evidence embedded in the pre-training process. The recent pre-train, prompt, and predict paradigm selects appropriate prompts to reformulate downstream tasks, so as to utilizing the PLM itself for prediction. However, for its success applications, prompts, verbalizer as well as model training should still be carefully designed for different tasks. As the first trial of using this new paradigm for IDRR, this paper develops a Connective-cloze Prompt (ConnPrompt) to transform the relation prediction task as a connective-cloze task. Specifically, we design two styles of ConnPrompt template: Insert-cloze Prompt (ICP) and Prefix-cloze Prompt (PCP) and construct an answer space mapping to the relation senses based on the hierarchy sense tags and implicit connectives. Furthermore, we use a multi-prompt ensemble to fuse predictions from different prompting results. Experiments on the PDTB corpus show that our method significantly outperforms the state-of-the-art algorithms, even with fewer training data.",
        "author": "Wei Xiang; Zhenglin Wang; Lu Dai; Bang Wang",
        "authorids": "/w/wei-xiang/; /z/zhenglin-wang/; /l/lu-dai/; /b/bang-wang/",
        "bibtex": "@inproceedings{xiang-etal-2022-connprompt,\n    title = \"{C}onn{P}rompt: Connective-cloze Prompt Learning for Implicit Discourse Relation Recognition\",\n    author = \"Xiang, Wei  and\n      Wang, Zhenglin  and\n      Dai, Lu  and\n      Wang, Bang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.75/\",\n    pages = \"902--911\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.75.pdf",
        "site": "https://aclanthology.org/2022.coling-1.75/",
        "pdf_size": 2621443,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5091384982229508717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Electronic Information and Communications",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.236",
        "title": "Constrained Regeneration for Cross-Lingual Query-Focused Extractive Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Query-focused summaries of foreign-language, retrieved documents can help a user understand whether a document is actually relevant to the query term. A standard approach to this problem is to first translate the source documents and then perform extractive summarization to find relevant snippets. However, in a cross-lingual setting, the query term does not necessarily appear in the translations of relevant documents. In this work, we show that constrained machine translation and constrained post-editing can improve human relevance judgments by including a query term in a summary when its translation appears in the source document. We also present several strategies for selecting only certain documents for regeneration which yield further improvements",
        "author": "Elsbeth Turcan; David Wan; Faisal Ladhak; Petra Galuscakova; Sukanta Sen; Svetlana Tchistiakova; Weijia Xu; Marine Carpuat; Kenneth Heafield; Douglas Oard; Kathleen McKeown",
        "authorids": "/e/elsbeth-turcan/; /d/david-wan/; /f/faisal-ladhak/; /p/petra-galuscakova/; /s/sukanta-sen/; /s/svetlana-tchistiakova/; /w/weijia-xu/; /m/marine-carpuat/; /k/kenneth-heafield/; /d/douglas-w-oard/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{turcan-etal-2022-constrained,\n    title = \"Constrained Regeneration for Cross-Lingual Query-Focused Extractive Summarization\",\n    author = \"Turcan, Elsbeth  and\n      Wan, David  and\n      Ladhak, Faisal  and\n      Galuscakova, Petra  and\n      Sen, Sukanta  and\n      Tchistiakova, Svetlana  and\n      Xu, Weijia  and\n      Carpuat, Marine  and\n      Heafield, Kenneth  and\n      Oard, Douglas  and\n      McKeown, Kathleen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.236/\",\n    pages = \"2668--2680\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.236.pdf",
        "site": "https://aclanthology.org/2022.coling-1.236/",
        "pdf_size": 419261,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12656709204931631809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, University of North Carolina at Chapel Hill; Universit\u00e9 Grenoble Alpes; School of Informatics, The University of Edinburgh; Department of Computer Science & UMIACS, The University of Maryland + Columbia University; Department of Computer Science, Columbia University; School of Informatics, The University of Edinburgh; School of Informatics, The University of Edinburgh; Department of Computer Science & UMIACS, The University of Maryland; Department of Computer Science & UMIACS, The University of Maryland; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;cs.unc.edu;cs.columbia.edu; ; ; ; ; ; ; ; ",
        "email": "cs.columbia.edu;cs.unc.edu;cs.columbia.edu; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;2;3;4+0;0;3;3;4;4;0",
        "aff_unique_norm": "Columbia University;University of North Carolina at Chapel Hill;Universit\u00e9 Grenoble Alpes;The University of Edinburgh;The University of Maryland",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;;School of Informatics;Department of Computer Science & UMIACS",
        "aff_unique_url": "https://www.columbia.edu;https://www.unc.edu;https://www.univ-grenoble-alpes.fr;https://www.ed.ac.uk;https://www.umd.edu",
        "aff_unique_abbr": "Columbia;UNC Chapel Hill;UGA;Edinburgh;UMD",
        "aff_campus_unique_index": "1;2;;2;2",
        "aff_campus_unique": ";Chapel Hill;Edinburgh",
        "aff_country_unique_index": "0;0;1;2;0+0;0;2;2;0;0;0",
        "aff_country_unique": "United States;France;United Kingdom"
    },
    {
        "id": "2022.coling-1.507",
        "title": "Content Type Profiling of Data-to-Text Generation Datasets",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Data-to-Text Generation (D2T) problems can be considered as a stream of time-stamped events with a text summary being produced for each. The problem becomes more challenging when event summaries contain complex insights derived from multiple records either within an event, or across several events from the event stream. It is important to understand the different types of content present in the summary to help us better define the system requirements so that we can build better systems. In this paper, we propose a novel typology of content types, that we use to classify the contents of event summaries. Using the typology, a profile of a dataset is generated as the distribution of the aggregated content types which captures the specific characteristics of the dataset and gives a measure of the complexity present in the problem. Extensive experimentation on different D2T datasets is performed and these demonstrate that neural systems struggle in generating contents of complex types.",
        "author": "Ashish Upadhyay; Stewart Massie",
        "authorids": "/a/ashish-upadhyay/; /s/stewart-massie/",
        "bibtex": "@inproceedings{upadhyay-massie-2022-content,\n    title = \"Content Type Profiling of Data-to-Text Generation Datasets\",\n    author = \"Upadhyay, Ashish  and\n      Massie, Stewart\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.507/\",\n    pages = \"5770--5782\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.507.pdf",
        "site": "https://aclanthology.org/2022.coling-1.507/",
        "pdf_size": 489983,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12382773894796457307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computing, Robert Gordon University, Aberdeen, UK; School of Computing, Robert Gordon University, Aberdeen, UK",
        "aff_domain": "rgu.ac.uk;rgu.ac.uk",
        "email": "rgu.ac.uk;rgu.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Robert Gordon University",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.rgu.ac.uk",
        "aff_unique_abbr": "RGU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Aberdeen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.552",
        "title": "Context-Tuning: Learning Contextualized Prompts for Natural Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, pretrained language models (PLMs) have had exceptional success in language generation. To leverage the rich knowledge encoded by PLMs, a simple yet powerful paradigm is to use prompts in the form of either discrete tokens or continuous embeddings. In existing studies, these prompting methods are typically independent of the inputs, lacking sufficient consideration of input semantics. To address this issue, we propose a novel continuous prompting approach, called context-tuning, to fine-tuning PLMs for natural language generation. Firstly, the prompts are derived based on the input text to elicit useful knowledge from PLMs for generation. We refer to such prompts as contextualized prompts. Secondly, we use continuous inverse prompting to improve the process of natural language generation by modeling an inverse generation process from output to input, making the generated text more relevant to the inputs. Furthermore, we utilize a lightweight context-tuning method that fine-tunes only 0.12% of the parameters while maintaining good performance. Our code is publicly available at https://github.com/RUCAIBox/Context-Tuning.",
        "author": "Tianyi Tang; Junyi Li; Wayne Xin Zhao; Ji-Rong Wen",
        "authorids": "/t/tianyi-tang/; /j/junyi-li/; /w/wayne-xin-zhao/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{tang-etal-2022-context,\n    title = \"Context-Tuning: Learning Contextualized Prompts for Natural Language Generation\",\n    author = \"Tang, Tianyi  and\n      Li, Junyi  and\n      Zhao, Wayne Xin  and\n      Wen, Ji-Rong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.552/\",\n    pages = \"6340--6354\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.552.pdf",
        "site": "https://aclanthology.org/2022.coling-1.552/",
        "pdf_size": 636125,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1741584370209153596&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods + Beijing Academy of Artificial Intelligence; School of Information, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods + Beijing Academy of Artificial Intelligence; DIRO, Universit\u00e9 de Montr\u00e9al + Beijing Key Laboratory of Big Data Management and Analysis Methods + Beijing Academy of Artificial Intelligence; Gaoling School of Artificial Intelligence, Renmin University of China + School of Information, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods + Beijing Academy of Artificial Intelligence",
        "aff_domain": "outlook.com;ruc.edu.cn;gmail.com; ",
        "email": "outlook.com;ruc.edu.cn;gmail.com; ",
        "github": "https://github.com/RUCAIBox/Context-Tuning",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+1+2;3+1+2;0+0+1+2",
        "aff_unique_norm": "Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods;Beijing Academy of Artificial Intelligence;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Big Data Management and Analysis;;DIRO",
        "aff_unique_url": "http://www.ruc.edu.cn;;https://www.baaic.cn;https://www.umontreal.ca",
        "aff_unique_abbr": "RUC;;BAAI;UdeM",
        "aff_campus_unique_index": "0;;2;0",
        "aff_campus_unique": "Beijing;;Montr\u00e9al",
        "aff_country_unique_index": "0+0+0;0+0+0;1+0+0;0+0+0+0",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2022.coling-1.26",
        "title": "Continual Few-shot Intent Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Intent detection is at the core of task-oriented dialogue systems. Existing intent detection systems are typically trained with a large amount of data over a predefined set of intent classes. However, newly emerged intents in multiple domains are commonplace in the real world. And it is time-consuming and impractical for dialogue systems to re-collect enough annotated data and re-train the model. These limitations call for an intent detection system that could continually recognize new intents with very few labeled examples. In this work, we study the Continual Few-shot Intent Detection (CFID) problem and construct a benchmark consisting of nine tasks with multiple domains and imbalanced classes. To address the key challenges of (a) catastrophic forgetting during continuous learning and (b) negative knowledge transfer across tasks, we propose the Prefix-guided Lightweight Encoder (PLE) with three auxiliary strategies, namely Pseudo Samples Replay (PSR), Teacher Knowledge Transfer (TKT) and Dynamic Weighting Replay (DWR). Extensive experiments demonstrate the effectiveness and efficiency of our method in preventing catastrophic forgetting and encouraging positive knowledge transfer across tasks.",
        "author": "Guodun Li; Yuchen Zhai; Qianglong Chen; Xing Gao; Ji Zhang; Yin Zhang",
        "authorids": "/g/guodun-li/; /y/yuchen-zhai/; /q/qianglong-chen/; /x/xing-gao/; /j/ji-zhang/; /y/yin-zhang/",
        "bibtex": "@inproceedings{li-etal-2022-continual,\n    title = \"Continual Few-shot Intent Detection\",\n    author = \"Li, Guodun  and\n      Zhai, Yuchen  and\n      Chen, Qianglong  and\n      Gao, Xing  and\n      Zhang, Ji  and\n      Zhang, Yin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.26/\",\n    pages = \"333--343\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.26.pdf",
        "site": "https://aclanthology.org/2022.coling-1.26/",
        "pdf_size": 623035,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13136508741522761204&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; College of Computer Science and Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;alibaba-inc.com;alibaba-inc.com;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;alibaba-inc.com;alibaba-inc.com;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "Zhejiang University;Alibaba Group",
        "aff_unique_dep": "College of Computer Science and Technology;DAMO Academy",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "ZJU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.268",
        "title": "Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Since open social platforms allow for a large and continuous flow of unverified information, rumors can emerge unexpectedly and spread quickly. However, existing rumor detection (RD) models often assume the same training and testing distributions and can not cope with the continuously changing social network environment. This paper proposed a Continual Prompt-Tuning RD (CPT-RD) framework, which avoids catastrophic forgetting (CF) of upstream tasks during sequential task learning and enables bidirectional knowledge transfer between domain tasks. Specifically, we propose the following strategies: (a) Our design explicitly decouples shared and domain-specific knowledge, thus reducing the interference among different domains during optimization; (b) Several technologies aim to transfer knowledge of upstream tasks to deal with emergencies; (c) A task-conditioned prompt-wise hypernetwork (TPHNet) is used to consolidate past domains. In addition, CPT-RD avoids CF without the necessity of a rehearsal buffer. Finally, CPT-RD is evaluated on English and Chinese RD datasets and is effective and efficient compared to prior state-of-the-art methods.",
        "author": "Yuhui Zuo; Wei Zhu; Guoyong GUET Cai",
        "authorids": "/y/yuhui-zuo/; /w/wei-zhu/; /g/guoyong-guet-cai/",
        "bibtex": "https://aclanthology.org/2022.coling-1.268.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.268.pdf",
        "site": "https://aclanthology.org/2022.coling-1.268/",
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17101126408706595778&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.554",
        "title": "Continuous Decomposition of Granularity for Neural Paraphrase Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While Transformers have had significant success in paragraph generation, they treat sentences as linear sequences of tokens and often neglect their hierarchical information. Prior work has shown that decomposing the levels of granularity (e.g., word, phrase, or sentence) for input tokens has produced substantial improvements, suggesting the possibility of enhancing Transformers via more fine-grained modeling of granularity. In this work, we present continuous decomposition of granularity for neural paraphrase generation (C-DNPG): an advanced extension of multi-head self-attention with: 1) a granularity head that automatically infers the hierarchical structure of a sentence by neurally estimating the granularity level of each input token; and 2) two novel attention masks, namely, granularity resonance and granularity scope, to efficiently encode granularity into attention. Experiments on two benchmarks, including Quora question pairs and Twitter URLs have shown that C-DNPG outperforms baseline models by a significant margin. Qualitative analysis reveals that C-DNPG indeed captures fine-grained levels of granularity with effectiveness.",
        "author": "Xiaodong Gu; Zhaowei Zhang; Sang-Woo Lee; Kang Min Yoo; Jung-Woo Ha",
        "authorids": "/x/xiaodong-gu/; /z/zhaowei-zhang/; /s/sang-woo-lee/; /k/kang-min-yoo/; /j/jung-woo-ha/",
        "bibtex": "@inproceedings{gu-etal-2022-continuous,\n    title = \"Continuous Decomposition of Granularity for Neural Paraphrase Generation\",\n    author = \"Gu, Xiaodong  and\n      Zhang, Zhaowei  and\n      Lee, Sang-Woo  and\n      Yoo, Kang Min  and\n      Ha, Jung-Woo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.554/\",\n    pages = \"6369--6378\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.554.pdf",
        "site": "https://aclanthology.org/2022.coling-1.554/",
        "pdf_size": 537716,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1104933693318729757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Software, Shanghai Jiao Tong University; School of Software, Shanghai Jiao Tong University; NA VER AI Lab; NA VER AI Lab; NA VER AI Lab",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;navercorp.com;navercorp.com;navercorp.com",
        "email": "sjtu.edu.cn;sjtu.edu.cn;navercorp.com;navercorp.com;navercorp.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;NAVER Corporation",
        "aff_unique_dep": "School of Software;AI Lab",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.naver.com",
        "aff_unique_abbr": "SJTU;NAVER",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;1;1;1",
        "aff_country_unique": "China;South Korea"
    },
    {
        "id": "2022.coling-1.354",
        "title": "Contrast Sets for Stativity of English Verbs in Context",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "For the task of classifying verbs in context as dynamic or stative, current models approach human performance, but only for particular data sets. To better understand the performance of such models, and how well they are able to generalize beyond particular test sets, we apply the contrast set (Gardner et al., 2020) methodology to stativity classification. We create nearly 300 contrastive pairs by perturbing test set instances just enough to change their labels from one class to the other, while preserving coherence, meaning, and well-formedness. Contrastive evaluation shows that a model with near-human performance on an in-distribution test set degrades substantially when applied to transformed examples, showing that the stative vs. dynamic classification task is more complex than the model performance might otherwise suggest. Code and data are freely available.",
        "author": "Daniel Chen; Alexis Palmer",
        "authorids": "/d/daniel-chen/; /a/alexis-palmer/",
        "bibtex": "@inproceedings{chen-palmer-2022-contrast,\n    title = \"Contrast Sets for Stativity of {E}nglish Verbs in Context\",\n    author = \"Chen, Daniel  and\n      Palmer, Alexis\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.354/\",\n    pages = \"4028--4036\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.354.pdf",
        "site": "https://aclanthology.org/2022.coling-1.354/",
        "pdf_size": 202733,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13719203418083344970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Linguistics, University of Colorado; Department of Linguistics, University of Colorado",
        "aff_domain": "colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu",
        "github": "https://github.com/dchensta/se_contrast",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.140",
        "title": "Conversational QA Dataset Generation with Answer Revision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational question-answer generation is a task that automatically generates a large-scale conversational question answering dataset based on input passages. In this paper, we introduce a novel framework that extracts question-worthy phrases from a passage and then generates corresponding questions considering previous conversations. In particular, our framework revises the extracted answers after generating questions so that answers exactly match paired questions. Experimental results show that our simple answer revision approach leads to significant improvement in the quality of synthetic data. Moreover, we prove that our framework can be effectively utilized for domain adaptation of conversational question answering.",
        "author": "Seonjeong Hwang; Gary Geunbae Lee",
        "authorids": "/s/seonjeong-hwang/; /g/gary-geunbae-lee/",
        "bibtex": "@inproceedings{hwang-lee-2022-conversational,\n    title = \"Conversational {QA} Dataset Generation with Answer Revision\",\n    author = \"Hwang, Seonjeong  and\n      Lee, Gary Geunbae\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.140/\",\n    pages = \"1636--1644\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.140.pdf",
        "site": "https://aclanthology.org/2022.coling-1.140/",
        "pdf_size": 477640,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14500670726789810095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Graduate School of Artificial Intelligence, POSTECH, Pohang, South Korea; Computer Science and Engineering, Graduate School of Artificial Intelligence, POSTECH, Pohang, South Korea",
        "aff_domain": "postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Graduate School of Artificial Intelligence",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pohang",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.517",
        "title": "Coordination Generation via Synchronized Text-Infilling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generating synthetic data for supervised learning from large-scale pre-trained language models has enhanced performances across several NLP tasks, especially in low-resource scenarios. In particular, many studies of data augmentation employ masked language models to replace words with other words in a sentence. However, most of them are evaluated on sentence classification tasks and cannot immediately be applied to tasks related to the sentence structure. In this paper, we propose a simple yet effective approach to generating sentences with a coordinate structure in which the boundaries of its conjuncts are explicitly specified. For a given span in a sentence, our method embeds a mask with a coordinating conjunction in two ways (\u201dX and [mask]\u201d, \u201d[mask] and X\u201d) and forces masked language models to fill the two blanks with an identical text. To achieve this, we introduce decoding methods for BERT and T5 models with the constraint that predictions for different masks are synchronized. Furthermore, we develop a training framework that effectively selects synthetic examples for the supervised coordination disambiguation task. We demonstrate that our method produces promising coordination instances that provide gains for the task in low-resource settings.",
        "author": "Hiroki Teranishi; Yuji Matsumoto",
        "authorids": "/h/hiroki-teranishi/; /y/yuji-matsumoto/",
        "bibtex": "@inproceedings{teranishi-matsumoto-2022-coordination,\n    title = \"Coordination Generation via Synchronized Text-Infilling\",\n    author = \"Teranishi, Hiroki  and\n      Matsumoto, Yuji\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.517/\",\n    pages = \"5914--5924\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.517.pdf",
        "site": "https://aclanthology.org/2022.coling-1.517/",
        "pdf_size": 540257,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:c72h97JtrekJ:scholar.google.com/&scioq=Coordination+Generation+via+Synchronized+Text-Infilling&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "RIKEN Center for Advanced Intelligence Project; RIKEN Center for Advanced Intelligence Project",
        "aff_domain": "riken.jp;riken.jp",
        "email": "riken.jp;riken.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "RIKEN",
        "aff_unique_dep": "Center for Advanced Intelligence Project",
        "aff_unique_url": "https://www.riken.jp/en/",
        "aff_unique_abbr": "RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.38",
        "title": "CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge-grounded dialog systems need to incorporate smooth transitions among knowledge selected for generating responses, to ensure that dialog flows naturally. For document-grounded dialog systems, the inter- and intra-document knowledge relations can be used to model such conversational flows. We develop a novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture the inter-document relationships based on commonsense and similarity and the intra-document co-referential structures of knowledge segments within the grounding documents. We propose CorefDiffs, a Co-referential and Differential flow management method, to linearize the static Coref-MDG into conversational sequence logic. CorefDiffs performs knowledge selection by accounting for contextual graph structures and the knowledge difference sequences. CorefDiffs significantly outperforms the state-of-the-art by 9.5%, 7.4% and 8.2% on three public benchmarks. This demonstrates that the effective modeling of co-reference and knowledge difference for dialog flows are critical for transitions in document-grounded conversation.",
        "author": "Lin Xu; Qixian Zhou; Jinlan Fu; Min-Yen Kan; See-Kiong Ng",
        "authorids": "/l/lin-xu/; /q/qixian-zhou/; /j/jinlan-fu/; /m/min-yen-kan/; /s/see-kiong-ng/",
        "bibtex": "https://aclanthology.org/2022.coling-1.38.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.38.pdf",
        "site": "https://aclanthology.org/2022.coling-1.38/",
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13410662860722613653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2022.coling-1.469",
        "title": "Cross-lingual Feature Extraction from Monolingual Corpora for Low-resource Unsupervised Bilingual Lexicon Induction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite their progress in high-resource language settings, unsupervised bilingual lexicon induction (UBLI) models often fail on corpora with low-resource distant language pairs due to insufficient initialization. In this work, we propose a cross-lingual feature extraction (CFE) method to learn the cross-lingual features from monolingual corpora for low-resource UBLI, enabling representations of words with the same meaning leveraged by the initialization step. By integrating cross-lingual representations with pre-trained word embeddings in a fully unsupervised initialization on UBLI, the proposed method outperforms existing state-of-the-art methods on low-resource language pairs (EN-VI, EN-TH, EN-ZH, EN-JA). The ablation study also proves that the learned cross-lingual features can enhance the representational ability and robustness of the existing embedding model.",
        "author": "Zihao Feng; Hailong Cao; Tiejun Zhao; Weixuan Wang; Wei Peng",
        "authorids": "/z/zihao-feng/; /h/hailong-cao/; /t/tiejun-zhao/; /w/weixuan-wang/; /w/wei-peng/",
        "bibtex": "@inproceedings{feng-etal-2022-cross,\n    title = \"Cross-lingual Feature Extraction from Monolingual Corpora for Low-resource Unsupervised Bilingual Lexicon Induction\",\n    author = \"Feng, Zihao  and\n      Cao, Hailong  and\n      Zhao, Tiejun  and\n      Wang, Weixuan  and\n      Peng, Wei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.469/\",\n    pages = \"5278--5287\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.469.pdf",
        "site": "https://aclanthology.org/2022.coling-1.469/",
        "pdf_size": 462090,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15488783503051336015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Faculty of Computing, Harbin Institute of Technology; Faculty of Computing, Harbin Institute of Technology + Artificial Intelligence Application Research Center, Huawei Technologies Co., Ltd; Faculty of Computing, Harbin Institute of Technology; Artificial Intelligence Application Research Center, Huawei Technologies Co., Ltd; Artificial Intelligence Application Research Center, Huawei Technologies Co., Ltd",
        "aff_domain": "outlook.com;hit.edu.cn;hit.edu.cn;huawei.com;huawei.com",
        "email": "outlook.com;hit.edu.cn;hit.edu.cn;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;1;1",
        "aff_unique_norm": "Harbin Institute of Technology;Huawei Technologies Co., Ltd",
        "aff_unique_dep": "Faculty of Computing;Artificial Intelligence Application Research Center",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.huawei.com",
        "aff_unique_abbr": "HIT;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.210",
        "title": "Cross-modal Contrastive Attention Model for Medical Report Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Medical report automatic generation has gained increasing interest recently as a way to help radiologists write reports more efficiently. However, this image-to-text task is rather challenging due to the typical data biases: 1) Normal physiological structures dominate the images, with only tiny abnormalities; 2) Normal descriptions accordingly dominate the reports. Existing methods have attempted to solve these problems, but they neglect to exploit useful information from similar historical cases. In this paper, we propose a novel Cross-modal Contrastive Attention (CMCA) model to capture both visual and semantic information from similar cases, with mainly two modules: a Visual Contrastive Attention Module for refining the unique abnormal regions compared to the retrieved case images; a Cross-modal Attention Module for matching the positive semantic information from the case reports. Extensive experiments on two widely-used benchmarks, IU X-Ray and MIMIC-CXR, demonstrate that the proposed model outperforms the state-of-the-art methods on almost all metrics. Further analyses also validate that our proposed model is able to improve the reports with more accurate abnormal findings and richer descriptions.",
        "author": "Xiao Song; Xiaodan Zhang; Junzhong Ji; Ying Liu; Pengxu Wei",
        "authorids": "/x/xiao-song/; /x/xiaodan-zhang/; /j/junzhong-ji/; /y/ying-liu/; /p/pengxu-wei/",
        "bibtex": "@inproceedings{song-etal-2022-cross,\n    title = \"Cross-modal Contrastive Attention Model for Medical Report Generation\",\n    author = \"Song, Xiao  and\n      Zhang, Xiaodan  and\n      Ji, Junzhong  and\n      Liu, Ying  and\n      Wei, Pengxu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.210/\",\n    pages = \"2388--2397\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.210.pdf",
        "site": "https://aclanthology.org/2022.coling-1.210/",
        "pdf_size": 9218367,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11706257864966526709&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing University of Technology; Beijing University of Technology; Beijing University of Technology+Peking University Third Hospital; Peking University Third Hospital; Sun Yat-sen University",
        "aff_domain": "emails.bjut.edu.cn;bjut.edu.cn;bjut.edu.cn;163.com;mail.sysu.edu.cn",
        "email": "emails.bjut.edu.cn;bjut.edu.cn;bjut.edu.cn;163.com;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1;1;2",
        "aff_unique_norm": "Beijing University of Technology;Peking University Third Hospital;Sun Yat-sen University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.bjut.edu.cn;http://www.puh3.net.cn;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "BJUT;;SYSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.293",
        "title": "Curating a Large-Scale Motivational Interviewing Dataset Using Peer Support Forums",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A significant limitation in developing therapeutic chatbots to support people going through psychological distress is the lack of high-quality, large-scale datasets capturing conversations between clients and trained counselors. As a remedy, researchers have focused their attention on scraping conversational data from peer support platforms such as Reddit. But the extent to which the responses from peers align with responses from trained counselors is understudied. We address this gap by analyzing the differences between responses from counselors and peers by getting trained counselors to annotate \u224817K such responses using Motivational Interviewing Treatment Integrity (MITI) code, a well-established behavioral coding system that differentiates between favorable and unfavorable responses. We developed an annotation pipeline with several stages of quality control. Due to its design, this method was able to achieve 97% of coverage, meaning that out of the 17.3K responses we successfully labeled 16.8K with a moderate agreement. We use this data to conclude the extent to which conversational data from peer support platforms align with real therapeutic conversations and discuss in what ways they can be exploited to train therapeutic chatbots.",
        "author": "Anuradha Welivita; Pearl Pu",
        "authorids": "/a/anuradha-welivita/; /p/pearl-pu/",
        "bibtex": "@inproceedings{welivita-pu-2022-curating,\n    title = \"Curating a Large-Scale Motivational Interviewing Dataset Using Peer Support Forums\",\n    author = \"Welivita, Anuradha  and\n      Pu, Pearl\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.293/\",\n    pages = \"3315--3330\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.293.pdf",
        "site": "https://aclanthology.org/2022.coling-1.293/",
        "pdf_size": 6237739,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14614760879257425196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer and Communication Sciences, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Switzerland; School of Computer and Communication Sciences, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": "School of Computer and Communication Sciences",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.coling-1.141",
        "title": "DABERT: Dual Attention Enhanced BERT for Semantic Matching",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiveness of our proposed method.",
        "author": "Sirui Wang; Di Liang; Jian Song; Yuntao Li; Wei Wu",
        "authorids": "/s/sirui-wang/; /d/di-liang/; /j/jian-song/; /y/yuntao-li/; /w/wei-wu/",
        "bibtex": "@inproceedings{wang-etal-2022-dabert,\n    title = \"{DABERT}: Dual Attention Enhanced {BERT} for Semantic Matching\",\n    author = \"Wang, Sirui  and\n      Liang, Di  and\n      Song, Jian  and\n      Li, Yuntao  and\n      Wu, Wei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.141/\",\n    pages = \"1645--1654\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.141.pdf",
        "site": "https://aclanthology.org/2022.coling-1.141/",
        "pdf_size": 703758,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4617192645031776609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tsinghua University, Beijing, China* + Meituan Inc.,Beijing, China\u2020; Tsinghua University, Beijing, China* + Meituan Inc.,Beijing, China\u2020; Meituan Inc.,Beijing, China\u2020; Meituan Inc.,Beijing, China\u2020; Meituan Inc.,Beijing, China\u2020",
        "aff_domain": "meituan.com;meituan.com;meituan.com;meituan.com;meituan.com",
        "email": "meituan.com;meituan.com;meituan.com;meituan.com;meituan.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;1;1",
        "aff_unique_norm": "Tsinghua University;Meituan Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.meituan.com",
        "aff_unique_abbr": "THU;Meituan",
        "aff_campus_unique_index": "0+0;0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.182",
        "title": "DCT-Centered Temporal Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most previous work on temporal relation extraction only focused on extracting the temporal relations among events or suffered from the issue of different expressions of events, timexes and Document Creation Time (DCT). Moreover, DCT can act as a hub to semantically connect the other events and timexes in a document. Unfortunately, previous work cannot benefit from such critical information. To address the above issues, we propose a unified DCT-centered Temporal Relation Extraction model DTRE to identify the relations among events, timexes and DCT. Specifically, sentence-style DCT representation is introduced to address the first issue and unify event expressions, timexes and DCT. Then, a DCT-aware graph is applied to obtain their contextual structural representations. Furthermore, a DCT-anchoring multi-task learning framework is proposed to jointly predict three types of temporal relations in a batch. Finally, we apply a DCT-guided global inference to further enhance the global consistency among different relations. Experimental results on three datasets show that our DTRE outperforms several SOTA baselines on E-E, E-T and E-D significantly.",
        "author": "Liang Wang; Peifeng Li; Sheng Xu",
        "authorids": "/l/liang-wang/; /p/peifeng-li/; /s/sheng-xu/",
        "bibtex": "@inproceedings{wang-etal-2022-dct,\n    title = \"{DCT}-Centered Temporal Relation Extraction\",\n    author = \"Wang, Liang  and\n      Li, Peifeng  and\n      Xu, Sheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.182/\",\n    pages = \"2087--2097\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.182.pdf",
        "site": "https://aclanthology.org/2022.coling-1.182/",
        "pdf_size": 476695,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=161699184126161561&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology Soochow University, Jiangsu, China; School of Computer Science and Technology Soochow University, Jiangsu, China; School of Computer Science and Technology Soochow University, Jiangsu, China",
        "aff_domain": "vip.qq.com;suda.edu.cn;stu.suda.edu.cn",
        "email": "vip.qq.com;suda.edu.cn;stu.suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.219",
        "title": "DESED: Dialogue-based Explanation for Sentence-level Event Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Many recent sentence-level event detection efforts focus on enriching sentence semantics, e.g., via multi-task or prompt-based learning. Despite the promising performance, these methods commonly depend on label-extensive manual annotations or require domain expertise to design sophisticated templates and rules. This paper proposes a new paradigm, named dialogue-based explanation, to enhance sentence semantics for event detection. By saying dialogue-based explanation of an event, we mean explaining it through a consistent information-intensive dialogue, with the original event description as the start utterance. We propose three simple dialogue generation methods, whose outputs are then fed into a hybrid attention mechanism to characterize the complementary event semantics. Extensive experimental results on two event detection datasets verify the effectiveness of our method and suggest promising research opportunities in the dialogue-based explanation paradigm.",
        "author": "Yinyi Wei; Shuaipeng Liu; Jianwei Lv; Xiangyu Xi; Hailei Yan; Wei Ye; Tong Mo; Fan Yang; Guanglu Wan",
        "authorids": "/y/yinyi-wei/; /s/shuaipeng-liu/; /j/jianwei-lv/; /x/xiangyu-xi/; /h/hailei-yan/; /w/wei-ye/; /t/tong-mo/; /f/fan-yang/; /g/guanglu-wan/",
        "bibtex": "@inproceedings{wei-etal-2022-desed,\n    title = \"{DESED}: Dialogue-based Explanation for Sentence-level Event Detection\",\n    author = \"Wei, Yinyi  and\n      Liu, Shuaipeng  and\n      Lv, Jianwei  and\n      Xi, Xiangyu  and\n      Yan, Hailei  and\n      Ye, Wei  and\n      Mo, Tong  and\n      Yang, Fan  and\n      Wan, Guanglu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.219/\",\n    pages = \"2483--2493\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.219.pdf",
        "site": "https://aclanthology.org/2022.coling-1.219/",
        "pdf_size": 1109588,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7646620644055024367&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Peking University; Meituan Group, Beijing, China; Meituan Group, Beijing, China; Meituan Group, Beijing, China; Meituan Group, Beijing, China; National Engineering Research Center for Software Engineering, Peking University; Peking University; Meituan Group, Beijing, China; Meituan Group, Beijing, China",
        "aff_domain": "pku.edu.cn;meituan.com; ; ; ;pku.edu.cn; ; ; ",
        "email": "pku.edu.cn;meituan.com; ; ; ;pku.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;0;0;1;1",
        "aff_unique_norm": "Peking University;Meituan Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.meituan.com",
        "aff_unique_abbr": "Peking U;Meituan",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.551",
        "title": "DISK: Domain-constrained Instance Sketch for Math Word Problem Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A math word problem (MWP) is a coherent narrative which reflects the underlying logic of math equations. Successful MWP generation can automate the writing of mathematics questions. Previous methods mainly generate MWP text based on inflexible pre-defined templates. In this paper, we propose a neural model for generating MWP text from math equations. Firstly, we incorporate a matching model conditioned on the domain knowledge to retrieve a MWP instance which is most consistent with the ground-truth, where the domain is a latent variable extracted with a domain summarizer. Secondly, by constructing a Quantity Cell Graph (QCG) from the retrieved MWP instance and reasoning over it, we improve the model\u2019s comprehension of real-world scenarios and derive a domain-constrained instance sketch to guide the generation. Besides, the QCG also interacts with the equation encoder to enhance the alignment between math tokens (e.g., quantities and variables) and MWP text. Experiments and empirical analysis on educational MWP set show that our model achieves impressive performance in both automatic evaluation metrics and human evaluation metrics.",
        "author": "Tianyang Cao; Shuang Zeng; Xiaodan Xu; Mairgup Mansur; Baobao Chang",
        "authorids": "/t/tianyang-cao/; /s/shuang-zeng/; /x/xiaodan-xu/; /m/mairgup-mansur/; /b/baobao-chang/",
        "bibtex": "@inproceedings{cao-etal-2022-disk,\n    title = \"{DISK}: Domain-constrained Instance Sketch for Math Word Problem Generation\",\n    author = \"Cao, Tianyang  and\n      Zeng, Shuang  and\n      Xu, Xiaodan  and\n      Mansur, Mairgup  and\n      Chang, Baobao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.551/\",\n    pages = \"6327--6339\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.551.pdf",
        "site": "https://aclanthology.org/2022.coling-1.551/",
        "pdf_size": 642555,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15924839218602862906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Key Laboratory of Computational Linguistics, MOE, Peking University + Tencent Inc.; Key Laboratory of Computational Linguistics, MOE, Peking University + School of Software and Microelectronics, Peking University; Key Laboratory of Computational Linguistics, MOE, Peking University; Tencent Inc.; Key Laboratory of Computational Linguistics, MOE, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;tencent.com;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;tencent.com;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+0;0;1;0",
        "aff_unique_norm": "Peking University;Tencent",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "PKU;Tencent",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.258",
        "title": "DP-Rewrite: Towards Reproducibility and Transparency in Differentially Private Text Rewriting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text rewriting with differential privacy (DP) provides concrete theoretical guarantees for protecting the privacy of individuals in textual documents. In practice, existing systems may lack the means to validate their privacy-preserving claims, leading to problems of transparency and reproducibility. We introduce DP-Rewrite, an open-source framework for differentially private text rewriting which aims to solve these problems by being modular, extensible, and highly customizable. Our system incorporates a variety of downstream datasets, models, pre-training procedures, and evaluation metrics to provide a flexible way to lead and validate private text rewriting research. To demonstrate our software in practice, we provide a set of experiments as a case study on the ADePT DP text rewriting system, detecting a privacy leak in its pre-training approach. Our system is publicly available, and we hope that it will help the community to make DP text rewriting research more accessible and transparent.",
        "author": "Timour Igamberdiev; Thomas Arnold; Ivan Habernal",
        "authorids": "/t/timour-igamberdiev/; /t/thomas-arnold/; /i/ivan-habernal/",
        "bibtex": "@inproceedings{igamberdiev-etal-2022-dp,\n    title = \"{DP}-Rewrite: Towards Reproducibility and Transparency in Differentially Private Text Rewriting\",\n    author = \"Igamberdiev, Timour  and\n      Arnold, Thomas  and\n      Habernal, Ivan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.258/\",\n    pages = \"2927--2933\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.258.pdf",
        "site": "https://aclanthology.org/2022.coling-1.258/",
        "pdf_size": 442543,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12103111982063109631&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Trustworthy Human Language Technologies; Ubiquitous Knowledge Processing Lab; Trustworthy Human Language Technologies + Ubiquitous Knowledge Processing Lab",
        "aff_domain": "tu-darmstadt.de;ukp.informatik.tu-darmstadt.de;tu-darmstadt.de",
        "email": "tu-darmstadt.de;ukp.informatik.tu-darmstadt.de;tu-darmstadt.de",
        "github": "https://github.com/trusthlt/dp-rewrite",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Trustworthy Human Language Technologies;University of Cambridge",
        "aff_unique_dep": ";Computer Laboratory",
        "aff_unique_url": ";https://www.cl.cam.ac.uk",
        "aff_unique_abbr": ";UKP Lab",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "2022.coling-1.103",
        "title": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Deep prompt tuning (DPT) has gained great success in most natural language processing (NLP) tasks. However, it is not well-investigated in dense retrieval where fine-tuning (FT) still dominates. When deploying multiple retrieval tasks using the same backbone model (e.g., RoBERTa), FT-based methods are unfriendly in terms of deployment cost: each new retrieval model needs to repeatedly deploy the backbone model without reuse. To reduce the deployment cost in such a scenario, this work investigates applying DPT in dense retrieval. The challenge is that directly applying DPT in dense retrieval largely underperforms FT methods. To compensate for the performance drop, we propose two model-agnostic and task-agnostic strategies for DPT-based retrievers, namely retrieval-oriented intermediate pretraining and unified negative mining, as a general approach that could be compatible with any pre-trained language model and retrieval task. The experimental results show that the proposed method (called DPTDR) outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct ablation studies to examine the effectiveness of each strategy in DPTDR. We believe this work facilitates the industry, as it saves enormous efforts and costs of deployment and increases the utility of computing resources. Our code is available at https://github.com/tangzhy/DPTDR.",
        "author": "Zhengyang Tang; Benyou Wang; Ting Yao",
        "authorids": "/z/zhengyang-tang/; /b/benyou-wang/; /t/ting-yao/",
        "bibtex": "@inproceedings{tang-etal-2022-dptdr,\n    title = \"{DPTDR}: Deep Prompt Tuning for Dense Passage Retrieval\",\n    author = \"Tang, Zhengyang  and\n      Wang, Benyou  and\n      Yao, Ting\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.103/\",\n    pages = \"1193--1202\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.103.pdf",
        "site": "https://aclanthology.org/2022.coling-1.103/",
        "pdf_size": 696194,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17873044771923498963&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent Inc. + The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen; Tencent Inc.",
        "aff_domain": "tencent.com;cuhk.edu.cn;tencent.com",
        "email": "tencent.com;cuhk.edu.cn;tencent.com",
        "github": "https://github.com/tangzhy/DPTDR",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "Tencent;The Chinese University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tencent.com;https://www.cuhk.edu.cn",
        "aff_unique_abbr": "Tencent;CUHK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.186",
        "title": "DRK: Discriminative Rule-based Knowledge for Relieving Prediction Confusions in Few-shot Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot relation extraction aims to identify the relation type between entities in a given text in the low-resource scenario. Albeit much progress, existing meta-learning methods still fall into prediction confusions owing to the limited inference ability over shallow text features. To relieve these confusions, this paper proposes a discriminative rule-based knowledge (DRK) method. Specifically, DRK adopts a logic-aware inference module to ease the word-overlap confusion, which introduces a logic rule to constrain the inference process, thereby avoiding the adverse effect of shallow text features. Also, DRK employs a discrimination finding module to alleviate the entity-type confusion, which explores distinguishable text features via a hierarchical contrastive learning. We conduct extensive experiments on four types of meta tasks and the results show promising improvements from DRK (6.0% accuracy gains on average). Besides, error analyses reveal the word-overlap and entity-type errors are the main courses of mispredictions in few-shot relation extraction.",
        "author": "Mengru Wang; Jianming Zheng; Fei Cai; Taihua Shao; Honghui Chen",
        "authorids": "/m/mengru-wang/; /j/jianming-zheng/; /f/fei-cai/; /t/taihua-shao/; /h/honghui-chen/",
        "bibtex": "@inproceedings{wang-etal-2022-drk,\n    title = \"{DRK}: Discriminative Rule-based Knowledge for Relieving Prediction Confusions in Few-shot Relation Extraction\",\n    author = \"Wang, Mengru  and\n      Zheng, Jianming  and\n      Cai, Fei  and\n      Shao, Taihua  and\n      Chen, Honghui\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.186/\",\n    pages = \"2129--2140\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.186.pdf",
        "site": "https://aclanthology.org/2022.coling-1.186/",
        "pdf_size": 604505,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7028319147803634427&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "Science and Technology on Information Systems Engineering Laboratory",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.220",
        "title": "Data Augmentation for Few-Shot Knowledge Graph Completion from Hierarchical Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot knowledge graph completion (FKGC) has become a new research focus in the field of knowledge graphs in recent years, which aims to predict the missing links for relations that only have a few associative triples. Existing models attempt to solve the problem via learning entity and relation representations. However, the limited training data severely hinders the performance of existing models. To this end, we propose to solve the FKGC problem with the data augmentation technique. Specifically, we perform data augmentation from two perspectives, i.e., inter-task view and intra-task view. The former generates new tasks for FKGC, while the latter enriches the support or query set for an individual task. It is worth noting that the proposed framework can be applied to a number of existing FKGC models. Experimental evaluation on two public datasets indicates our model is capable of achieving substantial improvements over baselines.",
        "author": "Yuanzhou Yao; Zhao Zhang; Yongjun Xu; Chao Li",
        "authorids": "/y/yuanzhou-yao/; /z/zhao-zhang/; /y/yongjun-xu/; /c/chao-li/",
        "bibtex": "@inproceedings{yao-etal-2022-data,\n    title = \"Data Augmentation for Few-Shot Knowledge Graph Completion from Hierarchical Perspective\",\n    author = \"Yao, Yuanzhou  and\n      Zhang, Zhao  and\n      Xu, Yongjun  and\n      Li, Chao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.220/\",\n    pages = \"2494--2503\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.220.pdf",
        "site": "https://aclanthology.org/2022.coling-1.220/",
        "pdf_size": 818306,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7059151045901098560&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Computeing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; Institute of Computeing Technology, Chinese Academy of Sciences, Beijing, China+Zhejiang Lab, Hangzhou, China; Institute of Computeing Technology, Chinese Academy of Sciences, Beijing, China; Zhejiang Lab, Hangzhou, China+Institute of Computeing Technology, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+2;0;2+0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Zhejiang Lab",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.ict.ac.cn;http://www.ucas.ac.cn;http://www.zhejianglab.com",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+1;0;1+0",
        "aff_campus_unique": "Beijing;Hangzhou",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.111",
        "title": "Debiasing Isn\u2019t Enough! \u2013 on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We study the relationship between task-agnostic intrinsic and task-specific extrinsic social bias evaluation measures for MLMs, and find that there exists only a weak correlation between these two types of evaluation measures. Moreover, we find that MLMs debiased using different methods still re-learn social biases during fine-tuning on downstream tasks. We identify the social biases in both training instances as well as their assigned labels as reasons for the discrepancy between intrinsic and extrinsic bias evaluation measurements. Overall, our findings highlight the limitations of existing MLM bias evaluation measures and raise concerns on the deployment of MLMs in downstream applications using those measures.",
        "author": "Masahiro Kaneko; Danushka Bollegala; Naoaki Okazaki",
        "authorids": "/m/masahiro-kaneko/; /d/danushka-bollegala/; /n/naoaki-okazaki/",
        "bibtex": "@inproceedings{kaneko-etal-2022-debiasing,\n    title = \"Debiasing Isn{'}t Enough! {--} on the Effectiveness of Debiasing {MLM}s and Their Social Biases in Downstream Tasks\",\n    author = \"Kaneko, Masahiro  and\n      Bollegala, Danushka  and\n      Okazaki, Naoaki\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.111/\",\n    pages = \"1299--1310\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.111.pdf",
        "site": "https://aclanthology.org/2022.coling-1.111/",
        "pdf_size": 572051,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12454659297513808313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tokyo Institute of Technology; University of Liverpool+Amazon; Tokyo Institute of Technology",
        "aff_domain": "nlp.c.titech.ac.jp;liverpool.ac.uk;c.titech.ac.jp",
        "email": "nlp.c.titech.ac.jp;liverpool.ac.uk;c.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Tokyo Institute of Technology;University of Liverpool;Amazon.com, Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.liverpool.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "Titech;Liv Uni;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "Japan;United Kingdom;United States"
    },
    {
        "id": "2022.coling-1.110",
        "title": "Debiasing Word Embeddings with Nonlinear Geometry",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Debiasing word embeddings has been largely limited to individual and independent social categories. However, real-world corpora typically present multiple social categories that possibly correlate or intersect with each other. For instance, \u201chair weaves\u201d is stereotypically associated with African American females, but neither African American nor females alone. Therefore, this work studies biases associated with multiple social categories: joint biases induced by the union of different categories and intersectional biases that do not overlap with the biases of the constituent categories. We first empirically observe that individual biases intersect non-trivially (i.e., over a one-dimensional subspace). Drawing from the intersectional theory in social science and the linguistic theory, we then construct an intersectional subspace to debias for multiple social categories using the nonlinear geometry of individual biases. Empirical evaluations corroborate the efficacy of our approach.",
        "author": "Lu Cheng; Nayoung Kim; Huan Liu",
        "authorids": "/l/lu-cheng/; /n/nayoung-kim/; /h/huan-liu/",
        "bibtex": "@inproceedings{cheng-etal-2022-debiasing,\n    title = \"Debiasing Word Embeddings with Nonlinear Geometry\",\n    author = \"Cheng, Lu  and\n      Kim, Nayoung  and\n      Liu, Huan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.110/\",\n    pages = \"1286--1298\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.110.pdf",
        "site": "https://aclanthology.org/2022.coling-1.110/",
        "pdf_size": 701241,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5851949403450564978&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois Chicago; School of Computing and Augmented Intelligence, Arizona State University; School of Computing and Augmented Intelligence, Arizona State University",
        "aff_domain": "uic.edu;asu.edu;asu.edu",
        "email": "uic.edu;asu.edu;asu.edu",
        "github": "https://github.com/GitHubLuCheng/Implementation-of-JoSEC-COLING-22",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Illinois Chicago;Arizona State University",
        "aff_unique_dep": "Department of Computer Science;School of Computing and Augmented Intelligence",
        "aff_unique_url": "https://www.uic.edu;https://www.asu.edu",
        "aff_unique_abbr": "UIC;ASU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Chicago;Tempe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.472",
        "title": "Deciphering and Characterizing Out-of-Vocabulary Words for Morphologically Rich Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a detailed foundational empirical case study of the nature of out-of-vocabulary words encountered in modern text in a moderate-resource language such as Bulgarian, and a multi-faceted distributional analysis of the underlying word-formation processes that can aid in their compositional translation, tagging, parsing, language modeling, and other NLP tasks. Given that out-of-vocabulary (OOV) words generally present a key open challenge to NLP and machine translation systems, especially toward the lower limit of resource availability, there are useful practical insights, as well as corpus-linguistic insights, from both a detailed manual and automatic taxonomic analysis of the types, multidimensional properties, and processing potential for multiple representative OOV data samples.",
        "author": "Georgie Botev; Arya D. McCarthy; Winston Wu; David Yarowsky",
        "authorids": "/g/georgie-botev/; /a/arya-d-mccarthy/; /w/winston-wu/; /d/david-yarowsky/",
        "bibtex": "@inproceedings{botev-etal-2022-deciphering,\n    title = \"Deciphering and Characterizing Out-of-Vocabulary Words for Morphologically Rich Languages\",\n    author = \"Botev, Georgie  and\n      McCarthy, Arya D.  and\n      Wu, Winston  and\n      Yarowsky, David\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.472/\",\n    pages = \"5309--5326\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.472.pdf",
        "site": "https://aclanthology.org/2022.coling-1.472/",
        "pdf_size": 512241,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3317446719269960244&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.199",
        "title": "Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural language understanding (NLU) models tend to rely on spurious correlations (i.e., dataset bias) to achieve high performance on in-distribution datasets but poor performance on out-of-distribution ones. Most of the existing debiasing methods often identify and weaken these samples with biased features (i.e., superficial surface features that cause such spurious correlations). However, down-weighting these samples obstructs the model in learning from the non-biased parts of these samples. To tackle this challenge, in this paper, we propose to eliminate spurious correlations in a fine-grained manner from a feature space perspective. Specifically, we introduce Random Fourier Features and weighted re-sampling to decorrelate the dependencies between features to mitigate spurious correlations. After obtaining decorrelated features, we further design a mutual-information-based method to purify them, which forces the model to learn features that are more relevant to tasks. Extensive experiments on two well-studied NLU tasks demonstrate that our method is superior to other comparative approaches.",
        "author": "Shihan Dou; Rui Zheng; Ting Wu; SongYang Gao; Junjie Shan; Qi Zhang; Yueming Wu; Xuanjing Huang",
        "authorids": "/s/shihan-dou/; /r/rui-zheng/; /t/ting-wu/; /s/songyang-gao/; /j/junjie-shan/; /q/qi-zhang/; /y/yueming-wu/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{dou-etal-2022-decorrelate,\n    title = \"Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective\",\n    author = \"Dou, Shihan  and\n      Zheng, Rui  and\n      Wu, Ting  and\n      Gao, SongYang  and\n      Shan, Junjie  and\n      Zhang, Qi  and\n      Wu, Yueming  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.199/\",\n    pages = \"2278--2287\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.199.pdf",
        "site": "https://aclanthology.org/2022.coling-1.199/",
        "pdf_size": 682507,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11338401909334662391&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; KTH Royal Institute of Technology; School of Computer Science, Fudan University+Shanghai Key Laboratory of Intelligent Information Processing; Nanyang Technological University; School of Computer Science, Fudan University",
        "aff_domain": "m.fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn; ;fudan.edu.cn; ;fudan.edu.cn",
        "email": "m.fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn; ;fudan.edu.cn; ;fudan.edu.cn",
        "github": "https://github.com/Coling2022-DePro/DePro",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;0+2;3;0",
        "aff_unique_norm": "Fudan University;KTH Royal Institute of Technology;Shanghai Key Laboratory of Intelligent Information Processing;Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science;;Intelligent Information Processing;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.kth.se;;https://www.ntu.edu.sg",
        "aff_unique_abbr": "Fudan;KTH;;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0+0;2;0",
        "aff_country_unique": "China;Sweden;Singapore"
    },
    {
        "id": "2022.coling-1.196",
        "title": "Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge Graph Embedding (KGE) has been proposed and successfully utilized to knowledge Graph Completion (KGC). But classic KGE paradigm often fail in unseen relation representations. Previous studies mainly utilize the textual descriptions of relations and its neighbor relations to represent unseen relations. In fact, the semantics of a relation can be expressed by three kinds of graphs: factual graph, ontology graph, textual description graph, and they can complement each other. A more common scenario in the real world is that seen and unseen relations appear at the same time. In this setting, the training set (only seen relations) and testing set (both seen and unseen relations) own different distributions. And the train-test inconsistency problem will make KGE methods easiy overfit on seen relations and under-performance on unseen relations. In this paper, we propose decoupling mixture-of-graph experts (DMoG) for unseen relations learning, which could represent the unseen relations in the factual graph by fusing ontology and textual graphs, and decouple fusing space and reasoning space to alleviate overfitting for seen relations. The experiments on two unseen only public datasets and a mixture dataset verify the effectiveness of the proposed method, which improves the state-of-the-art methods by 6.84% in Hits@10 on average.",
        "author": "Ran Song; Shizhu He; Suncong Zheng; Shengxiang Gao; Kang Liu; Zhengtao Yu; Jun Zhao",
        "authorids": "/r/ran-song/; /s/shizhu-he/; /s/suncong-zheng/; /s/shengxiang-gao/; /k/kang-liu/; /z/zhengtao-yu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{song-etal-2022-decoupling,\n    title = \"Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts\",\n    author = \"Song, Ran  and\n      He, Shizhu  and\n      Zheng, Suncong  and\n      Gao, Shengxiang  and\n      Liu, Kang  and\n      Yu, Zhengtao  and\n      Zhao, Jun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.196/\",\n    pages = \"2237--2246\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.196.pdf",
        "site": "https://aclanthology.org/2022.coling-1.196/",
        "pdf_size": 806382,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14381099897469913953&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Faculty of Information Engineering and Automation, Kunming University of Science and Technology+Yunnan Key Laboratory of Artificial Intelligence, Kunming, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China; Tencent AI Lab; Faculty of Information Engineering and Automation, Kunming University of Science and Technology+Yunnan Key Laboratory of Artificial Intelligence, Kunming, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China+Beijing Academy of Artificial Intelligence, Beijing, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology+Yunnan Key Laboratory of Artificial Intelligence, Kunming, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China",
        "aff_domain": "163.com;nlpr.ia.ac.cn;tencent.com;hotmail.com;nlpr.ia.ac.cn;hotmail.com;163.com",
        "email": "163.com;nlpr.ia.ac.cn;tencent.com;hotmail.com;nlpr.ia.ac.cn;hotmail.com;163.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2+3;4;0+1;2+3+5;0+1;2+3",
        "aff_unique_norm": "Kunming University of Science and Technology;Yunnan Key Laboratory of Artificial Intelligence;Chinese Academy of Sciences;University of Chinese Academy of Science;Tencent;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Faculty of Information Engineering and Automation;;Institute of Automation;School of Artificial Intelligence;Tencent AI Lab;",
        "aff_unique_url": "http://www.kust.edu.cn;;http://www.ia.cas.cn;http://www.ucas.ac.cn;https://ai.tencent.com;https://www.baaic.cn",
        "aff_unique_abbr": ";;CAS;UCAS;Tencent AI Lab;BAAI",
        "aff_campus_unique_index": "0+0;1+1;0+0;1+1+1;0+0;1+1",
        "aff_campus_unique": "Kunming;Beijing;",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.261",
        "title": "DeltaNet: Conditional Medical Report Generation for COVID-19 Diagnosis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Fast screening and diagnosis are critical in COVID-19 patient treatment. In addition to the gold standard RT-PCR, radiological imaging like X-ray and CT also works as an important means in patient screening and follow-up. However, due to the excessive number of patients, writing reports becomes a heavy burden for radiologists. To reduce the workload of radiologists, we propose DeltaNet to generate medical reports automatically. Different from typical image captioning approaches that generate reports with an encoder and a decoder, DeltaNet applies a conditional generation process. In particular, given a medical image, DeltaNet employs three steps to generate a report: 1) first retrieving related medical reports, i.e., the historical reports from the same or similar patients; 2) then comparing retrieved images and current image to find the differences; 3) finally generating a new report to accommodate identified differences based on the conditional report. We evaluate DeltaNet on a COVID-19 dataset, where DeltaNet outperforms state-of-the-art approaches. Besides COVID-19, the proposed DeltaNet can be applied to other diseases as well. We validate its generalization capabilities on the public IU-Xray and MIMIC-CXR datasets for chest-related diseases.",
        "author": "Xian Wu; Shuxin Yang; Zhaopeng Qiu; Shen Ge; Yangtian Yan; Xingwang Wu; Yefeng Zheng; S. Kevin Zhou; Li Xiao",
        "authorids": "/x/xian-wu/; /s/shuxin-yang/; /z/zhaopeng-qiu/; /s/shen-ge/; /y/yangtian-yan/; /x/xingwang-wu/; /y/yefeng-zheng/; /s/s-kevin-zhou/; /l/li-xiao/",
        "bibtex": "@inproceedings{wu-etal-2022-deltanet,\n    title = \"{D}elta{N}et: Conditional Medical Report Generation for {COVID}-19 Diagnosis\",\n    author = \"Wu, Xian  and\n      Yang, Shuxin  and\n      Qiu, Zhaopeng  and\n      Ge, Shen  and\n      Yan, Yangtian  and\n      Wu, Xingwang  and\n      Zheng, Yefeng  and\n      Zhou, S. Kevin  and\n      Xiao, Li\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.261/\",\n    pages = \"2952--2961\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.261.pdf",
        "site": "https://aclanthology.org/2022.coling-1.261/",
        "pdf_size": 406565,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11666637624212213281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tencent Jarvis Lab; Institute of Computing Technology of the Chinese Academy of Sciences+University of Chinese Academy of Sciences; Tencent Jarvis Lab; Tencent Jarvis Lab; Tencent Jarvis Lab; The First Affiliated Hospital of Anhui Medical University; Tencent Jarvis Lab; University of Science and Technology of China; Institute of Computing Technology of the Chinese Academy of Sciences+University of Chinese Academy of Sciences",
        "aff_domain": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;126.com;gmail.com;ict.ac.cn",
        "email": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;126.com;gmail.com;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1+2;0;0;0;3;0;4;1+2",
        "aff_unique_norm": "Tencent;Chinese Academy of Sciences;University of Chinese Academy of Sciences;Anhui Medical University;University of Science and Technology of China",
        "aff_unique_dep": "Jarvis Lab;Institute of Computing Technology;;The First Affiliated Hospital;",
        "aff_unique_url": "https://www.tencent.com;http://www.ict.ac.cn;http://www.ucas.ac.cn;http://www.ahmu.edu.cn/;http://www.ustc.edu.cn",
        "aff_unique_abbr": "Tencent;CAS;UCAS;;USTC",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.573",
        "title": "Demystifying Neural Fake News via Linguistic Feature-Based Interpretation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The spread of fake news can have devastating ramifications, and recent advancements to neural fake news generators have made it challenging to understand how misinformation generated by these models may best be confronted. We conduct a feature-based study to gain an interpretative understanding of the linguistic attributes that neural fake news generators may most successfully exploit. When comparing models trained on subsets of our features and confronting the models with increasingly advanced neural fake news, we find that stylistic features may be the most robust. We discuss our findings, subsequent analyses, and broader implications in the pages within.",
        "author": "Ankit Aich; Souvik Bhattacharya; Natalie Parde",
        "authorids": "/a/ankit-aich/; /s/souvik-bhattacharya/; /n/natalie-parde/",
        "bibtex": "@inproceedings{aich-etal-2022-demystifying,\n    title = \"Demystifying Neural Fake News via Linguistic Feature-Based Interpretation\",\n    author = \"Aich, Ankit  and\n      Bhattacharya, Souvik  and\n      Parde, Natalie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.573/\",\n    pages = \"6586--6599\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.573.pdf",
        "site": "https://aclanthology.org/2022.coling-1.573/",
        "pdf_size": 364993,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17948925569217275661&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Natural Language Processing Laboratory, Department of Computer Science, University of Illinois at Chicago; Natural Language Processing Laboratory, Department of Computer Science, University of Illinois at Chicago; Natural Language Processing Laboratory, Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.532",
        "title": "Denoising Large-Scale Image Captioning from Alt-text Data Using Content Selection Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Training large-scale image captioning (IC) models demands access to a rich and diverse set of training examples that are expensive to curate both in terms of time and man-power. Instead, alt-text based captions gathered from the web is a far cheaper alternative to scale with the downside of being noisy. Recent modeling approaches to IC often fall short in terms of performance in leveraging these noisy datasets in favor of clean annotations. We address this problem with a simple yet effective technique of breaking down the task into two smaller, more controllable tasks \u2013 skeleton prediction and skeleton-based caption generation. Specifically, we show that sub-selecting content words as skeletons helps in generating improved and denoised captions when leveraging rich yet noisy alt-text\u2013based uncurated datasets. We also show that the predicted English skeletons can further cross-lingually be leveraged to generate non-English captions, and present experimental results covering caption generation in French, Italian, German, Spanish and Hindi. We also show that skeleton-based prediction allows for better control of certain caption properties, such as length, content, and gender expression, providing a handle to perform human-in-the-loop interpretable semi-automatic corrections.",
        "author": "Khyathi Raghavi Chandu; Piyush Sharma; Soravit Changpinyo; Ashish V. Thapliyal; Radu Soricut",
        "authorids": "/k/khyathi-raghavi-chandu/; /p/piyush-sharma/; /s/soravit-changpinyo/; /a/ashish-v-thapliyal/; /r/radu-soricut/",
        "bibtex": "@inproceedings{chandu-etal-2022-denoising,\n    title = \"Denoising Large-Scale Image Captioning from Alt-text Data Using Content Selection Models\",\n    author = \"Chandu, Khyathi Raghavi  and\n      Sharma, Piyush  and\n      Changpinyo, Soravit  and\n      Thapliyal, Ashish V.  and\n      Soricut, Radu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.532/\",\n    pages = \"6089--6104\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.532.pdf",
        "site": "https://aclanthology.org/2022.coling-1.532/",
        "pdf_size": 1977346,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17526536989308132360&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Meta AI+Google Research; Uber+Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "gmail.com;uber.com;google.com;google.com;google.com",
        "email": "gmail.com;uber.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2+1;1;1;1",
        "aff_unique_norm": "Meta Platforms, Inc.;Google;Uber Technologies Inc.",
        "aff_unique_dep": "Meta AI;Google Research;",
        "aff_unique_url": "https://meta.com;https://research.google;https://www.uber.com",
        "aff_unique_abbr": "Meta;Google Research;Uber",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.94",
        "title": "Dense Template Retrieval for Customer Support",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Templated answers are used extensively in customer support scenarios, providing an efficient way to cover a plethora of topics, with an easily maintainable collection of templates. However, the number of templates is often too high for an agent to manually search. Automatically suggesting the correct template for a given question can thus improve the service efficiency, reducing costs and leading to a better customer satisfaction. In this work, we propose a dense retrieval framework for the customer support scenario, adapting a standard in-batch negatives technique to support unpaired sampling of queries and templates. We also propose a novel loss that extends the typical query-centric similarity, exploiting other similarity relations in the training data. Experiments show that our approach achieves considerable improvements, in terms of performance and training speed, over more standard dense retrieval methods. This includes methods such as DPR, and also ablated versions of the proposed approach.",
        "author": "Tiago Mesquita; Bruno Martins; Mariana Almeida",
        "authorids": "/t/tiago-mesquita/; /b/bruno-martins/; /m/mariana-almeida/",
        "bibtex": "@inproceedings{mesquita-etal-2022-dense,\n    title = \"Dense Template Retrieval for Customer Support\",\n    author = \"Mesquita, Tiago  and\n      Martins, Bruno  and\n      Almeida, Mariana\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.94/\",\n    pages = \"1106--1115\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.94.pdf",
        "site": "https://aclanthology.org/2022.coling-1.94/",
        "pdf_size": 376007,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13814176602207969483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Zendesk; INESC-ID & LUMLIS (Lisbon ELLIS Unit) + IST, University of Lisbon; Zendesk",
        "aff_domain": "zendesk.com;inesc-id.pt;zendesk.com",
        "email": "zendesk.com;inesc-id.pt;zendesk.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Zendesk;INESC-ID;Instituto Superior T\u00e9cnico",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.zendesk.com;https://www.inesc-id.pt;https://www.ist.utl.pt",
        "aff_unique_abbr": "Zendesk;INESC-ID;IST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lisbon",
        "aff_country_unique_index": "0;1+1;0",
        "aff_country_unique": "United States;Portugal"
    },
    {
        "id": "2022.coling-1.205",
        "title": "Dependency-aware Prototype Learning for Few-shot Relation Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot relation classification aims to classify the relation type between two given entities in a sentence by training with a few labeled instances for each relation. However, most of existing models fail to distinguish multiple relations that co-exist in one sentence. This paper presents a novel dependency-aware prototype learning (DAPL) method for few-shot relation classification. Concretely, we utilize dependency trees and shortest dependency paths (SDP) as structural information to complement the contextualized representations of input sentences by using the dependency-aware embedding as attention inputs to learn attentive sentence representations. In addition, we introduce a gate controlled update mechanism to update the dependency-aware representations according to the output of each network layer. Extensive experiments on the FewRel dataset show that DAPL achieves substantially better performance than strong baselines. For reproducibility, we will release our code and data upon the publication of this paper at https://github.com/publicstaticvo/DAPL.",
        "author": "Tianshu Yu; Min Yang; Xiaoyan Zhao",
        "authorids": "/t/tianshu-yu/; /m/min-yang/; /x/xiaoyan-zhao/",
        "bibtex": "@inproceedings{yu-etal-2022-dependency,\n    title = \"Dependency-aware Prototype Learning for Few-shot Relation Classification\",\n    author = \"Yu, Tianshu  and\n      Yang, Min  and\n      Zhao, Xiaoyan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.205/\",\n    pages = \"2339--2345\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.205.pdf",
        "site": "https://aclanthology.org/2022.coling-1.205/",
        "pdf_size": 348687,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10689233047935275422&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "aff_domain": "siat.ac.cn;siat.ac.cn;siat.ac.cn",
        "email": "siat.ac.cn;siat.ac.cn;siat.ac.cn",
        "github": "https://github.com/publicstaticvo/DAPL",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;University of Chinese Academy of Sciences",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.siat.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "SIAT;UCAS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.583",
        "title": "Detecting Minority Arguments for Mutual Understanding: A Moderation Tool for the Online Climate Change Debate",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Moderating user comments and promoting healthy understanding is a challenging task, especially in the context of polarized topics such as climate change. We propose a moderation tool to assist moderators in promoting mutual understanding in regard to this topic. The approach is twofold. First, we train classifiers to label incoming posts for the arguments they entail, with a specific focus on minority arguments. We apply active learning to further supplement the training data with rare arguments. Second, we dive deeper into singular arguments and extract the lexical patterns that distinguish each argument from the others. Our findings indicate that climate change arguments form clearly separable clusters in the embedding space. These classes are characterized by their own unique lexical patterns that provide a quick insight in an argument\u2019s key concepts. Additionally, supplementing our training data was necessary for our classifiers to be able to adequately recognize rare arguments. We argue that this detailed rundown of each argument provides insight into where others are coming from. These computational approaches can be part of the toolkit for content moderators and researchers struggling with polarized topics.",
        "author": "Cedric Waterschoot; Ernst van den Hemel; Antal van den Bosch",
        "authorids": "/c/cedric-waterschoot/; /e/ernst-van-den-hemel/; /a/antal-van-den-bosch/",
        "bibtex": "@inproceedings{waterschoot-etal-2022-detecting,\n    title = \"Detecting Minority Arguments for Mutual Understanding: A Moderation Tool for the Online Climate Change Debate\",\n    author = \"Waterschoot, Cedric  and\n      van den Hemel, Ernst  and\n      van den Bosch, Antal\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.583/\",\n    pages = \"6715--6725\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.583.pdf",
        "site": "https://aclanthology.org/2022.coling-1.583/",
        "pdf_size": 389874,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11897105217006590127&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "KNAW Meertens Instituut; KNAW Meertens Instituut; Utrecht Institute of Linguistics OTS, Utrecht University",
        "aff_domain": "meertens.knaw.nl;meertens.knaw.nl;uu.nl",
        "email": "meertens.knaw.nl;meertens.knaw.nl;uu.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Meertens Instituut;Utrecht University",
        "aff_unique_dep": ";Utrecht Institute of Linguistics OTS",
        "aff_unique_url": "https://www.meertens.knaw.nl;https://www.uu.nl",
        "aff_unique_abbr": "KNAW Meertens;UU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Utrecht",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.coling-1.372",
        "title": "Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the increased awareness of situations of mental crisis and their societal impact, online services providing emergency support are becoming commonplace in many countries. Computational models, trained on discussions between help-seekers and providers, can support suicide prevention by identifying at-risk individuals. However, the lack of domain-specific models, especially in low-resource languages, poses a significant challenge for the automatic detection of suicide risk. We propose a model that combines pre-trained language models (PLM) with a fixed set of manually crafted (and clinically approved) set of suicidal cues, followed by a two-stage fine-tuning process. Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly outperforming an array of strong baselines even early on in the conversation, which is critical for real-time detection in the field. Moreover, the model performs well across genders and age groups.",
        "author": "Amir Bialer; Daniel Izmaylov; Avi Segal; Oren Tsur; Yossi Levi-Belz; Kobi Gal",
        "authorids": "/a/amir-bialer/; /d/daniel-izmaylov/; /a/avi-segal/; /o/oren-tsur/; /y/yossi-levi-belz/; /k/kobi-gal/",
        "bibtex": "@inproceedings{bialer-etal-2022-detecting,\n    title = \"Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language\",\n    author = \"Bialer, Amir  and\n      Izmaylov, Daniel  and\n      Segal, Avi  and\n      Tsur, Oren  and\n      Levi-Belz, Yossi  and\n      Gal, Kobi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.372/\",\n    pages = \"4241--4250\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.372.pdf",
        "site": "https://aclanthology.org/2022.coling-1.372/",
        "pdf_size": 802354,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6727739189282980060&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Ben-Gurion University of the Negev; Ben-Gurion University of the Negev; Ben-Gurion University of the Negev; Ben-Gurion University of the Negev; Ruppin Academic Center; Ben-Gurion University of the Negev+University of Edinburgh",
        "aff_domain": "post.bgu.ac.il;post.bgu.ac.il;gmail.com;bgu.ac.il;ruppin.ac.il;bgu.ac.il",
        "email": "post.bgu.ac.il;post.bgu.ac.il;gmail.com;bgu.ac.il;ruppin.ac.il;bgu.ac.il",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;0+2",
        "aff_unique_norm": "Ben-Gurion University of the Negev;Ruppin Academic Center;University of Edinburgh",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.bgu.ac.il;https://www.ruppin.ac.il;https://www.ed.ac.uk",
        "aff_unique_abbr": "BGU;;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+1",
        "aff_country_unique": "Israel;United Kingdom"
    },
    {
        "id": "2022.coling-1.320",
        "title": "DiaBiz.Kom - towards a Polish Dialogue Act Corpus Based on ISO 24617-2 Standard",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This article presents the specification and evaluation of DiaBiz.Kom \u2013 the corpus of dialogue texts in Polish. The corpus contains transcriptions of telephone conversations conducted according to a prepared scenario. The transcripts of conversations have been manually annotated with a layer of information concerning communicative functions. DiaBiz.Kom is the first corpus of this type prepared for the Polish language and will be used to develop a system of dialog analysis and modules for creating advanced chatbots.",
        "author": "Marcin Oleksy; Jan Wieczorek; Dorota Dru\u017cy\u0142owska; Julia Klyus; Aleksandra Domoga\u0142a; Krzysztof Hwaszcz; Hanna K\u0119dzierska; Daria Miko\u015b; Anita Wr\u00f3\u017c",
        "authorids": "/m/marcin-oleksy/; /j/jan-wieczorek/; /d/dorota-druzylowska/; /j/julia-klyus/; /a/aleksandra-domogala/; /k/krzysztof-hwaszcz/; /h/hanna-kedzierska/; /d/daria-mikos/; /a/anita-wroz/",
        "bibtex": "@inproceedings{oleksy-etal-2022-diabiz,\n    title = \"{D}ia{B}iz.{K}om - towards a {P}olish Dialogue Act Corpus Based on {ISO} 24617-2 Standard\",\n    author = \"Oleksy, Marcin  and\n      Wieczorek, Jan  and\n      Dru{\\.z}y{\\l}owska, Dorota  and\n      Klyus, Julia  and\n      Domoga{\\l}a, Aleksandra  and\n      Hwaszcz, Krzysztof  and\n      K{\\k{e}}dzierska, Hanna  and\n      Miko{\\'s}, Daria  and\n      Wr{\\'o}{\\.z}, Anita\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.320/\",\n    pages = \"3631--3638\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.320.pdf",
        "site": "https://aclanthology.org/2022.coling-1.320/",
        "pdf_size": 157824,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11808799160578309297&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland; Wroc\u0142aw University of Science and Technology, Wroc\u0142aw, Poland",
        "aff_domain": "pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl",
        "email": "pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl;pwr.edu.pl",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Wroc\u0142aw University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.pwr.edu.pl",
        "aff_unique_abbr": "WUST",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Wroc\u0142aw",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "Poland"
    },
    {
        "id": "2022.coling-1.35",
        "title": "DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Retrieval-based conversational systems learn to rank response candidates for a given dialogue context by computing the similarity between their vector representations. However, training on a single textual form of the multi-turn context limits the ability of a model to learn representations that generalize to natural perturbations seen during inference. In this paper we propose a framework that incorporates augmented versions of a dialogue context into the learning objective. We utilize contrastive learning as an auxiliary objective to learn robust dialogue context representations that are invariant to perturbations injected through the augmentation method. We experiment with four benchmark dialogue datasets and demonstrate that our framework combines well with existing augmentation methods and can significantly improve over baseline BERT-based ranking architectures. Furthermore, we propose a novel data augmentation method, ConMix, that adds token level perturbations through stochastic mixing of tokens from other contexts in the batch. We show that our proposed augmentation method outperforms previous data augmentation approaches, and provides dialogue representations that are more robust to common perturbations seen during inference.",
        "author": "Lahari Poddar; Peiyao Wang; Julia Reinspach",
        "authorids": "/l/lahari-poddar/; /p/peiyao-wang/; /j/julia-reinspach/",
        "bibtex": "@inproceedings{poddar-etal-2022-dialaug,\n    title = \"{D}ial{A}ug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling\",\n    author = \"Poddar, Lahari  and\n      Wang, Peiyao  and\n      Reinspach, Julia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.35/\",\n    pages = \"441--450\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.35.pdf",
        "site": "https://aclanthology.org/2022.coling-1.35/",
        "pdf_size": 596212,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8554476766343994364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon.com, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.60",
        "title": "DialMed: A Dataset for Dialogue-based Medication Recommendation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Medication recommendation is a crucial task for intelligent healthcare systems. Previous studies mainly recommend medications with electronic health records (EHRs). However, some details of interactions between doctors and patients may be ignored or omitted in EHRs, which are essential for automatic medication recommendation. Therefore, we make the first attempt to recommend medications with the conversations between doctors and patients. In this work, we construct DIALMED, the first high-quality dataset for medical dialogue-based medication recommendation task. It contains 11, 996 medical dialogues related to 16 common diseases from 3 departments and 70 corresponding common medications. Furthermore, we propose a Dialogue structure and Disease knowledge aware Network (DDN), where a QA Dialogue Graph mechanism is designed to model the dialogue structure and the knowledge graph is used to introduce external disease knowledge. The extensive experimental results demonstrate that the proposed method is a promising solution to recommend medications with medical dialogues. The dataset and code are available at https://github.com/f-window/DialMed.",
        "author": "Zhenfeng He; Yuqiang Han; Zhenqiu Ouyang; Wei Gao; Hongxu Chen; Guandong Xu; Jian Wu",
        "authorids": "/z/zhenfeng-he/; /y/yuqiang-han/; /z/zhenqiu-ouyang/; /w/wei-gao/; /h/hongxu-chen/; /g/guandong-xu/; /j/jian-wu/",
        "bibtex": "@inproceedings{he-etal-2022-dialmed,\n    title = \"{D}ial{M}ed: A Dataset for Dialogue-based Medication Recommendation\",\n    author = \"He, Zhenfeng  and\n      Han, Yuqiang  and\n      Ouyang, Zhenqiu  and\n      Gao, Wei  and\n      Chen, Hongxu  and\n      Xu, Guandong  and\n      Wu, Jian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.60/\",\n    pages = \"721--733\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.60.pdf",
        "site": "https://aclanthology.org/2022.coling-1.60/",
        "pdf_size": 1174271,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16319534130680768101&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; Polytechnic Institute, Zhejiang University; Ningbo Institute of Technology, Zhejiang University; University of Technology Sydney; University of Technology Sydney; School of Public Health, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;uts.edu.au;uts.edu.au;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;uts.edu.au;uts.edu.au;zju.edu.cn",
        "github": "https://github.com/f-window/DialMed",
        "project": "https://www.guahao.com/",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;1;0",
        "aff_unique_norm": "Zhejiang University;University of Technology Sydney",
        "aff_unique_dep": "College of Computer Science and Technology;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.uts.edu.au",
        "aff_unique_abbr": "ZJU;UTS",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Hangzhou;Ningbo",
        "aff_country_unique_index": "0;0;0;0;1;1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.coling-1.74",
        "title": "Dialo-AP: A Dependency Parsing Based Argument Parser for Dialogues",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While neural approaches to argument mining (AM) have advanced considerably, most of the recent work has been limited to parsing monologues. With an urgent interest in the use of conversational agents for broader societal applications, there is a need to advance the state-of-the-art in argument parsers for dialogues. This enables progress towards more purposeful conversations involving persuasion, debate and deliberation. This paper discusses Dialo-AP, an end-to-end argument parser that constructs argument graphs from dialogues. We formulate AM as dependency parsing of elementary and argumentative discourse units; the system is trained using extensive pre-training and curriculum learning comprising nine diverse corpora. Dialo-AP is capable of generating argument graphs from dialogues by performing all sub-tasks of AM. Compared to existing state-of-the-art baselines, Dialo-AP achieves significant improvements across all tasks, which is further validated through rigorous human evaluation.",
        "author": "Sougata Saha; Souvik Das; Rohini K. Srihari",
        "authorids": "/s/sougata-saha/; /s/souvik-das/; /r/rohini-k-srihari/",
        "bibtex": "@inproceedings{saha-etal-2022-dialo,\n    title = \"Dialo-{AP}: A Dependency Parsing Based Argument Parser for Dialogues\",\n    author = \"Saha, Sougata  and\n      Das, Souvik  and\n      Srihari, Rohini K.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.74/\",\n    pages = \"887--901\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.74.pdf",
        "site": "https://aclanthology.org/2022.coling-1.74/",
        "pdf_size": 1852457,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4274497579927102057&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "State University of New York at Buffalo; State University of New York at Buffalo; State University of New York at Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "State University of New York at Buffalo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.buffalo.edu",
        "aff_unique_abbr": "SUNY Buffalo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Buffalo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.57",
        "title": "DialogueEIN: Emotion Interaction Network for Dialogue Affective Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Emotion Recognition in Conversation (ERC) has attracted increasing attention in the affective computing research field. Previous works have mainly focused on modeling the semantic interactions in the dialogue and implicitly inferring the evolution of the speakers\u2019 emotional states. Few works have considered the emotional interactions, which directly reflect the emotional evolution of speakers in the dialogue. According to psychological and behavioral studies, the emotional inertia and emotional stimulus are important factors that affect the speaker\u2019s emotional state in conversations. In this work, we propose a novel Dialogue Emotion Interaction Network, DialogueEIN, to explicitly model the intra-speaker, inter-speaker, global and local emotional interactions to respectively simulate the emotional inertia, emotional stimulus, global and local emotional evolution in dialogues. Extensive experiments on four ERC benchmark datasets, IEMOCAP, MELD, EmoryNLP and DailyDialog, show that our proposed DialogueEIN considering emotional interaction factors can achieve superior or competitive performance compared to state-of-the-art methods. Our codes and models are released.",
        "author": "Yuchen Liu; Jinming Zhao; Jingwen Hu; Ruichen Li; Qin Jin",
        "authorids": "/y/yuchen-liu/; /j/jinming-zhao/; /j/jingwen-hu/; /r/ruichen-li/; /q/qin-jin/",
        "bibtex": "@inproceedings{liu-etal-2022-dialogueein,\n    title = \"{D}ialogue{EIN}: Emotion Interaction Network for Dialogue Affective Analysis\",\n    author = \"Liu, Yuchen  and\n      Zhao, Jinming  and\n      Hu, Jingwen  and\n      Li, Ruichen  and\n      Jin, Qin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.57/\",\n    pages = \"684--693\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.57.pdf",
        "site": "https://aclanthology.org/2022.coling-1.57/",
        "pdf_size": 559969,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11268631130628271372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Information, Renmin University of China; Qiyuan Lab, Beijing, China; School of Information, Renmin University of China; School of Information, Renmin University of China; School of Information, Renmin University of China",
        "aff_domain": "ruc.edu.cn;qiyuanlab.com;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;qiyuanlab.com;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "github": "https://github.com/AIM3-RUC/DialogueEIN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Renmin University of China;Qiyuan Lab",
        "aff_unique_dep": "School of Information;",
        "aff_unique_url": "http://www.ruc.edu.cn;",
        "aff_unique_abbr": "RUC;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.160",
        "title": "Different Data, Different Modalities! Reinforced Data Splitting for Effective Multimodal Information Extraction from Social Media Posts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, multimodal information extraction from social media posts has gained increasing attention in the natural language processing community. Despite their success, current approaches overestimate the significance of images. In this paper, we argue that different social media posts should consider different modalities for multimodal information extraction. Multimodal models cannot always outperform unimodal models. Some posts are more suitable for the multimodal model, while others are more suitable for the unimodal model. Therefore, we propose a general data splitting strategy to divide the social media posts into two sets so that these two sets can achieve better performance under the information extraction models of the corresponding modalities. Specifically, for an information extraction task, we first propose a data discriminator that divides social media posts into a multimodal and a unimodal set. Then we feed these sets into the corresponding models. Finally, we combine the results of these two models to obtain the final extraction results. Due to the lack of explicit knowledge, we use reinforcement learning to train the data discriminator. Experiments on two different multimodal information extraction tasks demonstrate the effectiveness of our method. The source code of this paper can be found in https://github.com/xubodhu/RDS.",
        "author": "Bo Xu; Shizhou Huang; Ming Du; Hongya Wang; Hui Song; Chaofeng Sha; Yanghua Xiao",
        "authorids": "/b/bo-xu/; /s/shizhou-huang/; /m/ming-du/; /h/hongya-wang/; /h/hui-song/; /c/chaofeng-sha/; /y/yanghua-xiao/",
        "bibtex": "https://aclanthology.org/2022.coling-1.160.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.160.pdf",
        "site": "https://aclanthology.org/2022.coling-1.160/",
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9297126212358407838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.50",
        "title": "Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Out-of-Domain (OOD) detection is a key component in a task-oriented dialog system, which aims to identify whether a query falls outside the predefined supported intent set. Previous softmax-based detection algorithms are proved to be overconfident for OOD samples. In this paper, we analyze overconfident OOD comes from distribution uncertainty due to the mismatch between the training and test distributions, which makes the model can\u2019t confidently make predictions thus probably causes abnormal softmax scores. We propose a Bayesian OOD detection framework to calibrate distribution uncertainty using Monte-Carlo Dropout. Our method is flexible and easily pluggable to existing softmax-based baselines and gains 33.33% OOD F1 improvements with increasing only 0.41% inference time compared to MSP. Further analyses show the effectiveness of Bayesian learning for OOD detection.",
        "author": "Yanan Wu; Zhiyuan Zeng; Keqing He; Yutao Mou; Pei Wang; Weiran Xu",
        "authorids": "/y/yanan-wu/; /z/zhiyuan-zeng/; /k/keqing-he/; /y/yutao-mou/; /p/pei-wang/; /w/weiran-xu/",
        "bibtex": "@inproceedings{wu-etal-2022-distribution,\n    title = \"Distribution Calibration for Out-of-Domain Detection with {B}ayesian Approximation\",\n    author = \"Wu, Yanan  and\n      Zeng, Zhiyuan  and\n      He, Keqing  and\n      Mou, Yutao  and\n      Wang, Pei  and\n      Xu, Weiran\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.50/\",\n    pages = \"608--615\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.50.pdf",
        "site": "https://aclanthology.org/2022.coling-1.50/",
        "pdf_size": 614764,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2369432027666083216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Meituan Group; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/pris-nlp/COLING2022_Bayesian-for-OOD/",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BUPT;Meituan",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.194",
        "title": "Diverse Multi-Answer Retrieval with Determinantal Point Processes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Often questions provided to open-domain question answering systems are ambiguous. Traditional QA systems that provide a single answer are incapable of answering ambiguous questions since the question may be interpreted in several ways and may have multiple distinct answers. In this paper, we address multi-answer retrieval which entails retrieving passages that can capture majority of the diverse answers to the question. We propose a re-ranking based approach using Determinantal point processes utilizing BERT as kernels. Our method jointly considers query-passage relevance and passage-passage correlation to retrieve passages that are both query-relevant and diverse. Results demonstrate that our re-ranking technique outperforms state-of-the-art method on the AmbigQA dataset.",
        "author": "Poojitha Nandigam; Nikhil Rayaprolu; Manish Shrivastava",
        "authorids": "/p/poojitha-nandigam/; /n/nikhil-rayaprolu/; /m/manish-shrivastava/",
        "bibtex": "@inproceedings{nandigam-etal-2022-diverse,\n    title = \"Diverse Multi-Answer Retrieval with Determinantal Point Processes\",\n    author = \"Nandigam, Poojitha  and\n      Rayaprolu, Nikhil  and\n      Shrivastava, Manish\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.194/\",\n    pages = \"2220--2225\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.194.pdf",
        "site": "https://aclanthology.org/2022.coling-1.194/",
        "pdf_size": 253310,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5221727616389015489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technologies Research Centre (LTRC), International Institute of Information Technology, Hyderabad, India; Language Technologies Research Centre (LTRC), International Institute of Information Technology, Hyderabad, India; Language Technologies Research Centre (LTRC), International Institute of Information Technology, Hyderabad, India",
        "aff_domain": "research.iiit.ac.in;students.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;students.iiit.ac.in;iiit.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International Institute of Information Technology",
        "aff_unique_dep": "Language Technologies Research Centre",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.570",
        "title": "Diversifying Neural Text Generation with Part-of-Speech Guided Softmax and Sampling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural text generation models are likely to suffer from the low-diversity problem. Various decoding strategies and training-based methods have been proposed to promote diversity only by exploiting contextual features, but rarely do they consider incorporating syntactic structure clues. In this work, we propose using linguistic annotation, i.e., part-of-speech (POS), to guide the text generation. In detail, we introduce POS Guided Softmax to explicitly model two posterior probabilities: (i) next-POS, and (ii) next-token from the vocabulary of the target POS. A POS Guided Sampling strategy is further proposed to address the low-diversity problem by enriching the diversity of POS. Extensive experiments and human evaluations show that, compared with existing state-of-the-art methods, our POS Guided Softmax and Sampling (POSG) can generate more diverse text while maintaining comparable quality.",
        "author": "Zhixian Yang; Pengxuan Xu; Xiaojun Wan",
        "authorids": "/z/zhixian-yang/; /p/pengxuan-xu/; /x/xiaojun-wan/",
        "bibtex": "@inproceedings{yang-etal-2022-diversifying,\n    title = \"Diversifying Neural Text Generation with Part-of-Speech Guided Softmax and Sampling\",\n    author = \"Yang, Zhixian  and\n      Xu, Pengxuan  and\n      Wan, Xiaojun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.570/\",\n    pages = \"6547--6563\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.570.pdf",
        "site": "https://aclanthology.org/2022.coling-1.570/",
        "pdf_size": 702599,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2176443634040927671&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Wangxuan Institute of Computer Technology, Peking University; Center for Data Science, Peking University; The MOE Key Laboratory of Computational Linguistics, Peking University",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/FadedCosine/POS-Guided-Neural-Text-Generation",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.1",
        "title": "Do Language Models Make Human-like Predictions about the Coreferents of Italian Anaphoric Zero Pronouns?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Some languages allow arguments to be omitted in certain contexts. Yet human language comprehenders reliably infer the intended referents of these zero pronouns, in part because they construct expectations about which referents are more likely. We ask whether Neural Language Models also extract the same expectations. We test whether 12 contemporary language models display expectations that reflect human behavior when exposed to sentences with zero pronouns from five behavioral experiments conducted in Italian by Carminati (2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the human behavior from all the experiments, with others successfully modeling some of the results. This result suggests that human expectations about coreference can be derived from exposure to language, and also indicates features of language models that allow them to better reflect human behavior.",
        "author": "James A. Michaelov; Benjamin K. Bergen",
        "authorids": "/j/james-a-michaelov/; /b/benjamin-k-bergen/",
        "bibtex": "@inproceedings{michaelov-bergen-2022-language,\n    title = \"Do Language Models Make Human-like Predictions about the Coreferents of {I}talian Anaphoric Zero Pronouns?\",\n    author = \"Michaelov, James A.  and\n      Bergen, Benjamin K.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.1/\",\n    pages = \"1--14\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.1.pdf",
        "site": "https://aclanthology.org/2022.coling-1.1/",
        "pdf_size": 361202,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15141883686269955509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Cognitive Science, University of California, San Diego; Department of Cognitive Science, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Cognitive Science",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.188",
        "title": "DoSEA: A Domain-specific Entity-aware Framework for Cross-Domain Named Entity Recogition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-domain named entity recognition aims to improve performance in a target domain with shared knowledge from a well-studied source domain. The previous sequence-labeling based method focuses on promoting model parameter sharing among domains. However, such a paradigm essentially ignores the domain-specific information and suffers from entity type conflicts. To address these issues, we propose a novel machine reading comprehension based framework, named DoSEA, which can identify domain-specific semantic differences and mitigate the subtype conflicts between domains. Concretely, we introduce an entity existence discrimination task and an entity-aware training setting, to recognize inconsistent entity annotations in the source domain and bring additional reference to better share information across domains. Experiments on six datasets prove the effectiveness of our DoSEA. Our source code can be obtained from https://github.com/mhtang1995/DoSEA.",
        "author": "Minghao Tang; Peng Zhang; Yongquan He; Yongxiu Xu; Chengpeng Chao; Hongbo Xu",
        "authorids": "/m/minghao-tang/; /p/peng-zhang/; /y/yongquan-he/; /y/yongxiu-xu/; /c/chengpeng-chao/; /h/hongbo-xu/",
        "bibtex": "@inproceedings{tang-etal-2022-dosea,\n    title = \"{D}o{SEA}: A Domain-specific Entity-aware Framework for Cross-Domain Named Entity Recogition\",\n    author = \"Tang, Minghao  and\n      Zhang, Peng  and\n      He, Yongquan  and\n      Xu, Yongxiu  and\n      Chao, Chengpeng  and\n      Xu, Hongbo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.188/\",\n    pages = \"2147--2156\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.188.pdf",
        "site": "https://aclanthology.org/2022.coling-1.188/",
        "pdf_size": 476483,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2223923976169446870&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/mhtang1995/DoSEA",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.256",
        "title": "Doc-GCN: Heterogeneous Graph Convolutional Networks for Document Layout Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recognizing the layout of unstructured digital documents is crucial when parsing the documents into the structured, machine-readable format for downstream applications. Recent studies in Document Layout Analysis usually rely on visual cues to understand documents while ignoring other information, such as contextual information or the relationships between document layout components, which are vital to boost better layout analysis performance. Our Doc-GCN presents an effective way to harmonize and integrate heterogeneous aspects for Document Layout Analysis. We construct different graphs to capture the four main features aspects of document layout components, including syntactic, semantic, density, and appearance features. Then, we apply graph convolutional networks to enhance each aspect of features and apply the node-level pooling for integration. Finally, we concatenate features of all aspects and feed them into the 2-layer MLPs for document layout component classification. Our Doc-GCN achieves state-of-the-art results on three widely used DLA datasets: PubLayNet, FUNSD, and DocBank. The code will be released at https://github.com/adlnlp/doc_gcn",
        "author": "Siwen Luo; Yihao Ding; Siqu Long; Josiah Poon; Soyeon Caren Han",
        "authorids": "/s/siwen-luo/; /y/yihao-ding/; /s/siqu-long/; /j/josiah-poon/; /s/soyeon-caren-han/",
        "bibtex": "@inproceedings{luo-etal-2022-doc,\n    title = \"Doc-{GCN}: Heterogeneous Graph Convolutional Networks for Document Layout Analysis\",\n    author = \"Luo, Siwen  and\n      Ding, Yihao  and\n      Long, Siqu  and\n      Poon, Josiah  and\n      Han, Soyeon Caren\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.256/\",\n    pages = \"2906--2916\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.256.pdf",
        "site": "https://aclanthology.org/2022.coling-1.256/",
        "pdf_size": 1413459,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3118721840227434439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Sydney; The University of Sydney; The University of Sydney; The University of Sydney; The University of Sydney+The University of Western Australia",
        "aff_domain": "sydney.edu.au;sydney.edu.au;uni.sydney.edu.au;sydney.edu.au;sydney.edu.au+caren.han",
        "email": "sydney.edu.au;sydney.edu.au;uni.sydney.edu.au;sydney.edu.au;sydney.edu.au+caren.han",
        "github": "https://github.com/adlnlp/doc_gcn",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "University of Sydney;University of Western Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sydney.edu.au;https://www.uwa.edu.au",
        "aff_unique_abbr": "USYD;UWA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.coling-1.187",
        "title": "DocQueryNet: Value Retrieval with Arbitrary Queries for Form-like Documents",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose, DocQueryNet, a value retrieval method with arbitrary queries for form-like documents to reduce human effort of processing forms. Unlike previous methods that only address a fixed set of field items, our method predicts target value for an arbitrary query based on the understanding of the layout and semantics of a form. To further boost model performance, we propose a simple document language modeling (SimpleDLM) strategy to improve document understanding on large-scale model pre-training. Experimental results show that DocQueryNet outperforms previous designs significantly and the SimpleDLM further improves our performance on value retrieval by around 17% F1 score compared with the state-of-the-art pre-training method. Code is available here, https://github.com/salesforce/QVR-SimpleDLM.",
        "author": "Mingfei Gao; Le Xue; Chetan Ramaiah; Chen Xing; Ran Xu; Caiming Xiong",
        "authorids": "/m/mingfei-gao/; /l/le-xue/; /c/chetan-ramaiah/; /c/chen-xing/; /r/ran-xu/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{gao-etal-2022-docquerynet,\n    title = \"{D}oc{Q}uery{N}et: Value Retrieval with Arbitrary Queries for Form-like Documents\",\n    author = \"Gao, Mingfei  and\n      Xue, Le  and\n      Ramaiah, Chetan  and\n      Xing, Chen  and\n      Xu, Ran  and\n      Xiong, Caiming\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.187/\",\n    pages = \"2141--2146\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.187.pdf",
        "site": "https://aclanthology.org/2022.coling-1.187/",
        "pdf_size": 419852,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18394873742706885914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Salesforce Research, Palo Alto, USA; Salesforce Research, Palo Alto, USA; Salesforce Research, Palo Alto, USA; Salesforce Research, Palo Alto, USA; Salesforce Research, Palo Alto, USA; Salesforce Research, Palo Alto, USA",
        "aff_domain": "salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "email": "salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Salesforce Research",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://research.salesforce.com",
        "aff_unique_abbr": "Salesforce",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Palo Alto",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.213",
        "title": "Document-Level Relation Extraction via Pair-Aware and Entity-Enhanced Representation Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level relation extraction aims to recognize relations among multiple entity pairs from a whole piece of article. Recent methods achieve considerable performance but still suffer from two challenges: a) the relational entity pairs are sparse, b) the representation of entity pairs is insufficient. In this paper, we propose Pair-Aware and Entity-Enhanced(PAEE) model to solve the aforementioned two challenges. For the first challenge, we design a Pair-Aware Representation module to predict potential relational entity pairs, which constrains the relation extraction to the predicted entity pairs subset rather than all pairs; For the second, we introduce a Entity-Enhanced Representation module to assemble directional entity pairs and obtain a holistic understanding of the entire document. Experimental results show that our approach can obtain state-of-the-art performance on four benchmark datasets DocRED, DWIE, CDR and GDA.",
        "author": "Xiusheng Huang; Hang Yang; Yubo Chen; Jun Zhao; Kang Liu; Weijian Sun; Zuyu Zhao",
        "authorids": "/x/xiusheng-huang/; /h/hang-yang/; /y/yubo-chen/; /j/jun-zhao/; /k/kang-liu/; /w/weijian-sun/; /z/zuyu-zhao/",
        "bibtex": "@inproceedings{huang-etal-2022-document,\n    title = \"Document-Level Relation Extraction via Pair-Aware and Entity-Enhanced Representation Learning\",\n    author = \"Huang, Xiusheng  and\n      Yang, Hang  and\n      Chen, Yubo  and\n      Zhao, Jun  and\n      Liu, Kang  and\n      Sun, Weijian  and\n      Zhao, Zuyu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.213/\",\n    pages = \"2418--2428\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.213.pdf",
        "site": "https://aclanthology.org/2022.coling-1.213/",
        "pdf_size": 688312,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6574075754754849761&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China + National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China + Beijing Academy of Artificial Intelligence, Beijing, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;huawei.com;huawei.com",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1+2;3;3",
        "aff_unique_norm": "University of Chinese Academy of Sciences;National Laboratory of Pattern Recognition;Beijing Academy of Artificial Intelligence;Huawei Technologies Co., Ltd",
        "aff_unique_dep": "School of Artificial Intelligence;Institute of Automation;;",
        "aff_unique_url": "http://www.ucas.ac.cn;;https://www.baaic.cn;https://www.huawei.com",
        "aff_unique_abbr": "UCAS;;BAAI;Huawei",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0+0;1;1",
        "aff_campus_unique": "Beijing;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.183",
        "title": "Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level biomedical relation extraction (Bio-DocuRE) is an important branch of biomedical text mining that aims to automatically extract all relation facts from the biomedical text. Since there are a considerable number of relations in biomedical documents that need to be judged by other existing relations, logical reasoning has become a research hotspot in the past two years. However, current models with reasoning are single-granularity only based on one element information, ignoring the complementary fact of different granularity reasoning information. In addition, obtaining rich document information is a prerequisite for logical reasoning, but most of the previous models cannot sufficiently utilize document information, which limits the reasoning ability of the model. In this paper, we propose a novel Bio-DocuRE model called FILR, based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning. Specifically, FILR presents a multi-dimensional information fusion module MDIF to extract sufficient global document information. Then FILR proposes a multi-granularity reasoning module MGLR to obtain rich inference information through the reasoning of both entity-pairs and mention-pairs. We evaluate our FILR model on two widely used biomedical corpora CDR and GDA. Experimental results show that FILR achieves state-of-the-art performance.",
        "author": "Lishuang Li; Ruiyuan Lian; Hongbin Lu; Jingyao Tang",
        "authorids": "/l/lishuang-li/; /r/ruiyuan-lian/; /h/hongbin-lu/; /j/jingyao-tang/",
        "bibtex": "@inproceedings{li-etal-2022-document,\n    title = \"Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning\",\n    author = \"Li, Lishuang  and\n      Lian, Ruiyuan  and\n      Lu, Hongbin  and\n      Tang, Jingyao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.183/\",\n    pages = \"2098--2107\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.183.pdf",
        "site": "https://aclanthology.org/2022.coling-1.183/",
        "pdf_size": 654461,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17674094858437373292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology",
        "aff_domain": "dlut.edu.cn; ; ; ",
        "email": "dlut.edu.cn; ; ; ",
        "github": "https://github.com/Luguo-ry/FILR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://en.dlut.edu.cn/",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Dalian",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.231",
        "title": "Document-level Event Factuality Identification via Machine Reading Comprehension Frameworks with Transfer Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level Event Factuality Identification (DEFI) predicts the factuality of a specific event based on a document from which the event can be derived, which is a fundamental and crucial task in Natural Language Processing (NLP). However, most previous studies only considered sentence-level task and did not adopt document-level knowledge. Moreover, they modelled DEFI as a typical text classification task depending on annotated information heavily, and limited to the task-specific corpus only, which resulted in data scarcity. To tackle these issues, we propose a new framework formulating DEFI as Machine Reading Comprehension (MRC) tasks considering both Span-Extraction (Ext) and Multiple-Choice (Mch). Our model does not employ any other explicit annotated information, and utilizes Transfer Learning (TL) to extract knowledge from universal large-scale MRC corpora for cross-domain data augmentation. The empirical results on DLEFM corpus demonstrate that the proposed model outperforms several state-of-the-arts.",
        "author": "Zhong Qian; Heng Zhang; Peifeng Li; Qiaoming Zhu; Guodong Zhou",
        "authorids": "/z/zhong-qian/; /h/heng-zhang/; /p/peifeng-li/; /q/qiaoming-zhu/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{qian-etal-2022-document,\n    title = \"Document-level Event Factuality Identification via Machine Reading Comprehension Frameworks with Transfer Learning\",\n    author = \"Qian, Zhong  and\n      Zhang, Heng  and\n      Li, Peifeng  and\n      Zhu, Qiaoming  and\n      Zhou, Guodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.231/\",\n    pages = \"2622--2632\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.231.pdf",
        "site": "https://aclanthology.org/2022.coling-1.231/",
        "pdf_size": 793442,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4793614227278102595&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, Suzhou, China+AI Research Institute, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China+AI Research Institute, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China+AI Research Institute, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China+AI Research Institute, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China+AI Research Institute, Soochow University, Suzhou, China",
        "aff_domain": "suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.360",
        "title": "Does BERT Recognize an Agent? Modeling Dowty\u2019s Proto-Roles with Contextual Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Contextual embeddings build multidimensional representations of word tokens based on their context of occurrence. Such models have been shown to achieve a state-of-the-art performance on a wide variety of tasks. Yet, the community struggles in understanding what kind of semantic knowledge these representations encode. We report a series of experiments aimed at investigating to what extent one of such models, BERT, is able to infer the semantic relations that, according to Dowty\u2019s Proto-Roles theory, a verbal argument receives by virtue of its role in the event described by the verb. This hypothesis were put to test by learning a linear mapping from the BERT\u2019s verb embeddings to an interpretable space of semantic properties built from the linguistic dataset by White et al. (2016). In a first experiment we tested whether the semantic properties inferred from a typed version of the BERT embeddings would be more linguistically plausible than those produced by relying on static embeddings. We then move to evaluate the semantic properties inferred from the contextual embeddings both against those available in the original dataset, as well as by assessing their ability to model the semantic properties possessed by the agent of the verbs participating in the so-called causative alternation.",
        "author": "Mattia Proietti; Gianluca Lebani; Alessandro Lenci",
        "authorids": "/m/mattia-proietti/; /g/gianluca-e-lebani/; /a/alessandro-lenci/",
        "bibtex": "@inproceedings{proietti-etal-2022-bert,\n    title = \"Does {BERT} Recognize an Agent? Modeling {D}owty{'}s Proto-Roles with Contextual Embeddings\",\n    author = \"Proietti, Mattia  and\n      Lebani, Gianluca  and\n      Lenci, Alessandro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.360/\",\n    pages = \"4101--4112\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.360.pdf",
        "site": "https://aclanthology.org/2022.coling-1.360/",
        "pdf_size": 471285,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13225830175430485093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "CoLing Lab, University of Pisa; Ca\u2019 Foscari University of Venice; CoLing Lab, University of Pisa",
        "aff_domain": "studenti.unipi.it;unive.it;unipi.it",
        "email": "studenti.unipi.it;unive.it;unipi.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Pisa;Ca\u2019 Foscari University of Venice",
        "aff_unique_dep": "CoLing Lab;",
        "aff_unique_url": "https://www.unipi.it;https://www.unive.it",
        "aff_unique_abbr": ";Ca\u2019 Foscari",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Venice",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.coling-1.278",
        "title": "Does BERT Rediscover a Classical NLP Pipeline?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Does BERT store surface knowledge in its bottom layers, syntactic knowledge in its middle layers, and semantic knowledge in its upper layers? In re-examining Jawahar et al. (2019) and Tenney et al.\u2019s (2019a) probes into the structure of BERT, we have found that the pipeline-like separation that they asserted lacks conclusive empirical support. BERT\u2019s structure is, however, linguistically founded, although perhaps in a way that is more nuanced than can be explained by layers alone. We introduce a novel probe, called GridLoc, through which we can also take into account token positions, training rounds, and random seeds. Using GridLoc, we are able to detect other, stronger regularities that suggest that pseudo-cognitive appeals to layer depth may not be the preferable mode of explanation for BERT\u2019s inner workings.",
        "author": "Jingcheng Niu; Wenjie Lu; Gerald Penn",
        "authorids": "/j/jingcheng-niu/; /w/wenjie-lu/; /g/gerald-penn/",
        "bibtex": "@inproceedings{niu-etal-2022-bert,\n    title = \"Does {BERT} Rediscover a Classical {NLP} Pipeline?\",\n    author = \"Niu, Jingcheng  and\n      Lu, Wenjie  and\n      Penn, Gerald\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.278/\",\n    pages = \"3143--3153\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.278.pdf",
        "site": "https://aclanthology.org/2022.coling-1.278/",
        "pdf_size": 13850811,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=783074049121671843&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Toronto; University of Toronto; University of Toronto + Vector Institute",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "https://github.com/frankniujc/gridloc_probe",
        "project": "https://doi.org/10.5683/SP3/PCZHN4",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University of Toronto;Vector Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://vectorinstitute.ai/",
        "aff_unique_abbr": "U of T;Vector Institute",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.56",
        "title": "Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Since empathy plays a crucial role in increasing social bonding between people, many studies have designed their own dialogue agents to be empathetic using the well-established method of fine-tuning. However, they do not use prompt-based in-context learning, which has shown powerful performance in various natural language processing (NLP) tasks, for empathetic dialogue generation. Although several studies have investigated few-shot in-context learning for empathetic dialogue generation, an in-depth analysis of the generation of empathetic dialogue with in-context learning remains unclear, especially in GPT-3 (Brown et al., 2020). In this study, we explore whether GPT-3 can generate empathetic dialogues through prompt-based in-context learning in both zero-shot and few-shot settings. To enhance performance, we propose two new in-context example selection methods, called SITSM and EMOSITSM, that utilize emotion and situational information. We also introduce a new automatic evaluation method, DIFF-EPITOME, which reflects the human tendency to express empathy. From the analysis, we reveal that our DIFF-EPITOME is effective in measuring the degree of human empathy. We show that GPT-3 achieves competitive performance with Blender 90M, a state-of-the-art dialogue generative model, on both automatic and human evaluation. Our code is available at https://github.com/passing2961/EmpGPT-3.",
        "author": "Young-Jun Lee; Chae-Gyun Lim; Ho-Jin Choi",
        "authorids": "/y/young-jun-lee/; /c/chae-gyun-lim/; /h/ho-jin-choi/",
        "bibtex": "@inproceedings{lee-etal-2022-gpt,\n    title = \"Does {GPT}-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation\",\n    author = \"Lee, Young-Jun  and\n      Lim, Chae-Gyun  and\n      Choi, Ho-Jin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.56/\",\n    pages = \"669--683\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.56.pdf",
        "site": "https://aclanthology.org/2022.coling-1.56/",
        "pdf_size": 497803,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10634009802952645047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computing, KAIST; School of Computing, KAIST; School of Computing, KAIST",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/passing2961/EmpGPT-3",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.373",
        "title": "Does Meta-learning Help mBERT for Few-shot Question Generation in a Cross-lingual Transfer Setting for Indic Languages?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot Question Generation (QG) is an important and challenging problem in the Natural Language Generation (NLG) domain. Multilingual BERT (mBERT) has been successfully used in various Natural Language Understanding (NLU) applications. However, the question of how to utilize mBERT for few-shot QG, possibly with cross-lingual transfer, remains. In this paper, we try to explore how mBERT performs in few-shot QG (cross-lingual transfer) and also whether applying meta-learning on mBERT further improves the results. In our setting, we consider mBERT as the base model and fine-tune it using a seq-to-seq language modeling framework in a cross-lingual setting. Further, we apply the model agnostic meta-learning approach to our base model. We evaluate our model for two low-resource Indian languages, Bengali and Telugu, using the TyDi QA dataset. The proposed approach consistently improves the performance of the base model in few-shot settings and even works better than some heavily parameterized models. Human evaluation also confirms the effectiveness of our approach.",
        "author": "Aniruddha Roy; Rupak Kumar Thakur; Isha Sharma; Ashim Gupta; Amrith Krishna; Sudeshna Sarkar; Pawan Goyal",
        "authorids": "/a/aniruddha-roy/; /r/rupak-kumar-thakur/; /i/isha-sharma/; /a/ashim-gupta/; /a/amrith-krishna/; /s/sudeshna-sarkar/; /p/pawan-goyal/",
        "bibtex": "@inproceedings{roy-etal-2022-meta,\n    title = \"Does Meta-learning Help m{BERT} for Few-shot Question Generation in a Cross-lingual Transfer Setting for Indic Languages?\",\n    author = \"Roy, Aniruddha  and\n      Thakur, Rupak Kumar  and\n      Sharma, Isha  and\n      Gupta, Ashim  and\n      Krishna, Amrith  and\n      Sarkar, Sudeshna  and\n      Goyal, Pawan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.373/\",\n    pages = \"4251--4257\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.373.pdf",
        "site": "https://aclanthology.org/2022.coling-1.373/",
        "pdf_size": 165111,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5347314338841426893&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "IIT Kharagpur; Google India+IIT Kharagpur; IIT Kharagpur; University of Utah; Uniphore; IIT Kharagpur; IIT Kharagpur",
        "aff_domain": "iitkgp.ac.in;google.com;iitkgp.ac.in; ; ;cse.iitkgp.ac.in;cse.iitkgp.ac.in",
        "email": "iitkgp.ac.in;google.com;iitkgp.ac.in; ; ;cse.iitkgp.ac.in;cse.iitkgp.ac.in",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+0;0;2;3;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur;Google;University of Utah;Uniphore Software Systems",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.iitkgp.ac.in;https://www.google.com;https://www.utah.edu;https://www.uniphore.com",
        "aff_unique_abbr": "IIT KGP;Google India;Utah;Uniphore",
        "aff_campus_unique_index": "0;1+0;0;0;0",
        "aff_campus_unique": "Kharagpur;Bangalore;",
        "aff_country_unique_index": "0;0+0;0;1;0;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2022.coling-1.153",
        "title": "Domain Adaptation for Question Answering via Question Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question answering (QA) has demonstrated impressive progress in answering questions from customized domains. Nevertheless, domain adaptation remains one of the most elusive challenges for QA systems, especially when QA systems are trained in a source domain but deployed in a different target domain. In this work, we investigate the potential benefits of question classification for QA domain adaptation. We propose a novel framework: Question Classification for Question Answering (QC4QA). Specifically, a question classifier is adopted to assign question classes to both the source and target data. Then, we perform joint training in a self-supervised fashion via pseudo-labeling. For optimization, inter-domain discrepancy between the source and target domain is reduced via maximum mean discrepancy (MMD) distance. We additionally minimize intra-class discrepancy among QA samples of the same question class for fine-grained adaptation performance. To the best of our knowledge, this is the first work in QA domain adaptation to leverage question classification with self-supervised adaptation. We demonstrate the effectiveness of the proposed QC4QA with consistent improvements against the state-of-the-art baselines on multiple datasets.",
        "author": "Zhenrui Yue; Huimin Zeng; Ziyi Kou; Lanyu Shang; Dong Wang",
        "authorids": "/z/zhenrui-yue/; /h/huimin-zeng/; /z/ziyi-kou/; /l/lanyu-shang/; /d/dong-wang/",
        "bibtex": "@inproceedings{yue-etal-2022-domain,\n    title = \"Domain Adaptation for Question Answering via Question Classification\",\n    author = \"Yue, Zhenrui  and\n      Zeng, Huimin  and\n      Kou, Ziyi  and\n      Shang, Lanyu  and\n      Wang, Dong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.153/\",\n    pages = \"1776--1790\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.153.pdf",
        "site": "https://aclanthology.org/2022.coling-1.153/",
        "pdf_size": 984512,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3197857571287512828&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "School of Information Sciences; School of Information Sciences; School of Information Sciences; School of Information Sciences; School of Information Sciences",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "School of Information Sciences",
        "aff_unique_dep": "Information Sciences",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.coling-1.578",
        "title": "Domain Classification-based Source-specific Term Penalization for Domain Adaptation in Hate-speech Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "State-of-the-art approaches for hate-speech detection usually exhibit poor performance in out-of-domain settings. This occurs, typically, due to classifiers overemphasizing source-specific information that negatively impacts its domain invariance. Prior work has attempted to penalize terms related to hate-speech from manually curated lists using feature attribution methods, which quantify the importance assigned to input terms by the classifier when making a prediction. We, instead, propose a domain adaptation approach that automatically extracts and penalizes source-specific terms using a domain classifier, which learns to differentiate between domains, and feature-attribution scores for hate-speech classes, yielding consistent improvements in cross-domain evaluation.",
        "author": "Tulika Bose; Nikolaos Aletras; Irina Illina; Dominique Fohr",
        "authorids": "/t/tulika-bose/; /n/nikolaos-aletras/; /i/irina-illina/; /d/dominique-fohr/",
        "bibtex": "@inproceedings{bose-etal-2022-domain,\n    title = \"Domain Classification-based Source-specific Term Penalization for Domain Adaptation in Hate-speech Detection\",\n    author = \"Bose, Tulika  and\n      Aletras, Nikolaos  and\n      Illina, Irina  and\n      Fohr, Dominique\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.578/\",\n    pages = \"6656--6666\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.578.pdf",
        "site": "https://aclanthology.org/2022.coling-1.578/",
        "pdf_size": 469230,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3559122275045713685&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France; University of Sheffield, United Kingdom; Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France; Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France",
        "aff_domain": "loria.fr;sheffield.ac.uk;loria.fr;loria.fr",
        "email": "loria.fr;sheffield.ac.uk;loria.fr;loria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Universite de Lorraine;University of Sheffield",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.univ-lorraine.fr;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "UL;Sheffield",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nancy;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "id": "2022.coling-1.602",
        "title": "Domain Generalization for Text Classification with Memory-Based Supervised Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While there is much research on cross-domain text classification, most existing approaches focus on one-to-one or many-to-one domain adaptation. In this paper, we tackle the more challenging task of domain generalization, in which domain-invariant representations are learned from multiple source domains, without access to any data from the target domains, and classification decisions are then made on test documents in unseen target domains. We propose a novel framework based on supervised contrastive learning with a memory-saving queue. In this way, we explicitly encourage examples of the same class to be closer and examples of different classes to be further apart in the embedding space. We have conducted extensive experiments on two Amazon review sentiment datasets, and one rumour detection dataset. Experimental results show that our domain generalization method consistently outperforms state-of-the-art domain adaptation methods.",
        "author": "Qingyu Tan; Ruidan He; Lidong Bing; Hwee Tou Ng",
        "authorids": "/q/qingyu-tan/; /r/ruidan-he/; /l/lidong-bing/; /h/hwee-tou-ng/",
        "bibtex": "@inproceedings{tan-etal-2022-domain,\n    title = \"Domain Generalization for Text Classification with Memory-Based Supervised Contrastive Learning\",\n    author = \"Tan, Qingyu  and\n      He, Ruidan  and\n      Bing, Lidong  and\n      Ng, Hwee Tou\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.602/\",\n    pages = \"6916--6926\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.602.pdf",
        "site": "https://aclanthology.org/2022.coling-1.602/",
        "pdf_size": 992015,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16624808118216506247&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/tonytan48/MSCL",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.312",
        "title": "Domain- and Task-Adaptation for VaccinChatNL, a Dutch COVID-19 FAQ Answering Corpus and Classification Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "FAQs are important resources to find information. However, especially if a FAQ concerns many question-answer pairs, it can be a difficult and time-consuming job to find the answer you are looking for. A FAQ chatbot can ease this process by automatically retrieving the relevant answer to a user\u2019s question. We present VaccinChatNL, a Dutch FAQ corpus on the topic of COVID-19 vaccination. Starting with 50 question-answer pairs we built VaccinChat, a FAQ chatbot, which we used to gather more user questions that were also annotated with the appropriate or new answer classes. This iterative process of gathering user questions, annotating them, and retraining the model with the increased data set led to a corpus that now contains 12,883 user questions divided over 181 answers. We provide the first publicly available Dutch FAQ answering data set of this size with large groups of semantically equivalent human-paraphrased questions. Furthermore, our study shows that before fine-tuning a classifier, continued pre-training of Dutch language models with task- and/or domain-specific data improves classification results. In addition, we show that large groups of semantically similar questions are important for obtaining well-performing intent classification models.",
        "author": "Jeska Buhmann; Maxime De Bruyn; Ehsan Lotfi; Walter Daelemans",
        "authorids": "/j/jeska-buhmann/; /m/maxime-de-bruyn/; /e/ehsan-lotfi/; /w/walter-daelemans/",
        "bibtex": "@inproceedings{buhmann-etal-2022-domain,\n    title = \"Domain- and Task-Adaptation for {V}accin{C}hat{NL}, a {D}utch {COVID}-19 {FAQ} Answering Corpus and Classification Model\",\n    author = \"Buhmann, Jeska  and\n      De Bruyn, Maxime  and\n      Lotfi, Ehsan  and\n      Daelemans, Walter\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.312/\",\n    pages = \"3539--3549\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.312.pdf",
        "site": "https://aclanthology.org/2022.coling-1.312/",
        "pdf_size": 581075,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7854675062595658158&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "CLiPS Research Center, University of Antwerp, Belgium; CLiPS Research Center, University of Antwerp, Belgium; CLiPS Research Center, University of Antwerp, Belgium; CLiPS Research Center, University of Antwerp, Belgium",
        "aff_domain": "uantwerpen.be;uantwerpen.be;uantwerpen.be;uantwerpen.be",
        "email": "uantwerpen.be;uantwerpen.be;uantwerpen.be;uantwerpen.be",
        "github": "",
        "project": "https://vaccinchat.be",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Antwerp",
        "aff_unique_dep": "CLiPS Research Center",
        "aff_unique_url": "https://www.uantwerp.be/en",
        "aff_unique_abbr": "UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2022.coling-1.211",
        "title": "Domain-Specific NER via Retrieving Correlated Samples",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Successful Machine Learning based Named Entity Recognition models could fail on texts from some special domains, for instance, Chinese addresses and e-commerce titles, where requires adequate background knowledge. Such texts are also difficult for human annotators. In fact, we can obtain some potentially helpful information from correlated texts, which have some common entities, to help the text understanding. Then, one can easily reason out the correct answer by referencing correlated samples. In this paper, we suggest enhancing NER models with correlated samples. We draw correlated samples by the sparse BM25 retriever from large-scale in-domain unlabeled data. To explicitly simulate the human reasoning process, we perform a training-free entity type calibrating by majority voting. To capture correlation features in the training stage, we suggest to model correlated samples by the transformer-based multi-instance cross-encoder. Empirical results on datasets of the above two domains show the efficacy of our methods.",
        "author": "Xin Zhang; Yong Jiang; Xiaobin Wang; Xuming Hu; Yueheng Sun; Pengjun Xie; Meishan Zhang",
        "authorids": "/x/xin-zhang/; /y/yong-jiang/; /x/xiaobin-wang/; /x/xuming-hu/; /y/yueheng-sun/; /p/pengjun-xie/; /m/meishan-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2022-domain,\n    title = \"Domain-Specific {NER} via Retrieving Correlated Samples\",\n    author = \"Zhang, Xin  and\n      Jiang, Yong  and\n      Wang, Xiaobin  and\n      Hu, Xuming  and\n      Sun, Yueheng  and\n      Xie, Pengjun  and\n      Zhang, Meishan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.211/\",\n    pages = \"2398--2404\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.211.pdf",
        "site": "https://aclanthology.org/2022.coling-1.211/",
        "pdf_size": 436748,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12799397956692058478&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of New Media and Communication, Tianjin University, China; School of Software, Tsinghua University, China; School of New Media and Communication, Tianjin University, China; School of Software, Tsinghua University, China; College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China",
        "aff_domain": "tju.edu.cn;mails.tsinghua.edu.cn;gmail.com;foxmail.com;tju.edu.cn;gmail.com;hit.edu.cn",
        "email": "tju.edu.cn;mails.tsinghua.edu.cn;gmail.com;foxmail.com;tju.edu.cn;gmail.com;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;1;0;0;2",
        "aff_unique_norm": "Tianjin University;Tsinghua University;Harbin Institute of Technology",
        "aff_unique_dep": "School of New Media and Communication;School of Software;Institute of Computing and Intelligence",
        "aff_unique_url": "http://www.tju.edu.cn;https://www.tsinghua.edu.cn;http://www.hhit.edu.cn",
        "aff_unique_abbr": "Tianjin University;THU;HIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.405",
        "title": "Don\u2019t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent pre-trained language models (PLMs) achieved great success on many natural language processing tasks through learning linguistic features and contextualized sentence representation. Since attributes captured in stacked layers of PLMs are not clearly identified, straightforward approaches such as embedding the last layer are commonly preferred to derive sentence representations from PLMs. This paper introduces the attention-based pooling strategy, which enables the model to preserve layer-wise signals captured in each layer and learn digested linguistic features for downstream tasks. The contrastive learning objective can adapt the layer-wise attention pooling to both unsupervised and supervised manners. It results in regularizing the anisotropic space of pre-trained embeddings and being more uniform. We evaluate our model on standard semantic textual similarity (STS) and semantic search tasks. As a result, our method improved the performance of the base contrastive learned BERTbase and variants.",
        "author": "Dongsuk Oh; Yejin Kim; Hodong Lee; H. Howie Huang; Heuiseok Lim",
        "authorids": "/d/dongsuk-oh/; /y/yejin-kim/; /h/hodong-lee/; /h/h-howie-huang/; /h/heui-seok-lim/",
        "bibtex": "@inproceedings{oh-etal-2022-dont,\n    title = \"Don{'}t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling\",\n    author = \"Oh, Dongsuk  and\n      Kim, Yejin  and\n      Lee, Hodong  and\n      Huang, H. Howie  and\n      Lim, Heuiseok\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.405/\",\n    pages = \"4585--4592\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.405.pdf",
        "site": "https://aclanthology.org/2022.coling-1.405/",
        "pdf_size": 558827,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9784960236916184743&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science and Engineering, Korea University\u2021; Graph Lab., George Washington University\u00a7; Computer Science and Engineering, Korea University\u2021; Computer Science and Engineering, Korea University\u2021; Graph Lab., George Washington University\u2020",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;gwu.edu;gwu.edu",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;gwu.edu;gwu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "Korea University;George Washington University",
        "aff_unique_dep": "Computer Science and Engineering;Graph Lab.",
        "aff_unique_url": "https://www.korea.ac.kr;https://www.gwu.edu",
        "aff_unique_abbr": "KU;GWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2022.coling-1.409",
        "title": "DoubleMix: Simple Interpolation-Based Data Augmentation for Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper proposes a simple yet effective interpolation-based data augmentation approach termed DoubleMix, to improve the robustness of models in text classification. DoubleMix first leverages a couple of simple augmentation operations to generate several perturbed samples for each training data, and then uses the perturbed data and original data to carry out a two-step interpolation in the hidden space of neural models. Concretely, it first mixes up the perturbed data to a synthetic sample and then mixes up the original data and the synthetic perturbed data. DoubleMix enhances models\u2019 robustness by learning the \u201cshifted\u201d features in hidden space. On six text classification benchmark datasets, our approach outperforms several popular text augmentation methods including token-level, sentence-level, and hidden-level data augmentation techniques. Also, experiments in low-resource settings show our approach consistently improves models\u2019 performance when the training data is scarce. Extensive ablation studies and case studies confirm that each component of our approach contributes to the final performance and show that our approach exhibits superior performance on challenging counterexamples. Additionally, visual analysis shows that text features generated by our approach are highly interpretable.",
        "author": "Hui Chen; Wei Han; Diyi Yang; Soujanya Poria",
        "authorids": "/h/hui-chen/; /w/wei-han/; /d/diyi-yang/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{chen-etal-2022-doublemix,\n    title = \"{D}ouble{M}ix: Simple Interpolation-Based Data Augmentation for Text Classification\",\n    author = \"Chen, Hui  and\n      Han, Wei  and\n      Yang, Diyi  and\n      Poria, Soujanya\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.409/\",\n    pages = \"4622--4632\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.409.pdf",
        "site": "https://aclanthology.org/2022.coling-1.409/",
        "pdf_size": 745605,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2732513014716746214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Singapore University of Technology and Design; Singapore University of Technology and Design; Georgia Institute of Technology; Singapore University of Technology and Design",
        "aff_domain": "mymail.sutd.edu.sg;mymail.sutd.edu.sg;cc.gatech.edu;sutd.edu.sg",
        "email": "mymail.sutd.edu.sg;mymail.sutd.edu.sg;cc.gatech.edu;sutd.edu.sg",
        "github": "https://github.com/declare-lab/DoubleMix.git",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Singapore University of Technology and Design;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.gatech.edu",
        "aff_unique_abbr": "SUTD;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "2022.coling-1.500",
        "title": "Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A Visual Question Answering (VQA) model processes images and questions simultaneously with rich semantic information. The attention mechanism can highlight fine-grained features with critical information, thus ensuring that feature extraction emphasizes the objects related to the questions. However, unattended coarse-grained information is also essential for questions involving global elements. We believe that global coarse-grained information and local fine-grained information can complement each other to provide richer comprehensive information. In this paper, we propose a dual capsule attention mask network with mutual learning for VQA. Specifically, it contains two branches processing coarse-grained features and fine-grained features, respectively. We also design a novel stackable dual capsule attention module to fuse features and locate evidence. The two branches are combined to make final predictions for VQA. Experimental results show that our method outperforms the baselines in terms of VQA performance and interpretability and achieves new SOTA performance on the VQA-v2 dataset.",
        "author": "Weidong Tian; Haodong Li; Zhong-Qiu Zhao",
        "authorids": "/w/weidong-tian/; /h/haodong-li/; /z/zhong-qiu-zhao/",
        "bibtex": "@inproceedings{tian-etal-2022-dual,\n    title = \"Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering\",\n    author = \"Tian, Weidong  and\n      Li, Haodong  and\n      Zhao, Zhong-Qiu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.500/\",\n    pages = \"5678--5688\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.500.pdf",
        "site": "https://aclanthology.org/2022.coling-1.500/",
        "pdf_size": 1940918,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1328008483655146282&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "College of Computer and Information, Hefei University of Technology + Intelligent Manufacturing Institute of HFUT + Intelligent Interconnected Systems Laboratory of Anhui Province, Hefei University of Technology; College of Computer and Information, Hefei University of Technology + Intelligent Manufacturing Institute of HFUT; College of Computer and Information, Hefei University of Technology + Guangxi Academy of Science + Intelligent Manufacturing Institute of HFUT + Intelligent Interconnected Systems Laboratory of Anhui Province, Hefei University of Technology",
        "aff_domain": "hfut.edu.cn;163.com;hfut.edu.cn",
        "email": "hfut.edu.cn;163.com;hfut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0+0;0+0;0+1+0+0",
        "aff_unique_norm": "Hefei University of Technology;Guangxi Academy of Science",
        "aff_unique_dep": "College of Computer and Information;",
        "aff_unique_url": "http://www.hfut.edu.cn;http://www.gxas.cn",
        "aff_unique_abbr": "HUT;",
        "aff_campus_unique_index": "0+0;0;0+0",
        "aff_campus_unique": "Hefei;",
        "aff_country_unique_index": "0+0+0;0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.351",
        "title": "DynGL-SDP: Dynamic Graph Learning for Semantic Dependency Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A recent success in semantic dependency parsing shows that graph neural networks can make significant accuracy improvements, owing to its powerful ability in learning expressive graph representations. However, this work learns graph representations based on a static graph constructed by an existing parser, suffering from two drawbacks: (1) the static graph might be error-prone (e.g., noisy or incomplete), and (2) graph construction stage and graph representation learning stage are disjoint, the errors introduced in the graph construction stage cannot be corrected and might be accumulated to later stages. To address these two drawbacks, we propose a dynamic graph learning framework and apply it to semantic dependency parsing, for jointly learning graph structure and graph representations. Experimental results show that our parser outperforms the previous parsers on the SemEval-2015 Task 18 dataset in three languages (English, Chinese, and Czech).",
        "author": "Bin Li; Miao Gao; Yunlong Fan; Yikemaiti Sataer; Zhiqiang Gao; Yaocheng Gui",
        "authorids": "/b/bin-li/; /m/miao-gao/; /y/yunlong-fan/; /y/yikemaiti-sataer/; /z/zhiqiang-gao/; /y/yaocheng-gui/",
        "bibtex": "@inproceedings{li-etal-2022-dyngl,\n    title = \"{D}yn{GL}-{SDP}: Dynamic Graph Learning for Semantic Dependency Parsing\",\n    author = \"Li, Bin  and\n      Gao, Miao  and\n      Fan, Yunlong  and\n      Sataer, Yikemaiti  and\n      Gao, Zhiqiang  and\n      Gui, Yaocheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.351/\",\n    pages = \"3994--4004\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.351.pdf",
        "site": "https://aclanthology.org/2022.coling-1.351/",
        "pdf_size": 750606,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1751452433893213277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.21",
        "title": "Dynamic Dialogue Policy for Continual Reinforcement Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Continual learning is one of the key components of human learning and a necessary requirement of artificial intelligence. As dialogue can potentially span infinitely many topics and tasks, a task-oriented dialogue system must have the capability to continually learn, dynamically adapting to new challenges while preserving the knowledge it already acquired. Despite the importance, continual reinforcement learning of the dialogue policy has remained largely unaddressed. The lack of a framework with training protocols, baseline models and suitable metrics, has so far hindered research in this direction. In this work we fill precisely this gap, enabling research in dialogue policy optimisation to go from static to dynamic learning. We provide a continual learning algorithm, baseline architectures and metrics for assessing continual learning models. Moreover, we propose the dynamic dialogue policy transformer (DDPT), a novel dynamic architecture that can integrate new knowledge seamlessly, is capable of handling large state spaces and obtains significant zero-shot performance when being exposed to unseen domains, without any growth in network parameter size. We validate the strengths of DDPT in simulation with two user simulators as well as with humans.",
        "author": "Christian Geishauser; Carel van Niekerk; Hsien-chin Lin; Nurul Lubis; Michael Heck; Shutong Feng; Milica Ga\u0161i\u0107",
        "authorids": "/c/christian-geishauser/; /c/carel-van-niekerk/; /h/hsien-chin-lin/; /n/nurul-lubis/; /m/michael-heck/; /s/shutong-feng/; /m/milica-gasic/",
        "bibtex": "@inproceedings{geishauser-etal-2022-dynamic,\n    title = \"Dynamic Dialogue Policy for Continual Reinforcement Learning\",\n    author = \"Geishauser, Christian  and\n      van Niekerk, Carel  and\n      Lin, Hsien-chin  and\n      Lubis, Nurul  and\n      Heck, Michael  and\n      Feng, Shutong  and\n      Ga{\\v{s}}i{\\'c}, Milica\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.21/\",\n    pages = \"266--284\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.21.pdf",
        "site": "https://aclanthology.org/2022.coling-1.21/",
        "pdf_size": 1250252,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8262386395423010461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany",
        "aff_domain": "hhu.de;hhu.de;hhu.de;hhu.de;hhu.de;hhu.de;hhu.de",
        "email": "hhu.de;hhu.de;hhu.de;hhu.de;hhu.de;hhu.de;hhu.de",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Heinrich Heine University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hhu.de",
        "aff_unique_abbr": "HHU",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "D\u00fcsseldorf",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.333",
        "title": "Dynamic Nonlinear Mixup with Distance-based Sample Selection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Data augmentation with mixup has shown to be effective on the NLP tasks. Although its great success, the mixup still has shortcomings. First, vanilla mixup randomly selects one sample to generate the mixup sample for a given sample. It remains unclear how to best choose the input samples for the mixup. Second, linear interpolation limits the space of synthetic data and its regularization effect. In this paper, we propose the dynamic nonlinear mixup with distance-based sample selection, which not only generates multiple sample pairs based on the distance between each sample but also enlarges the space of synthetic samples. Specifically, we compute the distance between each input data by cosine similarity and select multiple samples for a given sample. Then we use the dynamic nonlinear mixup to fuse sample pairs. It does not use a linear, scalar mixing strategy, but a nonlinear interpolation strategy, where the mixing strategy is adaptively updated for the input and label pairs. Experiments on the multiple public datasets demonstrate that dynamic nonlinear mixup outperforms state-of-the-art methods.",
        "author": "Shaokang Zhang; Lei Jiang; Jianlong Tan",
        "authorids": "/s/shaokang-zhang/; /l/lei-jiang/; /j/jianlong-tan/",
        "bibtex": "@inproceedings{zhang-etal-2022-dynamic,\n    title = \"Dynamic Nonlinear Mixup with Distance-based Sample Selection\",\n    author = \"Zhang, Shaokang  and\n      Jiang, Lei  and\n      Tan, Jianlong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.333/\",\n    pages = \"3788--3797\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.333.pdf",
        "site": "https://aclanthology.org/2022.coling-1.333/",
        "pdf_size": 2314713,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10318134227089686352&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.450",
        "title": "Dynamic Position Encoding for Transformers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recurrent models have been dominating the field of neural machine translation (NMT) for the past few years. Transformers have radically changed it by proposing a novel architecture that relies on a feed-forward backbone and self-attention mechanism. Although Transformers are powerful, they could fail to properly encode sequential/positional information due to their non-recurrent nature. To solve this problem, position embeddings are defined exclusively for each time step to enrich word information. However, such embeddings are fixed after training regardless of the task and word ordering system of the source and target languages. In this paper, we address this shortcoming by proposing a novel architecture with new position embeddings that take the order of the target words into consideration. Instead of using predefined position embeddings, our solution generates new embeddings to refine each word\u2019s position information. Since we do not dictate the position of the source tokens and we learn them in an end-to-end fashion, we refer to our method as dynamic position encoding (DPE). We evaluated the impact of our model on multiple datasets to translate from English to German, French, and Italian and observed meaningful improvements in comparison to the original Transformer.",
        "author": "Joyce Zheng; Mehdi Rezagholizadeh; Peyman Passban",
        "authorids": "/j/joyce-zheng/; /m/mehdi-rezagholizadeh/; /p/peyman-passban/",
        "bibtex": "@inproceedings{zheng-etal-2022-dynamic,\n    title = \"Dynamic Position Encoding for Transformers\",\n    author = \"Zheng, Joyce  and\n      Rezagholizadeh, Mehdi  and\n      Passban, Peyman\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.450/\",\n    pages = \"5076--5084\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.450.pdf",
        "site": "https://aclanthology.org/2022.coling-1.450/",
        "pdf_size": 368253,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3707706992405314703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Huawei Noah\u2019s Ark Lab*; Huawei Noah\u2019s Ark Lab; Amazon\u2020",
        "aff_domain": "uwaterloo.ca;huawei.com;gmail.com",
        "email": "uwaterloo.ca;huawei.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Huawei;Amazon",
        "aff_unique_dep": "Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.huawei.com;https://www.amazon.com",
        "aff_unique_abbr": "Huawei;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.116",
        "title": "Dynamic Relevance Graph Network for Knowledge-Aware Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This work investigates the challenge of learning and reasoning for Commonsense Question Answering given an external source of knowledge in the form of a knowledge graph (KG). We propose a novel graph neural network architecture, called Dynamic Relevance Graph Network (DRGN). DRGN operates on a given KG subgraph based on the question and answers entities and uses the relevance scores between the nodes to establish new edges dynamically for learning node representations in the graph network. This explicit usage of relevance as graph edges has the following advantages, a) the model can exploit the existing relationships, re-scale the node weights, and influence the way the neighborhood nodes\u2019 representations are aggregated in the KG subgraph, b) It potentially recovers the missing edges in KG that are needed for reasoning. Moreover, as a byproduct, our model improves handling the negative questions due to considering the relevance between the question node and the graph entities. Our proposed approach shows competitive performance on two QA benchmarks, CommonsenseQA and OpenbookQA, compared to the state-of-the-art published results.",
        "author": "Chen Zheng; Parisa Kordjamshidi",
        "authorids": "/c/chen-zheng/; /p/parisa-kordjamshidi/",
        "bibtex": "@inproceedings{zheng-kordjamshidi-2022-dynamic,\n    title = \"Dynamic Relevance Graph Network for Knowledge-Aware Question Answering\",\n    author = \"Zheng, Chen  and\n      Kordjamshidi, Parisa\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.116/\",\n    pages = \"1357--1366\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.116.pdf",
        "site": "https://aclanthology.org/2022.coling-1.116/",
        "pdf_size": 562563,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5150344624044407552&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Michigan State University; Michigan State University",
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.87",
        "title": "E-VarM: Enhanced Variational Word Masks to Improve the Interpretability of Text Classification Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Enhancing the interpretability of text classification models can help increase the reliability of these models in real-world applications. Currently, most researchers focus on extracting task-specific words from inputs to improve the interpretability of the model. The competitive approaches exploit the Variational Information Bottleneck (VIB) to improve the performance of word masking at the word embedding layer to obtain task-specific words. However, these approaches ignore the multi-level semantics of the text, which can impair the interpretability of the model, and do not consider the risk of representation overlap caused by the VIB, which can impair the classification performance. In this paper, we propose an enhanced variational word masks approach, named E-VarM, to solve these two issues effectively. The E-VarM combines multi-level semantics from all hidden layers of the model to mask out task-irrelevant words and uses contrastive learning to readjust the distances between representations. Empirical studies on ten benchmark text classification datasets demonstrate that our approach outperforms the SOTA methods in simultaneously improving the interpretability and accuracy of the model.",
        "author": "Ling Ge; ChunMing Hu; Guanghui Ma; Junshuang Wu; Junfan Chen; JiHong Liu; Hong Zhang; Wenyi Qin; Richong Zhang",
        "authorids": "/l/ling-ge/; /c/chunming-hu/; /g/guanghui-ma/; /j/junshuang-wu/; /j/junfan-chen/; /j/jihong-liu/; /h/hong-zhang/; /w/wenyi-qin/; /r/richong-zhang/",
        "bibtex": "@inproceedings{ge-etal-2022-e,\n    title = \"{E}-{V}ar{M}: Enhanced Variational Word Masks to Improve the Interpretability of Text Classification Models\",\n    author = \"Ge, Ling  and\n      Hu, ChunMing  and\n      Ma, Guanghui  and\n      Wu, Junshuang  and\n      Chen, Junfan  and\n      Liu, JiHong  and\n      Zhang, Hong  and\n      Qin, Wenyi  and\n      Zhang, Richong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.87/\",\n    pages = \"1036--1050\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.87.pdf",
        "site": "https://aclanthology.org/2022.coling-1.87/",
        "pdf_size": 3871774,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16891815769838945612&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2022.coling-1.93",
        "title": "EM-PERSONA: EMotion-assisted Deep Neural Framework for PERSONAlity Subtyping from Suicide Notes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The World Health Organization has emphasised the need of stepping up suicide prevention efforts to meet the United Nation\u2019s Sustainable Development Goal target of 2030 (Goal 3: Good health and well-being). We address the challenging task of personality subtyping from suicide notes. Most research on personality subtyping has relied on statistical analysis and feature engineering. Moreover, state-of-the-art transformer models in the automated personality subtyping problem have received relatively less attention. We develop a novel EMotion-assisted PERSONAlity Detection Framework (EM-PERSONA). We annotate the benchmark CEASE-v2.0 suicide notes dataset with personality traits across four dichotomies: Introversion (I)-Extraversion (E), Intuition (N)-Sensing (S), Thinking (T)-Feeling (F), Judging (J)\u2013Perceiving (P). Our proposed method outperforms all baselines on comprehensive evaluation using multiple state-of-the-art systems. Across the four dichotomies, EM-PERSONA improved accuracy by 2.04%, 3.69%, 4.52%, and 3.42%, respectively, over the highest-performing single-task systems.",
        "author": "Soumitra Ghosh; Dhirendra Kumar Maurya; Asif Ekbal; Pushpak Bhattacharyya",
        "authorids": "/s/soumitra-ghosh/; /d/dhirendra-kumar-maurya/; /a/asif-ekbal/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{ghosh-etal-2022-em,\n    title = \"{EM}-{PERSONA}: {EM}otion-assisted Deep Neural Framework for {PERSONA}lity Subtyping from Suicide Notes\",\n    author = \"Ghosh, Soumitra  and\n      Maurya, Dhirendra Kumar  and\n      Ekbal, Asif  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.93/\",\n    pages = \"1098--1105\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.93.pdf",
        "site": "https://aclanthology.org/2022.coling-1.93/",
        "pdf_size": 497227,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13870922869560201225&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, IIT Patna, India; Department of Computer Science and Engineering, IIT Patna, India; Department of Computer Science and Engineering, IIT Patna, India; Department of Computer Science and Engineering, IIT Bombay, India",
        "aff_domain": "gmail.com;gmail.com;iitp.ac.in;cse.iitb.ac.in",
        "email": "gmail.com;gmail.com;iitp.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Indian Institute of Technology Patna;IIT Bombay",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitpatna.ac.in;https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Patna;IITB",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Patna;Bombay",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.185",
        "title": "ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level Event Causality Identification (DECI) aims to identify event-event causal relations in a document. Existing works usually build an event graph for global reasoning across multiple sentences. However, the edges between events have to be carefully designed through heuristic rules or external tools. In this paper, we propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI, to ease the graph construction and improve it over the noisy edge issue. Different from conventional event graphs, we define a pair of events as a node and build a complete event relational graph without any prior knowledge or tools. This naturally formulates DECI as a node classification problem, and thus we capture the causation transitivity among event pairs via a graph transformer. Furthermore, we design a criss-cross constraint and an adaptive focal loss for the imbalanced classification, to alleviate the issues of false positives and false negatives. Extensive experiments on two benchmark datasets show that ERGO greatly outperforms previous state-of-the-art (SOTA) methods (12.8% F1 gains on average).",
        "author": "Meiqi Chen; Yixin Cao; Kunquan Deng; Mukai Li; Kun Wang; Jing Shao; Yan Zhang",
        "authorids": "/m/meiqi-chen/; /y/yixin-cao/; /k/kunquan-deng/; /m/mukai-li/; /k/kun-wang/; /j/jing-shao/; /y/yan-zhang/",
        "bibtex": "@inproceedings{chen-etal-2022-ergo,\n    title = \"{ERGO}: Event Relational Graph Transformer for Document-level Event Causality Identification\",\n    author = \"Chen, Meiqi  and\n      Cao, Yixin  and\n      Deng, Kunquan  and\n      Li, Mukai  and\n      Wang, Kun  and\n      Shao, Jing  and\n      Zhang, Yan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.185/\",\n    pages = \"2118--2128\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.185.pdf",
        "site": "https://aclanthology.org/2022.coling-1.185/",
        "pdf_size": 1198756,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15560373161520159721&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Peking University; Singapore Management University; Beihang University; SenseTime Research; SenseTime Research; SenseTime Research; Peking University",
        "aff_domain": "stu.pku.edu.cn; ; ; ; ; ; ",
        "email": "stu.pku.edu.cn; ; ; ; ; ; ",
        "github": "https://github.com/chenmeiqii/ERGO.git",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;3;3;0",
        "aff_unique_norm": "Peking University;Singapore Management University;Beihang University;SenseTime",
        "aff_unique_dep": ";;;SenseTime Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.smu.edu.sg;http://www.buaa.edu.cn/;https://www.sensetime.com",
        "aff_unique_abbr": "Peking U;SMU;BUAA;SenseTime",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.coling-1.342",
        "title": "ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Contrastive learning has been attracting much attention for learning unsupervised sentence embeddings. The current state-of-the-art unsupervised method is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder (with dropout turned on) twice to obtain the two corresponding embeddings to build a positive pair. As the length information of a sentence will generally be encoded into the sentence embeddings due to the usage of position embedding in Transformer, each positive pair in unsup-SimCSE actually contains the same length information. And thus unsup-SimCSE trained with these positive pairs is probably biased, which would tend to consider that sentences of the same or similar length are more similar in semantics. Through statistical observations, we find that unsup-SimCSE does have such a problem. To alleviate it, we apply a simple repetition operation to modify the input sentence, and then pass the input sentence and its modified counterpart to the pre-trained Transformer encoder, respectively, to get the positive pair. Additionally, we draw inspiration from the community of computer vision and introduce a momentum contrast, enlarging the number of negative pairs without additional calculations. The proposed two modifications are applied on positive and negative pairs separately, and build a new sentence embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the proposed ESimCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that ESimCSE outperforms the state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on BERT-base.",
        "author": "Xing Wu; Chaochen Gao; Liangjun Zang; Jizhong Han; Zhongyuan Wang; Songlin Hu",
        "authorids": "/x/xing-wu/; /c/chaochen-gao/; /l/liangjun-zang/; /j/jizhong-han/; /z/zhongyuan-wang/; /s/songlin-hu/",
        "bibtex": "@inproceedings{wu-etal-2022-esimcse,\n    title = \"{ES}im{CSE}: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding\",\n    author = \"Wu, Xing  and\n      Gao, Chaochen  and\n      Zang, Liangjun  and\n      Han, Jizhong  and\n      Wang, Zhongyuan  and\n      Hu, Songlin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.342/\",\n    pages = \"3898--3907\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.342.pdf",
        "site": "https://aclanthology.org/2022.coling-1.342/",
        "pdf_size": 408330,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16248562080662565868&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China+Kuaishou Technology, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Kuaishou Technology, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;kuaishou.com;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;kuaishou.com;iie.ac.cn",
        "github": "https://github.com/caskcsg/ESimCSE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1+2;0+1;0;0;2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Kuaishou Technology",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.kuaishou.com",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0+0;0+0;0;0;0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0+0;0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.47",
        "title": "ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational machine reading comprehension (CMRC) aims to assist computers to understand an natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. Existing methods typically require three steps: (1) decision making based on entailment reasoning; (2) span extraction if required by the above decision; (3) question rephrasing based on the extracted span. However, for nearly all these methods, the span extraction and question rephrasing steps cannot fully exploit the fine-grained entailment reasoning information in decision making step because of their relative independence, which will further enlarge the information gap between decision making and question phrasing. Thus, to tackle this problem, we propose a novel end-to-end framework for conversational machine reading comprehension based on shared parameter mechanism, called entailment reasoning T5 (ET5). Despite the lightweight of our proposed framework, experimental results show that the proposed ET5 achieves new state-of-the-art results on the ShARC leaderboard with the BLEU-4 score of 55.2. Our model and code are publicly available.",
        "author": "Xiao Zhang; Heyan Huang; Zewen Chi; Xian-Ling Mao",
        "authorids": "/x/xiao-zhang/; /h/he-yan-huang/; /z/zewen-chi/; /x/xian-ling-mao/",
        "bibtex": "@inproceedings{zhang-etal-2022-et5,\n    title = \"{ET}5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension\",\n    author = \"Zhang, Xiao  and\n      Huang, Heyan  and\n      Chi, Zewen  and\n      Mao, Xian-Ling\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.47/\",\n    pages = \"570--579\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.47.pdf",
        "site": "https://aclanthology.org/2022.coling-1.47/",
        "pdf_size": 672019,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11021812569118554435&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Southeast Academy of Information Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Southeast Academy of Information Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Southeast Academy of Information Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Southeast Academy of Information Technology, Beijing Institute of Technology",
        "aff_domain": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "email": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "github": "https://github.com/Yottaxx/ET5",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0;0+1+0;0+1+0;0+1+0",
        "aff_unique_norm": "Beijing Institute of Technology;Beijing Engineering Research Center",
        "aff_unique_dep": "School of Computer Science and Technology;High Volume Language Information Processing and Cloud Computing Applications",
        "aff_unique_url": "http://www.bit.edu.cn/;",
        "aff_unique_abbr": "BIT;",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.277",
        "title": "Effect of Post-processing on Contextualized Word Representations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Post-processing of static embedding has been shown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principal components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy) and sequence classification tasks. Our findings raise interesting points in relation to the research studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.",
        "author": "Hassan Sajjad; Firoj Alam; Fahim Dalvi; Nadir Durrani",
        "authorids": "/h/hassan-sajjad/; /f/firoj-alam/; /f/fahim-dalvi/; /n/nadir-durrani/",
        "bibtex": "@inproceedings{sajjad-etal-2022-effect,\n    title = \"Effect of Post-processing on Contextualized Word Representations\",\n    author = \"Sajjad, Hassan  and\n      Alam, Firoj  and\n      Dalvi, Fahim  and\n      Durrani, Nadir\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.277/\",\n    pages = \"3127--3142\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.277.pdf",
        "site": "https://aclanthology.org/2022.coling-1.277/",
        "pdf_size": 1871137,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5217408966854332742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Faculty of Computer Science, Dalhousie University, Canada; Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar; Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar; Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar",
        "aff_domain": "dal.ca;hbku.edu.qa;hbku.edu.qa;hbku.edu.qa",
        "email": "dal.ca;hbku.edu.qa;hbku.edu.qa;hbku.edu.qa",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Dalhousie University;Hamad Bin Khalifa University",
        "aff_unique_dep": "Faculty of Computer Science;Qatar Computing Research Institute",
        "aff_unique_url": "https://www.dal.ca;https://www.qcri.org",
        "aff_unique_abbr": "Dal;HBKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Canada;Qatar"
    },
    {
        "id": "2022.coling-1.305",
        "title": "Effective Data Augmentation for Sentence Classification Using One VAE per Class",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In recent years, data augmentation has become an important field of machine learning. While images can use simple techniques such as cropping or rotating, textual data augmentation needs more complex manipulations to ensure that the generated examples are useful. Variational auto-encoders (VAE) and its conditional variant the Conditional-VAE (CVAE) are often used to generate new textual data, both relying on a good enough training of the generator so that it doesn\u2019t create examples of the wrong class. In this paper, we explore a simpler way to use VAE for data augmentation: the training of one VAE per class. We show on several dataset sizes, as well as on four different binary classification tasks, that it systematically outperforms other generative data augmentation techniques.",
        "author": "Fr\u00e9d\u00e9ric Piedboeuf; Philippe Langlais",
        "authorids": "/f/frederic-piedboeuf/; /p/philippe-langlais/",
        "bibtex": "@inproceedings{piedboeuf-langlais-2022-effective,\n    title = \"Effective Data Augmentation for Sentence Classification Using One {VAE} per Class\",\n    author = \"Piedboeuf, Fr{\\'e}d{\\'e}ric  and\n      Langlais, Philippe\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.305/\",\n    pages = \"3454--3464\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.305.pdf",
        "site": "https://aclanthology.org/2022.coling-1.305/",
        "pdf_size": 626544,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3925920917200743162&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "DIRO, RALI, Universit\u00e9 de Montr\u00e9al; DIRO, RALI, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "DIRO, RALI",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.504",
        "title": "Efficient Multilingual Multi-modal Pre-training through Triple Contrastive Loss",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Learning visual and textual representations in the shared space from web-scale image-text pairs improves the performance of diverse vision-and-language tasks, as well as modality-specific tasks. Many attempts in this framework have been made to connect English-only texts and images, and only a few works have been proposed to extend this framework in multilingual settings with the help of many translation pairs. In this multilingual approach, a typical setup is to use pairs of (image and English-text) and translation pairs. The major limitation of this approach is that the learning signal of aligning visual representation with under-resourced language representation is not strong, achieving a sub-optimal performance of vision-and-language tasks. In this work, we propose a simple yet effective enhancement scheme for previous multilingual multi-modal representation methods by using a limited number of pairs of images and non-English texts. In specific, our scheme fine-tunes a pre-trained multilingual model by minimizing a triplet contrastive loss on triplets of image and two different language texts with the same meaning, improving the connection between images and non-English texts. Experiments confirm that our enhancement strategy achieves performance gains in image-text retrieval, zero-shot image classification, and sentence embedding tasks.",
        "author": "Youhan Lee; KyungTae Lim; Woonhyuk Baek; Byungseok Roh; Saehoon Kim",
        "authorids": "/y/youhan-lee/; /k/kyungtae-lim/; /w/woonhyuk-baek/; /b/byungseok-roh/; /s/saehoon-kim/",
        "bibtex": "@inproceedings{lee-etal-2022-efficient,\n    title = \"Efficient Multilingual Multi-modal Pre-training through Triple Contrastive Loss\",\n    author = \"Lee, Youhan  and\n      Lim, KyungTae  and\n      Baek, Woonhyuk  and\n      Roh, Byungseok  and\n      Kim, Saehoon\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.504/\",\n    pages = \"5730--5744\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.504.pdf",
        "site": "https://aclanthology.org/2022.coling-1.504/",
        "pdf_size": 831899,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6467935232914330455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2022.coling-1.501",
        "title": "Emergence of Hierarchical Reference Systems in Multi-agent Communication",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In natural language, referencing objects at different levels of specificity is a fundamental pragmatic mechanism for efficient communication in context. We develop a novel communication game, the hierarchical reference game, to study the emergence of such reference systems in artificial agents. We consider a simplified world, in which concepts are abstractions over a set of primitive attributes (e.g., color, style, shape). Depending on how many attributes are combined, concepts are more general (\u201ccircle\u201d) or more specific (\u201cred dotted circle\u201d). Based on the context, the agents have to communicate at different levels of this hierarchy. Our results show that the agents learn to play the game successfully and can even generalize to novel concepts. To achieve abstraction, they use implicit (omitting irrelevant information) and explicit (indicating that attributes are irrelevant) strategies. In addition, the compositional structure underlying the concept hierarchy is reflected in the emergent protocols, indicating that the need to develop hierarchical reference systems supports the emergence of compositionality.",
        "author": "Xenia Ohmer; Marko Duda; Elia Bruni",
        "authorids": "/x/xenia-ohmer/; /m/marko-duda/; /e/elia-bruni/",
        "bibtex": "@inproceedings{ohmer-etal-2022-emergence,\n    title = \"Emergence of Hierarchical Reference Systems in Multi-agent Communication\",\n    author = \"Ohmer, Xenia  and\n      Duda, Marko  and\n      Bruni, Elia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.501/\",\n    pages = \"5689--5706\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.501.pdf",
        "site": "https://aclanthology.org/2022.coling-1.501/",
        "pdf_size": 1806078,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6778159609953476004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Osnabr\u00fcck University; Osnabr\u00fcck University; Osnabr\u00fcck University",
        "aff_domain": "uos.de;uos.de;uos.de",
        "email": "uos.de;uos.de;uos.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Osnabr\u00fcck",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-osnabrueck.de",
        "aff_unique_abbr": "UOS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.609",
        "title": "EmoMent: An Emotion Annotated Mental Health Corpus from Two South Asian Countries",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "People often utilise online media (e.g., Facebook, Reddit) as a platform to express their psychological distress and seek support. State-of-the-art NLP techniques demonstrate strong potential to automatically detect mental health issues from text. Research suggests that mental health issues are reflected in emotions (e.g., sadness) indicated in a person\u2019s choice of language. Therefore, we developed a novel emotion-annotated mental health corpus (EmoMent),consisting of 2802 Facebook posts (14845 sentences) extracted from two South Asian countries - Sri Lanka and India. Three clinical psychology postgraduates were involved in annotating these posts into eight categories, including \u2018mental illness\u2019 (e.g., depression) and emotions (e.g., \u2018sadness\u2019, \u2018anger\u2019). EmoMent corpus achieved \u2018very good\u2019 inter-annotator agreement of 98.3% (i.e. % with two or more agreement) and Fleiss\u2019 Kappa of 0.82. Our RoBERTa based models achieved an F1 score of 0.76 and a macro-averaged F1 score of 0.77 for the first task (i.e. predicting a mental health condition from a post) and the second task (i.e. extent of association of relevant posts with the categories defined in our taxonomy), respectively.",
        "author": "Thushari Atapattu; Mahen Herath; Charitha Elvitigala; Piyanjali de Zoysa; Kasun Gunawardana; Menasha Thilakaratne; Kasun de Zoysa; Katrina Falkner",
        "authorids": "/t/thushari-atapattu/; /m/mahen-herath/; /c/charitha-elvitigala/; /p/piyanjali-de-zoysa/; /k/kasun-gunawardana/; /m/menasha-thilakaratne/; /k/kasun-de-zoysa/; /k/katrina-falkner/",
        "bibtex": "@inproceedings{atapattu-etal-2022-emoment,\n    title = \"{E}mo{M}ent: An Emotion Annotated Mental Health Corpus from Two {S}outh {A}sian Countries\",\n    author = \"Atapattu, Thushari  and\n      Herath, Mahen  and\n      Elvitigala, Charitha  and\n      de Zoysa, Piyanjali  and\n      Gunawardana, Kasun  and\n      Thilakaratne, Menasha  and\n      de Zoysa, Kasun  and\n      Falkner, Katrina\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.609/\",\n    pages = \"6991--7001\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.609.pdf",
        "site": "https://aclanthology.org/2022.coling-1.609/",
        "pdf_size": 630885,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8382193122456356937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, The University of Adelaide, Australia; Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka; University of Colombo School of Computing, Sri Lanka; Department of Psychology, University of Colombo, Sri Lanka; University of Colombo School of Computing, Sri Lanka; School of Computer Science, The University of Adelaide, Australia; University of Colombo School of Computing, Sri Lanka; School of Computer Science, The University of Adelaide, Australia",
        "aff_domain": "adelaide.edu.au; ; ; ; ; ; ; ",
        "email": "adelaide.edu.au; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;2;2;0;2;0",
        "aff_unique_norm": "The University of Adelaide;University of Moratuwa;University of Colombo",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science & Engineering;School of Computing",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.mrt.ac.lk;https://www.soc.lk",
        "aff_unique_abbr": "Adelaide;;UoC",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Colombo",
        "aff_country_unique_index": "0;1;1;1;1;0;1;0",
        "aff_country_unique": "Australia;Sri Lanka"
    },
    {
        "id": "2022.coling-1.363",
        "title": "Emotion Enriched Retrofitted Word Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Word embeddings learned using the distributional hypothesis (e.g., GloVe, Word2vec) are good at encoding various lexical-semantic relations. However, they do not capture the emotion aspects of words. We present a novel retrofitting method for updating the vectors of emotion bearing words like fun, offence, angry, etc. The retrofitted embeddings achieve better inter-cluster and intra-cluster distance for words having the same emotions, e.g., the joy cluster containing words like fun, happiness, etc., and the anger cluster with words like offence, rage, etc., as evaluated through different cluster quality metrics. For the downstream tasks on sentiment analysis and sarcasm detection, simple classification models, such as SVM and Attention Net, learned using our retrofitted embeddings perform better than their pre-trained counterparts (about 1.5 % improvement in F1-score) as well as other benchmarks. Furthermore, the difference in performance is more pronounced in the limited data setting.",
        "author": "Sapan Shah; Sreedhar Reddy; Pushpak Bhattacharyya",
        "authorids": "/s/sapan-shah/; /s/sreedhar-reddy/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{shah-etal-2022-emotion,\n    title = \"Emotion Enriched Retrofitted Word Embeddings\",\n    author = \"Shah, Sapan  and\n      Reddy, Sreedhar  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.363/\",\n    pages = \"4136--4148\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.363.pdf",
        "site": "https://aclanthology.org/2022.coling-1.363/",
        "pdf_size": 5153396,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6618148680869288347&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "TCS Research, Tata Consultancy Services, Pune + Indian Institute of Technology Bombay, Mumbai; TCS Research, Tata Consultancy Services, Pune; Indian Institute of Technology Bombay, Mumbai",
        "aff_domain": "tcs.com;tcs.com;cse.iitb.ac.in",
        "email": "tcs.com;tcs.com;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Tata Consultancy Services;Indian Institute of Technology Bombay",
        "aff_unique_dep": "TCS Research;",
        "aff_unique_url": "https://www.tcs.com;https://www.iitb.ac.in",
        "aff_unique_abbr": "TCS;IIT Bombay",
        "aff_campus_unique_index": "0+1;0;1",
        "aff_campus_unique": "Pune;Mumbai",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.64",
        "title": "End-to-End Neural Bridging Resolution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The state of bridging resolution research is rather unsatisfactory: not only are state-of-the-art resolvers evaluated in unrealistic settings, but the neural models underlying these resolvers are weaker than those used for entity coreference resolution. In light of these problems, we evaluate bridging resolvers in an end-to-end setting, strengthen them with better encoders, and attempt to gain a better understanding of them via perturbation experiments and a manual analysis of their outputs.",
        "author": "Hideo Kobayashi; Yufang Hou; Vincent Ng",
        "authorids": "/h/hideo-kobayashi/; /y/yufang-hou/; /v/vincent-ng/",
        "bibtex": "@inproceedings{kobayashi-etal-2022-end,\n    title = \"End-to-End Neural Bridging Resolution\",\n    author = \"Kobayashi, Hideo  and\n      Hou, Yufang  and\n      Ng, Vincent\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.64/\",\n    pages = \"766--778\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.64.pdf",
        "site": "https://aclanthology.org/2022.coling-1.64/",
        "pdf_size": 403200,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5280536191527238443&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Human Language Technology Research Institute, University of Texas at Dallas, USA; IBM Research Europe, Ireland; Human Language Technology Research Institute, University of Texas at Dallas, USA",
        "aff_domain": "hlt.utdallas.edu;ie.ibm.com;hlt.utdallas.edu",
        "email": "hlt.utdallas.edu;ie.ibm.com;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Texas at Dallas;IBM Research Europe",
        "aff_unique_dep": "Human Language Technology Research Institute;",
        "aff_unique_url": "https://www.utdallas.edu;https://www.ibm.com/research/europe",
        "aff_unique_abbr": "UT Dallas;IBM RE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dallas;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Ireland"
    },
    {
        "id": "2022.coling-1.498",
        "title": "End-to-end Dense Video Captioning as Sequence Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dense video captioning aims to identify the events of interest in an input video, and generate descriptive captions for each event. Previous approaches usually follow a two-stage generative process, which first proposes a segment for each event, then renders a caption for each identified segment. Recent advances in large-scale sequence generation pretraining have seen great success in unifying task formulation for a great variety of tasks, but so far, more complex tasks such as dense video captioning are not able to fully utilize this powerful paradigm. In this work, we show how to model the two subtasks of dense video captioning jointly as one sequence generation task, and simultaneously predict the events and the corresponding descriptions. Experiments on YouCook2 and ViTT show encouraging results and indicate the feasibility of training complex tasks such as end-to-end dense video captioning integrated into large-scale pretrained models.",
        "author": "Wanrong Zhu; Bo Pang; Ashish V. Thapliyal; William Yang Wang; Radu Soricut",
        "authorids": "/w/wanrong-zhu/; /b/bo-pang/; /a/ashish-v-thapliyal/; /w/william-yang-wang/; /r/radu-soricut/",
        "bibtex": "@inproceedings{zhu-etal-2022-end,\n    title = \"End-to-end Dense Video Captioning as Sequence Generation\",\n    author = \"Zhu, Wanrong  and\n      Pang, Bo  and\n      Thapliyal, Ashish V.  and\n      Wang, William Yang  and\n      Soricut, Radu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.498/\",\n    pages = \"5651--5665\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.498.pdf",
        "site": "https://aclanthology.org/2022.coling-1.498/",
        "pdf_size": 2097560,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5673072724739668582&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "UC Santa Barbara; Google Research; Google Research; UC Santa Barbara; Google Research",
        "aff_domain": "cs.ucsb.edu;google.com;google.com;cs.ucsb.edu;google.com",
        "email": "cs.ucsb.edu;google.com;google.com;cs.ucsb.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of California, Santa Barbara;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.ucsb.edu;https://research.google",
        "aff_unique_abbr": "UCSB;Google Research",
        "aff_campus_unique_index": "0;1;1;0;1",
        "aff_campus_unique": "Santa Barbara;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.281",
        "title": "Enhancing Contextual Word Representations Using Embedding of Neighboring Entities in Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) such as BERT and RoBERTa have dramatically improved the performance of various natural language processing tasks. Although these models are trained on large amounts of raw text, they have no explicit grounding in real-world entities. Knowledge graphs (KGs) are manually annotated with factual knowledge and store the relations between nodes corresponding to entities as labeled edges. This paper proposes a mechanism called KG-attention, which integrates the structure of a KG into recent PLM architectures. Unlike the existing PLM+KG integration methods, KG-attention generalizes the embeddings of neighboring entities using the relation embeddings; accordingly, it can handle relations between unconnected entities in the KG. Experimental results demonstrated that our method achieved significant improvements in a relation classification task, an entity typing task, and several language comprehension tasks.",
        "author": "Ryoko Tokuhisa; Keisuke Kawano; Akihiro Nakamura; Satoshi Koide",
        "authorids": "/r/ryoko-tokuhisa/; /k/keisuke-kawano/; /a/akihiro-nakamura/; /s/satoshi-koide/",
        "bibtex": "@inproceedings{tokuhisa-etal-2022-enhancing,\n    title = \"Enhancing Contextual Word Representations Using Embedding of Neighboring Entities in Knowledge Graphs\",\n    author = \"Tokuhisa, Ryoko  and\n      Kawano, Keisuke  and\n      Nakamura, Akihiro  and\n      Koide, Satoshi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.281/\",\n    pages = \"3175--3186\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.281.pdf",
        "site": "https://aclanthology.org/2022.coling-1.281/",
        "pdf_size": 2788806,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10854795291098751751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.571",
        "title": "Enhancing Pre-trained Models with Text Structure Knowledge for Question Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Today the pre-trained language models achieve great success for question generation (QG) task and significantly outperform traditional sequence-to-sequence approaches. However, the pre-trained models treat the input passage as a flat sequence and are thus not aware of the text structure of input passage. For QG task, we model text structure as answer position and syntactic dependency, and propose answer localness modeling and syntactic mask attention to address these limitations. Specially, we present localness modeling with a Gaussian bias to enable the model to focus on answer-surrounded context, and propose a mask attention mechanism to make the syntactic structure of input passage accessible in question generation process. Experiments on SQuAD dataset show that our proposed two modules improve performance over the strong pre-trained model ProphetNet, and combing them together achieves very competitive results with the state-of-the-art pre-trained model.",
        "author": "Zichen Wu; Xin Jia; Fanyi Qu; Yunfang Wu",
        "authorids": "/z/zichen-wu/; /x/xin-jia/; /f/fanyi-qu/; /y/yunfang-wu/",
        "bibtex": "@inproceedings{wu-etal-2022-enhancing,\n    title = \"Enhancing Pre-trained Models with Text Structure Knowledge for Question Generation\",\n    author = \"Wu, Zichen  and\n      Jia, Xin  and\n      Qu, Fanyi  and\n      Wu, Yunfang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.571/\",\n    pages = \"6564--6574\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.571.pdf",
        "site": "https://aclanthology.org/2022.coling-1.571/",
        "pdf_size": 478860,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16899523291584810760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Key Laboratory of Computational Linguistics, Ministry of Education, China+School of Computer Science, Peking University, China; Key Laboratory of Computational Linguistics, Ministry of Education, China+School of Computer Science, Peking University, China; Key Laboratory of Computational Linguistics, Ministry of Education, China+School of Computer Science, Peking University, China; Key Laboratory of Computational Linguistics, Ministry of Education, China+School of Computer Science, Peking University, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Key Laboratory of Computational Linguistics;Peking University",
        "aff_unique_dep": "Ministry of Education;School of Computer Science",
        "aff_unique_url": ";http://www.pku.edu.cn",
        "aff_unique_abbr": ";Peking U",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.483",
        "title": "Enhancing Structure-aware Encoder with Extremely Limited Data for Graph-based Dependency Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dependency parsing is an important fundamental natural language processing task which analyzes the syntactic structure of an input sentence by illustrating the syntactic relations between words. To improve dependency parsing, leveraging existing dependency parsers and extra data (e.g., through semi-supervised learning) has been demonstrated to be effective, even though the final parsers are trained on inaccurate (but massive) data. In this paper, we propose a frustratingly easy approach to improve graph-based dependency parsing, where a structure-aware encoder is pre-trained on auto-parsed data by predicting the word dependencies and then fine-tuned on gold dependency trees, which differs from the usual pre-training process that aims to predict the context words along dependency paths. Experimental results and analyses demonstrate the effectiveness and robustness of our approach to benefit from the data (even with noise) processed by different parsers, where our approach outperforms strong baselines under different settings with different dependency standards and model architectures used in pre-training and fine-tuning. More importantly, further analyses find that only 2K auto-parsed sentences are required to obtain improvement when pre-training vanilla BERT-large based parser without requiring extra parameters.",
        "author": "Yuanhe Tian; Yan Song; Fei Xia",
        "authorids": "/y/yuanhe-tian/; /y/yan-song/; /f/fei-xia/",
        "bibtex": "@inproceedings{tian-etal-2022-enhancing,\n    title = \"Enhancing Structure-aware Encoder with Extremely Limited Data for Graph-based Dependency Parsing\",\n    author = \"Tian, Yuanhe  and\n      Song, Yan  and\n      Xia, Fei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.483/\",\n    pages = \"5438--5449\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.483.pdf",
        "site": "https://aclanthology.org/2022.coling-1.483/",
        "pdf_size": 525685,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7456022643343107727&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/synlp/DMPar",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.520",
        "title": "Enhancing Task-Specific Distillation in Small Data Regimes through Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large-scale pretrained language models have led to significant improvements in Natural Language Processing. Unfortunately, they come at the cost of high computational and storage requirements that complicate their deployment on low-resource devices. This issue can be addressed by distilling knowledge from larger models to smaller ones through pseudo-labels on task-specific datasets. However, this can be difficult for tasks with very limited data. To overcome this challenge, we present a novel approach where knowledge can be distilled from a teacher model to a student model through the generation of synthetic data. For this to be done, we first fine-tune the teacher and student models, as well as a Natural Language Generation (NLG) model, on the target task dataset. We then let both student and teacher work together to condition the NLG model to generate examples that can enhance the performance of the student. We tested our approach on two data generation methods: a) Targeted generation using the Monte Carlo Tree Search (MCTS) algorithm, and b) A Non-Targeted Text Generation (NTTG) method. We evaluate the effectiveness of our approaches against a baseline that uses the BERT model for data augmentation through random word replacement. By testing this approach on the SST-2, MRPC, YELP-2, DBpedia, and TREC-6 datasets, we consistently witnessed considerable improvements over the word-replacement baseline.",
        "author": "Husam Quteineh; Spyridon Samothrakis; Richard Sutcliffe",
        "authorids": "/h/husam-quteineh/; /s/spyridon-samothrakis/; /r/richard-sutcliffe/",
        "bibtex": "@inproceedings{quteineh-etal-2022-enhancing,\n    title = \"Enhancing Task-Specific Distillation in Small Data Regimes through Language Generation\",\n    author = \"Quteineh, Husam  and\n      Samothrakis, Spyridon  and\n      Sutcliffe, Richard\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.520/\",\n    pages = \"5955--5965\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.520.pdf",
        "site": "https://aclanthology.org/2022.coling-1.520/",
        "pdf_size": 633715,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8217513676042020917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.589",
        "title": "Entity-Level Sentiment Analysis (ELSA): An Exploratory Task Survey",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores the task of identifying the overall sentiment expressed towards volitional entities (persons and organizations) in a document - what we refer to as Entity-Level Sentiment Analysis (ELSA). While identifying sentiment conveyed towards an entity is well researched for shorter texts like tweets, we find little to no research on this specific task for longer texts with multiple mentions and opinions towards the same entity. This lack of research would be understandable if ELSA can be derived from existing tasks and models. To assess this, we annotate a set of professional reviews for their overall sentiment towards each volitional entity in the text. We sample from data already annotated for document-level, sentence-level, and target-level sentiment in a multi-domain review corpus, and our results indicate that there is no single proxy task that provides this overall sentiment we seek for the entities at a satisfactory level of performance. We present a suite of experiments aiming to assess the contribution towards ELSA provided by document-, sentence-, and target-level sentiment analysis, and provide a discussion of their shortcomings. We show that sentiment in our dataset is expressed not only with an entity mention as target, but also towards targets with a sentiment-relevant relation to a volitional entity. In our data, these relations extend beyond anaphoric coreference resolution, and our findings call for further research of the topic. Finally, we also present a survey of previous relevant work.",
        "author": "Egil R\u00f8nningstad; Erik Velldal; Lilja \u00d8vrelid",
        "authorids": "/e/egil-ronningstad/; /e/erik-velldal/; /l/lilja-ovrelid/",
        "bibtex": "@inproceedings{ronningstad-etal-2022-entity,\n    title = \"Entity-Level Sentiment Analysis ({ELSA}): An Exploratory Task Survey\",\n    author = \"R{\\o}nningstad, Egil  and\n      Velldal, Erik  and\n      {\\O}vrelid, Lilja\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.589/\",\n    pages = \"6773--6783\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.589.pdf",
        "site": "https://aclanthology.org/2022.coling-1.589/",
        "pdf_size": 374236,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1905563453596723430&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Oslo; University of Oslo; University of Oslo",
        "aff_domain": "ifi.uio.no;ifi.uio.no;ifi.uio.no",
        "email": "ifi.uio.no;ifi.uio.no;ifi.uio.no",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Oslo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uio.no",
        "aff_unique_abbr": "UiO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Norway"
    },
    {
        "id": "2022.coling-1.412",
        "title": "Equivariant Transduction through Invariant Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The ability to generalize compositionally is key to understanding the potentially infinite number of sentences that can be constructed in a human language from only a finite number of words. Investigating whether NLP models possess this ability has been a topic of interest: SCAN (Lake and Baroni, 2018) is one task specifically proposed to test for this property. Previous work has achieved impressive empirical results using a group-equivariant neural network that naturally encodes a useful inductive bias for SCAN (Gordon et al., 2020). Inspired by this, we introduce a novel group-equivariant architecture that incorporates a group-invariant hard alignment mechanism. We find that our network\u2019s structure allows it to develop stronger equivariance properties than existing group-equivariant approaches. We additionally find that it outperforms previous group-equivariant networks empirically on the SCAN task. Our results suggest that integrating group-equivariance into a variety of neural architectures is a potentially fruitful avenue of research, and demonstrate the value of careful analysis of the theoretical properties of such architectures.",
        "author": "Jennifer C. White; Ryan Cotterell",
        "authorids": "/j/jennifer-c-white/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{white-cotterell-2022-equivariant,\n    title = \"Equivariant Transduction through Invariant Alignment\",\n    author = \"White, Jennifer C.  and\n      Cotterell, Ryan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.412/\",\n    pages = \"4651--4663\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.412.pdf",
        "site": "https://aclanthology.org/2022.coling-1.412/",
        "pdf_size": 485699,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9424740604477226741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Cambridge; ETH Z\u00fcrich",
        "aff_domain": "cam.ac.uk;inf.ethz.ch",
        "email": "cam.ac.uk;inf.ethz.ch",
        "github": "https://github.com/rycolab/equivariant-transduction",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;ETH Z\u00fcrich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;ETHZ",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2022.coling-1.322",
        "title": "Establishing Annotation Quality in Multi-label Annotations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In many linguistic fields requiring annotated data, multiple interpretations of a single item are possible. Multi-label annotations more accurately reflect this possibility. However, allowing for multi-label annotations also affects the chance that two coders agree with each other. Calculating inter-coder agreement for multi-label datasets is therefore not trivial. In the current contribution, we evaluate different metrics for calculating agreement on multi-label annotations: agreement on the intersection of annotated labels, an augmented version of Cohen\u2019s Kappa, and precision, recall and F1. We propose a bootstrapping method to obtain chance agreement for each measure, which allows us to obtain an adjusted agreement coefficient that is more interpretable. We demonstrate how various measures affect estimates of agreement on simulated datasets and present a case study of discourse relation annotations. We also show how the proportion of double labels, and the entropy of the label distribution, influences the measures outlined above and how a bootstrapped adjusted agreement can make agreement measures more comparable across datasets in multi-label scenarios.",
        "author": "Marian Marchal; Merel Scholman; Frances Yung; Vera Demberg",
        "authorids": "/m/marian-marchal/; /m/merel-scholman/; /f/frances-yung/; /v/vera-demberg/",
        "bibtex": "@inproceedings{marchal-etal-2022-establishing,\n    title = \"Establishing Annotation Quality in Multi-label Annotations\",\n    author = \"Marchal, Marian  and\n      Scholman, Merel  and\n      Yung, Frances  and\n      Demberg, Vera\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.322/\",\n    pages = \"3659--3668\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.322.pdf",
        "site": "https://aclanthology.org/2022.coling-1.322/",
        "pdf_size": 293725,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4519479186752408271&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Language Science and Technology / Saarland University; Language Science and Technology / Saarland University; Language Science and Technology / Saarland University; Language Science and Technology / Saarland University",
        "aff_domain": "coli.uni-saarland.de;coli.uni-saarland.de;coli.uni-saarland.de;coli.uni-saarland.de",
        "email": "coli.uni-saarland.de;coli.uni-saarland.de;coli.uni-saarland.de;coli.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "Language Science and Technology",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "Uni Saar",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.398",
        "title": "Eureka: Neural Insight Learning for Knowledge Graph Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The human recognition system has presented the remarkable ability to effortlessly learn novel knowledge from only a few trigger events based on prior knowledge, which is called insight learning. Mimicking such behavior on Knowledge Graph Reasoning (KGR) is an interesting and challenging research problem with many practical applications. Simultaneously, existing works, such as knowledge embedding and few-shot learning models, have been limited to conducting KGR in either \u201cseen-to-seen\u201d or \u201cunseen-to-unseen\u201d scenarios. To this end, we propose a neural insight learning framework named Eureka to bridge the \u201cseen\u201d to \u201cunseen\u201d gap. Eureka is empowered to learn the seen relations with sufficient training triples while providing the flexibility of learning unseen relations given only one trigger without sacrificing its performance on seen relations. Eureka meets our expectation of the model to acquire seen and unseen relations at no extra cost, and eliminate the need to retrain when encountering emerging unseen relations. Experimental results on two real-world datasets demonstrate that the proposed framework also outperforms various state-of-the-art baselines on datasets of both seen and unseen relations.",
        "author": "Alex X. Zhang; Xun Liang; Bo Wu; Xiangping Zheng; Sensen Zhang; Yuhui Guo; Jun Wang; Xinyao Liu",
        "authorids": "/a/alex-x-zhang/; /x/xun-liang/; /b/bo-wu/; /x/xiangping-zheng/; /s/sensen-zhang/; /y/yuhui-guo/; /j/jun-wang/; /x/xinyao-liu/",
        "bibtex": "@inproceedings{zhang-etal-2022-eureka,\n    title = \"Eureka: Neural Insight Learning for Knowledge Graph Reasoning\",\n    author = \"Zhang, Alex X.  and\n      Liang, Xun  and\n      Wu, Bo  and\n      Zheng, Xiangping  and\n      Zhang, Sensen  and\n      Guo, Yuhui  and\n      Wang, Jun  and\n      Liu, Xinyao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.398/\",\n    pages = \"4517--4527\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.398.pdf",
        "site": "https://aclanthology.org/2022.coling-1.398/",
        "pdf_size": 248950,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_BgquDZj84wJ:scholar.google.com/&scioq=Eureka:+Neural+Insight+Learning+for+Knowledge+Graph+Reasoning&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China; Swinburne University of Technology; Hong Kong University of Science and Technology",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;swin.edu.au;163.com",
        "email": "ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;ruc.edu.cn;swin.edu.au;163.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;1;2",
        "aff_unique_norm": "Renmin University of China;Swinburne University of Technology;Hong Kong University of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.swinburne.edu.au;https://www.ust.hk",
        "aff_unique_abbr": "RUC;SUT;HKUST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.coling-1.290",
        "title": "Evaluating Diversity of Multiword Expressions in Annotated Text",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Diversity can be decomposed into three distinct concepts, namely: variety, balance and disparity. This paper borrows from the extensive formalization and measures of diversity developed in ecology in order to evaluate the variety and balance of multiword expression annotation produced by automatic annotation systems. The measures of richness, normalized richness, and two variations of Hill\u2019s evenness are considered in this paper. We observe how these measures behave against increasingly smaller samples of gold annotations of multiword expressions and use their comportment to validate or invalidate their pertinence for multiword expressions in annotated texts. We apply the validated measures to annotations in 14 languages produced by systems during the PARSEME shared task on automatic identification of multiword expressions and on the gold versions of the corpora. We also explore the limits of such evaluation by studying the impact of lemmatization errors in the Turkish corpus used in the shared task.",
        "author": "Adam Lion-Bouton; Yagmur Ozturk; Agata Savary; Jean-Yves Antoine",
        "authorids": "/a/adam-lion-bouton/; /y/yagmur-ozturk/; /a/agata-savary/; /j/jean-yves-antoine/",
        "bibtex": "@inproceedings{lion-bouton-etal-2022-evaluating,\n    title = \"Evaluating Diversity of Multiword Expressions in Annotated Text\",\n    author = \"Lion-Bouton, Adam  and\n      Ozturk, Yagmur  and\n      Savary, Agata  and\n      Antoine, Jean-Yves\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.290/\",\n    pages = \"3285--3295\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.290.pdf",
        "site": "https://aclanthology.org/2022.coling-1.290/",
        "pdf_size": 558155,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18307172878089318719&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Tours - LIFAT; Paris-Saclay University, CNRS - LISN; Paris-Saclay University, CNRS - LISN; University of Tours - LIFAT",
        "aff_domain": "gmail.com;gmail.com;universite-paris-saclay.fr;univ-tours.fr",
        "email": "gmail.com;gmail.com;universite-paris-saclay.fr;univ-tours.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Tours;Paris-Saclay University",
        "aff_unique_dep": "LIFAT;CNRS - LISN",
        "aff_unique_url": "https://www.univ-tours.fr;https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": ";Paris-Saclay",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.coling-1.393",
        "title": "Evaluating Word Embeddings in Extremely Under-Resourced Languages: A Case Study in Bribri",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Word embeddings are critical for numerous NLP tasks but their evaluation in actual under-resourced settings needs further examination. This paper presents a case study in Bribri, a Chibchan language from Costa Rica. Four experiments were adapted from English: Word similarities, WordSim353 correlations, odd-one-out tasks and analogies. Here we discuss their adaptation to an under-resourced Indigenous language and we use them to measure semantic and morphological learning. We trained 96 word2vec models with different hyperparameter combinations. The best models for this under-resourced scenario were Skip-grams with an intermediate size (100 dimensions) and large window sizes (10). These had an average correlation of r=0.28 with WordSim353, a 76% accuracy in semantic odd-one-out and 70% accuracy in structural/morphological odd-one-out. The performance was lower for the analogies: The best models could find the appropriate semantic target amongst the first 25 results approximately 60% of the times, but could only find the morphological/structural target 11% of the times. Future research needs to further explore the patterns of morphological/structural learning, to examine the behavior of deep learning embeddings, and to establish a human baseline. This project seeks to improve Bribri NLP and ultimately help in its maintenance and revitalization.",
        "author": "Rolando Coto-Solano",
        "authorids": "/r/rolando-coto-solano/",
        "bibtex": "@inproceedings{coto-solano-2022-evaluating,\n    title = \"Evaluating Word Embeddings in Extremely Under-Resourced Languages: A Case Study in {B}ribri\",\n    author = \"Coto-Solano, Rolando\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.393/\",\n    pages = \"4455--4467\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.393.pdf",
        "site": "https://aclanthology.org/2022.coling-1.393/",
        "pdf_size": 467805,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2808757630693921111&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Dartmouth College",
        "aff_domain": "dartmouth.edu",
        "email": "dartmouth.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Dartmouth College",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.dartmouth.edu",
        "aff_unique_abbr": "Dartmouth",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.124",
        "title": "Evaluating and Mitigating Inherent Linguistic Bias of African American English through Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent studies show that NLP models trained on standard English texts tend to produce biased outcomes against underrepresented English varieties. In this work, we conduct a pioneering study of the English variety use of African American English (AAE) in NLI task. First, we propose CodeSwitch, a greedy unidirectional morphosyntactically-informed rule-based translation method for data augmentation. Next, we use CodeSwitch to present a preliminary study to determine if demographic language features do in fact influence models to produce false predictions. Then, we conduct experiments on two popular datasets and propose two simple, yet effective and generalizable debiasing methods. Our findings show that NLI models (e.g. BERT) trained under our proposed frameworks outperform traditional large language models while maintaining or even improving the prediction performance. In addition, we intend to release CodeSwitch, in hopes of promoting dialectal language diversity in training data to both reduce the discriminatory societal impacts and improve model robustness of downstream NLP tasks.",
        "author": "Jamell Dacon; Haochen Liu; Jiliang Tang",
        "authorids": "/j/jamell-dacon/; /h/haochen-liu/; /j/jiliang-tang/",
        "bibtex": "@inproceedings{dacon-etal-2022-evaluating,\n    title = \"Evaluating and Mitigating Inherent Linguistic Bias of {A}frican {A}merican {E}nglish through Inference\",\n    author = \"Dacon, Jamell  and\n      Liu, Haochen  and\n      Tang, Jiliang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.124/\",\n    pages = \"1442--1454\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.124.pdf",
        "site": "https://aclanthology.org/2022.coling-1.124/",
        "pdf_size": 465578,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9696884964720143304&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Michigan State University; Michigan State University; Michigan State University",
        "aff_domain": "msu.edu;msu.edu;msu.edu",
        "email": "msu.edu;msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.301",
        "title": "Evaluating the Performance of Transformer-based Language Models for Neuroatypical Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Difficulties with social aspects of language are among the hallmarks of autism spectrum disorder (ASD). These communication differences are thought to contribute to the challenges that adults with ASD experience when seeking employment, underscoring the need for interventions that focus on improving areas of weakness in pragmatic and social language. In this paper, we describe a transformer-based framework for identifying linguistic features associated with social aspects of communication using a corpus of conversations between adults with and without ASD and neurotypical conversational partners produced while engaging in collaborative tasks. While our framework yields strong accuracy overall, performance is significantly worse for the language of participants with ASD, suggesting that they use a more diverse set of strategies for some social linguistic functions. These results, while showing promise for the development of automated language analysis tools to support targeted language interventions for ASD, also reveal weaknesses in the ability of large contextualized language models to model neuroatypical language.",
        "author": "Duanchen Liu; Zoey Liu; Qingyun Yang; Yujing Huang; Emily Prud\u2019hommeaux",
        "authorids": "/d/duanchen-liu/; /z/zoey-liu/; /q/qingyun-yang/; /y/yujing-huang/; /e/emily-prudhommeaux/",
        "bibtex": "@inproceedings{liu-etal-2022-evaluating,\n    title = \"Evaluating the Performance of Transformer-based Language Models for Neuroatypical Language\",\n    author = \"Liu, Duanchen  and\n      Liu, Zoey  and\n      Yang, Qingyun  and\n      Huang, Yujing  and\n      Prud{'}hommeaux, Emily\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.301/\",\n    pages = \"3412--3419\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.301.pdf",
        "site": "https://aclanthology.org/2022.coling-1.301/",
        "pdf_size": 232085,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15352870524670858841&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Boston College, Chestnut Hill MA, USA+MIT, Cambridge MA, USA; Department of Computer Science, Boston College, Chestnut Hill MA, USA; Department of Computer Science, Boston College, Chestnut Hill MA, USA+Cornell Tech, New York NY, USA; Department of Computer Science, Boston College, Chestnut Hill MA, USA; Department of Computer Science, Boston College, Chestnut Hill MA, USA",
        "aff_domain": "mit.edu;bc.edu;bc.edu;bc.edu;cornell.edu",
        "email": "mit.edu;bc.edu;bc.edu;bc.edu;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0+2;0;0",
        "aff_unique_norm": "Boston College;Massachusetts Institute of Technology;Cornell Tech",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.bostoncollege.edu;https://web.mit.edu;https://tech.cornell.edu",
        "aff_unique_abbr": "BC;MIT;CT",
        "aff_campus_unique_index": "0+1;0;0+2;0;0",
        "aff_campus_unique": "Chestnut Hill;Cambridge;New York",
        "aff_country_unique_index": "0+0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.201",
        "title": "Event Causality Extraction with Event Argument Correlations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event Causality Identification (ECI), which aims to detect whether a causality relation exists between two given textual events, is an important task for event causality understanding. However, the ECI task ignores crucial event structure and cause-effect causality component information, making it struggle for downstream applications. In this paper, we introduce a novel task, namely Event Causality Extraction (ECE), aiming to extract the cause-effect event causality pairs with their structured event information from plain texts. The ECE task is more challenging since each event can contain multiple event arguments, posing fine-grained correlations between events to decide the cause-effect event pair. Hence, we propose a method with a dual grid tagging scheme to capture the intra- and inter-event argument correlations for ECE. Further, we devise a event type-enhanced model architecture to realize the dual grid tagging scheme. Experiments demonstrate the effectiveness of our method, and extensive analyses point out several future directions for ECE.",
        "author": "Shiyao Cui; Jiawei Sheng; Xin Cong; Quangang Li; Tingwen Liu; Jinqiao Shi",
        "authorids": "/s/shiyao-cui/; /j/jiawei-sheng/; /x/xin-cong/; /q/quangang-li/; /t/tingwen-liu/; /j/jinqiao-shi/",
        "bibtex": "@inproceedings{cui-etal-2022-event,\n    title = \"Event Causality Extraction with Event Argument Correlations\",\n    author = \"Cui, Shiyao  and\n      Sheng, Jiawei  and\n      Cong, Xin  and\n      Li, Quangang  and\n      Liu, Tingwen  and\n      Shi, Jinqiao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.201/\",\n    pages = \"2300--2312\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.201.pdf",
        "site": "https://aclanthology.org/2022.coling-1.201/",
        "pdf_size": 700713,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4622795657841944183&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Beijing University of Posts and Telecommunications + Institute of Information Engineering, Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;bupt.edu.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;1;0;2+0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;http://www.bupt.edu.cn/",
        "aff_unique_abbr": "CAS;UCAS;BUPT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.200",
        "title": "Event Causality Identification via Derivative Prompt Joint Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper studies event causality identification, which aims at predicting the causality relation for a pair of events in a sentence. Regarding event causality identification as a supervised classification task, most existing methods suffer from the problem of insufficient annotated data. In this paper, we propose a new derivative prompt joint learning model for event causality identification, which leverages potential causal knowledge in the pre-trained language model to tackle the data scarcity problem. Specifically, rather than external data or knowledge augmentation, we derive two relevant prompt tasks from event causality identification to enhance the model\u2019s ability to identify explicit and implicit causality. We evaluate our model on two benchmark datasets and the results show that our model has great advantages over previous methods.",
        "author": "Shirong Shen; Heng Zhou; Tongtong Wu; Guilin Qi",
        "authorids": "/s/shirong-shen/; /h/heng-zhou/; /t/tongtong-wu/; /g/guilin-qi/",
        "bibtex": "@inproceedings{shen-etal-2022-event,\n    title = \"Event Causality Identification via Derivative Prompt Joint Learning\",\n    author = \"Shen, Shirong  and\n      Zhou, Heng  and\n      Wu, Tongtong  and\n      Qi, Guilin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.200/\",\n    pages = \"2288--2299\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.200.pdf",
        "site": "https://aclanthology.org/2022.coling-1.200/",
        "pdf_size": 606492,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10186659081367315407&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Southeast University, China; School of Computer Science and Engineering, Southeast University, China; School of Computer Science and Engineering, Southeast University, China; School of Computer Science and Engineering, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.172",
        "title": "Event Detection with Dual Relational Graph Attention Networks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event detection, which aims to identify instances of specific event types from pieces of text, is a fundamental task in information extraction. Most existing approaches leverage syntactic knowledge with a set of syntactic relations to enhance event detection. However, a side effect of these syntactic-based approaches is that they may confuse different syntactic relations and tend to introduce redundant or noisy information, which may lead to performance degradation. To this end, we propose a simple yet effective model named DualGAT (Dual Relational Graph Attention Networks), which exploits the complementary nature of syntactic and semantic relations to alleviate the problem. Specifically, we first construct a dual relational graph that both aggregates syntactic and semantic relations to the key nodes in the graph, so that event-relevant information can be comprehensively captured from multiple perspectives (i.e., syntactic and semantic views). We then adopt augmented relational graph attention networks to encode the graph and optimize its attention weights by introducing contextual information, which further improves the performance of event detection. Extensive experiments conducted on the standard ACE2005 benchmark dataset indicate that our method significantly outperforms the state-of-the-art methods and verifies the superiority of DualGAT over existing syntactic-based methods.",
        "author": "Jiaxin Mi; Po Hu; Peng Li",
        "authorids": "/j/jiaxin-mi/; /p/po-hu/; /p/peng-li/",
        "bibtex": "@inproceedings{mi-etal-2022-event,\n    title = \"Event Detection with Dual Relational Graph Attention Networks\",\n    author = \"Mi, Jiaxin  and\n      Hu, Po  and\n      Li, Peng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.172/\",\n    pages = \"1979--1989\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.172.pdf",
        "site": "https://aclanthology.org/2022.coling-1.172/",
        "pdf_size": 3717668,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5099170117919673268&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning, Central China Normal University, Wuhan, Hubei, China+School of Computer Science, Central China Normal University, Wuhan, Hubei, China+National Language Resources Monitoring & Research Center for Network Media, Central China Normal University, Wuhan, Hubei, China; Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning, Central China Normal University, Wuhan, Hubei, China+School of Computer Science, Central China Normal University, Wuhan, Hubei, China+National Language Resources Monitoring & Research Center for Network Media, Central China Normal University, Wuhan, Hubei, China; Institute for AI Industry Research (AIR), Tsinghua University, China",
        "aff_domain": "mail.ccnu.edu.cn;mail.ccnu.edu.cn;air.tsinghua.edu.cn",
        "email": "mail.ccnu.edu.cn;mail.ccnu.edu.cn;air.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0+0;0+0+0;1",
        "aff_unique_norm": "Central China Normal University;Tsinghua University",
        "aff_unique_dep": "Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning;Institute for AI Industry Research (AIR)",
        "aff_unique_url": "http://www.ccnu.edu.cn;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CCNU;Tsinghua",
        "aff_campus_unique_index": "0+0+0;0+0+0",
        "aff_campus_unique": "Wuhan;",
        "aff_country_unique_index": "0+0+0;0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.625",
        "title": "Event Extraction in Video Transcripts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event extraction (EE) is one of the fundamental tasks for information extraction whose goal is to identify mentions of events and their participants in text. Due to its importance, different methods and datasets have been introduced for EE. However, existing EE datasets are limited to formally written documents such as news articles or scientific papers. As such, the challenges of EE in informal and noisy texts are not adequately studied. In particular, video transcripts constitute an important domain that can benefit tremendously from EE systems (e.g., video retrieval), but has not been studied in EE literature due to the lack of necessary datasets. To address this limitation, we propose the first large-scale EE dataset obtained for transcripts of streamed videos on the video hosting platform Behance to promote future research in this area. In addition, we extensively evaluate existing state-of-the-art EE methods on our new dataset. We demonstrate that such systems cannot achieve adequate performance on the proposed dataset, revealing challenges and opportunities for further research effort.",
        "author": "Amir Pouran Ben Veyseh; Viet Dac Lai; Franck Dernoncourt; Thien Huu Nguyen",
        "authorids": "/a/amir-pouran-ben-veyseh/; /v/viet-dac-lai/; /f/franck-dernoncourt/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{veyseh-etal-2022-event,\n    title = \"Event Extraction in Video Transcripts\",\n    author = \"Veyseh, Amir Pouran Ben  and\n      Lai, Viet Dac  and\n      Dernoncourt, Franck  and\n      Nguyen, Thien Huu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.625/\",\n    pages = \"7156--7165\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.625.pdf",
        "site": "https://aclanthology.org/2022.coling-1.625/",
        "pdf_size": 907259,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:x7ErySk1GmUJ:scholar.google.com/&scioq=Event+Extraction+in+Video+Transcripts&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, University of Oregon, OR, USA; Department of Computer Science, University of Oregon, OR, USA; Adobe Research, Seattle, WA, USA; Department of Computer Science, University of Oregon, OR, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu;adobe.com;cs.uoregon.edu",
        "email": "cs.uoregon.edu;cs.uoregon.edu;adobe.com;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Oregon;Adobe Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uoregon.edu;https://research.adobe.com",
        "aff_unique_abbr": "UO;Adobe",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.317",
        "title": "Evons: A Dataset for Fake and Real News Virality Analysis and Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a novel collection of news articles originating from fake and real news media sources for the analysis and prediction of news virality. Unlike existing fake news datasets which either contain claims, or news article headline and body, in this collection each article is supported with a Facebook engagement count which we consider as an indicator of the article virality. In addition we also provide the article description and thumbnail image with which the article was shared on Facebook. These images were automatically annotated with object tags and color attributes. Using cloud based vision analysis tools, thumbnail images were also analyzed for faces and detected faces were annotated with facial attributes. We empirically investigate the use of this collection on an example task of article virality prediction.",
        "author": "Kriste Krstovski; Angela Soomin Ryu; Bruce Kogut",
        "authorids": "/k/kriste-krstovski/; /a/angela-soomin-ryu/; /b/bruce-kogut/",
        "bibtex": "@inproceedings{krstovski-etal-2022-evons,\n    title = \"Evons: A Dataset for Fake and Real News Virality Analysis and Prediction\",\n    author = \"Krstovski, Kriste  and\n      Ryu, Angela Soomin  and\n      Kogut, Bruce\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.317/\",\n    pages = \"3589--3596\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.317.pdf",
        "site": "https://aclanthology.org/2022.coling-1.317/",
        "pdf_size": 248826,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4016212660350241031&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Columbia Business School, Columbia University + Data Science Institute, Columbia University; Columbia Business School, Columbia University; Columbia Business School, Columbia University",
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu;columbia.edu",
        "github": "https://github.com/krstovski/evons",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Columbia Business School",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.156",
        "title": "Exploiting Hybrid Semantics of Relation Paths for Multi-hop Question Answering over Knowledge Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Answering natural language questions on knowledge graphs (KGQA) remains a great challenge in terms of understanding complex questions via multi-hop reasoning. Previous efforts usually exploit large-scale entity-related text corpus or knowledge graph (KG) embeddings as auxiliary information to facilitate answer selection. However, the rich semantics implied in off-the-shelf relation paths between entities is far from well explored. This paper proposes improving multi-hop KGQA by exploiting relation paths\u2019 hybrid semantics. Specifically, we integrate explicit textual information and implicit KG structural features of relation paths based on a novel rotate-and-scale entity link prediction framework. Extensive experiments on three existing KGQA datasets demonstrate the superiority of our method, especially in multi-hop scenarios. Further investigation confirms our method\u2019s systematical coordination between questions and relation paths to identify answer entities.",
        "author": "Zile Qiao; Wei Ye; Tong Zhang; Tong Mo; Weiping Li; Shikun Zhang",
        "authorids": "/z/zile-qiao/; /w/wei-ye/; /t/tong-zhang/; /t/tong-mo/; /w/weiping-li/; /s/shikun-zhang/",
        "bibtex": "@inproceedings{qiao-etal-2022-exploiting,\n    title = \"Exploiting Hybrid Semantics of Relation Paths for Multi-hop Question Answering over Knowledge Graphs\",\n    author = \"Qiao, Zile  and\n      Ye, Wei  and\n      Zhang, Tong  and\n      Mo, Tong  and\n      Li, Weiping  and\n      Zhang, Shikun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.156/\",\n    pages = \"1813--1822\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.156.pdf",
        "site": "https://aclanthology.org/2022.coling-1.156/",
        "pdf_size": 852438,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13538736581110548145&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.621",
        "title": "Exploiting Sentiment and Common Sense for Zero-shot Stance Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The stance detection task aims to classify the stance toward given documents and topics. Since the topics can be implicit in documents and unseen in training data for zero-shot settings, we propose to boost the transferability of the stance detection model by using sentiment and commonsense knowledge, which are seldom considered in previous studies. Our model includes a graph autoencoder module to obtain commonsense knowledge and a stance detection module with sentiment and commonsense. Experimental results show that our model outperforms the state-of-the-art methods on the zero-shot and few-shot benchmark dataset\u2013VAST. Meanwhile, ablation studies prove the significance of each module in our model. Analysis of the relations between sentiment, common sense, and stance indicates the effectiveness of sentiment and common sense.",
        "author": "Yun Luo; Zihan Liu; Yuefeng Shi; Stan Z. Li; Yue Zhang",
        "authorids": "/y/yun-luo/; /z/zihan-liu/; /y/yuefeng-shi/; /s/stan-z-li/; /y/yue-zhang/",
        "bibtex": "@inproceedings{luo-etal-2022-exploiting,\n    title = \"Exploiting Sentiment and Common Sense for Zero-shot Stance Detection\",\n    author = \"Luo, Yun  and\n      Liu, Zihan  and\n      Shi, Yuefeng  and\n      Li, Stan Z.  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.621/\",\n    pages = \"7112--7123\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.621.pdf",
        "site": "https://aclanthology.org/2022.coling-1.621/",
        "pdf_size": 604865,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1254368012317038685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science And Technology, Zhejiang University; School of Engineering, Westlake University; School of Engineering, Westlake University; School of Engineering, Westlake University; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1+2",
        "aff_unique_norm": "Zhejiang University;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": "School of Computer Science And Technology;School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;WIAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.617",
        "title": "Exploiting Unlabeled Data for Target-Oriented Opinion Words Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Target-oriented Opinion Words Extraction (TOWE) is a fine-grained sentiment analysis task that aims to extract the corresponding opinion words of a given opinion target from the sentence. Recently, deep learning approaches have made remarkable progress on this task. Nevertheless, the TOWE task still suffers from the scarcity of training data due to the expensive data annotation process. Limited labeled data increase the risk of distribution shift between test data and training data. In this paper, we propose exploiting massive unlabeled data to reduce the risk by increasing the exposure of the model to varying distribution shifts. Specifically, we propose a novel Multi-Grained Consistency Regularization (MGCR) method to make use of unlabeled data and design two filters specifically for TOWE to filter noisy data at different granularity. Extensive experimental results on four TOWE benchmark datasets indicate the superiority of MGCR compared with current state-of-the-art methods. The in-depth analysis also demonstrates the effectiveness of the different-granularity filters.",
        "author": "Yidong Wang; Hao Wu; Ao Liu; Wenxin Hou; Zhen Wu; Jindong Wang; Takahiro Shinozaki; Manabu Okumura; Yue Zhang",
        "authorids": "/y/yidong-wang/; /h/hao-wu/; /a/ao-liu/; /w/wenxin-hou/; /z/zhen-wu/; /j/jindong-wang/; /t/takahiro-shinozaki/; /m/manabu-okumura/; /y/yue-zhang/",
        "bibtex": "@inproceedings{wang-etal-2022-exploiting,\n    title = \"Exploiting Unlabeled Data for Target-Oriented Opinion Words Extraction\",\n    author = \"Wang, Yidong  and\n      Wu, Hao  and\n      Liu, Ao  and\n      Hou, Wenxin  and\n      Wu, Zhen  and\n      Wang, Jindong  and\n      Shinozaki, Takahiro  and\n      Okumura, Manabu  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.617/\",\n    pages = \"7075--7085\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.617.pdf",
        "site": "https://aclanthology.org/2022.coling-1.617/",
        "pdf_size": 427849,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6304063751925579907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology; Microsoft STCA; Nanjing University; Microsoft Research Asia; Tokyo Institute of Technology; Tokyo Institute of Technology; Westlake University",
        "aff_domain": "gmail.com;gmail.com; ; ;nju.edu.cn; ; ; ; ",
        "email": "gmail.com;gmail.com; ; ;nju.edu.cn; ; ; ; ",
        "github": "https://github.com/TOWESSL/TOWESSL",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;1;2;3;0;0;4",
        "aff_unique_norm": "Tokyo Institute of Technology;Microsoft;Nanjing University;Microsoft Research;Westlake University",
        "aff_unique_dep": ";STCA;;Research;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.microsoft.com;https://www.nju.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.westlake.edu.cn",
        "aff_unique_abbr": "Titech;Microsoft;Nanjing U;MSR Asia;WU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;1;2;2;0;0;2",
        "aff_country_unique": "Japan;United States;China"
    },
    {
        "id": "2022.coling-1.95",
        "title": "Exploring Label Hierarchy in a Generative Way for Hierarchical Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hierarchical Text Classification (HTC), which aims to predict text labels organized in hierarchical space, is a significant task lacking in investigation in natural language processing. Existing methods usually encode the entire hierarchical structure and fail to construct a robust label-dependent model, making it hard to make accurate predictions on sparse lower-level labels and achieving low Macro-F1. In this paper, we explore the level dependency and path dependency of the label hierarchy in a generative way for building the knowledge of upper-level labels of current path into lower-level ones, and thus propose a novel PAAM-HiA-T5 model for HTC: a hierarchy-aware T5 model with path-adaptive attention mechanism. Specifically, we generate a multi-level sequential label structure to exploit hierarchical dependency across different levels with Breadth-First Search (BFS) and T5 model. To further improve label dependency prediction within each path, we then propose an original path-adaptive attention mechanism (PAAM) to lead the model to adaptively focus on the path where the currently generated label is located, shielding the noise from other paths. Comprehensive experiments on three benchmark datasets show that PAAM-HiA-T5 greatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.",
        "author": "Wei Huang; Chen Liu; Bo Xiao; Yihua Zhao; Zhaoming Pan; Zhimin Zhang; Xinyun Yang; Guiquan Liu",
        "authorids": "/w/wei-huang/; /c/chen-liu/; /b/bo-xiao/; /y/yihua-zhao/; /z/zhaoming-pan/; /z/zhimin-zhang/; /x/xinyun-yang/; /g/guiquan-liu/",
        "bibtex": "@inproceedings{huang-etal-2022-exploring,\n    title = \"Exploring Label Hierarchy in a Generative Way for Hierarchical Text Classification\",\n    author = \"Huang, Wei  and\n      Liu, Chen  and\n      Xiao, Bo  and\n      Zhao, Yihua  and\n      Pan, Zhaoming  and\n      Zhang, Zhimin  and\n      Yang, Xinyun  and\n      Liu, Guiquan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.95/\",\n    pages = \"1116--1127\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.95.pdf",
        "site": "https://aclanthology.org/2022.coling-1.95/",
        "pdf_size": 967164,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1235052799033559406&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; NetEase Media Technology (Beijing) Co., Ltd. + University of Science and Technology of China; NetEase Media Technology (Beijing) Co., Ltd.; NetEase Media Technology (Beijing) Co., Ltd.; NetEase Media Technology (Beijing) Co., Ltd.; NetEase Media Technology (Beijing) Co., Ltd.; NetEase Media Technology (Beijing) Co., Ltd.; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;corp.netease.com;corp.netease.com;corp.netease.com;corp.netease.com;corp.netease.com;corp.netease.com;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;corp.netease.com;corp.netease.com;corp.netease.com;corp.netease.com;corp.netease.com;corp.netease.com;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1+0;1;1;1;1;1;0",
        "aff_unique_norm": "University of Science and Technology of China;NetEase Media Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.163.com",
        "aff_unique_abbr": "USTC;NetEase",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.16",
        "title": "Exploring Semantic Spaces for Detecting Clustering and Switching in Verbal Fluency",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this work, we explore the fitness of various word/concept representations in analyzing an experimental verbal fluency dataset providing human responses to 10 different category enumeration tasks. Based on human annotations of so-called clusters and switches between sub-categories in the verbal fluency sequences, we analyze whether lexical semantic knowledge represented in word embedding spaces (GloVe, fastText, ConceptNet, BERT) is suitable for detecting these conceptual clusters and switches within and across different categories. Our results indicate that ConceptNet embeddings, a distributional semantics method enriched with taxonomical relations, outperforms other semantic representations by a large margin. Moreover, category-specific analysis suggests that individual thresholds per category are more suited for the analysis of clustering and switching in particular embedding sub-space instead of a one-fits-all cross-category solution. The results point to interesting directions for future work on probing word embedding models on the verbal fluency task.",
        "author": "\u00d6zge Alacam; Simeon Sch\u00fcz; Martin Wegrzyn; Johanna Ki\u00dfler; Sina Zarrie\u00df",
        "authorids": "/o/ozge-alacam/; /s/simeon-junker/; /m/martin-wegrzyn/; /j/johanna-kissler/; /s/sina-zarriess/",
        "bibtex": "@inproceedings{alacam-etal-2022-exploring,\n    title = \"Exploring Semantic Spaces for Detecting Clustering and Switching in Verbal Fluency\",\n    author = {Alacam, {\\\"O}zge  and\n      Sch{\\\"u}z, Simeon  and\n      Wegrzyn, Martin  and\n      Ki{\\ss}ler, Johanna  and\n      Zarrie{\\ss}, Sina},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.16/\",\n    pages = \"178--191\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.16.pdf",
        "site": "https://aclanthology.org/2022.coling-1.16/",
        "pdf_size": 576027,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1922970320351422606&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Computational Linguistics; Computational Linguistics; Department of Psychology; Department of Psychology; Computational Linguistics",
        "aff_domain": "uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de",
        "email": "uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de;uni-bielefeld.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Computational Linguistics;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computational Linguistics;Department of Psychology",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.coling-1.335",
        "title": "Extracting a Knowledge Base of COVID-19 Events from Social Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a manually annotated corpus of 10,000 tweets containing public reports of five COVID-19 events, including positive and negative tests, deaths, denied access to testing, claimed cures and preventions. We designed slot-filling questions for each event type and annotated a total of 28 fine-grained slots, such as the location of events, recent travel, and close contacts. We show that our corpus can support fine-tuning BERT-based classifiers to automatically extract publicly reported events, which can be further collected for building a knowledge base. Our knowledge base is constructed over Twitter data covering two years and currently covers over 4.2M events. It can answer complex queries with high precision, such as \u201cWhich organizations have employees that tested positive in Philadelphia?\u201d We believe our proposed methodology could be quickly applied to develop knowledge bases for new domains in response to an emerging crisis, including natural disasters or future disease outbreaks.",
        "author": "Shi Zong; Ashutosh Baheti; Wei Xu; Alan Ritter",
        "authorids": "/s/shi-zong/; /a/ashutosh-baheti/; /w/wei-xu/; /a/alan-ritter/",
        "bibtex": "@inproceedings{zong-etal-2022-extracting,\n    title = \"Extracting a Knowledge Base of {COVID}-19 Events from Social Media\",\n    author = \"Zong, Shi  and\n      Baheti, Ashutosh  and\n      Xu, Wei  and\n      Ritter, Alan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.335/\",\n    pages = \"3810--3823\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.335.pdf",
        "site": "https://aclanthology.org/2022.coling-1.335/",
        "pdf_size": 3018339,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3910670171931262682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Waterloo; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "uwaterloo.ca;gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "uwaterloo.ca;gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "https://github.com/viczong/extract_COVID19_events_from_Twitter",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Waterloo;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://uwaterloo.ca;https://www.gatech.edu",
        "aff_unique_abbr": "UW;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.coling-1.63",
        "title": "Extractive Summarisation for German-language Data: A Text-level Approach with Discourse Features",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We examine the link between facets of Rhetorical Structure Theory (RST) and the selection of content for extractive summarisation, for German-language texts. For this purpose, we produce a set of extractive summaries for a dataset of German-language newspaper commentaries, a corpus which already has several layers of annotation. We provide an in-depth analysis of the connection between summary sentences and several RST-based features and transfer these insights to various automated summarisation models. Our results show that RST features are informative for the task of extractive summarisation, particularly nuclearity and relations at sentence-level.",
        "author": "Freya Hewett; Manfred Stede",
        "authorids": "/f/freya-hewett/; /m/manfred-stede/",
        "bibtex": "@inproceedings{hewett-stede-2022-extractive,\n    title = \"Extractive Summarisation for {G}erman-language Data: A Text-level Approach with Discourse Features\",\n    author = \"Hewett, Freya  and\n      Stede, Manfred\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.63/\",\n    pages = \"756--765\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.63.pdf",
        "site": "https://aclanthology.org/2022.coling-1.63/",
        "pdf_size": 1561149,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7666148914340024751&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "AI & Society Lab, Humboldt Institute for Internet and Society, Berlin, Germany + Applied Computational Linguistics, University of Potsdam, Potsdam, Germany; Applied Computational Linguistics, University of Potsdam, Potsdam, Germany",
        "aff_domain": "hiig.de;uni-potsdam.de",
        "email": "hiig.de;uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Humboldt Institute for Internet and Society;University of Potsdam",
        "aff_unique_dep": "AI & Society Lab;Applied Computational Linguistics",
        "aff_unique_url": ";https://www.uni-potsdam.de",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Berlin;Potsdam",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.476",
        "title": "FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot Named Entity Recognition (NER) is imperative for entity tagging in limited resource domains and thus received proper attention in recent years. Existing approaches for few-shot NER are evaluated mainly under in-domain settings. In contrast, little is known about how these inherently faithful models perform in cross-domain NER using a few labeled in-domain examples. This paper proposes a two-step rationale-centric data augmentation method to improve the model\u2019s generalization ability. Results on several datasets show that our model-agnostic method significantly improves the performance of cross-domain NER tasks compared to previous state-of-the-art methods compared to the counterfactual data augmentation and prompt-tuning methods.",
        "author": "Linyi Yang; Lifan Yuan; Leyang Cui; Wenyang Gao; Yue Zhang",
        "authorids": "/l/linyi-yang/; /l/lifan-yuan/; /l/leyang-cui/; /w/wenyang-gao/; /y/yue-zhang/",
        "bibtex": "@inproceedings{yang-etal-2022-factmix,\n    title = \"{F}act{M}ix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition\",\n    author = \"Yang, Linyi  and\n      Yuan, Lifan  and\n      Cui, Leyang  and\n      Gao, Wenyang  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.476/\",\n    pages = \"5360--5371\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.476.pdf",
        "site": "https://aclanthology.org/2022.coling-1.476/",
        "pdf_size": 1009960,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8079455770342741613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; Huazhong University of Science and Technology + Institute of Advanced Technology, Westlake Institute for Advanced Study; Tencent AI Lab; School of Engineering, Westlake University; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;gmail.com;tencent.com;westlake.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;gmail.com;tencent.com;westlake.edu.cn;westlake.edu.cn",
        "github": "https://github.com/lifan-yuan/FactMix",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2+1;3;0;0+1",
        "aff_unique_norm": "Westlake University;Westlake Institute for Advanced Study;Huazhong University of Science and Technology;Tencent",
        "aff_unique_dep": "School of Engineering;Institute of Advanced Technology;;Tencent AI Lab",
        "aff_unique_url": "https://www.westlake.edu.cn;http://www.wias.org.cn/;http://www.hust.edu.cn;https://ai.tencent.com",
        "aff_unique_abbr": ";WIAS;HUST;Tencent AI Lab",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.376",
        "title": "Fashioning Local Designs from Generic Speech Technologies in an Australian Aboriginal Community",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "An increasing number of papers have been addressing issues related to low-resource languages and the transcription bottleneck paradigm. After several years spent in Northern Australia, where some of the strongest Aboriginal languages are spoken, we could observe a gap between the motivations depicted in research contributions in this space and the Northern Australian context. In this paper, we address this gap in research by exploring the potential of speech recognition in an Aboriginal community. We describe our work from training a spoken term detection system to its implementation in an activity with Aboriginal participants. We report here on one side how speech recognition technologies can find their place in an Aboriginal context and, on the other, methodological paths that allowed us to reach better comprehension and engagement from Aboriginal participants.",
        "author": "\u00c9ric Le Ferrand; Steven Bird; Laurent Besacier",
        "authorids": "/e/eric-le-ferrand/; /s/steven-bird/; /l/laurent-besacier/",
        "bibtex": "@inproceedings{le-ferrand-etal-2022-fashioning,\n    title = \"Fashioning Local Designs from Generic Speech Technologies in an {A}ustralian Aboriginal Community\",\n    author = \"Le Ferrand, {\\'E}ric  and\n      Bird, Steven  and\n      Besacier, Laurent\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.376/\",\n    pages = \"4274--4285\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.376.pdf",
        "site": "https://aclanthology.org/2022.coling-1.376/",
        "pdf_size": 2255374,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6110967826239308537&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Northern Institute, Charles Darwin University, Australia+Laboratoire Informatique de Grenoble, Universit\u00e9 Grenoble Alpes, France; Northern Institute, Charles Darwin University, Australia; Laboratoire Informatique de Grenoble, Universit\u00e9 Grenoble Alpes, France",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Charles Darwin University;Universit\u00e9 Grenoble Alpes",
        "aff_unique_dep": ";Laboratoire Informatique de Grenoble",
        "aff_unique_url": "https://www.cdu.edu.au;https://www.univ-grenoble-alpes.fr",
        "aff_unique_abbr": "CDU;UGA",
        "aff_campus_unique_index": "0+1;0;1",
        "aff_campus_unique": "Northern Institute;Grenoble",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "Australia;France"
    },
    {
        "id": "2022.coling-1.365",
        "title": "Fast and Accurate End-to-End Span-based Semantic Role Labeling as Word-based Graph Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper proposes to cast end-to-end span-based SRL as a word-based graph parsing task. The major challenge is how to represent spans at the word level. Borrowing ideas from research on Chinese word segmentation and named entity recognition, we propose and compare four different schemata of graph representation, i.e., BES, BE, BIES, and BII, among which we find that the BES schema performs the best. We further gain interesting insights through detailed analysis. Moreover, we propose a simple constrained Viterbi procedure to ensure the legality of the output graph according to the constraints of the SRL structure. We conduct experiments on two widely used benchmark datasets, i.e., CoNLL05 and CoNLL12. Results show that our word-based graph parsing approach achieves consistently better performance than previous results, under all settings of end-to-end and predicate-given, without and with pre-trained language models (PLMs). More importantly, our model can parse 669/252 sentences per second, without and with PLMs respectively.",
        "author": "Shilin Zhou; Qingrong Xia; Zhenghua Li; Yu Zhang; Yu Hong; Min Zhang",
        "authorids": "/s/shilin-zhou/; /q/qingrong-xia/; /z/zhenghua-li/; /y/yu-zhang/; /y/yu-hong/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhou-etal-2022-fast,\n    title = \"Fast and Accurate End-to-End Span-based Semantic Role Labeling as Word-based Graph Parsing\",\n    author = \"Zhou, Shilin  and\n      Xia, Qingrong  and\n      Li, Zhenghua  and\n      Zhang, Yu  and\n      Hong, Yu  and\n      Zhang, Min\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.365/\",\n    pages = \"4160--4171\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.365.pdf",
        "site": "https://aclanthology.org/2022.coling-1.365/",
        "pdf_size": 593391,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17228705507453381457&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "outlook.com;outlook.com;gmail.com;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "outlook.com;outlook.com;gmail.com;suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Suzhou",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.443",
        "title": "FeatureBART: Feature Based Sequence-to-Sequence Pre-Training for Low-Resource NMT",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper we present FeatureBART, a linguistically motivated sequence-to-sequence monolingual pre-training strategy in which syntactic features such as lemma, part-of-speech and dependency labels are incorporated into the span prediction based pre-training framework (BART). These automatically extracted features are incorporated via approaches such as concatenation and relevance mechanisms, among which the latter is known to be better than the former. When used for low-resource NMT as a downstream task, we show that these feature based models give large improvements in bilingual settings and modest ones in multilingual settings over their counterparts that do not use features.",
        "author": "Abhisek Chakrabarty; Raj Dabre; Chenchen Ding; Hideki Tanaka; Masao Utiyama; Eiichiro Sumita",
        "authorids": "/a/abhisek-chakrabarty/; /r/raj-dabre/; /c/chenchen-ding/; /h/hideki-tanaka/; /m/masao-utiyama/; /e/eiichiro-sumita/",
        "bibtex": "@inproceedings{chakrabarty-etal-2022-featurebart,\n    title = \"{F}eature{BART}: Feature Based Sequence-to-Sequence Pre-Training for Low-Resource {NMT}\",\n    author = \"Chakrabarty, Abhisek  and\n      Dabre, Raj  and\n      Ding, Chenchen  and\n      Tanaka, Hideki  and\n      Utiyama, Masao  and\n      Sumita, Eiichiro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.443/\",\n    pages = \"5014--5020\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.443.pdf",
        "site": "https://aclanthology.org/2022.coling-1.443/",
        "pdf_size": 376583,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12987345485060622626&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "National Institute of Information and Communications Technology; National Institute of Information and Communications Technology; National Institute of Information and Communications Technology; National Institute of Information and Communications Technology; National Institute of Information and Communications Technology; National Institute of Information and Communications Technology",
        "aff_domain": "nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp",
        "email": "nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp;nict.go.jp",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "National Institute of Information and Communications Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nict.go.jp/",
        "aff_unique_abbr": "NICT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.223",
        "title": "Few Clean Instances Help Denoising Distant Supervision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing distantly supervised relation extractors usually rely on noisy data for both model training and evaluation, which may lead to garbage-in-garbage-out systems. To alleviate the problem, we study whether a small clean dataset could help improve the quality of distantly supervised models. We show that besides getting a more convincing evaluation of models, a small clean dataset also helps us to build more robust denoising models. Specifically, we propose a new criterion for clean instance selection based on influence functions. It collects sample-level evidence for recognizing good instances (which is more informative than loss-level evidence). We also propose a teacher-student mechanism for controlling purity of intermediate results when bootstrapping the clean set. The whole approach is model-agnostic and demonstrates strong performances on both denoising real (NYT) and synthetic noisy datasets.",
        "author": "Yufang Liu; Ziyin Huang; Yijun Wang; Changzhi Sun; Man Lan; Yuanbin Wu; Xiaofeng Mou; Ding Wang",
        "authorids": "/y/yufang-liu/; /z/ziyin-huang/; /y/yijun-wang/; /c/changzhi-sun/; /m/man-lan/; /y/yuanbin-wu/; /x/xiaofeng-mou/; /d/ding-wang/",
        "bibtex": "@inproceedings{liu-etal-2022-clean,\n    title = \"Few Clean Instances Help Denoising Distant Supervision\",\n    author = \"Liu, Yufang  and\n      Huang, Ziyin  and\n      Wang, Yijun  and\n      Sun, Changzhi  and\n      Lan, Man  and\n      Wu, Yuanbin  and\n      Mou, Xiaofeng  and\n      Wang, Ding\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.223/\",\n    pages = \"2528--2539\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.223.pdf",
        "site": "https://aclanthology.org/2022.coling-1.223/",
        "pdf_size": 457593,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4385296880564028520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Bytedance AI Lab; School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University; AI Innovation Center, Midea Group; AI Innovation Center, Midea Group",
        "aff_domain": "gmail.com;gmail.com; ; ; ;cs.ecnu.edu.cn; ; ",
        "email": "gmail.com;gmail.com; ; ; ;cs.ecnu.edu.cn; ; ",
        "github": "https://github.com/Airuibadi/IF_DSRE",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;2;0;0;3;3",
        "aff_unique_norm": "East China Normal University;Shanghai Jiao Tong University;Bytedance;Midea Group",
        "aff_unique_dep": "School of Computer Science and Technology;Department of Computer Science and Engineering;AI Lab;AI Innovation Center",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.sjtu.edu.cn;https://www.bytedance.com;https://www.mideaglobal.com",
        "aff_unique_abbr": "ECNU;SJTU;Bytedance;Midea",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.377",
        "title": "Few-Shot Pidgin Text Adaptation via Contrastive Fine-Tuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The surging demand for multilingual dialogue systems often requires a costly labeling process for each language addition. For low resource languages, human annotators are continuously tasked with the adaptation of resource-rich language utterances for each new domain. However, this prohibitive and impractical process can often be a bottleneck for low resource languages that are still without proper translation systems nor parallel corpus. In particular, it is difficult to obtain task-specific low resource language annotations for the English-derived creoles (e.g. Nigerian and Cameroonian Pidgin). To address this issue, we utilize the pretrained language models i.e. BART which has shown great potential in language generation/understanding \u2013 we propose to finetune the BART model to generate utterances in Pidgin by leveraging the proximity of the source and target languages, and utilizing positive and negative examples in constrastive training objectives. We collected and released the first parallel Pidgin-English conversation corpus in two dialogue domains and showed that this simple and effective technique is suffice to yield impressive results for English-to-Pidgin generation, which are two closely-related languages.",
        "author": "Ernie Chang; Jesujoba O. Alabi; David Ifeoluwa Adelani; Vera Demberg",
        "authorids": "/e/ernie-chang/; /j/jesujoba-alabi/; /d/david-ifeoluwa-adelani/; /v/vera-demberg/",
        "bibtex": "@inproceedings{chang-etal-2022-shot,\n    title = \"Few-Shot Pidgin Text Adaptation via Contrastive Fine-Tuning\",\n    author = \"Chang, Ernie  and\n      Alabi, Jesujoba O.  and\n      Adelani, David Ifeoluwa  and\n      Demberg, Vera\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.377/\",\n    pages = \"4286--4291\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.377.pdf",
        "site": "https://aclanthology.org/2022.coling-1.377/",
        "pdf_size": 389026,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13718021933077824942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Language Science and Technology, Saarland University, Germany+Masakhane NLP; Inria, France+Masakhane NLP; Spoken Language Systems (LSV), Saarland Informatics Campus, Germany+Masakhane NLP; Department of Language Science and Technology, Saarland University, Germany+Masakhane NLP",
        "aff_domain": "coli.uni-saarland.de;coli.uni-saarland.de;inria.fr;lsv.uni-saarland.de",
        "email": "coli.uni-saarland.de;coli.uni-saarland.de;inria.fr;lsv.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2+1;0+1;0+1",
        "aff_unique_norm": "Saarland University;Masakhane;Inria",
        "aff_unique_dep": "Department of Language Science and Technology;NLP;",
        "aff_unique_url": "https://www.uni-saarland.de;;https://www.inria.fr",
        "aff_unique_abbr": "Saarland U;;Inria",
        "aff_campus_unique_index": ";;1;",
        "aff_campus_unique": ";Saarbr\u00fccken",
        "aff_country_unique_index": "0+1;2+1;0+1;0+1",
        "aff_country_unique": "Germany;South Africa;France"
    },
    {
        "id": "2022.coling-1.329",
        "title": "Few-Shot Table Understanding: A Benchmark Dataset and Pre-Training Baseline",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot table understanding is a critical and challenging problem in real-world scenario as annotations over large amount of tables are usually costly. Pre-trained language models (PLMs), which have recently flourished on tabular data, have demonstrated their effectiveness for table understanding tasks. However, few-shot table understanding is rarely explored due to the deficiency of public table pre-training corpus and well-defined downstream benchmark tasks, especially in Chinese. In this paper, we establish a benchmark dataset, FewTUD, which consists of 5 different tasks with human annotations to systematically explore the few-shot table understanding in depth. Since there is no large number of public Chinese tables, we also collect a large-scale, multi-domain tabular corpus to facilitate future Chinese table pre-training, which includes one million tables and related natural language text with auxiliary supervised interaction signals. Finally, we present FewTPT, a novel table PLM with rich interactions over tabular data, and evaluate its performance comprehensively on the benchmark. Our dataset and model will be released to the public soon.",
        "author": "Ruixue Liu; Shaozu Yuan; Aijun Dai; Lei Shen; Tiangang Zhu; Meng Chen; Xiaodong He",
        "authorids": "/r/ruixue-liu/; /s/shaozu-yuan/; /a/aijun-dai/; /l/lei-shen/; /t/tiangang-zhu/; /m/meng-chen/; /x/xiaodong-he/",
        "bibtex": "@inproceedings{liu-etal-2022-shot,\n    title = \"Few-Shot Table Understanding: A Benchmark Dataset and Pre-Training Baseline\",\n    author = \"Liu, Ruixue  and\n      Yuan, Shaozu  and\n      Dai, Aijun  and\n      Shen, Lei  and\n      Zhu, Tiangang  and\n      Chen, Meng  and\n      He, Xiaodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.329/\",\n    pages = \"3741--3752\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.329.pdf",
        "site": "https://aclanthology.org/2022.coling-1.329/",
        "pdf_size": 2311060,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3524167676079738144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "JD AI, Beijing, China; JD AI, Beijing, China; JD AI, Beijing, China; JD AI, Beijing, China; JD AI, Beijing, China; JD AI, Beijing, China; JD AI, Beijing, China",
        "aff_domain": "jd.com;jd.com;jd.com;jd.com;jd.com;jd.com;jd.com",
        "email": "jd.com;jd.com;jd.com;jd.com;jd.com;jd.com;jd.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "JD AI",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jd.com",
        "aff_unique_abbr": "JD AI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.159",
        "title": "Few-shot Named Entity Recognition with Entity-level Prototypical Network Enhanced by Dispersedly Distributed Prototypes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot named entity recognition (NER) enables us to build a NER system for a new domain using very few labeled examples. However, existing prototypical networks for this task suffer from roughly estimated label dependency and closely distributed prototypes, thus often causing misclassifications. To address the above issues, we propose EP-Net, an Entity-level Prototypical Network enhanced by dispersedly distributed prototypes. EP-Net builds entity-level prototypes and considers text spans to be candidate entities, so it no longer requires the label dependency. In addition, EP-Net trains the prototypes from scratch to distribute them dispersedly and aligns spans to prototypes in the embedding space using a space projection. Experimental results on two evaluation tasks and the Few-NERD settings demonstrate that EP-Net consistently outperforms the previous strong models in terms of overall performance. Extensive analyses further validate the effectiveness of EP-Net.",
        "author": "Bin Ji; Shasha Li; Shaoduo Gan; Jie Yu; Jun Ma; Huijun Liu; Jing Yang",
        "authorids": "/b/bin-ji/; /s/shasha-li/; /s/shaoduo-gan/; /j/jie-yu/; /j/jun-ma/; /h/huijun-liu/; /j/jing-yang/",
        "bibtex": "@inproceedings{ji-etal-2022-shot,\n    title = \"Few-shot Named Entity Recognition with Entity-level Prototypical Network Enhanced by Dispersedly Distributed Prototypes\",\n    author = \"Ji, Bin  and\n      Li, Shasha  and\n      Gan, Shaoduo  and\n      Yu, Jie  and\n      Ma, Jun  and\n      Liu, Huijun  and\n      Yang, Jing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.159/\",\n    pages = \"1842--1854\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.159.pdf",
        "site": "https://aclanthology.org/2022.coling-1.159/",
        "pdf_size": 2528680,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=593038193991934989&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; Individual; College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;gmail.com;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;gmail.com;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology;Individual",
        "aff_unique_dep": "College of Computer;",
        "aff_unique_url": "http://www.nudt.edu.cn/;",
        "aff_unique_abbr": "NUDT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.coling-1.565",
        "title": "Few-shot Table-to-text Generation with Prefix-Controlled Generator",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural table-to-text generation approaches are data-hungry, limiting their adaption for low-resource real-world applications. Previous works mostly resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a table. However, they often contain hallucinated contents due to the uncontrolled nature of PLMs. Moreover, the topological differences between tables and sequences are rarely studied. Last but not least, fine-tuning on PLMs with a handful of instances may lead to over-fitting and catastrophic forgetting. To alleviate these problems, we propose a prompt-based approach, Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation. We prepend a task-specific prefix for a PLM to make the table structure better fit the pre-trained input. In addition, we generate an input-specific prefix to control the factual contents and word order of the generated text. Both automatic and human evaluations on different domains (humans, books and songs) of the Wikibio dataset prove the effectiveness of our approach.",
        "author": "Yutao Luo; Menghua Lu; Gongshen Liu; Shilin Wang",
        "authorids": "/y/yutao-luo/; /m/menghua-lu/; /g/gongshen-liu/; /s/shilin-wang/",
        "bibtex": "@inproceedings{luo-etal-2022-shot,\n    title = \"Few-shot Table-to-text Generation with Prefix-Controlled Generator\",\n    author = \"Luo, Yutao  and\n      Lu, Menghua  and\n      Liu, Gongshen  and\n      Wang, Shilin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.565/\",\n    pages = \"6493--6504\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.565.pdf",
        "site": "https://aclanthology.org/2022.coling-1.565/",
        "pdf_size": 623881,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9350445555713081711&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.168",
        "title": "Find the Funding: Entity Linking with Incomplete Funding Knowledge Bases",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic extraction of funding information from academic articles adds significant value to industry and research communities, including tracking research outcomes by funding organizations, profiling researchers and universities based on the received funding, and supporting open access policies. Two major challenges of identifying and linking funding entities are: (i) sparse graph structure of the Knowledge Base (KB), which makes the commonly used graph-based entity linking approaches suboptimal for the funding domain, (ii) missing entities in KB, which (unlike recent zero-shot approaches) requires marking entity mentions without KB entries as NIL. We propose an entity linking model that can perform NIL prediction and overcome data scarcity issues in a time and data-efficient manner. Our model builds on a transformer-based mention detection and a bi-encoder model to perform entity linking. We show that our model outperforms strong existing baselines.",
        "author": "Gizem Aydin; Seyed Amin Tabatabaei; George Tsatsaronis; Faegheh Hasibi",
        "authorids": "/g/gizem-aydin/; /s/seyed-amin-tabatabaei/; /g/george-tsatsaronis/; /f/faegheh-hasibi/",
        "bibtex": "@inproceedings{aydin-etal-2022-find,\n    title = \"Find the Funding: Entity Linking with Incomplete Funding Knowledge Bases\",\n    author = \"Aydin, Gizem  and\n      Tabatabaei, Seyed Amin  and\n      Tsatsaronis, George  and\n      Hasibi, Faegheh\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.168/\",\n    pages = \"1937--1942\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.168.pdf",
        "site": "https://aclanthology.org/2022.coling-1.168/",
        "pdf_size": 339982,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1481999203078178929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Radboud University; Elsevier; Elsevier; Radboud University",
        "aff_domain": "gmail.com;elsevier.com;elsevier.com;cs.ru.nl",
        "email": "gmail.com;elsevier.com;elsevier.com;cs.ru.nl",
        "github": "",
        "project": "https://www.crossref.org",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Radboud University;Elsevier",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ru.nl;https://www.elsevier.com",
        "aff_unique_abbr": "RU;Elsevier",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.coling-1.233",
        "title": "Finding Influential Instances for Distantly Supervised Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Distant supervision (DS) is a strong way to expand the datasets for enhancing relation extraction (RE) models but often suffers from high label noise. Current works based on attention, reinforcement learning, or GAN are black-box models so they neither provide meaningful interpretation of sample selection in DS nor stability on different domains. On the contrary, this work proposes a novel model-agnostic instance sampling method for DS by influence function (IF), namely REIF. Our method identifies favorable/unfavorable instances in the bag based on IF, then does dynamic instance sampling. We design a fast influence sampling algorithm that reduces the computational complexity from \ud835\udcaa(mn) to \ud835\udcaa(1), with analyzing its robustness on the selected sampling function. Experiments show that by simply sampling the favorable instances during training, REIF is able to win over a series of baselines which have complicated architectures. We also demonstrate that REIF can support interpretable instance selection.",
        "author": "Zifeng Wang; Rui Wen; Xi Chen; Shao-Lun Huang; Ningyu Zhang; Yefeng Zheng",
        "authorids": "/z/zifeng-wang/; /r/rui-wen/; /x/xi-chen/; /s/shao-lun-huang/; /n/ningyu-zhang/; /y/yefeng-zheng/",
        "bibtex": "@inproceedings{wang-etal-2022-finding,\n    title = \"Finding Influential Instances for Distantly Supervised Relation Extraction\",\n    author = \"Wang, Zifeng  and\n      Wen, Rui  and\n      Chen, Xi  and\n      Huang, Shao-Lun  and\n      Zhang, Ningyu  and\n      Zheng, Yefeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.233/\",\n    pages = \"2639--2650\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.233.pdf",
        "site": "https://aclanthology.org/2022.coling-1.233/",
        "pdf_size": 1538433,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2649204274604396155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana Champaign+TBSI, Tsinghua University; Tencent; Tencent; TBSI, Tsinghua University; Zhejiang University; Tencent",
        "aff_domain": "illinois.edu;tencent.com;tencent.com;tsinghua.edu.cn;zju.edu.cn;tencent.com",
        "email": "illinois.edu;tencent.com;tencent.com;tsinghua.edu.cn;zju.edu.cn;tencent.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;2;1;3;2",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Tsinghua University;Tencent Holdings Limited;Zhejiang University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://illinois.edu;https://www.tsinghua.edu.cn;https://www.tencent.com;https://www.zju.edu.cn",
        "aff_unique_abbr": "UIUC;THU;Tencent;ZJU",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Urbana-Champaign;TBSI;",
        "aff_country_unique_index": "0+1;1;1;1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.coling-1.179",
        "title": "Flat Multi-modal Interaction Transformer for Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-modal named entity recognition (MNER) aims at identifying entity spans and recognizing their categories in social media posts with the aid of images. However, in dominant MNER approaches, the interaction of different modalities is usually carried out through the alternation of self-attention and cross-attention or over-reliance on the gating machine, which results in imprecise and biased correspondence between fine-grained semantic units of text and image. To address this issue, we propose a Flat Multi-modal Interaction Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in sentences and general domain words to obtain visual cues. Then, we transform the fine-grained semantic representation of the vision and text into a unified lattice structure and design a novel relative position encoding to match different modalities in Transformer. Meanwhile, we propose to leverage entity boundary detection as an auxiliary task to alleviate visual bias. Experiments show that our methods achieve the new state-of-the-art performance on two benchmark datasets.",
        "author": "Junyu Lu; Dixiang Zhang; Jiaxing Zhang; Pingjian Zhang",
        "authorids": "/j/junyu-lu/; /d/dixiang-zhang/; /j/jiaxing-zhang/; /p/pingjian-zhang/",
        "bibtex": "@inproceedings{lu-etal-2022-flat,\n    title = \"Flat Multi-modal Interaction Transformer for Named Entity Recognition\",\n    author = \"Lu, Junyu  and\n      Zhang, Dixiang  and\n      Zhang, Jiaxing  and\n      Zhang, Pingjian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.179/\",\n    pages = \"2055--2064\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.179.pdf",
        "site": "https://aclanthology.org/2022.coling-1.179/",
        "pdf_size": 549660,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18315010152784558582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "South China University of Technology\u2663; International Digital Economy Academy (IDEA)\u2660; South China University of Technology\u2663; South China University of Technology\u2663",
        "aff_domain": "idea.edu.cn;mail.scut.edu.cn; ;scut.edu.cn",
        "email": "idea.edu.cn;mail.scut.edu.cn; ;scut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "South China University of Technology;International Digital Economy Academy",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.scut.edu.cn;",
        "aff_unique_abbr": "SCUT;IDEA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.coling-1.539",
        "title": "Focus-Driven Contrastive Learning for Medical Question Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic medical question summarization can significantly help the system to understand consumer health questions and retrieve correct answers. The Seq2Seq model based on maximum likelihood estimation (MLE) has been applied in this task, which faces two general problems: the model can not capture well question focus and and the traditional MLE strategy lacks the ability to understand sentence-level semantics. To alleviate these problems, we propose a novel question focus-driven contrastive learning framework (QFCL). Specially, we propose an easy and effective approach to generate hard negative samples based on the question focus, and exploit contrastive learning at both encoder and decoder to obtain better sentence level representations. On three medical benchmark datasets, our proposed model achieves new state-of-the-art results, and obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline BART model on three datasets respectively. Further human judgement and detailed analysis prove that our QFCL model learns better sentence representations with the ability to distinguish different sentence meanings, and generates high-quality summaries by capturing question focus.",
        "author": "Ming Zhang; Shuai Dou; Ziyang Wang; Yunfang Wu",
        "authorids": "/m/ming-zhang/; /s/shuai-dou/; /z/ziyang-wang/; /y/yunfang-wu/",
        "bibtex": "@inproceedings{zhang-etal-2022-focus-driven,\n    title = \"Focus-Driven Contrastive Learning for Medical Question Summarization\",\n    author = \"Zhang, Ming  and\n      Dou, Shuai  and\n      Wang, Ziyang  and\n      Wu, Yunfang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.539/\",\n    pages = \"6176--6186\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.539.pdf",
        "site": "https://aclanthology.org/2022.coling-1.539/",
        "pdf_size": 1782912,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1276954815390813935&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "MOE, Key Laboratory of Computational Linguistics, Peking University + School of Software and Microelectronics, Peking University; School of Computer Science, Peking University; MOE, Key Laboratory of Computational Linguistics, Peking University + School of Software and Microelectronics, Peking University; School of Computer Science, Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0;0+0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": ";1;;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.107",
        "title": "From Polarity to Intensity: Mining Morality from Semantic Space",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most works on computational morality focus on moral polarity recognition, i.e., distinguishing right from wrong. However, a discrete polarity label is not informative enough to reflect morality as it does not contain any degree or intensity information. Existing approaches to compute moral intensity are limited to word-level measurement and heavily rely on human labelling. In this paper, we propose MoralScore, a weakly-supervised framework that can automatically measure moral intensity from text. It only needs moral polarity labels, which are more robust and easier to acquire. Besides, the framework can capture latent moral information not only from words but also from sentence-level semantics which can provide a more comprehensive measurement. To evaluate the performance of our method, we introduce a set of evaluation metrics and conduct extensive experiments. Results show that our method achieves good performance on both automatic and human evaluations.",
        "author": "Chunxu Zhao; Pengyuan Liu; Dong Yu",
        "authorids": "/c/chunxu-zhao/; /p/pengyuan-liu/; /d/dong-yu/",
        "bibtex": "@inproceedings{zhao-etal-2022-polarity,\n    title = \"From Polarity to Intensity: Mining Morality from Semantic Space\",\n    author = \"Zhao, Chunxu  and\n      Liu, Pengyuan  and\n      Yu, Dong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.107/\",\n    pages = \"1250--1262\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.107.pdf",
        "site": "https://aclanthology.org/2022.coling-1.107/",
        "pdf_size": 1001279,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10773097487288120408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information Science, Beijing Language and Culture University, China; School of Information Science, Beijing Language and Culture University, China; School of Information Science, Beijing Language and Culture University, China",
        "aff_domain": "gmail.com;pku.edu.cn;126.com",
        "email": "gmail.com;pku.edu.cn;126.com",
        "github": "https://github.com/blcunlp/MoralScore",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Beijing Language and Culture University",
        "aff_unique_dep": "School of Information Science",
        "aff_unique_url": "http://www.blcu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.506",
        "title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in a graph.",
        "author": "Anthony Colas; Mehrdad Alvandipour; Daisy Zhe Wang",
        "authorids": "/a/anthony-colas/; /m/mehrdad-alvandipour/; /d/daisy-zhe-wang/",
        "bibtex": "@inproceedings{colas-etal-2022-gap,\n    title = \"{GAP}: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation\",\n    author = \"Colas, Anthony  and\n      Alvandipour, Mehrdad  and\n      Wang, Daisy Zhe\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.506/\",\n    pages = \"5755--5769\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.506.pdf",
        "site": "https://aclanthology.org/2022.coling-1.506/",
        "pdf_size": 656102,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=267138124808524651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Florida; Department of Computer Science, University of Florida; Department of Computer Science, University of Florida",
        "aff_domain": "ufl.edu;ufl.edu;ufl.edu",
        "email": "ufl.edu;ufl.edu;ufl.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.155",
        "title": "GLAF: Global-to-Local Aggregation and Fission Network for Semantic Level Fact Verification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Accurate fact verification depends on performing fine-grained reasoning over crucial entities by capturing their latent logical relations hidden in multiple evidence clues, which is generally lacking in existing fact verification models. In this work, we propose a novel Global-to-Local Aggregation and Fission network (GLAF) to fill this gap. Instead of treating entire sentences or all semantic elements within them as nodes to construct a coarse-grained or unstructured evidence graph as in previous methods, GLAF constructs a fine-grained and structured evidence graph by parsing the rambling sentences into structural triple-level reasoning clues and regarding them as graph nodes to achieve fine-grained and interpretable evidence graph reasoning. Specifically, to capture latent logical relations between the clues, GLAF first employs a local fission reasoning layer to conduct fine-grained multi-hop reasoning, and then uses a global evidence aggregation layer to achieve information sharing and the interchange of evidence clues for final claim label prediction. Experimental results on the FEVER dataset demonstrate the effectiveness of GLAF, showing that it achieves the state-of-the-art performance by obtaining a 77.62% FEVER score.",
        "author": "Zhiyuan Ma; Jianjun Li; Guohui Li; Yongjing Cheng",
        "authorids": "/z/zhiyuan-ma/; /j/jianjun-li/; /g/guohui-li/; /y/yongjing-cheng/",
        "bibtex": "@inproceedings{ma-etal-2022-glaf,\n    title = \"{GLAF}: Global-to-Local Aggregation and Fission Network for Semantic Level Fact Verification\",\n    author = \"Ma, Zhiyuan  and\n      Li, Jianjun  and\n      Li, Guohui  and\n      Cheng, Yongjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.155/\",\n    pages = \"1801--1812\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.155.pdf",
        "site": "https://aclanthology.org/2022.coling-1.155/",
        "pdf_size": 880079,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1251130637925915790&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 0,
        "aff": "Huazhong University of Science and Technology (HUST), China; Huazhong University of Science and Technology (HUST), China; Huazhong University of Science and Technology (HUST), China; National University of Defense Technology (NUDT), China",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;163.com",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;163.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Huazhong University of Science and Technology;National University of Defense Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.hust.edu.cn;http://www.nudt.edu.cn/",
        "aff_unique_abbr": "HUST;NUDT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.33",
        "title": "GRASP: Guiding Model with RelAtional Semantics Using Prompt for Dialogue Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The dialogue-based relation extraction (DialogRE) task aims to predict the relations between argument pairs that appear in dialogue. Most previous studies utilize fine-tuning pre-trained language models (PLMs) only with extensive features to supplement the low information density of the dialogue by multiple speakers. To effectively exploit inherent knowledge of PLMs without extra layers and consider scattered semantic cues on the relation between the arguments, we propose a Guiding model with RelAtional Semantics using Prompt (GRASP). We adopt a prompt-based fine-tuning approach and capture relational semantic clues of a given dialogue with 1) an argument-aware prompt marker strategy and 2) the relational clue detection task. In the experiments, GRASP achieves state-of-the-art performance in terms of both F1 and F1c scores on a DialogRE dataset even though our method only leverages PLMs without adding any extra layers.",
        "author": "Junyoung Son; Jinsung Kim; Jungwoo Lim; Heuiseok Lim",
        "authorids": "/j/junyoung-son/; /j/jinsung-kim/; /j/jungwoo-lim/; /h/heui-seok-lim/",
        "bibtex": "@inproceedings{son-etal-2022-grasp,\n    title = \"{GRASP}: Guiding Model with {R}el{A}tional Semantics Using Prompt for Dialogue Relation Extraction\",\n    author = \"Son, Junyoung  and\n      Kim, Jinsung  and\n      Lim, Jungwoo  and\n      Lim, Heuiseok\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.33/\",\n    pages = \"412--423\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.33.pdf",
        "site": "https://aclanthology.org/2022.coling-1.33/",
        "pdf_size": 491772,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5526066066282947030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science and Engineering, Korea University; Computer Science and Engineering, Korea University; Computer Science and Engineering, Korea University; Computer Science and Engineering, Korea University",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.22",
        "title": "GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Learning from multimodal data has become a popular research topic in recent years. Multimodal coreference resolution (MCR) is an important task in this area. MCR involves resolving the references across different modalities, e.g., text and images, which is a crucial capability for building next-generation conversational agents. MCR is challenging as it requires encoding information from different modalities and modeling associations between them. Although significant progress has been made for visual-linguistic tasks such as visual grounding, most of the current works involve single turn utterances and focus on simple coreference resolutions. In this work, we propose an MCR model that resolves coreferences made in multi-turn dialogues with scene images. We present GRAVL-BERT, a unified MCR framework which combines visual relationships between objects, background scenes, dialogue, and metadata by integrating Graph Neural Networks with VL-BERT. We present results on the SIMMC 2.0 multimodal conversational dataset, achieving the rank-1 on the DSTC-10 SIMMC 2.0 MCR challenge with F1 score 0.783. Our code is available at https://github.com/alexa/gravl-bert.",
        "author": "Danfeng Guo; Arpit Gupta; Sanchit Agarwal; Jiun-Yu Kao; Shuyang Gao; Arijit Biswas; Chien-Wei Lin; Tagyoung Chung; Mohit Bansal",
        "authorids": "/d/danfeng-guo/; /a/arpit-gupta/; /s/sanchit-agarwal/; /j/jiun-yu-kao/; /s/shuyang-gao/; /a/arijit-biswas/; /c/chien-wei-lin/; /t/tagyoung-chung/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{guo-etal-2022-gravl,\n    title = \"{GRAVL}-{BERT}: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution\",\n    author = \"Guo, Danfeng  and\n      Gupta, Arpit  and\n      Agarwal, Sanchit  and\n      Kao, Jiun-Yu  and\n      Gao, Shuyang  and\n      Biswas, Arijit  and\n      Lin, Chien-Wei  and\n      Chung, Tagyoung  and\n      Bansal, Mohit\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.22/\",\n    pages = \"285--297\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.22.pdf",
        "site": "https://aclanthology.org/2022.coling-1.22/",
        "pdf_size": 20124099,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9623704528876733967&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of California, Los Angeles; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; University of North Carolina, Chapel Hill + Amazon Alexa",
        "aff_domain": "g.ucla.edu; ;amazon.com; ; ; ; ; ; ",
        "email": "g.ucla.edu; ;amazon.com; ; ; ; ; ; ",
        "github": "https://github.com/alexa/gravl-bert",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;1;1;2+1",
        "aff_unique_norm": "University of California, Los Angeles;Amazon;University of North Carolina",
        "aff_unique_dep": ";Amazon Alexa;",
        "aff_unique_url": "https://www.ucla.edu;https://www.amazon.com/alexa;https://www.unc.edu",
        "aff_unique_abbr": "UCLA;Amazon Alexa;UNC",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Los Angeles;;Chapel Hill",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.546",
        "title": "GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs), to capture the global semantic information for text summarization. However, in these methods, there remain limitations in the way they capture and integrate the global semantic information. In this paper, we propose a novel model, the graph contrastive topic enhanced language model (GRETEL), that incorporates the graph contrastive topic model with the pre-trained language model, to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs, the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics. Experimental results on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods.",
        "author": "Qianqian Xie; Jimin Huang; Tulika Saha; Sophia Ananiadou",
        "authorids": "/q/qianqian-xie/; /j/jimin-huang/; /t/tulika-saha/; /s/sophia-ananiadou/",
        "bibtex": "@inproceedings{xie-etal-2022-gretel,\n    title = \"{GRETEL}: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization\",\n    author = \"Xie, Qianqian  and\n      Huang, Jimin  and\n      Saha, Tulika  and\n      Ananiadou, Sophia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.546/\",\n    pages = \"6259--6269\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.546.pdf",
        "site": "https://aclanthology.org/2022.coling-1.546/",
        "pdf_size": 733263,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14177067696035383872&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "National Centre for Text Mining, Department of Computer Science, The University of Manchester; Chancefocus AMC; National Centre for Text Mining, Department of Computer Science, The University of Manchester; National Centre for Text Mining, Department of Computer Science, The University of Manchester",
        "aff_domain": "manchester.ac.uk;chancefocus.com;manchester.ac.uk;manchester.ac.uk",
        "email": "manchester.ac.uk;chancefocus.com;manchester.ac.uk;manchester.ac.uk",
        "github": "https://github.com/xashely/GRETEL_extractive",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "The University of Manchester;Chancefocus AMC",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.manchester.ac.uk;",
        "aff_unique_abbr": "UoM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "2022.coling-1.271",
        "title": "Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Currently, human-bot symbiosis dialog systems, e.g. pre- and after-sales in E-commerce, are ubiquitous, and the dialog routing component is essential to improve the overall efficiency, reduce human resource cost and increase user experience. To satisfy this requirement, existing methods are mostly heuristic and cannot obtain high-quality performance. In this paper, we investigate the important problem by thoroughly mining both the data-to-task and task-to-task knowledge among various kinds of dialog data. To achieve the above target, we propose a comprehensive and general solution with multi-task learning framework, specifically including a novel dialog encoder and two tailored gated mechanism modules. The proposed Gated Mechanism enhanced Multi-task Model (G3M) can play the role of hierarchical information filtering and is non-invasive to the existing dialog systems. Experiments on two datasets collected from the real world demonstrate our method\u2019s effectiveness and the results achieve the state-of-the-art performance by relatively increasing 8.7%/11.8% on RMSE metric and 2.2%/4.4% on F1 metric.",
        "author": "Ziming Huang; Zhuoxuan Jiang; Ke Wang; Juntao Li; Shanshan Feng; Xian-Ling Mao",
        "authorids": "/z/ziming-huang/; /z/zhuoxuan-jiang/; /k/ke-wang/; /j/juntao-li/; /s/shanshan-feng/; /x/xian-ling-mao/",
        "bibtex": "@inproceedings{huang-etal-2022-gated,\n    title = \"Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing\",\n    author = \"Huang, Ziming  and\n      Jiang, Zhuoxuan  and\n      Wang, Ke  and\n      Li, Juntao  and\n      Feng, Shanshan  and\n      Mao, Xian-Ling\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.271/\",\n    pages = \"3064--3073\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.271.pdf",
        "site": "https://aclanthology.org/2022.coling-1.271/",
        "pdf_size": 1414489,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:PYcCELSTnY0J:scholar.google.com/&scioq=Gated+Mechanism+Enhanced+Multi-Task+Learning+for+Dialog+Routing&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.579",
        "title": "Generalizable Implicit Hate Speech Detection Using Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Hate speech detection has gained increasing attention with the growing prevalence of hateful contents. When a text contains an obvious hate word or expression, it is fairly easy to detect it. However, it is challenging to identify implicit hate speech in nuance or context when there are insufficient lexical cues. Recently, there are several attempts to detect implicit hate speech leveraging pre-trained language models such as BERT and HateBERT. Fine-tuning on an implicit hate speech dataset shows satisfactory performance when evaluated on the test set of the dataset used for training. However, we empirically confirm that the performance drops at least 12.5%p in F1 score when tested on the dataset that is different from the one used for training. We tackle this cross-dataset underperforming problem using contrastive learning. Based on our observation of common underlying implications in various forms of hate posts, we propose a novel contrastive learning method, ImpCon, that pulls an implication and its corresponding posts close in representation space. We evaluate the effectiveness of ImpCon by running cross-dataset evaluation on three implicit hate speech benchmarks. The experimental results on cross-dataset show that ImpCon improves at most 9.10% on BERT, and 8.71% on HateBERT.",
        "author": "Youngwook Kim; Shinwoo Park; Yo-Sub Han",
        "authorids": "/y/youngwook-kim/; /s/shinwoo-park/; /y/yo-sub-han/",
        "bibtex": "@inproceedings{kim-etal-2022-generalizable,\n    title = \"Generalizable Implicit Hate Speech Detection Using Contrastive Learning\",\n    author = \"Kim, Youngwook  and\n      Park, Shinwoo  and\n      Han, Yo-Sub\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.579/\",\n    pages = \"6667--6679\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.579.pdf",
        "site": "https://aclanthology.org/2022.coling-1.579/",
        "pdf_size": 1231006,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13642202609031881195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Yonsei University, Seoul, Republic of Korea; Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea; Department of Computer Science, Yonsei University, Seoul, Republic of Korea",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.59",
        "title": "Generalized Intent Discovery: Learning from Open World Dialogue System",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Traditional intent classification models are based on a pre-defined intent set and only recognize limited in-domain (IND) intent classes. But users may input out-of-domain (OOD) queries in a practical dialogue system. Such OOD queries can provide directions for future improvement. In this paper, we define a new task, Generalized Intent Discovery (GID), which aims to extend an IND intent classifier to an open-world intent set including IND and OOD intents. We hope to simultaneously classify a set of labeled IND intent classes while discovering and recognizing new unlabeled OOD types incrementally. We construct three public datasets for different application scenarios and propose two kinds of frameworks, pipeline-based and end-to-end for future work. Further, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future GID research.",
        "author": "Yutao Mou; Keqing He; Yanan Wu; Pei Wang; Jingang Wang; Wei Wu; Yi Huang; Junlan Feng; Weiran Xu",
        "authorids": "/y/yutao-mou/; /k/keqing-he/; /y/yanan-wu/; /p/pei-wang/; /j/jingang-wang/; /w/wei-wu/; /y/yi-huang/; /j/junlan-feng/; /w/weiran-xu/",
        "bibtex": "@inproceedings{mou-etal-2022-generalized,\n    title = \"Generalized Intent Discovery: Learning from Open World Dialogue System\",\n    author = \"Mou, Yutao  and\n      He, Keqing  and\n      Wu, Yanan  and\n      Wang, Pei  and\n      Wang, Jingang  and\n      Wu, Wei  and\n      Huang, Yi  and\n      Feng, Junlan  and\n      Xu, Weiran\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.59/\",\n    pages = \"707--720\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.59.pdf",
        "site": "https://aclanthology.org/2022.coling-1.59/",
        "pdf_size": 968057,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=755477757796329909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing University of Posts and Telecommunications; Meituan Group; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Meituan Group; Meituan Group; China Mobile Research Institute; China Mobile Research Institute; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;meituan.com;meituan.com;chinamobile.com;chinamobile.com;bupt.edu.cn",
        "email": "bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;meituan.com;meituan.com;chinamobile.com;chinamobile.com;bupt.edu.cn",
        "github": "https://github.com/myt517/GID_benchmark",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;0;0;1;1;2;2;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan Group;China Mobile",
        "aff_unique_dep": ";;Research Institute",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com;https://www.chinamobile.com/",
        "aff_unique_abbr": "BUPT;Meituan;CMRI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.438",
        "title": "Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A common recent approach to semantic parsing augments sequence-to-sequence models by retrieving and appending a set of training samples, called exemplars. The effectiveness of this recipe is limited by the ability to retrieve informative exemplars that help produce the correct parse, which is especially challenging in low-resource settings. Existing retrieval is commonly based on similarity of query and exemplar inputs. We propose GandR, a retrieval procedure that retrieves exemplars for which outputs are also similar. GandR first generates a preliminary prediction with input-based retrieval. Then, it retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction. GandR sets the state of the art on multiple low-resource semantic parsing tasks.",
        "author": "Yury Zemlyanskiy; Michiel de Jong; Joshua Ainslie; Panupong Pasupat; Peter Shaw; Linlu Qiu; Sumit Sanghai; Fei Sha",
        "authorids": "/y/yury-zemlyanskiy/; /m/michiel-de-jong/; /j/joshua-ainslie/; /p/panupong-pasupat/; /p/peter-shaw/; /l/linlu-qiu/; /s/sumit-sanghai/; /f/fei-sha/",
        "bibtex": "@inproceedings{zemlyanskiy-etal-2022-generate,\n    title = \"Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing\",\n    author = \"Zemlyanskiy, Yury  and\n      de Jong, Michiel  and\n      Ainslie, Joshua  and\n      Pasupat, Panupong  and\n      Shaw, Peter  and\n      Qiu, Linlu  and\n      Sanghai, Sumit  and\n      Sha, Fei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.438/\",\n    pages = \"4946--4951\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.438.pdf",
        "site": "https://aclanthology.org/2022.coling-1.438/",
        "pdf_size": 849414,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9690350347089269916&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Southern California\u2020; Google Research\u2021; Google Research\u2021; Google Research\u2021; Google Research\u2021; Google Research\u2021; Google Research\u2021; Google Research\u2021",
        "aff_domain": "usc.edu;usc.edu;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "usc.edu;usc.edu;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;1;1",
        "aff_unique_norm": "University of Southern California;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.usc.edu;https://research.google",
        "aff_unique_abbr": "USC;Google Research",
        "aff_campus_unique_index": "1;1;1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.162",
        "title": "Generating Temporally-ordered Event Sequences via Event Optimal Transport",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generating temporally-ordered event sequences in texts is important to natural language processing. Two emerging tasks in this direction are temporal event ordering (rearranging the set of events to correct order) and event infilling (generating an event at a specified position). To tackle the two related tasks, the existing method adopts a vanilla sequence-to-sequence model via maximum likelihood estimation (MLE). However, applying this approach to these tasks will cause two issues. One issue is that the MLE loss emphasizes strict local alignment and ignores the global semantics of the event. The other issue is that the model adopts a word-level objective to model events in texts, failing to evaluate the predicted results of the model from the perspective of event sequence. To alleviate these issues, we present a novel model to tackle the generation of temporally-ordered event sequences via Event Optimal Transport (EOT). First, we treat the events in the sequence as modeling units and explicitly extract the semantics of the events. Second, to provide event sequence-level evaluation of the predicted results of the model, we directly match events in sequences. Extensive experimental results show that our approach outperforms previous models on all evaluation datasets. In particular, the accuracy is improved by 7.7%, and the Macro F1 is improved by 7.2% on one of the datasets.",
        "author": "Bo Zhou; Yubo Chen; Kang Liu; Jun Zhao; Jiexin Xu; Xiaojian Jiang; Qiuxia Li",
        "authorids": "/b/bo-zhou/; /y/yubo-chen/; /k/kang-liu/; /j/jun-zhao/; /j/jiexin-xu/; /x/xiaojian-jiang/; /q/qiuxia-li/",
        "bibtex": "@inproceedings{zhou-etal-2022-generating,\n    title = \"Generating Temporally-ordered Event Sequences via Event Optimal Transport\",\n    author = \"Zhou, Bo  and\n      Chen, Yubo  and\n      Liu, Kang  and\n      Zhao, Jun  and\n      Xu, Jiexin  and\n      Jiang, Xiaojian  and\n      Li, Qiuxia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.162/\",\n    pages = \"1875--1884\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.162.pdf",
        "site": "https://aclanthology.org/2022.coling-1.162/",
        "pdf_size": 687960,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5651560482062136669&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA; School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA; School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA+Beijing Academy of Artificial Intelligence; School of Artificial Intelligence, University of Chinese Academy of Sciences+National Laboratory of Pattern Recognition, CASIA; China Merchants Bank; China Merchants Bank; China Merchants Bank",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;cmbchina.com;cmbchina.com;cmbchina.com",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;cmbchina.com;cmbchina.com;cmbchina.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1+2;0+1;3;3;3",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences, Institute of Automation;Beijing Academy of Artificial Intelligence;China Merchants Bank",
        "aff_unique_dep": "School of Artificial Intelligence;National Laboratory of Pattern Recognition;;",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ia.cas.cn;https://www.baaic.cn;https://www.cmbchina.com.cn",
        "aff_unique_abbr": "UCAS;CASIA;BAAI;CMB",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.544",
        "title": "Generation of Patient After-Visit Summaries to Support Physicians",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients\u2019 disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians.",
        "author": "Pengshan Cai; Fei Liu; Adarsha Bajracharya; Joe Sills; Alok Kapoor; Weisong Liu; Dan Berlowitz; David Levy; Richeek Pradhan; Hong Yu",
        "authorids": "/p/pengshan-cai/; /f/fei-liu-utdallas/; /a/adarsha-bajracharya/; /j/joe-sills/; /a/alok-kapoor/; /w/weisong-liu/; /d/dan-berlowitz/; /d/david-levy/; /r/richeek-pradhan/; /h/hong-yu/",
        "bibtex": "@inproceedings{cai-etal-2022-generation,\n    title = \"Generation of Patient After-Visit Summaries to Support Physicians\",\n    author = \"Cai, Pengshan  and\n      Liu, Fei  and\n      Bajracharya, Adarsha  and\n      Sills, Joe  and\n      Kapoor, Alok  and\n      Liu, Weisong  and\n      Berlowitz, Dan  and\n      Levy, David  and\n      Pradhan, Richeek  and\n      Yu, Hong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.544/\",\n    pages = \"6234--6247\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.544.pdf",
        "site": "https://aclanthology.org/2022.coling-1.544/",
        "pdf_size": 510100,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18281692179009397191&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Massachusetts, Amherst; Emory University; UMass Chan Medical School; University of Massachusetts, Lowell; McGill University; UMass Chan Medical School + University of Massachusetts, Lowell; UMass Chan Medical School + University of Massachusetts, Lowell; UMass Chan Medical School + University of Massachusetts, Lowell; McGill University; University of Massachusetts, Amherst + UMass Chan Medical School + University of Massachusetts, Lowell",
        "aff_domain": "cs.umass.edu;emory.edu;umassmemorial.org;baystatehealth.org;umassmemorial.org;uml.edu;uml.edu;uml.edu;gmail.com;cs.umass.edu",
        "email": "cs.umass.edu;emory.edu;umassmemorial.org;baystatehealth.org;umassmemorial.org;uml.edu;uml.edu;uml.edu;gmail.com;cs.umass.edu",
        "github": "https://github.com/pengshancai/AVS_gen",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;3;4;2+3;2+3;2+3;4;0+2+3",
        "aff_unique_norm": "University of Massachusetts Amherst;Emory University;University of Massachusetts Chan Medical School;University of Massachusetts Lowell;McGill University",
        "aff_unique_dep": ";;Medical School;;",
        "aff_unique_url": "https://www.umass.edu;https://www.emory.edu;https://umassmed.edu;https://www.uml.edu;https://www.mcgill.ca",
        "aff_unique_abbr": "UMass Amherst;Emory;UMass Chan;UMass Lowell;McGill",
        "aff_campus_unique_index": "0;2;3;2+3;2+3;2+3;0+2+3",
        "aff_campus_unique": "Amherst;;Chan;Lowell",
        "aff_country_unique_index": "0;0;0;0;1;0+0;0+0;0+0;1;0+0+0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2022.coling-1.282",
        "title": "Generic Overgeneralization in Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generic statements such as \u201cducks lay eggs\u201d make claims about kinds, e.g., ducks as a category. The generic overgeneralization effect refers to the inclination to accept false universal generalizations such as \u201call ducks lay eggs\u201d or \u201call lions have manes\u201d as true. In this paper, we investigate the generic overgeneralization effect in pre-trained language models experimentally. We show that pre-trained language models suffer from overgeneralization and tend to treat quantified generic statements such as \u201call ducks lay eggs\u201d as if they were true generics. Furthermore, we demonstrate how knowledge embedding methods can lessen this effect by injecting factual knowledge about kinds into pre-trained language models. To this end, we source factual knowledge about two types of generics, minority characteristic generics and majority characteristic generics, and inject this knowledge using a knowledge embedding model. Our results show that knowledge injection reduces, but does not eliminate, generic overgeneralization, and that majority characteristic generics of kinds are more susceptible to overgeneralization bias.",
        "author": "Sello Ralethe; Jan Buys",
        "authorids": "/s/sello-ralethe/; /j/jan-buys/",
        "bibtex": "@inproceedings{ralethe-buys-2022-generic,\n    title = \"Generic Overgeneralization in Pre-trained Language Models\",\n    author = \"Ralethe, Sello  and\n      Buys, Jan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.282/\",\n    pages = \"3187--3196\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.282.pdf",
        "site": "https://aclanthology.org/2022.coling-1.282/",
        "pdf_size": 193459,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11161516809521890190&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Cape Town, South Africa; Department of Computer Science, University of Cape Town, South Africa",
        "aff_domain": "myuct.ac.za;cs.uct.ac.za",
        "email": "myuct.ac.za;cs.uct.ac.za",
        "github": "https://github.com/sello-ralethe/GOG-in-PLMs",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cape Town",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uct.ac.za",
        "aff_unique_abbr": "UCT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Africa"
    },
    {
        "id": "2022.coling-1.12",
        "title": "Gestures Are Used Rationally: Information Theoretic Evidence from Neural Sequential Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Verbal communication is companied by rich non-verbal signals. The usage of gestures, poses, and facial expressions facilitates the information transmission in verbal channel. However, few computational studies have explored the non-verbal channels with finer theoretical lens. We extract gesture representations from monologue video data and train neural sequential models, in order to study the degree to which non-verbal signals can effectively transmit information. We focus on examining whether the gestures demonstrate the similar pattern of entropy rate constancy (ERC) found in words, as predicted by Information Theory. Positive results are shown to support the assumption, which leads to the conclusion that speakers indeed use simple gestures to convey information that enhances verbal communication, and the production of non-verbal information is rationally organized.",
        "author": "Yang Xu; Yang Cheng; Riya Bhatia",
        "authorids": "/y/yang-xu/; /y/yang-cheng/; /r/riya-bhatia/",
        "bibtex": "@inproceedings{xu-etal-2022-gestures,\n    title = \"Gestures Are Used Rationally: Information Theoretic Evidence from Neural Sequential Models\",\n    author = \"Xu, Yang  and\n      Cheng, Yang  and\n      Bhatia, Riya\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.12/\",\n    pages = \"134--140\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.12.pdf",
        "site": "https://aclanthology.org/2022.coling-1.12/",
        "pdf_size": 887801,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10411685796293558639&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, San Diego State University; University of Southern California; Carnegie Mellon University",
        "aff_domain": "sdsu.edu;usc.edu;andrew.cmu.edu",
        "email": "sdsu.edu;usc.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "San Diego State University;University of Southern California;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.sdsu.edu;https://www.usc.edu;https://www.cmu.edu",
        "aff_unique_abbr": "SDSU;USC;CMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "San Diego;Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.381",
        "title": "Global Readiness of Language Technology for Healthcare: What Would It Take to Combat the Next Pandemic?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The COVID-19 pandemic has brought out both the best and worst of language technology (LT). On one hand, conversational agents for information dissemination and basic diagnosis have seen widespread use, and arguably, had an important role in fighting against the pandemic. On the other hand, it has also become clear that such technologies are readily available for a handful of languages, and the vast majority of the global south is completely bereft of these benefits. What is the state of LT, especially conversational agents, for healthcare across the world\u2019s languages? And, what would it take to ensure global readiness of LT before the next pandemic? In this paper, we try to answer these questions through survey of existing literature and resources, as well as through a rapid chatbot building exercise for 15 Asian and African languages with varying amount of resource-availability. The study confirms the pitiful state of LT even for languages with large speaker bases, such as Sinhala and Hausa, and identifies the gaps that could help us prioritize research and investment strategies in LT for healthcare.",
        "author": "Ishani Mondal; Kabir Ahuja; Mohit Jain; Jacki O\u2019Neill; Kalika Bali; Monojit Choudhury",
        "authorids": "/i/ishani-mondal/; /k/kabir-ahuja/; /m/mohit-jain/; /j/jacki-oneill/; /k/kalika-bali/; /m/monojit-choudhury/",
        "bibtex": "@inproceedings{mondal-etal-2022-global,\n    title = \"Global Readiness of Language Technology for Healthcare: What Would It Take to Combat the Next Pandemic?\",\n    author = \"Mondal, Ishani  and\n      Ahuja, Kabir  and\n      Jain, Mohit  and\n      O{'}Neill, Jacki  and\n      Bali, Kalika  and\n      Choudhury, Monojit\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.381/\",\n    pages = \"4320--4335\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.381.pdf",
        "site": "https://aclanthology.org/2022.coling-1.381/",
        "pdf_size": 819780,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10184054475585152978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research Labs, Bangalore, India+Microsoft Africa Research Institute, Nairobi, Kenya; Microsoft Research Labs, Bangalore, India+Microsoft Africa Research Institute, Nairobi, Kenya; Microsoft Research Labs, Bangalore, India; Microsoft Africa Research Institute, Nairobi, Kenya; Microsoft Research Labs, Bangalore, India; Microsoft Research Labs, Bangalore, India",
        "aff_domain": "microsoft.com;microsoft.com; ;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com; ;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/kabirahuja2431/",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0;1;0;0",
        "aff_unique_norm": "Microsoft Research;Microsoft Africa Research Institute",
        "aff_unique_dep": "Research Labs;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/labs;https://www.microsoft.com/en-us/research/group/africa",
        "aff_unique_abbr": "MSR;MARI",
        "aff_campus_unique_index": "0+1;0+1;0;1;0;0",
        "aff_campus_unique": "Bangalore;Nairobi",
        "aff_country_unique_index": "0+1;0+1;0;1;0;0",
        "aff_country_unique": "India;Kenya"
    },
    {
        "id": "2022.coling-1.397",
        "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
        "author": "Adyasha Maharana; Mohit Bansal",
        "authorids": "/a/adyasha-maharana/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{maharana-bansal-2022-grada-graph,\n    title = \"{G}ra{DA}: Graph Generative Data Augmentation for Commonsense Reasoning\",\n    author = \"Maharana, Adyasha  and\n      Bansal, Mohit\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.397/\",\n    pages = \"4499--4516\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.397.pdf",
        "site": "https://aclanthology.org/2022.coling-1.397/",
        "pdf_size": 1130695,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7845102394731436427&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/adymaharana/GraDA",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.246",
        "title": "Grammatical Error Correction: Are We There Yet?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "There has been much recent progress in natural language processing, and grammatical error correction (GEC) is no exception. We found that state-of-the-art GEC systems (T5 and GECToR) outperform humans by a wide margin on the CoNLL-2014 test set, a benchmark GEC test corpus, as measured by the standard F0.5 evaluation metric. However, a careful examination of their outputs reveals that there are still classes of errors that they fail to correct. This suggests that creating new test data that more accurately measure the true performance of GEC systems constitutes important future work.",
        "author": "Muhammad Reza Qorib; Hwee Tou Ng",
        "authorids": "/m/muhammad-reza-qorib/; /h/hwee-tou-ng/",
        "bibtex": "@inproceedings{qorib-ng-2022-grammatical,\n    title = \"Grammatical Error Correction: Are We There Yet?\",\n    author = \"Qorib, Muhammad Reza  and\n      Ng, Hwee Tou\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.246/\",\n    pages = \"2794--2800\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.246.pdf",
        "site": "https://aclanthology.org/2022.coling-1.246/",
        "pdf_size": 320639,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3289614114291905970&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.coling-1.534",
        "title": "Graph-to-Text Generation with Dynamic Structure Pruning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.",
        "author": "Liang Li; Ruiying Geng; Bowen Li; Can Ma; Yinliang Yue; Binhua Li; Yongbin Li",
        "authorids": "/l/liang-li/; /r/ruiying-geng/; /b/bowen-li/; /c/can-ma/; /y/yinliang-yue/; /b/binhua-li/; /y/yongbin-li/",
        "bibtex": "@inproceedings{li-etal-2022-graph,\n    title = \"Graph-to-Text Generation with Dynamic Structure Pruning\",\n    author = \"Li, Liang  and\n      Geng, Ruiying  and\n      Li, Bowen  and\n      Ma, Can  and\n      Yue, Yinliang  and\n      Li, Binhua  and\n      Li, Yongbin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.534/\",\n    pages = \"6115--6127\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.534.pdf",
        "site": "https://aclanthology.org/2022.coling-1.534/",
        "pdf_size": 860429,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15832238651248541208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group",
        "aff_domain": "iie.ac.cn;alibaba-inc.com;alibaba-inc.com;iie.ac.cn;iie.ac.cn;alibaba-inc.com;alibaba-inc.com",
        "email": "iie.ac.cn;alibaba-inc.com;alibaba-inc.com;iie.ac.cn;iie.ac.cn;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;2;0;0;2;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Alibaba Group",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;DAMO Academy",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "CAS;UCAS;Alibaba",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.396",
        "title": "HCLD: A Hierarchical Framework for Zero-shot Cross-lingual Dialogue System",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, many task-oriented dialogue systems need to serve users in different languages. However, it is time-consuming to collect enough data of each language for training. Thus, zero-shot adaptation of cross-lingual task-oriented dialog systems has been studied. Most of existing methods consider the word-level alignments to conduct two main tasks for task-oriented dialogue system, i.e., intent detection and slot filling, and they rarely explore the dependency relations among these two tasks. In this paper, we propose a hierarchical framework to classify the pre-defined intents in the high-level and fulfill slot filling under the guidance of intent in the low-level. Particularly, we incorporate sentence-level alignment among different languages to enhance the performance of intent detection. The extensive experiments report that our proposed method achieves the SOTA performance on a public task-oriented dialog dataset.",
        "author": "Zhanyu Ma; Jian Ye; Xurui Yang; Jianfeng Liu",
        "authorids": "/z/zhanyu-ma/; /j/jian-ye/; /x/xurui-yang/; /j/jianfeng-liu/",
        "bibtex": "@inproceedings{ma-etal-2022-hcld,\n    title = \"{HCLD}: A Hierarchical Framework for Zero-shot Cross-lingual Dialogue System\",\n    author = \"Ma, Zhanyu  and\n      Ye, Jian  and\n      Yang, Xurui  and\n      Liu, Jianfeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.396/\",\n    pages = \"4492--4498\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.396.pdf",
        "site": "https://aclanthology.org/2022.coling-1.396/",
        "pdf_size": 541442,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15844023669519748348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences+Beijing Key Laboratory of Mobile Computing and Pervasive Device; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences+Beijing Key Laboratory of Mobile Computing and Pervasive Device; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences+Beijing Key Laboratory of Mobile Computing and Pervasive Device; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences+Beijing Key Laboratory of Mobile Computing and Pervasive Device",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1+2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Key Laboratory of Mobile Computing and Pervasive Device",
        "aff_unique_dep": "Institute of Computing Technology;;Mobile Computing and Pervasive Device",
        "aff_unique_url": "http://www.ict.ac.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.279",
        "title": "HG2Vec: Improved Word Embeddings from Dictionary and Thesaurus Based Heterogeneous Graph",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Learning word embeddings is an essential topic in natural language processing. Most existing works use a vast corpus as a primary source while training, but this requires massive time and space for data pre-processing and model training. We propose a new model, HG2Vec, that learns word embeddings utilizing only dictionaries and thesauri. Our model reaches the state-of-art on multiple word similarity and relatedness benchmarks. We demonstrate that dictionaries and thesauri are effective resources to learn word embeddings. In addition, we exploit a new context-focused loss that models transitive relationships between word pairs and balances the performance between similarity and relatedness benchmarks, yielding superior results.",
        "author": "Qitong Wang; Mohammed J Zaki",
        "authorids": "/q/qitong-wang/; /m/mohammed-j-zaki/",
        "bibtex": "@inproceedings{wang-zaki-2022-hg2vec,\n    title = \"{HG}2{V}ec: Improved Word Embeddings from Dictionary and Thesaurus Based Heterogeneous Graph\",\n    author = \"Wang, Qitong  and\n      Zaki, Mohammed J\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.279/\",\n    pages = \"3154--3163\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.279.pdf",
        "site": "https://aclanthology.org/2022.coling-1.279/",
        "pdf_size": 485527,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17434168070779009008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Computer Science Department, Rensselaer Polytechnic Institute; Computer Science Department, Rensselaer Polytechnic Institute",
        "aff_domain": "rpi.edu;cs.rpi.edu",
        "email": "rpi.edu;cs.rpi.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.rpi.edu",
        "aff_unique_abbr": "RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.259",
        "title": "Harnessing Abstractive Summarization for Fact-Checked Claim Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Social media platforms have become new battlegrounds for anti-social elements, with misinformation being the weapon of choice. Fact-checking organizations try to debunk as many claims as possible while staying true to their journalistic processes but cannot cope with its rapid dissemination. We believe that the solution lies in partial automation of the fact-checking life cycle, saving human time for tasks which require high cognition. We propose a new workflow for efficiently detecting previously fact-checked claims that uses abstractive summarization to generate crisp queries. These queries can then be executed on a general-purpose retrieval system associated with a collection of previously fact-checked claims. We curate an abstractive text summarization dataset comprising noisy claims from Twitter and their gold summaries. It is shown that retrieval performance improves 2x by using popular out-of-the-box summarization models and 3x by fine-tuning them on the accompanying dataset compared to verbatim querying. Our approach achieves Recall@5 and MRR of 35% and 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset, code, and models are available publicly: https://github.com/varadhbhatnagar/FC-Claim-Det/.",
        "author": "Varad Bhatnagar; Diptesh Kanojia; Kameswari Chebrolu",
        "authorids": "/v/varad-bhatnagar/; /d/diptesh-kanojia/; /k/kameswari-chebrolu/",
        "bibtex": "@inproceedings{bhatnagar-etal-2022-harnessing,\n    title = \"Harnessing Abstractive Summarization for Fact-Checked Claim Detection\",\n    author = \"Bhatnagar, Varad  and\n      Kanojia, Diptesh  and\n      Chebrolu, Kameswari\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.259/\",\n    pages = \"2934--2945\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.259.pdf",
        "site": "https://aclanthology.org/2022.coling-1.259/",
        "pdf_size": 444862,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9084595596149208436&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, IIT Bombay, India; Surrey Institute for People-Centred AI+Department of Computer Science, University of Surrey, United Kingdom; Department of Computer Science and Engineering, IIT Bombay, India",
        "aff_domain": "cse.iitb.ac.in;surrey.ac.uk;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;surrey.ac.uk;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+1;0",
        "aff_unique_norm": "IIT Bombay;University of Surrey",
        "aff_unique_dep": "Department of Computer Science and Engineering;Surrey Institute for People-Centred AI",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.surrey.ac.uk",
        "aff_unique_abbr": "IITB;",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Bombay;",
        "aff_country_unique_index": "0;1+1;0",
        "aff_country_unique": "India;United Kingdom"
    },
    {
        "id": "2022.coling-1.545",
        "title": "HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Graph Neural Network (GNN)-based models have proven effective in various Natural Language Processing (NLP) tasks in recent years. Specifically, in the case of the Extractive Document Summarization (EDS) task, modeling documents under graph structure is able to analyze the complex relations between semantic units (e.g., word-to-word, word-to-sentence, sentence-to-sentence) and enrich sentence representations via valuable information from their neighbors. However, long-form document summarization using graph-based methods is still an open research issue. The main challenge is to represent long documents in a graph structure in an effective way. In this regard, this paper proposes a new heterogeneous graph neural network (HeterGNN) model to improve the performance of long document summarization (HeterGraphLongSum). Specifically, the main idea is to add the passage nodes into the heterogeneous graph structure of word and sentence nodes for enriching the final representation of sentences. In this regard, HeterGraphLongSum is designed with three types of semantic units such as word, sentence, and passage. Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model for the extractive long document summarization problem. Especially, HeterGraphLongSum is able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g., BERT). The source code is available for further exploitation on the Github.",
        "author": "Tuan-Anh Phan; Ngoc-Dung Ngoc Nguyen; Khac-Hoai Nam Bui",
        "authorids": "/t/tuan-anh-phan/; /n/ngoc-dung-ngoc-nguyen/; /k/khac-hoai-nam-bui/",
        "bibtex": "@inproceedings{phan-etal-2022-hetergraphlongsum,\n    title = \"{H}eter{G}raph{L}ong{S}um: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization\",\n    author = \"Phan, Tuan-Anh  and\n      Nguyen, Ngoc-Dung Ngoc  and\n      Bui, Khac-Hoai Nam\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.545/\",\n    pages = \"6248--6258\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.545.pdf",
        "site": "https://aclanthology.org/2022.coling-1.545/",
        "pdf_size": 524572,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7228556788885243036&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Viettel Cyperspace Center, Viettel Group, Vietnam; Viettel Cyperspace Center, Viettel Group, Vietnam; Viettel Cyperspace Center, Viettel Group, Vietnam",
        "aff_domain": "viettel.com.vn;viettel.com.vn;viettel.com.vn",
        "email": "viettel.com.vn;viettel.com.vn;viettel.com.vn",
        "github": "https://github.com/tuananhphan97vn/HeterGraphLongSum",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Viettel Group",
        "aff_unique_dep": "Viettel Cyperspace Center",
        "aff_unique_url": "https://www.viettel.com.vn",
        "aff_unique_abbr": "Viettel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2022.coling-1.9",
        "title": "Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic depression detection on Twitter can help individuals privately and conveniently understand their mental health status in the early stages before seeing mental health professionals. Most existing black-box-like deep learning methods for depression detection largely focused on improving classification performance. However, explaining model decisions is imperative in health research because decision-making can often be high-stakes and life-and-death. Reliable automatic diagnosis of mental health problems including depression should be supported by credible explanations justifying models\u2019 predictions. In this work, we propose a novel explainable model for depression detection on Twitter. It comprises a novel encoder combining hierarchical attention mechanisms and feed-forward neural networks. To support psycholinguistic studies, our model leverages metaphorical concept mappings as input. Thus, it not only detects depressed individuals, but also identifies features of such users\u2019 tweets and associated metaphor concept mappings.",
        "author": "Sooji Han; Rui Mao; Erik Cambria",
        "authorids": "/s/sooji-han/; /r/rui-mao/; /e/erik-cambria/",
        "bibtex": "@inproceedings{han-etal-2022-hierarchical,\n    title = \"Hierarchical Attention Network for Explainable Depression Detection on {T}witter Aided by Metaphor Concept Mappings\",\n    author = \"Han, Sooji  and\n      Mao, Rui  and\n      Cambria, Erik\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.9/\",\n    pages = \"94--104\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.9.pdf",
        "site": "https://aclanthology.org/2022.coling-1.9/",
        "pdf_size": 497612,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4736990848637894385&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science and Engineering, Nanyang Technological University; School of Computer Science and Engineering, Nanyang Technological University; School of Computer Science and Engineering, Nanyang Technological University",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "github": "https://github.com/senticnet/depression-detection",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.coling-1.79",
        "title": "Hierarchical Information Matters: Text Classification via Tree Based Graph Neural Network",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text classification is a primary task in natural language processing (NLP). Recently, graph neural networks (GNNs) have developed rapidly and been applied to text classification tasks. As a special kind of graph data, the tree has a simpler data structure and can provide rich hierarchical information for text classification. Inspired by the structural entropy, we construct the coding tree of the graph by minimizing the structural entropy and propose HINT, which aims to make full use of the hierarchical information contained in the text for the task of text classification. Specifically, we first establish a dependency parsing graph for each text. Then we designed a structural entropy minimization algorithm to decode the key information in the graph and convert each graph to its corresponding coding tree. Based on the hierarchical structure of the coding tree, the representation of the entire graph is obtained by updating the representation of non-leaf nodes in the coding tree layer by layer. Finally, we present the effectiveness of hierarchical information in text classification. Experimental results show that HINT outperforms the state-of-the-art methods on popular benchmarks while having a simple structure and few parameters.",
        "author": "Chong Zhang; He Zhu; Xingyu Peng; Junran Wu; Ke Xu",
        "authorids": "/c/chong-zhang/; /h/he-zhu/; /x/xingyu-peng/; /j/junran-wu/; /k/ke-xu/",
        "bibtex": "@inproceedings{zhang-etal-2022-hierarchical,\n    title = \"Hierarchical Information Matters: Text Classification via Tree Based Graph Neural Network\",\n    author = \"Zhang, Chong  and\n      Zhu, He  and\n      Peng, Xingyu  and\n      Wu, Junran  and\n      Xu, Ke\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.79/\",\n    pages = \"950--959\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.79.pdf",
        "site": "https://aclanthology.org/2022.coling-1.79/",
        "pdf_size": 409440,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=772119277325555811&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "State Key Lab of Software Development Environment, Beihang University, Beijing, 100191, China; State Key Lab of Software Development Environment, Beihang University, Beijing, 100191, China; State Key Lab of Software Development Environment, Beihang University, Beijing, 100191, China; State Key Lab of Software Development Environment, Beihang University, Beijing, 100191, China; State Key Lab of Software Development Environment, Beihang University, Beijing, 100191, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "State Key Lab of Software Development Environment",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "BUAA",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.127",
        "title": "Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, Biomedical Question Answering (BQA) has attracted growing attention due to its application value and technical challenges. Most existing works treat it as a semantic matching task that predicts answers by computing confidence among questions, options and evidence sentences, which is insufficient for scenarios that require complex reasoning based on a deep understanding of biomedical evidences. We propose a novel model termed Hierarchical Representation-based Dynamic Reasoning Network (HDRN) to tackle this problem. It first constructs the hierarchical representations for biomedical evidences to learn semantics within and among evidences. It then performs dynamic reasoning based on the hierarchical representations of evidences to solve complex biomedical problems. Against the existing state-of-the-art model, the proposed model significantly improves more than 4.5%, 3% and 1.3% on three mainstream BQA datasets, PubMedQA, MedQA-USMLE and NLPEC. The ablation study demonstrates the superiority of each improvement of our model. The code will be released after the paper is published.",
        "author": "Jianguo Mao; Jiyuan Zhang; Zengfeng Zeng; Weihua Peng; Wenbin Jiang; Xiangdong Wang; Hong Liu; Yajuan Lyu",
        "authorids": "/j/jianguo-mao/; /j/jiyuan-zhang/; /z/zengfeng-zeng/; /w/weihua-peng/; /w/wenbin-jiang/; /x/xiangdong-wang/; /h/hong-liu/; /y/yajuan-lyu/",
        "bibtex": "@inproceedings{mao-etal-2022-hierarchical,\n    title = \"Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering\",\n    author = \"Mao, Jianguo  and\n      Zhang, Jiyuan  and\n      Zeng, Zengfeng  and\n      Peng, Weihua  and\n      Jiang, Wenbin  and\n      Wang, Xiangdong  and\n      Liu, Hong  and\n      Lyu, Yajuan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.127/\",\n    pages = \"1480--1489\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.127.pdf",
        "site": "https://aclanthology.org/2022.coling-1.127/",
        "pdf_size": 674248,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14937509979513620&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Baidu Inc., Beijing, China",
        "aff_domain": "ict.ac.cn;baidu.com;baidu.com;baidu.com;baidu.com;ict.ac.cn;ict.ac.cn;baidu.com",
        "email": "ict.ac.cn;baidu.com;baidu.com;baidu.com;baidu.com;ict.ac.cn;ict.ac.cn;baidu.com",
        "github": "https://github.com/mikeblueskydl/HDRN",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;2;2;2;0;0;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Baidu Inc.",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.ict.cas.cn;http://www.ucas.ac.cn;https://www.baidu.com",
        "aff_unique_abbr": "CAS;UCAS;Baidu",
        "aff_campus_unique_index": "0+0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.283",
        "title": "How about Time? Probing a Multilingual Language Model for Temporal Relations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages. Results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddings. While obtaining competitive results against state-of-the-art systems, our probes indicate a lack of suitable encoded information to properly address this task.",
        "author": "Tommaso Caselli; Irene Dini; Felice Dell\u2019Orletta",
        "authorids": "/t/tommaso-caselli/; /i/irene-dini/; /f/felice-dellorletta/",
        "bibtex": "@inproceedings{caselli-etal-2022-time,\n    title = \"How about Time? Probing a Multilingual Language Model for Temporal Relations\",\n    author = \"Caselli, Tommaso  and\n      Dini, Irene  and\n      Dell{'}Orletta, Felice\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.283/\",\n    pages = \"3197--3209\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.283.pdf",
        "site": "https://aclanthology.org/2022.coling-1.283/",
        "pdf_size": 406559,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5024187028252751881&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "CLCG, University of Groningen; ItaliaNLP Lab, ILC-CNR \u201cAntonio Zampolli\u201d Pisa; ItaliaNLP Lab, ILC-CNR \u201cAntonio Zampolli\u201d Pisa",
        "aff_domain": "rug.nl;ilc.cnr.it;ilc.cnr.it",
        "email": "rug.nl;ilc.cnr.it;ilc.cnr.it",
        "github": "https://github.com/irenedini/tlink_probing",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Groningen;ILC-CNR",
        "aff_unique_dep": "CLCG;ItaliaNLP Lab",
        "aff_unique_url": "https://www.rug.nl;http://www.ilm-cnr.it",
        "aff_unique_abbr": ";ILC-CNR",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pisa",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Netherlands;Italy"
    },
    {
        "id": "2022.coling-1.494",
        "title": "How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current language models have been criticised for learning language from text alone without connection between words and their meaning. Consequently, multimodal training has been proposed as a way for creating models with better language understanding by providing the lacking connection. We focus on pre-trained multimodal vision-and-language (VL) models for which there already are some results on their language understanding capabilities. An unresolved issue with evaluating the linguistic skills of these models, however, is that there is no established method for adapting them to text-only input without out-of-distribution uncertainty. To find the best approach, we investigate and compare seven possible methods for adapting three different pre-trained VL models to text-only input. Our evaluations on both GLUE and Visual Property Norms (VPN) show that care should be put into adapting VL models to zero-shot text-only tasks, while the models are less sensitive to how we adapt them to non-zero-shot tasks. We also find that the adaptation methods perform differently for different models and that unimodal model counterparts perform on par with the VL models regardless of adaptation, indicating that current VL models do not necessarily gain better language understanding from their multimodal training.",
        "author": "Lovisa Hagstr\u00f6m; Richard Johansson",
        "authorids": "/l/lovisa-hagstrom/; /r/richard-johansson/",
        "bibtex": "@inproceedings{hagstrom-johansson-2022-adapt,\n    title = \"How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?\",\n    author = {Hagstr{\\\"o}m, Lovisa  and\n      Johansson, Richard},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.494/\",\n    pages = \"5582--5596\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.494.pdf",
        "site": "https://aclanthology.org/2022.coling-1.494/",
        "pdf_size": 400383,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16796641004373389922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Chalmers University of Technology; Chalmers University of Technology + University of Gothenburg",
        "aff_domain": "chalmers.se;chalmers.se",
        "email": "chalmers.se;chalmers.se",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Chalmers University of Technology;University of Gothenburg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.chalmers.se;https://www.gu.se",
        "aff_unique_abbr": "Chalmers;GU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Sweden"
    },
    {
        "id": "2022.coling-1.527",
        "title": "How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatically evaluating the coherence of summaries is of great significance both to enable cost-efficient summarizer evaluation and as a tool for improving coherence by selecting high-scoring candidate summaries. While many different approaches have been suggested to model summary coherence, they are often evaluated using disparate datasets and metrics. This makes it difficult to understand their relative performance and identify ways forward towards better summary coherence modelling. In this work, we conduct a large-scale investigation of various methods for summary coherence modelling on an even playing field. Additionally, we introduce two novel analysis measures, _intra-system correlation_ and _bias matrices_, that help identify biases in coherence measures and provide robustness against system-level confounders. While none of the currently available automatic coherence measures are able to assign reliable coherence scores to system summaries across all evaluation metrics, large-scale language models fine-tuned on self-supervised tasks show promising results, as long as fine-tuning takes into account that they need to generalize across different summary lengths.",
        "author": "Julius Steen; Katja Markert",
        "authorids": "/j/julius-steen/; /k/katja-markert/",
        "bibtex": "@inproceedings{steen-markert-2022-find,\n    title = \"How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation\",\n    author = \"Steen, Julius  and\n      Markert, Katja\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.527/\",\n    pages = \"6035--6049\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.527.pdf",
        "site": "https://aclanthology.org/2022.coling-1.527/",
        "pdf_size": 840598,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5459573365169413167&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2022.coling-1.387",
        "title": "How to Parse a Creole: When Martinican Creole Meets French",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We investigate methods to develop a parser for Martinican Creole, a highly under-resourced language, using a French treebank. We compare transfer learning and multi-task learning models and examine different input features and strategies to handle the massive size imbalance between the treebanks. Surprisingly, we find that a simple concatenated (French + Martinican Creole) baseline yields optimal results even though it has access to only 80 Martinican Creole sentences. POS embeddings work better than lexical ones, but they suffer from negative transfer.",
        "author": "Ludovic Mompelat; Daniel Dakota; Sandra K\u00fcbler",
        "authorids": "/l/ludovic-mompelat/; /d/daniel-dakota/; /s/sandra-kubler/",
        "bibtex": "@inproceedings{mompelat-etal-2022-parse,\n    title = \"How to Parse a Creole: When Martinican Creole Meets {F}rench\",\n    author = {Mompelat, Ludovic  and\n      Dakota, Daniel  and\n      K{\\\"u}bler, Sandra},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.387/\",\n    pages = \"4397--4406\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.387.pdf",
        "site": "https://aclanthology.org/2022.coling-1.387/",
        "pdf_size": 247567,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12828879322850893990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Indiana University; Indiana University; Indiana University",
        "aff_domain": "iu.edu;iu.edu;indiana.edu",
        "email": "iu.edu;iu.edu;indiana.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.265",
        "title": "Human-in-the-loop Robotic Grasping Using BERT Scene Representation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current NLP techniques have been greatly applied in different domains. In this paper, we propose a human-in-the-loop framework for robotic grasping in cluttered scenes, investigating a language interface to the grasping process, which allows the user to intervene by natural language commands. This framework is constructed on a state-of-the-art grasping baseline, where we substitute a scene-graph representation with a text representation of the scene using BERT. Experiments on both simulation and physical robot show that the proposed method outperforms conventional object-agnostic and scene-graph based methods in the literature. In addition, we find that with human intervention, performance can be significantly improved. Our dataset and code are available on our project website https://sites.google.com/view/hitl-grasping-bert.",
        "author": "Yaoxian Song; Penglei Sun; Pengfei Fang; Linyi Yang; Yanghua Xiao; Yue Zhang",
        "authorids": "/y/yaoxian-song/; /p/penglei-sun/; /p/pengfei-fang/; /l/linyi-yang/; /y/yanghua-xiao/; /y/yue-zhang/",
        "bibtex": "@inproceedings{song-etal-2022-human,\n    title = \"Human-in-the-loop Robotic Grasping Using {BERT} Scene Representation\",\n    author = \"Song, Yaoxian  and\n      Sun, Penglei  and\n      Fang, Pengfei  and\n      Yang, Linyi  and\n      Xiao, Yanghua  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.265/\",\n    pages = \"2992--3006\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.265.pdf",
        "site": "https://aclanthology.org/2022.coling-1.265/",
        "pdf_size": 3562679,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15434144836512255818&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Fudan University + School of Engineering, Westlake University; School of Computer Science, Fudan University; School of Engineering, Australian National University; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Computer Science, Fudan University; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;fudan.edu.cn;anu.edu.au;westlake.edu.cn;fudan.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;fudan.edu.cn;anu.edu.au;westlake.edu.cn;fudan.edu.cn;westlake.edu.cn",
        "github": "",
        "project": "https://sites.google.com/view/hitl-grasping-bert",
        "author_num": 6,
        "aff_unique_index": "0+1;0;2;1+3;0;1+3",
        "aff_unique_norm": "Fudan University;Westlake University;Australian National University;Westlake Institute for Advanced Study",
        "aff_unique_dep": "School of Computer Science;School of Engineering;School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.westlake.edu.cn;https://www.anu.edu.au;http://www.wias.org.cn/",
        "aff_unique_abbr": "Fudan;;ANU;WIAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;1;0+0;0;0+0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.coling-1.121",
        "title": "IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the rapid development of automatic fake news detection technology, fact extraction and verification (FEVER) has been attracting more attention. The task aims to extract the most related fact evidences from millions of open-domain Wikipedia documents and then verify the credibility of corresponding claims. Although several strong models have been proposed for the task and they have made great process, we argue that they fail to utilize multi-view contextual information and thus cannot obtain better performance. In this paper, we propose to integrate multi-view contextual information (IMCI) for fact extraction and verification. For each evidence sentence, we define two kinds of context, i.e. intra-document context and inter-document context. Intra-document context consists of the document title and all the other sentences from the same document. Inter-document context consists of all other evidences which may come from different documents. Then we integrate the multi-view contextual information to encode the evidence sentences to handle the task. Our experimental results on FEVER 1.0 shared task show that our IMCI framework makes great progress on both fact extraction and verification, and achieves state-of-the-art performance with a winning FEVER score of 73.96% and label accuracy of 77.25% on the online blind test set. We also conduct ablation study to detect the impact of multi-view contextual information.",
        "author": "Hao Wang; Yangguang Li; Zhen Huang; Yong Dou",
        "authorids": "/h/hao-wang/; /y/yangguang-li/; /z/zhen-huang/; /y/yong-dou/",
        "bibtex": "@inproceedings{wang-etal-2022-imci,\n    title = \"{IMCI}: Integrate Multi-view Contextual Information for Fact Extraction and Verification\",\n    author = \"Wang, Hao  and\n      Li, Yangguang  and\n      Huang, Zhen  and\n      Dou, Yong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.121/\",\n    pages = \"1412--1421\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.121.pdf",
        "site": "https://aclanthology.org/2022.coling-1.121/",
        "pdf_size": 759959,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14360127311409645897&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "National University of Defense Technology, Changsha; SenseTime, Beijing; National University of Defense Technology, Changsha; National University of Defense Technology, Changsha",
        "aff_domain": "nudt.edu.cn;sensetime.com;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;sensetime.com;nudt.edu.cn;nudt.edu.cn",
        "github": "https://github.com/phoenixsecularbird/IMCI",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "National University of Defense Technology;SenseTime",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nudt.edu.cn;https://www.sensetime.com",
        "aff_unique_abbr": "NUDT;SenseTime",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Changsha;Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.316",
        "title": "IMPARA: Impact-Based Metric for GEC Using Parallel Data",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic evaluation of grammatical error correction (GEC) is essential in developing useful GEC systems. Existing methods for automatic evaluation require multiple reference sentences or manual scores. However, such resources are expensive, thereby hindering automatic evaluation for various domains and correction styles. This paper proposes an Impact-based Metric for GEC using PARAllel data, IMPARA, which utilizes correction impacts computed by parallel data comprising pairs of grammatical/ungrammatical sentences. As parallel data is cheaper than manually assessing evaluation scores, IMPARA can reduce the cost of data creation for automatic evaluation. Correlations between IMPARA and human scores indicate that IMPARA is comparable or better than existing evaluation methods. Furthermore, we find that IMPARA can perform evaluations that fit different domains and correction styles trained on various parallel data.",
        "author": "Koki Maeda; Masahiro Kaneko; Naoaki Okazaki",
        "authorids": "/k/koki-maeda/; /m/masahiro-kaneko/; /n/naoaki-okazaki/",
        "bibtex": "@inproceedings{maeda-etal-2022-impara,\n    title = \"{IMPARA}: Impact-Based Metric for {GEC} Using Parallel Data\",\n    author = \"Maeda, Koki  and\n      Kaneko, Masahiro  and\n      Okazaki, Naoaki\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.316/\",\n    pages = \"3578--3588\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.316.pdf",
        "site": "https://aclanthology.org/2022.coling-1.316/",
        "pdf_size": 645131,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15751873347310948623&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;nlp.c.titech.ac.jp",
        "email": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;nlp.c.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.569",
        "title": "Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained models have brought remarkable success on the text summarization task. For dialogue summarization, the subdomain of text summarization, utterances are concatenated to flat text before being processed. As a result, existing summarization systems based on pre-trained models are unable to recognize the unique format of the speaker-utterance pair well in the dialogue. To investigate this issue, we conduct probing tests and manual analysis, and find that the powerful pre-trained model can not identify different speakers well in the conversation, which leads to various factual errors. Moreover, we propose three speaker-aware supervised contrastive learning (SCL) tasks: Token-level SCL, Turn-level SCL, and Global-level SCL. Comprehensive experiments demonstrate that our methods achieve significant performance improvement on two mainstream dialogue summarization datasets. According to detailed human evaluations, pre-trained models equipped with SCL tasks effectively generate summaries with better factual consistency.",
        "author": "Zhichao Geng; Ming Zhong; Zhangyue Yin; Xipeng Qiu; Xuanjing Huang",
        "authorids": "/z/zhichao-geng/; /m/ming-zhong/; /z/zhangyue-yin/; /x/xipeng-qiu/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{geng-etal-2022-improving-abstractive,\n    title = \"Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning\",\n    author = \"Geng, Zhichao  and\n      Zhong, Ming  and\n      Yin, Zhangyue  and\n      Qiu, Xipeng  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.569/\",\n    pages = \"6540--6546\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.569.pdf",
        "site": "https://aclanthology.org/2022.coling-1.569/",
        "pdf_size": 394435,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10160993989536657957&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn; ;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn; ;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.461",
        "title": "Improving Both Domain Robustness and Domain Adaptability in Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We consider two problems of NMT domain adaptation using meta-learning. First, we want to reach domain robustness, i.e., we want to reach high quality on both domains seen in the training data and unseen domains. Second, we want our systems to be adaptive, i.e., making it possible to finetune systems with just hundreds of in-domain parallel sentences. We study the domain adaptability of meta-learning when improving the domain robustness of the model. In this paper, we propose a novel approach, RMLNMT (Robust Meta-Learning Framework for Neural Machine Translation Domain Adaptation), which improves the robustness of existing meta-learning models. More specifically, we show how to use a domain classifier in curriculum learning and we integrate the word-level domain mixing model into the meta-learning framework with a balanced sampling strategy. Experiments on English-German and English-Chinese translation show that RMLNMT improves in terms of both domain robustness and domain adaptability in seen and unseen domains.",
        "author": "Wen Lai; Jind\u0159ich Libovick\u00fd; Alexander Fraser",
        "authorids": "/w/wen-lai/; /j/jindrich-libovicky/; /a/alexander-fraser/",
        "bibtex": "@inproceedings{lai-etal-2022-improving-domain,\n    title = \"Improving Both Domain Robustness and Domain Adaptability in Machine Translation\",\n    author = \"Lai, Wen  and\n      Libovick{\\'y}, Jind{\\v{r}}ich  and\n      Fraser, Alexander\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.461/\",\n    pages = \"5191--5204\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.461.pdf",
        "site": "https://aclanthology.org/2022.coling-1.461/",
        "pdf_size": 546999,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2568880153700784602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Information and Language Processing, LMU Munich, Germany; Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic; Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;ufal.mff.cuni.cz;cis.lmu.de",
        "email": "cis.lmu.de;ufal.mff.cuni.cz;cis.lmu.de",
        "github": "https://github.com/lavine-lmu/RMLNMT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "LMU Munich;Charles University",
        "aff_unique_dep": "Center for Information and Language Processing;Faculty of Mathematics and Physics",
        "aff_unique_url": "https://www.lmu.de;https://www.cuni.cz",
        "aff_unique_abbr": "LMU;Charles University",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Munich;Prague",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Czech Republic"
    },
    {
        "id": "2022.coling-1.627",
        "title": "Improving Code-switched ASR with Linguistic Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper seeks to improve the performance of automatic speech recognition (ASR) systems operating on code-switched speech. Code-switching refers to the alternation of languages within a conversation, a phenomenon that is of increasing importance considering the rapid rise in the number of bilingual speakers in the world. It is particularly challenging for ASR owing to the relative scarcity of code-switching speech and text data, even when the individual languages are themselves well-resourced. This paper proposes to overcome this challenge by applying linguistic theories in order to generate more realistic code-switching text, necessary for language modelling in ASR. Working with English-Spanish code-switching, we find that Equivalence Constraint theory and part-of-speech labelling are particularly helpful for text generation, and bring 2% improvement to ASR performance.",
        "author": "Jie Chi; Peter Bell",
        "authorids": "/j/jie-chi/; /p/peter-bell/",
        "bibtex": "@inproceedings{chi-bell-2022-improving,\n    title = \"Improving Code-switched {ASR} with Linguistic Information\",\n    author = \"Chi, Jie  and\n      Bell, Peter\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.627/\",\n    pages = \"7171--7176\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.627.pdf",
        "site": "https://aclanthology.org/2022.coling-1.627/",
        "pdf_size": 203595,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10419286704861738058&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Centre for Speech Technology Research, University of Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, UK",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "Centre for Speech Technology Research",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.68",
        "title": "Improving Commonsense Contingent Reasoning by Pseudo-data and Its Application to the Related Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Contingent reasoning is one of the essential abilities in natural language understanding, and many language resources annotated with contingent relations have been constructed. However, despite the recent advances in deep learning, the task of contingent reasoning is still difficult for computers. In this study, we focus on the reasoning of contingent relation between basic events. Based on the existing data construction method, we automatically generate large-scale pseudo-problems and incorporate the generated data into training. We also investigate the generality of contingent knowledge through quantitative evaluation by performing transfer learning on the related tasks: discourse relation analysis, the Japanese Winograd Schema Challenge, and the JCommonsenseQA. The experimental results show the effectiveness of utilizing pseudo-problems for both the commonsense contingent reasoning task and the related tasks, which suggests the importance of contingent reasoning.",
        "author": "Kazumasa Omura; Sadao Kurohashi",
        "authorids": "/k/kazumasa-omura/; /s/sadao-kurohashi/",
        "bibtex": "@inproceedings{omura-kurohashi-2022-improving,\n    title = \"Improving Commonsense Contingent Reasoning by Pseudo-data and Its Application to the Related Tasks\",\n    author = \"Omura, Kazumasa  and\n      Kurohashi, Sadao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.68/\",\n    pages = \"812--823\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.68.pdf",
        "site": "https://aclanthology.org/2022.coling-1.68/",
        "pdf_size": 1222442,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8474422741338294319&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Graduate School of Informatics, Kyoto University; Graduate School of Informatics, Kyoto University",
        "aff_domain": "nlp.ist.i.kyoto-u.ac.jp;nlp.ist.i.kyoto-u.ac.jp",
        "email": "nlp.ist.i.kyoto-u.ac.jp;nlp.ist.i.kyoto-u.ac.jp",
        "github": "",
        "project": "https://nlp.ist.i.kyoto-u.ac.jp/EN/?KUCI",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.163",
        "title": "Improving Continual Relation Extraction through Prototypical Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Continual relation extraction (CRE) aims to extract relations towards the continuous and iterative arrival of new data, of which the major challenge is the catastrophic forgetting of old tasks. In order to alleviate this critical problem for enhanced CRE performance, we propose a novel Continual Relation Extraction framework with Contrastive Learning, namely CRECL, which is built with a classification network and a prototypical contrastive network to achieve the incremental-class learning of CRE. Specifically, in the contrastive network a given instance is contrasted with the prototype of each candidate relations stored in the memory module. Such contrastive learning scheme ensures the data distributions of all tasks more distinguishable, so as to alleviate the catastrophic forgetting further. Our experiment results not only demonstrate our CRECL\u2019s advantage over the state-of-the-art baselines on two public datasets, but also verify the effectiveness of CRECL\u2019s contrastive learning on improving performance.",
        "author": "Chengwei Hu; Deqing Yang; Haoliang Jin; Zhen Chen; Yanghua Xiao",
        "authorids": "/c/chengwei-hu/; /d/deqing-yang/; /h/haoliang-jin/; /z/zhen-chen/; /y/yanghua-xiao/",
        "bibtex": "@inproceedings{hu-etal-2022-improving,\n    title = \"Improving Continual Relation Extraction through Prototypical Contrastive Learning\",\n    author = \"Hu, Chengwei  and\n      Yang, Deqing  and\n      Jin, Haoliang  and\n      Chen, Zhen  and\n      Xiao, Yanghua\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.163/\",\n    pages = \"1885--1895\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.163.pdf",
        "site": "https://aclanthology.org/2022.coling-1.163/",
        "pdf_size": 811934,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16651186664400425779&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Data Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.195",
        "title": "Improving Deep Embedded Clustering via Learning Cluster-level Representations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Driven by recent advances in neural networks, various Deep Embedding Clustering (DEC) based short text clustering models are being developed. In these works, latent representation learning and text clustering are performed simultaneously. Although these methods are becoming increasingly popular, they use pure cluster-oriented objectives, which can produce meaningless representations. To alleviate this problem, several improvements have been developed to introduce additional learning objectives in the clustering process, such as models based on contrastive learning. However, existing efforts rely heavily on learning meaningful representations at the instance level. They have limited focus on learning global representations, which are necessary to capture the overall data structure at the cluster level. In this paper, we propose a novel DEC model, which we named the deep embedded clustering model with cluster-level representation learning (DECCRL) to jointly learn cluster and instance level representations. Here, we extend the embedded topic modelling approach to introduce reconstruction constraints to help learn cluster-level representations. Experimental results on real-world short text datasets demonstrate that our model produces meaningful clusters.",
        "author": "Qing Yin; Zhihua Wang; Yunya Song; Yida Xu; Shuai Niu; Liang Bai; Yike Guo; Xian Yang",
        "authorids": "/q/qing-yin/; /z/zhihua-wang/; /y/yunya-song/; /y/yida-xu/; /s/shuai-niu/; /l/liang-bai/; /y/yike-guo/; /x/xian-yang/",
        "bibtex": "@inproceedings{yin-etal-2022-improving,\n    title = \"Improving Deep Embedded Clustering via Learning Cluster-level Representations\",\n    author = \"Yin, Qing  and\n      Wang, Zhihua  and\n      Song, Yunya  and\n      Xu, Yida  and\n      Niu, Shuai  and\n      Bai, Liang  and\n      Guo, Yike  and\n      Yang, Xian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.195/\",\n    pages = \"2226--2236\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.195.pdf",
        "site": "https://aclanthology.org/2022.coling-1.195/",
        "pdf_size": 2895127,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7655991397347960967&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Alliance Manchester Business School, The University of Manchester; Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory; Department of Journalism, Hong Kong Baptist University; Department of Mathematics, Hong Kong Baptist University; Department of Computer Science, Hong Kong Baptist University; School of Computer and Information Technology, Shanxi University; Department of Computer Science, Hong Kong Baptist University; Alliance Manchester Business School, The University of Manchester",
        "aff_domain": "manchester.ac.uk;zju.edu.cn;hkbu.edu.hk;hkbu.edu.hk;hkbu.edu.hk;shanxiuniversity.edu.cn;hkbu.edu.hk;manchester.ac.uk",
        "email": "manchester.ac.uk;zju.edu.cn;hkbu.edu.hk;hkbu.edu.hk;hkbu.edu.hk;shanxiuniversity.edu.cn;hkbu.edu.hk;manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1+2;3;3;3;4;3;0",
        "aff_unique_norm": "The University of Manchester;Zhejiang University;Shanghai AI Laboratory;Hong Kong Baptist University;Shanxi University",
        "aff_unique_dep": "Alliance Manchester Business School;Shanghai Institute for Advanced Study;;Department of Journalism;School of Computer and Information Technology",
        "aff_unique_url": "https://www.manchester.ac.uk;http://www.zju.edu.cn;https://www.shanghai-ai-lab.com;https://www.hkbu.edu.hk;http://www.sxu.edu.cn",
        "aff_unique_abbr": "UM;ZJU;SAIL;HKBU;",
        "aff_campus_unique_index": "0;1;3;3;3;3;0",
        "aff_campus_unique": "Manchester;Shanghai;;Hong Kong SAR",
        "aff_country_unique_index": "0;1+1;1;1;1;1;1;0",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "2022.coling-1.250",
        "title": "Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Social media spreads both real news and fake news in various domains including politics, health, entertainment, etc. It is crucial to automatically detect fake news, especially for news of influential domains like politics and health because they may lead to serious social impact, e.g., panic in the COVID-19 pandemic. Some studies indicate the correlation between domains and perform multi-domain fake news detection. However, these multi-domain methods suffer from a seesaw problem that the performance of some domains is often improved by hurting the performance of other domains, which could lead to an unsatisfying performance in the specific target domains. To address this issue, we propose a Domain- and Instance-level Transfer Framework for Fake News Detection (DITFEND), which could improve the performance of specific target domains. To transfer coarse-grained domain-level knowledge, we train a general model with data of all domains from the meta-learning perspective. To transfer fine-grained instance-level knowledge and adapt the general model to a target domain, a language model is trained on the target domain to evaluate the transferability of each data instance in source domains and re-weight the instance\u2019s contribution. Experiments on two real-world datasets demonstrate the effectiveness of DITFEND. According to both offline and online experiments, the DITFEND shows superior effectiveness for fake news detection.",
        "author": "Qiong Nan; Danding Wang; Yongchun Zhu; Qiang Sheng; Yuhui Shi; Juan Cao; Jintao Li",
        "authorids": "/q/qiong-nan/; /d/danding-wang/; /y/yongchun-zhu/; /q/qiang-sheng/; /y/yuhui-shi/; /j/juan-cao/; /j/jintao-li/",
        "bibtex": "https://aclanthology.org/2022.coling-1.250.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.250.pdf",
        "site": "https://aclanthology.org/2022.coling-1.250/",
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9462514209329782958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.384",
        "title": "Improving Low-resource RRG Parsing with Cross-lingual Self-training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper considers the task of parsing low-resource languages in a scenario where parallel English data and also a limited seed of annotated sentences in the target language are available, as for example in bootstrapping parallel treebanks. We focus on constituency parsing using Role and Reference Grammar (RRG), a theory that has so far been understudied in computational linguistics but that is widely used in typological research, i.e., in particular in the context of low-resource languages. Starting from an existing RRG parser, we propose two strategies for low-resource parsing: first, we extend the parsing model into a cross-lingual parser, exploiting the parallel data in the high-resource language and unsupervised word alignments by providing internal states of the source-language parser to the target-language parser. Second, we adopt self-training, thereby iteratively expanding the training data, starting from the seed, by including the most confident new parses in each round. Both in simulated scenarios and with a real low-resource language (Daakaka), we find substantial and complementary improvements from both self-training and cross-lingual parsing. Moreover, we also experimented with using gloss embeddings in addition to token embeddings in the target language, and this also improves results. Finally, starting from what we have for Daakaka, we also consider parsing a related language (Dalkalaen) where glosses and English translations are available but no annotated trees at all, i.e., a no-resource scenario wrt. syntactic annotations. We start with cross-lingual parser trained on Daakaka with glosses and use self-training to adapt it to Dalkalaen. The results are surprisingly good.",
        "author": "Kilian Evang; Laura Kallmeyer; Jakub Waszczuk; Kilu von Prince; Tatiana Bladier; Simon Petitjean",
        "authorids": "/k/kilian-evang/; /l/laura-kallmeyer/; /j/jakub-waszczuk/; /k/kilu-von-prince/; /t/tatiana-bladier/; /s/simon-petitjean/",
        "bibtex": "@inproceedings{evang-etal-2022-improving,\n    title = \"Improving Low-resource {RRG} Parsing with Cross-lingual Self-training\",\n    author = \"Evang, Kilian  and\n      Kallmeyer, Laura  and\n      Waszczuk, Jakub  and\n      von Prince, Kilu  and\n      Bladier, Tatiana  and\n      Petitjean, Simon\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.384/\",\n    pages = \"4360--4371\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.384.pdf",
        "site": "https://aclanthology.org/2022.coling-1.384/",
        "pdf_size": 424599,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14110976820780383483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany; Heinrich Heine University D\u00fcsseldorf, Germany",
        "aff_domain": "hhu.de;hhu.de;hhu.de;hhu.de;hhu.de;hhu.de",
        "email": "hhu.de;hhu.de;hhu.de;hhu.de;hhu.de;hhu.de",
        "github": "",
        "project": "https://gitlab.com/treegrasp/rrgproj2",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Heinrich Heine University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hhu.de",
        "aff_unique_abbr": "HHU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "D\u00fcsseldorf",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.463",
        "title": "Improving Non-Autoregressive Neural Machine Translation via Modeling Localness",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Non-autoregressive translation (NAT) models, which eliminate the sequential dependencies within the target sentence, have achieved remarkable inference speed, but suffer from inferior translation quality. Towards exploring the underlying causes, we carry out a thorough preliminary study on the attention mechanism, which demonstrates the serious weakness in capturing localness compared with conventional autoregressive translation (AT). In response to this problem, we propose to improve the localness of NAT models by explicitly introducing the information about surrounding words. Specifically, temporal convolutions are incorporated into both encoder and decoder sides to obtain localness-aware representations. Extensive experiments on several typical translation datasets show that the proposed method can achieve consistent and significant improvements over strong NAT baselines. Further analyses on the WMT14 En-De translation task reveal that compared with baselines, our approach accelerates the convergence in training and can achieve equivalent performance with a reduction of 70% training steps.",
        "author": "Yong Wang; Xinwei Geng",
        "authorids": "/y/yong-wang/; /x/xinwei-geng/",
        "bibtex": "@inproceedings{wang-geng-2022-improving,\n    title = \"Improving Non-Autoregressive Neural Machine Translation via Modeling Localness\",\n    author = \"Wang, Yong  and\n      Geng, Xinwei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.463/\",\n    pages = \"5217--5226\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.463.pdf",
        "site": "https://aclanthology.org/2022.coling-1.463/",
        "pdf_size": 1424292,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=469190491242652526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Tencent Corporation, China; Tencent Corporation, China",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tencent Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.24",
        "title": "Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via Intent Conditioning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantic parsing (SP) is a core component of modern virtual assistants like Google Assistant and Amazon Alexa. While sequence-to-sequence based auto-regressive (AR) approaches are common for conversational SP, recent studies employ non-autoregressive (NAR) decoders and reduce inference latency while maintaining competitive parsing quality. However, a major drawback of NAR decoders is the difficulty of generating top-k (i.e., k-best) outputs with approaches such as beam search. To address this challenge, we propose a novel NAR semantic parser that introduces intent conditioning on the decoder. Inspired by the traditional intent and slot tagging parsers, we decouple the top-level intent prediction from the rest of a parse. As the top-level intent largely governs the syntax and semantics of a parse, the intent conditioning allows the model to better control beam search and improves the quality and diversity of top-k outputs. We introduce a hybrid teacher-forcing approach to avoid training and inference mismatch. We evaluate the proposed NAR on conversational SP datasets, TOP & TOPv2. Like the existing NAR models, we maintain the O(1) decoding time complexity while generating more diverse outputs and improving top-3 exact match (EM) by 2.4 points. In comparison with AR models, our model speeds up beam search inference by 6.7 times on CPU with competitive top-k EM.",
        "author": "Geunseob Oh; Rahul Goel; Chris Hidey; Shachi Paul; Aditya Gupta; Pararth Shah; Rushin Shah",
        "authorids": "/g/geunseob-oh/; /r/rahul-goel/; /c/chris-hidey/; /s/shachi-paul/; /a/aditya-gupta/; /p/pararth-shah/; /r/rushin-shah/",
        "bibtex": "@inproceedings{oh-etal-2022-improving,\n    title = \"Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via Intent Conditioning\",\n    author = \"Oh, Geunseob  and\n      Goel, Rahul  and\n      Hidey, Chris  and\n      Paul, Shachi  and\n      Gupta, Aditya  and\n      Shah, Pararth  and\n      Shah, Rushin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.24/\",\n    pages = \"310--322\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.24.pdf",
        "site": "https://aclanthology.org/2022.coling-1.24/",
        "pdf_size": 690923,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17341412898125001919&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Google; Google; Google; Google; Google; Google; Google",
        "aff_domain": "google.com;google.com; ; ; ; ; ",
        "email": "google.com;google.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.214",
        "title": "Improving Zero-Shot Entity Linking Candidate Generation with Ultra-Fine Entity Type Information",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Entity linking, which aims at aligning ambiguous entity mentions to their referent entities in a knowledge base, plays a key role in multiple natural language processing tasks. Recently, zero-shot entity linking task has become a research hotspot, which links mentions to unseen entities to challenge the generalization ability. For this task, the training set and test set are from different domains, and thus entity linking models tend to be overfitting due to the tendency of memorizing the properties of entities that appear frequently in the training set. We argue that general ultra-fine-grained type information can help the linking models to learn contextual commonality and improve their generalization ability to tackle the overfitting problem. However, in the zero-shot entity linking setting, any type information is not available and entities are only identified by textual descriptions. Thus, we first extract the ultra-fine entity type information from the entity textual descriptions. Then, we propose a hierarchical multi-task model to improve the high-level zero-shot entity linking candidate generation task by utilizing the entity typing task as an auxiliary low-level task, which introduces extracted ultra-fine type information into the candidate generation task. Experimental results demonstrate the effectiveness of utilizing the ultra-fine entity type information and our proposed method achieves state-of-the-art performance.",
        "author": "Xuhui Sui; Ying Zhang; Kehui Song; Baohang Zhou; Guoqing Zhao; Xin Wei; Xiaojie Yuan",
        "authorids": "/x/xuhui-sui/; /y/ying-zhang/; /k/kehui-song/; /b/baohang-zhou/; /g/guoqing-zhao/; /x/xin-wei/; /x/xiaojie-yuan/",
        "bibtex": "@inproceedings{sui-etal-2022-improving,\n    title = \"Improving Zero-Shot Entity Linking Candidate Generation with Ultra-Fine Entity Type Information\",\n    author = \"Sui, Xuhui  and\n      Zhang, Ying  and\n      Song, Kehui  and\n      Zhou, Baohang  and\n      Zhao, Guoqing  and\n      Wei, Xin  and\n      Yuan, Xiaojie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.214/\",\n    pages = \"2429--2437\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.214.pdf",
        "site": "https://aclanthology.org/2022.coling-1.214/",
        "pdf_size": 609569,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11271380158261668241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science, TKLNDST, Nankai University, China; College of Computer Science, TKLNDST, Nankai University, China; College of Computer Science, TKLNDST, Nankai University, China; College of Computer Science, TKLNDST, Nankai University, China; Mashang Consumer Finanace Co, Ltd; Mashang Consumer Finanace Co, Ltd; College of Computer Science, TKLNDST, Nankai University, China",
        "aff_domain": "dbis.nankai.edu.cn;dbis.nankai.edu.cn;dbis.nankai.edu.cn;dbis.nankai.edu.cn;msxf.com;msxf.com;nankai.edu.cn",
        "email": "dbis.nankai.edu.cn;dbis.nankai.edu.cn;dbis.nankai.edu.cn;dbis.nankai.edu.cn;msxf.com;msxf.com;nankai.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;1;0",
        "aff_unique_norm": "Nankai University;Mashang Consumer Finance Co., Ltd.",
        "aff_unique_dep": "College of Computer Science;",
        "aff_unique_url": "http://www.nankai.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.513",
        "title": "Improving Zero-Shot Multilingual Text Generation via Iterative Distillation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The demand for multilingual dialogue systems often requires a costly labeling process, where human translators derive utterances in low resource languages from resource rich language annotation. To this end, we explore leveraging the inductive biases for target languages learned by numerous pretrained teacher models by transferring them to student models via sequence-level knowledge distillation. By assuming no target language text, the both the teacher and student models need to learn from the target distribution in a few/zero-shot manner. On the MultiATIS++ benchmark, we explore the effectiveness of our proposed technique to derive the multilingual text for 6 languages, using only the monolingual English data and the pretrained models. We show that training on the synthetic multilingual generation outputs yields close performance to training on human annotations in both slot F1 and intent accuracy; the synthetic text also scores high in naturalness and correctness based on human evaluation.",
        "author": "Ernie Chang; Alex Marin; Vera Demberg",
        "authorids": "/e/ernie-chang/; /a/alex-marin/; /v/vera-demberg/",
        "bibtex": "@inproceedings{chang-etal-2022-improving,\n    title = \"Improving Zero-Shot Multilingual Text Generation via Iterative Distillation\",\n    author = \"Chang, Ernie  and\n      Marin, Alex  and\n      Demberg, Vera\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.513/\",\n    pages = \"5876--5881\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.513.pdf",
        "site": "https://aclanthology.org/2022.coling-1.513/",
        "pdf_size": 427787,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15406098620654592077&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of Language Science and Technology, Saarland University; Microsoft Corporation, Redmond, WA; Dept. of Language Science and Technology, Saarland University",
        "aff_domain": "coli.uni-saarland.de; ;coli.uni-saarland.de",
        "email": "coli.uni-saarland.de; ;coli.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Saarland University;Microsoft Corporation",
        "aff_unique_dep": "Dept. of Language Science and Technology;",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.microsoft.com",
        "aff_unique_abbr": "Saarland U;Microsoft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2022.coling-1.496",
        "title": "In-the-Wild Video Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing video understanding datasets mostly focus on human interactions, with little attention being paid to the \u201cin the wild\u201d settings, where the videos are recorded outdoors. We propose WILDQA, a video understanding dataset of videos recorded in outside settings. In addition to video question answering (Video QA), we also introduce the new task of identifying visual support for a given question and answer (Video Evidence Selection). Through evaluations using a wide range of baseline models, we show that WILDQA poses new challenges to the vision and language research communities. The dataset is available at https: //lit.eecs.umich.edu/wildqa/.",
        "author": "Santiago Castro; Naihao Deng; Pingxuan Huang; Mihai Burzo; Rada Mihalcea",
        "authorids": "/s/santiago-castro/; /n/naihao-deng/; /p/pingxuan-huang/; /m/mihai-burzo/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{castro-etal-2022-wild,\n    title = \"In-the-Wild Video Question Answering\",\n    author = \"Castro, Santiago  and\n      Deng, Naihao  and\n      Huang, Pingxuan  and\n      Burzo, Mihai  and\n      Mihalcea, Rada\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.496/\",\n    pages = \"5613--5635\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.496.pdf",
        "site": "https://aclanthology.org/2022.coling-1.496/",
        "pdf_size": 2016607,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "https://lit.eecs.umich.edu/wildqa/",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.30",
        "title": "Incorporating Causal Analysis into Diversified and Logical Response Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Although the Conditional Variational Auto-Encoder (CVAE) model can generate more diversified responses than the traditional Seq2Seq model, the responses often have low relevance with the input words or are illogical with the question. A causal analysis is carried out to study the reasons behind, and a methodology of searching for the mediators and mitigating the confounding bias in dialogues is provided. Specifically, we propose to predict the mediators to preserve relevant information and auto-regressively incorporate the mediators into generating process. Besides, a dynamic topic graph guided conditional variational auto-encoder (TGG-CVAE) model is utilized to complement the semantic space and reduce the confounding bias in responses. Extensive experiments demonstrate that the proposed model is able to generate both relevant and informative responses, and outperforms the state-of-the-art in terms of automatic metrics and human evaluations.",
        "author": "Jiayi Liu; Wei Wei; Zhixuan Chu; Xing Gao; Ji Zhang; Tan Yan; Yulin Kang",
        "authorids": "/j/jiayi-liu/; /w/wei-wei/; /z/zhixuan-chu/; /x/xing-gao/; /j/ji-zhang/; /t/tan-yan/; /y/yulin-kang/",
        "bibtex": "@inproceedings{liu-etal-2022-incorporating-causal,\n    title = \"Incorporating Causal Analysis into Diversified and Logical Response Generation\",\n    author = \"Liu, Jiayi  and\n      Wei, Wei  and\n      Chu, Zhixuan  and\n      Gao, Xing  and\n      Zhang, Ji  and\n      Yan, Tan  and\n      Kang, Yulin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.30/\",\n    pages = \"378--388\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.30.pdf",
        "site": "https://aclanthology.org/2022.coling-1.30/",
        "pdf_size": 602099,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17238589253076793802&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.631",
        "title": "Incorporating Instructional Prompts into a Unified Generative Framework for Joint Multiple Intent Detection and Slot Filling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The joint multiple Intent Detection (ID) and Slot Filling (SF) is a significant challenge in spoken language understanding. Because the slots in an utterance may relate to multi-intents, most existing approaches focus on utilizing task-specific components to capture the relations between intents and slots. The customized networks restrict models from modeling commonalities between tasks and generalization for broader applications. To address the above issue, we propose a Unified Generative framework (UGEN) based on a prompt-based paradigm, and formulate the task as a question-answering problem. Specifically, we design 5-type templates as instructional prompts, and each template includes a question that acts as the driver to teach UGEN to grasp the paradigm, options that list the candidate intents or slots to reduce the answer search space, and the context denotes original utterance. Through the instructional prompts, UGEN is guided to understand intents, slots, and their implicit correlations. On two popular multi-intent benchmark datasets, experimental results demonstrate that UGEN achieves new SOTA performances on full-data and surpasses the baselines by a large margin on 5-shot (28.1%) and 10-shot (23%) scenarios, which verify that UGEN is robust and effective.",
        "author": "Yangjun Wu; Han Wang; Dongxiang Zhang; Gang Chen; Hao Zhang",
        "authorids": "/y/yangjun-wu/; /h/han-wang/; /d/dongxiang-zhang/; /g/gang-chen/; /h/hao-zhang/",
        "bibtex": "@inproceedings{wu-etal-2022-incorporating,\n    title = \"Incorporating Instructional Prompts into a Unified Generative Framework for Joint Multiple Intent Detection and Slot Filling\",\n    author = \"Wu, Yangjun  and\n      Wang, Han  and\n      Zhang, Dongxiang  and\n      Chen, Gang  and\n      Zhang, Hao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.631/\",\n    pages = \"7203--7208\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.631.pdf",
        "site": "https://aclanthology.org/2022.coling-1.631/",
        "pdf_size": 652988,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12033171733959650840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zjuici.com;zju.edu.cn;zju.edu.cn;zju.edu.cn;zjuici.com",
        "email": "zjuici.com;zju.edu.cn;zju.edu.cn;zju.edu.cn;zjuici.com",
        "github": "https://github.com/Young1993/UGEN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.189",
        "title": "Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Lifelong event detection aims to incrementally update a model with new event types and data while retaining the capability on previously learned old types. One critical challenge is that the model would catastrophically forget old types when continually trained on new data. In this paper, we introduce Episodic Memory Prompts (EMP) to explicitly retain the learned task-specific knowledge. Our method adopts continuous prompt for each task and they are optimized to instruct the model prediction and learn event-specific representation. The EMPs learned in previous tasks are carried along with the model in subsequent tasks, and can serve as a memory module that keeps the old knowledge and transferring to new tasks. Experiment results demonstrate the effectiveness of our method. Furthermore, we also conduct a comprehensive analysis of the new and old event types in lifelong learning.",
        "author": "Minqian Liu; Shiyu Chang; Lifu Huang",
        "authorids": "/m/minqian-liu/; /s/shiyu-chang/; /l/lifu-huang/",
        "bibtex": "@inproceedings{liu-etal-2022-incremental,\n    title = \"Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection\",\n    author = \"Liu, Minqian  and\n      Chang, Shiyu  and\n      Huang, Lifu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.189/\",\n    pages = \"2157--2165\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.189.pdf",
        "site": "https://aclanthology.org/2022.coling-1.189/",
        "pdf_size": 1251618,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17014826067773525693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Virginia Tech; University of California Santa Barbara; Virginia Tech",
        "aff_domain": "vt.edu;ucsb.edu;vt.edu",
        "email": "vt.edu;ucsb.edu;vt.edu",
        "github": "https://github.com/VT-NLP/Incremental_Prompting",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Virginia Tech;University of California, Santa Barbara",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vt.edu;https://www.ucsb.edu",
        "aff_unique_abbr": "VT;UCSB",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.340",
        "title": "InferES : A Natural Language Inference Corpus for Spanish Featuring Negation-Based Contrastive and Adversarial Examples",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper we present InferES - an original corpus for Natural Language Inference (NLI) in European Spanish. We propose, implement, and analyze a variety of corpus-creating strategies utilizing expert linguists and crowd workers. The objectives behind InferES are to provide high-quality data, and at the same time to facilitate the systematic evaluation of automated systems. Specifically, we focus on measuring and improving the performance of machine learning systems on negation-based adversarial examples and their ability to generalize across out-of-distribution topics. We train two transformer models on InferES (8,055 gold examples) in a variety of scenarios. Our best model obtains 72.8% accuracy, leaving a lot of room for improvement. The \u201chypothesis-only\u201d baseline performs only 2%-5% higher than majority, indicating much fewer annotation artifacts than prior work. We show that models trained on InferES generalize very well across topics (both in- and out-of-distribution) and perform moderately well on negation-based adversarial examples.",
        "author": "Venelin Kovatchev; Mariona Taul\u00e9",
        "authorids": "/v/venelin-kovatchev/; /m/mariona-taule/",
        "bibtex": "@inproceedings{kovatchev-taule-2022-inferes,\n    title = \"{I}nfer{ES} : A Natural Language Inference Corpus for {S}panish Featuring Negation-Based Contrastive and Adversarial Examples\",\n    author = \"Kovatchev, Venelin  and\n      Taul{\\'e}, Mariona\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.340/\",\n    pages = \"3873--3884\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.340.pdf",
        "site": "https://aclanthology.org/2022.coling-1.340/",
        "pdf_size": 380263,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=132898063437764933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Information, The University of Texas at Austin; Centre de Llenguatge i Computaci\u00f3, Institut de Recerca en Sistemes Complexos, Universitat de Barcelona",
        "aff_domain": "utexas.edu;ub.edu",
        "email": "utexas.edu;ub.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "The University of Texas at Austin;Universitat de Barcelona",
        "aff_unique_dep": "School of Information;Centre de Llenguatge i Computaci\u00f3, Institut de Recerca en Sistemes Complexos",
        "aff_unique_url": "https://www.utexas.edu;https://www.ub.edu",
        "aff_unique_abbr": "UT Austin;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Spain"
    },
    {
        "id": "2022.coling-1.458",
        "title": "Informative Language Representation Learning for Massively Multilingual Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In a multilingual neural machine translation model that fully shares parameters across all languages, an artificial language token is usually used to guide translation into the desired target language. However, recent studies show that prepending language tokens sometimes fails to navigate the multilingual neural machine translation models into right translation directions, especially on zero-shot translation. To mitigate this issue, we propose two methods, language embedding embodiment and language-aware multi-head attention, to learn informative language representations to channel translation into right directions. The former embodies language embeddings into different critical switching points along the information flow from the source to the target, aiming at amplifying translation direction guiding signals. The latter exploits a matrix, instead of a vector, to represent a language in the continuous space. The matrix is chunked into multiple heads so as to learn language representations in multiple subspaces. Experiment results on two datasets for massively multilingual neural machine translation demonstrate that language-aware multi-head attention benefits both supervised and zero-shot translation and significantly alleviates the off-target translation issue. Further linguistic typology prediction experiments show that matrix-based language representations learned by our methods are capable of capturing rich linguistic typology features.",
        "author": "Renren Jin; Deyi Xiong",
        "authorids": "/r/renren-jin/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{jin-xiong-2022-informative,\n    title = \"Informative Language Representation Learning for Massively Multilingual Neural Machine Translation\",\n    author = \"Jin, Renren  and\n      Xiong, Deyi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.458/\",\n    pages = \"5158--5174\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.458.pdf",
        "site": "https://aclanthology.org/2022.coling-1.458/",
        "pdf_size": 737962,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14312579638320268490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn",
        "github": "https://github.com/cordercorder/nmt-multi",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.454",
        "title": "Interactive Post-Editing for Verbosity Controlled Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We explore Interactive Post-Editing (IPE) models for human-in-loop translation to help correct translation errors and rephrase it with a desired style variation. We specifically study verbosity for style variations and build on top of multi-source transformers that can read source and hypothesis to improve the latter with user inputs. Token-level interaction inputs for error corrections and length interaction inputs for verbosity control are used by the model to generate a suitable translation. We report BERTScore to evaluate semantic quality with other relevant metrics for translations from English to German, French and Spanish languages. Our model achieves superior BERTScore over state-of-the-art machine translation models while maintaining the desired token-level and verbosity preference.",
        "author": "Prabhakar Gupta; Anil Nelakanti; Grant M. Berry; Abhishek Sharma",
        "authorids": "/p/prabhakar-gupta/; /a/anil-nelakanti/; /g/grant-m-berry/; /a/abhishek-sharma/",
        "bibtex": "@inproceedings{gupta-etal-2022-interactive,\n    title = \"Interactive Post-Editing for Verbosity Controlled Translation\",\n    author = \"Gupta, Prabhakar  and\n      Nelakanti, Anil  and\n      Berry, Grant M.  and\n      Sharma, Abhishek\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.454/\",\n    pages = \"5119--5128\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.454.pdf",
        "site": "https://aclanthology.org/2022.coling-1.454/",
        "pdf_size": 323720,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8854281820018951316&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon Prime Video; Amazon Prime Video; Department of Spanish, Villanova University; Amazon Prime Video",
        "aff_domain": "amazon.com;amazon.com;villanova.edu;amazon.com",
        "email": "amazon.com;amazon.com;villanova.edu;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Amazon;Villanova University",
        "aff_unique_dep": "Prime Video;Department of Spanish",
        "aff_unique_url": "https://www.primevideo.com;https://www.villanova.edu",
        "aff_unique_abbr": "Amazon Prime Video;Villanova",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.65",
        "title": "Investigating the Performance of Transformer-Based NLI Models on Presuppositional Inferences",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Presuppositions are assumptions that are taken for granted by an utterance, and identifying them is key to a pragmatic interpretation of language. In this paper, we investigate the capabilities of transformer models to perform NLI on cases involving presupposition. First, we present simple heuristics to create alternative \u201ccontrastive\u201d test cases based on the ImpPres dataset and investigate the model performance on those test cases. Second, to better understand how the model is making its predictions, we analyze samples from sub-datasets of ImpPres and examine model performance on them. Overall, our findings suggest that NLI-trained transformer models seem to be exploiting specific structural and lexical cues as opposed to performing some kind of pragmatic reasoning.",
        "author": "Jad Kabbara; Jackie Chi Kit Cheung",
        "authorids": "/j/jad-kabbara/; /j/jackie-chi-kit-cheung/",
        "bibtex": "@inproceedings{kabbara-cheung-2022-investigating,\n    title = \"Investigating the Performance of Transformer-Based {NLI} Models on Presuppositional Inferences\",\n    author = \"Kabbara, Jad  and\n      Cheung, Jackie Chi Kit\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.65/\",\n    pages = \"779--785\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.65.pdf",
        "site": "https://aclanthology.org/2022.coling-1.65/",
        "pdf_size": 165532,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14083790417436542930&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, McGill University, Montreal, QC, Canada + Montreal Institute for Learning Algorithms (Mila), Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada + Montreal Institute for Learning Algorithms (Mila), Montreal, QC, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "McGill University;Montreal Institute for Learning Algorithms",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.mcgill.ca;https://mila.quebec",
        "aff_unique_abbr": "McGill;Mila",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.448",
        "title": "Iterative Constrained Back-Translation for Unsupervised Domain Adaptation of Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Back-translation has been proven to be effective in unsupervised domain adaptation of neural machine translation (NMT). However, the existing back-translation methods mainly improve domain adaptability by generating in-domain pseudo-parallel data that contains sentence-structural knowledge, paying less attention to the in-domain lexical knowledge, which may lead to poor translation of unseen in-domain words. In this paper, we propose an Iterative Constrained Back-Translation (ICBT) method to incorporate in-domain lexical knowledge on the basis of BT for unsupervised domain adaptation of NMT. Specifically, we apply lexical constraints into back-translation to generate pseudo-parallel data with in-domain lexical knowledge, and then perform round-trip iterations to incorporate more lexical knowledge. Based on this, we further explore sampling strategies of constrained words in ICBT to introduce more targeted lexical knowledge, via domain specificity and confidence estimation. Experimental results on four domains show that our approach achieves state-of-the-art results, improving the BLEU score by up to 3.08 compared to the strongest baseline, which demonstrates the effectiveness of our approach.",
        "author": "Hongxiao Zhang; Hui Huang; Jiale Gao; Yufeng Chen; Jinan Xu; Jian Liu",
        "authorids": "/h/hongxiao-zhang/; /h/hui-huang/; /j/jiale-gao/; /y/yufeng-chen/; /j/jinan-xu/; /j/jian-liu/",
        "bibtex": "@inproceedings{zhang-etal-2022-iterative,\n    title = \"Iterative Constrained Back-Translation for Unsupervised Domain Adaptation of Machine Translation\",\n    author = \"Zhang, Hongxiao  and\n      Huang, Hui  and\n      Gao, Jiale  and\n      Chen, Yufeng  and\n      Xu, Jinan  and\n      Liu, Jian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.448/\",\n    pages = \"5054--5065\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.448.pdf",
        "site": "https://aclanthology.org/2022.coling-1.448/",
        "pdf_size": 1349537,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14570936713468590463&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing Jiaotong University; Harbin Institute of Technology; Beijing Jiaotong University; Beijing Jiaotong University; Beijing Jiaotong University; Beijing Jiaotong University",
        "aff_domain": "bjtu.edu.cn;126.com;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn",
        "email": "bjtu.edu.cn;126.com;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn",
        "github": "https://github.com/zzzxiaohong/ICBT",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Beijing Jiaotong University;Harbin Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.njtu.edu.cn/en;http://www.hit.edu.cn/",
        "aff_unique_abbr": "BJTU;HIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Harbin",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.478",
        "title": "Iterative Span Selection: Self-Emergence of Resolving Orders in Semantic Role Labeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantic Role Labeling (SRL) is the task of labeling semantic arguments for marked semantic predicates. Semantic arguments and their predicates are related in various distinct manners, of which certain semantic arguments are a necessity while others serve as an auxiliary to their predicates. To consider such roles and relations of the arguments in the labeling order, we introduce iterative argument identification (IAI), which combines global decoding and iterative identification for the semantic arguments. In experiments, we first realize that the model with random argument labeling orders outperforms other heuristic orders such as the conventional left-to-right labeling order. Combined with simple reinforcement learning, the proposed model spontaneously learns the optimized labeling orders that are different from existing heuristic orders. The proposed model with the IAI algorithm achieves competitive or outperforming results from the existing models in the standard benchmark datasets of span-based SRL: CoNLL-2005 and CoNLL-2012.",
        "author": "Shuhei Kurita; Hiroki Ouchi; Kentaro Inui; Satoshi Sekine",
        "authorids": "/s/shuhei-kurita/; /h/hiroki-ouchi/; /k/kentaro-inui/; /s/satoshi-sekine/",
        "bibtex": "@inproceedings{kurita-etal-2022-iterative,\n    title = \"Iterative Span Selection: Self-Emergence of Resolving Orders in Semantic Role Labeling\",\n    author = \"Kurita, Shuhei  and\n      Ouchi, Hiroki  and\n      Inui, Kentaro  and\n      Sekine, Satoshi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.478/\",\n    pages = \"5383--5397\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.478.pdf",
        "site": "https://aclanthology.org/2022.coling-1.478/",
        "pdf_size": 2354458,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17335380404903444494&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "RIKEN+JST PRESTO; Nara Institute of Science and Technology+RIKEN; Tohoku University+RIKEN; RIKEN",
        "aff_domain": "riken.jp;is.naist.jp;tohoku.ac.jp;riken.jp",
        "email": "riken.jp;is.naist.jp;tohoku.ac.jp;riken.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2+0;3+0;0",
        "aff_unique_norm": "RIKEN;Japan Science and Technology Agency;Nara Institute of Science and Technology;Tohoku University",
        "aff_unique_dep": ";PRESTO;;",
        "aff_unique_url": "https://www.riken.jp;https://www.jst.go.jp;https://www.nist.go.jp;https://www.tohoku.ac.jp",
        "aff_unique_abbr": "RIKEN;JST;NIST;Tohoku U",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.523",
        "title": "JPG - Jointly Learn to Align: Automated Disease Prediction and Radiology Report Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automated radiology report generation aims to generate paragraphs that describe fine-grained visual differences among cases, especially those between the normal and the diseased. Existing methods seldom consider the cross-modal alignment between textual and visual features and tend to ignore disease tags as an auxiliary for report generation. To bridge the gap between textual and visual information, in this study, we propose a \u201cJointly learning framework for automated disease Prediction and radiology report Generation (JPG)\u201d to improve the quality of reports through the interaction between the main task (report generation) and two auxiliary tasks (feature alignment and disease prediction). The feature alignment and disease prediction help the model learn text-correlated visual features and record diseases as keywords so that it can output high-quality reports. Besides, the improved reports in turn provide additional harder samples for feature alignment and disease prediction to learn more precise visual and textual representations and improve prediction accuracy. All components are jointly trained in a manner that helps improve them iteratively and progressively. Experimental results demonstrate the effectiveness of JPG on the most commonly used IU X-RAY dataset, showing its superior performance over multiple state-of-the-art image captioning and medical report generation methods with regard to BLEU, METEOR, and ROUGE metrics.",
        "author": "Jingyi You; Dongyuan Li; Manabu Okumura; Kenji Suzuki",
        "authorids": "/j/jingyi-you/; /d/dongyuan-li/; /m/manabu-okumura/; /k/kenji-suzuki/",
        "bibtex": "@inproceedings{you-etal-2022-jpg,\n    title = \"{JPG} - Jointly Learn to Align: Automated Disease Prediction and Radiology Report Generation\",\n    author = \"You, Jingyi  and\n      Li, Dongyuan  and\n      Okumura, Manabu  and\n      Suzuki, Kenji\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.523/\",\n    pages = \"5989--6001\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.523.pdf",
        "site": "https://aclanthology.org/2022.coling-1.523/",
        "pdf_size": 1752701,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10064996110940997750&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Tokyo Institute of Technology\u2661; Tokyo Institute of Technology\u2661; Tokyo Institute of Technology\u2661; Tokyo Institute of Technology\u2660",
        "aff_domain": "lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;m.titech.ac.jp",
        "email": "lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;m.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.606",
        "title": "Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Emotion cause pair extraction (ECPE), as one of the derived subtasks of emotion cause analysis (ECA), shares rich inter-related features with emotion extraction (EE) and cause extraction (CE). Therefore EE and CE are frequently utilized as auxiliary tasks for better feature learning, modeled via multi-task learning (MTL) framework by prior works to achieve state-of-the-art (SoTA) ECPE results. However, existing MTL-based methods either fail to simultaneously model the specific features and the interactive feature in between, or suffer from the inconsistency of label prediction. In this work, we consider addressing the above challenges for improving ECPE by performing two alignment mechanisms with a novel A\u02c62Net model. We first propose a feature-task alignment to explicitly model the specific emotion-&cause-specific features and the shared interactive feature. Besides, an inter-task alignment is implemented, in which the label distance between the ECPE and the combinations of EE&CE are learned to be narrowed for better label consistency. Evaluations of benchmarks show that our methods outperform current best-performing systems on all ECA subtasks. Further analysis proves the importance of our proposed alignment mechanisms for the task.",
        "author": "Shunjie Chen; Xiaochuan Shi; Jingye Li; Shengqiong Wu; Hao Fei; Fei Li; Donghong Ji",
        "authorids": "/s/shunjie-chen/; /x/xiaochuan-shi/; /j/jingye-li/; /s/shengqiong-wu/; /h/hao-fei/; /f/fei-li/; /d/donghong-ji/",
        "bibtex": "@inproceedings{chen-etal-2022-joint,\n    title = \"Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction\",\n    author = \"Chen, Shunjie  and\n      Shi, Xiaochuan  and\n      Li, Jingye  and\n      Wu, Shengqiong  and\n      Fei, Hao  and\n      Li, Fei  and\n      Ji, Donghong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.606/\",\n    pages = \"6955--6965\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.606.pdf",
        "site": "https://aclanthology.org/2022.coling-1.606/",
        "pdf_size": 527463,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11633277862072535284&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/csj199813/A2Net_ECPE",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.41",
        "title": "Joint Goal Segmentation and Goal Success Prediction on Multi-Domain Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "To evaluate the performance of a multi-domain goal-oriented Dialogue System (DS), it is important to understand what the users\u2019 goals are for the conversations and whether those goals are successfully achieved. The success rate of goals directly correlates with user satisfaction and perceived usefulness of the DS. In this paper, we propose a novel automatic dialogue evaluation framework that jointly performs two tasks: goal segmentation and goal success prediction. We extend the RoBERTa-IQ model (Gupta et al., 2021) by adding multi-task learning heads for goal segmentation and success prediction. Using an annotated dataset from a commercial DS, we demonstrate that our proposed model reaches an accuracy that is on-par with single-pass human annotation comparing to a three-pass gold annotation benchmark.",
        "author": "Meiguo Wang; Benjamin Yao; Bin Guo; Xiaohu Liu; Yu Zhang; Tuan-Hung Pham; Chenlei Guo",
        "authorids": "/m/meiguo-wang/; /b/benjamin-yao/; /b/bin-guo/; /x/xiaohu-liu/; /y/yu-zhang/; /t/tuan-hung-pham/; /c/chenlei-guo/",
        "bibtex": "@inproceedings{wang-etal-2022-joint,\n    title = \"Joint Goal Segmentation and Goal Success Prediction on Multi-Domain Conversations\",\n    author = \"Wang, Meiguo  and\n      Yao, Benjamin  and\n      Guo, Bin  and\n      Liu, Xiaohu  and\n      Zhang, Yu  and\n      Pham, Tuan-Hung  and\n      Guo, Chenlei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.41/\",\n    pages = \"505--509\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.41.pdf",
        "site": "https://aclanthology.org/2022.coling-1.41/",
        "pdf_size": 291318,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18056993763149666145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.171",
        "title": "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.",
        "author": "Jianhao Shen; Chenguang Wang; Linyuan Gong; Dawn Song",
        "authorids": "/j/jianhao-shen/; /c/chenguang-wang/; /l/linyuan-gong/; /d/dawn-song/",
        "bibtex": "@inproceedings{shen-etal-2022-joint,\n    title = \"Joint Language Semantic and Structure Embedding for Knowledge Graph Completion\",\n    author = \"Shen, Jianhao  and\n      Wang, Chenguang  and\n      Gong, Linyuan  and\n      Song, Dawn\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.171/\",\n    pages = \"1965--1978\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.171.pdf",
        "site": "https://aclanthology.org/2022.coling-1.171/",
        "pdf_size": 753784,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3636315181226579097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Peking University; Washington University in St. Louis; UC Berkeley; UC Berkeley",
        "aff_domain": "pku.edu.cn;wustl.edu;berkeley.edu;berkeley.edu",
        "email": "pku.edu.cn;wustl.edu;berkeley.edu;berkeley.edu",
        "github": "https://github.com/pkusjh/LASS",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Peking University;Washington University in St. Louis;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.pku.edu.cn;https://wustl.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Peking U;WashU;UC Berkeley",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";St. Louis;Berkeley",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.311",
        "title": "K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online News Comment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Online hate speech detection has become an important issue due to the growth of online content, but resources in languages other than English are extremely limited. We introduce K-MHaS, a new multi-label dataset for hate speech detection that effectively handles Korean language patterns. The dataset consists of 109k utterances from news comments and provides a multi-label classification using 1 to 4 labels, and handles subjectivity and intersectionality. We evaluate strong baselines on K-MHaS. KR-BERT with a sub-character tokenizer outperforms others, recognizing decomposed characters in each hate speech class.",
        "author": "Jean Lee; Taejun Lim; Heejun Lee; Bogeun Jo; Yangsok Kim; Heegeun Yoon; Soyeon Caren Han",
        "authorids": "/j/jean-lee/; /t/taejun-lim/; /h/heejun-lee/; /b/bogeun-jo/; /y/yangsok-kim/; /h/heegeun-yoon/; /s/soyeon-caren-han/",
        "bibtex": "@inproceedings{lee-etal-2022-k,\n    title = \"K-{MH}a{S}: A Multi-label Hate Speech Detection Dataset in {K}orean Online News Comment\",\n    author = \"Lee, Jean  and\n      Lim, Taejun  and\n      Lee, Heejun  and\n      Jo, Bogeun  and\n      Kim, Yangsok  and\n      Yoon, Heegeun  and\n      Han, Soyeon Caren\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.311/\",\n    pages = \"3530--3538\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.311.pdf",
        "site": "https://aclanthology.org/2022.coling-1.311/",
        "pdf_size": 416193,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15353785530031624759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Sydney1+The University of Western Australia2; The University of Sydney1+The University of Western Australia2; BigWave AI3; BigWave AI3; Keimyung University4; National Information Society Agency5; The University of Sydney1+The University of Western Australia2",
        "aff_domain": "sydney.edu.au;sydney.edu.au;gmail.com;gmail.com;kmu.ac.kr;nia.or.kr;sydney.edu.au",
        "email": "sydney.edu.au;sydney.edu.au;gmail.com;gmail.com;kmu.ac.kr;nia.or.kr;sydney.edu.au",
        "github": "https://github.com/adlnlp/K-MHaS",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;2;2;3;4;0+1",
        "aff_unique_norm": "The University of Sydney;The University of Western Australia;BigWave AI;Keimyung University;National Information Society Agency",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.sydney.edu.au;https://www.uwa.edu.au;;https://www.keimyung.ac.kr;https://www.nia.or.kr",
        "aff_unique_abbr": "USYD;UWA;;KMU;NIA",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;2;2;0+0",
        "aff_country_unique": "Australia;;South Korea"
    },
    {
        "id": "2022.coling-1.601",
        "title": "KC-ISA: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Sentiment analysis has always been an important research direction in natural language processing. The research can be divided into explicit sentiment analysis and implicit sentiment analysis according to whether there are sentiment words in language expression. There have been many research results in explicit sentiment analysis. However, implicit sentiment analysis is rarely studied. Compared with explicit sentiment expression, implicit sentiment expression usually omits a lot of knowledge and common sense, and context also has an important impact on implicit sentiment expression. In this paper, we use a knowledge graph to supplement implicit sentiment expression and propose a novel Implicit Sentiment Analysis model combining Knowledge enhancement and Context features (dubbed KC-ISA). The KC-ISA model can effectively integrate external knowledge and contextual features by the coattention mechanism. Finally, we conduct experiments on the SMP2019 implicit sentiment analysis dataset. Moreover, to verify the generality of the model, we also conduct experiments on two common sentiment analysis datasets. The results on three datasets show that our proposed KC-ISA model can achieve better results on text sentiment analysis.",
        "author": "Minghao Xu; Daling Wang; Shi Feng; Zhenfei Yang; Yifei Zhang",
        "authorids": "/m/minghao-xu/; /d/daling-wang/; /s/shi-feng/; /z/zhenfei-yang/; /y/yifei-zhang/",
        "bibtex": "@inproceedings{xu-etal-2022-kc,\n    title = \"{KC}-{ISA}: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features\",\n    author = \"Xu, Minghao  and\n      Wang, Daling  and\n      Feng, Shi  and\n      Yang, Zhenfei  and\n      Zhang, Yifei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.601/\",\n    pages = \"6906--6915\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.601.pdf",
        "site": "https://aclanthology.org/2022.coling-1.601/",
        "pdf_size": 587937,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11805051338255615455&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Northeastern University; Northeastern University; Northeastern University; Northeastern University; Northeastern University",
        "aff_domain": "stu.neu.edu.cn;cse.neu.edu.cn;cse.neu.edu.cn;stu.neu.edu.cn;cse.neu.edu.cn",
        "email": "stu.neu.edu.cn;cse.neu.edu.cn;cse.neu.edu.cn;stu.neu.edu.cn;cse.neu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.229",
        "title": "KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge Graph Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Learning the embeddings of knowledge graphs (KG) is vital in artificial intelligence, and can benefit various downstream applications, such as recommendation and question answering. In recent years, many research efforts have been proposed for knowledge graph embedding (KGE). However, most previous KGE methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function. To address this problem, we propose a simple yet efficient contrastive learning framework for tensor decomposition based (TDB) KGE, which can shorten the semantic distance of the related entities and entity-relation couples in different triples and thus improve the performance of KGE. We evaluate our proposed method on three standard KGE datasets: WN18RR, FB15k-237 and YAGO3-10. Our method can yield some new state-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset.",
        "author": "Zhiping Luo; Wentao Xu; Weiqing Liu; Jiang Bian; Jian Yin; Tie-Yan Liu",
        "authorids": "/z/zhiping-luo/; /w/wentao-xu/; /w/weiqing-liu/; /j/jiang-bian/; /j/jian-yin/; /t/tie-yan-liu/",
        "bibtex": "@inproceedings{luo-etal-2022-kge,\n    title = \"{KGE}-{CL}: Contrastive Learning of Tensor Decomposition Based Knowledge Graph Embeddings\",\n    author = \"Luo, Zhiping  and\n      Xu, Wentao  and\n      Liu, Weiqing  and\n      Bian, Jiang  and\n      Yin, Jian  and\n      Liu, Tie-Yan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.229/\",\n    pages = \"2598--2607\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.229.pdf",
        "site": "https://aclanthology.org/2022.coling-1.229/",
        "pdf_size": 464478,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1745742787507112005&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China+Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China+Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China+Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Microsoft Research Asia, Beijing, China",
        "aff_domain": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;microsoft.com;microsoft.com;mail.sysu.edu.cn;microsoft.com",
        "email": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;microsoft.com;microsoft.com;mail.sysu.edu.cn;microsoft.com",
        "github": "https://github.com/Wentao-Xu/KGE-CL",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;2;2;0+1;2",
        "aff_unique_norm": "Sun Yat-sen University;Guangdong Key Laboratory of Big Data Analysis and Processing;Microsoft Research Asia",
        "aff_unique_dep": "School of Computer Science and Engineering;;Research",
        "aff_unique_url": "http://www.sysu.edu.cn;;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "SYSU;;MSRA",
        "aff_campus_unique_index": "0+0;0+0;1;1;2+0;1",
        "aff_campus_unique": "Guangzhou;Beijing;Zhuhai",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.518",
        "title": "KHANQ: A Dataset for Generating Deep Questions in Education",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Designing in-depth educational questions is a time-consuming and cognitively demanding task. Therefore, it is intriguing to study how to build Question Generation (QG) models to automate the question creation process. However, existing QG datasets are not suitable for educational question generation because the questions are not real questions asked by humans during learning and can be solved by simply searching for information. To bridge this gap, we present KHANQ, a challenging dataset for educational question generation, containing 1,034 high-quality learner-generated questions seeking an in-depth understanding of the taught online courses in Khan Academy. Each data sample is carefully paraphrased and annotated as a triple of 1) Context: an independent paragraph on which the question is based; 2) Prompt: a text prompt for the question (e.g., the learner\u2019s background knowledge); 3) Question: a deep question based on Context and coherent with Prompt. By conducting a human evaluation on the aspects of appropriateness, coverage, coherence, and complexity, we show that state-of-the-art QG models which perform well on shallow question generation datasets have difficulty in generating useful educational questions. This makes KHANQ a challenging testbed for educational question generation.",
        "author": "Huanli Gong; Liangming Pan; Hengchang Hu",
        "authorids": "/h/huanli-gong/; /l/liangming-pan/; /h/hengchang-hu/",
        "bibtex": "@inproceedings{gong-etal-2022-khanq,\n    title = \"{KHANQ}: A Dataset for Generating Deep Questions in Education\",\n    author = \"Gong, Huanli  and\n      Pan, Liangming  and\n      Hu, Hengchang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.518/\",\n    pages = \"5925--5938\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.518.pdf",
        "site": "https://aclanthology.org/2022.coling-1.518/",
        "pdf_size": 479739,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4636065102381940183&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "The Ohio State University, Columbus, OH, USA; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore",
        "aff_domain": "osu.edu;u.nus.edu;u.nus.edu",
        "email": "osu.edu;u.nus.edu;u.nus.edu",
        "github": "",
        "project": "https://www.khanacademy.org/",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "The Ohio State University;National University of Singapore",
        "aff_unique_dep": ";School of Computing",
        "aff_unique_url": "https://www.osu.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "OSU;NUS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Columbus;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "2022.coling-1.425",
        "title": "KNOT: Knowledge Distillation Using Optimal Transport for Solving NLP Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a new approach, Knowledge Distillation using Optimal Transport (KNOT), to distill the natural language semantic knowledge from multiple teacher networks to a student network. KNOT aims to train a (global) student model by learning to minimize the optimal transport cost of its assigned probability distribution over the labels to the weighted sum of probabilities predicted by the (local) teacher models, under the constraints that the student model does not have access to teacher models\u2019 parameters or training data. To evaluate the quality of knowledge transfer, we introduce a new metric, Semantic Distance (SD), that measures semantic closeness between the predicted and ground truth label distributions. The proposed method shows improvements in the global model\u2019s SD performance over the baseline across three NLP tasks while performing on par with Entropy-based distillation on standard accuracy and F1 metrics. The implementation pertaining to this work is publicly available at https://github.com/declare-lab/KNOT.",
        "author": "Rishabh Bhardwaj; Tushar Vaidya; Soujanya Poria",
        "authorids": "/r/rishabh-bhardwaj/; /t/tushar-vaidya/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{bhardwaj-etal-2022-knot,\n    title = \"{KNOT}: Knowledge Distillation Using Optimal Transport for Solving {NLP} Tasks\",\n    author = \"Bhardwaj, Rishabh  and\n      Vaidya, Tushar  and\n      Poria, Soujanya\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.425/\",\n    pages = \"4801--4820\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.425.pdf",
        "site": "https://aclanthology.org/2022.coling-1.425/",
        "pdf_size": 7096401,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18279474369059850663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Singapore University of Technology and Design, Singapore; Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore",
        "aff_domain": "mymail.sutd.edu.sg;ntu.edu.sg;sutd.edu.sg",
        "email": "mymail.sutd.edu.sg;ntu.edu.sg;sutd.edu.sg",
        "github": "https://github.com/declare-lab/KNOT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Singapore University of Technology and Design;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.ntu.edu.sg",
        "aff_unique_abbr": "SUTD;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.coling-1.165",
        "title": "Key Mention Pairs Guided Document-Level Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Document-level Relation Extraction (DocRE) aims at extracting relations between entities in a given document. Since different mention pairs may express different relations or even no relation, it is crucial to identify key mention pairs responsible for the entity-level relation labels. However, most recent studies treat different mentions equally while predicting the relations between entities, leading to sub-optimal performance. To this end, we propose a novel DocRE model called Key Mention pairs Guided Relation Extractor (KMGRE) to directly model mention-level relations, containing two modules: a mention-level relation extractor and a key instance classifier. These two modules could be iteratively optimized with an EM-based algorithm to enhance each other. We also propose a new method to solve the multi-label problem in optimizing the mention-level relation extractor. Experimental results on two public DocRE datasets demonstrate that the proposed model is effective and outperforms previous state-of-the-art models.",
        "author": "Feng Jiang; Jianwei Niu; Shasha Mo; Shengda Fan",
        "authorids": "/f/feng-jiang/; /j/jianwei-niu/; /s/shasha-mo/; /s/shengda-fan/",
        "bibtex": "@inproceedings{jiang-etal-2022-key,\n    title = \"Key Mention Pairs Guided Document-Level Relation Extraction\",\n    author = \"Jiang, Feng  and\n      Niu, Jianwei  and\n      Mo, Shasha  and\n      Fan, Shengda\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.165/\",\n    pages = \"1904--1914\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.165.pdf",
        "site": "https://aclanthology.org/2022.coling-1.165/",
        "pdf_size": 484603,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7913326322593653882&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing 100191, China; School of Cyber Science and Technology, Beihang University, Beijing 100191, China; School of Cyber Science and Technology, Beihang University, Beijing 100191, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "Beihang",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.624",
        "title": "Keyphrase Prediction from Video Transcripts: New Dataset and Directions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Keyphrase Prediction (KP) is an established NLP task, aiming to yield representative phrases to summarize the main content of a given document. Despite major progress in recent years, existing works on KP have mainly focused on formal texts such as scientific papers or weblogs. The challenges of KP in informal-text domains are not yet fully studied. To this end, this work studies new challenges of KP in transcripts of videos, an understudied domain for KP that involves informal texts and non-cohesive presentation styles. A bottleneck for KP research in this domain involves the lack of high-quality and large-scale annotated data that hinders the development of advanced KP models. To address this issue, we introduce a large-scale manually-annotated KP dataset in the domain of live-stream video transcripts obtained by automatic speech recognition tools. Concretely, transcripts of 500+ hours of videos streamed on the behance.net platform are manually labeled with important keyphrases. Our analysis of the dataset reveals the challenging nature of KP in transcripts. Moreover, for the first time in KP, we demonstrate the idea of improving KP for long documents (i.e., transcripts) by feeding models with paragraph-level keyphrases, i.e., hierarchical extraction. To foster future research, we will publicly release the dataset and code.",
        "author": "Amir Pouran Ben Veyseh; Quan Hung Tran; Seunghyun Yoon; Varun Manjunatha; Hanieh Deilamsalehy; Rajiv Jain; Trung Bui; Walter W. Chang; Franck Dernoncourt; Thien Huu Nguyen",
        "authorids": "/a/amir-pouran-ben-veyseh/; /q/quan-hung-tran/; /s/seunghyun-yoon/; /v/varun-manjunatha/; /h/hanieh-deilamsalehy/; /r/rajiv-jain/; /t/trung-bui/; /w/walter-w-chang/; /f/franck-dernoncourt/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{veyseh-etal-2022-keyphrase,\n    title = \"Keyphrase Prediction from Video Transcripts: New Dataset and Directions\",\n    author = \"Veyseh, Amir Pouran Ben  and\n      Tran, Quan Hung  and\n      Yoon, Seunghyun  and\n      Manjunatha, Varun  and\n      Deilamsalehy, Hanieh  and\n      Jain, Rajiv  and\n      Bui, Trung  and\n      Chang, Walter W.  and\n      Dernoncourt, Franck  and\n      Nguyen, Thien Huu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.624/\",\n    pages = \"7147--7155\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.624.pdf",
        "site": "https://aclanthology.org/2022.coling-1.624/",
        "pdf_size": 228978,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:a-bfSb0H6XcJ:scholar.google.com/&scioq=Keyphrase+Prediction+from+Video+Transcripts:+New+Dataset+and+Directions&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of Oregon, OR, USA+Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Department of Computer Science, University of Oregon, OR, USA+Adobe Research, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu; ; ; ; ; ; ;adobe.com; ",
        "email": "cs.uoregon.edu;cs.uoregon.edu; ; ; ; ; ; ;adobe.com; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;1;1;1;1;1;1;1;0+1",
        "aff_unique_norm": "University of Oregon;Adobe Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uoregon.edu;https://research.adobe.com",
        "aff_unique_abbr": "UO;Adobe",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.169",
        "title": "KiPT: Knowledge-injected Prompt Tuning for Event Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event detection aims to detect events from the text by identifying and classifying event triggers (the most representative words). Most of the existing works rely heavily on complex downstream networks and require sufficient training data. Thus, those models may be structurally redundant and perform poorly when data is scarce. Prompt-based models are easy to build and are promising for few-shot tasks. However, current prompt-based methods may suffer from low precision because they have not introduced event-related semantic knowledge (e.g., part of speech, semantic correlation, etc.). To address these problems, this paper proposes a Knowledge-injected Prompt Tuning (KiPT) model. Specifically, the event detection task is formulated into a condition generation task. Then, knowledge-injected prompts are constructed using external knowledge bases, and a prompt tuning strategy is leveraged to optimize the prompts. Extensive experiments indicate that KiPT outperforms strong baselines, especially in few-shot scenarios.",
        "author": "Haochen Li; Tong Mo; Hongcheng Fan; Jingkun Wang; Jiaxi Wang; Fuhao Zhang; Weiping Li",
        "authorids": "/h/haochen-li/; /t/tong-mo/; /h/hongcheng-fan/; /j/jingkun-wang/; /j/jiaxi-wang/; /f/fuhao-zhang/; /w/weiping-li/",
        "bibtex": "@inproceedings{li-etal-2022-kipt,\n    title = \"{K}i{PT}: Knowledge-injected Prompt Tuning for Event Detection\",\n    author = \"Li, Haochen  and\n      Mo, Tong  and\n      Fan, Hongcheng  and\n      Wang, Jingkun  and\n      Wang, Jiaxi  and\n      Zhang, Fuhao  and\n      Li, Weiping\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.169/\",\n    pages = \"1943--1952\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.169.pdf",
        "site": "https://aclanthology.org/2022.coling-1.169/",
        "pdf_size": 955246,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5465923985299190277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Peking University; Peking University; Peking University; Peking University; Peking University; Chinese Academy of Surveying and mapping; Peking University",
        "aff_domain": "pku.edu.cn;ss.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;ac.cn;ss.pku.edu.cn",
        "email": "pku.edu.cn;ss.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;ac.cn;ss.pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Peking University;Chinese Academy of Surveying and Mapping",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.casgm.ac.cn",
        "aff_unique_abbr": "Peking U;CASGM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.435",
        "title": "Knowledge Distillation with Reptile Meta-Learning for Pretrained Language Model Compression",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The billions, and sometimes even trillions, of parameters involved in pre-trained language models significantly hamper their deployment in resource-constrained devices and real-time applications. Knowledge distillation (KD) can transfer knowledge from the original model (i.e., teacher) into a compact model (i.e., student) to achieve model compression. However, previous KD methods have usually frozen the teacher and applied its immutable output feature maps as soft labels to guide the student\u2019s training. Moreover, the goal of the teacher is to achieve the best performance on downstream tasks rather than knowledge transfer. Such a fixed architecture may limit the teacher\u2019s teaching and student\u2019s learning abilities. Herein, a knowledge distillation method with reptile meta-learning is proposed to facilitate the transfer of knowledge from the teacher to the student. The teacher can continuously meta-learn the student\u2019s learning objective to adjust its parameters for maximizing the student\u2019s performance throughout the distillation process. In this way, the teacher learns to teach, produces more suitable soft labels, and transfers more appropriate knowledge to the student, resulting in improved performance. Unlike previous KD using meta-learning, the proposed method only needs to calculate the first-order derivatives to update the teacher, leading to lower computational cost but better convergence. Extensive experiments on the GLUE benchmark show the competitive performance achieved by the proposed method. For reproducibility, the code for this paper is available at: https://github.com/maxinge8698/ReptileDistil.",
        "author": "Xinge Ma; Jin Wang; Liang-Chih Yu; Xuejie Zhang",
        "authorids": "/x/xinge-ma/; /j/jin-wang/; /l/liang-chih-yu/; /x/xuejie-zhang/",
        "bibtex": "@inproceedings{ma-etal-2022-knowledge,\n    title = \"Knowledge Distillation with Reptile Meta-Learning for Pretrained Language Model Compression\",\n    author = \"Ma, Xinge  and\n      Wang, Jin  and\n      Yu, Liang-Chih  and\n      Zhang, Xuejie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.435/\",\n    pages = \"4907--4917\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.435.pdf",
        "site": "https://aclanthology.org/2022.coling-1.435/",
        "pdf_size": 653828,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10223120213146975892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China; School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China; Department of Information Management, Yuan Ze University, Taiwan; School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China",
        "aff_domain": "ynu.edu.cn;saturn.yzu.edu.tw; ; ",
        "email": "ynu.edu.cn;saturn.yzu.edu.tw; ; ",
        "github": "https://github.com/maxinge8698/ReptileDistil",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Yunnan University;Yuan Ze University",
        "aff_unique_dep": "School of Information Science and Engineering;Department of Information Management",
        "aff_unique_url": "http://www.ynu.edu.cn;https://www.yzu.edu.tw",
        "aff_unique_abbr": "YNU;YZU",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Yunnan;Taiwan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.352",
        "title": "Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge Graph Completion (KGC) has been recently extended to multiple knowledge graph (KG) structures, initiating new research directions, e.g. static KGC, temporal KGC and few-shot KGC. Previous works often design KGC models closely coupled with specific graph structures, which inevitably results in two drawbacks: 1) structure-specific KGC models are mutually incompatible; 2) existing KGC methods are not adaptable to emerging KGs. In this paper, we propose KG-S2S, a Seq2Seq generative framework that could tackle different verbalizable graph structures by unifying the representation of KG facts into \u201cflat\u201d text, regardless of their original form. To remedy the KG structure information loss from the \u201cflat\u201d text, we further improve the input representations of entities and relations, and the inference algorithm in KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many competitive baselines, setting new state-of-the-art performance. Finally, we analyze KG-S2S\u2019s ability on the different relations and the Non-entity Generations.",
        "author": "Chen Chen; Yufei Wang; Bing Li; Kwok-Yan Lam",
        "authorids": "/c/chen-chen/; /y/yufei-wang/; /b/bing-li/; /k/kwok-yan-lam/",
        "bibtex": "@inproceedings{chen-etal-2022-knowledge,\n    title = \"Knowledge Is Flat: A {S}eq2{S}eq Generative Framework for Various Knowledge Graph Completion\",\n    author = \"Chen, Chen  and\n      Wang, Yufei  and\n      Li, Bing  and\n      Lam, Kwok-Yan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.352/\",\n    pages = \"4005--4017\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.352.pdf",
        "site": "https://aclanthology.org/2022.coling-1.352/",
        "pdf_size": 970298,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9936409467335742873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Nanyang Technological University, Singapore; Macquarie University, Sydney, Australia; Centre for Frontier AI Research (CFAR), A*STAR, Singapore; Nanyang Technological University, Singapore",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;students.mq.edu.au;ihpc.a-star.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;students.mq.edu.au;ihpc.a-star.edu.sg",
        "github": "https://github.com/chenchens190009/KG-S2S",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Nanyang Technological University;Macquarie University;A*STAR",
        "aff_unique_dep": ";;Centre for Frontier AI Research (CFAR)",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.mq.edu.au;https://www.a-star.edu.sg",
        "aff_unique_abbr": "NTU;MQ;A*STAR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Sydney",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Singapore;Australia"
    },
    {
        "id": "2022.coling-1.325",
        "title": "KoBEST: Korean Balanced Evaluation of Significant Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A well-formulated benchmark plays a critical role in spurring advancements in the natural language processing (NLP) field, as it allows objective and precise evaluation of diverse models. As modern language models (LMs) have become more elaborate and sophisticated, more difficult benchmarks that require linguistic knowledge and reasoning have been proposed. However, most of these benchmarks only support English, and great effort is necessary to construct benchmarks for other low resource languages. To this end, we propose a new benchmark named Korean balanced evaluation of significant tasks (KoBEST), which consists of five Korean-language downstream tasks. Professional Korean linguists designed the tasks that require advanced Korean linguistic knowledge. Moreover, our data is purely annotated by humans and thoroughly reviewed to guarantee high data quality. We also provide baseline models and human performance results. Our dataset is available on the Huggingface.",
        "author": "Myeongjun Jang; Dohyung Kim; Deuk Sin Kwon; Eric Davis",
        "authorids": "/m/myeongjun-jang/; /d/dohyung-kim/; /d/deuk-sin-kwon/; /e/eric-davis/",
        "bibtex": "@inproceedings{jang-etal-2022-kobest,\n    title = \"{K}o{BEST}: {K}orean Balanced Evaluation of Significant Tasks\",\n    author = \"Jang, Myeongjun  and\n      Kim, Dohyung  and\n      Kwon, Deuk Sin  and\n      Davis, Eric\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.325/\",\n    pages = \"3697--3708\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.325.pdf",
        "site": "https://aclanthology.org/2022.coling-1.325/",
        "pdf_size": 2066388,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16821219712984612962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, University of Oxford, UK; Language Super Intelligence Labs, SK Telecom, South Korea; Language Super Intelligence Labs, SK Telecom, South Korea; Language Super Intelligence Labs, SK Telecom, South Korea",
        "aff_domain": "cs.ox.ac.uk;sktair.com;sktair.com;sktair.com",
        "email": "cs.ox.ac.uk;sktair.com;sktair.com;sktair.com",
        "github": "",
        "project": "https://huggingface.co/datasets/skt/kobest_v1",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Oxford;SK Telecom",
        "aff_unique_dep": "Department of Computer Science;Language Super Intelligence Labs",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.sktelecom.com",
        "aff_unique_abbr": "Oxford;SKT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United Kingdom;South Korea"
    },
    {
        "id": "2022.coling-1.308",
        "title": "KoCHET: A Korean Cultural Heritage Corpus for Entity-related Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As digitized traditional cultural heritage documents have rapidly increased, resulting in an increased need for preservation and management, practical recognition of entities and typification of their classes has become essential. To achieve this, we propose KoCHET - a Korean cultural heritage corpus for the typical entity-related tasks, i.e., named entity recognition (NER), relation extraction (RE), and entity typing (ET). Advised by cultural heritage experts based on the data construction guidelines of government-affiliated organizations, KoCHET consists of respectively 112,362, 38,765, 113,198 examples for NER, RE, and ET tasks, covering all entity types related to Korean cultural heritage. Moreover, unlike the existing public corpora, modified redistribution can be allowed both domestic and foreign researchers. Our experimental results make the practical usability of KoCHET more valuable in terms of cultural heritage. We also provide practical insights of KoCHET in terms of statistical and linguistic analysis. Our corpus is freely available at https://github.com/Gyeongmin47/KoCHET.",
        "author": "Gyeongmin Kim; Jinsung Kim; Junyoung Son; Heuiseok Lim",
        "authorids": "/g/gyeongmin-kim/; /j/jinsung-kim/; /j/junyoung-son/; /h/heui-seok-lim/",
        "bibtex": "@inproceedings{kim-etal-2022-kochet,\n    title = \"{K}o{CHET}: A {K}orean Cultural Heritage Corpus for Entity-related Tasks\",\n    author = \"Kim, Gyeongmin  and\n      Kim, Jinsung  and\n      Son, Junyoung  and\n      Lim, Heuiseok\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.308/\",\n    pages = \"3496--3505\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.308.pdf",
        "site": "https://aclanthology.org/2022.coling-1.308/",
        "pdf_size": 2126401,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10910784704426008220&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Korea University; Korea University; Korea University; Korea University",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "github": "https://github.com/Gyeongmin47/KoCHET",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.610",
        "title": "LEGO-ABSA: A Prompt-based Task Assemblable Unified Generative Framework for Multi-task Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) has received increasing attention recently. ABSA can be divided into multiple tasks according to the different extracted elements. Existing generative methods usually treat the output as a whole string rather than the combination of different elements and only focus on a single task at once. This paper proposes a unified generative multi-task framework that can solve multiple ABSA tasks by controlling the type of task prompts consisting of multiple element prompts. Further, the proposed approach can train on simple tasks and transfer to difficult tasks by assembling task prompts, like assembling Lego bricks. We conduct experiments on six ABSA tasks across multiple benchmarks. Our proposed multi-task approach achieves new state-of-the-art results in almost all tasks and competitive results in task transfer scenarios.",
        "author": "Tianhao Gao; Jun Fang; Hanyu Liu; Zhiyuan Liu; Chao Liu; Pengzhang Liu; Yongjun Bao; Weipeng Yan",
        "authorids": "/t/tianhao-gao/; /j/jun-fang/; /h/hanyu-liu/; /z/zhiyuan-liu/; /c/chao-liu/; /p/pengzhang-liu/; /y/yongjun-bao/; /w/weipeng-yan/",
        "bibtex": "@inproceedings{gao-etal-2022-lego,\n    title = \"{LEGO}-{ABSA}: A Prompt-based Task Assemblable Unified Generative Framework for Multi-task Aspect-based Sentiment Analysis\",\n    author = \"Gao, Tianhao  and\n      Fang, Jun  and\n      Liu, Hanyu  and\n      Liu, Zhiyuan  and\n      Liu, Chao  and\n      Liu, Pengzhang  and\n      Bao, Yongjun  and\n      Yan, Weipeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.610/\",\n    pages = \"7002--7012\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.610.pdf",
        "site": "https://aclanthology.org/2022.coling-1.610/",
        "pdf_size": 1124195,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12843493074064859365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Software and Microelectronics, Peking University+JD, Retail, Beijing, China; JD, Retail, Beijing, China; JD, Retail, Beijing, China; JD, Retail, Beijing, China; JD, Retail, Beijing, China; JD, Retail, Beijing, China; JD, Retail, Beijing, China; JD, Retail, Beijing, China",
        "aff_domain": "pku.edu.cn;jd.com;jd.com;jd.com;jd.com;jd.com;jd.com;jd.com",
        "email": "pku.edu.cn;jd.com;jd.com;jd.com;jd.com;jd.com;jd.com;jd.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;1;1;1",
        "aff_unique_norm": "Peking University;JD.com",
        "aff_unique_dep": "School of Software and Microelectronics;Retail",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.jd.com",
        "aff_unique_abbr": "PKU;JD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.572",
        "title": "LFKQG: A Controlled Generation Framework with Local Fine-tuning for Question Generation over Knowledge Bases",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question generation over knowledge bases (KBQG) aims at generating natural questions about a subgraph, which can be answered by a given answer entity. Existing KBQG models still face two main challenges: (1) Most models often focus on the most relevant part of the answer entity, while neglecting the rest of the subgraph. (2) There are a large number of out-of-vocabulary (OOV) predicates in real-world scenarios, which are hard to adapt for most KBQG models. To address these challenges, we propose LFKQG, a controlled generation framework for Question Generation over Knowledge Bases. (1) LFKQG employs a simple controlled generation method to generate the questions containing the critical entities in the subgraph, ensuring the question is relevant to the whole subgraph. (2) We propose an optimization strategy called local fine-tuning, which can make good use of the rich information hidden in the pre-trained model to improve the ability of the model to adapt the OOV predicates. Extensive experiments show that our method outperforms existing methods significantly on three widely-used benchmark datasets SimpleQuestion, PathQuestions, and WebQuestions.",
        "author": "Zichu Fei; Xin Zhou; Tao Gui; Qi Zhang; Xuanjing Huang",
        "authorids": "/z/zichu-fei/; /x/xin-zhou/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{fei-etal-2022-lfkqg,\n    title = \"{LFKQG}: A Controlled Generation Framework with Local Fine-tuning for Question Generation over Knowledge Bases\",\n    author = \"Fei, Zichu  and\n      Zhou, Xin  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.572/\",\n    pages = \"6575--6585\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.572.pdf",
        "site": "https://aclanthology.org/2022.coling-1.572/",
        "pdf_size": 855469,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14661154604711547137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2; School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2; Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China3; School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2; School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2 + Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China3",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;0+1;0+1+0",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Intelligent Information Processing",
        "aff_unique_dep": "School of Computer Science;Intelligent Information Processing",
        "aff_unique_url": "https://www.fudan.edu.cn;",
        "aff_unique_abbr": "Fudan;",
        "aff_campus_unique_index": ";;1;;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.91",
        "title": "LIME: Weakly-Supervised Text Classification without Seeds",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In weakly-supervised text classification, only label names act as sources of supervision. Predominant approaches to weakly-supervised text classification utilize a two-phase framework, where test samples are first assigned pseudo-labels and are then used to train a neural text classifier. In most previous work, the pseudo-labeling step is dependent on obtaining seed words that best capture the relevance of each class label. We present LIME, a framework for weakly-supervised text classification that entirely replaces the brittle seed-word generation process with entailment-based pseudo-classification. We find that combining weakly-supervised classification and textual entailment mitigates shortcomings of both, resulting in a more streamlined and effective classification pipeline. With just an off-the-shelf textual entailment model, LIME outperforms recent baselines in weakly-supervised text classification and achieves state-of-the-art in 4 benchmarks.",
        "author": "Seongmin Park; Jihwa Lee",
        "authorids": "/s/seongmin-park/; /j/jihwa-lee/",
        "bibtex": "@inproceedings{park-lee-2022-lime,\n    title = \"{LIME}: Weakly-Supervised Text Classification without Seeds\",\n    author = \"Park, Seongmin  and\n      Lee, Jihwa\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.91/\",\n    pages = \"1083--1088\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.91.pdf",
        "site": "https://aclanthology.org/2022.coling-1.91/",
        "pdf_size": 385437,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8270308870276516369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ActionPower, Seoul, Republic of Korea; ActionPower, Seoul, Republic of Korea",
        "aff_domain": "actionpower.kr;actionpower.kr",
        "email": "actionpower.kr;actionpower.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ActionPower",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.18",
        "title": "LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and Example Extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST out-performs a strong baseline of Machine Translation with Slot Alignment by +4.14 points absolute on ST F1 Score across 6 languages, while matching performance on IC. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent IC+ST and show significant improvements over a baseline which uses Back-Translation, Paraphrasing and Slot Catalog Resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.",
        "author": "Andy Rosenbaum; Saleh Soltan; Wael Hamza; Yannick Versley; Markus Boese",
        "authorids": "/a/andy-rosenbaum/; /s/saleh-soltan/; /w/wael-hamza/; /y/yannick-versley/; /m/markus-boese/",
        "bibtex": "@inproceedings{rosenbaum-etal-2022-linguist,\n    title = \"{LINGUIST}: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging\",\n    author = \"Rosenbaum, Andy  and\n      Soltan, Saleh  and\n      Hamza, Wael  and\n      Versley, Yannick  and\n      Boese, Markus\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.18/\",\n    pages = \"218--241\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.18.pdf",
        "site": "https://aclanthology.org/2022.coling-1.18/",
        "pdf_size": 678665,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10109557226946473822&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon, Cambridge, USA; Amazon, New York, USA; Amazon, Dallas, USA; Amazon, Aachen, Germany; Amazon, Aachen, Germany",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.de;amazon.de",
        "email": "amazon.com;amazon.com;amazon.com;amazon.de;amazon.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Amazon;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amazon.com;https://www.amazon.com",
        "aff_unique_abbr": "Amazon;Amazon",
        "aff_campus_unique_index": "0;1;2;3;3",
        "aff_campus_unique": "Cambridge;New York;Dallas;Aachen",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "2022.coling-1.505",
        "title": "LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Understanding spatial and visual information is essential for a navigation agent who follows natural language instructions. The current Transformer-based VLN agents entangle the orientation and vision information, which limits the gain from the learning of each information source. In this paper, we design a neural agent with explicit Orientation and Vision modules. Those modules learn to ground spatial information and landmark mentions in the instructions to the visual environment more effectively. To strengthen the spatial reasoning and visual perception of the agent, we design specific pre-training tasks to feed and better utilize the corresponding modules in our final navigation model. We evaluate our approach on both Room2room (R2R) and Room4room (R4R) datasets and achieve the state of the art results on both benchmarks.",
        "author": "Yue Zhang; Parisa Kordjamshidi",
        "authorids": "/y/yue-zhang/; /p/parisa-kordjamshidi/",
        "bibtex": "@inproceedings{zhang-kordjamshidi-2022-lovis,\n    title = \"{LOV}i{S}: Learning Orientation and Visual Signals for Vision and Language Navigation\",\n    author = \"Zhang, Yue  and\n      Kordjamshidi, Parisa\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.505/\",\n    pages = \"5745--5754\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.505.pdf",
        "site": "https://aclanthology.org/2022.coling-1.505/",
        "pdf_size": 1177686,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1985524060708571003&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Michigan State University; Michigan State University",
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.193",
        "title": "Label Smoothing for Text Mining",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current text mining models are trained with 0-1 hard label that indicates whether an instance belongs to a class, ignoring rich information of the relevance degree. Soft label, which involved each label of varying degrees than the hard label, is considered more suitable for describing instances. The process of generating soft labels from hard labels is defined as label smoothing (LS). Classical LS methods focus on universal data mining tasks so that they ignore the valuable text features in text mining tasks. This paper presents a novel keyword-based LS method to automatically generate soft labels from hard labels via exploiting the relevance between labels and text instances. Generated soft labels are then incorporated into existing models as auxiliary targets during the training stage, capable of improving models without adding any extra parameters. Results of extensive experiments on text classification and large-scale text retrieval datasets demonstrate that soft labels generated by our method contain rich knowledge of text features, improving the performance of corresponding models under both balanced and unbalanced settings.",
        "author": "Peiyang Liu; Xiangyu Xi; Wei Ye; Shikun Zhang",
        "authorids": "/p/peiyang-liu/; /x/xiangyu-xi/; /w/wei-ye/; /s/shikun-zhang/",
        "bibtex": "@inproceedings{liu-etal-2022-label,\n    title = \"Label Smoothing for Text Mining\",\n    author = \"Liu, Peiyang  and\n      Xi, Xiangyu  and\n      Ye, Wei  and\n      Zhang, Shikun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.193/\",\n    pages = \"2210--2219\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.193.pdf",
        "site": "https://aclanthology.org/2022.coling-1.193/",
        "pdf_size": 869715,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7951065598641154408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "National Engineering Research Center for Software Engineering, Peking University, Beijing, China + PX Securities, Shenzhen, China; Meituan, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China",
        "aff_domain": "pku.edu.cn;meituan.com;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;meituan.com;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;0;0",
        "aff_unique_norm": "Peking University;PX Securities;Meituan",
        "aff_unique_dep": "National Engineering Research Center for Software Engineering;;",
        "aff_unique_url": "http://www.pku.edu.cn;;https://www.meituan.com",
        "aff_unique_abbr": "PKU;;Meituan",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.2",
        "title": "Language Acquisition through Intention Reading and Pattern Finding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "One of AI\u2019s grand challenges consists in the development of autonomous agents with communication systems offering the robustness, flexibility and adaptivity found in human languages. While the processes through which children acquire language are by now relatively well understood, a faithful computational operationalisation of the underlying mechanisms is still lacking. Two main cognitive processes are involved in child language acquisition. First, children need to reconstruct the intended meaning of observed utterances, a process called intention reading. Then, they can gradually abstract away from concrete utterances in a process called pattern finding and acquire productive schemata that generalise over form and meaning. In this paper, we introduce a mechanistic model of the intention reading process and its integration with pattern finding capacities. Concretely, we present an agent-based simulation in which an agent learns a grammar that enables them to ask and answer questions about a scene. This involves the reconstruction of queries that correspond to observed questions based on the answer and scene alone, and the generalization of linguistic schemata based on these reconstructed question-query pairs. The result is a productive grammar which can be used to map between natural language questions and queries without ever having observed the queries.",
        "author": "Jens Nevens; Jonas Doumen; Paul Van Eecke; Katrien Beuls",
        "authorids": "/j/jens-nevens/; /j/jonas-doumen/; /p/paul-van-eecke/; /k/katrien-beuls/",
        "bibtex": "@inproceedings{nevens-etal-2022-language,\n    title = \"Language Acquisition through Intention Reading and Pattern Finding\",\n    author = \"Nevens, Jens  and\n      Doumen, Jonas  and\n      Van Eecke, Paul  and\n      Beuls, Katrien\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.2/\",\n    pages = \"15--25\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.2.pdf",
        "site": "https://aclanthology.org/2022.coling-1.2/",
        "pdf_size": 516472,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5454092693387303140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Artificial Intelligence Laboratory, Vrije Universiteit Brussel; Itec, imec research group at KU Leuven + KU Leuven, Faculty of Arts; Artificial Intelligence Laboratory, Vrije Universiteit Brussel + Itec, imec research group at KU Leuven + KU Leuven, Faculty of Arts; Facult\u00e9 d\u2019informatique, Universit\u00e9 de Namur",
        "aff_domain": "ai.vub.ac.be;ai.vub.ac.be;kuleuven.be;unamur.be",
        "email": "ai.vub.ac.be;ai.vub.ac.be;kuleuven.be;unamur.be",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+1;0+1+1;2",
        "aff_unique_norm": "Vrije Universiteit Brussel;KU Leuven;Universit\u00e9 de Namur",
        "aff_unique_dep": "Artificial Intelligence Laboratory;imec research group;Facult\u00e9 d\u2019informatique",
        "aff_unique_url": "https://www.vub.be;https://www.kuleuven.be;https://www.unamur.be",
        "aff_unique_abbr": "VUB;KU Leuven;UNamur",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0+0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2022.coling-1.447",
        "title": "Language Branch Gated Multilingual Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge transfer across languages is crucial for multilingual neural machine translation. In this paper, we propose language branch (LB) gated multilingual neural machine translation that encourages knowledge transfer within the same language branch with a LB-gated module that is integrated into both the encoder and decoder. The LB-gated module distinguishes LB-specific parameters from global parameters shared by all languages and routes languages from the same LB to the corresponding LB-specific network. Comprehensive experiments on the OPUS-100 dataset show that the proposed approach substantially improves translation quality on both middle- and low-resource languages over previous methods. Further analysis demonstrates its ability in learning similarities between language branches.",
        "author": "Haoran Sun; Deyi Xiong",
        "authorids": "/h/haoran-sun/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{sun-xiong-2022-language,\n    title = \"Language Branch Gated Multilingual Neural Machine Translation\",\n    author = \"Sun, Haoran  and\n      Xiong, Deyi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.447/\",\n    pages = \"5046--5053\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.447.pdf",
        "site": "https://aclanthology.org/2022.coling-1.447/",
        "pdf_size": 368421,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13590583207356570010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "gmail.com;tju.edu.cn",
        "email": "gmail.com;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.470",
        "title": "Language-Independent Approach for Morphological Disambiguation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a language-independent approach for morphological disambiguation which has been regarded as an extension of POS tagging, jointly predicting complex morphological tags. In the proposed approach, all words, roots, POS and morpheme tags are embedded into vectors, and contexts representations from surface word and morphological contexts are calculated. Then the inner products between analyses and the context\u2019s representations are computed to perform the disambiguation. The underlying hypothesis is that the correct morphological analysis should be closer to the context in a vector space. Experimental results show that the proposed approach outperforms the existing models on seven different language datasets. Concretely, compared with the baselines of MarMot and a sophisticated neural model (Seq2Seq), the proposed approach achieves around 6% improvement in average accuracy for all languages while running about 6 and 33 times faster than MarMot and Seq2Seq, respectively.",
        "author": "Alymzhan Toleu; Gulmira Tolegen; Rustam Mussabayev",
        "authorids": "/a/alymzhan-toleu/; /g/gulmira-tolegen/; /r/rustam-mussabayev/",
        "bibtex": "@inproceedings{toleu-etal-2022-language,\n    title = \"Language-Independent Approach for Morphological Disambiguation\",\n    author = \"Toleu, Alymzhan  and\n      Tolegen, Gulmira  and\n      Mussabayev, Rustam\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.470/\",\n    pages = \"5288--5297\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.470.pdf",
        "site": "https://aclanthology.org/2022.coling-1.470/",
        "pdf_size": 370837,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11397283426606521815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information and Computational Technologies, 050010, 28 Shevchenko str., Almaty, Kazakhstan; Institute of Information and Computational Technologies, 050010, 28 Shevchenko str., Almaty, Kazakhstan; Institute of Information and Computational Technologies, 050010, 28 Shevchenko str., Almaty, Kazakhstan",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute of Information and Computational Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Kazakhstan"
    },
    {
        "id": "2022.coling-1.628",
        "title": "Language-specific Effects on Automatic Speech Recognition Errors for World Englishes",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite recent advancements in automated speech recognition (ASR) technologies, reports of unequal performance across speakers of different demographic groups abound. At the same time, the focus on performance metrics such as the Word Error Rate (WER) in prior studies limit the specificity and scope of recommendations that can be offered for system engineering to overcome these challenges. The current study bridges this gap by investigating the performance of Otter\u2019s automatic captioning system on native and non-native English speakers of different language background through a linguistic analysis of segment-level errors. By examining language-specific error profiles for vowels and consonants motivated by linguistic theory, we find that certain categories of errors can be predicted from the phonological structure of a speaker\u2019s native language.",
        "author": "June Choe; Yiran Chen; May Pik Yu Chan; Aini Li; Xin Gao; Nicole Holliday",
        "authorids": "/j/june-choe/; /y/yiran-chen/; /m/may-pik-yu-chan/; /a/aini-li/; /x/xin-gao/; /n/nicole-holliday/",
        "bibtex": "@inproceedings{choe-etal-2022-language,\n    title = \"Language-specific Effects on Automatic Speech Recognition Errors for World Englishes\",\n    author = \"Choe, June  and\n      Chen, Yiran  and\n      Chan, May Pik Yu  and\n      Li, Aini  and\n      Gao, Xin  and\n      Holliday, Nicole\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.628/\",\n    pages = \"7177--7186\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.628.pdf",
        "site": "https://aclanthology.org/2022.coling-1.628/",
        "pdf_size": 1251983,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16426550503767021467&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Linguistics, University of Pennsylvania, USA; Department of Linguistics, University of Pennsylvania, USA; Department of Linguistics, University of Pennsylvania, USA; Department of Linguistics, University of Pennsylvania, USA; Department of Linguistics, University of Pennsylvania, USA; Department of Linguistics, University of Pennsylvania, USA",
        "aff_domain": "sas.upenn.edu;sas.upenn.edu;sas.upenn.edu;sas.upenn.edu;sas.upenn.edu;sas.upenn.edu",
        "email": "sas.upenn.edu;sas.upenn.edu;sas.upenn.edu;sas.upenn.edu;sas.upenn.edu;sas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.410",
        "title": "Large Sequence Representation Learning via Multi-Stage Latent Transformers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present LANTERN, a multi-stage transformer architecture for named-entity recognition (NER) designed to operate on indefinitely large text sequences (i.e. > 512 elements). For a given image of a form with structured text, our method uses language and spatial features to predict the entity tags of each text element. It breaks the quadratic computational constraints of the attention mechanism by operating over a learned latent space representation which encodes the input sequence via the cross-attention mechanism while having the multi-stage encoding component as a refinement over the NER predictions. As a proxy task, we propose RADAR, an LSTM classifier operating at character level, which predicts the relevance of a word with respect to the entity-recognition task. Additionally, we formulate a challenging novel NER use case, nutritional information extraction from food product labels. We created a dataset with 11,926 images depicting food product labels entitled TREAT dataset, with fully detailed annotations. Our method achieves superior performance against two competitive models designed for long sequences on the proposed TREAT dataset.",
        "author": "Ionut-Catalin Sandu; Daniel Voinea; Alin-Ionut Popa",
        "authorids": "/i/ionut-catalin-sandu/; /d/daniel-voinea/; /a/alin-ionut-popa/",
        "bibtex": "@inproceedings{sandu-etal-2022-large,\n    title = \"Large Sequence Representation Learning via Multi-Stage Latent Transformers\",\n    author = \"Sandu, Ionut-Catalin  and\n      Voinea, Daniel  and\n      Popa, Alin-Ionut\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.410/\",\n    pages = \"4633--4639\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.410.pdf",
        "site": "https://aclanthology.org/2022.coling-1.410/",
        "pdf_size": 10643999,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11183786912361199829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.300",
        "title": "Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The evaluation of recent embedding-based evaluation metrics for text generation is primarily based on measuring their correlation with human evaluations on standard benchmarks. However, these benchmarks are mostly from similar domains to those used for pretraining word embeddings. This raises concerns about the (lack of) generalization of embedding-based metrics to new and noisy domains that contain a different vocabulary than the pretraining data. In this paper, we examine the robustness of BERTScore, one of the most popular embedding-based metrics for text generation. We show that (a) an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, (b) taking embeddings from the first layer of pretrained models improves the robustness of all metrics, and (c) the highest robustness is achieved when using character-level embeddings, instead of token-based embeddings, from the first layer of the pretrained model.",
        "author": "Doan Nam Long Vu; Nafise Sadat Moosavi; Steffen Eger",
        "authorids": "/d/doan-nam-long-vu/; /n/nafise-sadat-moosavi/; /s/steffen-eger/",
        "bibtex": "@inproceedings{vu-etal-2022-layer,\n    title = \"Layer or Representation Space: What Makes {BERT}-based Evaluation Metrics Robust?\",\n    author = \"Vu, Doan Nam Long  and\n      Moosavi, Nafise Sadat  and\n      Eger, Steffen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.300/\",\n    pages = \"3401--3411\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.300.pdf",
        "site": "https://aclanthology.org/2022.coling-1.300/",
        "pdf_size": 887224,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18325131067114707189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, The University of Sheffield, UK; NLLG, Faculty of Technology, Bielefeld University, Germany",
        "aff_domain": "stud.tu-darmstadt.de; ; ",
        "email": "stud.tu-darmstadt.de; ; ",
        "github": "https://github.com/long21wt/robust-bert-based-metrics",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Technical University of Darmstadt;The University of Sheffield;Bielefeld University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;Faculty of Technology",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://www.sheffield.ac.uk;https://www.uni-bielefeld.de",
        "aff_unique_abbr": "TUD;Sheffield;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "2022.coling-1.276",
        "title": "LayerConnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained Language Models (PLMs) are the cornerstone of the modern Natural Language Processing (NLP). However, as PLMs become heavier, fine tuning all their parameters loses their efficiency. Existing parameter-efficient methods generally focus on reducing the trainable parameters in PLMs but neglect the inference speed, which limits the ability to deploy PLMs. In this paper, we propose LayerConnect (hypernetwork-assisted inter-layer connectors) to enhance inference efficiency. Specifically, a light-weight connector with a linear structure is inserted between two Transformer layers, and the parameters inside each connector are tuned by a hypernetwork comprising an interpolator and a down-sampler. We perform extensive experiments on the widely used the GLUE benchmark. The experimental results verify the inference efficiency of our model. Compared to Adapter, our model parameters are reduced to approximately 11.75%, while the performance degradation is kept to less than 5% (2.5 points on average).",
        "author": "Haoxiang Shi; Rongsheng Zhang; Jiaan Wang; Cen Wang; Yinhe Zheng; Tetsuya Sakai",
        "authorids": "/h/haoxiang-shi/; /r/rongsheng-zhang/; /j/jiaan-wang/; /c/cen-wang/; /y/yinhe-zheng/; /t/tetsuya-sakai/",
        "bibtex": "@inproceedings{shi-etal-2022-layerconnect,\n    title = \"{L}ayer{C}onnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency\",\n    author = \"Shi, Haoxiang  and\n      Zhang, Rongsheng  and\n      Wang, Jiaan  and\n      Wang, Cen  and\n      Zheng, Yinhe  and\n      Sakai, Tetsuya\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.276/\",\n    pages = \"3120--3126\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.276.pdf",
        "site": "https://aclanthology.org/2022.coling-1.276/",
        "pdf_size": 368651,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11853205380094912174&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Waseda University, Tokyo, Japan+NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; Soochow University, Suzhou, China+NetEase Inc., Hangzhou, China; KDDI Research Inc., Japan; Lingxin AI, China; Waseda University, Tokyo, Japan",
        "aff_domain": "toki.waseda.jp;corp.netease.com;stu.suda.edu.cn;kddi-research.jp;163.com;waseda.jp",
        "email": "toki.waseda.jp;corp.netease.com;stu.suda.edu.cn;kddi-research.jp;163.com;waseda.jp",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;2+1;3;4;0",
        "aff_unique_norm": "Waseda University;NetEase Inc.;Soochow University;KDDI Research Inc.;Lingxin AI",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.waseda.ac.jp;https://www.163.com;https://www.soochow.edu.cn;https://www.kddi-research.com;",
        "aff_unique_abbr": "Waseda;NetEase;;KDDI;",
        "aff_campus_unique_index": "0+1;1;2+1;0",
        "aff_campus_unique": "Tokyo;Hangzhou;Suzhou;",
        "aff_country_unique_index": "0+1;1;1+1;0;1;0",
        "aff_country_unique": "Japan;China"
    },
    {
        "id": "2022.coling-1.427",
        "title": "Learn2Weight: Parameter Adaptation against Similar-domain Adversarial Attacks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent work in black-box adversarial attacks for NLP systems has attracted attention. Prior black-box attacks assume that attackers can observe output labels from target models based on selected inputs. In this work, inspired by adversarial transferability, we propose a new type of black-box NLP adversarial attack that an attacker can choose a similar domain and transfer the adversarial examples to the target domain and cause poor performance in target model. Based on domain adaptation theory, we then propose a defensive strategy, called Learn2Weight, which trains to predict the weight adjustments for target model in order to defense the attack of similar-domain adversarial examples. Using Amazon multi-domain sentiment classification dataset, we empirically show that Learn2Weight model is effective against the attack compared to standard black-box defense methods such as adversarial training and defense distillation. This work contributes to the growing literature on machine learning safety.",
        "author": "Siddhartha Datta",
        "authorids": "/s/siddhartha-datta/",
        "bibtex": "@inproceedings{datta-2022-learn2weight,\n    title = \"{L}earn2{W}eight: Parameter Adaptation against Similar-domain Adversarial Attacks\",\n    author = \"Datta, Siddhartha\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.427/\",\n    pages = \"4832--4843\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.427.pdf",
        "site": "https://aclanthology.org/2022.coling-1.427/",
        "pdf_size": 785610,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13813960949334463662&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Oxford",
        "aff_domain": "cs.ox.ac.uk",
        "email": "cs.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.618",
        "title": "Learnable Dependency-based Double Graph Structure for Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dependency tree-based methods might be susceptible to the dependency tree due to that they inevitably introduce noisy information and neglect the rich relation information between words. In this paper, we propose a learnable dependency-based double graph (LD2G) model for aspect-based sentiment classification. We use multi-task learning for domain adaptive pretraining, which combines Biaffine Attention and Mask Language Model by incorporating features such as structure, relations and linguistic features in the sentiment text. Then we utilize the dependency enhanced double graph-based MPNN to deeply fuse structure features and relation features that are affected with each other for ASC. Experiment on four benchmark datasets shows that our model is superior to the state-of-the-art approaches.",
        "author": "Yinglong Ma; Yunhe Pang",
        "authorids": "/y/yinglong-ma/; /y/yunhe-pang/",
        "bibtex": "@inproceedings{ma-pang-2022-learnable,\n    title = \"Learnable Dependency-based Double Graph Structure for Aspect-based Sentiment Analysis\",\n    author = \"Ma, Yinglong  and\n      Pang, Yunhe\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.618/\",\n    pages = \"7086--7092\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.618.pdf",
        "site": "https://aclanthology.org/2022.coling-1.618/",
        "pdf_size": 448900,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3644381999027610030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2022.coling-1.456",
        "title": "Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "K-Nearest Neighbor Neural Machine Translation (kNNMT) successfully incorporates external corpus by retrieving word-level representations at test time. Generally, kNNMT borrows the off-the-shelf context representation in the translation task, e.g., the output of the last decoder layer, as the query vector of the retrieval task. In this work, we highlight that coupling the representations of these two tasks is sub-optimal for fine-grained retrieval. To alleviate it, we leverage supervised contrastive learning to learn the distinctive retrieval representation derived from the original context representation. We also propose a fast and effective approach to constructing hard negative samples. Experimental results on five domains show that our approach improves the retrieval accuracy and BLEU score compared to vanilla kNNMT.",
        "author": "Qiang Wang; Rongxiang Weng; Ming Chen",
        "authorids": "/q/qiang-wang/; /r/rongxiang-weng/; /m/ming-chen/",
        "bibtex": "@inproceedings{wang-etal-2022-learning-decoupled,\n    title = \"Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation\",\n    author = \"Wang, Qiang  and\n      Weng, Rongxiang  and\n      Chen, Ming\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.456/\",\n    pages = \"5142--5147\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.456.pdf",
        "site": "https://aclanthology.org/2022.coling-1.456/",
        "pdf_size": 467995,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9153264633865117303&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.175",
        "title": "Learning Hierarchy-Aware Quaternion Knowledge Graph Embeddings with Representing Relations as 3D Rotations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links. It is crucial for knowledge graph embedding models to model and infer various relation patterns, such as symmetry/antisymmetry. However, many existing approaches fail to model semantic hierarchies, which are common in the real world. We propose a new model called HRQE, which represents entities as pure quaternions. The relational embedding consists of two parts: (a) Using unit quaternions to represent the rotation part in 3D space, where the head entities are rotated by the corresponding relations through Hamilton product. (b) Using scale parameters to constrain the modulus of entities to make them have hierarchical distributions. To the best of our knowledge, HRQE is the first model that can encode symmetry/antisymmetry, inversion, composition, multiple relation patterns and learn semantic hierarchies simultaneously. Experimental results demonstrate the effectiveness of HRQE against some of the SOTA methods on four well-established knowledge graph completion benchmarks.",
        "author": "Jinfa Yang; Xianghua Ying; Yongjie Shi; Xin Tong; Ruibin Wang; Taiyan Chen; Bowei Xing",
        "authorids": "/j/jinfa-yang/; /x/xianghua-ying/; /y/yongjie-shi/; /x/xin-tong/; /r/ruibin-wang/; /t/taiyan-chen/; /b/bowei-xing/",
        "bibtex": "@inproceedings{yang-etal-2022-learning-hierarchy,\n    title = \"Learning Hierarchy-Aware Quaternion Knowledge Graph Embeddings with Representing Relations as 3{D} Rotations\",\n    author = \"Yang, Jinfa  and\n      Ying, Xianghua  and\n      Shi, Yongjie  and\n      Tong, Xin  and\n      Wang, Ruibin  and\n      Chen, Taiyan  and\n      Xing, Bowei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.175/\",\n    pages = \"2011--2023\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.175.pdf",
        "site": "https://aclanthology.org/2022.coling-1.175/",
        "pdf_size": 7484504,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8423384593853655871&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University; Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University; Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University; Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University; Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University; Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University; Key Laboratory of Machine Perception (MOE) School of Intelligence Science and Technology, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Intelligence Science and Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.590",
        "title": "Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Target-oriented multimodal sentiment classification (TMSC) is a new subtask of aspect-based sentiment analysis, which aims to determine the sentiment polarity of the opinion target mentioned in a (sentence, image) pair. Recently, dominant works employ the attention mechanism to capture the corresponding visual representations of the opinion target, and then aggregate them as evidence to make sentiment predictions. However, they still suffer from two problems: (1) The granularity of the opinion target in two modalities is inconsistent, which causes visual attention sometimes fail to capture the corresponding visual representations of the target; (2) Even though it is captured, there are still significant differences between the visual representations expressing the same mood, which brings great difficulty to sentiment prediction. To this end, we propose a novel Knowledge-enhanced Framework (KEF) in this paper, which can successfully exploit adjective-noun pairs extracted from the image to improve the visual attention capability and sentiment prediction capability of the TMSC task. Extensive experimental results show that our framework consistently outperforms state-of-the-art works on two public datasets.",
        "author": "Fei Zhao; Zhen Wu; Siyu Long; Xinyu Dai; Shujian Huang; Jiajun Chen",
        "authorids": "/f/fei-zhao/; /z/zhen-wu/; /s/siyu-long/; /x/xinyu-dai/; /s/shujian-huang/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{zhao-etal-2022-learning-adjective,\n    title = \"Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification\",\n    author = \"Zhao, Fei  and\n      Wu, Zhen  and\n      Long, Siyu  and\n      Dai, Xinyu  and\n      Huang, Shujian  and\n      Chen, Jiajun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.590/\",\n    pages = \"6784--6794\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.590.pdf",
        "site": "https://aclanthology.org/2022.coling-1.590/",
        "pdf_size": 2172030,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7071606912894569023&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China",
        "aff_domain": "smail.nju.edu.cn;nju.edu.cn;smail.nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;nju.edu.cn;smail.nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.490",
        "title": "Learning to Focus on the Foreground for Temporal Sentence Grounding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Temporal sentence grounding (TSG) is crucial and fundamental for video understanding. Previous works typically model the target activity referred to the sentence query in a video by extracting the appearance information from each whole frame. However, these methods fail to distinguish visually similar background noise and capture subtle details of small objects. Although a few recent works additionally adopt a detection model to filter out the background contents and capture local appearances of foreground objects, they rely on the quality of the detection model and suffer from the time-consuming detection process. To this end, we propose a novel detection-free framework for TSG\u2014Grounding with Learnable Foreground (GLF), which efficiently learns to locate the foreground regions related to the query in consecutive frames for better modelling the target activity. Specifically, we first split each video frame into multiple patch candidates of equal size, and reformulate the foreground detection problem as a patch localization task. Then, we develop a self-supervised coarse-to-fine paradigm to learn to locate the most query-relevant patch in each frame and aggregate them among the video for final grounding. Further, we employ a multi-scale patch reasoning strategy to capture more fine-grained foreground information. Extensive experiments on three challenging datasets (Charades-STA, TACoS, ActivityNet) show that the proposed GLF outperforms state-of-the-art methods.",
        "author": "Daizong Liu; Wei Hu",
        "authorids": "/d/daizong-liu/; /w/wei-hu/",
        "bibtex": "@inproceedings{liu-hu-2022-learning,\n    title = \"Learning to Focus on the Foreground for Temporal Sentence Grounding\",\n    author = \"Liu, Daizong  and\n      Hu, Wei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.490/\",\n    pages = \"5532--5541\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.490.pdf",
        "site": "https://aclanthology.org/2022.coling-1.490/",
        "pdf_size": 2889008,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1123971357842919421&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.260",
        "title": "Learning to Generate Explanation from e-Hospital Services for Medical Suggestion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Explaining the reasoning of neural models has attracted attention in recent years. Providing highly-accessible and comprehensible explanations in natural language is useful for humans to understand model\u2019s prediction results. In this work, we present a pilot study to investigate explanation generation with a narrative and causal structure for the scenario of health consulting. Our model generates a medical suggestion regarding the patient\u2019s concern and provides an explanation as the outline of the reasoning. To align the generated explanation with the suggestion, we propose a novel discourse-aware mechanism with multi-task learning. Experimental results show that our model achieves promising performances in both quantitative and human evaluation.",
        "author": "Wei-Lin Chen; An-Zi Yen; Hen-Hsen Huang; Hsin-Hsi Chen",
        "authorids": "/w/wei-lin-chen/; /a/an-zi-yen/; /h/hen-hsen-huang/; /h/hsin-hsi-chen/",
        "bibtex": "@inproceedings{chen-etal-2022-learning-generate,\n    title = \"Learning to Generate Explanation from e-Hospital Services for Medical Suggestion\",\n    author = \"Chen, Wei-Lin  and\n      Yen, An-Zi  and\n      Huang, Hen-Hsen  and\n      Chen, Hsin-Hsi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.260/\",\n    pages = \"2946--2951\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.260.pdf",
        "site": "https://aclanthology.org/2022.coling-1.260/",
        "pdf_size": 260512,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7584408835848217345&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Taiwan; Institute of Information Science, Academia Sinica, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taiwan",
        "aff_domain": "nlg.csie.ntu.edu.tw;nycu.edu.tw;iis.sinica.edu.tw;ntu.edu.tw",
        "email": "nlg.csie.ntu.edu.tw;nycu.edu.tw;iis.sinica.edu.tw;ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "National Taiwan University;National Yang Ming Chiao Tung University;Academia Sinica",
        "aff_unique_dep": "Department of Computer Science and Information Engineering;Department of Computer Science;Institute of Information Science",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.nctu.edu.tw;https://www.sinica.edu.tw",
        "aff_unique_abbr": "NTU;NYCU;AS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.23",
        "title": "Learning to Improve Persona Consistency in Multi-party Dialogue Generation via Text Knowledge Enhancement",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In an open-domain dialogue system, the consistent persona is a key factor to generate real and coherent dialogues. Existing methods suffer from the incomprehensive persona tags that have unique and obscure meanings to describe human\u2019s personality. Besides, the addressee information, which is closely related to express personality in multi-party dialogues, has been neglected. In this paper, we construct a multi-party personalized dialogue dataset and propose a graph convolution network model (PersonaTKG) with addressee selecting mechanism that integrates personas, dialogue utterances, and external text knowledge in a unified graph. Extensive experiments have shown that PersonaTKG outperforms the baselines by large margins and effectively improves persona consistency in the generated responses.",
        "author": "Dongshi Ju; Shi Feng; Pengcheng Lv; Daling Wang; Yifei Zhang",
        "authorids": "/d/dongshi-ju/; /s/shi-feng/; /p/pengcheng-lv/; /d/daling-wang/; /y/yifei-zhang/",
        "bibtex": "@inproceedings{ju-etal-2022-learning,\n    title = \"Learning to Improve Persona Consistency in Multi-party Dialogue Generation via Text Knowledge Enhancement\",\n    author = \"Ju, Dongshi  and\n      Feng, Shi  and\n      Lv, Pengcheng  and\n      Wang, Daling  and\n      Zhang, Yifei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.23/\",\n    pages = \"298--309\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.23.pdf",
        "site": "https://aclanthology.org/2022.coling-1.23/",
        "pdf_size": 3511733,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5934358983051688027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Northeastern University; Northeastern University; Northeastern University; Northeastern University; Northeastern University",
        "aff_domain": "163.com;cse.neu.edu.cn;foxmail.com;cse.neu.edu.cn;cse.neu.edu.cn",
        "email": "163.com;cse.neu.edu.cn;foxmail.com;cse.neu.edu.cn;cse.neu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.143",
        "title": "Less Is Better: Recovering Intended-Feature Subspace to Robustify NLU Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Datasets with significant proportions of bias present threats for training a trustworthy model on NLU tasks. Despite yielding great progress, current debiasing methods impose excessive reliance on the knowledge of bias attributes. Definition of the attributes, however, is elusive and varies across different datasets. In addition, leveraging these attributes at input level to bias mitigation may leave a gap between intrinsic properties and the underlying decision rule. To narrow down this gap and liberate the supervision on bias, we suggest extending bias mitigation into feature space. Therefore, a novel model, Recovering Intended-Feature Subspace with Knowledge-Free (RISK) is developed. Assuming that shortcut features caused by various biases are unintended for prediction, RISK views them as redundant features. When delving into a lower manifold to remove redundancies, RISK reveals that an extremely low-dimensional subspace with intended features can robustly represent the highly biased dataset. Empirical results demonstrate our model can consistently improve model generalization to out-of-distribution set, and achieves a new state-of-the-art performance.",
        "author": "Ting Wu; Tao Gui",
        "authorids": "/t/ting-wu/; /t/tao-gui/",
        "bibtex": "@inproceedings{wu-gui-2022-less,\n    title = \"Less Is Better: Recovering Intended-Feature Subspace to Robustify {NLU} Models\",\n    author = \"Wu, Ting  and\n      Gui, Tao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.143/\",\n    pages = \"1666--1676\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.143.pdf",
        "site": "https://aclanthology.org/2022.coling-1.143/",
        "pdf_size": 2600340,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14596991344395348028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Fudan University; Fudan University",
        "aff_domain": "m.fudan.edu.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn;fudan.edu.cn",
        "github": "https://github.com/CuteyThyme/RISK.git",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.209",
        "title": "LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most NER methods rely on extensive labeled data for model training, which struggles in the low-resource scenarios with limited training data. Existing dominant approaches usually suffer from the challenge that the target domain has different label sets compared with a resource-rich source domain, which can be concluded as class transfer and domain transfer. In this paper, we propose a lightweight tuning paradigm for low-resource NER via pluggable prompting (LightNER). Specifically, we construct the unified learnable verbalizer of entity categories to generate the entity span sequence and entity categories without any label-specific classifiers, thus addressing the class transfer issue. We further propose a pluggable guidance module by incorporating learnable parameters into the self-attention layer as guidance, which can re-modulate the attention and adapt pre-trained weights. Note that we only tune those inserted module with the whole parameter of the pre-trained language model fixed, thus, making our approach lightweight and flexible for low-resource scenarios and can better transfer knowledge across domains. Experimental results show that LightNER can obtain comparable performance in the standard supervised setting and outperform strong baselines in low-resource settings.",
        "author": "Xiang Chen; Lei Li; Shumin Deng; Chuanqi Tan; Changliang Xu; Fei Huang; Luo Si; Huajun Chen; Ningyu Zhang",
        "authorids": "/x/xiang-chen/; /l/lei-li/; /s/shumin-deng/; /c/chuanqi-tan/; /c/changliang-xu/; /f/fei-huang/; /l/luo-si/; /h/huajun-chen/; /n/ningyu-zhang/",
        "bibtex": "@inproceedings{chen-etal-2022-lightner,\n    title = \"{L}ight{NER}: A Lightweight Tuning Paradigm for Low-resource {NER} via Pluggable Prompting\",\n    author = \"Chen, Xiang  and\n      Li, Lei  and\n      Deng, Shumin  and\n      Tan, Chuanqi  and\n      Xu, Changliang  and\n      Huang, Fei  and\n      Si, Luo  and\n      Chen, Huajun  and\n      Zhang, Ningyu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.209/\",\n    pages = \"2374--2387\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.209.pdf",
        "site": "https://aclanthology.org/2022.coling-1.209/",
        "pdf_size": 827312,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17740552800691425343&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2022.coling-1.77",
        "title": "Linguistically Motivated Features for Classifying Shorter Text into Fiction and Non-Fiction Genre",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This work deploys linguistically motivated features to classify paragraph-level text into fiction and non-fiction genre using a logistic regression model and infers lexical and syntactic properties that distinguish the two genres. Previous works have focused on classifying document-level text into fiction and non-fiction genres, while in this work, we deal with shorter texts which are closer to real-world applications like sentiment analysis of tweets. Going beyond simple POS tag ratios proposed in Qureshi et al.(2019) for document-level classification, we extracted multiple linguistically motivated features belonging to four categories: Lexical features, POS ratio features, Syntactic features and Raw features. For the task of short-text classification, a model containing 28 best-features (selected via Recursive feature elimination with cross-validation; RFECV) confers an accuracy jump of 15.56 % over a baseline model consisting of 2 POS-ratio features found effective in previous work (cited above). The efficacy of the above model containing a linguistically motivated feature set also transfers over to another dataset viz, Baby BNC corpus. We also compared the classification accuracy of the logistic regression model with two deep-learning models. A 1D CNN model gives an increase of 2% accuracy over the logistic Regression classifier on both corpora. And the BERT-base-uncased model gives the best classification accuracy of 97% on Brown corpus and 98% on Baby BNC corpus. Although both the deep learning models give better results in terms of classification accuracy, the problem of interpreting these models remains unsolved. In contrast, regression model coefficients revealed that fiction texts tend to have more character-level diversity and have lower lexical density (quantified using content-function word ratios) compared to non-fiction texts. Moreover, subtle differences in word order exist between the two genres, i.e., in fiction texts Verbs precede Adverbs (inter-alia).",
        "author": "Arman Kazmi; Sidharth Ranjan; Arpit Sharma; Rajakrishnan Rajkumar",
        "authorids": "/a/arman-kazmi/; /s/sidharth-ranjan/; /a/arpit-sharma/; /r/rajakrishnan-rajkumar/",
        "bibtex": "@inproceedings{kazmi-etal-2022-linguistically,\n    title = \"Linguistically Motivated Features for Classifying Shorter Text into Fiction and Non-Fiction Genre\",\n    author = \"Kazmi, Arman  and\n      Ranjan, Sidharth  and\n      Sharma, Arpit  and\n      Rajkumar, Rajakrishnan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.77/\",\n    pages = \"922--937\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.77.pdf",
        "site": "https://aclanthology.org/2022.coling-1.77/",
        "pdf_size": 489250,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10925853173456085771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "IISER Bhopal1; IIT Delhi2; IISER Bhopal1; IISER Bhopal1",
        "aff_domain": "gmail.com;gmail.com;iiserb.ac.in;iiserb.ac.in",
        "email": "gmail.com;gmail.com;iiserb.ac.in;iiserb.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Indian Institute of Science Education and Research, Bhopal;Indian Institute of Technology Delhi",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iiserbhopal.ac.in;https://www.iitd.ac.in",
        "aff_unique_abbr": "IISER Bhopal;IIT Delhi",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Bhopal;Delhi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.449",
        "title": "Linguistically-Motivated Yor\u00f9b\u00e1-English Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Translating between languages where certain features are marked morphologically in one but absent or marked contextually in the other is an important test case for machine translation. When translating into English which marks (in)definiteness morphologically, from Yor\u00f9b\u00e1 which uses bare nouns but marks these features contextually, ambiguities arise. In this work, we perform fine-grained analysis on how an SMT system compares with two NMT systems (BiLSTM and Transformer) when translating bare nouns in Yor\u00f9b\u00e1 into English. We investigate how the systems what extent they identify BNs, correctly translate them, and compare with human translation patterns. We also analyze the type of errors each model makes and provide a linguistic description of these errors. We glean insights for evaluating model performance in low-resource settings. In translating bare nouns, our results show the transformer model outperforms the SMT and BiLSTM models for 4 categories, the BiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the NMT models for 1 category.",
        "author": "Ife Adebara; Muhammad Abdul-Mageed; Miikka Silfverberg",
        "authorids": "/i/ife-adebara/; /m/muhammad-abdul-mageed/; /m/miikka-silfverberg/",
        "bibtex": "@inproceedings{adebara-etal-2022-linguistically,\n    title = \"Linguistically-Motivated {Y}or{\\`u}b{\\'a}-{E}nglish Machine Translation\",\n    author = \"Adebara, Ife  and\n      Abdul-Mageed, Muhammad  and\n      Silfverberg, Miikka\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.449/\",\n    pages = \"5066--5075\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.449.pdf",
        "site": "https://aclanthology.org/2022.coling-1.449/",
        "pdf_size": 402568,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1156265365437632987&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Linguistics1 + Deep Learning and Natural Language Processing Group2; Department of Linguistics1 + Deep Learning and Natural Language Processing Group2; Department of Linguistics1 + Deep Learning and Natural Language Processing Group2",
        "aff_domain": "ubc.ca;ubc.ca;ubc.ca",
        "email": "ubc.ca;ubc.ca;ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University Affiliation Not Specified;University of California, Berkeley",
        "aff_unique_dep": "Department of Linguistics;Department of Computer Science",
        "aff_unique_url": ";https://www.berkeley.edu",
        "aff_unique_abbr": ";UC Berkeley",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2022.coling-1.303",
        "title": "LipKey: A Large-Scale News Dataset for Absent Keyphrases Generation and Abstractive Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Summaries, keyphrases, and titles are different ways of concisely capturing the content of a document. While most previous work has released the datasets of keyphrases and summarization separately, in this work, we introduce LipKey, the largest news corpus with human-written abstractive summaries, absent keyphrases, and titles. We jointly use the three elements via multi-task training and training as joint structured inputs, in the context of document summarization. We find that including absent keyphrases and titles as additional context to the source document improves transformer-based summarization models.",
        "author": "Fajri Koto; Timothy Baldwin; Jey Han Lau",
        "authorids": "/f/fajri-koto/; /t/timothy-baldwin/; /j/jey-han-lau/",
        "bibtex": "@inproceedings{koto-etal-2022-lipkey,\n    title = \"{L}ip{K}ey: A Large-Scale News Dataset for Absent Keyphrases Generation and Abstractive Summarization\",\n    author = \"Koto, Fajri  and\n      Baldwin, Timothy  and\n      Lau, Jey Han\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.303/\",\n    pages = \"3427--3437\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.303.pdf",
        "site": "https://aclanthology.org/2022.coling-1.303/",
        "pdf_size": 453435,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13475367614347279548&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne + Department of Natural Language Processing, MBZUAI; School of Computing and Information Systems, The University of Melbourne",
        "aff_domain": "gmail.com;ldwin.net;gmail.com",
        "email": "gmail.com;ldwin.net;gmail.com",
        "github": "https://github.com/fajri91/LipKey",
        "project": "https://www.liputan6.com",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "The University of Melbourne;MBZUAI",
        "aff_unique_dep": "School of Computing and Information Systems;Department of Natural Language Processing",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UniMelb;MBZUAI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne;",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Australia;United Arab Emirates"
    },
    {
        "id": "2022.coling-1.83",
        "title": "Locally Distributed Activation Vectors for Guided Feature Attribution",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Explaining the predictions of a deep neural network (DNN) is a challenging problem. Many attempts at interpreting those predictions have focused on attribution-based methods, which assess the contributions of individual features to each model prediction. However, attribution-based explanations do not always provide faithful explanations to the target model, e.g., noisy gradients can result in unfaithful feature attribution for back-propagation methods. We present a method to learn explanations-specific representations while constructing deep network models for text classification. These representations can be used to faithfully interpret black-box predictions, i.e., highlighting the most important input features and their role in any particular prediction. We show that learning specific representations improves model interpretability across various tasks, for both qualitative and quantitative evaluations, while preserving predictive performance.",
        "author": "Housam K. B. Bashier; Mi-Young Kim; Randy Goebel",
        "authorids": "/h/housam-k-b-bashier/; /m/mi-young-kim/; /r/randy-goebel/",
        "bibtex": "@inproceedings{bashier-etal-2022-locally,\n    title = \"Locally Distributed Activation Vectors for Guided Feature Attribution\",\n    author = \"Bashier, Housam K. B.  and\n      Kim, Mi-Young  and\n      Goebel, Randy\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.83/\",\n    pages = \"994--1005\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.83.pdf",
        "site": "https://aclanthology.org/2022.coling-1.83/",
        "pdf_size": 1787244,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oxxrjnZs3SwJ:scholar.google.com/&scioq=Locally+Distributed+Activation+Vectors+for+Guided+Feature+Attribution&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Computing Science, University of Alberta; Department of Science, Augustana Faculty, University of Alberta + Alberta Machine Intelligence Institute; Department of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Alberta;Alberta Machine Intelligence Institute",
        "aff_unique_dep": "Department of Computing Science;",
        "aff_unique_url": "https://www.ualberta.ca;https://www.ami.ualberta.ca/",
        "aff_unique_abbr": "UAlberta;AMII",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Augustana Faculty",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.142",
        "title": "Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-hop reasoning requires aggregating multiple documents to answer a complex question. Existing methods usually decompose the multi-hop question into simpler single-hop questions to solve the problem for illustrating the explainable reasoning process. However, they ignore grounding on the supporting facts of each reasoning step, which tends to generate inaccurate decompositions. In this paper, we propose an interpretable stepwise reasoning framework to incorporate both single-hop supporting sentence identification and single-hop question generation at each intermediate step, and utilize the inference of the current hop for the next until reasoning out the final result. We employ a unified reader model for both intermediate hop reasoning and final hop inference and adopt joint optimization for more accurate and robust multi-hop reasoning. We conduct experiments on two benchmark datasets HotpotQA and 2WikiMultiHopQA. The results show that our method can effectively boost performance and also yields a better interpretable reasoning process without decomposition supervision.",
        "author": "Siyuan Wang; Zhongyu Wei; Zhihao Fan; Qi Zhang; Xuanjing Huang",
        "authorids": "/s/siyuan-wang/; /z/zhongyu-wei/; /z/zhihao-fan/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{wang-etal-2022-locate,\n    title = \"Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering\",\n    author = \"Wang, Siyuan  and\n      Wei, Zhongyu  and\n      Fan, Zhihao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.142/\",\n    pages = \"1655--1665\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.142.pdf",
        "site": "https://aclanthology.org/2022.coling-1.142/",
        "pdf_size": 630421,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13561864063783246169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Data Science, Fudan University, China; School of Data Science, Fudan University, China + Research Institute of Intelligent and Complex Systems, Fudan University, China; School of Data Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "https://github.com/WangsyGit/StepwiseQA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+0;0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Data Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.145",
        "title": "Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question answering over knowledge bases (KBQA) for complex questions is a challenging task in natural language processing. Recently, generation-based methods that translate natural language questions to executable logical forms have achieved promising performance. These methods use auxiliary information to augment the logical form generation of questions with unseen KB items or novel combinations, but the noise introduced can also leads to more incorrect results. In this work, we propose GMT-KBQA, a Generation-based KBQA method via Multi-Task learning, to better retrieve and utilize auxiliary information. GMT-KBQA first obtains candidate entities and relations through dense retrieval, and then introduces a multi-task model which jointly learns entity disambiguation, relation classification, and logical form generation. Experimental results show that GMT-KBQA achieves state-of-the-art results on both ComplexWebQuestions and WebQuestionsSP datasets. Furthermore, the detailed evaluation demonstrates that GMT-KBQA benefits from the auxiliary tasks and has a strong generalization capability.",
        "author": "Xixin Hu; Xuan Wu; Yiheng Shu; Yuzhong Qu",
        "authorids": "/x/xixin-hu/; /x/xuan-wu/; /y/yiheng-shu/; /y/yuzhong-qu/",
        "bibtex": "@inproceedings{hu-etal-2022-logical,\n    title = \"Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases\",\n    author = \"Hu, Xixin  and\n      Wu, Xuan  and\n      Shu, Yiheng  and\n      Qu, Yuzhong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.145/\",\n    pages = \"1687--1696\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.145.pdf",
        "site": "https://aclanthology.org/2022.coling-1.145/",
        "pdf_size": 437168,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2423834910015280575&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/HXX97/GMT-KBQA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.292",
        "title": "MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Acronym extraction is the task of identifying acronyms and their expanded forms in texts that is necessary for various NLP applications. Despite major progress for this task in recent years, one limitation of existing AE research is that they are limited to the English language and certain domains (i.e., scientific and biomedical). Challenges of AE in other languages and domains are mainly unexplored. As such, lacking annotated datasets in multiple languages and domains has been a major issue to prevent research in this direction. To address this limitation, we propose a new dataset for multilingual and multi-domain AE. Specifically, 27,200 sentences in 6 different languages and 2 new domains, i.e., legal and scientific, are manually annotated for AE. Our experiments on the dataset show that AE in different languages and learning settings has unique challenges, emphasizing the necessity of further research on multilingual and multi-domain AE.",
        "author": "Amir Pouran Ben Veyseh; Nicole Meister; Seunghyun Yoon; Rajiv Jain; Franck Dernoncourt; Thien Huu Nguyen",
        "authorids": "/a/amir-pouran-ben-veyseh/; /n/nicole-meister/; /s/seunghyun-yoon/; /r/rajiv-jain/; /f/franck-dernoncourt/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{veyseh-etal-2022-macronym,\n    title = \"{MACRONYM}: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction\",\n    author = \"Veyseh, Amir Pouran Ben  and\n      Meister, Nicole  and\n      Yoon, Seunghyun  and\n      Jain, Rajiv  and\n      Dernoncourt, Franck  and\n      Nguyen, Thien Huu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.292/\",\n    pages = \"3309--3314\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.292.pdf",
        "site": "https://aclanthology.org/2022.coling-1.292/",
        "pdf_size": 184013,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14455219613721017234&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, University of Oregon, OR, USA; Department of Computer Science, Stanford University, CA, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Department of Computer Science, University of Oregon, OR, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu;stanford.edu;adobe.com;adobe.com;adobe.com",
        "email": "cs.uoregon.edu;cs.uoregon.edu;stanford.edu;adobe.com;adobe.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;2;2;0",
        "aff_unique_norm": "University of Oregon;Stanford University;Adobe Research",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;",
        "aff_unique_url": "https://www.uoregon.edu;https://www.stanford.edu;https://research.adobe.com",
        "aff_unique_abbr": "UO;Stanford;Adobe",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.262",
        "title": "MCS: An In-battle Commentary System for MOBA Games",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper introduces a generative system for in-battle real-time commentary in mobile MOBA games. Event commentary is important for battles in MOBA games, which is applicable to a wide range of scenarios like live streaming, e-sports commentary and combat information analysis. The system takes real-time match statistics and events as input, and an effective transform method is designed to convert match statistics and utterances into consistent encoding space. This paper presents the general framework and implementation details of the proposed system, and provides experimental results on large-scale real-world match data.",
        "author": "Xiaofeng Qi; Chao Li; Zhongping Liang; Jigang Liu; Cheng Zhang; Yuanxin Wei; Lin Yuan; Guang Yang; Lanxiao Huang; Min Li",
        "authorids": "/x/xiaofeng-qi/; /c/chao-li/; /z/zhongping-liang/; /j/jigang-liu/; /c/cheng-zhang/; /y/yuanxin-wei/; /l/lin-yuan/; /g/guang-yang/; /l/lanxiao-huang/; /m/min-li/",
        "bibtex": "@inproceedings{qi-etal-2022-mcs,\n    title = \"{MCS}: An In-battle Commentary System for {MOBA} Games\",\n    author = \"Qi, Xiaofeng  and\n      Li, Chao  and\n      Liang, Zhongping  and\n      Liu, Jigang  and\n      Zhang, Cheng  and\n      Wei, Yuanxin  and\n      Yuan, Lin  and\n      Yang, Guang  and\n      Huang, Lanxiao  and\n      Li, Min\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.262/\",\n    pages = \"2962--2967\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.262.pdf",
        "site": "https://aclanthology.org/2022.coling-1.262/",
        "pdf_size": 1618813,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2256377942687028792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China; Tencent Timi L1 Studio, Chengdu, China",
        "aff_domain": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "email": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "https://youtu.be/G0lKZKd7eco",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Timi L1 Studio",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Chengdu",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.206",
        "title": "MECI: A Multilingual Dataset for Event Causality Identification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event Causality Identification (ECI) is the task of detecting causal relations between events mentioned in the text. Although this task has been extensively studied for English materials, it is under-explored for many other languages. A major reason for this issue is the lack of multilingual datasets that provide consistent annotations for event causality relations in multiple non-English languages. To address this issue, we introduce a new multilingual dataset for ECI, called MECI. The dataset employs consistent annotation guidelines for five typologically different languages, i.e., English, Danish, Spanish, Turkish, and Urdu. Our dataset thus enable a new research direction on cross-lingual transfer learning for ECI. Our extensive experiments demonstrate high quality for MECI that can provide ample research challenges and directions for future research. We will publicly release MECI to promote research on multilingual ECI.",
        "author": "Viet Dac Lai; Amir Pouran Ben Veyseh; Minh Van Nguyen; Franck Dernoncourt; Thien Huu Nguyen",
        "authorids": "/v/viet-dac-lai/; /a/amir-pouran-ben-veyseh/; /m/minh-van-nguyen/; /f/franck-dernoncourt/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{lai-etal-2022-meci,\n    title = \"{MECI}: A Multilingual Dataset for Event Causality Identification\",\n    author = \"Lai, Viet Dac  and\n      Veyseh, Amir Pouran Ben  and\n      Nguyen, Minh Van  and\n      Dernoncourt, Franck  and\n      Nguyen, Thien Huu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.206/\",\n    pages = \"2346--2356\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.206.pdf",
        "site": "https://aclanthology.org/2022.coling-1.206/",
        "pdf_size": 789837,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2162334127585437763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Dept. of Computer Science, University of Oregon, OR, USA; Dept. of Computer Science, University of Oregon, OR, USA; Dept. of Computer Science, University of Oregon, OR, USA; Adobe Research, Seattle, WA, USA; Dept. of Computer Science, University of Oregon, OR, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu;cs.uoregon.edu;adobe.com;cs.uoregon.edu",
        "email": "cs.uoregon.edu;cs.uoregon.edu;cs.uoregon.edu;adobe.com;cs.uoregon.edu",
        "github": "https://github.com/nlp-uoregon/meci-dataset",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Oregon;Adobe Research",
        "aff_unique_dep": "Dept. of Computer Science;",
        "aff_unique_url": "https://www.uoregon.edu;https://research.adobe.com",
        "aff_unique_abbr": "UO;Adobe",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.102",
        "title": "MICO: Selective Search with Mutual Information Co-training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In contrast to traditional exhaustive search, selective search first clusters documents into several groups before all the documents are searched exhaustively by a query, to limit the search executed within one group or only a few groups. Selective search is designed to reduce the latency and computation in modern large-scale search systems. In this study, we propose MICO, a Mutual Information CO-training framework for selective search with minimal supervision using the search logs. After training, MICO does not only cluster the documents, but also routes unseen queries to the relevant clusters for efficient retrieval. In our empirical experiments, MICO significantly improves the performance on multiple metrics of selective search and outperforms a number of existing competitive baselines.",
        "author": "Zhanyu Wang; Xiao Zhang; Hyokun Yun; Choon Hui Teo; Trishul Chilimbi",
        "authorids": "/z/zhanyu-wang/; /x/xiao-zhang/; /h/hyokun-yun/; /c/choon-hui-teo/; /t/trishul-chilimbi/",
        "bibtex": "@inproceedings{wang-etal-2022-mico,\n    title = \"{MICO}: Selective Search with Mutual Information Co-training\",\n    author = \"Wang, Zhanyu  and\n      Zhang, Xiao  and\n      Yun, Hyokun  and\n      Teo, Choon Hui  and\n      Chilimbi, Trishul\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.102/\",\n    pages = \"1179--1192\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.102.pdf",
        "site": "https://aclanthology.org/2022.coling-1.102/",
        "pdf_size": 1767407,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17555552074495202536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Purdue University; Amazon; Amazon; Amazon; Amazon",
        "aff_domain": "purdue.edu;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "purdue.edu;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Purdue University;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.purdue.edu;https://www.amazon.com",
        "aff_unique_abbr": "Purdue;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.8",
        "title": "Machine Reading, Fast and Slow: When Do Models \u201cUnderstand\u201d Language?",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Two of the most fundamental issues in Natural Language Understanding (NLU) at present are: (a) how it can established whether deep learning-based models score highly on NLU benchmarks for the \u201dright\u201d reasons; and (b) what those reasons would even be. We investigate the behavior of reading comprehension models with respect to two linguistic \u201dskills\u201d: coreference resolution and comparison. We propose a definition for the reasoning steps expected from a system that would be \u201dreading slowly\u201d, and compare that with the behavior of five models of the BERT family of various sizes, observed through saliency scores and counterfactual explanations. We find that for comparison (but not coreference) the systems based on larger encoders are more likely to rely on the \u201dright\u201d information, but even they struggle with generalization, suggesting that they still learn specific lexical patterns rather than the general principles of comparison.",
        "author": "Sagnik Ray Choudhury; Anna Rogers; Isabelle Augenstein",
        "authorids": "/s/sagnik-ray-choudhury/; /a/anna-rogers/; /i/isabelle-augenstein/",
        "bibtex": "@inproceedings{ray-choudhury-etal-2022-machine,\n    title = \"Machine Reading, Fast and Slow: When Do Models {\\textquotedblleft}Understand{\\textquotedblright} Language?\",\n    author = \"Ray Choudhury, Sagnik  and\n      Rogers, Anna  and\n      Augenstein, Isabelle\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.8/\",\n    pages = \"78--93\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.8.pdf",
        "site": "https://aclanthology.org/2022.coling-1.8/",
        "pdf_size": 471677,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10201598618549793531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Michigan+University of Copenhagen; University of Copenhagen; University of Copenhagen",
        "aff_domain": "gmail.com;sodas.ku.dk;di.ku.dk",
        "email": "gmail.com;sodas.ku.dk;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Michigan;University of Copenhagen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.ku.dk",
        "aff_unique_abbr": "UM;UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "United States;Denmark"
    },
    {
        "id": "2022.coling-1.615",
        "title": "Making Parameter-efficient Tuning More Efficient: A Unified Framework for Classification Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large pre-trained language models (PLMs) have demonstrated superior performance in industrial applications. Recent studies have explored parameter-efficient PLM tuning, which only updates a small amount of task-specific parameters while achieving both high efficiency and comparable performance against standard fine-tuning. However, all these methods ignore the inefficiency problem caused by the task-specific output layers, which is inflexible for us to re-use PLMs and introduces non-negligible parameters. In this work, we focus on the text classification task and propose plugin-tuning, a framework that further improves the efficiency of existing parameter-efficient methods with a unified classifier. Specifically, we re-formulate both token and sentence classification tasks into a unified language modeling task, and map label spaces of different tasks into the same vocabulary space. In this way, we can directly re-use the language modeling heads of PLMs, avoiding introducing extra parameters for different tasks. We conduct experiments on six classification benchmarks. The experimental results show that plugin-tuning can achieve comparable performance against fine-tuned PLMs, while further saving around 50% parameters on top of other parameter-efficient methods.",
        "author": "Xin Zhou; Ruotian Ma; Yicheng Zou; Xuanting Chen; Tao Gui; Qi Zhang; Xuanjing Huang; Rui Xie; Wei Wu",
        "authorids": "/x/xin-zhou/; /r/ruotian-ma/; /y/yicheng-zou/; /x/xuanting-chen/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/; /r/rui-xie/; /w/wei-wu/",
        "bibtex": "@inproceedings{zhou-etal-2022-making,\n    title = \"Making Parameter-efficient Tuning More Efficient: A Unified Framework for Classification Tasks\",\n    author = \"Zhou, Xin  and\n      Ma, Ruotian  and\n      Zou, Yicheng  and\n      Chen, Xuanting  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing  and\n      Xie, Rui  and\n      Wu, Wei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.615/\",\n    pages = \"7053--7064\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.615.pdf",
        "site": "https://aclanthology.org/2022.coling-1.615/",
        "pdf_size": 631923,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18407923391445742320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2022.coling-1.430",
        "title": "MaxMatch-Dropout: Subword Regularization for WordPiece",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a subword regularization method for WordPiece, which uses a maximum matching algorithm for tokenization. The proposed method, MaxMatch-Dropout, randomly drops words in a search using the maximum matching algorithm. It realizes finetuning with subword regularization for popular pretrained language models such as BERT-base. The experimental results demonstrate that MaxMatch-Dropout improves the performance of text classification and machine translation tasks as well as other subword regularization methods. Moreover, we provide a comparative analysis of subword regularization methods: subword regularization with SentencePiece (Unigram), BPE-Dropout, and MaxMatch-Dropout.",
        "author": "Tatsuya Hiraoka",
        "authorids": "/t/tatsuya-hiraoka/",
        "bibtex": "@inproceedings{hiraoka-2022-maxmatch,\n    title = \"{M}ax{M}atch-Dropout: Subword Regularization for {W}ord{P}iece\",\n    author = \"Hiraoka, Tatsuya\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.430/\",\n    pages = \"4864--4872\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.430.pdf",
        "site": "https://aclanthology.org/2022.coling-1.430/",
        "pdf_size": 465101,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4406045695707199969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2022.coling-1.574",
        "title": "Measuring Geographic Performance Disparities of Offensive Language Classifiers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text classifiers are applied at scale in the form of one-size-fits-all solutions. Nevertheless, many studies show that classifiers are biased regarding different languages and dialects. When measuring and discovering these biases, some gaps present themselves and should be addressed. First, \u201cDoes language, dialect, and topical content vary across geographical regions?\u201d and secondly \u201cIf there are differences across the regions, do they impact model performance?\u201d. We introduce a novel dataset called GeoOLID with more than 14 thousand examples across 15 geographically and demographically diverse cities to address these questions. We perform a comprehensive analysis of geographical-related content and their impact on performance disparities of offensive language detection models. Overall, we find that current models do not generalize across locations. Likewise, we show that while offensive language models produce false positives on African American English, model performance is not correlated with each city\u2019s minority population proportions. Warning: This paper contains offensive language.",
        "author": "Brandon Lwowski; Paul Rad; Anthony Rios",
        "authorids": "/b/brandon-lwowski/; /p/paul-rad/; /a/anthony-rios/",
        "bibtex": "@inproceedings{lwowski-etal-2022-measuring,\n    title = \"Measuring Geographic Performance Disparities of Offensive Language Classifiers\",\n    author = \"Lwowski, Brandon  and\n      Rad, Paul  and\n      Rios, Anthony\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.574/\",\n    pages = \"6600--6616\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.574.pdf",
        "site": "https://aclanthology.org/2022.coling-1.574/",
        "pdf_size": 964809,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13334755642151434501&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Information Systems and Cyber Security; Department of Information Systems and Cyber Security + Department of Computer Science; Department of Information Systems and Cyber Security",
        "aff_domain": "utsa.edu;utsa.edu;utsa.edu",
        "email": "utsa.edu;utsa.edu;utsa.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "1",
        "aff_unique_norm": ";Unknown Institution",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.coling-1.5",
        "title": "Measuring Morphological Fusion Using Partial Information Decomposition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Morphological systems across languages vary when it comes to the relation between form and meaning. In some languages, a single meaning feature corresponds to a single morpheme, whereas in other languages, multiple meaning features are bundled together into one morpheme. The two types of languages have been called agglutinative and fusional, respectively, but this distinction does not capture the graded nature of the phenomenon. We provide a mathematically precise way of characterizing morphological systems using partial information decomposition, a framework for decomposing mutual information into three components: unique, redundant, and synergistic information. We show that highly fusional languages are characterized by high levels of synergy.",
        "author": "Michaela Socolof; Jacob Louis Hoover; Richard Futrell; Alessandro Sordoni; Timothy J. O\u2019Donnell",
        "authorids": "/m/michaela-socolof/; /j/jacob-hoover-vigly/; /r/richard-futrell/; /a/alessandro-sordoni/; /t/timothy-odonnell/",
        "bibtex": "@inproceedings{socolof-etal-2022-measuring,\n    title = \"Measuring Morphological Fusion Using Partial Information Decomposition\",\n    author = \"Socolof, Michaela  and\n      Hoover, Jacob Louis  and\n      Futrell, Richard  and\n      Sordoni, Alessandro  and\n      O{'}Donnell, Timothy J.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.5/\",\n    pages = \"44--54\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.5.pdf",
        "site": "https://aclanthology.org/2022.coling-1.5/",
        "pdf_size": 392310,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14926782940903062904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "McGill University/Mila; McGill University/Mila; University of California, Irvine; Microsoft Research; McGill University/Mila + Canada CIFAR AI Chair",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0+3",
        "aff_unique_norm": "McGill University;University of California, Irvine;Microsoft Corporation;Canadian Institute for Advanced Research",
        "aff_unique_dep": "Mila;;Microsoft Research;AI Chair",
        "aff_unique_url": "https://www.mcgill.ca;https://www.uci.edu;https://www.microsoft.com/en-us/research;https://www.cifar.ca",
        "aff_unique_abbr": "McGill;UCI;MSR;CIFAR",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;0;1;1;0+0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.coling-1.343",
        "title": "Measuring Robustness for NLP",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The quality of Natural Language Processing (NLP) models is typically measured by the accuracy or error rate of a predefined test set. Because the evaluation and optimization of these measures are narrowed down to a specific domain like news and cannot be generalized to other domains like Twitter, we often observe that a system reported with human parity results generates surprising errors in real-life use scenarios. We address this weakness with a new approach that uses an NLP quality measure based on robustness. Unlike previous work that has defined robustness using Minimax to bound worst cases, we measure robustness based on the consistency of cross-domain accuracy and introduce the coefficient of variation and (epsilon, gamma)-Robustness. Our measures demonstrate higher agreements with human evaluation than accuracy scores like BLEU on ranking Machine Translation (MT) systems. Our experiments of sentiment analysis and MT tasks show that incorporating our robustness measures into learning objectives significantly enhances the final NLP prediction accuracy over various domains, such as biomedical and social media.",
        "author": "Yu Yu; Abdul Rafae Khan; Jia Xu",
        "authorids": "/y/yu-yu/; /a/abdul-rafae-khan/; /j/jia-xu/",
        "bibtex": "@inproceedings{yu-etal-2022-measuring,\n    title = \"Measuring Robustness for {NLP}\",\n    author = \"Yu, Yu  and\n      Khan, Abdul Rafae  and\n      Xu, Jia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.343/\",\n    pages = \"3908--3916\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.343.pdf",
        "site": "https://aclanthology.org/2022.coling-1.343/",
        "pdf_size": 3592149,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16490643576107274628&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Engineering and Science, Steven Institute of Technology, NJ 07030, USA; School of Engineering and Science, Steven Institute of Technology, NJ 07030, USA; School of Engineering and Science, Steven Institute of Technology, NJ 07030, USA",
        "aff_domain": "stevens.edu;stevens.edu;stevens.edu",
        "email": "stevens.edu;stevens.edu;stevens.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Steven Institute of Technology",
        "aff_unique_dep": "School of Engineering and Science",
        "aff_unique_url": "https://www.stevens.edu",
        "aff_unique_abbr": "SIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "NJ",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.198",
        "title": "MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26% to 86%. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction.",
        "author": "Saadullah Amin; Pasquale Minervini; David Chang; Pontus Stenetorp; Guenter Neumann",
        "authorids": "/s/saadullah-amin/; /p/pasquale-minervini/; /d/david-chang/; /p/pontus-stenetorp/; /g/gunter-neumann/",
        "bibtex": "@inproceedings{amin-etal-2022-meddistant19,\n    title = \"{M}ed{D}istant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction\",\n    author = \"Amin, Saadullah  and\n      Minervini, Pasquale  and\n      Chang, David  and\n      Stenetorp, Pontus  and\n      Neumann, Guenter\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.198/\",\n    pages = \"2259--2277\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.198.pdf",
        "site": "https://aclanthology.org/2022.coling-1.198/",
        "pdf_size": 1708954,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7134430749526179655&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "German Research Center for Artificial Intelligence + Saarland Informatics Campus, Saarland University; UCL Centre for Artificial Intelligence; Yale Center for Medical Informatics; UCL Centre for Artificial Intelligence; German Research Center for Artificial Intelligence + Saarland Informatics Campus, Saarland University",
        "aff_domain": "dfki.de;cs.ucl.ac.uk;yale.edu;cs.ucl.ac.uk;dfki.de",
        "email": "dfki.de;cs.ucl.ac.uk;yale.edu;cs.ucl.ac.uk;dfki.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;3;2;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;Saarland University;University College London;Yale University",
        "aff_unique_dep": ";;Centre for Artificial Intelligence;Yale Center for Medical Informatics",
        "aff_unique_url": "https://www.dfki.de/;https://www.uni-saarland.de;https://www.ucl.ac.uk;https://medicine.yale.edu",
        "aff_unique_abbr": "DFKI;Uni Saar;UCL;Yale",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Saarland Informatics Campus",
        "aff_country_unique_index": "0+0;1;2;1;0+0",
        "aff_country_unique": "Germany;United Kingdom;United States"
    },
    {
        "id": "2022.coling-1.241",
        "title": "Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Current medical question answering systems have difficulty processing long, detailed and informally worded questions submitted by patients, called Consumer Health Questions (CHQs). To address this issue, we introduce a medical question understanding and answering system with knowledge grounding and semantic self-supervision. Our system is a pipeline that first summarizes a long, medical, user-written question, using a supervised summarization loss. Then, our system performs a two-step retrieval to return answers. The system first matches the summarized user question with an FAQ from a trusted medical knowledge base, and then retrieves a fixed number of relevant sentences from the corresponding answer document. In the absence of labels for question matching or answer relevance, we design 3 novel, self-supervised and semantically-guided losses. We evaluate our model against two strong retrieval-based question answering baselines. Evaluators ask their own questions and rate the answers retrieved by our baselines and own system according to their relevance. They find that our system retrieves more relevant answers, while achieving speeds 20 times faster. Our self-supervised losses also help the summarizer achieve higher scores in ROUGE, as well as in human evaluation metrics.",
        "author": "Khalil Mrini; Harpreet Singh; Franck Dernoncourt; Seunghyun Yoon; Trung Bui; Walter W. Chang; Emilia Farcas; Ndapa Nakashole",
        "authorids": "/k/khalil-mrini/; /h/harpreet-singh/; /f/franck-dernoncourt/; /s/seunghyun-yoon/; /t/trung-bui/; /w/walter-w-chang/; /e/emilia-farcas/; /n/ndapandula-nakashole/",
        "bibtex": "@inproceedings{mrini-etal-2022-medical,\n    title = \"Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision\",\n    author = \"Mrini, Khalil  and\n      Singh, Harpreet  and\n      Dernoncourt, Franck  and\n      Yoon, Seunghyun  and\n      Bui, Trung  and\n      Chang, Walter W.  and\n      Farcas, Emilia  and\n      Nakashole, Ndapa\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.241/\",\n    pages = \"2734--2747\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.241.pdf",
        "site": "https://aclanthology.org/2022.coling-1.241/",
        "pdf_size": 932439,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1885324691882548535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, San Diego; University of California, San Diego; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California, San Diego; University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu;ucsd.edu",
        "github": "https://github.com/KhalilMrini/Medical-Question-Answering",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;1;1;1;0;0",
        "aff_unique_norm": "University of California, San Diego;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCSD;Adobe",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.620",
        "title": "Mere Contrastive Learning for Cross-Domain Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-domain sentiment analysis aims to predict the sentiment of texts in the target domain using the model trained on the source domain to cope with the scarcity of labeled data. Previous studies are mostly cross-entropy-based methods for the task, which suffer from instability and poor generalization. In this paper, we explore contrastive learning on the cross-domain sentiment analysis task. We propose a modified contrastive objective with in-batch negative samples so that the sentence representations from the same class can be pushed close while those from the different classes become further apart in the latent space. Experiments on two widely used datasets show that our model can achieve state-of-the-art performance in both cross-domain and multi-domain sentiment analysis tasks. Meanwhile, visualizations demonstrate the effectiveness of transferring knowledge learned in the source domain to the target domain and the adversarial test verifies the robustness of our model.",
        "author": "Yun Luo; Fang Guo; Zihan Liu; Yue Zhang",
        "authorids": "/y/yun-luo/; /f/fang-guo/; /z/zihan-liu/; /y/yue-zhang/",
        "bibtex": "@inproceedings{luo-etal-2022-mere,\n    title = \"Mere Contrastive Learning for Cross-Domain Sentiment Analysis\",\n    author = \"Luo, Yun  and\n      Guo, Fang  and\n      Liu, Zihan  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.620/\",\n    pages = \"7099--7111\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.620.pdf",
        "site": "https://aclanthology.org/2022.coling-1.620/",
        "pdf_size": 1737693,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11670461449093497421&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.533",
        "title": "Meta-CQG: A Meta-Learning Framework for Complex Question Generation over Knowledge Bases",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Complex question generation over knowledge bases (KB) aims to generate natural language questions involving multiple KB relations or functional constraints. Existing methods train one encoder-decoder-based model to fit all questions. However, such a one-size-fits-all strategy may not perform well since complex questions exhibit an uneven distribution in many dimensions, such as question types, involved KB relations, and query structures, resulting in insufficient learning for long-tailed samples under different dimensions. To address this problem, we propose a meta-learning framework for complex question generation. The meta-trained generator can acquire universal and transferable meta-knowledge and quickly adapt to long-tailed samples through a few most related training samples. To retrieve similar samples for each input query, we design a self-supervised graph retriever to learn distributed representations for samples, and contrastive learning is leveraged to improve the learned representations. We conduct experiments on both WebQuestionsSP and ComplexWebQuestion, and results on long-tailed samples of different dimensions have been significantly improved, which demonstrates the effectiveness of the proposed framework.",
        "author": "Kun Zhang; Yunqi Qiu; Yuanzhuo Wang; Long Bai; Wei Li; Xuhui Jiang; Huawei Shen; Xueqi Cheng",
        "authorids": "/k/kun-zhang-ucas/; /y/yunqi-qiu/; /y/yuanzhuo-wang/; /l/long-bai/; /w/wei-li/; /x/xuhui-jiang/; /h/huawei-shen/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{zhang-etal-2022-meta,\n    title = \"Meta-{CQG}: A Meta-Learning Framework for Complex Question Generation over Knowledge Bases\",\n    author = \"Zhang, Kun  and\n      Qiu, Yunqi  and\n      Wang, Yuanzhuo  and\n      Bai, Long  and\n      Li, Wei  and\n      Jiang, Xuhui  and\n      Shen, Huawei  and\n      Cheng, Xueqi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.533/\",\n    pages = \"6105--6114\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.533.pdf",
        "site": "https://aclanthology.org/2022.coling-1.533/",
        "pdf_size": 828540,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1374457063957601423&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2022.coling-1.287",
        "title": "MetaPrompting: Learning to Learn Better Prompts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prompting method is regarded as one of the crucial progress for few-shot nature language processing. Recent research on prompting moves from discrete tokens based \u201chard prompts\u201d to continuous \u201csoft prompts\u201d, which employ learnable vectors as pseudo prompt tokens and achieve better performance. Though showing promising prospects, these soft-prompting methods are observed to rely heavily on good initialization to take effect. Unfortunately, obtaining a perfect initialization for soft prompts requires understanding of inner language models working and elaborate design, which is no easy task and has to restart from scratch for each new task. To remedy this, we propose a generalized soft prompting method called MetaPrompting, which adopts the well-recognized model-agnostic meta-learning algorithm to automatically find better prompt initialization that facilitates fast adaptation to new prompting tasks. Extensive experiments show MetaPrompting tackles soft prompt initialization problem and brings significant improvement on three different datasets (over 7 points improvement in accuracy for 1-shot setting), achieving new state-of-the-art performance.",
        "author": "Yutai Hou; Hongyuan Dong; Xinghao Wang; Bohan Li; Wanxiang Che",
        "authorids": "/y/yutai-hou/; /h/hongyuan-dong/; /x/xinghao-wang/; /b/bohan-li/; /w/wanxiang-che/",
        "bibtex": "@inproceedings{hou-etal-2022-metaprompting,\n    title = \"{M}eta{P}rompting: Learning to Learn Better Prompts\",\n    author = \"Hou, Yutai  and\n      Dong, Hongyuan  and\n      Wang, Xinghao  and\n      Li, Bohan  and\n      Che, Wanxiang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.287/\",\n    pages = \"3251--3262\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.287.pdf",
        "site": "https://aclanthology.org/2022.coling-1.287/",
        "pdf_size": 448171,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1584214125780915071&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Research Center for Social Computing and Information Retrieval; Research Center for Social Computing and Information Retrieval; Research Center for Social Computing and Information Retrieval; Research Center for Social Computing and Information Retrieval; Research Center for Social Computing and Information Retrieval",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Research Center for Social Computing and Information Retrieval",
        "aff_unique_dep": "Research Center",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.180",
        "title": "MetaSLRCL: A Self-Adaptive Learning Rate and Curriculum Learning Based Framework for Few-Shot Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Due to the lack of labeled data in many realistic scenarios, a number of few-shot learning methods for text classification have been proposed, among which the meta learning based ones have recently attracted much attention. Such methods usually consist of a learner as the classifier and a meta learner for specializing the learner to different tasks. For the learner, learning rate is crucial to its performance. However, existing methods treat it as a hyper parameter and adjust it manually, which is time-consuming and laborious. Intuitively, for different tasks and neural network layers, the learning rates should be different and self-adaptive. For the meta learner, it requires a good generalization ability so as to quickly adapt to new tasks. Motivated by these issues, we propose a novel meta learning framework, called MetaSLRCL, for few-shot text classification. Specifically, we present a novel meta learning mechanism to obtain different learning rates for different tasks and neural network layers so as to enable the learner to quickly adapt to new training data. Moreover, we propose a task-oriented curriculum learning mechanism to help the meta learner achieve a better generalization ability by learning from different tasks with increasing difficulties. Extensive experiments on three benchmark datasets demonstrate the effectiveness of MetaSLRCL.",
        "author": "Kailin Zhao; Xiaolong Jin; Saiping Guan; Jiafeng Guo; Xueqi Cheng",
        "authorids": "/k/kailin-zhao/; /x/xiaolong-jin/; /s/saiping-guan/; /j/jiafeng-guo/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{zhao-etal-2022-metaslrcl,\n    title = \"{M}eta{SLRCL}: A Self-Adaptive Learning Rate and Curriculum Learning Based Framework for Few-Shot Text Classification\",\n    author = \"Zhao, Kailin  and\n      Jin, Xiaolong  and\n      Guan, Saiping  and\n      Guo, Jiafeng  and\n      Cheng, Xueqi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.180/\",\n    pages = \"2065--2074\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.180.pdf",
        "site": "https://aclanthology.org/2022.coling-1.180/",
        "pdf_size": 979056,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11020902367847294515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2022.coling-1.364",
        "title": "Metaphor Detection via Linguistics Enhanced Siamese Network",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper we present MisNet, a novel model for word level metaphor detection. MisNet converts two linguistic rules, i.e., Metaphor Identification Procedure (MIP) and Selectional Preference Violation (SPV) into semantic matching tasks. MIP module computes the similarity between the contextual meaning and the basic meaning of a target word. SPV module perceives the incongruity between target words and their contexts. To better represent basic meanings, MisNet utilizes dictionary resources. Empirical results indicate that MisNet achieves competitive performance on several datasets.",
        "author": "Shenglong Zhang; Ying Liu",
        "authorids": "/s/shenglong-zhang/; /y/ying-liu/",
        "bibtex": "@inproceedings{zhang-liu-2022-metaphor,\n    title = \"Metaphor Detection via Linguistics Enhanced {S}iamese Network\",\n    author = \"Zhang, Shenglong  and\n      Liu, Ying\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.364/\",\n    pages = \"4149--4159\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.364.pdf",
        "site": "https://aclanthology.org/2022.coling-1.364/",
        "pdf_size": 532151,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11436097668136530010&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Tsinghua University, Beijing, China, 100084; Tsinghua University, Beijing, China, 100084",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.7",
        "title": "Metaphorical Polysemy Detection: Conventional Metaphor Meets Word Sense Disambiguation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Linguists distinguish between novel and conventional metaphor, a distinction which the metaphor detection task in NLP does not take into account. Instead, metaphoricity is formulated as a property of a token in a sentence, regardless of metaphor type. In this paper, we investigate the limitations of treating conventional metaphors in this way, and advocate for an alternative which we name \u2018metaphorical polysemy detection\u2019 (MPD). In MPD, only conventional metaphoricity is treated, and it is formulated as a property of word senses in a lexicon. We develop the first MPD model, which learns to identify conventional metaphors in the English WordNet. To train it, we present a novel training procedure that combines metaphor detection with \u2018word sense disambiguation\u2019 (WSD). For evaluation, we manually annotate metaphor in two subsets of WordNet. Our model significantly outperforms a strong baseline based on a state-of-the-art metaphor detection model, attaining an ROC-AUC score of .78 (compared to .65) on one of the sets. Additionally, when paired with a WSD model, our approach outperforms a state-of-the-art metaphor detection model at identifying conventional metaphors in text (.659 F1 compared to .626).",
        "author": "Rowan Hall Maudslay; Simone Teufel",
        "authorids": "/r/rowan-hall-maudslay/; /s/simone-teufel/",
        "bibtex": "@inproceedings{maudslay-teufel-2022-metaphorical,\n    title = \"Metaphorical Polysemy Detection: Conventional Metaphor Meets Word Sense Disambiguation\",\n    author = \"Maudslay, Rowan Hall  and\n      Teufel, Simone\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.7/\",\n    pages = \"65--77\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.7.pdf",
        "site": "https://aclanthology.org/2022.coling-1.7/",
        "pdf_size": 530534,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14853300021747574963&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Dept. of Computer Science & Technology+Magdalene College, University of Cambridge; Dept. of Computer Science & Technology, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "University of Bedfordshire;University of Cambridge",
        "aff_unique_dep": "Department of Computer Science & Technology;",
        "aff_unique_url": "https://www.beds.ac.uk/;https://www.cam.ac.uk",
        "aff_unique_abbr": ";Cambridge",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.207",
        "title": "Method Entity Extraction from Biomedical Texts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the field of Natural Language Processing (NLP), extracting method entities from biomedical text has been a challenging task. Scientific research papers commonly consist of complex keywords and domain-specific terminologies, and new terminologies are continuously appearing. In this research, we find method terminologies in biomedical text using both rule-based and machine learning techniques. We first use linguistic features to extract method sentence candidates from a large corpus of biomedical text. Then, we construct a silver standard biomedical corpus composed of these sentences. With a rule-based method that makes use of the Stanza dependency parsing module, we label the method entities in these sentences. Using this silver standard corpus we train two machine learning algorithms to automatically extract method entities from biomedical text. Our results show that it is possible to develop machine learning models that can automatically extract method entities to a reasonable accuracy without the need for a gold standard dataset.",
        "author": "Waqar Bin Kalim; Robert E. Mercer",
        "authorids": "/w/waqar-bin-kalim/; /r/robert-e-mercer/",
        "bibtex": "@inproceedings{kalim-mercer-2022-method,\n    title = \"Method Entity Extraction from Biomedical Texts\",\n    author = \"Kalim, Waqar Bin  and\n      Mercer, Robert E.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.207/\",\n    pages = \"2357--2362\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.207.pdf",
        "site": "https://aclanthology.org/2022.coling-1.207/",
        "pdf_size": 150799,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8919228299427399058&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, The University of Western Ontario, London, Ontario, Canada; Department of Computer Science, The University of Western Ontario, London, Ontario, Canada",
        "aff_domain": "gmail.com;csd.uwo.ca",
        "email": "gmail.com;csd.uwo.ca",
        "github": "https://github.com/waqarkalim/method-mention-extraction-from-biomedical-text",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The University of Western Ontario",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uwo.ca",
        "aff_unique_abbr": "UWO",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.548",
        "title": "Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we propose to leverage the unique characteristics of dialogues sharing commonsense knowledge across participants, to resolve the difficulties in summarizing them. We present SICK, a framework that uses commonsense inferences as additional context. Compared to previous work that solely relies on the input dialogue, SICK uses an external knowledge model to generate a rich set of commonsense inferences and selects the most probable one with a similarity-based selection method. Built upon SICK, SICK++ utilizes commonsense as supervision, where the task of generating commonsense inferences is added upon summarizing the dialogue in a multi-task learning setting. Experimental results show that with injected commonsense knowledge, our framework generates more informative and consistent summaries than existing methods.",
        "author": "Seungone Kim; Se June Joo; Hyungjoo Chae; Chaehyeong Kim; Seung-won Hwang; Jinyoung Yeo",
        "authorids": "/s/seungone-kim/; /s/se-june-joo/; /h/hyungjoo-chae/; /c/chaehyeong-kim/; /s/seung-won-hwang/; /j/jinyoung-yeo/",
        "bibtex": "@inproceedings{kim-etal-2022-mind,\n    title = \"Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization\",\n    author = \"Kim, Seungone  and\n      Joo, Se June  and\n      Chae, Hyungjoo  and\n      Kim, Chaehyeong  and\n      Hwang, Seung-won  and\n      Yeo, Jinyoung\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.548/\",\n    pages = \"6285--6300\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.548.pdf",
        "site": "https://aclanthology.org/2022.coling-1.548/",
        "pdf_size": 1050097,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12637455326864144273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Yonsei University; Yonsei University; Yonsei University+Tutoring at Market Designers; Yonsei University; Seoul National University; Yonsei University+Tutoring at Market Designers",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;snu.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;snu.ac.kr;yonsei.ac.kr",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0+1;0;2;0+1",
        "aff_unique_norm": "Yonsei University;Market Designers;Seoul National University",
        "aff_unique_dep": ";Tutoring;",
        "aff_unique_url": "https://www.yonsei.ac.kr;;https://www.snu.ac.kr",
        "aff_unique_abbr": "Yonsei;;SNU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2022.coling-1.167",
        "title": "Mining Health-related Cause-Effect Statements with High Precision at Large Scale",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "An efficient assessment of the health relatedness of text passages is important to mine the web at scale to conduct health sociological analyses or to develop a health search engine. We propose a new efficient and effective termhood score for predicting the health relatedness of phrases and sentences, which achieves 69% recall at over 90% precision on a web dataset with cause-effect statements. It is more effective than state-of-the-art medical entity linkers and as effective but much faster than BERT-based approaches. Using our method, we compile the Webis Medical CauseNet 2022, a new resource of 7.8 million health-related cause-effect statements such as \u201cStudies show that stress induces insomnia\u201d in which the cause (\u2018stress\u2019) and effect (\u2018insomnia\u2019) are labeled.",
        "author": "Ferdinand Schlatt; Dieter Bettin; Matthias Hagen; Benno Stein; Martin Potthast",
        "authorids": "/f/ferdinand-schlatt/; /d/dieter-bettin/; /m/matthias-hagen/; /b/benno-stein/; /m/martin-potthast/",
        "bibtex": "@inproceedings{schlatt-etal-2022-mining,\n    title = \"Mining Health-related Cause-Effect Statements with High Precision at Large Scale\",\n    author = \"Schlatt, Ferdinand  and\n      Bettin, Dieter  and\n      Hagen, Matthias  and\n      Stein, Benno  and\n      Potthast, Martin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.167/\",\n    pages = \"1925--1936\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.167.pdf",
        "site": "https://aclanthology.org/2022.coling-1.167/",
        "pdf_size": 444832,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=955622141558190381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Martin-Luther-Universit\u00e4t Halle-Wittenberg; Westf\u00e4lische Wilhelms-Universit\u00e4t M\u00fcnster; Martin-Luther-Universit\u00e4t Halle-Wittenberg; Bauhaus-Universit\u00e4t Weimar; Universit\u00e4t Leipzig",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/webis-de/COLING-22",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;3",
        "aff_unique_norm": "Martin-Luther-Universit\u00e4t;Westf\u00e4lische Wilhelms-Universit\u00e4t M\u00fcnster;Bauhaus-Universit\u00e4t Weimar;University of Leipzig",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uni-halle.de;https://www.uni-muenster.de;https://www.bauhaus-university.de;https://www.uni-leipzig.de",
        "aff_unique_abbr": "MLU;WWU;Bauhaus-Uni Weimar;Uni Leipzig",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Halle-Wittenberg;;Weimar",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.138",
        "title": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions, including superlative, intersection, and multi-hop questions, which were naturally elicited from crowd workers. We run baselines over Mintaka, the best of which achieves 38% hits@1 in English and 31% hits@1 multilingually, showing that existing models have room for improvement. We release Mintaka at https://github.com/amazon-research/mintaka.",
        "author": "Priyanka Sen; Alham Fikri Aji; Amir Saffari",
        "authorids": "/p/priyanka-sen/; /a/alham-fikri-aji/; /a/amir-saffari/",
        "bibtex": "@inproceedings{sen-etal-2022-mintaka,\n    title = \"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering\",\n    author = \"Sen, Priyanka  and\n      Aji, Alham Fikri  and\n      Saffari, Amir\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.138/\",\n    pages = \"1604--1619\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.138.pdf",
        "site": "https://aclanthology.org/2022.coling-1.138/",
        "pdf_size": 615123,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2968638957943930508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon Alexa AI, Cambridge, UK; Amazon Alexa AI, Cambridge, UK; Amazon Alexa AI, Cambridge, UK",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "https://github.com/amazon-research/mintaka",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Alexa AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.403",
        "title": "Mitigating the Diminishing Effect of Elastic Weight Consolidation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Elastic weight consolidation (EWC, Kirkpatrick et al. 2017) is a promising approach to addressing catastrophic forgetting in sequential training. We find that the effect of EWC can diminish when fine-tuning large-scale pre-trained language models on different datasets. We present two simple objective functions to mitigate this problem by rescaling the components of EWC. Experiments on natural language inference and fact-checking tasks indicate that our methods require much smaller values for the trade-off parameters to achieve results comparable to EWC.",
        "author": "Canasai Kruengkrai; Junichi Yamagishi",
        "authorids": "/c/canasai-kruengkrai/; /j/junichi-yamagishi/",
        "bibtex": "@inproceedings{kruengkrai-yamagishi-2022-mitigating,\n    title = \"Mitigating the Diminishing Effect of Elastic Weight Consolidation\",\n    author = \"Kruengkrai, Canasai  and\n      Yamagishi, Junichi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.403/\",\n    pages = \"4568--4574\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.403.pdf",
        "site": "https://aclanthology.org/2022.coling-1.403/",
        "pdf_size": 466417,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=334517920846677435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "National Institute of Informatics, Japan; National Institute of Informatics, Japan",
        "aff_domain": "nii.ac.jp;nii.ac.jp",
        "email": "nii.ac.jp;nii.ac.jp",
        "github": "https://github.com/nii-yamagishilab/ewc",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Institute of Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nii.ac.jp",
        "aff_unique_abbr": "NII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.411",
        "title": "MockingBERT: A Method for Retroactively Adding Resilience to NLP Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Protecting NLP models against misspellings whether accidental or adversarial has been the object of research interest for the past few years. Existing remediations have typically either compromised accuracy or required full model re-training with each new class of attacks. We propose a novel method of retroactively adding resilience to misspellings to transformer-based NLP models. This robustness can be achieved without the need for re-training of the original NLP model and with only a minimal loss of language understanding performance on inputs without misspellings. Additionally we propose a new efficient approximate method of generating adversarial misspellings, which significantly reduces the cost needed to evaluate a model\u2019s resilience to adversarial attacks.",
        "author": "Jan Jezabek; Akash Singh",
        "authorids": "/j/jan-jezabek/; /a/akash-singh/",
        "bibtex": "@inproceedings{jezabek-singh-2022-mockingbert,\n    title = \"{M}ocking{BERT}: A Method for Retroactively Adding Resilience to {NLP} Models\",\n    author = \"Jezabek, Jan  and\n      Singh, Akash\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.411/\",\n    pages = \"4640--4650\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.411.pdf",
        "site": "https://aclanthology.org/2022.coling-1.411/",
        "pdf_size": 316646,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5085365790604766707&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Hedgefrog Software LLC; Salesforce Inc.",
        "aff_domain": "hedgefrogsoft.com;salesforce.com",
        "email": "hedgefrogsoft.com;salesforce.com",
        "github": "https://github.com/akash13singh/resilient_nlp/",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Hedgefrog Software LLC;Salesforce",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.salesforce.com",
        "aff_unique_abbr": ";Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.599",
        "title": "Modeling Aspect Correlation for Aspect-based Sentiment Analysis via Recurrent Inverse Learning Guidance",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) aims to distinguish sentiment polarity of every specific aspect in a given sentence. Previous researches have realized the importance of interactive learning with context and aspects. However, these methods are ill-studied to learn complex sentence with multiple aspects due to overlapped polarity feature. And they do not consider the correlation between aspects to distinguish overlapped feature. In order to solve this problem, we propose a new method called Recurrent Inverse Learning Guided Network (RILGNet). Our RILGNet has two points to improve the modeling of aspect correlation and the selecting of aspect feature. First, we use Recurrent Mechanism to improve the joint representation of aspects, which enhances the aspect correlation modeling iteratively. Second, we propose Inverse Learning Guidance to improve the selection of aspect feature by considering aspect correlation, which provides more useful information to determine polarity. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of RILGNet, and we further prove that RILGNet is state-of-the-art method in multiaspect scenarios.",
        "author": "Longfeng Li; Haifeng Sun; Qi Qi; Jingyu Wang; Jing Wang; Jianxin Liao",
        "authorids": "/l/longfeng-li/; /h/haifeng-sun/; /q/qi-qi/; /j/jingyu-wang/; /j/jing-wang/; /j/jianxin-liao/",
        "bibtex": "https://aclanthology.org/2022.coling-1.599.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.599.pdf",
        "site": "https://aclanthology.org/2022.coling-1.599/",
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2273952773684142515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.126",
        "title": "Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Machine reading comprehension (MRC) poses new challenges to logical reasoning, which aims to understand the implicit logical relations entailed in the given contexts and perform inference over them. Due to the complexity of logic, logical connections exist at different granularity levels. However, most existing methods of logical reasoning individually focus on either entity-aware or discourse-based information but ignore the hierarchical relations that may even have mutual effects. This paper proposes a holistic graph network (HGN) that deals with context at both discourse-level and word-level as the basis for logical reasoning to provide a more fine-grained relation extraction. Specifically, node-level and type-level relations, which can be interpreted as bridges in the reasoning process, are modeled by a hierarchical interaction mechanism to improve the interpretation of MRC systems. Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and natural language inference datasets (SNLI and ANLI) show the effectiveness and generalization of our method, and in-depth analysis verifies its capability to understand complex logical relations.",
        "author": "Jialin Chen; Zhuosheng Zhang; Hai Zhao",
        "authorids": "/j/jialin-chen/; /z/zhuosheng-zhang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{chen-etal-2022-modeling-hierarchical,\n    title = \"Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension\",\n    author = \"Chen, Jialin  and\n      Zhang, Zhuosheng  and\n      Zhao, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.126/\",\n    pages = \"1467--1479\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.126.pdf",
        "site": "https://aclanthology.org/2022.coling-1.126/",
        "pdf_size": 929727,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15718930597732561667&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Mathematical Sciences, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "School of Mathematical Sciences",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.622",
        "title": "Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The existing research efforts in Multimodal Sentiment Analysis (MSA) have focused on developing the expressive ability of neural networks to fuse information from different modalities. However, these approaches lack a mechanism to understand the complex relations within and across different modalities, since some sentiments may be scattered in different modalities. To this end, in this paper, we propose a novel hierarchical graph contrastive learning (HGraph-CL) framework for MSA, aiming to explore the intricate relations of intra- and inter-modal representations for sentiment extraction. Specifically, regarding the intra-modal level, we build a unimodal graph for each modality representation to account for the modality-specific sentiment implications. Based on it, a graph contrastive learning strategy is adopted to explore the potential relations based on unimodal graph augmentations. Furthermore, we construct a multimodal graph of each instance based on the unimodal graphs to grasp the sentiment relations between different modalities. Then, in light of the multimodal augmentation graphs, a graph contrastive learning strategy over the inter-modal level is proposed to ulteriorly seek the possible graph structures for precisely learning sentiment relations. This essentially allows the framework to understand the appropriate graph structures for learning intricate relations among different modalities. Experimental results on two benchmark datasets show that the proposed framework outperforms the state-of-the-art baselines in MSA.",
        "author": "Zijie Lin; Bin Liang; Yunfei Long; Yixue Dang; Min Yang; Min Zhang; Ruifeng Xu",
        "authorids": "/z/zijie-lin/; /b/bin-liang/; /y/yunfei-long/; /y/yixue-dang/; /m/min-yang/; /m/min-zhang/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{lin-etal-2022-modeling,\n    title = \"Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis\",\n    author = \"Lin, Zijie  and\n      Liang, Bin  and\n      Long, Yunfei  and\n      Dang, Yixue  and\n      Yang, Min  and\n      Zhang, Min  and\n      Xu, Ruifeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.622/\",\n    pages = \"7124--7135\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.622.pdf",
        "site": "https://aclanthology.org/2022.coling-1.622/",
        "pdf_size": 1027787,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14543992426108359693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.349",
        "title": "Modelling Commonsense Properties Using Pre-Trained Bi-Encoders",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Grasping the commonsense properties of everyday concepts is an important prerequisite to language understanding. While contextualised language models are reportedly capable of predicting such commonsense properties with human-level accuracy, we argue that such results have been inflated because of the high similarity between training and test concepts. This means that models which capture concept similarity can perform well, even if they do not capture any knowledge of the commonsense properties themselves. In settings where there is no overlap between the properties that are considered during training and testing, we find that the empirical performance of standard language models drops dramatically. To address this, we study the possibility of fine-tuning language models to explicitly model concepts and their properties. In particular, we train separate concept and property encoders on two types of readily available data: extracted hyponym-hypernym pairs and generic sentences. Our experimental results show that the resulting encoders allow us to predict commonsense properties with much higher accuracy than is possible by directly fine-tuning language models. We also present experimental results for the related task of unsupervised hypernym discovery.",
        "author": "Amit Gajbhiye; Luis Espinosa-Anke; Steven Schockaert",
        "authorids": "/a/amit-gajbhiye/; /l/luis-espinosa-anke/; /s/steven-schockaert/",
        "bibtex": "@inproceedings{gajbhiye-etal-2022-modelling,\n    title = \"Modelling Commonsense Properties Using Pre-Trained Bi-Encoders\",\n    author = \"Gajbhiye, Amit  and\n      Espinosa-Anke, Luis  and\n      Schockaert, Steven\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.349/\",\n    pages = \"3971--3983\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.349.pdf",
        "site": "https://aclanthology.org/2022.coling-1.349/",
        "pdf_size": 353969,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14843418660242128815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "CardiffNLP, Cardiff University, United Kingdom; CardiffNLP, Cardiff University, United Kingdom + AMPLYFI, United Kingdom; CardiffNLP, Cardiff University, United Kingdom",
        "aff_domain": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "email": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "github": "https://github.com/amitgajbhiye/biencoder_concept_propertytask",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Cardiff University;AMPLYFI",
        "aff_unique_dep": "CardiffNLP;",
        "aff_unique_url": "https://www.cardiff.ac.uk;",
        "aff_unique_abbr": "Cardiff;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cardiff;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.353",
        "title": "Modelling Frequency, Attestation, and Corpus-Based Information with OntoLex-FrAC",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "OntoLex-Lemon has become a de facto standard for lexical resources in the web of data. This paper provides the first overall description of the emerging OntoLex module for Frequency, Attestations, and Corpus-Based Information (OntoLex-FrAC) that is intended to complement OntoLex-Lemon with the necessary vocabulary to represent major types of information found in or automatically derived from corpora, for applications in both language technology and the language sciences.",
        "author": "Christian Chiarcos; Elena-Simona Apostol; Besim Kabashi; Ciprian-Octavian Truic\u0103",
        "authorids": "/c/christian-chiarcos/; /e/elena-simona-apostol/; /b/besim-kabashi/; /c/ciprian-octavian-truica/",
        "bibtex": "@inproceedings{chiarcos-etal-2022-modelling-frequency,\n    title = \"Modelling Frequency, Attestation, and Corpus-Based Information with {O}nto{L}ex-{F}r{AC}\",\n    author = \"Chiarcos, Christian  and\n      Apostol, Elena-Simona  and\n      Kabashi, Besim  and\n      Truic{\\u{a}}, Ciprian-Octavian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.353/\",\n    pages = \"4018--4027\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.353.pdf",
        "site": "https://aclanthology.org/2022.coling-1.353/",
        "pdf_size": 376688,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10350282855270819807&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Applied Computational Linguistics, Goethe University Frankfurt, Germany+Institute for Digital Humanities, University of Cologne, Germany; Department of Information Technology, Uppsala University, Sweden+Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Romania; Computational and Corpus Linguistics, University of Erlangen-Nuremberg, Germany; Department of Information Technology, Uppsala University, Sweden+Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Romania",
        "aff_domain": "cs.uni-frankfurt.de;it.uu.se;fau.de;it.uu.se",
        "email": "cs.uni-frankfurt.de;it.uu.se;fau.de;it.uu.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2+3;4;2+3",
        "aff_unique_norm": "Goethe University Frankfurt;University of Cologne;Uppsala University;University Politehnica of Bucharest;University of Erlangen-Nuremberg",
        "aff_unique_dep": "Applied Computational Linguistics;Institute for Digital Humanities;Department of Information Technology;Faculty of Automatic Control and Computers;Computational and Corpus Linguistics",
        "aff_unique_url": "https://www.uni-frankfurt.de;https://www.uni-koeln.de/;https://www.uu.se;https://www.upb.ro;https://www.uni-erlangen.de/",
        "aff_unique_abbr": "Goethe U Frankfurt;;UU;;",
        "aff_campus_unique_index": "0;;",
        "aff_campus_unique": "Frankfurt;",
        "aff_country_unique_index": "0+0;1+2;0;1+2",
        "aff_country_unique": "Germany;Sweden;Romania"
    },
    {
        "id": "2022.coling-1.309",
        "title": "MonoByte: A Pool of Monolingual Byte-level Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The zero-shot cross-lingual ability of models pretrained on multilingual and even monolingual corpora has spurred many hypotheses to explain this intriguing empirical result. However, due to the costs of pretraining, most research uses public models whose pretraining methodology, such as the choice of tokenization, corpus size, and computational budget, might differ drastically. When researchers pretrain their own models, they often do so under a constrained budget, and the resulting models might underperform significantly compared to SOTA models. These experimental differences led to various inconsistent conclusions about the nature of the cross-lingual ability of these models. To help further research on the topic, we released 10 monolingual byte-level models rigorously pretrained under the same configuration with a large compute budget (equivalent to 420 days on a V100) and corpora that are 4 times larger than the original BERT\u2019s. Because they are tokenizer-free, the problem of unseen token embeddings is eliminated, thus allowing researchers to try a wider range of cross-lingual experiments in languages with different scripts. Additionally, we release two models pretrained on non-natural language texts that can be used in sanity-check experiments. Experiments on QA and NLI tasks show that our monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen our understanding of cross-lingual transferability in language models.",
        "author": "Hugo Abonizio; Leandro Rodrigues de Souza; Roberto Lotufo; Rodrigo Nogueira",
        "authorids": "/h/hugo-abonizio/; /l/leandro-rodrigues-de-souza/; /r/roberto-lotufo/; /r/rodrigo-nogueira/",
        "bibtex": "@inproceedings{abonizio-etal-2022-monobyte,\n    title = \"{M}ono{B}yte: A Pool of Monolingual Byte-level Language Models\",\n    author = \"Abonizio, Hugo  and\n      de Souza, Leandro Rodrigues  and\n      Lotufo, Roberto  and\n      Nogueira, Rodrigo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.309/\",\n    pages = \"3506--3513\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.309.pdf",
        "site": "https://aclanthology.org/2022.coling-1.309/",
        "pdf_size": 233866,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15614691928581733055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "FEEC, UNICAMP, Brazil+NeuralMind, Brazil; FEEC, UNICAMP, Brazil; FEEC, UNICAMP, Brazil+NeuralMind, Brazil; FEEC, UNICAMP, Brazil+NeuralMind, Brazil",
        "aff_domain": "gmail.com;g.unicamp.br;unicamp.br;unicamp.br",
        "email": "gmail.com;g.unicamp.br;unicamp.br;unicamp.br",
        "github": "",
        "project": "https://huggingface.co/monobyte",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0+1;0+1",
        "aff_unique_norm": "Universidade de Campinas;NeuralMind",
        "aff_unique_dep": "Faculdade de Engenharia El\u00e9trica e de Computa\u00e7\u00e3o;",
        "aff_unique_url": "https://www.unicamp.br;",
        "aff_unique_abbr": "UNICAMP;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Campinas;",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "2022.coling-1.612",
        "title": "MuCDN: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As an emerging research topic in natural language processing community, emotion recognition in multi-party conversations has attained increasing interest. Previous approaches that focus either on dyadic or multi-party scenarios exert much effort to cope with the challenge of emotional dynamics and achieve appealing results. However, since emotional interactions among speakers are often more complicated within the entangled multi-party conversations, these works are limited in capturing effective emotional clues in conversational context. In this work, we propose Mutual Conversational Detachment Network (MuCDN) to clearly and effectively understand the conversational context by separating conversations into detached threads. Specifically, two detachment ways are devised to perform context and speaker-specific modeling within detached threads and they are bridged through a mutual module. Experimental results on two datasets show that our model achieves better performance over the baseline models.",
        "author": "Weixiang Zhao; Yanyan Zhao; Bing Qin",
        "authorids": "/w/weixiang-zhao/; /y/yanyan-zhao/; /b/bing-qin/",
        "bibtex": "@inproceedings{zhao-etal-2022-mucdn,\n    title = \"{M}u{CDN}: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations\",\n    author = \"Zhao, Weixiang  and\n      Zhao, Yanyan  and\n      Qin, Bing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.612/\",\n    pages = \"7020--7030\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.612.pdf",
        "site": "https://aclanthology.org/2022.coling-1.612/",
        "pdf_size": 490830,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2191294587427438457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.96",
        "title": "MuSeCLIR: A Multiple Senses and Cross-lingual Information Retrieval Dataset",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper addresses a deficiency in existing cross-lingual information retrieval (CLIR) datasets and provides a robust evaluation of CLIR systems\u2019 disambiguation ability. CLIR is commonly tackled by combining translation and traditional IR. Due to translation ambiguity, the problem of ambiguity is worse in CLIR than in monolingual IR. But existing auto-generated CLIR datasets are dominated by searches for named entity mentions, which does not provide a good measure for disambiguation performance, as named entity mentions can often be transliterated across languages and tend not to have multiple translations. Therefore, we introduce a new evaluation dataset (MuSeCLIR) to address this inadequacy. The dataset focusses on polysemous common nouns with multiple possible translations. MuSeCLIR is constructed from multilingual Wikipedia and supports searches on documents written in European (French, German, Italian) and Asian (Chinese, Japanese) languages. We provide baseline statistical and neural model results on MuSeCLIR which show that MuSeCLIR has a higher requirement on the ability of systems to disambiguate query terms.",
        "author": "Wing Yan Li; Julie Weeds; David Weir",
        "authorids": "/w/wing-yan-li/; /j/julie-weeds/; /d/david-weir/",
        "bibtex": "@inproceedings{li-etal-2022-museclir,\n    title = \"{M}u{S}e{CLIR}: A Multiple Senses and Cross-lingual Information Retrieval Dataset\",\n    author = \"Li, Wing Yan  and\n      Weeds, Julie  and\n      Weir, David\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.96/\",\n    pages = \"1128--1135\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.96.pdf",
        "site": "https://aclanthology.org/2022.coling-1.96/",
        "pdf_size": 692196,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9423807545427712787&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "TAG laboratory, University of Sussex, Brighton, UK; TAG laboratory, University of Sussex, Brighton, UK; TAG laboratory, University of Sussex, Brighton, UK",
        "aff_domain": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk",
        "email": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Sussex",
        "aff_unique_dep": "TAG laboratory",
        "aff_unique_url": "https://www.sussex.ac.uk",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Brighton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.54",
        "title": "MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Building dialogue generation systems in a zero-shot scenario remains a huge challenge, since the typical zero-shot approaches in dialogue generation rely heavily on large-scale pre-trained language generation models such as GPT-3 and T5. The research on zero-shot dialogue generation without cumbersome language models is limited due to lacking corresponding parallel dialogue corpora. In this paper, we propose a simple but effective Multilingual learning framework for Zero-shot Dialogue Generation (dubbed as MulZDG) that can effectively transfer knowledge from an English corpus with large-scale training samples to a non-English corpus with zero samples. Besides, MulZDG can be viewed as a multilingual data augmentation method to improve the performance of the resource-rich language. First, we construct multilingual code-switching dialogue datasets via translation utterances randomly selected from monolingual English datasets. Then we employ MulZDG to train a unified multilingual dialogue model based on the code-switching datasets. The MulZDG can conduct implicit semantic alignment between different languages. Experiments on DailyDialog and DSTC7 datasets demonstrate that MulZDG not only achieve competitive performance under zero-shot case compared to training with sufficient examples but also greatly improve the performance of the source language.",
        "author": "Yongkang Liu; Shi Feng; Daling Wang; Yifei Zhang",
        "authorids": "/y/yongkang-liu/; /s/shi-feng/; /d/daling-wang/; /y/yifei-zhang/",
        "bibtex": "@inproceedings{liu-etal-2022-mulzdg,\n    title = \"{M}ul{ZDG}: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation\",\n    author = \"Liu, Yongkang  and\n      Feng, Shi  and\n      Wang, Daling  and\n      Zhang, Yifei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.54/\",\n    pages = \"648--659\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.54.pdf",
        "site": "https://aclanthology.org/2022.coling-1.54/",
        "pdf_size": 774633,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11151457947905728216&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; Northeastern University, Shenyang, China",
        "aff_domain": "163.com;cse.neu.edu.cn;cse.neu.edu.cn;cse.neu.edu.cn",
        "email": "163.com;cse.neu.edu.cn;cse.neu.edu.cn;cse.neu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.neu.edu.cn/",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shenyang",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.512",
        "title": "Multi Graph Neural Network for Extractive Long Document Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Heterogeneous Graph Neural Networks (HeterGNN) have been recently introduced as an emergent approach for extracting document summarization (EDS) by exploiting the cross-relations between words and sentences. However, applying HeterGNN for long documents is still an open research issue. One of the main majors is the lacking of inter-sentence connections. In this regard, this paper exploits how to apply HeterGNN for long documents by building a graph on sentence-level nodes (homogeneous graph) and combine with HeterGNN for capturing the semantic information in terms of both inter and intra-sentence connections. Experiments on two benchmark datasets of long documents such as PubMed and ArXiv show that our method is able to achieve state-of-the-art results in this research field.",
        "author": "Xuan-Dung Doan; Le-Minh Nguyen; Khac-Hoai Nam Bui",
        "authorids": "/x/xuan-dung-doan/; /m/minh-le-nguyen/; /k/khac-hoai-nam-bui/",
        "bibtex": "@inproceedings{doan-etal-2022-multi,\n    title = \"Multi Graph Neural Network for Extractive Long Document Summarization\",\n    author = \"Doan, Xuan-Dung  and\n      Nguyen, Le-Minh  and\n      Bui, Khac-Hoai Nam\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.512/\",\n    pages = \"5870--5875\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.512.pdf",
        "site": "https://aclanthology.org/2022.coling-1.512/",
        "pdf_size": 316105,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4128929828810538592&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Viettel Cyperspace Center, Viettel Group, Vietnam; Japan Advanced Institute of Science and Technology, Japan; Viettel Cyperspace Center, Viettel Group, Vietnam",
        "aff_domain": "viettel.com.vn;jaist.ac.jp;viettel.com.vn",
        "email": "viettel.com.vn;jaist.ac.jp;viettel.com.vn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Viettel Group;Japan Advanced Institute of Science and Technology",
        "aff_unique_dep": "Viettel Cyperspace Center;",
        "aff_unique_url": "https://www.viettel.com.vn;https://www.jaist.ac.jp",
        "aff_unique_abbr": "Viettel;JAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Vietnam;Japan"
    },
    {
        "id": "2022.coling-1.516",
        "title": "Multi-Attribute Controlled Text Generation with Contrastive-Generator and External-Discriminator",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Though existing researches have achieved impressive results in controlled text generation, they focus mainly on single-attribute control. However, in applications like automatic comments, the topic and sentiment need to be controlled simultaneously. In this work, we propose a new framework for multi-attribute controlled text generation. To achieve this, we design a contrastive-generator that can effectively generate texts with more attributes. In order to increase the convergence of the text on the desired attributes, we adopt an external-discriminator to distinguish whether the generated text holds the desired attributes. Moreover, we propose top-n weighted decoding to further improve the relevance of texts to attributes. Automated evaluations and human evaluations show that our framework achieves remarkable controllability in multi-attribute generation while keeping the text fluent and diverse. It also yields promising performance on zero-shot generation.",
        "author": "Guisheng Liu; Yi Li; Yanqing Guo; Xiangyang Luo; Bo Wang",
        "authorids": "/g/guisheng-liu/; /y/yi-li/; /y/yanqing-guo/; /x/xiangyang-luo/; /b/bo-wang/",
        "bibtex": "@inproceedings{liu-etal-2022-multi-attribute,\n    title = \"Multi-Attribute Controlled Text Generation with Contrastive-Generator and External-Discriminator\",\n    author = \"Liu, Guisheng  and\n      Li, Yi  and\n      Guo, Yanqing  and\n      Luo, Xiangyang  and\n      Wang, Bo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.516/\",\n    pages = \"5904--5913\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.516.pdf",
        "site": "https://aclanthology.org/2022.coling-1.516/",
        "pdf_size": 414860,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14702393320975476282&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; Information Engineering University; Dalian University of Technology",
        "aff_domain": "mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;126.com;dlut.edu.cn",
        "email": "mail.dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;126.com;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Dalian University of Technology;Information Engineering University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.dlut.edu.cn/;",
        "aff_unique_abbr": "DUT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.543",
        "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.",
        "author": "Pancheng Wang; Shasha Li; Kunyuan Pang; Liangliang He; Dong Li; Jintao Tang; Ting Wang",
        "authorids": "/p/pancheng-wang/; /s/shasha-li/; /k/kunyuan-pang/; /l/liangliang-he/; /d/dong-li/; /j/jintao-tang/; /t/ting-wang/",
        "bibtex": "@inproceedings{wang-etal-2022-multi,\n    title = \"Multi-Document Scientific Summarization from a Knowledge Graph-Centric View\",\n    author = \"Wang, Pancheng  and\n      Li, Shasha  and\n      Pang, Kunyuan  and\n      He, Liangliang  and\n      Li, Dong  and\n      Tang, Jintao  and\n      Wang, Ting\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.543/\",\n    pages = \"6222--6233\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.543.pdf",
        "site": "https://aclanthology.org/2022.coling-1.543/",
        "pdf_size": 793527,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17304614627830437891&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "https://github.com/muguruzawang/KGSum",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "http://www.nudt.edu.cn",
        "aff_unique_abbr": "NUDT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Changsha",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.519",
        "title": "Multi-Figurative Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Figurative language generation is the task of reformulating a given text in the desired figure of speech while still being faithful to the original context. We take the first step towards multi-figurative language modelling by providing a benchmark for the automatic generation of five common figurative forms in English. We train mFLAG employing a scheme for multi-figurative language pre-training on top of BART, and a mechanism for injecting the target figurative information into the encoder; this enables the generation of text with the target figurative form from another figurative form without parallel figurative-figurative sentence pairs. Our approach outperforms all strong baselines. We also offer some qualitative analysis and reflections on the relationship between the different figures of speech.",
        "author": "Huiyuan Lai; Malvina Nissim",
        "authorids": "/h/huiyuan-lai/; /m/malvina-nissim/",
        "bibtex": "@inproceedings{lai-nissim-2022-multi,\n    title = \"Multi-Figurative Language Generation\",\n    author = \"Lai, Huiyuan  and\n      Nissim, Malvina\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.519/\",\n    pages = \"5939--5954\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.519.pdf",
        "site": "https://aclanthology.org/2022.coling-1.519/",
        "pdf_size": 1498894,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3034227756840687603&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Language and Cognition (CLCG) University of Groningen / The Netherlands; Center for Language and Cognition (CLCG) University of Groningen / The Netherlands",
        "aff_domain": "rug.nl;rug.nl",
        "email": "rug.nl;rug.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Groningen",
        "aff_unique_dep": "Center for Language and Cognition (CLCG)",
        "aff_unique_url": "https://www.rug.nl",
        "aff_unique_abbr": "RUG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.coling-1.486",
        "title": "Multi-Layer Pseudo-Siamese Biaffine Model for Dependency Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Biaffine method is a strong and efficient method for graph-based dependency parsing. However, previous work only used the biaffine method at the end of the dependency parser as a scorer, and its application in multi-layer form is ignored. In this paper, we propose a multi-layer pseudo-Siamese biaffine model for neural dependency parsing. In this model, we modify the biaffine method so that it can be utilized in multi-layer form, and use pseudo-Siamese biaffine module to construct arc weight matrix for final prediction. In our proposed multi-layer architecture, the biaffine method plays important roles in both scorer and attention mechanism at the same time in each layer. We evaluate our model on PTB, CTB, and UD. The model achieves state-of-the-art results on these datasets. Further experiments show the benefits of introducing multi-layer form and pseudo-Siamese module into the biaffine method with low efficiency loss.",
        "author": "Ziyao Xu; Houfeng Wang; Bingdong Wang",
        "authorids": "/z/ziyao-xu/; /h/houfeng-wang/; /b/bingdong-wang/",
        "bibtex": "@inproceedings{xu-etal-2022-multi,\n    title = \"Multi-Layer Pseudo-{S}iamese Biaffine Model for Dependency Parsing\",\n    author = \"Xu, Ziyao  and\n      Wang, Houfeng  and\n      Wang, Bingdong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.486/\",\n    pages = \"5476--5487\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.486.pdf",
        "site": "https://aclanthology.org/2022.coling-1.486/",
        "pdf_size": 660993,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8699709323032695097&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "MOE Key Lab of Computational Linguistics, Peking University; MOE Key Lab of Computational Linguistics, Peking University; Beijing Huilan Technology Co., Ltd.",
        "aff_domain": "pku.edu.cn;pku.edu.cn;huilan.com",
        "email": "pku.edu.cn;pku.edu.cn;huilan.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Peking University;Beijing Huilan Technology Co., Ltd.",
        "aff_unique_dep": "MOE Key Lab of Computational Linguistics;",
        "aff_unique_url": "http://www.pku.edu.cn;",
        "aff_unique_abbr": "PKU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.535",
        "title": "Multi-Perspective Document Revision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents a novel multi-perspective document revision task. In conventional studies on document revision, tasks such as grammatical error correction, sentence reordering, and discourse relation classification have been performed individually; however, these tasks simultaneously should be revised to improve the readability and clarity of a whole document. Thus, our study defines multi-perspective document revision as a task that simultaneously revises multiple perspectives. To model the task, we design a novel Japanese multi-perspective document revision dataset that simultaneously handles seven perspectives to improve the readability and clarity of a document. Although a large amount of data that simultaneously handles multiple perspectives is needed to model multi-perspective document revision elaborately, it is difficult to prepare such a large amount of this data. Therefore, our study offers a multi-perspective document revision modeling method that can use a limited amount of matched data (i.e., data for the multi-perspective document revision task) and external partially-matched data (e.g., data for the grammatical error correction task). Experiments using our created dataset demonstrate the effectiveness of using multiple partially-matched datasets to model the multi-perspective document revision task.",
        "author": "Mana Ihori; Hiroshi Sato; Tomohiro Tanaka; Ryo Masumura",
        "authorids": "/m/mana-ihori/; /h/hiroshi-sato/; /t/tomohiro-tanaka/; /r/ryo-masumura/",
        "bibtex": "@inproceedings{ihori-etal-2022-multi,\n    title = \"Multi-Perspective Document Revision\",\n    author = \"Ihori, Mana  and\n      Sato, Hiroshi  and\n      Tanaka, Tomohiro  and\n      Masumura, Ryo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.535/\",\n    pages = \"6128--6138\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.535.pdf",
        "site": "https://aclanthology.org/2022.coling-1.535/",
        "pdf_size": 2721224,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Xbk3Pj5Y68gJ:scholar.google.com/&scioq=Multi-Perspective+Document+Revision&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.92",
        "title": "Multi-Stage Framework with Refinement Based Point Set Registration for Unsupervised Bi-Lingual Word Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-lingual alignment of word embeddings are important in knowledge transfer across languages, for improving machine translation and other multi-lingual applications. Current unsupervised approaches relying on learning structure-preserving transformations, using adversarial networks and refinement strategies, suffer from instability and convergence issues. This paper proposes BioSpere, a novel multi-stage framework for unsupervised mapping of bi-lingual word embeddings onto a shared vector space, by combining adversarial initialization, refinement procedure and point set registration. Experiments for parallel dictionary induction and word similarity demonstrate state-of-the-art unsupervised results for BioSpere on diverse languages \u2013 showcasing robustness against variable adversarial performance.",
        "author": "Silviu Vlad Oprea; Sourav Dutta; Haytham Assem",
        "authorids": "/s/silviu-vlad-oprea/; /s/sourav-dutta/; /h/haytham-assem/",
        "bibtex": "@inproceedings{oprea-etal-2022-multi,\n    title = \"Multi-Stage Framework with Refinement Based Point Set Registration for Unsupervised Bi-Lingual Word Alignment\",\n    author = \"Oprea, Silviu Vlad  and\n      Dutta, Sourav  and\n      Assem, Haytham\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.92/\",\n    pages = \"1089--1097\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.92.pdf",
        "site": "https://aclanthology.org/2022.coling-1.92/",
        "pdf_size": 854123,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15277292009326164453&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon Alexa AI, Cambridge, UK + Huawei Research, Ireland; Huawei Research Centre, Dublin, Ireland; Amazon Alexa AI, Cambridge, UK + Huawei Research, Ireland",
        "aff_domain": "amazon.co.uk;huawei.com;amazon.co.uk",
        "email": "amazon.co.uk;huawei.com;amazon.co.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Amazon;Huawei Research;Huawei Research Centre",
        "aff_unique_dep": "Alexa AI;;",
        "aff_unique_url": "https://www.amazon.com;https://www.huawei.com/research;https://www.huawei.com",
        "aff_unique_abbr": "Amazon;Huawei;Huawei",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Cambridge;;Dublin",
        "aff_country_unique_index": "0+1;1;0+1",
        "aff_country_unique": "United Kingdom;Ireland"
    },
    {
        "id": "2022.coling-1.444",
        "title": "Multi-level Community-awareness Graph Neural Networks for Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural Machine Translation (NMT) aims to translate the source- to the target-language while preserving the original meaning. Linguistic information such as morphology, syntactic, and semantics shall be grasped in token embeddings to produce a high-quality translation. Recent works have leveraged the powerful Graph Neural Networks (GNNs) to encode such language knowledge into token embeddings. Specifically, they use a trained parser to construct semantic graphs given sentences and then apply GNNs. However, most semantic graphs are tree-shaped and too sparse for GNNs which cause the over-smoothing problem. To alleviate this problem, we propose a novel Multi-level Community-awareness Graph Neural Network (MC-GNN) layer to jointly model local and global relationships between words and their linguistic roles in multiple communities. Intuitively, the MC-GNN layer substitutes a self-attention layer at the encoder side of a transformer-based machine translation model. Extensive experiments on four language-pair datasets with common evaluation metrics show the remarkable improvements of our method while reducing the time complexity in very long sentences.",
        "author": "Binh Nguyen; Long Nguyen; Dien Dinh",
        "authorids": "/b/binh-nguyen/; /l/long-nguyen/; /d/dinh-dien/",
        "bibtex": "@inproceedings{nguyen-etal-2022-multi,\n    title = \"Multi-level Community-awareness Graph Neural Networks for Neural Machine Translation\",\n    author = \"Nguyen, Binh  and\n      Nguyen, Long  and\n      Dinh, Dien\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.444/\",\n    pages = \"5021--5028\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.444.pdf",
        "site": "https://aclanthology.org/2022.coling-1.444/",
        "pdf_size": 299336,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13984347858597832813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam",
        "aff_domain": "apcs.fitus.edu.vn;fit.hcmus.edu.vn;fit.hcmus.edu.vn",
        "email": "apcs.fitus.edu.vn;fit.hcmus.edu.vn;fit.hcmus.edu.vn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of Science;Vietnam National University",
        "aff_unique_dep": "Faculty of Information Technology;",
        "aff_unique_url": ";https://www.vnu.edu.vn",
        "aff_unique_abbr": ";VNU",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Ho Chi Minh City",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2022.coling-1.227",
        "title": "Multi-modal Contrastive Representation Learning for Entity Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-modal entity alignment aims to identify equivalent entities between two different multi-modal knowledge graphs, which consist of structural triples and images associated with entities. Most previous works focus on how to utilize and encode information from different modalities, while it is not trivial to leverage multi-modal knowledge in entity alignment because of the modality heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive Learning based Entity Alignment model, to obtain effective joint representations for multi-modal entity alignment. Different from previous works, MCLEA considers task-oriented modality and models the inter-modal relationships for each entity representation. In particular, MCLEA firstly learns multiple individual representations from multiple modalities, and then performs contrastive learning to jointly model intra-modal and inter-modal interactions. Extensive experimental results show that MCLEA outperforms state-of-the-art baselines on public datasets under both supervised and unsupervised settings.",
        "author": "Zhenxi Lin; Ziheng Zhang; Meng Wang; Yinghui Shi; Xian Wu; Yefeng Zheng",
        "authorids": "/z/zhenxi-lin/; /z/ziheng-zhang/; /m/meng-wang/; /y/yinghui-shi/; /x/xian-wu/; /y/yefeng-zheng/",
        "bibtex": "@inproceedings{lin-etal-2022-multi,\n    title = \"Multi-modal Contrastive Representation Learning for Entity Alignment\",\n    author = \"Lin, Zhenxi  and\n      Zhang, Ziheng  and\n      Wang, Meng  and\n      Shi, Yinghui  and\n      Wu, Xian  and\n      Zheng, Yefeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.227/\",\n    pages = \"2572--2584\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.227.pdf",
        "site": "https://aclanthology.org/2022.coling-1.227/",
        "pdf_size": 1008909,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12494078240840784534&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent Jarvis Lab, Shenzhen, China; Tencent Jarvis Lab, Shenzhen, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; Tencent Jarvis Lab, Shenzhen, China; Tencent Jarvis Lab, Shenzhen, China",
        "aff_domain": "tencent.com;tencent.com;seu.edu.cn;seu.edu.cn;tencent.com;tencent.com",
        "email": "tencent.com;tencent.com;seu.edu.cn;seu.edu.cn;tencent.com;tencent.com",
        "github": "https://github.com/lzxlin/MCLEA",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Tencent;Southeast University",
        "aff_unique_dep": "Jarvis Lab;School of Computer Science and Engineering",
        "aff_unique_url": "https://www.tencent.com;https://www.seu.edu.cn/",
        "aff_unique_abbr": "Tencent;SEU",
        "aff_campus_unique_index": "0;0;1;1;0;0",
        "aff_campus_unique": "Shenzhen;Nanjing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.10",
        "title": "Multi-view and Cross-view Brain Decoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Can we build multi-view decoders that can decode concepts from brain recordings corresponding to any view (picture, sentence, word cloud) of stimuli? Can we build a system that can use brain recordings to automatically describe what a subject is watching using keywords or sentences? How about a system that can automatically extract important keywords from sentences that a subject is reading? Previous brain decoding efforts have focused only on single view analysis and hence cannot help us build such systems. As a first step toward building such systems, inspired by Natural Language Processing literature on multi-lingual and cross-lingual modeling, we propose two novel brain decoding setups: (1) multi-view decoding (MVD) and (2) cross-view decoding (CVD). In MVD, the goal is to build an MV decoder that can take brain recordings for any view as input and predict the concept. In CVD, the goal is to train a model which takes brain recordings for one view as input and decodes a semantic vector representation of another view. Specifically, we study practically useful CVD tasks like image captioning, image tagging, keyword extraction, and sentence formation. Our extensive experiments lead to MVD models with ~0.68 average pairwise accuracy across view pairs, and also CVD models with ~0.8 average pairwise accuracy across tasks. Analysis of the contribution of different brain networks reveals exciting cognitive insights: (1) Models trained on picture or sentence view of stimuli are better MV decoders than a model trained on word cloud view. (2) Our extensive analysis across 9 broad regions, 11 language sub-regions and 16 visual sub-regions of the brain help us localize, for the first time, the parts of the brain involved in cross-view tasks like image captioning, image tagging, sentence formation and keyword extraction. We make the code publicly available.",
        "author": "Subba Reddy Oota; Jashn Arora; Manish Gupta; Raju S. Bapi",
        "authorids": "/s/subba-reddy-oota/; /j/jashn-arora/; /m/manish-gupta/; /r/raju-s-bapi/",
        "bibtex": "@inproceedings{oota-etal-2022-multi,\n    title = \"Multi-view and Cross-view Brain Decoding\",\n    author = \"Oota, Subba Reddy  and\n      Arora, Jashn  and\n      Gupta, Manish  and\n      Bapi, Raju S.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.10/\",\n    pages = \"105--115\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.10.pdf",
        "site": "https://aclanthology.org/2022.coling-1.10/",
        "pdf_size": 3731466,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8820548815361254471&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "IIIT Hyderabad, India + INRIA, Bordeaux, France; IIIT Hyderabad, India; Microsoft, India + IIIT Hyderabad, India; IIIT Hyderabad, India",
        "aff_domain": "inria.fr;research.iiit.ac.in;microsoft.com;iiit.ac.in",
        "email": "inria.fr;research.iiit.ac.in;microsoft.com;iiit.ac.in",
        "github": "",
        "project": "https://tinyurl.com/MVCVBD",
        "author_num": 4,
        "aff_unique_index": "0+1;0;2+0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;INRIA;Microsoft Corporation",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.inria.fr;https://www.microsoft.com/en-in",
        "aff_unique_abbr": "IIIT Hyderabad;INRIA;Microsoft",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Hyderabad;Bordeaux;",
        "aff_country_unique_index": "0+1;0;0+0;0",
        "aff_country_unique": "India;France"
    },
    {
        "id": "2022.coling-1.334",
        "title": "MultiCoNER: A Large-scale Multilingual Dataset for Complex Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present AnonData, a large multilingual dataset for Named Entity Recognition that covers 3 domains (Wiki sentences, questions, and search queries) across 11 languages, as well as multilingual and code-mixing subsets. This dataset is designed to represent contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities like movie titles, and long-tail entity distributions. The 26M token dataset is compiled from public resources using techniques such as heuristic-based sentence sampling, template extraction and slotting, and machine translation. We tested the performance of two NER models on our dataset: a baseline XLM-RoBERTa model, and a state-of-the-art NER GEMNET model that leverages gazetteers. The baseline achieves moderate performance (macro-F1=54%). GEMNET, which uses gazetteers, improvement significantly (average improvement of macro-F1=+30%) and demonstrates the difficulty of our dataset. AnonData poses challenges even for large pre-trained language models, and we believe that it can help further research in building robust NER systems.",
        "author": "Shervin Malmasi; Anjie Fang; Besnik Fetahu; Sudipta Kar; Oleg Rokhlenko",
        "authorids": "/s/shervin-malmasi/; /a/anjie-fang/; /b/besnik-fetahu/; /s/sudipta-kar/; /o/oleg-rokhlenko/",
        "bibtex": "@inproceedings{malmasi-etal-2022-multiconer,\n    title = \"{M}ulti{C}o{NER}: A Large-scale Multilingual Dataset for Complex Named Entity Recognition\",\n    author = \"Malmasi, Shervin  and\n      Fang, Anjie  and\n      Fetahu, Besnik  and\n      Kar, Sudipta  and\n      Rokhlenko, Oleg\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.334/\",\n    pages = \"3798--3809\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.334.pdf",
        "site": "https://aclanthology.org/2022.coling-1.334/",
        "pdf_size": 521849,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6849283900275391015&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon.com, Seattle, WA, USA; Amazon.com, Seattle, WA, USA; Amazon.com, Seattle, WA, USA; Amazon.com, Seattle, WA, USA; Amazon.com, Seattle, WA, USA",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "https://registry.opendata.aws/multiconer/",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon.com",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.368",
        "title": "Multilingual Word Sense Disambiguation with Unified Sense Representation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As a key natural language processing (NLP) task, word sense disambiguation (WSD) evaluates how well NLP models can understand the fine-grained semantics of words under specific contexts. Benefited from the large-scale annotation, current WSD systems have achieved impressive performances in English by combining supervised learning with lexical knowledge. However, such success is hard to be replicated in other languages, where we only have very limited annotations. In this paper, based on that the multilingual lexicon BabelNet describing the same set of concepts across languages, we propose to build knowledge and supervised based Multilingual Word Sense Disambiguation (MWSD) systems. We build unified sense representations for multiple languages and address the annotation scarcity problem for MWSD by transferring annotations from rich sourced languages. With the unified sense representations, annotations from multiple languages can be jointly trained to benefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets demonstrate the effectiveness of our methodology.",
        "author": "Ying Su; Hongming Zhang; Yangqiu Song; Tong Zhang",
        "authorids": "/y/ying-su/; /h/hongming-zhang/; /y/yangqiu-song/; /t/tong-zhang/",
        "bibtex": "@inproceedings{su-etal-2022-multilingual,\n    title = \"Multilingual Word Sense Disambiguation with Unified Sense Representation\",\n    author = \"Su, Ying  and\n      Zhang, Hongming  and\n      Song, Yangqiu  and\n      Zhang, Tong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.368/\",\n    pages = \"4193--4202\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.368.pdf",
        "site": "https://aclanthology.org/2022.coling-1.368/",
        "pdf_size": 465158,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=784070984142693876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "HKUST; HKUST+Tencent AI lab, Seattle; HKUST; HKUST",
        "aff_domain": "connect.ust.hk;cse.ust.hk;cse.ust.hk;ust.hk",
        "email": "connect.ust.hk;cse.ust.hk;cse.ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tencent",
        "aff_unique_dep": ";AI lab",
        "aff_unique_url": "https://www.ust.hk;https://ai.tencent.com",
        "aff_unique_abbr": "HKUST;Tencent AI lab",
        "aff_campus_unique_index": "0;0+1;0;0",
        "aff_campus_unique": "Hong Kong SAR;Seattle",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.355",
        "title": "Multilingual and Multimodal Topic Modelling with Pretrained Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper presents M3L-Contrast\u2014a novel multimodal multilingual (M3L) neural topic model for comparable data that maps texts from multiple languages and images into a shared topic space. Our model is trained jointly on texts and images and takes advantage of pretrained document and image embeddings to abstract the complexities between different languages and modalities. As a multilingual topic model, it produces aligned language-specific topics and as multimodal model, it infers textual representations of semantic concepts in images. We demonstrate that our model is competitive with a zero-shot topic model in predicting topic distributions for comparable multilingual data and significantly outperforms a zero-shot model in predicting topic distributions for comparable texts and images. We also show that our model performs almost as well on unaligned embeddings as it does on aligned embeddings.",
        "author": "Elaine Zosa; Lidia Pivovarova",
        "authorids": "/e/elaine-zosa/; /l/lidia-pivovarova/",
        "bibtex": "@inproceedings{zosa-pivovarova-2022-multilingual,\n    title = \"Multilingual and Multimodal Topic Modelling with Pretrained Embeddings\",\n    author = \"Zosa, Elaine  and\n      Pivovarova, Lidia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.355/\",\n    pages = \"4037--4048\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.355.pdf",
        "site": "https://aclanthology.org/2022.coling-1.355/",
        "pdf_size": 2737513,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17928389194745712206&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland",
        "aff_domain": "helsinki.fi;helsinki.fi",
        "email": "helsinki.fi;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Helsinki",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.helsinki.fi",
        "aff_unique_abbr": "UH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Helsinki",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "2022.coling-1.239",
        "title": "Multimodal Semi-supervised Learning for Disaster Tweet Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "During natural disasters, people often use social media platforms, such as Twitter, to post information about casualties and damage produced by disasters. This information can help relief authorities gain situational awareness in nearly real time, and enable them to quickly distribute resources where most needed. However, annotating data for this purpose can be burdensome, subjective and expensive. In this paper, we investigate how to leverage the copious amounts of unlabeled data generated on social media by disaster eyewitnesses and affected individuals during disaster events. To this end, we propose a semi-supervised learning approach to improve the performance of neural models on several multimodal disaster tweet classification tasks. Our approach shows significant improvements, obtaining up to 7.7% improvements in F-1 in low-data regimes and 1.9% when using the entire training data. We make our code and data publicly available at https://github.com/iustinsirbu13/multimodal-ssl-for-disaster-tweet-classification.",
        "author": "Iustin Sirbu; Tiberiu Sosea; Cornelia Caragea; Doina Caragea; Traian Rebedea",
        "authorids": "/i/iustin-sirbu/; /t/tiberiu-sosea/; /c/cornelia-caragea/; /d/doina-caragea/; /t/traian-rebedea/",
        "bibtex": "@inproceedings{sirbu-etal-2022-multimodal,\n    title = \"Multimodal Semi-supervised Learning for Disaster Tweet Classification\",\n    author = \"Sirbu, Iustin  and\n      Sosea, Tiberiu  and\n      Caragea, Cornelia  and\n      Caragea, Doina  and\n      Rebedea, Traian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.239/\",\n    pages = \"2711--2723\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.239.pdf",
        "site": "https://aclanthology.org/2022.coling-1.239/",
        "pdf_size": 804723,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14811746186338563262&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University Politehnica of Bucharest; University of Illinois Chicago; University of Illinois Chicago; Kansas State University; University Politehnica of Bucharest",
        "aff_domain": "upb.ro;uic.edu;uic.edu;ksu.edu;upb.ro",
        "email": "upb.ro;uic.edu;uic.edu;ksu.edu;upb.ro",
        "github": "https://github.com/iustinsirbu13/multimodal-ssl-for-disaster-tweet-classification",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "University Politehnica of Bucharest;University of Illinois at Chicago;Kansas State University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.upb.ro;https://www.uic.edu;https://www.k-state.edu",
        "aff_unique_abbr": "UPB;UIC;K-State",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "Romania;United States"
    },
    {
        "id": "2022.coling-1.286",
        "title": "NSP-BERT: A Prompt-based Few-Shot Learner through an Original Pre-training Task \u2014\u2014 Next Sentence Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Using prompts to utilize language models to perform various downstream tasks, also known as prompt-based learning or prompt-learning, has lately gained significant success in comparison to the pre-train and fine-tune paradigm. Nonetheless, virtually most prompt-based methods are token-level such as PET based on mask language model (MLM). In this paper, we attempt to accomplish several NLP tasks in the zero-shot and few-shot scenarios using a BERT original pre-training task abandoned by RoBERTa and other models\u2014\u2014Next Sentence Prediction (NSP). Unlike token-level techniques, our sentence-level prompt-based method NSP-BERT does not need to fix the length of the prompt or the position to be predicted, allowing it to handle tasks such as entity linking with ease. NSP-BERT can be applied to a variety of tasks based on its properties. We present an NSP-tuning approach with binary cross-entropy loss for single-sentence classification tasks that is competitive compared to PET and EFL. By continuing to train BERT on RoBERTa\u2019s corpus, the model\u2019s performance improved significantly, which indicates that the pre-training corpus is another important determinant of few-shot besides model size and prompt method.",
        "author": "Yi Sun; Yu Zheng; Chao Hao; Hangping Qiu",
        "authorids": "/y/yi-sun/; /y/yu-zheng/; /c/chao-hao/; /h/hangping-qiu/",
        "bibtex": "@inproceedings{sun-etal-2022-nsp,\n    title = \"{NSP}-{BERT}: A Prompt-based Few-Shot Learner through an Original Pre-training Task {---}{---} Next Sentence Prediction\",\n    author = \"Sun, Yi  and\n      Zheng, Yu  and\n      Hao, Chao  and\n      Qiu, Hangping\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.286/\",\n    pages = \"3233--3250\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.286.pdf",
        "site": "https://aclanthology.org/2022.coling-1.286/",
        "pdf_size": 1205893,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6429938890767901551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Army Engineering University of PLA, Nanjing, China; Army Engineering University of PLA, Nanjing, China; Army Engineering University of PLA, Nanjing, China; Army Engineering University of PLA, Nanjing, China",
        "aff_domain": "sina.com;outlook.com;163.com;163.com",
        "email": "sina.com;outlook.com;163.com;163.com",
        "github": "https://github.com/sunyilgdx/Prompts4Keras",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Army Engineering University of PLA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.592",
        "title": "Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Within textual emotion classification, the set of relevant labels depends on the domain and application scenario and might not be known at the time of model development. This conflicts with the classical paradigm of supervised learning in which the labels need to be predefined. A solution to obtain a model with a flexible set of labels is to use the paradigm of zero-shot learning as a natural language inference task, which in addition adds the advantage of not needing any labeled training data. This raises the question how to prompt a natural language inference model for zero-shot learning emotion classification. Options for prompt formulations include the emotion name anger alone or the statement \u201cThis text expresses anger\u201d. With this paper, we analyze how sensitive a natural language inference-based zero-shot-learning classifier is to such changes to the prompt under consideration of the corpus: How carefully does the prompt need to be selected? We perform experiments on an established set of emotion datasets presenting different language registers according to different sources (tweets, events, blogs) with three natural language inference models and show that indeed the choice of a particular prompt formulation needs to fit to the corpus. We show that this challenge can be tackled with combinations of multiple prompts. Such ensemble is more robust across corpora than individual prompts and shows nearly the same performance as the individual best prompt for a particular corpus.",
        "author": "Flor Miriam Plaza-del-Arco; Mar\u00eda-Teresa Mart\u00edn-Valdivia; Roman Klinger",
        "authorids": "/f/flor-miriam-plaza-del-arco/; /m/maria-teresa-martin-valdivia/; /r/roman-klinger/",
        "bibtex": "@inproceedings{plaza-del-arco-etal-2022-natural,\n    title = \"Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora\",\n    author = \"Plaza-del-Arco, Flor Miriam  and\n      Mart{\\'i}n-Valdivia, Mar{\\'i}a-Teresa  and\n      Klinger, Roman\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.592/\",\n    pages = \"6805--6817\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.592.pdf",
        "site": "https://aclanthology.org/2022.coling-1.592/",
        "pdf_size": 420731,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12559294037602607925&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "SINAI, Computer Science Department, CEATIC, Universidad de Ja\u00e9n, Spain+Institut f\u00fcr Maschinelle Sprachverarbeitung, University of Stuttgart, Germany; SINAI, Computer Science Department, CEATIC, Universidad de Ja\u00e9n, Spain; Institut f\u00fcr Maschinelle Sprachverarbeitung, University of Stuttgart, Germany",
        "aff_domain": "ujaen.es;ujaen.es;ims.uni-stuttgart.de",
        "email": "ujaen.es;ujaen.es;ims.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Universidad de Ja\u00e9n;University of Stuttgart",
        "aff_unique_dep": "Computer Science Department;Institut f\u00fcr Maschinelle Sprachverarbeitung",
        "aff_unique_url": "https://www.ujaen.es;https://www.uni-stuttgart.de",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "Spain;Germany"
    },
    {
        "id": "2022.coling-1.272",
        "title": "Negation, Coordination, and Quantifiers in Contextualized Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the success of contextualized language models, much research explores what these models really learn and in which cases they still fail. Most of this work focuses on specific NLP tasks and on the learning outcome. Little research has attempted to decouple the models\u2019 weaknesses from specific tasks and focus on the embeddings per se and their mode of learning. In this paper, we take up this research opportunity: based on theoretical linguistic insights, we explore whether the semantic constraints of function words are learned and how the surrounding context impacts their embeddings. We create suitable datasets, provide new insights into the inner workings of LMs vis-a-vis function words and implement an assisting visual web interface for qualitative analysis.",
        "author": "Aikaterini-Lida Kalouli; Rita Sevastjanova; Christin Beck; Maribel Romero",
        "authorids": "/a/aikaterini-lida-kalouli/; /r/rita-sevastjanova/; /c/christin-beck/; /m/maribel-romero/",
        "bibtex": "@inproceedings{kalouli-etal-2022-negation,\n    title = \"Negation, Coordination, and Quantifiers in Contextualized Language Models\",\n    author = \"Kalouli, Aikaterini-Lida  and\n      Sevastjanova, Rita  and\n      Beck, Christin  and\n      Romero, Maribel\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.272/\",\n    pages = \"3074--3085\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.272.pdf",
        "site": "https://aclanthology.org/2022.coling-1.272/",
        "pdf_size": 727112,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4614924212433312224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "CIS - LMU Munich; University of Konstanz; University of Konstanz; University of Konstanz",
        "aff_domain": "cis.lmu.de;uni.kn;uni.kn;uni.kn",
        "email": "cis.lmu.de;uni.kn;uni.kn;uni.kn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Ludwig Maximilian University of Munich;University of Konstanz",
        "aff_unique_dep": "Computer Science and Information Systems;",
        "aff_unique_url": "https://www.lmu.de;https://www.uni-konstanz.de",
        "aff_unique_abbr": "LMU;Uni Konstanz",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.218",
        "title": "Nested Named Entity Recognition as Corpus Aware Holistic Structure Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As a fundamental natural language processing task and one of core knowledge extraction techniques, named entity recognition (NER) is widely used to extract information from texts for downstream tasks. Nested NER is a branch of NER in which the named entities (NEs) are nested with each other. However, most of the previous studies on nested NER usually apply linear structure to model the nested NEs which are actually accommodated in a hierarchical structure. Thus in order to address this mismatch, this work models the full nested NEs in a sentence as a holistic structure, then we propose a holistic structure parsing algorithm to disclose the entire NEs once for all. Besides, there is no research on applying corpus-level information to NER currently. To make up for the loss of this information, we introduce Point-wise Mutual Information (PMI) and other frequency features from corpus-aware statistics for even better performance by holistic modeling from sentence-level to corpus-level. Experiments show that our model yields promising results on widely-used benchmarks which approach or even achieve state-of-the-art. Further empirical studies show that our proposed corpus-aware features can substantially improve NER domain adaptation, which demonstrates the surprising advantage of our proposed corpus-level holistic structure modeling.",
        "author": "Yifei Yang; Zuchao Li; Hai Zhao",
        "authorids": "/y/yifei-yang/; /z/zuchao-li/; /h/hai-zhao/",
        "bibtex": "@inproceedings{yang-etal-2022-nested,\n    title = \"Nested Named Entity Recognition as Corpus Aware Holistic Structure Parsing\",\n    author = \"Yang, Yifei  and\n      Li, Zuchao  and\n      Zhao, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.218/\",\n    pages = \"2472--2482\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.218.pdf",
        "site": "https://aclanthology.org/2022.coling-1.218/",
        "pdf_size": 515116,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9475940930738078875&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/yangyifei729/NerAsParsing",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.17",
        "title": "Neuro-Symbolic Visual Dialog",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose Neuro-Symbolic Visual Dialog (NSVD) \u2014the first method to combine deep learning and symbolic program execution for multi-round visually-grounded reasoning. NSVD significantly outperforms existing purely-connectionist methods on two key challenges inherent to visual dialog: long-distance co-reference resolution as well as vanishing question-answering performance. We demonstrate the latter by proposing a more realistic and stricter evaluation scheme in which we use predicted answers for the full dialog history when calculating accuracy. We describe two variants of our model and show that using this new scheme, our best model achieves an accuracy of 99.72% on CLEVR-Dialog\u2014a relative improvement of more than 10% over the state of the art\u2014while only requiring a fraction of training data. Moreover, we demonstrate that our neuro-symbolic models have a higher mean first failure round, are more robust against incomplete dialog histories, and generalise better not only to dialogs that are up to three times longer than those seen during training but also to unseen question types and scenes.",
        "author": "Adnen Abdessaied; Mihai B\u00e2ce; Andreas Bulling",
        "authorids": "/a/adnen-abdessaied/; /m/mihai-bace/; /a/andreas-bulling/",
        "bibtex": "@inproceedings{abdessaied-etal-2022-neuro,\n    title = \"Neuro-Symbolic Visual Dialog\",\n    author = \"Abdessaied, Adnen  and\n      B{\\^a}ce, Mihai  and\n      Bulling, Andreas\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.17/\",\n    pages = \"192--217\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.17.pdf",
        "site": "https://aclanthology.org/2022.coling-1.17/",
        "pdf_size": 7082867,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7291461732486867587&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Institute for Visualization and Interactive Systems (VIS), University of Stuttgart, Germany; Institute for Visualization and Interactive Systems (VIS), University of Stuttgart, Germany; Institute for Visualization and Interactive Systems (VIS), University of Stuttgart, Germany",
        "aff_domain": "vis.uni-stuttgart.de;vis.uni-stuttgart.de;vis.uni-stuttgart.de",
        "email": "vis.uni-stuttgart.de;vis.uni-stuttgart.de;vis.uni-stuttgart.de",
        "github": "",
        "project": "https://perceptualui.org/publications/abdessaied22_coling/",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "Institute for Visualization and Interactive Systems (VIS)",
        "aff_unique_url": "https://www.vis.uni-stuttgart.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.73",
        "title": "New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent research shows that pre-trained language models, built to generate text conditioned on some context, learn to encode syntactic knowledge to a certain degree. This has motivated researchers to move beyond the sentence-level and look into their ability to encode less studied discourse-level phenomena. In this paper, we add to the body of probing research by investigating discourse entity representations in large pre-trained language models in English. Motivated by early theories of discourse and key pieces of previous work, we focus on the information-status of entities as discourse-new or discourse-old. We present two probing models, one based on binary classification and another one on sequence labeling. The results of our experiments show that pre-trained language models do encode information on whether an entity has been introduced before or not in the discourse. However, this information alone is not sufficient to find the entities in a discourse, opening up interesting questions about the definition of entities for future work.",
        "author": "Sharid Lo\u00e1iciga; Anne Beyer; David Schlangen",
        "authorids": "/s/sharid-loaiciga/; /a/anne-beyer/; /d/david-schlangen/",
        "bibtex": "@inproceedings{loaiciga-etal-2022-new,\n    title = \"New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities\",\n    author = \"Lo{\\'a}iciga, Sharid  and\n      Beyer, Anne  and\n      Schlangen, David\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.73/\",\n    pages = \"875--886\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.73.pdf",
        "site": "https://aclanthology.org/2022.coling-1.73/",
        "pdf_size": 376753,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3153669704706917163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "CLASP, Dept. of Philosophy, Linguistics and Theory of Science, University of Gothenburg; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany",
        "aff_domain": "gu.se;uni-potsdam.de;uni-potsdam.de",
        "email": "gu.se;uni-potsdam.de;uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Gothenburg;University of Potsdam",
        "aff_unique_dep": "Dept. of Philosophy, Linguistics and Theory of Science;Department of Linguistics",
        "aff_unique_url": "https://www.gu.se;https://www.uni-potsdam.de",
        "aff_unique_abbr": "GU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Sweden;Germany"
    },
    {
        "id": "2022.coling-1.402",
        "title": "Noise Learning for Text Classification: A Benchmark",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Noise Learning is important in the task of text classification which depends on massive labeled data that could be error-prone. However, we find that noise learning in text classification is relatively underdeveloped: 1. many methods that have been proven effective in the image domain are not explored in text classification, 2. it is difficult to conduct a fair comparison between previous studies as they do experiments in different noise settings. In this work, we adapt four state-of-the-art methods of noise learning from the image domain to text classification. Moreover, we conduct comprehensive experiments on our benchmark of noise learning with seven commonly-used methods, four datasets, and five noise modes. Additionally, most previous works are based on an implicit hypothesis that the commonly-used datasets such as TREC, Ag-News and Chnsenticorp contain no errors. However, these datasets indeed contain 0.61% to 15.77% noise labels which we define as intrinsic noise that can cause inaccurate evaluation. Therefore, we build a new dataset Golden-Chnsenticorp( G-Chnsenticorp) without intrinsic noise to more accurately compare the effects of different noise learning methods. To the best of our knowledge, this is the first benchmark of noise learning for text classification.",
        "author": "Bo Liu; Wandi Xu; Yuejia Xiang; Xiaojun Wu; Lejian He; Bowen Zhang; Li Zhu",
        "authorids": "/b/bo-liu/; /w/wandi-xu/; /y/yuejia-xiang/; /x/xiaojun-wu/; /l/lejian-he/; /b/bowen-zhang/; /l/li-zhu/",
        "bibtex": "@inproceedings{liu-etal-2022-noise,\n    title = \"Noise Learning for Text Classification: A Benchmark\",\n    author = \"Liu, Bo  and\n      Xu, Wandi  and\n      Xiang, Yuejia  and\n      Wu, Xiaojun  and\n      He, Lejian  and\n      Zhang, Bowen  and\n      Zhu, Li\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.402/\",\n    pages = \"4557--4567\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.402.pdf",
        "site": "https://aclanthology.org/2022.coling-1.402/",
        "pdf_size": 510513,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16253948908911703084&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Sofrware Engineering, Xi\u2019an Jiaotong University + the CETC Key Laboratory of Smart City Model Simulation and Intelligent Technology + The Smart City Research Institute of CETC and National Center for Applied Mathematics Shenzhen(NCAMS); Northeastern University; Tencent; the CETC Key Laboratory of Smart City Model Simulation and Intelligent Technology + The Smart City Research Institute of CETC and National Center for Applied Mathematics Shenzhen(NCAMS); Cornell University; Shenzhen Technology University; School of Sofrware Engineering, Xi\u2019an Jiaotong University",
        "aff_domain": "stu.xjtu.edu.cn;stumail.neu.edu.cn;tencent.com;cetc.com.cn;cornell.edu;foxmail.com;xjtu.edu.cn",
        "email": "stu.xjtu.edu.cn;stumail.neu.edu.cn;tencent.com;cetc.com.cn;cornell.edu;foxmail.com;xjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+2;3;4;1+2;5;6;0",
        "aff_unique_norm": "Xi\u2019an Jiaotong University;CETC Key Laboratory of Smart City Model Simulation and Intelligent Technology;China Electronics Technology Group Corporation (CETC);Northeastern University;Tencent Holdings Limited;Cornell University;Shenzhen Technology University",
        "aff_unique_dep": "School of Software Engineering;Smart City Model Simulation and Intelligent Technology;Smart City Research Institute;;;;",
        "aff_unique_url": "https://www.xjtu.edu.cn;;;https://www.northeastern.edu;https://www.tencent.com;https://www.cornell.edu;https://www.sztu.edu.cn",
        "aff_unique_abbr": "XJTU;;CETC;NEU;Tencent;Cornell;",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0+0+0;1;0;0+0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.561",
        "title": "Noise-injected Consistency Training and Entropy-constrained Pseudo Labeling for Semi-supervised Extractive Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Labeling large amounts of extractive summarization data is often prohibitive expensive due to time, financial, and expertise constraints, which poses great challenges to incorporating summarization system in practical applications. This limitation can be overcome by semi-supervised approaches: consistency-training and pseudo-labeling to make full use of unlabeled data. Researches on the two, however, are conducted independently, and very few works try to connect them. In this paper, we first use the noise-injected consistency training paradigm to regularize model predictions. Subsequently, we propose a novel entropy-constrained pseudo labeling strategy to obtain high-confidence labels from unlabeled predictions, which can obtain high-confidence labels from unlabeled predictions by comparing the entropy of supervised and unsupervised predictions. By combining consistency training and pseudo-labeling, this framework enforce a low-density separation between classes, which decently improves the performance of supervised learning over an insufficient labeled extractive summarization dataset.",
        "author": "Yiming Wang; Qianren Mao; Junnan Liu; Weifeng Jiang; Hongdong Zhu; Jianxin Li",
        "authorids": "/y/yiming-wang/; /q/qianren-mao/; /j/junnan-liu/; /w/weifeng-jiang/; /h/hongdong-zhu/; /j/jianxin-li/",
        "bibtex": "@inproceedings{wang-etal-2022-noise,\n    title = \"Noise-injected Consistency Training and Entropy-constrained Pseudo Labeling for Semi-supervised Extractive Summarization\",\n    author = \"Wang, Yiming  and\n      Mao, Qianren  and\n      Liu, Junnan  and\n      Jiang, Weifeng  and\n      Zhu, Hongdong  and\n      Li, Jianxin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.561/\",\n    pages = \"6447--6456\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.561.pdf",
        "site": "https://aclanthology.org/2022.coling-1.561/",
        "pdf_size": 638838,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4392058815059926794&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China+The State Key Laboratory of Software Development Environment, Beihang University, China+Institute of Artificial Intelligence, Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China+The State Key Laboratory of Software Development Environment, Beihang University, China+Institute of Artificial Intelligence, Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China+The State Key Laboratory of Software Development Environment, Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China+The State Key Laboratory of Software Development Environment, Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China+The State Key Laboratory of Software Development Environment, Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China+The State Key Laboratory of Software Development Environment, Beihang University, China+Institute of Artificial Intelligence, Beihang University, China",
        "aff_domain": "act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn",
        "email": "act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0+0;0+0+0;0+0;0+0;0+0;0+0+0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "Beijing Advanced Innovation Center for Big Data and Brain Computing",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "Beihang",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.452",
        "title": "Noise-robust Cross-modal Interactive Learning with Text2Image Mask for Multi-modal Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-modal neural machine translation (MNMT) aims to improve textual level machine translation performance in the presence of text-related images. Most of the previous works on MNMT focus on multi-modal fusion methods with full visual features. However, text and its corresponding image may not match exactly, visual noise is generally inevitable. The irrelevant image regions may mislead or distract the textual attention and cause model performance degradation. This paper proposes a noise-robust multi-modal interactive fusion approach with cross-modal relation-aware mask mechanism for MNMT. A text-image relation-aware attention module is constructed through the cross-modal interaction mask mechanism, and visual features are extracted based on the text-image interaction mask knowledge. Then a noise-robust multi-modal adaptive fusion approach is presented by fusion the relevant visual and textual features for machine translation. We validate our method on the Multi30K dataset. The experimental results show the superiority of our proposed model, and achieve the state-of-the-art scores in all En-De, En-Fr and En-Cs translation tasks.",
        "author": "Junjie Ye; Junjun Guo; Yan Xiang; Kaiwen Tan; Zhengtao Yu",
        "authorids": "/j/junjie-ye/; /j/junjun-guo/; /y/yan-xiang/; /k/kaiwen-tan/; /z/zhengtao-yu/",
        "bibtex": "@inproceedings{ye-etal-2022-noise,\n    title = \"Noise-robust Cross-modal Interactive Learning with {T}ext2{I}mage Mask for Multi-modal Neural Machine Translation\",\n    author = \"Ye, Junjie  and\n      Guo, Junjun  and\n      Xiang, Yan  and\n      Tan, Kaiwen  and\n      Yu, Zhengtao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.452/\",\n    pages = \"5098--5108\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.452.pdf",
        "site": "https://aclanthology.org/2022.coling-1.452/",
        "pdf_size": 12066062,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17862708072600521599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China+Yunnan Key Laboratory of Arti\ufb01cial Intelligence, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China+Yunnan Key Laboratory of Arti\ufb01cial Intelligence, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China+Yunnan Key Laboratory of Arti\ufb01cial Intelligence, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China+Yunnan Key Laboratory of Arti\ufb01cial Intelligence, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China+Yunnan Key Laboratory of Arti\ufb01cial Intelligence, Kunming, China",
        "aff_domain": "qq.com;163.com;126.com;qq.com;hotmail.com",
        "email": "qq.com;163.com;126.com;qq.com;hotmail.com",
        "github": "https://github.com/nlp-mmt/Noise-robust-Text2image-Mask",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Kunming University of Science and Technology;Yunnan Key Laboratory of Artificial Intelligence",
        "aff_unique_dep": "Faculty of Information Engineering and Automation;Artificial Intelligence",
        "aff_unique_url": "http://www.kust.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Kunming",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.371",
        "title": "Noisy Label Regularisation for Textual Regression",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Training with noisy labelled data is known to be detrimental to model performance, especially for high-capacity neural network models in low-resource domains. Our experiments suggest that standard regularisation strategies, such as weight decay and dropout, are ineffective in the face of noisy labels. We propose a simple noisy label detection method that prevents error propagation from the input layer. The approach is based on the observation that the projection of noisy labels is learned through memorisation at advanced stages of learning, and that the Pearson correlation is sensitive to outliers. Extensive experiments over real-world human-disagreement annotations as well as randomly-corrupted and data-augmented labels, across various tasks and domains, demonstrate that our method is effective, regularising noisy labels and improving generalisation performance.",
        "author": "Yuxia Wang; Timothy Baldwin; Karin Verspoor",
        "authorids": "/y/yuxia-wang/; /t/timothy-baldwin/; /k/karin-verspoor/",
        "bibtex": "@inproceedings{wang-etal-2022-noisy,\n    title = \"Noisy Label Regularisation for Textual Regression\",\n    author = \"Wang, Yuxia  and\n      Baldwin, Timothy  and\n      Verspoor, Karin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.371/\",\n    pages = \"4228--4240\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.371.pdf",
        "site": "https://aclanthology.org/2022.coling-1.371/",
        "pdf_size": 493603,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9246821143249892952&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The University of Melbourne, Melbourne, Victoria, Australia+MBZUAI, Abu Dhabi, UAE; The University of Melbourne, Melbourne, Victoria, Australia+RMIT University, Melbourne, Victoria, Australia; RMIT University, Melbourne, Victoria, Australia",
        "aff_domain": "student.unimelb.edu.au;ldwin.net;rmit.edu.au",
        "email": "student.unimelb.edu.au;ldwin.net;rmit.edu.au",
        "github": "https://github.com/yuxiaw/Regularise-Regression-Noisy-Labels",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+2;2",
        "aff_unique_norm": "The University of Melbourne;Mohamed Bin Zayed University of Artificial Intelligence;RMIT University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.mbzuali.ac.ae;https://www.rmit.edu.au",
        "aff_unique_abbr": "UniMelb;MBZUAI;RMIT",
        "aff_campus_unique_index": "0+1;0+0;0",
        "aff_campus_unique": "Melbourne;Abu Dhabi",
        "aff_country_unique_index": "0+1;0+0;0",
        "aff_country_unique": "Australia;United Arab Emirates"
    },
    {
        "id": "2022.coling-1.228",
        "title": "Nonparametric Forest-Structured Neural Topic Modeling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural topic models have been widely used in discovering the latent semantics from a corpus. Recently, there are several researches on hierarchical neural topic models since the relationships among topics are valuable for data analysis and exploration. However, the existing hierarchical neural topic models are limited to generate a single topic tree. In this study, we present a nonparametric forest-structured neural topic model by firstly applying the self-attention mechanism to capture parent-child topic relationships, and then build a sparse directed acyclic graph to form a topic forest. Experiments indicate that our model can automatically learn a forest-structured topic hierarchy with indefinite numbers of trees and leaves, and significantly outperforms the baseline models on topic hierarchical rationality and affinity.",
        "author": "Zhihong Zhang; Xuewen Zhang; Yanghui Rao",
        "authorids": "/z/zhihong-zhang/; /x/xuewen-zhang/; /y/yanghui-rao/",
        "bibtex": "@inproceedings{zhang-etal-2022-nonparametric,\n    title = \"Nonparametric Forest-Structured Neural Topic Modeling\",\n    author = \"Zhang, Zhihong  and\n      Zhang, Xuewen  and\n      Rao, Yanghui\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.228/\",\n    pages = \"2585--2597\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.228.pdf",
        "site": "https://aclanthology.org/2022.coling-1.228/",
        "pdf_size": 2149773,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13484900700191646005&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China",
        "aff_domain": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.383",
        "title": "Noun Class Disambiguation in Runyankore and Related Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Bantu languages are spoken by communities in more than half of the countries on the African continent by an estimated third of a billion people. Despite this populous and the amount of high quality linguistic research done over the years, Bantu languages are still computationally under-resourced. The biggest limitation to the development of computational methods for processing Bantu language text is their complex grammatical structure, chiefly in the system of noun classes. We investigated the use of a combined syntactic and semantic method to disambiguate among singular nouns with the same class prefix but belonging to different noun classes. This combination uses the semantic generalizations of the types of nouns in each class to overcome the limitations of relying only on the prefixes they take. We used the nearest neighbors of a query word as semantic generalizations, and developed a tool to determine the noun class based on resources in Runyankore, a Bantu language indigenous to Uganda. We also investigated whether, with the same Runyankore resources, our method had utility in other Bantu languages, Luganda, indigenous to Uganda, and Kinyarwanda, indigenous to Rwanda. For all three languages, the combined approach resulted in an improvement in accuracy, as compared to using only the syntactic or the semantic approach.",
        "author": "Joan Byamugisha",
        "authorids": "/j/joan-byamugisha/",
        "bibtex": "@inproceedings{byamugisha-2022-noun,\n    title = \"Noun Class Disambiguation in {R}unyankore and Related Languages\",\n    author = \"Byamugisha, Joan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.383/\",\n    pages = \"4350--4359\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.383.pdf",
        "site": "https://aclanthology.org/2022.coling-1.383/",
        "pdf_size": 369439,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8128156173751600491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "IBM Research Africa",
        "aff_domain": "ibm.com",
        "email": "ibm.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM Research",
        "aff_unique_dep": "IBM Research Africa",
        "aff_unique_url": "https://www.ibm.com/research/africa",
        "aff_unique_abbr": "IBM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Unknown"
    },
    {
        "id": "2022.coling-1.338",
        "title": "Noun-MWP: Math Word Problems Meet Noun Answers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce a new type of problems for math word problem (MWP) solvers, named Noun-MWPs, whose answer is a non-numerical string containing a noun from the problem text. We present a novel method to empower existing MWP solvers to handle Noun-MWPs, and apply the method on Expression-Pointer Transformer (EPT). Our model, N-EPT, solves Noun-MWPs significantly better than other models, and at the same time, solves conventional MWPs as well. Solving Noun-MWPs may lead to bridging MWP solvers and traditional question-answering NLP models.",
        "author": "Taehun Cha; Jaeheun Jung; Donghun Lee",
        "authorids": "/t/taehun-cha/; /j/jaeheun-jung/; /d/donghun-lee-ku/",
        "bibtex": "@inproceedings{cha-etal-2022-noun,\n    title = \"Noun-{MWP}: Math Word Problems Meet Noun Answers\",\n    author = \"Cha, Taehun  and\n      Jung, Jaeheun  and\n      Lee, Donghun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.338/\",\n    pages = \"3847--3857\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.338.pdf",
        "site": "https://aclanthology.org/2022.coling-1.338/",
        "pdf_size": 2540681,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:HzYNBn0qbuQJ:scholar.google.com/&scioq=Noun-MWP:+Math+Word+Problems+Meet+Noun+Answers&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Department of Mathematics, Korea University; Department of Mathematics, Korea University; Department of Mathematics, Korea University",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.509",
        "title": "Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Research on Automatic Story Generation (ASG) relies heavily on human and automatic evaluation. However, there is no consensus on which human evaluation criteria to use, and no analysis of how well automatic criteria correlate with them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a set of 6 orthogonal and comprehensive human criteria, carefully motivated by the social sciences literature. We also present HANNA, an annotated dataset of 1,056 stories produced by 10 different ASG systems. HANNA allows us to quantitatively evaluate the correlations of 72 automatic metrics with human criteria. Our analysis highlights the weaknesses of current metrics for ASG and allows us to formulate practical recommendations for ASG evaluation.",
        "author": "Cyril Chhun; Pierre Colombo; Fabian M. Suchanek; Chlo\u00e9 Clavel",
        "authorids": "/c/cyril-chhun/; /p/pierre-colombo/; /f/fabian-suchanek/; /c/chloe-clavel/",
        "bibtex": "@inproceedings{chhun-etal-2022-human,\n    title = \"Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation\",\n    author = \"Chhun, Cyril  and\n      Colombo, Pierre  and\n      Suchanek, Fabian M.  and\n      Clavel, Chlo{\\'e}\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.509/\",\n    pages = \"5794--5836\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.509.pdf",
        "site": "https://aclanthology.org/2022.coling-1.509/",
        "pdf_size": 12302214,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2489873938105964049&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris; Lab of Mathematics and Informatics (MICS), CentraleSup\u00e9lec, Universit\u00e9 Paris-Saclay + Laboratoire des Signaux et Syst\u00e8mes (L2S), CentraleSup\u00e9lec, CNRS, Universit\u00e9 Paris-Saclay; LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris; LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris",
        "aff_domain": "telecom-paris.fr; ; ; ",
        "email": "telecom-paris.fr; ; ; ",
        "github": "https://github.com/dig-team/hanna-benchmark-asg",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+1;0;0",
        "aff_unique_norm": "T\u00e9l\u00e9com Paris;CentraleSup\u00e9lec",
        "aff_unique_dep": "LTCI;Lab of Mathematics and Informatics (MICS)",
        "aff_unique_url": "https://www.telecom-paris.fr;https://www.centralesupelec.fr",
        "aff_unique_abbr": "T\u00e9l\u00e9com Paris;CentraleSup\u00e9lec",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.coling-1.575",
        "title": "Offensive Content Detection via Synthetic Code-Switched Text",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The prevalent use of offensive content in social media has become an important reason for concern for online platforms (customer service chat-boxes, social media platforms, etc). Classifying offensive and hate-speech content in online settings is an essential task in many applications that needs to be addressed accordingly. However, online text from online platforms can contain code-switching, a combination of more than one language. The non-availability of labeled code-switched data for low-resourced code-switching combinations adds difficulty to this problem. To overcome this, we release a real-world dataset containing around 10k samples for testing for three language combinations en-fr, en-es, and en-de, and a synthetic code-switched textual dataset containing ~30k samples for training In this paper, we describe the process for gathering the human-generated data and our algorithm for creating synthetic code-switched offensive content data. We also introduce the results of a keyword classification baseline and a multi-lingual transformer-based classification model.",
        "author": "Cesa Salaam; Franck Dernoncourt; Trung Bui; Danda Rawat; Seunghyun Yoon",
        "authorids": "/c/cesa-salaam/; /f/franck-dernoncourt/; /t/trung-bui/; /d/danda-rawat/; /s/seunghyun-yoon/",
        "bibtex": "@inproceedings{salaam-etal-2022-offensive,\n    title = \"Offensive Content Detection via Synthetic Code-Switched Text\",\n    author = \"Salaam, Cesa  and\n      Dernoncourt, Franck  and\n      Bui, Trung  and\n      Rawat, Danda  and\n      Yoon, Seunghyun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.575/\",\n    pages = \"6617--6624\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.575.pdf",
        "site": "https://aclanthology.org/2022.coling-1.575/",
        "pdf_size": 455006,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5828795029724455559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Howard University; Adobe Research; Adobe Research; Howard University; Adobe Research",
        "aff_domain": "bison.howard.edu;adobe.com;adobe.com;howard.edu;adobe.com",
        "email": "bison.howard.edu;adobe.com;adobe.com;howard.edu;adobe.com",
        "github": "",
        "project": "https://tinyurl.com/adobehuman; https://tinyurl.com/adobesynthetic",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "Howard University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.howard.edu;https://research.adobe.com",
        "aff_unique_abbr": "HU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.445",
        "title": "On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-Training (PT) of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains (some- times, even worse) on resource-rich NMT on par with its Random-Initialization (RI) counterpart. We take the first step to investigate the complementarity between PT and RI in resource-rich scenarios via two probing analyses, and find that: 1) PT improves NOT the accuracy, but the generalization by achieving flatter loss landscapes than that of RI; 2) PT improves NOT the confidence of lexical choice, but the negative diversity by assigning smoother lexical probability distributions than that of RI. Based on these insights, we propose to combine their complementarities with a model fusion algorithm that utilizes optimal transport to align neurons between PT and RI. Experiments on two resource-rich translation benchmarks, WMT\u201917 English-Chinese (20M) and WMT\u201919 English-German (36M), show that PT and RI could be nicely complementary to each other, achieving substantial improvements considering both translation accuracy, generalization, and negative diversity. Probing tools and code are released at: https://github.com/zanchangtong/PTvsRI.",
        "author": "Changtong Zan; Liang Ding; Li Shen; Yu Cao; Weifeng Liu; Dacheng Tao",
        "authorids": "/c/changtong-zan/; /l/liang-ding/; /l/li-shen/; /y/yu-cao/; /w/weifeng-liu/; /d/dacheng-tao/",
        "bibtex": "@inproceedings{zan-etal-2022-complementarity,\n    title = \"On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation\",\n    author = \"Zan, Changtong  and\n      Ding, Liang  and\n      Shen, Li  and\n      Cao, Yu  and\n      Liu, Weifeng  and\n      Tao, Dacheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.445/\",\n    pages = \"5029--5034\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.445.pdf",
        "site": "https://aclanthology.org/2022.coling-1.445/",
        "pdf_size": 483641,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11390028112728577750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Control Science and Engineering, China University of Petroleum (East China); JD Explore Academy; JD Explore Academy; The University of Sydney; College of Control Science and Engineering, China University of Petroleum (East China); JD Explore Academy+The University of Sydney",
        "aff_domain": "s.upc.edu.cn;jd.com; ; ; ; ",
        "email": "s.upc.edu.cn;jd.com; ; ; ; ",
        "github": "https://github.com/zanchangtong/PTvsRI",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2;0;1+2",
        "aff_unique_norm": "China University of Petroleum (East China);JD Explore Academy;University of Sydney",
        "aff_unique_dep": "College of Control Science and Engineering;;",
        "aff_unique_url": "http://www.cup.edu.cn;;https://www.sydney.edu.au",
        "aff_unique_abbr": "CUP;;USYD",
        "aff_campus_unique_index": "0;0;",
        "aff_campus_unique": "East China;",
        "aff_country_unique_index": "0;2;0;2",
        "aff_country_unique": "China;;Australia"
    },
    {
        "id": "2022.coling-1.275",
        "title": "On the Nature of BERT: Correlating Fine-Tuning and Linguistic Competence",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Several studies in the literature on the interpretation of Neural Language Models (NLM) focus on the linguistic generalization abilities of pre-trained models. However, little attention is paid to how the linguistic knowledge of the models changes during the fine-tuning steps. In this paper, we contribute to this line of research by showing to what extent a wide range of linguistic phenomena are forgotten across 50 epochs of fine-tuning, and how the preserved linguistic knowledge is correlated with the resolution of the fine-tuning task. To this end, we considered a quite understudied task where linguistic information plays the main role, i.e. the prediction of the evolution of written language competence of native language learners. In addition, we investigate whether it is possible to predict the fine-tuned NLM accuracy across the 50 epochs solely relying on the assessed linguistic competence. Our results are encouraging and show a high relationship between the model\u2019s linguistic competence and its ability to solve a linguistically-based downstream task.",
        "author": "Federica Merendi; Felice Dell\u2019Orletta; Giulia Venturi",
        "authorids": "/f/federica-merendi/; /f/felice-dellorletta/; /g/giulia-venturi/",
        "bibtex": "@inproceedings{merendi-etal-2022-nature,\n    title = \"On the Nature of {BERT}: Correlating Fine-Tuning and Linguistic Competence\",\n    author = \"Merendi, Federica  and\n      Dell{'}Orletta, Felice  and\n      Venturi, Giulia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.275/\",\n    pages = \"3109--3119\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.275.pdf",
        "site": "https://aclanthology.org/2022.coling-1.275/",
        "pdf_size": 1622377,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9528297366185886424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of Pisa; Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d, Pisa; Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d, Pisa",
        "aff_domain": "studenti.unipi.it;ilc.cnr.it;ilc.cnr.it",
        "email": "studenti.unipi.it;ilc.cnr.it;ilc.cnr.it",
        "github": "",
        "project": "www.italianlp.it",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Pisa;Istituto di Linguistica Computazionale \"Antonio Zampolli\"",
        "aff_unique_dep": "Department of Computer Science;Linguistica Computazionale",
        "aff_unique_url": "https://www.unipi.it;http://www.ilc.cnr.it/",
        "aff_unique_abbr": "UNIPi;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pisa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.coling-1.567",
        "title": "On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Word ordering is a constrained language generation task taking unordered words as input. Existing work uses linear models and neural networks for the task, yet pre-trained language models have not been studied in word ordering, let alone why they help. We use BART as an instance and show its effectiveness in the task. To explain why BART helps word ordering, we extend analysis with probing and empirically identify that syntactic dependency knowledge in BART is a reliable explanation. We also report performance gains with BART in the related partial tree linearization task, which readily extends our analysis.",
        "author": "Zebin Ou; Meishan Zhang; Yue Zhang",
        "authorids": "/z/zebin-ou/; /m/meishan-zhang/; /y/yue-zhang/",
        "bibtex": "@inproceedings{ou-etal-2022-role,\n    title = \"On the Role of Pre-trained Language Models in Word Ordering: A Case Study with {BART}\",\n    author = \"Ou, Zebin  and\n      Zhang, Meishan  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.567/\",\n    pages = \"6516--6529\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.567.pdf",
        "site": "https://aclanthology.org/2022.coling-1.567/",
        "pdf_size": 1453468,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4100427066396439192&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Engineering, Westlake University; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen); Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;gmail.com",
        "email": "westlake.edu.cn;westlake.edu.cn;gmail.com",
        "github": "https://github.com/simtony/BART-word-orderer",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Westlake University;Harbin Institute of Technology;Westlake Institute for Advanced Study",
        "aff_unique_dep": "School of Engineering;Institute of Computing and Intelligence;Institute of Advanced Technology",
        "aff_unique_url": "https://www.westlake.edu.cn;http://www.hit.edu.cn/;http://www.wias.org.cn/",
        "aff_unique_abbr": ";HIT;WIAS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.347",
        "title": "One Word, Two Sides: Traces of Stance in Contextualized Word Representations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The way we use words is influenced by our opinion. We investigate whether this is reflected in contextualized word embeddings. For example, is the representation of \u201canimal\u201d different between people who would abolish zoos and those who would not? We explore this question from a Lexical Semantic Change standpoint. Our experiments with BERT embeddings derived from datasets with stance annotations reveal small but significant differences in word representations between opposing stances.",
        "author": "Aina Gar\u00ed Soler; Matthieu Labeau; Chlo\u00e9 Clavel",
        "authorids": "/a/aina-gari-soler/; /m/matthieu-labeau/; /c/chloe-clavel/",
        "bibtex": "@inproceedings{gari-soler-etal-2022-one,\n    title = \"One Word, Two Sides: Traces of Stance in Contextualized Word Representations\",\n    author = \"Gar{\\'i} Soler, Aina  and\n      Labeau, Matthieu  and\n      Clavel, Chlo{\\'e}\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.347/\",\n    pages = \"3950--3959\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.347.pdf",
        "site": "https://aclanthology.org/2022.coling-1.347/",
        "pdf_size": 487773,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4140083237813041039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "LTCI, T\u00e9l\u00e9com-Paris, Institut Polytechnique de Paris, France; LTCI, T\u00e9l\u00e9com-Paris, Institut Polytechnique de Paris, France; LTCI, T\u00e9l\u00e9com-Paris, Institut Polytechnique de Paris, France",
        "aff_domain": "telecom-paris.fr;telecom-paris.fr;telecom-paris.fr",
        "email": "telecom-paris.fr;telecom-paris.fr;telecom-paris.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "T\u00e9l\u00e9com-Paris",
        "aff_unique_dep": "LTCI",
        "aff_unique_url": "https://www.telecom-paris.fr",
        "aff_unique_abbr": "T\u00e9l\u00e9com-Paris",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.coling-1.614",
        "title": "One-Teacher and Multiple-Student Knowledge Distillation on Sentiment Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge distillation is an effective method to transfer knowledge from a large pre-trained teacher model to a compacted student model. However, in previous studies, the distilled student models are still large and remain impractical in highly speed-sensitive systems (e.g., an IR system). In this study, we aim to distill a deep pre-trained model into an extremely compacted shallow model like CNN. Specifically, we propose a novel one-teacher and multiple-student knowledge distillation approach to distill a deep pre-trained teacher model into multiple shallow student models with ensemble learning. Moreover, we leverage large-scale unlabeled data to improve the performance of students. Empirical studies on three sentiment classification tasks demonstrate that our approach achieves better results with much fewer parameters (0.9%-18%) and extremely high speedup ratios (100X-1000X).",
        "author": "Xiaoqin Chang; Sophia Yat Mei Lee; Suyang Zhu; Shoushan Li; Guodong Zhou",
        "authorids": "/x/xiaoqin-chang/; /s/sophia-yat-mei-lee/; /s/suyang-zhu/; /s/shoushan-li/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{chang-etal-2022-one,\n    title = \"One-Teacher and Multiple-Student Knowledge Distillation on Sentiment Classification\",\n    author = \"Chang, Xiaoqin  and\n      Lee, Sophia Yat Mei  and\n      Zhu, Suyang  and\n      Li, Shoushan  and\n      Zhou, Guodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.614/\",\n    pages = \"7042--7052\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.614.pdf",
        "site": "https://aclanthology.org/2022.coling-1.614/",
        "pdf_size": 542772,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12125014530855655925&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Natural Language Processing Lab, Soochow University, China; Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University; Natural Language Processing Lab, Soochow University, China; Natural Language Processing Lab, Soochow University, China; Natural Language Processing Lab, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;polyu.edu.hk;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn",
        "email": "stu.suda.edu.cn;polyu.edu.hk;stu.suda.edu.cn;stu.suda.edu.cn;stu.suda.edu.cn",
        "github": "https://github.com/strive-hhh/OTMS-KD",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Soochow University;The Hong Kong Polytechnic University",
        "aff_unique_dep": "Natural Language Processing Lab;Department of Chinese and Bilingual Studies",
        "aff_unique_url": "https://www.soochow.edu.cn;https://www.polyu.edu.hk",
        "aff_unique_abbr": ";PolyU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.170",
        "title": "OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Event extraction (EE) is an essential task of information extraction, which aims to extract structured event information from unstructured text. Most prior work focuses on extracting flat events while neglecting overlapped or nested ones. A few models for overlapped and nested EE includes several successive stages to extract event triggers and arguments,which suffer from error propagation. Therefore, we design a simple yet effective tagging scheme and model to formulate EE as word-word relation recognition, called OneEE. The relations between trigger or argument words are simultaneously recognized in one stage with parallel grid tagging, thus yielding a very fast event extraction speed. The model is equipped with an adaptive event fusion module to generate event-aware representations and a distance-aware predictor to integrate relative distance information for word-word relation recognition, which are empirically demonstrated to be effective mechanisms. Experiments on 3 overlapped and nested EE benchmarks, namely FewFC, Genia11, and Genia13, show that OneEE achieves the state-of-the-art (SOTA) results. Moreover, the inference speed of OneEE is faster than those of baselines in the same condition, and can be further substantially improved since it supports parallel inference.",
        "author": "Hu Cao; Jingye Li; Fangfang Su; Fei Li; Hao Fei; Shengqiong Wu; Bobo Li; Liang Zhao; Donghong Ji",
        "authorids": "/h/hu-cao/; /j/jingye-li/; /f/fangfang-su/; /f/fei-li/; /h/hao-fei/; /s/shengqiong-wu/; /b/bobo-li/; /l/liang-zhao/; /d/donghong-ji/",
        "bibtex": "@inproceedings{cao-etal-2022-oneee,\n    title = \"{O}ne{EE}: A One-Stage Framework for Fast Overlapping and Nested Event Extraction\",\n    author = \"Cao, Hu  and\n      Li, Jingye  and\n      Su, Fangfang  and\n      Li, Fei  and\n      Fei, Hao  and\n      Wu, Shengqiong  and\n      Li, Bobo  and\n      Zhao, Liang  and\n      Ji, Donghong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.170/\",\n    pages = \"1953--1964\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.170.pdf",
        "site": "https://aclanthology.org/2022.coling-1.170/",
        "pdf_size": 812147,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11663466181632339702&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Department of Computing and Mathematics, University of S\u00e3o Paulo, Brazil; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China",
        "aff_domain": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn; ; ;whu.edu.cn; ; ",
        "email": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn; ; ;whu.edu.cn; ; ",
        "github": "https://github.com/Cao-Hu/OneEE",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;1;1;0;2;0",
        "aff_unique_norm": "Wuhan University;National University of Singapore;University of S\u00e3o Paulo",
        "aff_unique_dep": "School of Cyber Science and Engineering;School of Computing;Department of Computing and Mathematics",
        "aff_unique_url": "http://www.whu.edu.cn/;https://www.nus.edu.sg;https://www.usp.br",
        "aff_unique_abbr": "WHU;NUS;USP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;1;0;2;0",
        "aff_country_unique": "China;Singapore;Brazil"
    },
    {
        "id": "2022.coling-1.40",
        "title": "Open-Domain Dialog Evaluation Using Follow-Ups Likelihood",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatic evaluation of open-domain dialogs remains an unsolved problem. Existing methods do not correlate strongly with human annotations. In this paper, we present a new automated evaluation method based on the use of follow-ups. We measure the probability that a language model will continue the conversation with a fixed set of follow-ups (e.g. not really relevant here, what are you trying to say?). When compared against twelve existing methods, our new evaluation achieves the highest correlation with human evaluations.",
        "author": "Maxime De Bruyn; Ehsan Lotfi; Jeska Buhmann; Walter Daelemans",
        "authorids": "/m/maxime-de-bruyn/; /e/ehsan-lotfi/; /j/jeska-buhmann/; /w/walter-daelemans/",
        "bibtex": "@inproceedings{de-bruyn-etal-2022-open,\n    title = \"Open-Domain Dialog Evaluation Using Follow-Ups Likelihood\",\n    author = \"De Bruyn, Maxime  and\n      Lotfi, Ehsan  and\n      Buhmann, Jeska  and\n      Daelemans, Walter\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.40/\",\n    pages = \"496--504\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.40.pdf",
        "site": "https://aclanthology.org/2022.coling-1.40/",
        "pdf_size": 352464,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9419919506341199674&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "CLiPS Research Center, University of Antwerp, Belgium; CLiPS Research Center, University of Antwerp, Belgium; CLiPS Research Center, University of Antwerp, Belgium; CLiPS Research Center, University of Antwerp, Belgium",
        "aff_domain": "uantwerpen.be; ; ; ",
        "email": "uantwerpen.be; ; ; ",
        "github": "https://github.com/maximedb/full",
        "project": "https://pypi.org/project/full/",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Antwerp",
        "aff_unique_dep": "CLiPS Research Center",
        "aff_unique_url": "https://www.uantwerp.be/en",
        "aff_unique_abbr": "UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2022.coling-1.433",
        "title": "OpticE: A Coherence Theory-Based Model for Link Prediction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge representation learning is a key step required for link prediction tasks with knowledge graphs (KGs). During the learning process, the semantics of each entity are embedded by a vector or a point in a feature space. The distance between these points is a measure of semantic similarity. However, in a KG, while two entities may have similar semantics in some relations, they have different semantics in others. It is ambiguous to assign a fixed distance to depict the variant semantic similarity of entities. To alleviate the semantic ambiguity in KGs, we design a new embedding approach named OpticE, which is derived from the well-known physical phenomenon of optical interference. It is a lightweight and relation-adaptive model based on coherence theory, in which each entity\u2019s semantics vary automatically regarding different relations. In addition, a unique negative sampling method is proposed to combine the multimapping properties and self-adversarial learning during the training process. The experimental results obtained on practical KG benchmarks show that the OpticE model, with elegant structures, can compete with existing link prediction methods.",
        "author": "Xiangyu Gui; Feng Zhao; Langjunqing Jin; Hai Jin",
        "authorids": "/x/xiangyu-gui/; /f/feng-zhao/; /l/langjunqing-jin/; /h/hai-jin/",
        "bibtex": "@inproceedings{gui-etal-2022-optice,\n    title = \"{O}ptic{E}: A Coherence Theory-Based Model for Link Prediction\",\n    author = \"Gui, Xiangyu  and\n      Zhao, Feng  and\n      Jin, Langjunqing  and\n      Jin, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.433/\",\n    pages = \"4892--4901\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.433.pdf",
        "site": "https://aclanthology.org/2022.coling-1.433/",
        "pdf_size": 488560,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4527270040997436027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China",
        "aff_domain": "hust.edu.cn; ; ; ",
        "email": "hust.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.208",
        "title": "Optimal Partial Transport Based Sentence Selection for Long-form Document Matching",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "One typical approach to long-form document matching is first conducting alignment between cross-document sentence pairs, and then aggregating all of the sentence-level matching signals. However, this approach could be problematic because the alignment between documents is partial \u2014 despite two documents as a whole are well-matched, most of the sentences could still be dissimilar. Those dissimilar sentences lead to spurious sentence-level matching signals which may overwhelm the real ones, increasing the difficulties of learning the matching function. Therefore, accurately selecting the key sentences for document matching is becoming a challenging issue. To address the issue, we propose a novel matching approach that equips existing document matching models with an Optimal Partial Transport (OPT) based component, namely OPT-Match, which selects the sentences that play a major role in matching. Enjoying the partial transport properties of OPT, the selected key sentences can not only effectively enhance the matching accuracy, but also be explained as the rationales for the matching results. Extensive experiments on four publicly available datasets demonstrated that existing methods equipped with OPT-Match consistently outperformed the corresponding underlying methods. Evaluations also showed that the key sentences selected by OPT-Match were consistent with human-provided rationales.",
        "author": "Weijie Yu; Liang Pang; Jun Xu; Bing Su; Zhenhua Dong; Ji-Rong Wen",
        "authorids": "/w/weijie-yu/; /l/liang-pang/; /j/jun-xu/; /b/bing-su/; /z/zhenhua-dong/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{yu-etal-2022-optimal,\n    title = \"Optimal Partial Transport Based Sentence Selection for Long-form Document Matching\",\n    author = \"Yu, Weijie  and\n      Pang, Liang  and\n      Xu, Jun  and\n      Su, Bing  and\n      Dong, Zhenhua  and\n      Wen, Ji-Rong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.208/\",\n    pages = \"2363--2373\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.208.pdf",
        "site": "https://aclanthology.org/2022.coling-1.208/",
        "pdf_size": 1402396,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13589742623155850321&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Information, Renmin University of China; Institute of Computing Technology, Chinese Academy of Sciences; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Noah\u2019s Ark Lab, Huawei; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_domain": "ruc.edu.cn;ict.ac.cn;ruc.edu.cn;ruc.edu.cn;huawei.com;ruc.edu.cn",
        "email": "ruc.edu.cn;ict.ac.cn;ruc.edu.cn;ruc.edu.cn;huawei.com;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0+2;0+2;3;0+2",
        "aff_unique_norm": "Renmin University of China;Chinese Academy of Sciences;Beijing Key Laboratory of Big Data Management and Analysis Methods;Huawei",
        "aff_unique_dep": "School of Information;Institute of Computing Technology;Big Data Management and Analysis;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.ruc.edu.cn;http://www.ict.ac.cn;;https://www.huawei.com",
        "aff_unique_abbr": "RUC;CAS;;Huawei",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.133",
        "title": "Original Content Is All You Need! an Empirical Study on Leveraging Answer Summary for WikiHowQA Answer Selection Task",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Answer selection task requires finding appropriate answers to questions from informative but crowdsourced candidates. A key factor impeding its solution by current answer selection approaches is the redundancy and lengthiness issues of crowdsourced answers. Recently, Deng et al. (2020) constructed a new dataset, WikiHowQA, which contains a corresponding reference summary for each original lengthy answer. And their experiments show that leveraging the answer summaries helps to attend the essential information in original lengthy answers and improve the answer selection performance under certain circumstances. However, when given a question and a set of long candidate answers, human beings could effortlessly identify the correct answer without the aid of additional answer summaries since the original answers contain all the information volume that answer summaries contain. In addition, pretrained language models have been shown superior or comparable to human beings on many natural language processing tasks. Motivated by those, we design a series of neural models, either pretraining-based or non-pretraining-based, to check wether the additional answer summaries are helpful for ranking the relevancy degrees of question-answer pairs on WikiHowQA dataset. Extensive automated experiments and hand analysis show that the additional answer summaries are not useful for achieving the best performance.",
        "author": "Liang Wen; Juan Li; Houfeng Wang; Yingwei Luo; Xiaolin Wang; Xiaodong Zhang; Zhicong Cheng; Dawei Yin",
        "authorids": "/l/liang-wen/; /j/juan-li/; /h/houfeng-wang/; /y/yingwei-luo/; /x/xiaolin-wang/; /x/xiaodong-zhang/; /z/zhicong-cheng/; /d/dawei-yin/",
        "bibtex": "@inproceedings{wen-etal-2022-original,\n    title = \"Original Content Is All You Need! an Empirical Study on Leveraging Answer Summary for {W}iki{H}ow{QA} Answer Selection Task\",\n    author = \"Wen, Liang  and\n      Li, Juan  and\n      Wang, Houfeng  and\n      Luo, Yingwei  and\n      Wang, Xiaolin  and\n      Zhang, Xiaodong  and\n      Cheng, Zhicong  and\n      Yin, Dawei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.133/\",\n    pages = \"1546--1555\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.133.pdf",
        "site": "https://aclanthology.org/2022.coling-1.133/",
        "pdf_size": 379118,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NBW4mbdG5RAJ:scholar.google.com/&scioq=Original+Content+Is+All+You+Need!+an+Empirical+Study+on+Leveraging+Answer+Summary+for+WikiHowQA+Answer+Selection+Task&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "http://www.wikihow.com",
        "author_num": 8
    },
    {
        "id": "2022.coling-1.503",
        "title": "Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the great progress of Visual Question Answering (VQA), current VQA models heavily rely on the superficial correlation between the question type and its corresponding frequent answers (i.e., language priors) to make predictions, without really understanding the input. In this work, we define the training instances with the same question type but different answers as superficially similar instances, and attribute the language priors to the confusion of VQA model on such instances. To solve this problem, we propose a novel training framework that explicitly encourages the VQA model to distinguish between the superficially similar instances. Specifically, for each training instance, we first construct a set that contains its superficially similar counterparts. Then we exploit the proposed distinguishing module to increase the distance between the instance and its counterparts in the answer space. In this way, the VQA model is forced to further focus on the other parts of the input beyond the question type, which helps to overcome the language priors. Experimental results show that our method achieves the state-of-the-art performance on VQA-CP v2. Codes are available at Distinguishing-VQA.",
        "author": "Yike Wu; Yu Zhao; Shiwan Zhao; Ying Zhang; Xiaojie Yuan; Guoqing Zhao; Ning Jiang",
        "authorids": "/y/yike-wu/; /y/yu-zhao/; /s/shiwan-zhao/; /y/ying-zhang/; /x/xiaojie-yuan/; /g/guoqing-zhao/; /n/ning-jiang/",
        "bibtex": "@inproceedings{wu-etal-2022-overcoming,\n    title = \"Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances\",\n    author = \"Wu, Yike  and\n      Zhao, Yu  and\n      Zhao, Shiwan  and\n      Zhang, Ying  and\n      Yuan, Xiaojie  and\n      Zhao, Guoqing  and\n      Jiang, Ning\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.503/\",\n    pages = \"5721--5729\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.503.pdf",
        "site": "https://aclanthology.org/2022.coling-1.503/",
        "pdf_size": 1068681,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13467615000635071962&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Nankai University, Tianjin, China; Nankai University, Tianjin, China; Nankai University, Tianjin, China; Nankai University, Tianjin, China; Nankai University, Tianjin, China; Mashang Consumer Finance Co, Ltd; Mashang Consumer Finance Co, Ltd",
        "aff_domain": "nankai.edu.cn;dbis.nankai.edu.cn;gmail.com;nankai.edu.cn;nankai.edu.cn; ; ",
        "email": "nankai.edu.cn;dbis.nankai.edu.cn;gmail.com;nankai.edu.cn;nankai.edu.cn; ; ",
        "github": "",
        "project": "Distinguishing-VQA",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;1",
        "aff_unique_norm": "Nankai University;Mashang Consumer Finance",
        "aff_unique_dep": ";Consumer Finance",
        "aff_unique_url": "http://www.nankai.edu.cn;",
        "aff_unique_abbr": "NKU;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Tianjin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.451",
        "title": "PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While end-to-end neural machine translation (NMT) has achieved impressive progress, noisy input usually leads models to become fragile and unstable. Generating adversarial examples as the augmented data has been proved to be useful to alleviate this problem. Existing methods for adversarial example generation (AEG) are word-level or character-level, which ignore the ubiquitous phrase structure. In this paper, we propose a Phrase-level Adversarial Example Generation (PAEG) framework to enhance the robustness of the translation model. Our method further improves the gradient-based word-level AEG method by adopting a phrase-level substitution strategy. We verify our method on three benchmarks, including LDC Chinese-English, IWSLT14 German-English, and WMT14 English-German tasks. Experimental results demonstrate that our approach significantly improves translation performance and robustness to noise compared to previous strong baselines.",
        "author": "Juncheng Wan; Jian Yang; Shuming Ma; Dongdong Zhang; Weinan Zhang; Yong Yu; Zhoujun Li",
        "authorids": "/j/juncheng-wan/; /j/jian-yang/; /s/shuming-ma/; /d/dongdong-zhang/; /w/weinan-zhang/; /y/yong-yu/; /z/zhoujun-li/",
        "bibtex": "@inproceedings{wan-etal-2022-paeg,\n    title = \"{PAEG}: Phrase-level Adversarial Example Generation for Neural Machine Translation\",\n    author = \"Wan, Juncheng  and\n      Yang, Jian  and\n      Ma, Shuming  and\n      Zhang, Dongdong  and\n      Zhang, Weinan  and\n      Yu, Yong  and\n      Li, Zhoujun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.451/\",\n    pages = \"5085--5097\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.451.pdf",
        "site": "https://aclanthology.org/2022.coling-1.451/",
        "pdf_size": 632648,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11745009773461808828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University; State Key Lab of Software Development Environment, Beihang University; Microsoft Research Asia; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; State Key Lab of Software Development Environment, Beihang University",
        "aff_domain": "apex.sjtu.edu.cn;buaa.edu.cn;microsoft.com;microsoft.com;apex.sjtu.edu.cn;apex.sjtu.edu.cn;buaa.edu.cn",
        "email": "apex.sjtu.edu.cn;buaa.edu.cn;microsoft.com;microsoft.com;apex.sjtu.edu.cn;apex.sjtu.edu.cn;buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;0;0;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Beihang University;Microsoft Research",
        "aff_unique_dep": ";State Key Lab of Software Development Environment;Research",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.buaa.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "SJTU;Beihang;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.423",
        "title": "PARSE: An Efficient Search Method for Black-box Adversarial Text Attacks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural networks are vulnerable to adversarial examples. The adversary can successfully attack a model even without knowing model architecture and parameters, i.e., under a black-box scenario. Previous works on word-level attacks widely use word importance ranking (WIR) methods and complex search methods, including greedy search and heuristic algorithms, to find optimal substitutions. However, these methods fail to balance the attack success rate and the cost of attacks, such as the number of queries to the model and the time consumption. In this paper, We propose PAthological woRd Saliency sEarch (PARSE) that performs the search under dynamic search space following the subarea importance. Experiments show that PARSE can achieve comparable attack success rates to complex search methods while saving numerous queries and time, e.g., saving at most 74% of queries and 90% of time compared with greedy search when attacking the examples from Yelp dataset. The adversarial examples crafted by PARSE are also of high quality, highly transferable, and can effectively improve model robustness in adversarial training.",
        "author": "Pengwei Zhan; Chao Zheng; Jing Yang; Yuxiang Wang; Liming Wang; Yang Wu; Yunjian Zhang",
        "authorids": "/p/pengwei-zhan/; /c/chao-zheng/; /j/jing-yang/; /y/yuxiang-wang/; /l/liming-wang/; /y/yang-wu/; /y/yunjian-zhang/",
        "bibtex": "@inproceedings{zhan-etal-2022-parse,\n    title = \"{PARSE}: An Efficient Search Method for Black-box Adversarial Text Attacks\",\n    author = \"Zhan, Pengwei  and\n      Zheng, Chao  and\n      Yang, Jing  and\n      Wang, Yuxiang  and\n      Wang, Liming  and\n      Wu, Yang  and\n      Zhang, Yunjian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.423/\",\n    pages = \"4776--4787\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.423.pdf",
        "site": "https://aclanthology.org/2022.coling-1.423/",
        "pdf_size": 679092,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13851920454188330024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.192",
        "title": "PCBERT: Parent and Child BERT for Chinese Few-shot NER",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Achieving good performance on few-shot or zero-shot datasets has been a long-term challenge for NER. The conventional semantic transfer approaches on NER will decrease model performance when the semantic distribution is quite different, especially in Chinese few-shot NER. Recently, prompt-tuning has been thoroughly considered for low-resource tasks. But there is no effective prompt-tuning approach for Chinese few-shot NER. In this work, we propose a prompt-based Parent and Child BERT (PCBERT) for Chinese few-shot NER. To train an annotating model on high-resource datasets and then discover more implicit labels on low-resource datasets. We further design a label extension strategy to achieve label transferring from high-resource datasets. We evaluated our model on Weibo and the other three sampling Chinese NER datasets, and the experimental result demonstrates our approach\u2019s effectiveness in few-shot learning.",
        "author": "Peichao Lai; Feiyang Ye; Lin Zhang; Zhiwei Chen; Yanggeng Fu; Yingjie Wu; Yilei Wang",
        "authorids": "/p/peichao-lai/; /f/feiyang-ye/; /l/lin-zhang/; /z/zhiwei-chen/; /y/yanggeng-fu/; /y/yingjie-wu/; /y/yilei-wang/",
        "bibtex": "@inproceedings{lai-etal-2022-pcbert,\n    title = \"{PCBERT}: Parent and Child {BERT} for {C}hinese Few-shot {NER}\",\n    author = \"Lai, Peichao  and\n      Ye, Feiyang  and\n      Zhang, Lin  and\n      Chen, Zhiwei  and\n      Fu, Yanggeng  and\n      Wu, Yingjie  and\n      Wang, Yilei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.192/\",\n    pages = \"2199--2209\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.192.pdf",
        "site": "https://aclanthology.org/2022.coling-1.192/",
        "pdf_size": 4369626,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1620508590688133465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer and Data Science, Fuzhou University, China; College of Computer and Data Science, Fuzhou University, China; College of Computer and Data Science, Fuzhou University, China; College of Computer and Data Science, Fuzhou University, China; College of Computer and Data Science, Fuzhou University, China; College of Computer and Data Science, Fuzhou University, China; College of Computer and Data Science, Fuzhou University, China",
        "aff_domain": "fzu.edu.cn; ; ; ; ; ;fzu.edu.cn",
        "email": "fzu.edu.cn; ; ; ; ; ;fzu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Fuzhou University",
        "aff_unique_dep": "College of Computer and Data Science",
        "aff_unique_url": "https://www.fzu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.34",
        "title": "PEPDS: A Polite and Empathetic Persuasive Dialogue System for Charity Donation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Persuasive conversations for a social cause often require influencing other person\u2019s attitude or intention that may fail even with compelling arguments. The use of emotions and different types of polite tones as needed with facts may enhance the persuasiveness of a message. To incorporate these two aspects, we propose a polite, empathetic persuasive dialogue system (PEPDS). First, in a Reinforcement Learning setting, a Maximum Likelihood Estimation loss based model is fine-tuned by designing an efficient reward function consisting of five different sub rewards viz. Persuasion, Emotion, Politeness-Strategy Consistency, Dialogue-Coherence and Non-repetitiveness. Then, to generate empathetic utterances for non-empathetic ones, an Empathetic transfer model is built upon the RL fine-tuned model. Due to the unavailability of an appropriate dataset, by utilizing the PERSUASIONFORGOOD dataset, we create two datasets, viz. EPP4G and ETP4G. EPP4G is used to train three transformer-based classification models as per persuasiveness, emotion and politeness strategy to achieve respective reward feedbacks. The ETP4G dataset is used to train an empathetic transfer model. Our experimental results demonstrate that PEPDS increases the rate of persuasive responses with emotion and politeness acknowledgement compared to the current state-of-the-art dialogue models, while also enhancing the dialogue\u2019s engagement and maintaining the linguistic quality.",
        "author": "Kshitij Mishra; Azlaan Mustafa Samad; Palak Totala; Asif Ekbal",
        "authorids": "/k/kshitij-mishra/; /a/azlaan-mustafa-samad/; /p/palak-totala/; /a/asif-ekbal/",
        "bibtex": "@inproceedings{mishra-etal-2022-pepds,\n    title = \"{PEPDS}: A Polite and Empathetic Persuasive Dialogue System for Charity Donation\",\n    author = \"Mishra, Kshitij  and\n      Samad, Azlaan Mustafa  and\n      Totala, Palak  and\n      Ekbal, Asif\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.34/\",\n    pages = \"424--440\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.34.pdf",
        "site": "https://aclanthology.org/2022.coling-1.34/",
        "pdf_size": 5804330,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4707635122875291257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Patna; Department of Computer Science and Engineering, Indian Institute of Technology Patna; Department of Computer Science and Engineering, Indian Institute of Technology Patna; Department of Computer Science and Engineering, Indian Institute of Technology Patna",
        "aff_domain": "iitp.ac.in;gmail.com;iitp.ac.in;iitp.ac.in",
        "email": "iitp.ac.in;gmail.com;iitp.ac.in;iitp.ac.in",
        "github": "PEPDS_github",
        "project": "PEPDS_ai_nlp_ml",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Patna",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitp.ac.in",
        "aff_unique_abbr": "IIT Patna",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Patna",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.547",
        "title": "PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A personification is a figure of speech that endows inanimate entities with properties and actions typically seen as requiring animacy. In this paper, we explore the task of personification generation. To this end, we propose PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification data for Learning Enhanced generation. We curate a corpus of personifications called PersonifCorp, together with automatically generated de-personified literalizations of these personifications. We demonstrate the usefulness of this parallel corpus by training a seq2seq model to personify a given literal input. Both automatic and human evaluations show that fine-tuning with PersonifCorp leads to significant gains in personification-related qualities such as animacy and interestingness. A detailed qualitative analysis also highlights key strengths and imperfections of PINEAPPLE over baselines, demonstrating a strong ability to generate diverse and creative personifications that enhance the overall appeal of a sentence.",
        "author": "Sedrick Scott Keh; Kevin Lu; Varun Gangal; Steven Y. Feng; Harsh Jhamtani; Malihe Alikhani; Eduard Hovy",
        "authorids": "/s/sedrick-scott-keh/; /k/kevin-lu/; /v/varun-gangal/; /s/steven-y-feng/; /h/harsh-jhamtani/; /m/malihe-alikhani/; /e/eduard-hovy/",
        "bibtex": "@inproceedings{keh-etal-2022-pineapple,\n    title = \"{PINEAPPLE}: Personifying {IN}animate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation\",\n    author = \"Keh, Sedrick Scott  and\n      Lu, Kevin  and\n      Gangal, Varun  and\n      Feng, Steven Y.  and\n      Jhamtani, Harsh  and\n      Alikhani, Malihe  and\n      Hovy, Eduard\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.547/\",\n    pages = \"6270--6284\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.547.pdf",
        "site": "https://aclanthology.org/2022.coling-1.547/",
        "pdf_size": 834118,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5972907401059648701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University; University of Waterloo; Carnegie Mellon University+Stanford University; Stanford University; Carnegie Mellon University; University of Pittsburgh; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;uwaterloo.ca;cs.cmu.edu;stanford.edu;cs.cmu.edu;pitt.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;uwaterloo.ca;cs.cmu.edu;stanford.edu;cs.cmu.edu;pitt.edu;cs.cmu.edu",
        "github": "https://github.com/sedrickkeh/PINEAPPLE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0+2;2;0;3;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Waterloo;Stanford University;University of Pittsburgh",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cmu.edu;https://uwaterloo.ca;https://www.stanford.edu;https://www.pitt.edu",
        "aff_unique_abbr": "CMU;UW;Stanford;Pitt",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;1;0+0;0;0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2022.coling-1.553",
        "title": "PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot abstractive summarization has become a challenging task in natural language generation. To support it, we developed a novel soft prompts architecture coupled with a prompt pre-training plus prompt fine-tuning paradigm, which is effective and tunes only extremely light parameters. To meet the structure of the generation models, the soft prompts comprise continuous input embeddings across an encoder and a decoder. Importantly, a new inner-prompt placed in the text is introduced to capture document-level information. The aim is to devote attention to understanding the document that better prompts the model to generate document-related content. In the training process, the prompt pre-training with self-supervised pseudo-data firstly teaches the model basic summarizing capability. Then, with few-shot examples, only the designed lightweight soft prompts are fine-tuned. Experimental results on the CNN/DailyMail and XSum datasets show that our method, with only 0.1% of the parameters, outperforms full-model tuning where all model parameters are tuned. It also surpasses Prompt Tuning by a large margin and delivers competitive results against Prefix-Tuning with 3% of the parameters.",
        "author": "Xiaochen Liu; Yang Gao; Yu Bai; Jiawei Li; Yinan Hu; Heyan Huang; Boxing Chen",
        "authorids": "/x/xiaochen-liu/; /y/yang-gao/; /y/yu-bai/; /j/jiawei-li/; /y/yinan-hu/; /h/he-yan-huang/; /b/boxing-chen/",
        "bibtex": "@inproceedings{liu-etal-2022-psp,\n    title = \"{PSP}: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization\",\n    author = \"Liu, Xiaochen  and\n      Gao, Yang  and\n      Bai, Yu  and\n      Li, Jiawei  and\n      Hu, Yinan  and\n      Huang, Heyan  and\n      Chen, Boxing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.553/\",\n    pages = \"6355--6368\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.553.pdf",
        "site": "https://aclanthology.org/2022.coling-1.553/",
        "pdf_size": 1153189,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4525269142499007615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology",
        "aff_domain": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;gmail.com",
        "email": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.bit.edu.cn/",
        "aff_unique_abbr": "BIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.473",
        "title": "PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most existing slot filling models tend to memorize inherent patterns of entities and corresponding contexts from training data. However, these models can lead to system failure or undesirable outputs when being exposed to spoken language perturbation or variation in practice. We propose a perturbed semantic structure awareness transferring method for training perturbation-robust slot filling models. Specifically, we introduce two MLM-based training strategies to respectively learn contextual semantic structure and word distribution from unsupervised language perturbation corpus. Then, we transfer semantic knowledge learned from upstream training procedure into the original samples and filter generated data by consistency processing. These procedures aims to enhance the robustness of slot filling models. Experimental results show that our method consistently outperforms the previous basic methods and gains strong generalization while preventing the model from memorizing inherent patterns of entities and contexts.",
        "author": "Guanting Dong; Daichi Guo; Liwen Wang; Xuefeng Li; Zechen Wang; Chen Zeng; Keqing He; Jinzheng Zhao; Hao Lei; Xinyue Cui; Yi Huang; Junlan Feng; Weiran Xu",
        "authorids": "/g/guanting-dong/; /d/daichi-guo/; /l/liwen-wang/; /x/xuefeng-li/; /z/zechen-wang/; /c/chen-zeng/; /k/keqing-he/; /j/jinzheng-zhao/; /h/hao-lei/; /x/xinyue-cui/; /y/yi-huang/; /j/junlan-feng/; /w/weiran-xu/",
        "bibtex": "@inproceedings{dong-etal-2022-pssat,\n    title = \"{PSSAT}: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling\",\n    author = \"Dong, Guanting  and\n      Guo, Daichi  and\n      Wang, Liwen  and\n      Li, Xuefeng  and\n      Wang, Zechen  and\n      Zeng, Chen  and\n      He, Keqing  and\n      Zhao, Jinzheng  and\n      Lei, Hao  and\n      Cui, Xinyue  and\n      Huang, Yi  and\n      Feng, Junlan  and\n      Xu, Weiran\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.473/\",\n    pages = \"5327--5334\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.473.pdf",
        "site": "https://aclanthology.org/2022.coling-1.473/",
        "pdf_size": 317382,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7879993540223587053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Meituan Group; University of Surrey; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; China Mobile Research Institute; China Mobile Research Institute; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn; ; ; ; ; ; ; ; ; ; ; ; ",
        "email": "bupt.edu.cn; ; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 13,
        "aff_unique_index": "0;0;0;0;0;0;1;2;0;0;3;3;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan Group;University of Surrey;China Mobile",
        "aff_unique_dep": ";;;Research Institute",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com;https://www.surrey.ac.uk;https://www.chinamobile.com/",
        "aff_unique_abbr": "BUPT;Meituan;Surrey;CMRI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;1;0;0;0;0;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.coling-1.53",
        "title": "Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Real human conversation data are complicated, heterogeneous, and noisy, from which building open-domain dialogue systems remains a challenging task. In fact, such dialogue data still contains a wealth of information and knowledge, however, they are not fully explored. In this paper, we show existing open-domain dialogue generation methods that memorize context-response paired data with autoregressive or encode-decode language models underutilize the training data. Different from current approaches, using external knowledge, we explore a retrieval-generation training framework that can take advantage of the heterogeneous and noisy training data by considering them as \u201cevidence\u201d. In particular, we use BERTScore for retrieval, which gives better qualities of the evidence and generation. Experiments over publicly available datasets demonstrate that our method can help models generate better responses, even such training data are usually impressed as low-quality data. Such performance gain is comparable with those improved by enlarging the training set, even better. We also found that the model performance has a positive correlation with the relevance of the retrieved evidence. Moreover, our method performed well on zero-shot experiments, which indicates that our method can be more robust to real-world data.",
        "author": "Yihe Wang; Yitong Li; Yasheng Wang; Fei Mi; Pingyi Zhou; Xin Wang; Jin Liu; Xin Jiang; Qun Liu",
        "authorids": "/y/yihe-wang/; /y/yitong-li/; /y/yasheng-wang/; /f/fei-mi/; /p/pingyi-zhou/; /x/xin-wang/; /j/jin-liu/; /x/xin-jiang/; /q/qun-liu/",
        "bibtex": "@inproceedings{wang-etal-2022-pan,\n    title = \"Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation\",\n    author = \"Wang, Yihe  and\n      Li, Yitong  and\n      Wang, Yasheng  and\n      Mi, Fei  and\n      Zhou, Pingyi  and\n      Wang, Xin  and\n      Liu, Jin  and\n      Jiang, Xin  and\n      Liu, Qun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.53/\",\n    pages = \"636--647\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.53.pdf",
        "site": "https://aclanthology.org/2022.coling-1.53/",
        "pdf_size": 493021,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16734277689743977457&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Wuhan University + Noah\u2019s Ark Lab, Huawei; Noah\u2019s Ark Lab, Huawei + Huawei Technologies Ltd.; Noah\u2019s Ark Lab, Huawei; Noah\u2019s Ark Lab, Huawei; Noah\u2019s Ark Lab, Huawei; School of Computer Science, Wuhan University; School of Computer Science, Wuhan University; Noah\u2019s Ark Lab, Huawei; Noah\u2019s Ark Lab, Huawei",
        "aff_domain": "whu.edu.cn;huawei.com;huawei.com;huawei.com;huawei.com;whu.edu.cn;whu.edu.cn;huawei.com;huawei.com",
        "email": "whu.edu.cn;huawei.com;huawei.com;huawei.com;huawei.com;whu.edu.cn;whu.edu.cn;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1+2;1;1;1;0;0;1;1",
        "aff_unique_norm": "Wuhan University;Huawei;Huawei Technologies",
        "aff_unique_dep": "School of Computer Science;Noah\u2019s Ark Lab;",
        "aff_unique_url": "http://www.whu.edu.cn;https://www.huawei.com;https://www.huawei.com",
        "aff_unique_abbr": "WHU;Huawei;Huawei",
        "aff_campus_unique_index": "0;;0;0",
        "aff_campus_unique": "Wuhan;",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.341",
        "title": "ParaZh-22M: A Large-Scale Chinese Parabank via Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Paraphrasing, i.e., restating the same meaning in different ways, is an important data augmentation approach for natural language processing (NLP). Zhang et al. (2019b) propose to extract sentence-level paraphrases from multiple Chinese translations of the same source texts, and construct the PKU Paraphrase Bank of 0.5M sentence pairs. However, despite being the largest Chinese parabank to date, the size of PKU parabank is limited by the availability of one-to-many sentence translation data, and cannot well support the training of large Chinese paraphrasers. In this paper, we relieve the restriction with one-to-many sentence translation data, and construct ParaZh-22M, a larger Chinese parabank that is composed of 22M sentence pairs, based on one-to-one bilingual sentence translation data and machine translation (MT). In our data augmentation experiments, we show that paraphrasing based on ParaZh-22M can bring about consistent and significant improvements over several strong baselines on a wide range of Chinese NLP tasks, including a number of Chinese natural language understanding benchmarks (CLUE) and low-resource machine translation.",
        "author": "Wenjie Hao; Hongfei Xu; Deyi Xiong; Hongying Zan; Lingling Mu",
        "authorids": "/w/wenjie-hao/; /h/hongfei-xu/; /d/deyi-xiong/; /h/hongying-zan/; /l/lingling-mu/",
        "bibtex": "@inproceedings{hao-etal-2022-parazh,\n    title = \"{P}ara{Z}h-22{M}: A Large-Scale {C}hinese Parabank via Machine Translation\",\n    author = \"Hao, Wenjie  and\n      Xu, Hongfei  and\n      Xiong, Deyi  and\n      Zan, Hongying  and\n      Mu, Lingling\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.341/\",\n    pages = \"3885--3897\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.341.pdf",
        "site": "https://aclanthology.org/2022.coling-1.341/",
        "pdf_size": 358893,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3783520936215716261&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Zhengzhou University; Zhengzhou University; Tianjin University; Zhengzhou University+Peng Cheng Laboratory; Zhengzhou University",
        "aff_domain": "163.com;foxmail.com;tju.edu.cn;zzu.edu.cn;zzu.edu.cn",
        "email": "163.com;foxmail.com;tju.edu.cn;zzu.edu.cn;zzu.edu.cn",
        "github": "https://github.com/haowj9977/parazh-22M",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0+2;0",
        "aff_unique_norm": "Zhengzhou University;Tianjin University;Peng Cheng Laboratory",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.zzu.edu.cn;http://www.tju.edu.cn;http://www.pcl.ac.cn",
        "aff_unique_abbr": "ZZU;TJU;PCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.288",
        "title": "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, Mixture-of-Experts (short as MoE) architecture has achieved remarkable success in increasing the model capacity of large-scale language models. However, MoE requires incorporating significantly more parameters than the base model being extended. In this paper, we propose building a parameter-efficient MoE architecture by sharing information across experts. We adopt matrix product operator (MPO, a tensor decomposition from quantum many-body physics) to reconstruct the parameter matrix in the expert layer and increase model capacity for pre-trained language models by sharing parameters of the central tensor (containing the core information) among different experts while enabling the specificity through the auxiliary tensors (complementing the central tensor) of different experts. To address the unbalanced optimization issue, we further design the gradient mask strategy for the MPO-based MoE architecture. Extensive experiments based on T5 and GPT-2 show improved performance and efficiency of the pre-trained language model (27.2x reduction in total parameters for the superior model performance, compared with the Switch Transformers). Our code is publicly available at https://github.com/RUCAIBox/MPO/MPOE.",
        "author": "Ze-Feng Gao; Peiyu Liu; Wayne Xin Zhao; Zhong-Yi Lu; Ji-Rong Wen",
        "authorids": "/z/ze-feng-gao/; /p/peiyu-liu/; /w/wayne-xin-zhao/; /z/zhong-yi-lu/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{gao-etal-2022-parameter,\n    title = \"Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models\",\n    author = \"Gao, Ze-Feng  and\n      Liu, Peiyu  and\n      Zhao, Wayne Xin  and\n      Lu, Zhong-Yi  and\n      Wen, Ji-Rong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.288/\",\n    pages = \"3263--3273\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.288.pdf",
        "site": "https://aclanthology.org/2022.coling-1.288/",
        "pdf_size": 1613724,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13417137980549905519&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods+Beijing Academy of Artificial Intelligence; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods+Beijing Academy of Artificial Intelligence; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Department of Physics, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China+School of Information, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods+Beijing Academy of Artificial Intelligence",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn;gmail.com;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;ruc.edu.cn;gmail.com;ruc.edu.cn;ruc.edu.cn",
        "github": "https://github.com/RUCAIBox/MPOE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1+2;0+1;0;0+0+1+2",
        "aff_unique_norm": "Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Big Data Management and Analysis;",
        "aff_unique_url": "http://www.ruc.edu.cn;;https://www.baaic.cn",
        "aff_unique_abbr": "RUC;;BAAI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.90",
        "title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "State-of-the-art neural (re)rankers are notoriously data-hungry which \u2013 given the lack of large-scale training data in languages other than English \u2013 makes them rarely used in multilingual and cross-lingual retrieval settings. Current approaches therefore commonly transfer rankers trained on English data to other languages and cross-lingual setups by means of multilingual encoders: they fine-tune all parameters of pretrained massively multilingual Transformers (MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy them in the target language(s). In this work, we show that two parameter-efficient approaches to cross-lingual transfer, namely Sparse Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more effective zero-shot transfer to multilingual and cross-lingual retrieval tasks. We first train language adapters (or SFTMs) via Masked Language Modelling and then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping all other parameters fixed. At inference, this modular design allows us to compose the ranker by applying the (re)ranking adapter (or SFTM) trained with source language data together with the language adapter (or SFTM) of a target language. We carry out a large scale evaluation on the CLEF-2003 and HC4 benchmarks and additionally, as another contribution, extend the former with queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed parameter-efficient methods outperform standard zero-shot transfer with full MMT fine-tuning, while being more modular and reducing training times. The gains are particularly pronounced for low-resource languages, where our approaches also substantially outperform the competitive machine translation-based rankers.",
        "author": "Robert Litschko; Ivan Vuli\u0107; Goran Glava\u0161",
        "authorids": "/r/robert-litschko/; /i/ivan-vulic/; /g/goran-glavas/",
        "bibtex": "@inproceedings{litschko-etal-2022-parameter,\n    title = \"Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval\",\n    author = \"Litschko, Robert  and\n      Vuli{\\'c}, Ivan  and\n      Glava{\\v{s}}, Goran\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.90/\",\n    pages = \"1071--1082\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.90.pdf",
        "site": "https://aclanthology.org/2022.coling-1.90/",
        "pdf_size": 622487,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16966682153037687101&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.555",
        "title": "Paraphrase Generation as Unsupervised Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we propose a new paradigm for paraphrase generation by treating the task as unsupervised machine translation (UMT) based on the assumption that there must be pairs of sentences expressing the same meaning in a large-scale unlabeled monolingual corpus. The proposed paradigm first splits a large unlabeled corpus into multiple clusters, and trains multiple UMT models using pairs of these clusters. Then based on the paraphrase pairs produced by these UMT models, a unified surrogate model can be trained to serve as the final model to generate paraphrases, which can be directly used for test in the unsupervised setup, or be finetuned on labeled datasets in the supervised setup. The proposed method offers merits over machine-translation-based paraphrase generation methods, as it avoids reliance on bilingual sentence pairs. It also allows human intervene with the model so that more diverse paraphrases can be generated using different filtering criteria. Extensive experiments on existing paraphrase dataset for both the supervised and unsupervised setups demonstrate the effectiveness the proposed paradigm.",
        "author": "Xiaofei Sun; Yufei Tian; Yuxian Meng; Nanyun Peng; Fei Wu; Jiwei Li; Chun Fan",
        "authorids": "/x/xiaofei-sun/; /y/yufei-tian/; /y/yuxian-meng/; /n/nanyun-peng/; /f/fei-wu/; /j/jiwei-li/; /c/chun-fan/",
        "bibtex": "@inproceedings{sun-etal-2022-paraphrase,\n    title = \"Paraphrase Generation as Unsupervised Machine Translation\",\n    author = \"Sun, Xiaofei  and\n      Tian, Yufei  and\n      Meng, Yuxian  and\n      Peng, Nanyun  and\n      Wu, Fei  and\n      Li, Jiwei  and\n      Fan, Chun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.555/\",\n    pages = \"6379--6391\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.555.pdf",
        "site": "https://aclanthology.org/2022.coling-1.555/",
        "pdf_size": 470155,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13869198131967677135&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Zhejiang University+Shanghai AI Laboratory+Shanghai Institute for Advanced Study of Zhejiang University; University of California, Los Angeles; Shannon.AI; University of California, Los Angeles; Zhejiang University+Shanghai AI Laboratory+Shanghai Institute for Advanced Study of Zhejiang University; Shannon.AI+Zhejiang University; Peng Cheng Laboratory+National Biomedical Imaging Center, Peking University+Computer Center, Peking University",
        "aff_domain": "zju.edu.cn;ucla.edu; ;shannonai.com; ; ; ",
        "email": "zju.edu.cn;ucla.edu; ;shannonai.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+0;2;3;2;0+1+0;3+0;4+5+5",
        "aff_unique_norm": "Zhejiang University;Shanghai AI Laboratory;University of California, Los Angeles;Shannon.AI;Peng Cheng Laboratory;Peking University",
        "aff_unique_dep": ";;;;;National Biomedical Imaging Center",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.shanghai-ai-lab.com;https://www.ucla.edu;https://www.shannon.ai;http://www.pcl.ac.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": "ZJU;SAIL;UCLA;Shannon.AI;PCL;PKU",
        "aff_campus_unique_index": "1;2;2;1;;3",
        "aff_campus_unique": ";Shanghai;Los Angeles;Beijing",
        "aff_country_unique_index": "0+0+0;1;1;1;0+0+0;1+0;0+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.481",
        "title": "Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantic parsing converts natural language utterances into structured logical expressions. We consider two such formal representations: Propositional Logic (PL) and First-order Logic (FOL). The paucity of labeled data is a major challenge in this field. In previous works, dual reinforcement learning has been proposed as an approach to reduce dependence on labeled data. However, this method has the following limitations: 1) The reward needs to be set manually and is not applicable to all kinds of logical expressions. 2) The training process easily collapses when models are trained with only the reward from dual reinforcement learning. In this paper, we propose a scoring model to automatically learn a model-based reward, and an effective training strategy based on curriculum learning is further proposed to stabilize the training process. In addition to the technical contribution, a Chinese-PL/FOL dataset is constructed to compensate for the paucity of labeled data in this field. Experimental results show that the proposed method outperforms competitors on several datasets. Furthermore, by introducing PL/FOL generated by our model, the performance of existing Natural Language Inference (NLI) models is further enhanced.",
        "author": "Xuantao Lu; Jingping Liu; Zhouhong Gu; Hanwen Tong; Chenhao Xie; Junyang Huang; Yanghua Xiao; Wenguang Wang",
        "authorids": "/x/xuantao-lu/; /j/jingping-liu/; /z/zhouhong-gu/; /h/hanwen-tong/; /c/chenhao-xie/; /j/junyang-huang/; /y/yanghua-xiao/; /w/wenguang-wang/",
        "bibtex": "@inproceedings{lu-etal-2022-parsing,\n    title = \"Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning\",\n    author = \"Lu, Xuantao  and\n      Liu, Jingping  and\n      Gu, Zhouhong  and\n      Tong, Hanwen  and\n      Xie, Chenhao  and\n      Huang, Junyang  and\n      Xiao, Yanghua  and\n      Wang, Wenguang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.481/\",\n    pages = \"5419--5431\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.481.pdf",
        "site": "https://aclanthology.org/2022.coling-1.481/",
        "pdf_size": 927669,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18242398492829513879&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2022.coling-1.378",
        "title": "Penalizing Divergence: Multi-Parallel Translation for Low-Resource Languages of North America",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper explores a special case in multilingual machine translation: so called multi-parallel translation, where the target data for all language pairs are identical. While multi-parallelism offers benefits which are not available in a standard translation setting, translation models can easily overfit when training data are limited. We introduce a regularizer, the divergence penalty, which penalizes the translation model when it represents source sentences with identical target translations in divergent ways. Experiments on very low-resourced Indigenous North American languages show that an initially deficient multilingual translator can improve by 4.9 BLEU through mBART pre-training, and 5.5 BLEU points with the strategic addition of monolingual data, and that a divergence penalty leads to further increases of 0.4 BLEU. Further experiments on Germanic languages demonstrate a improvement of 0.5 BLEU when applying the divergence penalty. An investigation of the neural encoder representations learned by our translation models shows that the divergence penalty encourages models to learn a unified neural interlingua.",
        "author": "Garrett Nicolai; Changbing Yang; Miikka Silfverberg",
        "authorids": "/g/garrett-nicolai/; /c/changbing-yang/; /m/miikka-silfverberg/",
        "bibtex": "@inproceedings{nicolai-etal-2022-penalizing,\n    title = \"Penalizing Divergence: Multi-Parallel Translation for Low-Resource Languages of {N}orth {A}merica\",\n    author = \"Nicolai, Garrett  and\n      Yang, Changbing  and\n      Silfverberg, Miikka\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.378/\",\n    pages = \"4292--4298\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.378.pdf",
        "site": "https://aclanthology.org/2022.coling-1.378/",
        "pdf_size": 609252,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TtQgqJ1RGuIJ:scholar.google.com/&scioq=Penalizing+Divergence:+Multi-Parallel+Translation+for+Low-Resource+Languages+of+North+America&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "University of British Columbia; University of British Columbia; University of British Columbia",
        "aff_domain": "ubc.ca;ubc.ca;ubc.ca",
        "email": "ubc.ca;ubc.ca;ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.119",
        "title": "Perform like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge graph (KG) inference aims to address the natural incompleteness of KGs, including rule learning-based and KG embedding (KGE) models. However, the rule learning-based models suffer from low efficiency and generalization while KGE models lack interpretability. To address these challenges, we propose a novel and effective closed-loop neural-symbolic learning framework EngineKG via incorporating our developed KGE and rule learning modules. KGE module exploits symbolic rules and paths to enhance the semantic association between entities and relations for improving KG embeddings and interpretability. A novel rule pruning mechanism is proposed in the rule learning module by leveraging paths as initial candidate rules and employing KG embeddings together with concepts for extracting more high-quality rules. Experimental results on four real-world datasets show that our model outperforms the relevant baselines on link prediction tasks, demonstrating the superiority of our KG inference model in a neural-symbolic learning fashion. The source code and datasets of this paper are available at https://github.com/ngl567/EngineKG.",
        "author": "Guanglin Niu; Bo Li; Yongfei Zhang; Shiliang Pu",
        "authorids": "/g/guanglin-niu/; /b/bo-li-bh/; /y/yongfei-zhang/; /s/shiliang-pu/",
        "bibtex": "@inproceedings{niu-etal-2022-perform,\n    title = \"Perform like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference\",\n    author = \"Niu, Guanglin  and\n      Li, Bo  and\n      Zhang, Yongfei  and\n      Pu, Shiliang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.119/\",\n    pages = \"1391--1400\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.119.pdf",
        "site": "https://aclanthology.org/2022.coling-1.119/",
        "pdf_size": 650302,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3789180876096047305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Artificial Intelligence, Beihang University, Beijing, China+Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Institute of Artificial Intelligence, Beihang University, Beijing, China+Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China+State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Hikvision Research Institute, Hangzhou, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;hikvision.com",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;hikvision.com",
        "github": "https://github.com/ngl567/EngineKG",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;1",
        "aff_unique_norm": "Beihang University;Hikvision Research Institute",
        "aff_unique_dep": "Institute of Artificial Intelligence;",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.hikvision.com/cn/",
        "aff_unique_abbr": "BUAA;HRI",
        "aff_campus_unique_index": "0+1;0+1;0+0;1",
        "aff_campus_unique": "Beijing;Hangzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.380",
        "title": "Persian Natural Language Inference: A Meta-learning Approach",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Incorporating information from other languages can improve the results of tasks in low-resource languages. A powerful method of building functional natural language processing systems for low-resource languages is to combine multilingual pre-trained representations with cross-lingual transfer learning. In general, however, shared representations are learned separately, either across tasks or across languages. This paper proposes a meta-learning approach for inferring natural language in Persian. Alternately, meta-learning uses different task information (such as QA in Persian) or other language information (such as natural language inference in English). Also, we investigate the role of task augmentation strategy for forming additional high-quality tasks. We evaluate the proposed method using four languages and an auxiliary task. Compared to the baseline approach, the proposed model consistently outperforms it, improving accuracy by roughly six percent. We also examine the effect of finding appropriate initial parameters using zero-shot evaluation and CCA similarity.",
        "author": "Heydar Soudani; Mohammad Hassan Mojab; Hamid Beigy",
        "authorids": "/h/heydar-soudani/; /m/mohammad-hassan-mojab/; /h/hamid-beigy/",
        "bibtex": "@inproceedings{soudani-etal-2022-persian,\n    title = \"{P}ersian Natural Language Inference: A Meta-learning Approach\",\n    author = \"Soudani, Heydar  and\n      Mojab, Mohammad Hassan  and\n      Beigy, Hamid\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.380/\",\n    pages = \"4306--4319\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.380.pdf",
        "site": "https://aclanthology.org/2022.coling-1.380/",
        "pdf_size": 958645,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TXPsQFOFFxEJ:scholar.google.com/&scioq=Persian+Natural+Language+Inference:+A+Meta-learning+Approach&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Sharif University of Technology; Sharif University of Technology; Sharif University of Technology",
        "aff_domain": "ce.sharif.edu;ce.sharif.edu;sharif.edu",
        "email": "ce.sharif.edu;ce.sharif.edu;sharif.edu",
        "github": "https://github.com/HassanMojab/MetaNLI",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sharif University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sharif.edu",
        "aff_unique_abbr": "SUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2022.coling-1.537",
        "title": "Phrase-Level Localization of Inconsistency Errors in Summarization by Weak Supervision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Although the fluency of automatically generated abstractive summaries has improved significantly with advanced methods, the inconsistency that remains in summarization is recognized as an issue to be addressed. In this study, we propose a methodology for localizing inconsistency errors in summarization. A synthetic dataset that contains a variety of factual errors likely to be produced by a common summarizer is created by applying sentence fusion, compression, and paraphrasing operations. In creating the dataset, we automatically label erroneous phrases and the dependency relations between them as \u201cinconsistent,\u201d which can contribute to detecting errors more adequately than existing models that rely only on dependency arc-level labels. Subsequently, this synthetic dataset is employed as weak supervision to train a model called SumPhrase, which jointly localizes errors in a summary and their corresponding sentences in the source document. The empirical results demonstrate that our SumPhrase model can detect factual errors in summarization more effectively than existing weakly supervised methods owing to the phrase-level labeling. Moreover, the joint identification of error-corresponding original sentences is proven to be effective in improving error detection accuracy.",
        "author": "Masato Takatsuka; Tetsunori Kobayashi; Yoshihiko Hayashi",
        "authorids": "/m/masato-takatsuka/; /t/tetsunori-kobayashi/; /y/yoshihiko-hayashi/",
        "bibtex": "@inproceedings{takatsuka-etal-2022-phrase,\n    title = \"Phrase-Level Localization of Inconsistency Errors in Summarization by Weak Supervision\",\n    author = \"Takatsuka, Masato  and\n      Kobayashi, Tetsunori  and\n      Hayashi, Yoshihiko\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.537/\",\n    pages = \"6151--6164\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.537.pdf",
        "site": "https://aclanthology.org/2022.coling-1.537/",
        "pdf_size": 1889236,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18036732748404378277&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.253",
        "title": "PlugAT: A Plug and Play Module to Defend against Textual Adversarial Attack",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Adversarial training, which minimizes the loss of adversarially perturbed examples, has received considerable attention. However, these methods require modifying all model parameters and optimizing the model from scratch, which is parameter inefficient and unfriendly to the already deployed models. As an alternative, we propose a pluggable defense module PlugAT, to provide robust predictions by adding a few trainable parameters to the model inputs while keeping the original model frozen. To reduce the potential side effects of using defense modules, we further propose a novel forgetting restricted adversarial training, which filters out bad adversarial examples that impair the performance of original ones. The PlugAT-equipped BERT model substantially improves robustness over several strong baselines on various text classification tasks, whilst training only 9.1% parameters. We observe that defense modules trained under the same model architecture have domain adaptation ability between similar text classification datasets.",
        "author": "Rui Zheng; Rong Bao; Qin Liu; Tao Gui; Qi Zhang; Xuanjing Huang; Rui Xie; Wei Wu",
        "authorids": "/r/rui-zheng/; /r/rong-bao/; /q/qin-liu/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/; /r/rui-xie/; /w/wei-wu/",
        "bibtex": "@inproceedings{zheng-etal-2022-plugat,\n    title = \"{P}lug{AT}: A Plug and Play Module to Defend against Textual Adversarial Attack\",\n    author = \"Zheng, Rui  and\n      Bao, Rong  and\n      Liu, Qin  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing  and\n      Xie, Rui  and\n      Wu, Wei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.253/\",\n    pages = \"2873--2882\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.253.pdf",
        "site": "https://aclanthology.org/2022.coling-1.253/",
        "pdf_size": 657461,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15141254916736592065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Fudan University+Ant Group; School of Computer Science, Fudan University+Viterbi School of Engineering, University of Southern California; Viterbi School of Engineering, University of Southern California; Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China+Shanghai Collaborative Innovation Center of Intelligent Visual Computing; School of Computer Science, Fudan University+Shanghai Collaborative Innovation Center of Intelligent Visual Computing; School of Computer Science, Fudan University; Meituan Inc., Beijing, China; Shanghai Collaborative Innovation Center of Intelligent Visual Computing",
        "aff_domain": "fudan.edu.cn;m.fudan.edu.cn;usc.edu;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ;fudan.edu.cn",
        "email": "fudan.edu.cn;m.fudan.edu.cn;usc.edu;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+2;2;0+3;0+3;0;4;3",
        "aff_unique_norm": "Fudan University;Ant Group;University of Southern California;Shanghai Collaborative Innovation Center of Intelligent Visual Computing;Meituan Inc.",
        "aff_unique_dep": "School of Computer Science;;Viterbi School of Engineering;Intelligent Visual Computing;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.antgroup.com;https://www.usc.edu;;https://www.meituan.com",
        "aff_unique_abbr": "Fudan;Ant Group;USC;;Meituan",
        "aff_campus_unique_index": ";1;1;2;;3",
        "aff_campus_unique": ";Los Angeles;Shanghai;Beijing",
        "aff_country_unique_index": "0+0;0+1;1;0+0;0+0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.538",
        "title": "PoliSe: Reinforcing Politeness Using User Sentiment for Customer Care Response Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The interaction between a consumer and the customer service representative greatly contributes to the overall customer experience. Therefore, to ensure customers\u2019 comfort and retention, it is important that customer service agents and chatbots connect with users on social, cordial, and empathetic planes. In the current work, we automatically identify the sentiment of the user and transform the neutral responses into polite responses conforming to the sentiment and the conversational history. Our technique is basically a reinforced multi-task network- the primary task being \u2018polite response generation\u2019 and the secondary task being \u2018sentiment analysis\u2019- that uses a Transformer based encoder-decoder. We use sentiment annotated conversations from Twitter as the training data. The detailed evaluation shows that our proposed approach attains superior performance compared to the baseline models.",
        "author": "Mauajama Firdaus; Asif Ekbal; Pushpak Bhattacharyya",
        "authorids": "/m/mauajama-firdaus/; /a/asif-ekbal/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{firdaus-etal-2022-polise,\n    title = \"{P}oli{S}e: Reinforcing Politeness Using User Sentiment for Customer Care Response Generation\",\n    author = \"Firdaus, Mauajama  and\n      Ekbal, Asif  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.538/\",\n    pages = \"6165--6175\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.538.pdf",
        "site": "https://aclanthology.org/2022.coling-1.538/",
        "pdf_size": 388008,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13346181188804602541&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computing Science, University of Alberta, Canada; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Bombay, India",
        "aff_domain": "gmail.com;iitp.ac.in;gmail.com",
        "email": "gmail.com;iitp.ac.in;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Alberta;Indian Institute of Technology Patna;Indian Institute of Technology Bombay",
        "aff_unique_dep": "Department of Computing Science;Department of Computer Science and Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ualberta.ca;https://www.iitp.ac.in;https://www.iitb.ac.in",
        "aff_unique_abbr": "UAlberta;IIT Patna;IIT Bombay",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Patna;Bombay",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;India"
    },
    {
        "id": "2022.coling-1.480",
        "title": "Position Offset Label Prediction for Grammatical Error Correction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We introduce a novel position offset label prediction subtask to the encoder-decoder architecture for grammatical error correction (GEC) task. To keep the meaning of the input sentence unchanged, only a few words should be inserted or deleted during correction, and most of tokens in the erroneous sentence appear in the paired correct sentence with limited position movement. Inspired by this observation, we design an auxiliary task to predict position offset label (POL) of tokens, which is naturally capable of integrating different correction editing operations into a unified framework. Based on the predicted POL, we further propose a new copy mechanism (P-copy) to replace the vanilla copy module. Experimental results on Chinese, English and Japanese datasets demonstrate that our proposed POL-Pc framework obviously improves the performance of baseline models. Moreover, our model yields consistent performance gain over various data augmentation methods. Especially, after incorporating synthetic data, our model achieves a 38.95 F-0.5 score on Chinese GEC dataset, which outperforms the previous state-of-the-art by a wide margin of 1.98 points.",
        "author": "Xiuyu Wu; Jingsong Yu; Xu Sun; Yunfang Wu",
        "authorids": "/x/xiuyu-wu/; /j/jingsong-yu/; /x/xu-sun/; /y/yunfang-wu/",
        "bibtex": "@inproceedings{wu-etal-2022-position,\n    title = \"Position Offset Label Prediction for Grammatical Error Correction\",\n    author = \"Wu, Xiuyu  and\n      Yu, Jingsong  and\n      Sun, Xu  and\n      Wu, Yunfang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.480/\",\n    pages = \"5409--5418\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.480.pdf",
        "site": "https://aclanthology.org/2022.coling-1.480/",
        "pdf_size": 680125,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13147378000260601104&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "MOE, Key Laboratory of Computational Linguistics, Peking University + School of Software and Microelectronics, Peking University; MOE, Key Laboratory of Computational Linguistics, Peking University + School of Software and Microelectronics, Peking University; MOE, Key Laboratory of Computational Linguistics, Peking University + School of Computer Science, Peking University; MOE, Key Laboratory of Computational Linguistics, Peking University + School of Computer Science, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": ";;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.319",
        "title": "Possible Stories: Evaluating Situated Commonsense Reasoning under Multiple Possible Scenarios",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The possible consequences for the same context may vary depending on the situation we refer to. However, current studies in natural language processing do not focus on situated commonsense reasoning under multiple possible scenarios. This study frames this task by asking multiple questions with the same set of possible endings as candidate answers, given a short story text. Our resulting dataset, Possible Stories, consists of more than 4.5K questions over 1.3K story texts in English. We discover that even current strong pretrained language models struggle to answer the questions consistently, highlighting that the highest accuracy in an unsupervised setting (60.2%) is far behind human accuracy (92.5%). Through a comparison with existing datasets, we observe that the questions in our dataset contain minimal annotation artifacts in the answer options. In addition, our dataset includes examples that require counterfactual reasoning, as well as those requiring readers\u2019 reactions and fictional information, suggesting that our dataset can serve as a challenging testbed for future studies on situated commonsense reasoning.",
        "author": "Mana Ashida; Saku Sugawara",
        "authorids": "/m/mana-ashida/; /s/saku-sugawara/",
        "bibtex": "@inproceedings{ashida-sugawara-2022-possible,\n    title = \"Possible Stories: Evaluating Situated Commonsense Reasoning under Multiple Possible Scenarios\",\n    author = \"Ashida, Mana  and\n      Sugawara, Saku\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.319/\",\n    pages = \"3606--3630\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.319.pdf",
        "site": "https://aclanthology.org/2022.coling-1.319/",
        "pdf_size": 3717518,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14580695760863076474&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Yahoo Japan Corporation + Tokyo Metropolitan University; National Institute of Informatics",
        "aff_domain": "yahoo-corp.jp;nii.ac.jp",
        "email": "yahoo-corp.jp;nii.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Yahoo Japan Corporation;Tokyo Metropolitan University;National Institute of Informatics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yahoo.co.jp;https://www.tmuc.ac.jp;https://www.nii.ac.jp/",
        "aff_unique_abbr": "Yahoo Japan;TMU;NII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.289",
        "title": "Pre-trained Token-replaced Detection Model as Few-shot Learner",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.",
        "author": "Zicheng Li; Shoushan Li; Guodong Zhou",
        "authorids": "/z/zicheng-li/; /s/shoushan-li/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{li-etal-2022-pre-trained,\n    title = \"Pre-trained Token-replaced Detection Model as Few-shot Learner\",\n    author = \"Li, Zicheng  and\n      Li, Shoushan  and\n      Zhou, Guodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.289/\",\n    pages = \"3274--3284\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.289.pdf",
        "site": "https://aclanthology.org/2022.coling-1.289/",
        "pdf_size": 434729,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14242084983571963322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Natural Language Processing Lab, Soochow University, China; Natural Language Processing Lab, Soochow University, China; Natural Language Processing Lab, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "https://github.com/cjfarmer/TRD_FSL",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Natural Language Processing Lab",
        "aff_unique_url": "https://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.515",
        "title": "PrefScore: Pairwise Preference Learning for Reference-free Summarization Quality Assessment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Evaluating machine-generated summaries without a human-written reference summary has been a need for a long time. Inspired by preference labeling in existing work of summarization evaluation, we propose to judge summary quality by learning the preference rank of summaries using the Bradley-Terry power ranking model from inferior summaries generated by corrupting base summaries. Extensive experiments on several datasets show that our weakly supervised scheme can produce scores highly correlated with human ratings.",
        "author": "Ge Luo; Hebi Li; Youbiao He; Forrest Sheng Bao",
        "authorids": "/g/ge-luo/; /h/hebi-li/; /y/youbiao-he/; /f/forrest-bao/",
        "bibtex": "@inproceedings{luo-etal-2022-prefscore,\n    title = \"{P}ref{S}core: Pairwise Preference Learning for Reference-free Summarization Quality Assessment\",\n    author = \"Luo, Ge  and\n      Li, Hebi  and\n      He, Youbiao  and\n      Bao, Forrest Sheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.515/\",\n    pages = \"5896--5903\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.515.pdf",
        "site": "https://aclanthology.org/2022.coling-1.515/",
        "pdf_size": 366252,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14632218224183129745&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.348",
        "title": "Prepositions Matter in Quantifier Scope Disambiguation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Although it is widely agreed that world knowledge plays a significant role in quantifier scope disambiguation (QSD), there has been only very limited work on how to integrate this knowledge into a QSD model. This paper contributes to this scarce line of research by incorporating into a machine learning model our knowledge about relations, as conveyed by a manageable closed class of function words: prepositions. For data, we use a scope-disambiguated corpus created by AnderBois, Brasoveanu and Henderson, which is additionally annotated with prepositional senses using Schneider et al\u2019s Semantic Network of Adposition and Case Supersenses (SNACS) scheme. By applying Manshadi and Allen\u2019s method to the corpus, we were able to inspect the information gain provided by prepositions for the QSD task. Statistical analysis of the performance of the classifiers, trained in scenarios with and without preposition information, supports the claim that prepositional senses have a strong positive impact on the learnability of automatic QSD systems.",
        "author": "Aleksander Leczkowski; Justyna Grudzi\u0144ska; Manuel Vargas Guzm\u00e1n; Aleksander Wawer; Aleksandra Siemieniuk",
        "authorids": "/a/aleksander-leczkowski/; /j/justyna-grudzinska/; /m/manuel-vargas-guzman/; /a/aleksander-wawer/; /a/aleksandra-siemieniuk/",
        "bibtex": "@inproceedings{leczkowski-etal-2022-prepositions,\n    title = \"Prepositions Matter in Quantifier Scope Disambiguation\",\n    author = \"Leczkowski, Aleksander  and\n      Grudzi{\\'n}ska, Justyna  and\n      Vargas Guzm{\\'a}n, Manuel  and\n      Wawer, Aleksander  and\n      Siemieniuk, Aleksandra\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.348/\",\n    pages = \"3960--3970\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.348.pdf",
        "site": "https://aclanthology.org/2022.coling-1.348/",
        "pdf_size": 400251,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11529557391672334138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Faculty of Psychology, University of Warsaw; Faculty of Philosophy, University of Warsaw; Faculty of Philosophy, University of Warsaw; Institute of Computer Science, Polish Academy of Sciences; Faculty of Polish Studies, University of Warsaw",
        "aff_domain": "gmail.com;uw.edu.pl;uw.edu.pl;ipipan.waw.pl;uw.edu.pl",
        "email": "gmail.com;uw.edu.pl;uw.edu.pl;ipipan.waw.pl;uw.edu.pl",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Warsaw;Polish Academy of Sciences",
        "aff_unique_dep": "Faculty of Psychology;Institute of Computer Science",
        "aff_unique_url": "https://www.uw.edu.pl;https://www.pan.pl",
        "aff_unique_abbr": "UW;PAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Poland"
    },
    {
        "id": "2022.coling-1.418",
        "title": "Pro-KD: Progressive Distillation by Following the Footsteps of the Teacher",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the ever growing scale of neural models, knowledge distillation (KD) attracts more attention as a prominent tool for neural model compression. However, there are counter intuitive observations in the literature showing some challenging limitations of KD. A case in point is that the best performing checkpoint of the teacher might not necessarily be the best teacher for training the student in KD. Therefore, one important question would be how to find the best checkpoint of the teacher for distillation? Searching through the checkpoints of the teacher would be a very tedious and computationally expensive process, which we refer to as the checkpoint-search problem. Moreover, another observation is that larger teachers might not necessarily be better teachers in KD, which is referred to as the capacity-gap problem. To address these challenging problems, in this work, we introduce our progressive knowledge distillation (Pro-KD) technique which defines a smoother training path for the student by following the training footprints of the teacher instead of solely relying on distilling from a single mature fully-trained teacher. We demonstrate that our technique is quite effective in mitigating the capacity-gap problem and the checkpoint search problem. We evaluate our technique using a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and CIFAR-100), natural language understanding tasks of the GLUE benchmark, and question answering (SQuAD 1.1 and 2.0) using BERT-based models and consistently got superior results over state-of-the-art techniques.",
        "author": "Mehdi Rezagholizadeh; Aref Jafari; Puneeth S.M. Saladi; Pranav Sharma; Ali Saheb Pasand; Ali Ghodsi",
        "authorids": "/m/mehdi-rezagholizadeh/; /a/aref-jafari/; /p/puneeth-s-m-saladi/; /p/pranav-sharma/; /a/ali-saheb-pasand/; /a/ali-ghodsi/",
        "bibtex": "@inproceedings{rezagholizadeh-etal-2022-pro,\n    title = \"Pro-{KD}: Progressive Distillation by Following the Footsteps of the Teacher\",\n    author = \"Rezagholizadeh, Mehdi  and\n      Jafari, Aref  and\n      Saladi, Puneeth S.M.  and\n      Sharma, Pranav  and\n      Pasand, Ali Saheb  and\n      Ghodsi, Ali\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.418/\",\n    pages = \"4714--4727\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.418.pdf",
        "site": "https://aclanthology.org/2022.coling-1.418/",
        "pdf_size": 560021,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14245001634055904155&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Huawei Noah\u2019s Ark Lab+University of Waterloo; Huawei Noah\u2019s Ark Lab+University of Waterloo; Huawei Noah\u2019s Ark Lab; University of Waterloo; Huawei Noah\u2019s Ark Lab+University of Waterloo; University of Waterloo",
        "aff_domain": "huawei.com;uwaterloo.ca;huawei.com;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "email": "huawei.com;uwaterloo.ca;huawei.com;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0;1;0+1;1",
        "aff_unique_norm": "Huawei;University of Waterloo",
        "aff_unique_dep": "Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.huawei.com;https://uwaterloo.ca",
        "aff_unique_abbr": "Huawei;UW",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0;1;0+1;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2022.coling-1.237",
        "title": "Programmable Annotation with Diversed Heuristics and Data Denoising",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Neural natural language generation (NLG) and understanding (NLU) models are costly and require massive amounts of annotated data to be competitive. Recent data programming frameworks address this bottleneck by allowing human supervision to be provided as a set of labeling functions to construct generative models that synthesize weak labels at scale. However, these labeling functions are difficult to build from scratch for NLG/NLU models, as they often require complex rule sets to be specified. To this end, we propose a novel data programming framework that can jointly construct labeled data for language generation and understanding tasks \u2013 by allowing the annotators to modify an automatically-inferred alignment rule set between sequence labels and text, instead of writing rules from scratch. Further, to mitigate the effect of poor quality labels, we propose a dually-regularized denoising mechanism for optimizing the NLU and NLG models. On two benchmarks we show that the framework can generate high-quality data that comes within a 1.48 BLEU and 6.42 slot F1 of the 100% human-labeled data (42k instances) with just 100 labeled data samples \u2013 outperforming benchmark annotation frameworks and other semi-supervised approaches.",
        "author": "Ernie Chang; Alex Marin; Vera Demberg",
        "authorids": "/e/ernie-chang/; /a/alex-marin/; /v/vera-demberg/",
        "bibtex": "@inproceedings{chang-etal-2022-programmable,\n    title = \"Programmable Annotation with Diversed Heuristics and Data Denoising\",\n    author = \"Chang, Ernie  and\n      Marin, Alex  and\n      Demberg, Vera\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.237/\",\n    pages = \"2681--2691\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.237.pdf",
        "site": "https://aclanthology.org/2022.coling-1.237/",
        "pdf_size": 722728,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AhP58wERl5wJ:scholar.google.com/&scioq=Programmable+Annotation+with+Diversed+Heuristics+and+Data+Denoising&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Dept. of Language Science and Technology, Saarland University; Microsoft Corporation, Redmond, WA; Dept. of Language Science and Technology, Saarland University",
        "aff_domain": "coli.uni-saarland.de; ; ",
        "email": "coli.uni-saarland.de; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Saarland University;Microsoft Corporation",
        "aff_unique_dep": "Dept. of Language Science and Technology;",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.microsoft.com",
        "aff_unique_abbr": "Saarland U;Microsoft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2022.coling-1.122",
        "title": "Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prompt-based fine-tuning for pre-trained models has proven effective for many natural language processing tasks under few-shot settings in general domain. However, tuning with prompt in biomedical domain has not been investigated thoroughly. Biomedical words are often rare in general domain, but quite ubiquitous in biomedical contexts, which dramatically deteriorates the performance of pre-trained models on downstream biomedical applications even after fine-tuning, especially in low-resource scenarios. We propose a simple yet effective approach to helping models learn rare biomedical words during tuning with prompt. Experimental results show that our method can achieve up to 6% improvement in biomedical natural language inference task without any extra parameters or training steps using few-shot vanilla prompt settings.",
        "author": "Haochun Wang; Chi Liu; Nuwa Xi; Sendong Zhao; Meizhi Ju; Shiwei Zhang; Ziheng Zhang; Yefeng Zheng; Bing Qin; Ting Liu",
        "authorids": "/h/haochun-wang/; /c/chi-liu/; /n/nuwa-xi/; /s/sendong-zhao/; /m/meizhi-ju/; /s/shiwei-zhang/; /z/ziheng-zhang/; /y/yefeng-zheng/; /b/bing-qin/; /t/ting-liu/",
        "bibtex": "@inproceedings{wang-etal-2022-prompt,\n    title = \"Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words\",\n    author = \"Wang, Haochun  and\n      Liu, Chi  and\n      Xi, Nuwa  and\n      Zhao, Sendong  and\n      Ju, Meizhi  and\n      Zhang, Shiwei  and\n      Zhang, Ziheng  and\n      Zheng, Yefeng  and\n      Qin, Bing  and\n      Liu, Ting\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.122/\",\n    pages = \"1422--1431\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.122.pdf",
        "site": "https://aclanthology.org/2022.coling-1.122/",
        "pdf_size": 377173,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16340730597958366836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Tencent Jarvis Lab, Shenzhen, China; Tencent Jarvis Lab, Shenzhen, China; Tencent Jarvis Lab, Shenzhen, China; Tencent Jarvis Lab, Shenzhen, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;outlook.com;tencent.com;tencent.com;tencent.com;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;outlook.com;tencent.com;tencent.com;tencent.com;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;1;1;1;1;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Tencent",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval;Jarvis Lab",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.tencent.com",
        "aff_unique_abbr": "HIT;Tencent",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.154",
        "title": "Prompt-based Conservation Learning for Multi-hop Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-hop question answering (QA) requires reasoning over multiple documents to answer a complex question and provide interpretable supporting evidence. However, providing supporting evidence is not enough to demonstrate that a model has performed the desired reasoning to reach the correct answer. Most existing multi-hop QA methods fail to answer a large fraction of sub-questions, even if their parent questions are answered correctly. In this paper, we propose the Prompt-based Conservation Learning (PCL) framework for multi-hop QA, which acquires new knowledge from multi-hop QA tasks while conserving old knowledge learned on single-hop QA tasks, mitigating forgetting. Specifically, we first train a model on existing single-hop QA tasks, and then freeze this model and expand it by allocating additional sub-networks for the multi-hop QA task. Moreover, to condition pre-trained language models to stimulate the kind of reasoning required for specific multi-hop questions, we learn soft prompts for the novel sub-networks to perform type-specific reasoning. Experimental results on the HotpotQA benchmark show that PCL is competitive for multi-hop QA and retains good performance on the corresponding single-hop sub-questions, demonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.",
        "author": "Zhenyun Deng; Yonghua Zhu; Yang Chen; Qianqian Qi; Michael Witbrock; Patricia Riddle",
        "authorids": "/z/zhenyun-deng/; /y/yonghua-zhu/; /y/yang-chen/; /q/qianqian-qi/; /m/michael-j-witbrock/; /p/patricia-riddle/",
        "bibtex": "@inproceedings{deng-etal-2022-prompt,\n    title = \"Prompt-based Conservation Learning for Multi-hop Question Answering\",\n    author = \"Deng, Zhenyun  and\n      Zhu, Yonghua  and\n      Chen, Yang  and\n      Qi, Qianqian  and\n      Witbrock, Michael  and\n      Riddle, Patricia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.154/\",\n    pages = \"1791--1800\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.154.pdf",
        "site": "https://aclanthology.org/2022.coling-1.154/",
        "pdf_size": 910807,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17294789979683630349&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, University of Auckland, New Zealand; School of Computer Science, University of Auckland, New Zealand; School of Computer Science, University of Auckland, New Zealand; School of Computer Science, University of Auckland, New Zealand; School of Computer Science, University of Auckland, New Zealand; School of Computer Science, University of Auckland, New Zealand",
        "aff_domain": "aucklanduni.ac.nz;aucklanduni.ac.nz;aucklanduni.ac.nz;auckland.ac.nz;auckland.ac.nz;auckland.ac.nz",
        "email": "aucklanduni.ac.nz;aucklanduni.ac.nz;aucklanduni.ac.nz;auckland.ac.nz;auckland.ac.nz;auckland.ac.nz",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Auckland",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.auckland.ac.nz",
        "aff_unique_abbr": "UoA",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Auckland",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "2022.coling-1.164",
        "title": "Prompt-based Text Entailment for Low-Resource Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained Language Models (PLMs) have been applied in NLP tasks and achieve promising results. Nevertheless, the fine-tuning procedure needs labeled data of the target domain, making it difficult to learn in low-resource and non-trivial labeled scenarios. To address these challenges, we propose Prompt-based Text Entailment (PTE) for low-resource named entity recognition, which better leverages knowledge in the PLMs. We first reformulate named entity recognition as the text entailment task. The original sentence with entity type-specific prompts is fed into PLMs to get entailment scores for each candidate. The entity type with the top score is then selected as final label. Then, we inject tagging labels into prompts and treat words as basic units instead of n-gram spans to reduce time complexity in generating candidates by n-grams enumeration. Experimental results demonstrate that the proposed method PTE achieves competitive performance on the CoNLL03 dataset, and better than fine-tuned counterparts on the MIT Movie and Few-NERD dataset in low-resource settings.",
        "author": "Dongfang Li; Baotian Hu; Qingcai Chen",
        "authorids": "/d/dongfang-li/; /b/baotian-hu/; /q/qingcai-chen/",
        "bibtex": "@inproceedings{li-etal-2022-prompt-based-text,\n    title = \"Prompt-based Text Entailment for Low-Resource Named Entity Recognition\",\n    author = \"Li, Dongfang  and\n      Hu, Baotian  and\n      Chen, Qingcai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.164/\",\n    pages = \"1896--1903\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.164.pdf",
        "site": "https://aclanthology.org/2022.coling-1.164/",
        "pdf_size": 335729,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5701404576556859482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Harbin Institute of Technology (Shenzhen), Shenzhen, China+Peng Cheng Laboratory, Shenzhen, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China+Peng Cheng Laboratory, Shenzhen, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China+Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "gmail.com;hit.edu.cn;hit.edu.cn",
        "email": "gmail.com;hit.edu.cn;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Harbin Institute of Technology;Peng Cheng Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://en.hhit.edu.cn/;",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.564",
        "title": "Psychology-guided Controllable Story Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Controllable story generation is a challenging task in the field of NLP, which has attracted increasing research interest in recent years. However, most existing works generate a whole story conditioned on the appointed keywords or emotions, ignoring the psychological changes of the protagonist. Inspired by psychology theories, we introduce global psychological state chains, which include the needs and emotions of the protagonists, to help a story generation system create more controllable and well-planned stories. In this paper, we propose a Psychology-guided Controllable Story Generation System (PICS) to generate stories that adhere to the given leading context and desired psychological state chains for the protagonist. Specifically, psychological state trackers are employed to memorize the protagonist\u2019s local psychological states to capture their inner temporal relationships. In addition, psychological state planners are adopted to gain the protagonist\u2019s global psychological states for story planning. Eventually, a psychology controller is designed to integrate the local and global psychological states into the story context representation for composing psychology-guided stories. Automatic and manual evaluations demonstrate that PICS outperforms baselines, and each part of PICS shows effectiveness for writing stories with more consistent psychological changes.",
        "author": "Yuqiang Xie; Yue Hu; Yunpeng Li; Guanqun Bi; Luxi Xing; Wei Peng",
        "authorids": "/y/yuqiang-xie/; /y/yue-hu/; /y/yunpeng-li/; /g/guanqun-bi/; /l/luxi-xing/; /w/wei-peng/",
        "bibtex": "@inproceedings{xie-etal-2022-psychology,\n    title = \"Psychology-guided Controllable Story Generation\",\n    author = \"Xie, Yuqiang  and\n      Hu, Yue  and\n      Li, Yunpeng  and\n      Bi, Guanqun  and\n      Xing, Luxi  and\n      Peng, Wei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.564/\",\n    pages = \"6480--6492\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.564.pdf",
        "site": "https://aclanthology.org/2022.coling-1.564/",
        "pdf_size": 1826685,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14060043942558013006&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.337",
        "title": "QSTS: A Question-Sensitive Text Similarity Measure for Question Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While question generation (QG) has received significant focus in conversation modeling and text generation research, the problems of comparing questions and evaluation of QG models have remained inadequately addressed. Indeed, QG models continue to be evaluated using traditional measures such as BLEU, METEOR, and ROUGE scores which were designed for other text generation problems. We propose QSTS, a novel Question-Sensitive Text Similarity measure for comparing two questions by characterizing their target intent based on question class, named-entity, and semantic similarity information from the two questions. We show that QSTS addresses several shortcomings of existing measures that depend on n-gram overlap scores and obtains superior results compared to traditional measures on publicly-available QG datasets. We also collect a novel dataset SimQG, for enabling question similarity research in QG contexts. SimQG contains questions generated by state-of-the-art QG models along with human judgements on their relevance with respect to the passage context they were generated for as well as when compared to the given reference question. Using SimQG, we showcase the key aspect of QSTS that differentiates it from all existing measures. QSTS is not only able to characterize similarity between two questions, but is also able to score questions with respect to passage contexts. Thus QSTS is, to our knowledge, the first metric that enables the measurement of QG performance in a reference-free manner.",
        "author": "Sujatha Das Gollapalli; See-Kiong Ng",
        "authorids": "/s/sujatha-das-gollapalli/; /s/see-kiong-ng/",
        "bibtex": "@inproceedings{gollapalli-ng-2022-qsts,\n    title = \"{QSTS}: A Question-Sensitive Text Similarity Measure for Question Generation\",\n    author = \"Gollapalli, Sujatha Das  and\n      Ng, See-Kiong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.337/\",\n    pages = \"3835--3846\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.337.pdf",
        "site": "https://aclanthology.org/2022.coling-1.337/",
        "pdf_size": 340815,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2113994841296485611&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Data Science, National University of Singapore; Institute of Data Science, National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Institute of Data Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.coling-1.460",
        "title": "QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the recent advance in neural machine translation demonstrating its importance, research on quality estimation (QE) has been steadily progressing. QE aims to automatically predict the quality of machine translation (MT) output without reference sentences. Despite its high utility in the real world, there remain several limitations concerning manual QE data creation: inevitably incurred non-trivial costs due to the need for translation experts, and issues with data scaling and language expansion. To tackle these limitations, we present QUAK, a Korean-English synthetic QE dataset generated in a fully automatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and QUAK-H, produced through three strategies that are relatively free from language constraints. Since each strategy requires no human effort, which facilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M for QUAK-M. As an experiment, we quantitatively analyze word-level QE results in various ways while performing statistical analysis. Moreover, we show that datasets scaled in an efficient way also contribute to performance improvements by observing meaningful performance gains in QUAK-M, P when adding data up to 1.58M.",
        "author": "Sugyeong Eo; Chanjun Park; Hyeonseok Moon; Jaehyung Seo; Gyeongmin Kim; Jungseob Lee; Heuiseok Lim",
        "authorids": "/s/sugyeong-eo/; /c/chanjun-park/; /h/hyeonseok-moon/; /j/jaehyung-seo/; /g/gyeongmin-kim/; /j/jungseob-lee/; /h/heui-seok-lim/",
        "bibtex": "@inproceedings{eo-etal-2022-quak,\n    title = \"{QUAK}: A Synthetic Quality Estimation Dataset for {K}orean-{E}nglish Neural Machine Translation\",\n    author = \"Eo, Sugyeong  and\n      Park, Chanjun  and\n      Moon, Hyeonseok  and\n      Seo, Jaehyung  and\n      Kim, Gyeongmin  and\n      Lee, Jungseob  and\n      Lim, Heuiseok\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.460/\",\n    pages = \"5181--5190\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.460.pdf",
        "site": "https://aclanthology.org/2022.coling-1.460/",
        "pdf_size": 1438179,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5481970214923769944&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.coling-1.112",
        "title": "Quantifying Bias from Decoding Techniques in Natural Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural language generation (NLG) models can propagate social bias towards particular demography. Though several studies investigated bias from data and model, NLG task distinctively uses stochastic decoder that can positively or negatively impact the bias-sensitive tokens initially predicted by the model. To address this gap in research, we present an extensive analysis of bias from decoding techniques for open-domain language generation considering the entire decoding space. We analyze to what extent bias metrics like toxicity and sentiment are impacted by the individual components of decoder algorithms. To this extent, we also analyze the trade-off between bias scores and human-annotated generation quality throughout the decoder space. Together, these methods reveal the imperative of testing inference time bias and provide evidence on the usefulness of inspecting the entire decoding spectrum.",
        "author": "Mayukh Das; Wolf Tilo Balke",
        "authorids": "/m/mayukh-das/; /w/wolf-tilo-balke/",
        "bibtex": "@inproceedings{das-balke-2022-quantifying,\n    title = \"Quantifying Bias from Decoding Techniques in Natural Language Generation\",\n    author = \"Das, Mayukh  and\n      Balke, Wolf Tilo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.112/\",\n    pages = \"1311--1323\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.112.pdf",
        "site": "https://aclanthology.org/2022.coling-1.112/",
        "pdf_size": 457782,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=38189123230753795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2022.coling-1.562",
        "title": "Question Generation Based on Grammar Knowledge and Fine-grained Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Question generation is the task of automatically generating questions based on given context and answers, and there are problems that the types of questions and answers do not match. In minority languages such as Tibetan, since the grammar rules are complex and the training data is small, the related research on question generation is still in its infancy. To solve the above problems, this paper constructs a question type classifier and a question generator. We perform fine-grained division of question types and integrate grammatical knowledge into question type classifiers to improve the accuracy of question types. Then, the types predicted by the question type classifier are fed into the question generator. Our model improves the accuracy of interrogative words in generated questions, and the BLEU-4 on SQuAD reaches 17.52, the BLEU-4 on HotpotQA reaches 19.31, the BLEU-4 on TibetanQA reaches 25.58.",
        "author": "Yuan Sun; Sisi Liu; Zhengcuo Dan; Xiaobing Zhao",
        "authorids": "/y/yuan-sun/; /s/sisi-liu/; /z/zhengcuo-dan/; /x/xiaobing-zhao/",
        "bibtex": "@inproceedings{sun-etal-2022-question,\n    title = \"Question Generation Based on Grammar Knowledge and Fine-grained Classification\",\n    author = \"Sun, Yuan  and\n      Liu, Sisi  and\n      Dan, Zhengcuo  and\n      Zhao, Xiaobing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.562/\",\n    pages = \"6457--6467\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.562.pdf",
        "site": "https://aclanthology.org/2022.coling-1.562/",
        "pdf_size": 1010699,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12188261015483763201&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Minzu University of China+National Language Resource Monitoring & Research Center Minority Languages Branch; Minzu University of China+National Language Resource Monitoring & Research Center Minority Languages Branch; Minzu University of China+National Language Resource Monitoring & Research Center Minority Languages Branch; Minzu University of China+National Language Resource Monitoring & Research Center Minority Languages Branch",
        "aff_domain": "gmail.com;qq.com;163.com; ",
        "email": "gmail.com;qq.com;163.com; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Minzu University of China;National Language Resource Monitoring & Research Center",
        "aff_unique_dep": ";Minority Languages Branch",
        "aff_unique_url": "http://www.muc.edu.cn/;",
        "aff_unique_abbr": "MUC;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.coling-1.174",
        "title": "RSGT: Relational Structure Guided Temporal Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Temporal relation extraction aims to extract temporal relations between event pairs, which is crucial for natural language understanding. Few efforts have been devoted to capturing the global features. In this paper, we propose RSGT: Relational Structure Guided Temporal Relation Extraction to extract the relational structure features that can fit for both inter-sentence and intra-sentence relations. Specifically, we construct a syntactic-and-semantic-based graph to extract relational structures. Then we present a graph neural network based model to learn the representation of this graph. After that, an auxiliary temporal neighbor prediction task is used to fine-tune the encoder to get more comprehensive node representations. Finally, we apply a conflict detection and correction algorithm to adjust the wrongly predicted labels. Experiments on two well-known datasets, MATRES and TB-Dense, demonstrate the superiority of our method (2.3% F1 improvement on MATRES, 3.5% F1 improvement on TB-Dense).",
        "author": "Jie Zhou; Shenpo Dong; Hongkui Tu; Xiaodong Wang; Yong Dou",
        "authorids": "/j/jie-zhou/; /s/shenpo-dong/; /h/hongkui-tu/; /x/xiaodong-wang/; /y/yong-dou/",
        "bibtex": "@inproceedings{zhou-etal-2022-rsgt,\n    title = \"{RSGT}: Relational Structure Guided Temporal Relation Extraction\",\n    author = \"Zhou, Jie  and\n      Dong, Shenpo  and\n      Tu, Hongkui  and\n      Wang, Xiaodong  and\n      Dou, Yong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.174/\",\n    pages = \"2001--2010\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.174.pdf",
        "site": "https://aclanthology.org/2022.coling-1.174/",
        "pdf_size": 512164,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1140042074313878903&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "College of Computer",
        "aff_unique_url": "http://www.nudt.edu.cn/",
        "aff_unique_abbr": "NUDT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.459",
        "title": "Rare but Severe Neural Machine Translation Errors Induced by Minimal Deletion: An Empirical Study on Chinese and English",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We examine the inducement of rare but severe errors in English-Chinese and Chinese-English in-domain neural machine translation by minimal deletion of source text with character-based models. By deleting a single character, we can induce severe translation errors. We categorize these errors and compare the results of deleting single characters and single words. We also examine the effect of training data size on the number and types of pathological cases induced by these minimal perturbations, finding significant variation. We find that deleting a word hurts overall translation score more than deleting a character, but certain errors are more likely to occur when deleting characters, with language direction also influencing the effect.",
        "author": "Ruikang Shi; Alvin Grissom II; Duc Minh Trinh",
        "authorids": "/r/ruikang-shi/; /a/alvin-grissom-ii/; /d/duc-minh-trinh/",
        "bibtex": "@inproceedings{shi-etal-2022-rare,\n    title = \"Rare but Severe Neural Machine Translation Errors Induced by Minimal Deletion: An Empirical Study on {C}hinese and {E}nglish\",\n    author = \"Shi, Ruikang  and\n      Grissom II, Alvin  and\n      Trinh, Duc Minh\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.459/\",\n    pages = \"5175--5180\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.459.pdf",
        "site": "https://aclanthology.org/2022.coling-1.459/",
        "pdf_size": 360240,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3905365711261560647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Haverford College; Haverford College; Haverford College",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Haverford College",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.haverford.edu",
        "aff_unique_abbr": "Haverford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.66",
        "title": "Re-Examining FactBank: Predicting the Author\u2019s Presentation of Factuality",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a corrected version of a subset of the FactBank data set. Previously published results on FactBank are no longer valid. We perform experiments on FactBank using multiple training paradigms, data smoothing techniques, and polarity classifiers. We argue that f-measure is an important alternative evaluation metric for factuality. We provide new state-of-the-art results for four corpora including FactBank. We perform an error analysis on Factbank combined with two similar corpora.",
        "author": "John Murzaku; Peter Zeng; Magdalena Markowska; Owen Rambow",
        "authorids": "/j/john-murzaku/; /p/peter-zeng/; /m/magdalena-markowska/; /o/owen-rambow/",
        "bibtex": "@inproceedings{murzaku-etal-2022-examining,\n    title = \"Re-Examining {F}act{B}ank: Predicting the Author{'}s Presentation of Factuality\",\n    author = \"Murzaku, John  and\n      Zeng, Peter  and\n      Markowska, Magdalena  and\n      Rambow, Owen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.66/\",\n    pages = \"786--796\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.66.pdf",
        "site": "https://aclanthology.org/2022.coling-1.66/",
        "pdf_size": 245689,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1126643416614552435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science+Institute for Advanced Computational Science; Department of Computer Science; Department of Linguistics+Institute for Advanced Computational Science; Department of Linguistics+Institute for Advanced Computational Science",
        "aff_domain": "cs.stonybrook.edu; ; ; ",
        "email": "cs.stonybrook.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;2+1;2+1",
        "aff_unique_norm": "Unknown Institution;Stony Brook University;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Institute for Advanced Computational Science;Department of Linguistics",
        "aff_unique_url": ";https://www.stonybrook.edu/ics;",
        "aff_unique_abbr": ";SBU IACS;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2022.coling-1.177",
        "title": "Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The introduction of multimodal information and pretraining technique significantly improves entity recognition from visually-rich documents. However, most of the existing methods pay unnecessary attention to irrelevant regions of the current document while ignoring the potentially valuable information in related documents. To deal with this problem, this work proposes a cross-document semantic enhancement method, which consists of two modules: 1) To prevent distractions from irrelevant regions in the current document, we design a learnable attention mask mechanism, which is used to adaptively filter redundant information in the current document. 2) To further enrich the entity-related context, we propose a cross-document information awareness technique, which enables the model to collect more evidence across documents to assist in prediction. The experimental results on two documents understanding benchmarks covering eight languages demonstrate that our method outperforms the SOTA methods.",
        "author": "Jun Zhao; Xin Zhao; WenYu Zhan; Tao Gui; Qi Zhang; Liang Qiao; Zhanzhan Cheng; Shiliang Pu",
        "authorids": "/j/jun-zhao/; /w/wayne-xin-zhao/; /w/wenyu-zhan/; /t/tao-gui/; /q/qi-zhang/; /l/liang-qiao/; /z/zhanzhan-cheng/; /s/shiliang-pu/",
        "bibtex": "@inproceedings{zhao-etal-2022-read-extensively,\n    title = \"Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents {NER}\",\n    author = \"Zhao, Jun  and\n      Zhao, Xin  and\n      Zhan, WenYu  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Qiao, Liang  and\n      Cheng, Zhanzhan  and\n      Pu, Shiliang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.177/\",\n    pages = \"2034--2043\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.177.pdf",
        "site": "https://aclanthology.org/2022.coling-1.177/",
        "pdf_size": 831038,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4015862304599612909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; Institute of Modern Languages and Linguistics, Fudan University; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; Hikvision Research Institute, Shanghai, China; Hikvision Research Institute, Shanghai, China; Hikvision Research Institute, Shanghai, China",
        "aff_domain": "fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ; ; ",
        "email": "fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;1;1;1",
        "aff_unique_norm": "Fudan University;Hikvision Research Institute",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.hikvision.com/cn/",
        "aff_unique_abbr": "Fudan;HRI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.295",
        "title": "RealMedDial: A Real Telemedical Dialogue Dataset Collected from Online Chinese Short-Video Clips",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Intelligent medical services have attracted great research interests for providing automated medical consultation. However, the lack of corpora becomes a main obstacle to related research, particularly data from real scenarios. In this paper, we construct RealMedDial, a Chinese medical dialogue dataset based on real medical consultation. RealMedDial contains 2,637 medical dialogues and 24,255 utterances obtained from Chinese short-video clips of real medical consultations. We collected and annotated a wide range of meta-data with respect to medical dialogue including doctor profiles, hospital departments, diseases and symptoms for fine-grained analysis on language usage pattern and clinical diagnosis. We evaluate the performance of medical response generation, department routing and doctor recommendation on RealMedDial. Results show that RealMedDial are applicable to a wide range of NLP tasks with respect to medical dialogue.",
        "author": "Bo Xu; Hongtong Zhang; Jian Wang; Xiaokun Zhang; Dezhi Hao; Linlin Zong; Hongfei Lin; Fenglong Ma",
        "authorids": "/b/bo-xu/; /h/hongtong-zhang/; /j/jian-wang/; /x/xiaokun-zhang/; /d/dezhi-hao/; /l/linlin-zong/; /h/hongfei-lin/; /f/fenglong-ma/",
        "bibtex": "@inproceedings{xu-etal-2022-realmeddial,\n    title = \"{R}eal{M}ed{D}ial: A Real Telemedical Dialogue Dataset Collected from Online {C}hinese Short-Video Clips\",\n    author = \"Xu, Bo  and\n      Zhang, Hongtong  and\n      Wang, Jian  and\n      Zhang, Xiaokun  and\n      Hao, Dezhi  and\n      Zong, Linlin  and\n      Lin, Hongfei  and\n      Ma, Fenglong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.295/\",\n    pages = \"3342--3352\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.295.pdf",
        "site": "https://aclanthology.org/2022.coling-1.295/",
        "pdf_size": 1978529,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2638104712858072731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; College of Information Sciences and Technology, Pennsylvania State University, USA",
        "aff_domain": "dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;psu.edu",
        "email": "dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;mail.dlut.edu.cn;dlut.edu.cn;psu.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Dalian University of Technology;Pennsylvania State University",
        "aff_unique_dep": "School of Computer Science and Technology;College of Information Sciences and Technology",
        "aff_unique_url": "http://en.dlut.edu.cn/;https://www.psu.edu",
        "aff_unique_abbr": "DUT;PSU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Dalian;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.190",
        "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Text-to-SQL has attracted attention from both the natural language processing and database communities because of its ability to convert the semantics in natural language into SQL queries and its practical application in building natural language interfaces to database systems. The major challenges in text-to-SQL lie in encoding the meaning of natural utterances, decoding to SQL queries, and translating the semantics between these two forms. These challenges have been addressed to different extents by the recent advances. However, there is still a lack of comprehensive surveys for this task. To this end, we review recent progress on text-to-SQL for datasets, methods, and evaluation and provide this systematic survey, addressing the aforementioned challenges and discussing potential future directions. We hope this survey can serve as quick access to existing work and motivate future research.",
        "author": "Naihao Deng; Yulong Chen; Yue Zhang",
        "authorids": "/n/naihao-deng/; /y/yulong-chen/; /y/yue-zhang/",
        "bibtex": "@inproceedings{deng-etal-2022-recent,\n    title = \"Recent Advances in Text-to-{SQL}: A Survey of What We Have and What We Expect\",\n    author = \"Deng, Naihao  and\n      Chen, Yulong  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.190/\",\n    pages = \"2166--2187\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.190.pdf",
        "site": "https://aclanthology.org/2022.coling-1.190/",
        "pdf_size": 638576,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17784095364613668256&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan; Westlake University; Westlake University",
        "aff_domain": "umich.edu;gmail.com;wias.org.cn",
        "email": "umich.edu;gmail.com;wias.org.cn",
        "github": "https://github.com/text-to-sql-survey-coling22/text-to-sql-survey-coling22.github.io",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Michigan;Westlake University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.westlake.edu.cn",
        "aff_unique_abbr": "UM;WU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.coling-1.31",
        "title": "Reciprocal Learning of Knowledge Retriever and Response Ranker for Knowledge-Grounded Conversations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Grounding dialogue agents with knowledge documents has sparked increased attention in both academia and industry. Recently, a growing body of work is trying to build retrieval-based knowledge-grounded dialogue systems. While promising, these approaches require collecting pairs of dialogue context and the corresponding ground-truth knowledge sentences that contain the information regarding the dialogue context. Unfortunately, hand-labeling data to that end is time-consuming, and many datasets and applications lack such knowledge annotations. In this paper, we propose a reciprocal learning approach to jointly optimize a knowledge retriever and a response ranker for knowledge-grounded response retrieval without ground-truth knowledge labels. Specifically, the knowledge retriever uses the feedback from the response ranker as pseudo supervised signals of knowledge retrieval for updating its parameters, while the response ranker also receives the top-ranked knowledge sentences from knowledge retriever for optimization. Evaluation results on two public benchmarks show that our model can significantly outperform previous state-of-the-art methods.",
        "author": "Jiazhan Feng; Chongyang Tao; Zhen Li; Chang Liu; Tao Shen; Dongyan Zhao",
        "authorids": "/j/jiazhan-feng/; /c/chongyang-tao/; /z/zhen-li/; /c/chang-liu/; /t/tao-shen/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{feng-etal-2022-reciprocal,\n    title = \"Reciprocal Learning of Knowledge Retriever and Response Ranker for Knowledge-Grounded Conversations\",\n    author = \"Feng, Jiazhan  and\n      Tao, Chongyang  and\n      Li, Zhen  and\n      Liu, Chang  and\n      Shen, Tao  and\n      Zhao, Dongyan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.31/\",\n    pages = \"389--399\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.31.pdf",
        "site": "https://aclanthology.org/2022.coling-1.31/",
        "pdf_size": 367679,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11349991858602908337&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + School of Artificial Intelligence, Peking University + State Key Laboratory of Media Convergence Production Technology and Systems; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University + School of Artificial Intelligence, Peking University + State Key Laboratory of Media Convergence Production Technology and Systems; Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University; University of Technology Sydney; Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + State Key Laboratory of Media Convergence Production Technology and Systems",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;uts.edu.au;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;uts.edu.au;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0+1;0;0+0+1;0+0;2;0+0+1",
        "aff_unique_norm": "Peking University;State Key Laboratory of Media Convergence Production Technology and Systems;University of Technology Sydney",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;;",
        "aff_unique_url": "http://www.pku.edu.cn;;https://www.uts.edu.au",
        "aff_unique_abbr": "PKU;;UTS",
        "aff_campus_unique_index": ";;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0;0+0+0;0+0;1;0+0+0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.coling-1.626",
        "title": "Recycle Your Wav2Vec2 Codebook: A Speech Perceiver for Keyword Spotting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Speech information in a pretrained wav2vec2.0 model is usually leveraged through its encoder, which has at least 95M parameters, being not so suitable for small footprint Keyword Spotting. In this work, we show an efficient way of profiting from wav2vec2.0\u2019s linguistic knowledge, by recycling the phonetic information encoded in its latent codebook, which has been typically thrown away after pretraining. We do so by transferring the codebook as weights for the latent bottleneck of a Keyword Spotting Perceiver, thus initializing such model with phonetic embeddings already. The Perceiver design relies on cross-attention between these embeddings and input data to generate better representations. Our method delivers accuracy gains compared to random initialization, at no latency costs. Plus, we show that the phonetic embeddings can easily be downsampled with k-means clustering, speeding up inference in 3.5 times at only slight accuracy penalties.",
        "author": "Guillermo C\u00e1mbara; Jordi Luque; Mireia Farr\u00fas",
        "authorids": "/g/guillermo-cambara/; /j/jordi-luque/; /m/mireia-farrus/",
        "bibtex": "@inproceedings{cambara-etal-2022-recycle,\n    title = \"Recycle Your {W}av2{V}ec2 Codebook: A Speech Perceiver for Keyword Spotting\",\n    author = \"C{\\'a}mbara, Guillermo  and\n      Luque, Jordi  and\n      Farr{\\'u}s, Mireia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.626/\",\n    pages = \"7166--7170\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.626.pdf",
        "site": "https://aclanthology.org/2022.coling-1.626/",
        "pdf_size": 316020,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aSzQ7KB2MjAJ:scholar.google.com/&scioq=Recycle+Your+Wav2Vec2+Codebook:+A+Speech+Perceiver+for+Keyword+Spotting&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "TALN Research Group, Universitat Pompeu Fabra, Barcelona, Spain; Telef\u00f3nica Research, Telef\u00f3nica I+D, Barcelona, Spain; Centre de Llenguatge i Computaci\u00f3, UBICS, Universitat de Barcelona, Barcelona, Spain",
        "aff_domain": "upf.edu;telefonica.com;ub.edu",
        "email": "upf.edu;telefonica.com;ub.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "Universitat Pompeu Fabra;Telef\u00f3nica;D;Universitat de Barcelona",
        "aff_unique_dep": "TALN Research Group;Telef\u00f3nica Research;;Centre de Llenguatge i Computaci\u00f3, UBICS",
        "aff_unique_url": "https://www.upf.edu;https://www.telefonica.com;;https://www.ub.edu",
        "aff_unique_abbr": ";Telef\u00f3nica;;UB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Barcelona;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2022.coling-1.151",
        "title": "Reducing Spurious Correlations for Answer Selection by Feature Decorrelation and Language Debiasing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Deep neural models have become the mainstream in answer selection, yielding state-of-the-art performance. However, these models tend to rely on spurious correlations between prediction labels and input features, which in general suffer from robustness and generalization. In this paper, we propose a novel Spurious Correlation reduction method to improve the robustness of the neural ANswer selection models (SCAN) from the sample and feature perspectives by removing the feature dependencies and language biases in answer selection. First, from the sample perspective, we propose a feature decorrelation module by learning a weight for each instance at the training phase to remove the feature dependencies and reduce the spurious correlations without prior knowledge of such correlations. Second, from the feature perspective, we propose a feature debiasing module with contrastive learning to alleviate the negative language biases (spurious correlations) and further improve the robustness of the AS models. Experimental results on three benchmark datasets show that SCAN achieves substantial improvements over strong baselines. For reproducibility, we will release our code and data upon the publication of this paper.",
        "author": "Zeyi Zhong; Min Yang; Ruifeng Xu",
        "authorids": "/z/zeyi-zhong/; /m/min-yang/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{zhong-etal-2022-reducing,\n    title = \"Reducing Spurious Correlations for Answer Selection by Feature Decorrelation and Language Debiasing\",\n    author = \"Zhong, Zeyi  and\n      Yang, Min  and\n      Xu, Ruifeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.151/\",\n    pages = \"1753--1764\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.151.pdf",
        "site": "https://aclanthology.org/2022.coling-1.151/",
        "pdf_size": 541431,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2372240153850955040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences+University of Science and Technology of China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; Harbin Institute of Technology (Shenzhen)",
        "aff_domain": "siat.ac.cn;siat.ac.cn;hit.edu.cn",
        "email": "siat.ac.cn;siat.ac.cn;hit.edu.cn",
        "github": "https://github.com/xish9/SCAN",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;2",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;University of Science and Technology of China;Harbin Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.siat.cas.cn;http://www.ustc.edu.cn;http://en.hhit.edu.cn/",
        "aff_unique_abbr": "SIAT;USTC;HIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.401",
        "title": "Reinforcement Learning with Large Action Spaces for Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Applying Reinforcement learning (RL) following maximum likelihood estimation (MLE) pre-training is a versatile method for enhancing neural machine translation (NMT) performance. However, recent work has argued that the gains produced by RL for NMT are mostly due to promoting tokens that have already received a fairly high probability in pre-training. We hypothesize that the large action space is a main obstacle to RL\u2019s effectiveness in MT, and conduct two sets of experiments that lend support to our hypothesis. First, we find that reducing the size of the vocabulary improves RL\u2019s effectiveness. Second, we find that effectively reducing the dimension of the action space without changing the vocabulary also yields notable improvement as evaluated by BLEU, semantic similarity, and human evaluation. Indeed, by initializing the network\u2019s final fully connected layer (that maps the network\u2019s internal dimension to the vocabulary dimension), with a layer that generalizes over similar actions, we obtain a substantial improvement in RL performance: 1.5 BLEU points on average.",
        "author": "Asaf Yehudai; Leshem Choshen; Lior Fox; Omri Abend",
        "authorids": "/a/asaf-yehudai/; /l/leshem-choshen/; /l/lior-fox/; /o/omri-abend/",
        "bibtex": "@inproceedings{yehudai-etal-2022-reinforcement,\n    title = \"Reinforcement Learning with Large Action Spaces for Neural Machine Translation\",\n    author = \"Yehudai, Asaf  and\n      Choshen, Leshem  and\n      Fox, Lior  and\n      Abend, Omri\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.401/\",\n    pages = \"4544--4556\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.401.pdf",
        "site": "https://aclanthology.org/2022.coling-1.401/",
        "pdf_size": 1074838,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14710055393727584097&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, The Hebrew University of Jerusalem; Department of Computer Science, The Hebrew University of Jerusalem; Department of Computer Science, The Hebrew University of Jerusalem; Department of Computer Science, The Hebrew University of Jerusalem",
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "github": "https://github.com/AsafYehudai/Reinforcement-Learning-with-Large-Action-Spaces-for-Neural-Machine-Translation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The Hebrew University of Jerusalem",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.coling-1.136",
        "title": "Repo4QA: Answering Coding Questions via Dense Retrieval on GitHub Repositories",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Open-source platforms such as GitHub and Stack Overflow both play significant roles in current software ecosystems. It is crucial but time-consuming for developers to raise programming questions in coding forums such as Stack Overflow and be navigated to actual solutions on GitHub repositories. In this paper, we dedicate to accelerating this activity. We find that traditional information retrieval-based methods fail to handle the long and complex questions in coding forums, and thus cannot find suitable coding repositories. To effectively and efficiently bridge the semantic gap between repositories and real-world coding questions, we introduce a specialized dataset named Repo4QA, which includes over 12,000 question-repository pairs constructed from Stack Overflow and GitHub. Furthermore, we propose QuRep, a CodeBERT-based model that jointly learns the representation of both questions and repositories. Experimental results demonstrate that our model simultaneously captures the semantic features in both questions and repositories through supervised contrastive loss and hard negative sampling. We report that our approach outperforms existing state-of-art methods by 3%-8% on MRR and 5%-8% on P@1.",
        "author": "Minyu Chen; Guoqiang Li; Chen Ma; Jingyang Li; Hongfei Fu",
        "authorids": "/m/minyu-chen/; /g/guoqiang-li/; /c/chen-ma/; /j/jingyang-li/; /h/hongfei-fu/",
        "bibtex": "@inproceedings{chen-etal-2022-repo4qa,\n    title = \"{R}epo4{QA}: Answering Coding Questions via Dense Retrieval on {G}it{H}ub Repositories\",\n    author = \"Chen, Minyu  and\n      Li, Guoqiang  and\n      Ma, Chen  and\n      Li, Jingyang  and\n      Fu, Hongfei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.136/\",\n    pages = \"1580--1592\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.136.pdf",
        "site": "https://aclanthology.org/2022.coling-1.136/",
        "pdf_size": 695140,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8481444090789275341&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; City University of Hong Kong; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cityu.edu.hk;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cityu.edu.hk;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/Minkow/Repo4QA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;City University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.cityu.edu.hk",
        "aff_unique_abbr": "SJTU;CityU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.328",
        "title": "Reproducibility and Automation of the Appraisal Taxonomy",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "There is a lack of reproducibility in results from experiments that apply the Appraisal taxonomy. Appraisal is widely used by linguists to study how people judge things or people. Automating Appraisal could be beneficial for use cases such as moderating online comments. Past work in Appraisal annotation has been descriptive in nature and, the lack of publicly available data sets hinders the progress of automation. In this work, we are interested in two things; first, measuring the performance of automated approaches to Appraisal classification in the publicly available Australasian Language Technology Association (ALTA) Shared Task Challenge data set. Second, we are interested in reproducing the annotation of the ALTA data set. Four additional annotators, each with a different linguistics background, were employed to re-annotate the data set. Our results show a poor level of agreement at more detailed Appraisal categories (Fleiss Kappa = 0.059) and a fair level of agreement (Kappa = 0.372) at coarse-level categories. We find similar results when using automated approaches that are available publicly. Our empirical evidence suggests that at present, automating classification is practical only when considering coarse-level categories of the taxonomy.",
        "author": "Pradeesh Parameswaran; Andrew Trotman; Veronica Liesaputra; David Eyers",
        "authorids": "/p/pradeesh-parameswaran/; /a/andrew-trotman/; /v/veronica-liesaputra/; /d/david-eyers/",
        "bibtex": "@inproceedings{parameswaran-etal-2022-reproducibility,\n    title = \"Reproducibility and Automation of the Appraisal Taxonomy\",\n    author = \"Parameswaran, Pradeesh  and\n      Trotman, Andrew  and\n      Liesaputra, Veronica  and\n      Eyers, David\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.328/\",\n    pages = \"3731--3740\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.328.pdf",
        "site": "https://aclanthology.org/2022.coling-1.328/",
        "pdf_size": 296850,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11235765949244597550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Otago, New Zealand; Department of Computer Science, University of Otago, New Zealand; Department of Computer Science, University of Otago, New Zealand; Department of Computer Science, University of Otago, New Zealand",
        "aff_domain": "cs.otago.ac.nz;cs.otago.ac.nz;cs.otago.ac.nz;cs.otago.ac.nz",
        "email": "cs.otago.ac.nz;cs.otago.ac.nz;cs.otago.ac.nz;cs.otago.ac.nz",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Otago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.otago.ac.nz",
        "aff_unique_abbr": "Otago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "2022.coling-1.331",
        "title": "Resource of Wikipedias in 31 Languages Categorized into Fine-Grained Named Entities",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper describes a resource of Wikipedias in 31 languages categorized into Extended Named Entity (ENE), which has 219 fine-grained NE categories. We first categorized 920 K Japanese Wikipedia pages according to the ENE scheme using machine learning, followed by manual validation. We then organized a shared task of Wikipedia categorization into 30 languages. The training data were provided by Japanese categorization and the language links, and the task was to categorize the Wikipedia pages into 30 languages, with no language links from Japanese Wikipedia (20M pages in total). Thirteen groups with 24 systems participated in the 2020 and 2021 tasks, sharing their outputs for resource-building. The Japanese categorization accuracy was 98.5%, and the best performance among the 30 languages ranges from 80 to 93 in F-measure. Using ensemble learning, we created outputs with an average F-measure of 86.8, which is 1.7 better than the best single systems. The total size of the resource is 32.5M pages, including the training data. We call this resource creation scheme \u201cResource by Collaborative Contribution (RbCC)\u201d. We also constructed structuring tasks (attribute extraction and link prediction) using RbCC under our ongoing project, \u201cSHINRA.\u201d",
        "author": "Satoshi Sekine; Kouta Nakayama; Masako Nomoto; Maya Ando; Asuka Sumida; Koji Matsuda",
        "authorids": "/s/satoshi-sekine/; /k/kouta-nakayama/; /m/masako-nomoto/; /m/maya-ando/; /a/asuka-sumida/; /k/koji-matsuda/",
        "bibtex": "@inproceedings{sekine-etal-2022-resource,\n    title = \"Resource of {W}ikipedias in 31 Languages Categorized into Fine-Grained Named Entities\",\n    author = \"Sekine, Satoshi  and\n      Nakayama, Kouta  and\n      Nomoto, Masako  and\n      Ando, Maya  and\n      Sumida, Asuka  and\n      Matsuda, Koji\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.331/\",\n    pages = \"3769--3777\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.331.pdf",
        "site": "https://aclanthology.org/2022.coling-1.331/",
        "pdf_size": 1852619,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:l6wmpDbOF54J:scholar.google.com/&scioq=Resource+of+Wikipedias+in+31+Languages+Categorized+into+Fine-Grained+Named+Entities&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "RIKEN, AIP; RIKEN, AIP; RIKEN, AIP; free; RIKEN, AIP; RIKEN, AIP",
        "aff_domain": "riken.jp; ; ; ; ; ",
        "email": "riken.jp; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "RIKEN;",
        "aff_unique_dep": "Advanced Institute for Computational Science (AICS);",
        "aff_unique_url": "https://www.riken.jp;",
        "aff_unique_abbr": "RIKEN;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan;"
    },
    {
        "id": "2022.coling-1.99",
        "title": "Rethinking Data Augmentation in Text-to-text Paradigm",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As manually labelling data can be costly, some recent studies tend to augment the training data for improving the generalization power of machine learning models, known as data augmentation (DA). With the arise of pre-trained language models (PLMs), some recent works on DA try to synthesize new samples benefiting from the knowledge learned from PLM\u2019s pre-training. Along the same direction, we in this paper propose to integrate text-to-text language models and construct a new two-phase framework for augmentation: 1) a fine-tuning phase where PLMs are well adapted to downstream classification with the help of two novel schemes, and 2) a generation phase where the fine-tuned models are leveraged to create new samples for performance lifting. This paradigm opens up a new way of designing fine-tuning scheme to better serve DA in an easy-to-implement manner, and can be easily extended to other desired tasks. We evaluate our proposal on two public classification datasets and demonstrate its effectiveness with remarkable gains.",
        "author": "Yanan Chen; Yang Liu",
        "authorids": "/y/yanan-chen/; /y/yang-liu-wl/",
        "bibtex": "@inproceedings{chen-liu-2022-rethinking,\n    title = \"Rethinking Data Augmentation in Text-to-text Paradigm\",\n    author = \"Chen, Yanan  and\n      Liu, Yang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.99/\",\n    pages = \"1157--1162\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.99.pdf",
        "site": "https://aclanthology.org/2022.coling-1.99/",
        "pdf_size": 322962,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6791551937471768716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Dept. of Physics and Computer Science, Wilfrid Laurier University; Dept. of Physics and Computer Science, Wilfrid Laurier University",
        "aff_domain": "mylaurier.ca;wlu.ca",
        "email": "mylaurier.ca;wlu.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Wilfrid Laurier University",
        "aff_unique_dep": "Dept. of Physics and Computer Science",
        "aff_unique_url": "https://www.wlu.ca",
        "aff_unique_abbr": "WLU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.coling-1.13",
        "title": "Revisiting Statistical Laws of Semantic Shift in Romance Cognates",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This article revisits statistical relationships across Romance cognates between lexical semantic shift and six intra-linguistic variables, such as frequency and polysemy. Cognates are words that are derived from a common etymon, in this case, a Latin ancestor. Despite their shared etymology, some cognate pairs have experienced semantic shift. The degree of semantic shift is quantified using cosine distance between the cognates\u2019 corresponding word embeddings. In the previous literature, frequency and polysemy have been reported to be correlated with semantic shift; however, the understanding of their effects needs revision because of various methodological defects. In the present study, we perform regression analysis under improved experimental conditions, and demonstrate a genuine negative effect of frequency and positive effect of polysemy on semantic shift. Furthermore, we reveal that morphologically complex etyma are more resistant to semantic shift and that the cognates that have been in use over a longer timespan are prone to greater shift in meaning. These findings add to our understanding of the historical process of semantic change.",
        "author": "Yoshifumi Kawasaki; Ma\u00eblys Salingre; Marzena Karpinska; Hiroya Takamura; Ryo Nagata",
        "authorids": "/y/yoshifumi-kawasaki/; /m/maelys-salingre/; /m/marzena-karpinska/; /h/hiroya-takamura/; /r/ryo-nagata/",
        "bibtex": "@inproceedings{kawasaki-etal-2022-revisiting,\n    title = \"Revisiting Statistical Laws of Semantic Shift in {R}omance Cognates\",\n    author = {Kawasaki, Yoshifumi  and\n      Salingre, Ma{\\\"e}lys  and\n      Karpinska, Marzena  and\n      Takamura, Hiroya  and\n      Nagata, Ryo},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.13/\",\n    pages = \"141--151\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.13.pdf",
        "site": "https://aclanthology.org/2022.coling-1.13/",
        "pdf_size": 1450449,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17922064973453987928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "The University of Tokyo; Mejiro University; University of Massachusetts Amherst; AIST; Konan University",
        "aff_domain": "g.ecc.u-tokyo.ac.jp;mejiro.ac.jp;cs.umass.edu;aist.go.jp;ml.hyogo-u.ac.jp",
        "email": "g.ecc.u-tokyo.ac.jp;mejiro.ac.jp;cs.umass.edu;aist.go.jp;ml.hyogo-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "University of Tokyo;Mejiro University;University of Massachusetts Amherst;Advanced Industrial Science and Technology;Konan University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.mejiro-u.ac.jp;https://www.umass.edu;https://www.aist.go.jp;https://www.konan-u.ac.jp",
        "aff_unique_abbr": "UTokyo;Mejiro U;UMass Amherst;AIST;Konan U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2022.coling-1.374",
        "title": "Revisiting Syllables in Language Modelling and Their Application on Low-Resource Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language modelling and machine translation tasks mostly use subword or character inputs, but syllables are seldom used. Syllables provide shorter sequences than characters, require less-specialised extracting rules than morphemes, and their segmentation is not impacted by the corpus size. In this study, we first explore the potential of syllables for open-vocabulary language modelling in 21 languages. We use rule-based syllabification methods for six languages and address the rest with hyphenation, which works as a syllabification proxy. With a comparable perplexity, we show that syllables outperform characters and other subwords. Moreover, we study the importance of syllables on neural machine translation for a non-related and low-resource language-pair (Spanish\u2013Shipibo-Konibo). In pairwise and multilingual systems, syllables outperform unsupervised subwords, and further morphological segmentation methods, when translating into a highly synthetic language with a transparent orthography (Shipibo-Konibo). Finally, we perform some human evaluation, and discuss limitations and opportunities.",
        "author": "Arturo Oncevay; Kervy Dante Rivas Rojas; Liz Karen Chavez Sanchez; Roberto Zariquiey",
        "authorids": "/a/arturo-oncevay/; /k/kervy-dante-rivas-rojas/; /l/liz-karen-chavez-sanchez/; /r/roberto-zariquiey/",
        "bibtex": "@inproceedings{oncevay-etal-2022-revisiting,\n    title = \"Revisiting Syllables in Language Modelling and Their Application on Low-Resource Machine Translation\",\n    author = \"Oncevay, Arturo  and\n      Rivas Rojas, Kervy Dante  and\n      Chavez Sanchez, Liz Karen  and\n      Zariquiey, Roberto\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.374/\",\n    pages = \"4258--4267\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.374.pdf",
        "site": "https://aclanthology.org/2022.coling-1.374/",
        "pdf_size": 403337,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1448104777669396786&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Informatics, University of Edinburgh, Scotland+Pontificia Universidad Cat\u00f3lica del Per\u00fa (IA-PUCP | Chana Field Station), Peru; Pontificia Universidad Cat\u00f3lica del Per\u00fa (IA-PUCP | Chana Field Station), Peru; Pontificia Universidad Cat\u00f3lica del Per\u00fa (IA-PUCP | Chana Field Station), Peru; Pontificia Universidad Cat\u00f3lica del Per\u00fa (IA-PUCP | Chana Field Station), Peru",
        "aff_domain": "ed.ac.uk; ; ;pucp.edu.pe",
        "email": "ed.ac.uk; ; ;pucp.edu.pe",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "University of Edinburgh;Pontificia Universidad Cat\u00f3lica del Per\u00fa",
        "aff_unique_dep": "School of Informatics;",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.pucp.edu.pe",
        "aff_unique_abbr": "Edinburgh;PUCP",
        "aff_campus_unique_index": "0+1;1;1;1",
        "aff_campus_unique": "Edinburgh;Chana Field Station",
        "aff_country_unique_index": "0+1;1;1;1",
        "aff_country_unique": "United Kingdom;Peru"
    },
    {
        "id": "2022.coling-1.479",
        "title": "Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a recent paradigm that attempts to induce constituency parse trees relying only on the internal knowledge of pre-trained language models. While attractive in the perspective that similar to in-context learning, it does not require task-specific fine-tuning, the practical effectiveness of such an approach still remains unclear, except that it can function as a probe for investigating language models\u2019 inner workings. In this work, we mathematically reformulate CPE-PLM and propose two advanced ensemble methods tailored for it, demonstrating that the new parsing paradigm can be competitive with common unsupervised parsers by introducing a set of heterogeneous PLMs combined using our techniques. Furthermore, we explore some scenarios where the trees generated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM is more effective than typical supervised parsers in few-shot settings.",
        "author": "Taeuk Kim",
        "authorids": "/t/taeuk-kim/",
        "bibtex": "@inproceedings{kim-2022-revisiting,\n    title = \"Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models\",\n    author = \"Kim, Taeuk\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.479/\",\n    pages = \"5398--5408\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.479.pdf",
        "site": "https://aclanthology.org/2022.coling-1.479/",
        "pdf_size": 736068,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9906227114427272126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Computer Science & Dept. of Artificial Intelligence, Hanyang University, Seoul, South Korea",
        "aff_domain": "hanyang.ac.kr",
        "email": "hanyang.ac.kr",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Hanyang University",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "http://www.hanyang.ac.kr",
        "aff_unique_abbr": "HYU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.429",
        "title": "Reweighting Strategy Based on Synthetic Data Identification for Sentence Similarity",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantically meaningful sentence embeddings are important for numerous tasks in natural language processing. To obtain such embeddings, recent studies explored the idea of utilizing synthetically generated data from pretrained language models(PLMs) as a training corpus. However, PLMs often generate sentences different from the ones written by human. We hypothesize that treating all these synthetic examples equally for training can have an adverse effect on learning semantically meaningful embeddings. To analyze this, we first train a classifier that identifies machine-written sentences and observe that the linguistic features of the sentences identified as written by a machine are significantly different from those of human-written sentences. Based on this, we propose a novel approach that first trains the classifier to measure the importance of each sentence. The distilled information from the classifier is then used to train a reliable sentence embedding model. Through extensive evaluation on four real-world datasets, we demonstrate that our model trained on synthetic data generalizes well and outperforms the baselines.",
        "author": "TaeHee Kim; ChaeHun Park; Jimin Hong; Radhika Dua; Edward Choi; Jaegul Choo",
        "authorids": "/t/taehee-kim/; /c/chaehun-park/; /j/jimin-hong/; /r/radhika-dua/; /e/edward-choi/; /j/jaegul-choo/",
        "bibtex": "@inproceedings{kim-etal-2022-reweighting,\n    title = \"Reweighting Strategy Based on Synthetic Data Identification for Sentence Similarity\",\n    author = \"Kim, TaeHee  and\n      Park, ChaeHun  and\n      Hong, Jimin  and\n      Dua, Radhika  and\n      Choi, Edward  and\n      Choo, Jaegul\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.429/\",\n    pages = \"4853--4863\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.429.pdf",
        "site": "https://aclanthology.org/2022.coling-1.429/",
        "pdf_size": 374697,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15589503205883906201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "KAIST AI\u2020; KAIST AI\u2020; KAIST AI; KAIST AI; KAIST AI; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/ddehun/coling2022_reweighting_sts",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "KAIST AI",
        "aff_unique_url": "https://www.kaist.edu",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.436",
        "title": "RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge graph embedding, which aims to learn representations of entities and relations in knowledge graphs, finds applications in various downstream tasks. The key to success of knowledge graph embedding models are the ability to model relation patterns including symmetry/antisymmetry, inversion, commutative composition and non-commutative composition. Although existing methods fail in modeling the non-commutative composition patterns, several approaches support this pattern by modeling beyond Euclidean space and complex space. Nevertheless, expanding to complicated spaces such as quaternion can easily lead to a substantial increase in the amount of parameters, which greatly reduces the computational efficiency. In this paper, we propose a new knowledge graph embedding method called RotateCT, which first transforms the coordinates of each entity, and then represents each relation as a rotation from head entity to tail entity in complex space. By design, RotateCT can infer the non-commutative composition patterns and improve the computational efficiency. Experiments on multiple datasets empirically show that RotateCT outperforms most state-of-the-art methods on link prediction and path query answering.",
        "author": "Yao Dong; Lei Wang; Ji Xiang; Xiaobo Guo; Yuqiang Xie",
        "authorids": "/y/yao-dong/; /l/lei-wang/; /j/ji-xiang/; /x/xiaobo-guo/; /y/yuqiang-xie/",
        "bibtex": "@inproceedings{dong-etal-2022-rotatect,\n    title = \"{R}otate{CT}: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space\",\n    author = \"Dong, Yao  and\n      Wang, Lei  and\n      Xiang, Ji  and\n      Guo, Xiaobo  and\n      Xie, Yuqiang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.436/\",\n    pages = \"4918--4932\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.436.pdf",
        "site": "https://aclanthology.org/2022.coling-1.436/",
        "pdf_size": 629907,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1087465021297130308&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.225",
        "title": "Ruleformer: Context-aware Rule Mining over Knowledge Graph",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Rule mining is an effective approach for reasoning over knowledge graph (KG). Existing works mainly concentrate on mining rules. However, there might be several rules that could be applied for reasoning for one relation, and how to select appropriate rules for completion of different triples has not been discussed. In this paper, we propose to take the context information into consideration, which helps select suitable rules for the inference tasks. Based on this idea, we propose a transformer-based rule mining approach, Ruleformer. It consists of two blocks: 1) an encoder extracting the context information from subgraph of head entities with modified attention mechanism, and 2) a decoder which aggregates the subgraph information from the encoder output and generates the probability of relations for each step of reasoning. The basic idea behind Ruleformer is regarding rule mining process as a sequence to sequence task. To make the subgraph a sequence input to the encoder and retain the graph structure, we devise a relational attention mechanism in Transformer. The experiment results show the necessity of considering these information in rule mining task and the effectiveness of our model.",
        "author": "Zezhong Xu; Peng Ye; Hui Chen; Meng Zhao; Huajun Chen; Wen Zhang",
        "authorids": "/z/zezhong-xu/; /p/peng-ye/; /h/hui-chen/; /m/meng-zhao/; /h/huajun-chen/; /w/wen-zhang/",
        "bibtex": "@inproceedings{xu-etal-2022-ruleformer,\n    title = \"Ruleformer: Context-aware Rule Mining over Knowledge Graph\",\n    author = \"Xu, Zezhong  and\n      Ye, Peng  and\n      Chen, Hui  and\n      Zhao, Meng  and\n      Chen, Huajun  and\n      Zhang, Wen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.225/\",\n    pages = \"2551--2560\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.225.pdf",
        "site": "https://aclanthology.org/2022.coling-1.225/",
        "pdf_size": 839566,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15697968210861816085&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Zhejiang University & AZFT Joint Lab for Knowledge Engine, China; Zhejiang University & AZFT Joint Lab for Knowledge Engine, China; Alibaba Group; Huawei; Zhejiang University & AZFT Joint Lab for Knowledge Engine, China+Hangzhou Innovation Center, Zhejiang University; Zhejiang University & AZFT Joint Lab for Knowledge Engine, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;alibaba-inc.com;huawei.com;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;alibaba-inc.com;huawei.com;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/zjukg/ruleformer",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0+0;0",
        "aff_unique_norm": "Zhejiang University;Alibaba Group;Huawei Technologies Co., Ltd.",
        "aff_unique_dep": "Joint Lab for Knowledge Engine;;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.alibaba.com;https://www.huawei.com",
        "aff_unique_abbr": "ZJU;Alibaba;Huawei",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.499",
        "title": "SANCL: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "With the boom of e-commerce, Multimodal Review Helpfulness Prediction (MRHP) that identifies the helpfulness score of multimodal product reviews has become a research hotspot. Previous work on this task focuses on attention-based modality fusion, information integration, and relation modeling, which primarily exposes the following drawbacks: 1) the model may fail to capture the really essential information due to its indiscriminate attention formulation; 2) lack appropriate modeling methods that takes full advantage of correlation among provided data. In this paper, we propose SANCL: Selective Attention and Natural Contrastive Learning for MRHP. SANCL adopts a probe-based strategy to enforce high attention weights on the regions of greater significance. It also constructs a contrastive learning framework based on natural matching properties in the dataset. Experimental results on two benchmark datasets with three categories show that SANCL achieves state-of-the-art baseline performance with lower memory consumption.",
        "author": "Wei Han; Hui Chen; Zhen Hai; Soujanya Poria; Lidong Bing",
        "authorids": "/w/wei-han/; /h/hui-chen/; /z/zhen-hai/; /s/soujanya-poria/; /l/lidong-bing/",
        "bibtex": "@inproceedings{han-etal-2022-sancl,\n    title = \"{SANCL}: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning\",\n    author = \"Han, Wei  and\n      Chen, Hui  and\n      Hai, Zhen  and\n      Poria, Soujanya  and\n      Bing, Lidong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.499/\",\n    pages = \"5666--5677\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.499.pdf",
        "site": "https://aclanthology.org/2022.coling-1.499/",
        "pdf_size": 945818,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=804202606832767636&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Singapore University of Technology and Design; Singapore University of Technology and Design; DAMO Academy, Alibaba Group + Singapore University of Technology and Design; Singapore University of Technology and Design; DAMO Academy, Alibaba Group",
        "aff_domain": "mymail.sutd.edu.sg;mymail.sutd.edu.sg;alibaba-inc.com;sutd.edu.sg;alibaba-inc.com",
        "email": "mymail.sutd.edu.sg;mymail.sutd.edu.sg;alibaba-inc.com;sutd.edu.sg;alibaba-inc.com",
        "github": "https://github.com/declare-lab/SANCL",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+0;0;1",
        "aff_unique_norm": "Singapore University of Technology and Design;Alibaba Group",
        "aff_unique_dep": ";DAMO Academy",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.alibaba-group.com",
        "aff_unique_abbr": "SUTD;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0;0;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2022.coling-1.202",
        "title": "SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Unlabeled Entity Problem (UEP) in Named Entity Recognition (NER) datasets seriously hinders the improvement of NER performance. This paper proposes SCL-RAI to cope with this problem. Firstly, we decrease the distance of span representations with the same label while increasing it for different ones via span-based contrastive learning, which relieves the ambiguity among entities and improves the robustness of the model over unlabeled entities. Then we propose retrieval augmented inference to mitigate the decision boundary shifting problem. Our method significantly outperforms the previous SOTA method by 4.21% and 8.64% F1-score on two real-world datasets.",
        "author": "Shuzheng Si; Shuang Zeng; Jiaxing Lin; Baobao Chang",
        "authorids": "/s/shuzheng-si/; /s/shuang-zeng/; /j/jiaxing-lin/; /b/baobao-chang/",
        "bibtex": "@inproceedings{si-etal-2022-scl,\n    title = \"{SCL}-{RAI}: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in {NER}\",\n    author = \"Si, Shuzheng  and\n      Zeng, Shuang  and\n      Lin, Jiaxing  and\n      Chang, Baobao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.202/\",\n    pages = \"2313--2318\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.202.pdf",
        "site": "https://aclanthology.org/2022.coling-1.202/",
        "pdf_size": 608479,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7161445856754578160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China+School of Software and Microelectronics, Peking University, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China+School of Software and Microelectronics, Peking University, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China+School of Software and Microelectronics, Peking University, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China+School of Software and Microelectronics, Peking University, China",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.224",
        "title": "SEE-Few: Seed, Expand and Entail for Few-shot Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Few-shot named entity recognition (NER) aims at identifying named entities based on only few labeled instances. Current few-shot NER methods focus on leveraging existing datasets in the rich-resource domains which might fail in a training-from-scratch setting where no source-domain data is used. To tackle training-from-scratch setting, it is crucial to make full use of the annotation information (the boundaries and entity types). Therefore, in this paper, we propose a novel multi-task (Seed, Expand and Entail) learning framework, SEE-Few, for Few-shot NER without using source domain data. The seeding and expanding modules are responsible for providing as accurate candidate spans as possible for the entailing module. The entailing module reformulates span classification as a textual entailment task, leveraging both the contextual clues and entity type information. All the three modules share the same text encoder and are jointly learned. Experimental results on several benchmark datasets under the training-from-scratch setting show that the proposed method outperformed several state-of-the-art few-shot NER methods with a large margin. Our code is available at https://github.com/unveiled-the-red-hat/SEE-Few.",
        "author": "Zeng Yang; Linhai Zhang; Deyu Zhou",
        "authorids": "/z/zeng-yang/; /l/linhai-zhang/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{yang-etal-2022-see,\n    title = \"{SEE}-Few: Seed, Expand and Entail for Few-shot Named Entity Recognition\",\n    author = \"Yang, Zeng  and\n      Zhang, Linhai  and\n      Zhou, Deyu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.224/\",\n    pages = \"2540--2550\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.224.pdf",
        "site": "https://aclanthology.org/2022.coling-1.224/",
        "pdf_size": 558974,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15628198743130401567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/unveiled-the-red-hat/SEE-Few",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.406",
        "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature\u2014presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",
        "author": "Edoardo Mosca; Ferenc Szigeti; Stella Tragianni; Daniel Gallagher; Georg Groh",
        "authorids": "/e/edoardo-mosca/; /f/ferenc-szigeti/; /s/stella-tragianni/; /d/daniel-gallagher/; /g/georg-groh/",
        "bibtex": "@inproceedings{mosca-etal-2022-shap,\n    title = \"{SHAP}-Based Explanation Methods: A Review for {NLP} Interpretability\",\n    author = \"Mosca, Edoardo  and\n      Szigeti, Ferenc  and\n      Tragianni, Stella  and\n      Gallagher, Daniel  and\n      Groh, Georg\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.406/\",\n    pages = \"4593--4603\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.406.pdf",
        "site": "https://aclanthology.org/2022.coling-1.406/",
        "pdf_size": 459140,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16433318952448109025&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "TU Munich, Department of Informatics, Germany; TU Munich, Department of Informatics, Germany; TU Munich, Department of Informatics, Germany; University College Dublin, Department of Informatics, Ireland; TU Munich, Department of Informatics, Germany",
        "aff_domain": "tum.de;tum.de;tum.de;ucdconnect.ie;in.tum.de",
        "email": "tum.de;tum.de;tum.de;ucdconnect.ie;in.tum.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Technical University of Munich;University College Dublin",
        "aff_unique_dep": "Department of Informatics;Department of Informatics",
        "aff_unique_url": "https://www.tum.de;https://www.ucd.ie",
        "aff_unique_abbr": "TUM;UCD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Germany;Ireland"
    },
    {
        "id": "2022.coling-1.117",
        "title": "SISER: Semantic-Infused Selective Graph Reasoning for Fact Verification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This study proposes Semantic-Infused SElective Graph Reasoning (SISER) for fact verification, which newly presents semantic-level graph reasoning and injects its reasoning-enhanced representation into other types of graph-based and sequence-based reasoning methods. SISER combines three reasoning types: 1) semantic-level graph reasoning, which uses a semantic graph from evidence sentences, whose nodes are elements of a triple \u2013 <Subject, Verb, Object>, 2) \u201csemantic-infused\u201d sentence-level \u201cselective\u201d graph reasoning, which combine semantic-level and sentence-level representations and perform graph reasoning in a selective manner using the node selection mechanism, and 3) sequence reasoning, which concatenates all evidence sentences and performs attention-based reasoning. Experiment results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that SISER outperforms the previous graph-based approaches and achieves state-of-the-art performance.",
        "author": "Eunhwan Park; Jong-Hyeon Lee; DongHyeon Jeon; Seonhoon Kim; Inho Kang; Seung-Hoon Na",
        "authorids": "/e/eunhwan-park/; /j/jong-hyeon-lee/; /d/donghyeon-jeon/; /s/seonhoon-kim/; /i/inho-kang/; /s/seung-hoon-na/",
        "bibtex": "@inproceedings{park-etal-2022-siser,\n    title = \"{SISER}: Semantic-Infused Selective Graph Reasoning for Fact Verification\",\n    author = \"Park, Eunhwan  and\n      Lee, Jong-Hyeon  and\n      Jeon, DongHyeon  and\n      Kim, Seonhoon  and\n      Kang, Inho  and\n      Na, Seung-Hoon\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.117/\",\n    pages = \"1367--1378\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.117.pdf",
        "site": "https://aclanthology.org/2022.coling-1.117/",
        "pdf_size": 701454,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17284023750331504807&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://competitions.codalab.org/competitions/18814",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.108",
        "title": "SOS: Systematic Offensive Stereotyping Bias in Word Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Systematic Offensive stereotyping (SOS) in word embeddings could lead to associating marginalised groups with hate speech and profanity, which might lead to blocking and silencing those groups, especially on social media platforms. In this [id=stk]work, we introduce a quantitative measure of the SOS bias, [id=stk]validate it in the most commonly used word embeddings, and investigate if it explains the performance of different word embeddings on the task of hate speech detection. Results show that SOS bias exists in almost all examined word embeddings and that [id=stk]the proposed SOS bias metric correlates positively with the statistics of published surveys on online extremism. We also show that the [id=stk]proposed metric reveals distinct information [id=stk]compared to established social bias metrics. However, we do not find evidence that SOS bias explains the performance of hate speech detection models based on the different word embeddings.",
        "author": "Fatma Elsafoury; Steve R. Wilson; Stamos Katsigiannis; Naeem Ramzan",
        "authorids": "/f/fatma-elsafoury/; /s/steve-r-wilson/; /s/stamos-katsigiannis/; /n/naeem-ramzan/",
        "bibtex": "@inproceedings{elsafoury-etal-2022-sos,\n    title = \"{SOS}: Systematic Offensive Stereotyping Bias in Word Embeddings\",\n    author = \"Elsafoury, Fatma  and\n      Wilson, Steve R.  and\n      Katsigiannis, Stamos  and\n      Ramzan, Naeem\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.108/\",\n    pages = \"1263--1274\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.108.pdf",
        "site": "https://aclanthology.org/2022.coling-1.108/",
        "pdf_size": 407603,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16463086476622265703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.46",
        "title": "SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-training methods with contrastive learning objectives have shown remarkable success in dialog understanding tasks. However, current contrastive learning solely considers the self-augmented dialog samples as positive samples and treats all other dialog samples as negative ones, which enforces dissimilar representations even for dialogs that are semantically related. In this paper, we propose SPACE-2, a tree-structured pre-trained conversation model, which learns dialog representations from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised contrastive pre-training. Concretely, we first define a general semantic tree structure (STS) to unify the inconsistent annotation schema across different dialog datasets, so that the rich structural information stored in all labeled data can be exploited. Then we propose a novel multi-view score function to increase the relevance of all possible dialogs that share similar STSs and only push away other completely different dialogs during supervised contrastive pre-training. To fully exploit unlabeled dialogs, a basic self-supervised contrastive loss is also added to refine the learned representations. Experiments show that our method can achieve new state-of-the-art results on the DialoGLUE benchmark consisting of seven datasets and four popular dialog understanding tasks.",
        "author": "Wanwei He; Yinpei Dai; Binyuan Hui; Min Yang; Zheng Cao; Jianbo Dong; Fei Huang; Luo Si; Yongbin Li",
        "authorids": "/w/wanwei-he/; /y/yinpei-dai/; /b/binyuan-hui/; /m/min-yang/; /z/zheng-cao/; /j/jianbo-dong/; /f/fei-huang/; /l/luo-si/; /y/yongbin-li/",
        "bibtex": "@inproceedings{he-etal-2022-space,\n    title = \"{SPACE}-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding\",\n    author = \"He, Wanwei  and\n      Dai, Yinpei  and\n      Hui, Binyuan  and\n      Yang, Min  and\n      Cao, Zheng  and\n      Dong, Jianbo  and\n      Huang, Fei  and\n      Si, Luo  and\n      Li, Yongbin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.46/\",\n    pages = \"553--569\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.46.pdf",
        "site": "https://aclanthology.org/2022.coling-1.46/",
        "pdf_size": 3039698,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10938158750510309816&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China+University of Chinese Academy of Sciences, China; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group",
        "aff_domain": "siat.ac.cn;alibaba-inc.com;alibaba-inc.com;siat.ac.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "siat.ac.cn;alibaba-inc.com;alibaba-inc.com;siat.ac.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/space-2",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;2;2;0;2;2;2;2;2",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;University of Chinese Academy of Sciences;Alibaba Group",
        "aff_unique_dep": ";;DAMO Academy",
        "aff_unique_url": "http://www.siat.cas.cn;http://www.ucas.ac.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "SIAT;UCAS;Alibaba",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.596",
        "title": "SSR: Utilizing Simplified Stance Reasoning Process for Robust Stance Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dataset bias in stance detection tasks allows models to achieve superior performance without using targets. Most existing debiasing methods are task-agnostic, which fail to utilize task knowledge to better discriminate between genuine and bias features. Motivated by how humans tackle stance detection tasks, we propose to incorporate the stance reasoning process as task knowledge to assist in learning genuine features and reducing reliance on bias features. The full stance reasoning process usually involves identifying the span of the mentioned target and corresponding opinion expressions, such fine-grained annotations are hard and expensive to obtain. To alleviate this, we simplify the stance reasoning process to relax the granularity of annotations from token-level to sentence-level, where labels for sub-tasks can be easily inferred from existing resources. We further implement those sub-tasks by maximizing mutual information between the texts and the opinioned targets. To evaluate whether stance detection models truly understand the task from various aspects, we collect and construct a series of new test sets. Our proposed model achieves better performance than previous task-agnostic debiasing methods on most of those new test sets while maintaining comparable performances to existing stance detection models.",
        "author": "Jianhua Yuan; Yanyan Zhao; Yanyue Lu; Bing Qin",
        "authorids": "/j/jianhua-yuan/; /y/yanyan-zhao/; /y/yanyue-lu/; /b/bing-qin/",
        "bibtex": "@inproceedings{yuan-etal-2022-ssr,\n    title = \"{SSR}: Utilizing Simplified Stance Reasoning Process for Robust Stance Detection\",\n    author = \"Yuan, Jianhua  and\n      Zhao, Yanyan  and\n      Lu, Yanyue  and\n      Qin, Bing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.596/\",\n    pages = \"6846--6858\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.596.pdf",
        "site": "https://aclanthology.org/2022.coling-1.596/",
        "pdf_size": 649700,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5835784113733364626&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 0,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology+Pengcheng Laboratory",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Harbin Institute of Technology;Pengcheng Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.hit.edu.cn/;",
        "aff_unique_abbr": "HIT;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.178",
        "title": "STAD: Self-Training with Ambiguous Data for Low-Resource Relation Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a simple yet effective self-training approach, named as STAD, for low-resource relation extraction. The approach first classifies the auto-annotated instances into two groups: confident instances and uncertain instances, according to the probabilities predicted by a teacher model. In contrast to most previous studies, which mainly only use the confident instances for self-training, we make use of the uncertain instances. To this end, we propose a method to identify ambiguous but useful instances from the uncertain instances and then divide the relations into candidate-label set and negative-label set for each ambiguous instance. Next, we propose a set-negative training method on the negative-label sets for the ambiguous instances and a positive training method for the confident instances. Finally, a joint-training method is proposed to build the final relation extraction system on all data. Experimental results on two widely used datasets SemEval2010 Task-8 and Re-TACRED with low-resource settings demonstrate that this new self-training approach indeed achieves significant and consistent improvements when comparing to several competitive self-training systems.",
        "author": "Junjie Yu; Xing Wang; Jiangjiang Zhao; Chunjie Yang; Wenliang Chen",
        "authorids": "/j/junjie-yu/; /x/xing-wang/; /j/jiangjiang-zhao/; /c/chunjie-yang/; /w/wenliang-chen/",
        "bibtex": "@inproceedings{yu-etal-2022-stad,\n    title = \"{STAD}: Self-Training with Ambiguous Data for Low-Resource Relation Extraction\",\n    author = \"Yu, Junjie  and\n      Wang, Xing  and\n      Zhao, Jiangjiang  and\n      Yang, Chunjie  and\n      Chen, Wenliang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.178/\",\n    pages = \"2044--2054\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.178.pdf",
        "site": "https://aclanthology.org/2022.coling-1.178/",
        "pdf_size": 545527,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7875619836864165955&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Tencent AI Lab, Shenzhen, China; China Mobile Online Marketing and Services Center, China; China Mobile Online Marketing and Services Center, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;tencent.com;cmos.chinamobile.com;cmos.chinamobile.com;suda.edu.cn",
        "email": "stu.suda.edu.cn;tencent.com;cmos.chinamobile.com;cmos.chinamobile.com;suda.edu.cn",
        "github": "https://github.com/jjyunlp/STAD",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "Soochow University;Tencent AI Lab;China Mobile",
        "aff_unique_dep": "Institute of Artificial Intelligence, School of Computer Science and Technology;AI Lab;Online Marketing and Services Center",
        "aff_unique_url": "http://www.soochow.edu.cn;https://ai.tencent.com;https://www.chinamobile.com",
        "aff_unique_abbr": ";Tencent AI Lab;CM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.471",
        "title": "SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper aims to improve the performance of text-to-SQL parsing by exploring the intrinsic uncertainties in the neural network based approaches (called SUN). From the data uncertainty perspective, it is indisputable that a single SQL can be learned from multiple semantically-equivalent questions. Different from previous methods that are limited to one-to-one mapping, we propose a data uncertainty constraint to explore the underlying complementary semantic information among multiple semantically-equivalent questions (many-to-one) and learn the robust feature representations with reduced spurious associations. In this way, we can reduce the sensitivity of the learned representations and improve the robustness of the parser. From the model uncertainty perspective, there is often structural information (dependence) among the weights of neural networks. To improve the generalizability and stability of neural text-to-SQL parsers, we propose a model uncertainty constraint to refine the query representations by enforcing the output representations of different perturbed encoding networks to be consistent with each other. Extensive experiments on five benchmark datasets demonstrate that our method significantly outperforms strong competitors and achieves new state-of-the-art results.",
        "author": "Bowen Qin; Lihan Wang; Binyuan Hui; Bowen Li; Xiangpeng Wei; Binhua Li; Fei Huang; Luo Si; Min Yang; Yongbin Li",
        "authorids": "/b/bowen-qin/; /l/lihan-wang/; /b/binyuan-hui/; /b/bowen-li/; /x/xiangpeng-wei/; /b/binhua-li/; /f/fei-huang/; /l/luo-si/; /m/min-yang/; /y/yongbin-li/",
        "bibtex": "@inproceedings{qin-etal-2022-sun,\n    title = \"{SUN}: Exploring Intrinsic Uncertainties in Text-to-{SQL} Parsers\",\n    author = \"Qin, Bowen  and\n      Wang, Lihan  and\n      Hui, Binyuan  and\n      Li, Bowen  and\n      Wei, Xiangpeng  and\n      Li, Binhua  and\n      Huang, Fei  and\n      Si, Luo  and\n      Yang, Min  and\n      Li, Yongbin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.471/\",\n    pages = \"5298--5308\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.471.pdf",
        "site": "https://aclanthology.org/2022.coling-1.471/",
        "pdf_size": 508827,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5685434932336144623&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences+University of Chinese Academy of Sciences; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; DAMO Academy, Alibaba Group",
        "aff_domain": "siat.ac.cn;siat.ac.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;siat.ac.cn;alibaba-inc.com",
        "email": "siat.ac.cn;siat.ac.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;siat.ac.cn;alibaba-inc.com",
        "github": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;0+1;2;2;2;2;2;2;0;2",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;University of Chinese Academy of Sciences;Alibaba Group",
        "aff_unique_dep": ";;DAMO Academy",
        "aff_unique_url": "http://www.siat.cas.cn;http://www.ucas.ac.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "SIAT;UCAS;Alibaba",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.502",
        "title": "Scene Graph Modification as Incremental Structure Expanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A scene graph is a semantic representation that expresses the objects, attributes, and relationships between objects in a scene. Scene graphs play an important role in many cross modality tasks, as they are able to capture the interactions between images and texts. In this paper, we focus on scene graph modification (SGM), where the system is required to learn how to update an existing scene graph based on a natural language query. Unlike previous approaches that rebuilt the entire scene graph, we frame SGM as a graph expansion task by introducing the incremental structure expanding (ISE). ISE constructs the target graph by incrementally expanding the source graph without changing the unmodified structure. Based on ISE, we further propose a model that iterates between nodes prediction and edges prediction, inferring more accurate and harmonious expansion decisions progressively. In addition, we construct a challenging dataset that contains more complicated queries and larger scene graphs than existing datasets. Experiments on four benchmarks demonstrate the effectiveness of our approach, which surpasses the previous state-of-the-art model by large margins.",
        "author": "Xuming Hu; Zhijiang Guo; Yu Fu; Lijie Wen; Philip S. Yu",
        "authorids": "/x/xuming-hu/; /z/zhijiang-guo/; /y/yu-fu/; /l/lijie-wen/; /p/philip-s-yu/",
        "bibtex": "@inproceedings{hu-etal-2022-scene,\n    title = \"Scene Graph Modification as Incremental Structure Expanding\",\n    author = \"Hu, Xuming  and\n      Guo, Zhijiang  and\n      Fu, Yu  and\n      Wen, Lijie  and\n      Yu, Philip S.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.502/\",\n    pages = \"5707--5720\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.502.pdf",
        "site": "https://aclanthology.org/2022.coling-1.502/",
        "pdf_size": 1003964,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2011367407524620564&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Tsinghua University; University of Cambridge; Tsinghua University; Tsinghua University+University of Illinois at Chicago; Tsinghua University+University of Illinois at Chicago",
        "aff_domain": "mails.tsinghua.edu.cn;cam.ac.uk;mails.tsinghua.edu.cn;tsinghua.edu.cn;uic.edu",
        "email": "mails.tsinghua.edu.cn;cam.ac.uk;mails.tsinghua.edu.cn;tsinghua.edu.cn;uic.edu",
        "github": "https://github.com/THU-BPM/SGM",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0+2;0+2",
        "aff_unique_norm": "Tsinghua University;University of Cambridge;University of Illinois at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cam.ac.uk;https://www.uic.edu",
        "aff_unique_abbr": "THU;Cambridge;UIC",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Cambridge;Chicago",
        "aff_country_unique_index": "0;1;0;0+2;0+2",
        "aff_country_unique": "China;United Kingdom;United States"
    },
    {
        "id": "2022.coling-1.28",
        "title": "Schema Encoding for Transferable Dialogue State Tracking",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Dialogue state tracking (DST) is an essential sub-task for task-oriented dialogue systems. Recent work has focused on deep neural models for DST. However, the neural models require a large dataset for training. Furthermore, applying them to another domain needs a new dataset because the neural models are generally trained to imitate the given dataset. In this paper, we propose Schema Encoding for Transferable Dialogue State Tracking (SET-DST), which is a neural DST method for effective transfer to new domains. Transferable DST could assist developments of dialogue systems even with few dataset on target domains. We use a schema encoder not just to imitate the dataset but to comprehend the schema of the dataset. We aim to transfer the model to new domains by encoding new schemas and using them for DST on multi-domain settings. As a result, SET-DST improved the joint accuracy by 1.46 points on MultiWOZ 2.1.",
        "author": "Hyunmin Jeon; Gary Geunbae Lee",
        "authorids": "/h/hyunmin-jeon/; /g/gary-geunbae-lee/",
        "bibtex": "@inproceedings{jeon-lee-2022-schema,\n    title = \"Schema Encoding for Transferable Dialogue State Tracking\",\n    author = \"Jeon, Hyunmin  and\n      Lee, Gary Geunbae\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.28/\",\n    pages = \"355--366\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.28.pdf",
        "site": "https://aclanthology.org/2022.coling-1.28/",
        "pdf_size": 610854,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4010057214793137918&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science and Engineering, POSTECH, Pohang, South Korea; Computer Science and Engineering, Graduate School of Artificial Intelligence, POSTECH, Pohang, South Korea",
        "aff_domain": "postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pohang",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.43",
        "title": "Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In knowledge-grounded dialogue generation, pre-trained language models (PLMs) can be expected to deepen the fusing of dialogue context and knowledge because of their superior ability of semantic understanding. Unlike adopting the plain text knowledge, it is thorny to leverage the structural commonsense knowledge when using PLMs because most PLMs can only operate plain texts. Thus, linearizing commonsense knowledge facts into plan text is a compulsory trick. However, a dialogue is always aligned to a lot of retrieved fact candidates; as a result, the linearized text is always lengthy and then significantly increases the burden of using PLMs. To address this issue, we propose a novel two-stage framework SAKDP. In the first pre-screening stage, we use a ranking network PriorRanking to estimate the relevance of a retrieved knowledge fact. Thus, facts can be clustered into three sections of different priorities. As priority decreases, the relevance decreases, and the number of included facts increases. In the next dialogue generation stage, we use section-aware strategies to encode the linearized knowledge. The powerful but expensive PLM is only used for a few facts in the higher priority sections, reaching the performance-efficiency balance. Both the automatic and human evaluation demonstrate the superior performance of this work.",
        "author": "Sixing Wu; Ying Li; Ping Xue; Dawei Zhang; Zhonghai Wu",
        "authorids": "/s/sixing-wu/; /y/ying-li/; /p/ping-xue/; /d/dawei-zhang/; /z/zhonghai-wu/",
        "bibtex": "@inproceedings{wu-etal-2022-section,\n    title = \"Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model\",\n    author = \"Wu, Sixing  and\n      Li, Ying  and\n      Xue, Ping  and\n      Zhang, Dawei  and\n      Wu, Zhonghai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.43/\",\n    pages = \"521--531\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.43.pdf",
        "site": "https://aclanthology.org/2022.coling-1.43/",
        "pdf_size": 720783,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13241903535105489810&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China+National Research Center of Software Engineering, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Computer Science, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China+National Research Center of Software Engineering, Peking University, Beijing, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn; ; ; ",
        "email": "pku.edu.cn;pku.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+0;0;0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "0;0+0;0;0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.39",
        "title": "SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper introduces a novel Self-supervised Fine-grained Dialogue Evaluation framework (SelF-Eval). The core idea is to model the correlation between turn quality and the entire dialogue quality. We first propose a novel automatic data construction method that can automatically assign fine-grained scores for arbitrarily dialogue data. Then we train SelF-Eval with a multi-level contrastive learning schema which helps to distinguish different score levels. Experimental results on multiple benchmarks show that SelF-Eval is highly consistent with human evaluations and better than the state-of-the-art models. We give a detailed analysis of the experiments in this paper. Our code is available on GitHub.",
        "author": "Longxuan Ma; Ziyu Zhuang; Weinan Zhang; Mingda Li; Ting Liu",
        "authorids": "/l/longxuan-ma/; /z/ziyu-zhuang/; /w/weinan-zhang/; /m/mingda-li/; /t/ting-liu/",
        "bibtex": "@inproceedings{ma-etal-2022-self,\n    title = \"{S}el{F}-Eval: Self-supervised Fine-grained Dialogue Evaluation\",\n    author = \"Ma, Longxuan  and\n      Zhuang, Ziyu  and\n      Zhang, Weinan  and\n      Li, Mingda  and\n      Liu, Ting\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.39/\",\n    pages = \"485--495\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.39.pdf",
        "site": "https://aclanthology.org/2022.coling-1.39/",
        "pdf_size": 673256,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10243167873666677441&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2022.coling-1.510",
        "title": "Selective Token Generation for Few-shot Natural Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Natural language modeling with limited training data is a challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability. Among them, additive learning that incorporates a task-specific adapter on top of the fixed large-scale PLM has been popularly used in the few-shot setting. However, this added adapter is still easy to disregard the knowledge of the PLM especially for few-shot natural language generation (NLG) since an entire sequence is usually generated by only the newly trained adapter. Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) that selectively outputs language tokens between the task-general PLM and the task-specific adapter during both training and inference. This output token selection over the two generators allows the adapter to take into account solely the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training. In addition, to obtain the complementary adapter from the PLM for each few-shot task, we exploit a separate selecting module that is also simultaneously trained using RL. Experimental results on various few-shot NLG tasks including question answering, data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.",
        "author": "Daejin Jo; Taehwan Kwon; Eun-Sol Kim; Sungwoong Kim",
        "authorids": "/d/daejin-jo/; /t/taehwan-kwon/; /e/eun-sol-kim/; /s/sungwoong-kim/",
        "bibtex": "@inproceedings{jo-etal-2022-selective,\n    title = \"Selective Token Generation for Few-shot Natural Language Generation\",\n    author = \"Jo, Daejin  and\n      Kwon, Taehwan  and\n      Kim, Eun-Sol  and\n      Kim, Sungwoong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.510/\",\n    pages = \"5837--5856\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.510.pdf",
        "site": "https://aclanthology.org/2022.coling-1.510/",
        "pdf_size": 3771400,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18415752843018562690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Kakao Brain; Kakao Brain; Hanyang University; Kakao Brain",
        "aff_domain": "kakaobrain.com;kakaobrain.com;hanyang.ac.kr;kakaobrain.com",
        "email": "kakaobrain.com;kakaobrain.com;hanyang.ac.kr;kakaobrain.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Kakao Brain;Hanyang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://brain.kakao.com;https://www.hanyang.ac.kr",
        "aff_unique_abbr": "Kakao Brain;HYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.123",
        "title": "Self-Supervised Intermediate Fine-Tuning of Biomedical Language Models for Interpreting Patient Case Descriptions",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Interpreting patient case descriptions has emerged as a challenging problem for biomedical NLP, where the aim is typically to predict diagnoses, to recommended treatments, or to answer questions about cases more generally. Previous work has found that biomedical language models often lack the knowledge that is needed for such tasks. In this paper, we aim to improve their performance through a self-supervised intermediate fine-tuning strategy based on PubMed abstracts. Our solution builds on the observation that many of these abstracts are case reports, and thus essentially patient case descriptions. As a general strategy, we propose to fine-tune biomedical language models on the task of predicting masked medical concepts from such abstracts. We find that the success of this strategy crucially depends on the selection of the medical concepts to be masked. By ensuring that these concepts are sufficiently salient, we can substantially boost the performance of biomedical language models, achieving state-of-the-art results on two benchmarks.",
        "author": "Israa Alghanmi; Luis Espinosa-Anke; Steven Schockaert",
        "authorids": "/i/israa-alghanmi/; /l/luis-espinosa-anke/; /s/steven-schockaert/",
        "bibtex": "@inproceedings{alghanmi-etal-2022-self,\n    title = \"Self-Supervised Intermediate Fine-Tuning of Biomedical Language Models for Interpreting Patient Case Descriptions\",\n    author = \"Alghanmi, Israa  and\n      Espinosa-Anke, Luis  and\n      Schockaert, Steven\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.123/\",\n    pages = \"1432--1441\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.123.pdf",
        "site": "https://aclanthology.org/2022.coling-1.123/",
        "pdf_size": 240847,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9615112870306766092&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "CardiffNLP, Cardiff University, UK+AMPLYFI, UK; CardiffNLP, Cardiff University, UK; CardiffNLP, Cardiff University, UK",
        "aff_domain": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "email": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Cardiff University;AMPLYFI",
        "aff_unique_dep": "CardiffNLP;",
        "aff_unique_url": "https://www.cardiff.ac.uk;https://www.amplyfi.com",
        "aff_unique_abbr": "Cardiff;AMPLYFI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.80",
        "title": "SelfMix: Robust Learning against Textual Label Noise with Self-Mixup Training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The conventional success of textual classification relies on annotated data, and the new paradigm of pre-trained language models (PLMs) still requires a few labeled data for downstream tasks. However, in real-world applications, label noise inevitably exists in training data, damaging the effectiveness, robustness, and generalization of the models constructed on such data. Recently, remarkable achievements have been made to mitigate this dilemma in visual data, while only a few explore textual data. To fill this gap, we present SelfMix, a simple yet effective method, to handle label noise in text classification tasks. SelfMix uses the Gaussian Mixture Model to separate samples and leverages semi-supervised learning. Unlike previous works requiring multiple models, our method utilizes the dropout mechanism on a single model to reduce the confirmation bias in self-training and introduces a textual level mixup training strategy. Experimental results on three text classification benchmarks with different types of text show that the performance of our proposed method outperforms these strong baselines designed for both textual and visual data under different noise ratios and noise types. Our anonymous code is available at https://github.com/noise-learning/SelfMix.",
        "author": "Dan Qiao; Chenchen Dai; Yuyang Ding; Juntao Li; Qiang Chen; Wenliang Chen; Min Zhang",
        "authorids": "/d/dan-qiao/; /c/chenchen-dai/; /y/yuyang-ding/; /j/juntao-li/; /q/qiang-chen/; /w/wenliang-chen/; /m/min-zhang/",
        "bibtex": "@inproceedings{qiao-etal-2022-selfmix,\n    title = \"{S}elf{M}ix: Robust Learning against Textual Label Noise with Self-Mixup Training\",\n    author = \"Qiao, Dan  and\n      Dai, Chenchen  and\n      Ding, Yuyang  and\n      Li, Juntao  and\n      Chen, Qiang  and\n      Chen, Wenliang  and\n      Zhang, Min\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.80/\",\n    pages = \"960--970\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.80.pdf",
        "site": "https://aclanthology.org/2022.coling-1.80/",
        "pdf_size": 1414439,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1526414651955675047&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China; Alibaba Group; Institute of Computer Science and Technology, Soochow University, China; Institute of Computer Science and Technology, Soochow University, China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;suda.edu.cn;suda.edu.cn;suda.edu.cn;alibaba-inc.com",
        "email": "gmail.com;gmail.com;gmail.com;suda.edu.cn;suda.edu.cn;suda.edu.cn;alibaba-inc.com",
        "github": "https://github.com/noise-learning/SelfMix",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;0;0",
        "aff_unique_norm": "Soochow University;Alibaba Group",
        "aff_unique_dep": "Institute of Computer Science and Technology;",
        "aff_unique_url": "https://eng.suda.edu.cn/;https://www.alibaba.com",
        "aff_unique_abbr": ";Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.541",
        "title": "Semantic Overlap Summarization among Multiple Alternative Narratives: An Exploratory Study",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we introduce an important yet relatively unexplored NLP task called Semantic Overlap Summarization (SOS), which entails generating a single summary from multiple alternative narratives which can convey the common information provided by those narratives. As no benchmark dataset is readily available for this task, we created one by collecting 2,925 alternative narrative pairs from the web and then, went through the tedious process of manually creating 411 different reference summaries by engaging human annotators. As a way to evaluate this novel task, we first conducted a systematic study by borrowing the popular ROUGE metric from text-summarization literature and discovered that ROUGE is not suitable for our task. Subsequently, we conducted further human annotations to create 200 document-level and 1,518 sentence-level ground-truth overlap labels. Our experiments show that the sentence-wise annotation technique with three overlap labels, i.e., Absent (A), Partially-Present (PP), and Present (P), yields a higher correlation with human judgment and higher inter-rater agreement compared to the ROUGE metric.",
        "author": "Naman Bansal; Mousumi Akter; Shubhra Kanti Karmaker Santu",
        "authorids": "/n/naman-bansal/; /m/mousumi-akter/; /s/shubhra-kanti-karmaker-santu/",
        "bibtex": "@inproceedings{bansal-etal-2022-semantic,\n    title = \"Semantic Overlap Summarization among Multiple Alternative Narratives: An Exploratory Study\",\n    author = \"Bansal, Naman  and\n      Akter, Mousumi  and\n      Karmaker Santu, Shubhra Kanti\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.541/\",\n    pages = \"6195--6207\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.541.pdf",
        "site": "https://aclanthology.org/2022.coling-1.541/",
        "pdf_size": 636754,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1195601687092633399&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Big Data Intelligence (BDI) Lab, Department of Computer Science and Software Engineering, College of Engineering, Auburn University; Big Data Intelligence (BDI) Lab, Department of Computer Science and Software Engineering, College of Engineering, Auburn University; Big Data Intelligence (BDI) Lab, Department of Computer Science and Software Engineering, College of Engineering, Auburn University",
        "aff_domain": "auburn.edu;auburn.edu;auburn.edu",
        "email": "auburn.edu;auburn.edu;auburn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Auburn University",
        "aff_unique_dep": "Department of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.auburn.edu",
        "aff_unique_abbr": "Auburn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.370",
        "title": "Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures inside Arguments",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantic role labeling (SRL) is a fundamental yet challenging task in the NLP community. Recent works of SRL mainly fall into two lines: 1) BIO-based; 2) span-based. Despite ubiquity, they share some intrinsic drawbacks of not considering internal argument structures, potentially hindering the model\u2019s expressiveness. The key challenge is arguments are flat structures, and there are no determined subtree realizations for words inside arguments. To remedy this, in this paper, we propose to regard flat argument spans as latent subtrees, accordingly reducing SRL to a tree parsing task. In particular, we equip our formulation with a novel span-constrained TreeCRF to make tree structures span-aware and further extend it to the second-order case. We conduct extensive experiments on CoNLL05 and CoNLL12 benchmarks. Results reveal that our methods perform favorably better than all previous syntax-agnostic works, achieving new state-of-the-art under both end-to-end and w/ gold predicates settings.",
        "author": "Yu Zhang; Qingrong Xia; Shilin Zhou; Yong Jiang; Guohong Fu; Min Zhang",
        "authorids": "/y/yu-zhang/; /q/qingrong-xia/; /s/shilin-zhou/; /y/yong-jiang/; /g/guohong-fu/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2022-semantic,\n    title = \"Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures inside Arguments\",\n    author = \"Zhang, Yu  and\n      Xia, Qingrong  and\n      Zhou, Shilin  and\n      Jiang, Yong  and\n      Fu, Guohong  and\n      Zhang, Min\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.370/\",\n    pages = \"4212--4227\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.370.pdf",
        "site": "https://aclanthology.org/2022.coling-1.370/",
        "pdf_size": 652956,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15628229784829141366&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China + Huawei Cloud, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China + Huawei Cloud, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China + Huawei Cloud, China; DAMO Academy, Alibaba Group, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China + Huawei Cloud, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China",
        "aff_domain": "outlook.com;outlook.com;huawei.com;alibaba-inc.com;suda.edu.cn;suda.edu.cn",
        "email": "outlook.com;outlook.com;huawei.com;alibaba-inc.com;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;2;0+1;0",
        "aff_unique_norm": "Soochow University;Huawei Cloud;Alibaba Group",
        "aff_unique_dep": "School of Computer Science and Technology;;DAMO Academy",
        "aff_unique_url": "http://www.soochow.edu.cn;https://www.huaweicloud.com;https://www.alibaba.com",
        "aff_unique_abbr": "Soochow U;Huawei Cloud;Alibaba",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Suzhou;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.78",
        "title": "Semantic Sentence Matching via Interacting Syntax Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Studies have shown that the sentence\u2019s syntactic structures are important for semantic sentence matching. A typical approach is encoding each sentence\u2019s syntactic structure into an embedding vector, which can be combined with other features to predict the final matching scores. Though successes have been observed, embedding the whole syntactic structures as one vector inevitably overlooks the fine-grained syntax matching patterns, e.g. the alignment of specific term dependencies relations in the two inputted sentences. In this paper, we formalize the task of semantic sentence matching as a problem of graph matching in which each sentence is represented as a directed graph according to its syntactic structures. The syntax matching patterns (i.e. similar syntactic structures) between two sentences, therefore, can be extracted as the sub-graph structure alignments. The proposed method, referred to as Interacted Syntax Graphs (ISG), represents two sentences\u2019 syntactic alignments as well as their semantic matching signals into one association graph. After that, the neural quadratic assignment programming (QAP) is adapted to extract syntactic matching patterns from the association graph. In this way, the syntactic structures fully interact in a fine granularity during the matching process. Experimental results on three public datasets demonstrated that ISG can outperform the state-of-the-art baselines effectively and efficiently. The empirical analysis also showed that ISG can match sentences in an interpretable way.",
        "author": "Chen Xu; Jun Xu; Zhenhua Dong; Ji-Rong Wen",
        "authorids": "/c/chen-xu/; /j/jun-xu/; /z/zhenhua-dong/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{xu-etal-2022-semantic,\n    title = \"Semantic Sentence Matching via Interacting Syntax Graphs\",\n    author = \"Xu, Chen  and\n      Xu, Jun  and\n      Dong, Zhenhua  and\n      Wen, Ji-Rong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.78/\",\n    pages = \"938--949\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.78.pdf",
        "site": "https://aclanthology.org/2022.coling-1.78/",
        "pdf_size": 2946680,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10288351433307853331&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods; Huawei Noah\u2019s Ark Lab; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn;huawei.com;ruc.edu.cn",
        "email": "ruc.edu.cn;ruc.edu.cn;huawei.com;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;2;0+1",
        "aff_unique_norm": "Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods;Huawei",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Big Data Management and Analysis;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.ruc.edu.cn;;https://www.huawei.com",
        "aff_unique_abbr": "RUC;;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.135",
        "title": "Semantic Structure Based Query Graph Prediction for Question Answering over Knowledge Graph",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Building query graphs from natural language questions is an important step in complex question answering over knowledge graph (Complex KGQA). In general, a question can be correctly answered if its query graph is built correctly and the right answer is then retrieved by issuing the query graph against the KG. Therefore, this paper focuses on query graph generation from natural language questions. Existing approaches for query graph generation ignore the semantic structure of a question, resulting in a large number of noisy query graph candidates that undermine prediction accuracies. In this paper, we define six semantic structures from common questions in KGQA and develop a novel Structure-BERT to predict the semantic structure of a question. By doing so, we can first filter out noisy candidate query graphs by the predicted semantic structures, and then rank the remaining candidates with a BERT-based ranking model. Extensive experiments on two popular benchmarks MetaQA and WebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to state-of-the-arts.",
        "author": "Mingchen Li; Shihao Ji",
        "authorids": "/m/mingchen-li/; /s/shihao-ji/",
        "bibtex": "@inproceedings{li-ji-2022-semantic,\n    title = \"Semantic Structure Based Query Graph Prediction for Question Answering over Knowledge Graph\",\n    author = \"Li, Mingchen  and\n      Ji, Shihao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.135/\",\n    pages = \"1569--1579\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.135.pdf",
        "site": "https://aclanthology.org/2022.coling-1.135/",
        "pdf_size": 1380417,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14238513146184865647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Georgia State University, Atlanta, USA; Department of Computer Science, Georgia State University, Atlanta, USA",
        "aff_domain": "student.gsu.edu;gsu.edu",
        "email": "student.gsu.edu;gsu.edu",
        "github": "https://github.com/ToneLi/SSKGQA",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.gsu.edu",
        "aff_unique_abbr": "GSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.267",
        "title": "Semantic-Preserving Adversarial Code Comprehension",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Based on the tremendous success of pre-trained language models (PrLMs) for source code comprehension tasks, current literature studies either ways to further improve the performance (generalization) of PrLMs, or their robustness against adversarial attacks. However, they have to compromise on the trade-off between the two aspects and none of them consider improving both sides in an effective and practical way. To fill this gap, we propose Semantic-Preserving Adversarial Code Embeddings (SPACE) to find the worst-case semantic-preserving attacks while forcing the model to predict the correct labels under these worst cases. Experiments and analysis demonstrate that SPACE can stay robust against state-of-the-art attacks while boosting the performance of PrLMs for code.",
        "author": "Yiyang Li; Hongqiu Wu; Hai Zhao",
        "authorids": "/y/yiyang-li/; /h/hongqiu-wu/; /h/hai-zhao/",
        "bibtex": "@inproceedings{li-etal-2022-semantic,\n    title = \"Semantic-Preserving Adversarial Code Comprehension\",\n    author = \"Li, Yiyang  and\n      Wu, Hongqiu  and\n      Zhao, Hai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.267/\",\n    pages = \"3017--3028\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.267.pdf",
        "site": "https://aclanthology.org/2022.coling-1.267/",
        "pdf_size": 606376,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8624736006113445678&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/EricLee8/SPACE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.49",
        "title": "Semantic-based Pre-training for Dialogue Understanding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Pre-trained language models have made great progress on dialogue tasks. However, these models are typically trained on surface dialogue text, thus are proven to be weak in understanding the main semantic meaning of a dialogue context. We investigate Abstract Meaning Representation (AMR) as explicit semantic knowledge for pre-training models to capture the core semantic information in dialogues during pre-training. In particular, we propose a semantic-based pre-training framework that extends the standard pre-training framework (Devlin et al.,2019) by three tasks for learning 1) core semantic units, 2) semantic relations and 3) the overall semantic representation according to AMR graphs. Experiments on the understanding of both chit-chats and task-oriented dialogues show the superiority of our model. To our knowledge, we are the first to leverage a deep semantic representation for dialogue pre-training.",
        "author": "Xuefeng Bai; Linfeng Song; Yue Zhang",
        "authorids": "/x/xuefeng-bai/; /l/linfeng-song/; /y/yue-zhang/",
        "bibtex": "@inproceedings{bai-etal-2022-semantic,\n    title = \"Semantic-based Pre-training for Dialogue Understanding\",\n    author = \"Bai, Xuefeng  and\n      Song, Linfeng  and\n      Zhang, Yue\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.49/\",\n    pages = \"592--607\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.49.pdf",
        "site": "https://aclanthology.org/2022.coling-1.49/",
        "pdf_size": 591642,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4439098045174348792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "\u2660School of Engineering, Westlake University, China + \u2662Institute of Advanced Technology, Westlake Institute for Advanced Study, China; \u2663Tencent AI Lab, Bellevue, WA, USA; \u2660School of Engineering, Westlake University, China + \u2662Institute of Advanced Technology, Westlake Institute for Advanced Study, China",
        "aff_domain": "westlake.edu.cn;tencent.com;westlake.edu.cn",
        "email": "westlake.edu.cn;tencent.com;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Westlake University;Westlake Institute for Advanced Study;Tencent AI Lab",
        "aff_unique_dep": "School of Engineering;Institute of Advanced Technology;AI Lab",
        "aff_unique_url": "https://www.westlake.edu.cn;http://www.wias.org.cn/;https://ai.tencent.com",
        "aff_unique_abbr": ";WIAS;Tencent AI Lab",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Bellevue",
        "aff_country_unique_index": "0+0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.457",
        "title": "Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper introduces a new data augmentation method for neural machine translation that can enforce stronger semantic consistency both within and across languages. Our method is based on Conditional Masked Language Model (CMLM) which is bi-directional and can be conditional on both left and right context, as well as the label. We demonstrate that CMLM is a good technique for generating context-dependent word distributions. In particular, we show that CMLM is capable of enforcing semantic consistency by conditioning on both source and target during substitution. In addition, to enhance diversity, we incorporate the idea of soft word substitution for data augmentation which replaces a word with a probabilistic distribution over the vocabulary. Experiments on four translation datasets of different scales show that the overall solution results in more realistic data augmentation and better translation quality. Our approach consistently achieves the best performance in comparison with strong and recent works and yields improvements of up to 1.90 BLEU points over the baseline.",
        "author": "Qiao Cheng; Jin Huang; Yitao Duan",
        "authorids": "/q/qiao-cheng/; /j/jin-huang/; /y/yitao-duan/",
        "bibtex": "@inproceedings{cheng-etal-2022-semantically,\n    title = \"Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model\",\n    author = \"Cheng, Qiao  and\n      Huang, Jin  and\n      Duan, Yitao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.457/\",\n    pages = \"5148--5157\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.457.pdf",
        "site": "https://aclanthology.org/2022.coling-1.457/",
        "pdf_size": 349142,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11589100921388527178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "NetEase Youdao; NetEase Youdao; NetEase Youdao",
        "aff_domain": "rd.netease.com;rd.netease.com;rd.netease.com",
        "email": "rd.netease.com;rd.netease.com;rd.netease.com",
        "github": "https://github.com/netease-youdao/cmlm_da",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NetEase",
        "aff_unique_dep": "Youdao",
        "aff_unique_url": "https://www.youdao.com",
        "aff_unique_abbr": "Youdao",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.428",
        "title": "Sentence-aware Adversarial Meta-Learning for Few-Shot Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Meta-learning has emerged as an effective approach for few-shot text classification. However, current studies fail to realize the importance of the semantic interaction between sentence features and neglect to enhance the generalization ability of the model to new tasks. In this paper, we integrate an adversarial network architecture into the meta-learning system and leverage cost-effective modules to build a novel few-shot classification framework named SaAML. Significantly, our approach can exploit the temporal convolutional network to encourage more discriminative representation learning and explore the attention mechanism to promote more comprehensive feature expression, thus resulting in better adaptation for new classes. Through a series of experiments on four benchmark datasets, we demonstrate that our new framework acquires considerable superiority over state-of-the-art methods in all datasets, increasing the performance of 1-shot classification and 5-shot classification by 7.15% and 2.89%, respectively.",
        "author": "Suhe Wang; Xiaoyuan Liu; Bo Liu; Diwen Dong",
        "authorids": "/s/suhe-wang/; /x/xiaoyuan-liu/; /b/bo-liu/; /d/diwen-dong/",
        "bibtex": "@inproceedings{wang-etal-2022-sentence-aware,\n    title = \"Sentence-aware Adversarial Meta-Learning for Few-Shot Text Classification\",\n    author = \"Wang, Suhe  and\n      Liu, Xiaoyuan  and\n      Liu, Bo  and\n      Dong, Diwen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.428/\",\n    pages = \"4844--4852\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.428.pdf",
        "site": "https://aclanthology.org/2022.coling-1.428/",
        "pdf_size": 469001,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11025124501092009093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "College of Computer",
        "aff_unique_url": "http://www.nudt.edu.cn",
        "aff_unique_abbr": "NUDT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Changsha",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.582",
        "title": "Sentiment Interpretable Logic Tensor Network for Aspect-Term Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Aspect-term sentiment analysis (ATSA) is an important task that aims to infer the sentiment towards the given aspect-terms. It is often required in the industry that ATSA should be performed with interpretability, computational efficiency and high accuracy. However, such an ATSA method has not yet been developed. This study aims to develop an ATSA method that fulfills all these requirements. To achieve the goal, we propose a novel Sentiment Interpretable Logic Tensor Network (SILTN). SILTN is interpretable because it is a neurosymbolic formalism and a computational model that supports learning and reasoning about data with a differentiable first-order logic language (FOL). To realize SILTN with high inferring accuracy, we propose a novel learning strategy called the two-stage syntax knowledge distillation (TSynKD). Using widely used datasets, we experimentally demonstrate that the proposed TSynKD is effective for improving the accuracy of SILTN, and the SILTN has both high interpretability and computational efficiency.",
        "author": "Bowen Zhang; Xu Huang; Zhichao Huang; Hu Huang; Baoquan Zhang; Xianghua Fu; Liwen Jing",
        "authorids": "/b/bowen-zhang/; /x/xu-huang/; /z/zhichao-huang/; /h/hu-huang/; /b/baoquan-zhang/; /x/xianghua-fu/; /l/liwen-jing/",
        "bibtex": "@inproceedings{zhang-etal-2022-sentiment,\n    title = \"Sentiment Interpretable Logic Tensor Network for Aspect-Term Sentiment Analysis\",\n    author = \"Zhang, Bowen  and\n      Huang, Xu  and\n      Huang, Zhichao  and\n      Huang, Hu  and\n      Zhang, Baoquan  and\n      Fu, Xianghua  and\n      Jing, Liwen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.582/\",\n    pages = \"6705--6714\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.582.pdf",
        "site": "https://aclanthology.org/2022.coling-1.582/",
        "pdf_size": 720439,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5342293331287777944&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; JD Intelligent Cities Research, Beijing, China; School of Cyber Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China+Shenzhen innovation Institute (Xinstitute), Shenzhen, China; College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China+Shenzhen innovation Institute (Xinstitute), Shenzhen, China",
        "aff_domain": "sztu.edu.cn;hit.edu.cn;jd.com;ustc.edu.cn;hit.edu.cn;sztu.edu.cn;connect.ust.hk",
        "email": "sztu.edu.cn;hit.edu.cn;jd.com;ustc.edu.cn;hit.edu.cn;sztu.edu.cn;connect.ust.hk",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;1;0+4;0+4",
        "aff_unique_norm": "Shenzhen Technology University;Harbin Institute of Technology;JD Intelligent Cities Research;University of Science and Technology of China;Shenzhen innovation Institute",
        "aff_unique_dep": "College of Big Data and Internet;School of Computer Science and Technology;;School of Cyber Science and Technology;",
        "aff_unique_url": "https://www.sztu.edu.cn;http://www.hit.edu.cn/;;http://www.ustc.edu.cn;",
        "aff_unique_abbr": ";HIT;;USTC;Xinstitute",
        "aff_campus_unique_index": "0;0;1;2;0;0;0",
        "aff_campus_unique": "Shenzhen;Beijing;Hefei;",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.184",
        "title": "Simple Yet Powerful: An Overlooked Architecture for Nested Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Named Entity Recognition (NER) is an important task in Natural Language Processing that aims to identify text spans belonging to predefined categories. Traditional NER systems ignore nested entities, which are entities contained in other entity mentions. Although several methods have been proposed to address this case, most of them rely on complex task-specific structures and ignore potentially useful baselines for the task. We argue that this creates an overly optimistic impression of their performance. This paper revisits the Multiple LSTM-CRF (MLC) model, a simple, overlooked, yet powerful approach based on training independent sequence labeling models for each entity type. Extensive experiments with three nested NER corpora show that, regardless of the simplicity of this model, its performance is better or at least as well as more sophisticated methods. Furthermore, we show that the MLC architecture achieves state-of-the-art results in the Chilean Waiting List corpus by including pre-trained language models. In addition, we implemented an open-source library that computes task-specific metrics for nested NER. The results suggest that metrics used in previous work do not measure well the ability of a model to detect nested entities, while our metrics provide new evidence on how existing approaches handle the task.",
        "author": "Matias Rojas; Felipe Bravo-Marquez; Jocelyn Dunstan",
        "authorids": "/m/matias-rojas/; /f/felipe-bravo-marquez/; /j/jocelyn-dunstan/",
        "bibtex": "@inproceedings{rojas-etal-2022-simple,\n    title = \"Simple Yet Powerful: An Overlooked Architecture for Nested Named Entity Recognition\",\n    author = \"Rojas, Matias  and\n      Bravo-Marquez, Felipe  and\n      Dunstan, Jocelyn\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.184/\",\n    pages = \"2108--2117\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.184.pdf",
        "site": "https://aclanthology.org/2022.coling-1.184/",
        "pdf_size": 429099,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1790015886099385979&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.484",
        "title": "Simple and Effective Graph-to-Graph Annotation Conversion",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Annotation conversion is an effective way to construct datasets under new annotation guidelines based on existing datasets with little human labour. Previous work has been limited in conversion between tree-structured datasets and mainly focused on feature-based models which are not easily applicable to new conversions. In this paper, we propose two simple and effective graph-to-graph annotation conversion approaches, namely Label Switching and Graph2Graph Linear Transformation, which use pseudo data and inherit parameters to guide graph conversions respectively. These methods are able to deal with conversion between graph-structured annotations and require no manually designed features. To verify their effectiveness, we manually construct a graph-structured parallel annotated dataset and evaluate the proposed approaches on it as well as other existing parallel annotated datasets. Experimental results show that the proposed approaches outperform strong baselines with higher conversion score. To further validate the quality of converted graphs, we utilize them to train the target parser and find graphs generated by our approaches lead to higher parsing score than those generated by the baselines.",
        "author": "Yuxuan Wang; Zhilin Lei; Yuqiu Ji; Wanxiang Che",
        "authorids": "/y/yuxuan-wang/; /z/zhilin-lei/; /y/yuqiu-ji/; /w/wanxiang-che/",
        "bibtex": "@inproceedings{wang-etal-2022-simple,\n    title = \"Simple and Effective Graph-to-Graph Annotation Conversion\",\n    author = \"Wang, Yuxuan  and\n      Lei, Zhilin  and\n      Ji, Yuqiu  and\n      Che, Wanxiang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.484/\",\n    pages = \"5450--5460\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.484.pdf",
        "site": "https://aclanthology.org/2022.coling-1.484/",
        "pdf_size": 557387,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10277562120204386873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Zhejiang Lab\u2666; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology\u2661; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology\u2661; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology\u2661\u2020",
        "aff_domain": "zhejianglab.com;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "zhejianglab.com;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "https://github.com/WangYuxuan93/G2GConversion",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Zhejiang Lab;Harbin Institute of Technology",
        "aff_unique_dep": ";Research Center for Social Computing and Information Retrieval",
        "aff_unique_url": "http://www.zhejianglab.com;http://www.hit.edu.cn/",
        "aff_unique_abbr": ";HIT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Harbin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.345",
        "title": "Singlish Message Paraphrasing: A Joint Task of Creole Translation and Text Normalization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Within the natural language processing community, English is by far the most resource-rich language. There is emerging interest in conducting translation via computational approaches to conform its dialects or creole languages back to standard English. This computational approach paves the way to leverage generic English language backbones, which are beneficial for various downstream tasks. However, in practical online communication scenarios, the use of language varieties is often accompanied by noisy user-generated content, making this translation task more challenging. In this work, we introduce a joint paraphrasing task of creole translation and text normalization of Singlish messages, which can shed light on how to process other language varieties and dialects. We formulate the task in three different linguistic dimensions: lexical level normalization, syntactic level editing, and semantic level rewriting. We build an annotated dataset of Singlish-to-Standard English messages, and report performance on a perturbation-resilient sequence-to-sequence model. Experimental results show that the model produces reasonable generation results, and can improve the performance of downstream tasks like stance detection.",
        "author": "Zhengyuan Liu; Shikang Ni; Ai Ti Aw; Nancy F. Chen",
        "authorids": "/z/zhengyuan-liu/; /s/shikang-ni/; /a/aiti-aw/; /n/nancy-chen/",
        "bibtex": "@inproceedings{liu-etal-2022-singlish,\n    title = \"{S}inglish Message Paraphrasing: A Joint Task of Creole Translation and Text Normalization\",\n    author = \"Liu, Zhengyuan  and\n      Ni, Shikang  and\n      Aw, Ai Ti  and\n      Chen, Nancy F.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.345/\",\n    pages = \"3924--3936\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.345.pdf",
        "site": "https://aclanthology.org/2022.coling-1.345/",
        "pdf_size": 1900190,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7596310750012863322&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute for Infocomm Research, A*STAR, Singapore; University of Cambridge, UK + Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore",
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg; ;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg; ;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Institute for Infocomm Research;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.cam.ac.uk",
        "aff_unique_abbr": "I2R;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1+0;0;0",
        "aff_country_unique": "Singapore;United Kingdom"
    },
    {
        "id": "2022.coling-1.6",
        "title": "Smells like Teen Spirit: An Exploration of Sensorial Style in Literary Genres",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "It is well recognized that sensory perceptions and language have interconnections through numerous studies in psychology, neuroscience, and sensorial linguistics. Set in this rich context we ask whether the use of sensorial language in writings is part of linguistic style? This question is important from the view of stylometrics research where a rich set of language features have been explored, but with insufficient attention given to features related to sensorial language. Taking this as the goal we explore several angles about sensorial language and style in collections of lyrics, novels, and poetry. We find, for example, that individual use of sensorial language is not a random phenomenon; choice is likely involved. Also, sensorial style is generally stable over time - the shifts are extremely small. Moreover, style can be extracted from just a few hundred sentences that have sensorial terms. We also identify representative and distinctive features within each genre. For example, we observe that 4 of the top 6 representative features in novels collection involved individuals using olfactory language where we expected them to use non-olfactory language.",
        "author": "Osama Khalid; Padmini Srinivasan",
        "authorids": "/o/osama-khalid/; /p/padmini-srinivasan/",
        "bibtex": "@inproceedings{khalid-srinivasan-2022-smells,\n    title = \"Smells like Teen Spirit: An Exploration of Sensorial Style in Literary Genres\",\n    author = \"Khalid, Osama  and\n      Srinivasan, Padmini\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.6/\",\n    pages = \"55--64\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.6.pdf",
        "site": "https://aclanthology.org/2022.coling-1.6/",
        "pdf_size": 1399107,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8164243813852513241&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Iowa; University of Iowa",
        "aff_domain": "uiowa.edu;uiowa.edu",
        "email": "uiowa.edu;uiowa.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Iowa",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uiowa.edu",
        "aff_unique_abbr": "UIowa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.434",
        "title": "Smoothed Contrastive Learning for Unsupervised Sentence Embedding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Unsupervised contrastive sentence embedding models, e.g., unsupervised SimCSE, use the InfoNCE loss function in training. Theoretically, we expect to use larger batches to get more adequate comparisons among samples and avoid overfitting. However, increasing batch size leads to performance degradation when it exceeds a threshold, which is probably due to the introduction of false-negative pairs through statistical observation. To alleviate this problem, we introduce a simple smoothing strategy upon the InfoNCE loss function, termed Gaussian Smoothed InfoNCE (GS-InfoNCE). In other words, we add random Gaussian noise as an extension to the negative pairs without increasing the batch size. Through experiments on the semantic text similarity tasks, though simple, the proposed smoothing strategy brings improvements to unsupervised SimCSE.",
        "author": "Xing Wu; Chaochen Gao; Yipeng Su; Jizhong Han; Zhongyuan Wang; Songlin Hu",
        "authorids": "/x/xing-wu/; /c/chaochen-gao/; /y/yipeng-su/; /j/jizhong-han/; /z/zhongyuan-wang/; /s/songlin-hu/",
        "bibtex": "@inproceedings{wu-etal-2022-smoothed,\n    title = \"Smoothed Contrastive Learning for Unsupervised Sentence Embedding\",\n    author = \"Wu, Xing  and\n      Gao, Chaochen  and\n      Su, Yipeng  and\n      Han, Jizhong  and\n      Wang, Zhongyuan  and\n      Hu, Songlin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.434/\",\n    pages = \"4902--4906\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.434.pdf",
        "site": "https://aclanthology.org/2022.coling-1.434/",
        "pdf_size": 400662,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3912038776161220225&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China+Kuaishou Technology, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Kuaishou Technology, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;kuaishou.com;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;kuaishou.com;iie.ac.cn",
        "github": "https://github.com/caskcsg/gsInfoNCE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1+2;0+1;0;0+1;2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Kuaishou Technology",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.kuaishou.com",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0+0;0+0;0;0+0;0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0+0;0+0;0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.580",
        "title": "Social Bot-Aware Graph Neural Network for Early Rumor Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Early rumor detection is a key challenging task to prevent rumors from spreading widely. Sociological research shows that social bots\u2019 behavior in the early stage has become the main reason for rumors\u2019 wide spread. However, current models do not explicitly distinguish genuine users from social bots, and their failure in identifying rumors timely. Therefore, this paper aims at early rumor detection by accounting for social bots\u2019 behavior, and presents a Social Bot-Aware Graph Neural Network, named SBAG. SBAG firstly pre-trains a multi-layer perception network to capture social bot features, and then constructs multiple graph neural networks by embedding the features to model the early propagation of posts, which is further used to detect rumors. Extensive experiments on three benchmark datasets show that SBAG achieves significant improvements against the baselines and also identifies rumors within 3 hours while maintaining more than 90% accuracy.",
        "author": "Zhen Huang; Zhilong Lv; Xiaoyun Han; Binyang Li; Menglong Lu; Dongsheng Li",
        "authorids": "/z/zhen-huang/; /z/zhilong-lv/; /x/xiaoyun-han/; /b/binyang-li/; /m/menglong-lu/; /d/dongsheng-li/",
        "bibtex": "@inproceedings{huang-etal-2022-social,\n    title = \"Social Bot-Aware Graph Neural Network for Early Rumor Detection\",\n    author = \"Huang, Zhen  and\n      Lv, Zhilong  and\n      Han, Xiaoyun  and\n      Li, Binyang  and\n      Lu, Menglong  and\n      Li, Dongsheng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.580/\",\n    pages = \"6680--6690\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.580.pdf",
        "site": "https://aclanthology.org/2022.coling-1.580/",
        "pdf_size": 2820223,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11924827218323581913&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; University of International Relations; National University of Defense Technology; National University of Defense Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;uir.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;nudt.edu.cn;uir.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "https://github.com/sky-star-moon/SBAG",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "National University of Defense Technology;University of International Relations",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nudt.edu.cn/;http://uir.edu.cn",
        "aff_unique_abbr": "NUDT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.114",
        "title": "Social Norms-Grounded Machine Ethics in Complex Narrative Situation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Ethical judgment aims to determine if a person in a narrative situation acts under people\u2019s social norms under a culture, so it is crucial to understand actions in narratives and achieve machine ethics. Recent works depend on data-driven methods to directly judge the ethics of complex real-world narratives but face two major challenges. First, they cannot well handle dilemma situations due to a lack of basic knowledge about social norms. Second, they focus merely on sparse situation-level judgment regardless of the social norms involved during the judgment, leading to a black box. In this work, inspired by previous knowledge-grounded and -augmented paradigms, we propose to complement a complex situation with grounded social norms. Besides a norm-grounding knowledge model, we present a novel norm-supported ethical judgment model in line with neural module networks to alleviate dilemma situations and improve norm-level explainability. Empirically, our model improves state-of-the-art performance on two narrative judgment benchmarks.",
        "author": "Tao Shen; Xiubo Geng; Daxin Jiang",
        "authorids": "/t/tao-shen/; /x/xiubo-geng/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{shen-etal-2022-social,\n    title = \"Social Norms-Grounded Machine Ethics in Complex Narrative Situation\",\n    author = \"Shen, Tao  and\n      Geng, Xiubo  and\n      Jiang, Daxin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.114/\",\n    pages = \"1333--1343\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.114.pdf",
        "site": "https://aclanthology.org/2022.coling-1.114/",
        "pdf_size": 1418626,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2519702850750901387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Microsoft Corporation; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.526",
        "title": "Source-summary Entity Aggregation in Abstractive Summarization",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In a text, entities mentioned earlier can be referred to in later discourse by a more general description. For example, Celine Dion and Justin Bieber can be referred to by Canadian singers or celebrities. In this work, we study this phenomenon in the context of summarization, where entities from a source text are generalized in the summary. We call such instances source-summary entity aggregations. We categorize these aggregations into two types and analyze them in the Cnn/Dailymail corpus, showing that they are reasonably frequent. We then examine how well three state-of-the-art summarization systems can generate such aggregations within summaries. We also develop techniques to encourage them to generate more aggregations. Our results show that there is significant room for improvement in producing semantically correct aggregations.",
        "author": "Jos\u00e9 \u00c1ngel Gonz\u00e1lez; Annie Louis; Jackie Chi Kit Cheung",
        "authorids": "/j/jose-angel-gonzalez/; /a/annie-louis/; /j/jackie-chi-kit-cheung/",
        "bibtex": "@inproceedings{gonzalez-etal-2022-source,\n    title = \"Source-summary Entity Aggregation in Abstractive Summarization\",\n    author = \"Gonz{\\'a}lez, Jos{\\'e} {\\'A}ngel  and\n      Louis, Annie  and\n      Cheung, Jackie Chi Kit\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.526/\",\n    pages = \"6019--6034\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.526.pdf",
        "site": "https://aclanthology.org/2022.coling-1.526/",
        "pdf_size": 352822,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18109340225993532535&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "Valencian Research Institute for Artificial Intelligence, Valencia; Google Research, London; McGill University/MILA, Montreal Canada CIFAR AI Chair",
        "aff_domain": "dsic.upv.es;google.com;cs.mcgill.ca",
        "email": "dsic.upv.es;google.com;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Valencian Research Institute for Artificial Intelligence;Google;McGill University",
        "aff_unique_dep": ";Google Research;MILA",
        "aff_unique_url": "https://www.vrai.es;https://research.google;https://www.mcgill.ca",
        "aff_unique_abbr": "VRAI;Google;McGill",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Valencia;London;Montreal",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Spain;United Kingdom;Canada"
    },
    {
        "id": "2022.coling-1.61",
        "title": "Speaker Clustering in Textual Dialogue with Pairwise Utterance Relation and Cross-corpus Dialogue Act Supervision",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a speaker clustering model for textual dialogues, which groups the utterances of a multi-party dialogue without speaker annotations, so that the actual speakers are identical inside each cluster. We find that, without knowing the speakers, the interactions between utterances are still implied in the text, which suggest the relations between speakers. In this work, we model the semantic content of utterance with a pre-trained language model, and the relations between speakers with an utterance-level pairwise matrix. The semantic content representation can be further instructed by cross-corpus dialogue act modeling. The speaker labels are finally generated by spectral clustering. Experiments show that our model outperforms the sequence classification baseline, and benefits from the auxiliary dialogue act classification task. We also discuss the detail of determining the number of speakers (clusters), eliminating the interference caused by semantic similarity, and the impact of utterance distance.",
        "author": "Zhihua Su; Qiang Zhou",
        "authorids": "/z/zhihua-su/; /q/qiang-zhou/",
        "bibtex": "@inproceedings{su-zhou-2022-speaker,\n    title = \"Speaker Clustering in Textual Dialogue with Pairwise Utterance Relation and Cross-corpus Dialogue Act Supervision\",\n    author = \"Su, Zhihua  and\n      Zhou, Qiang\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.61/\",\n    pages = \"734--744\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.61.pdf",
        "site": "https://aclanthology.org/2022.coling-1.61/",
        "pdf_size": 537115,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5057677382808751051&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University + Beijing National Research Center for Information Science and Technology, Tsinghua University; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University + Beijing National Research Center for Information Science and Technology, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Computer Science and Technology",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Tsinghua",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.477",
        "title": "Speaker-Aware Discourse Parsing on Multi-Party Dialogues",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Discourse parsing on multi-party dialogues is an important but difficult task in dialogue systems and conversational analysis. It is believed that speaker interactions are helpful for this task. However, most previous research ignores speaker interactions between different speakers. To this end, we present a speaker-aware model for this task. Concretely, we propose a speaker-context interaction joint encoding (SCIJE) approach, using the interaction features between different speakers. In addition, we propose a second-stage pre-training task, same speaker prediction (SSP), enhancing the conversational context representations by predicting whether two utterances are from the same speaker. Experiments on two standard benchmark datasets show that the proposed model achieves the best-reported performance in the literature. We will release the codes of this paper to facilitate future research.",
        "author": "Nan Yu; Guohong Fu; Min Zhang",
        "authorids": "/n/nan-yu/; /g/guohong-fu/; /m/min-zhang/",
        "bibtex": "@inproceedings{yu-etal-2022-speaker,\n    title = \"Speaker-Aware Discourse Parsing on Multi-Party Dialogues\",\n    author = \"Yu, Nan  and\n      Fu, Guohong  and\n      Zhang, Min\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.477/\",\n    pages = \"5372--5382\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.477.pdf",
        "site": "https://aclanthology.org/2022.coling-1.477/",
        "pdf_size": 866637,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13948481179277486332&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China+Institute of Arti\ufb01cial Intelligence, Soochow University, China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China",
        "aff_domain": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "https://github.com/yunan4nlp/SA-DPMD",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+0;1",
        "aff_unique_norm": "Soochow University;Harbin Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Technology;Institute of Computing and Intelligence",
        "aff_unique_url": "https://eng.suda.edu.cn/;http://www.hhit.edu.cn",
        "aff_unique_abbr": "Soochow U;HIT",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.453",
        "title": "Speeding up Transformer Decoding via an Attention Refinement Network",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Despite the revolutionary advances made by Transformer in Neural Machine Translation (NMT), inference efficiency remains an obstacle due to the heavy use of attention operations in auto-regressive decoding. We thereby propose a lightweight attention structure called Attention Refinement Network (ARN) for speeding up Transformer. Specifically, we design a weighted residual network, which reconstructs the attention by reusing the features across layers. To further improve the Transformer efficiency, we merge the self-attention and cross-attention components for parallel computing. Extensive experiments on ten WMT machine translation tasks show that the proposed model yields an average of 1.35x faster (with almost no decrease in BLEU) over the state-of-the-art inference implementation. Results on widely used WMT14 En-De machine translation tasks demonstrate that our model achieves a higher speed-up, giving highly competitive performance compared to AAN and SAN models with fewer parameter numbers.",
        "author": "Kaixin Wu; Yue Zhang; Bojie Hu; Tong Zhang",
        "authorids": "/k/kaixin-wu/; /y/yue-zhang/; /b/bojie-hu/; /t/tong-zhang/",
        "bibtex": "@inproceedings{wu-etal-2022-speeding,\n    title = \"Speeding up Transformer Decoding via an Attention Refinement Network\",\n    author = \"Wu, Kaixin  and\n      Zhang, Yue  and\n      Hu, Bojie  and\n      Zhang, Tong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.453/\",\n    pages = \"5109--5118\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.453.pdf",
        "site": "https://aclanthology.org/2022.coling-1.453/",
        "pdf_size": 771860,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5085483929787515873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent Minority-Mandarin Translation, Beijing, China; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; Tencent Minority-Mandarin Translation, Beijing, China; Tencent Minority-Mandarin Translation, Beijing, China",
        "aff_domain": "tencent.com;westlake.edu.cn;tencent.com;tencent.com",
        "email": "tencent.com;westlake.edu.cn;tencent.com;tencent.com",
        "github": "https://github.com/Kaixin-Wu-for-Open-Source/ARNF",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;0;0",
        "aff_unique_norm": "Tencent;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": "Minority-Mandarin Translation;School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.tencent.com;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "Tencent;;WIAS",
        "aff_campus_unique_index": "0;;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.3",
        "title": "Stability of Syntactic Dialect Classification over Space and Time",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper analyses the degree to which dialect classifiers based on syntactic representations remain stable over space and time. While previous work has shown that the combination of grammar induction and geospatial text classification produces robust dialect models, we do not know what influence both changing grammars and changing populations have on dialect models. This paper constructs a test set for 12 dialects of English that spans three years at monthly intervals with a fixed spatial distribution across 1,120 cities. Syntactic representations are formulated within the usage-based Construction Grammar paradigm (CxG). The decay rate of classification performance for each dialect over time allows us to identify regions undergoing syntactic change. And the distribution of classification accuracy within dialect regions allows us to identify the degree to which the grammar of a dialect is internally heterogeneous. The main contribution of this paper is to show that a rigorous evaluation of dialect classification models can be used to find both variation over space and change over time.",
        "author": "Jonathan Dunn; Sidney Wong",
        "authorids": "/j/jonathan-dunn/; /s/sidney-wong/",
        "bibtex": "@inproceedings{dunn-wong-2022-stability,\n    title = \"Stability of Syntactic Dialect Classification over Space and Time\",\n    author = \"Dunn, Jonathan  and\n      Wong, Sidney\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.3/\",\n    pages = \"26--36\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.3.pdf",
        "site": "https://aclanthology.org/2022.coling-1.3/",
        "pdf_size": 652034,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3733118357325508235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Linguistics, University of Canterbury + New Zealand Institute for Language, Brain and Behaviour; Geospatial Research Institute Toi Hangarau",
        "aff_domain": "canterbury.ac.nz;pg.canterbury.ac.nz",
        "email": "canterbury.ac.nz;pg.canterbury.ac.nz",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "University of Canterbury;New Zealand Institute for Language, Brain and Behaviour;Geospatial Research Institute",
        "aff_unique_dep": "Department of Linguistics;;Toi Hangarau",
        "aff_unique_url": "https://www.canterbury.ac.nz;;",
        "aff_unique_abbr": "UC;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "2022.coling-1.474",
        "title": "String Editing Based Chinese Grammatical Error Diagnosis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chinese Grammatical Error Diagnosis (CGED) suffers the problems of numerous types of grammatical errors and insufficiency of training data. In this paper, we propose a string editing based CGED model that requires less training data by using a unified workflow to handle various types of grammatical errors. Two measures are proposed in our model to enhance the performance of CGED. First, the detection and correction of grammatical errors are divided into different stages. In the stage of error detection, the model only outputs the types of grammatical errors so that the tag vocabulary size is significantly reduced compared with other string editing based models. Secondly, the correction of some grammatical errors is converted to the task of masked character inference, which has plenty of training data and mature solutions. Experiments on datasets of NLPTEA-CGED demonstrate that our model outperforms other CGED models in many aspects.",
        "author": "Haihua Xie; Xiaoqing Lyu; Xuefei Chen",
        "authorids": "/h/haihua-xie/; /x/xiaoqing-lyu/; /x/xuefei-chen/",
        "bibtex": "@inproceedings{xie-etal-2022-string,\n    title = \"String Editing Based {C}hinese Grammatical Error Diagnosis\",\n    author = \"Xie, Haihua  and\n      Lyu, Xiaoqing  and\n      Chen, Xuefei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.474/\",\n    pages = \"5335--5344\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.474.pdf",
        "site": "https://aclanthology.org/2022.coling-1.474/",
        "pdf_size": 626866,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7905194083584208&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Yanqi Lake Beijing Institute of Mathematical Sciences and Applications / Beijing, China; Wangxuan Institute of Computer Technology, Peking University / Beijing, China; AI Research Center, Cloopen Group Holding Ltd / Beijing, China",
        "aff_domain": "bimsa.cn;pku.edu.cn;163.com",
        "email": "bimsa.cn;pku.edu.cn;163.com",
        "github": "https://github.com/xiebimsa/se-cged",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Beijing Institute of Mathematical Sciences and Applications;Peking University;Cloopen Group Holding Ltd",
        "aff_unique_dep": "Mathematical Sciences and Applications;Wangxuan Institute of Computer Technology;AI Research Center",
        "aff_unique_url": ";http://www.pku.edu.cn;",
        "aff_unique_abbr": ";PKU;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Yanqi Lake;Beijing;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.585",
        "title": "Structural Bias for Aspect Sentiment Triplet Extraction",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Structural bias has recently been exploited for aspect sentiment triplet extraction (ASTE) and led to improved performance. On the other hand, it is recognized that explicitly incorporating structural bias would have a negative impact on efficiency, whereas pretrained language models (PLMs) can already capture implicit structures. Thus, a natural question arises: Is structural bias still a necessity in the context of PLMs? To answer the question, we propose to address the efficiency issues by using an adapter to integrate structural bias in the PLM and using a cheap-to-compute relative position structure in place of the syntactic dependency structure. Benchmarking evaluation is conducted on the SemEval datasets. The results show that our proposed structural adapter is beneficial to PLMs and achieves state-of-the-art performance over a range of strong baselines, yet with a light parameter demand and low latency. Meanwhile, we give rise to the concern that the current evaluation default with data of small scale is under-confident. Consequently, we release a large-scale dataset for ASTE. The results on the new dataset hint that the structural adapter is confidently effective and efficient to a large scale. Overall, we draw the conclusion that structural bias shall still be a necessity even with PLMs.",
        "author": "Chen Zhang; Lei Ren; Fang Ma; Jingang Wang; Wei Wu; Dawei Song",
        "authorids": "/c/chen-zhang/; /l/lei-ren/; /f/fang-ma/; /j/jingang-wang/; /w/wei-wu/; /d/dawei-song/",
        "bibtex": "@inproceedings{zhang-etal-2022-structural,\n    title = \"Structural Bias for Aspect Sentiment Triplet Extraction\",\n    author = \"Zhang, Chen  and\n      Ren, Lei  and\n      Ma, Fang  and\n      Wang, Jingang  and\n      Wu, Wei  and\n      Song, Dawei\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.585/\",\n    pages = \"6736--6745\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.585.pdf",
        "site": "https://aclanthology.org/2022.coling-1.585/",
        "pdf_size": 445174,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1414351956252309577&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Beijing Institute of Technology; Meituan NLP; Beijing Institute of Technology + Meituan NLP; Beijing Institute of Technology + Meituan NLP; Meituan NLP; Beijing Institute of Technology",
        "aff_domain": "bit.edu.cn;163.com;bit.edu.cn;meituan.com;meituan.com;bit.edu.cn",
        "email": "bit.edu.cn;163.com;bit.edu.cn;meituan.com;meituan.com;bit.edu.cn",
        "github": "https://github.com/GeneZC/StructBias",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0+1;0+1;1;0",
        "aff_unique_norm": "Beijing Institute of Technology;Meituan",
        "aff_unique_dep": ";NLP",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BIT;Meituan",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.251",
        "title": "Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their splendid large-scale models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we conduct unsupervised domain adaptation and multi-victim ensemble to showing that attackers could potentially surpass victims, which is beyond previous understanding of model extraction. Extensive experiments on both benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models on transferred domains. We consider our work as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.",
        "author": "Qiongkai Xu; Xuanli He; Lingjuan Lyu; Lizhen Qu; Gholamreza Haffari",
        "authorids": "/q/qiongkai-xu/; /x/xuanli-he/; /l/lingjuan-lyu/; /l/lizhen-qu/; /g/gholamreza-haffari/",
        "bibtex": "@inproceedings{xu-etal-2022-student,\n    title = \"Student Surpasses Teacher: Imitation Attack for Black-Box {NLP} {API}s\",\n    author = \"Xu, Qiongkai  and\n      He, Xuanli  and\n      Lyu, Lingjuan  and\n      Qu, Lizhen  and\n      Haffari, Gholamreza\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.251/\",\n    pages = \"2849--2860\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.251.pdf",
        "site": "https://aclanthology.org/2022.coling-1.251/",
        "pdf_size": 494649,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8390583273366694088&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Melbourne; Monash University; Sony AI; Monash University; Monash University",
        "aff_domain": "unimelb.edu.au;monash.edu;sony.com;monash.edu;monash.edu",
        "email": "unimelb.edu.au;monash.edu;sony.com;monash.edu;monash.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;1;1",
        "aff_unique_norm": "University of Melbourne;Monash University;Sony",
        "aff_unique_dep": ";;Sony AI",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.monash.edu;https://www.sony.com",
        "aff_unique_abbr": "UniMelb;Monash;Sony AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Australia;Japan"
    },
    {
        "id": "2022.coling-1.4",
        "title": "Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Both humans and neural language models are able to perform subject verb number agreement (SVA). In principle, semantics shouldn\u2019t interfere with this task, which only requires syntactic knowledge. In this work we test whether meaning interferes with this type of agreement in English in syntactic structures of various complexities. To do so, we generate both semantically well-formed and nonsensical items. We compare the performance of BERT-base to that of humans, obtained with a psycholinguistic online crowdsourcing experiment. We find that BERT and humans are both sensitive to our semantic manipulation: They fail more often when presented with nonsensical items, especially when their syntactic structure features an attractor (a noun phrase between the subject and the verb that has not the same number as the subject). We also find that the effect of meaningfulness on SVA errors is stronger for BERT than for humans, showing higher lexical sensitivity of the former on this task.",
        "author": "Karim Lasri; Olga Seminck; Alessandro Lenci; Thierry Poibeau",
        "authorids": "/k/karim-lasri/; /o/olga-seminck/; /a/alessandro-lenci/; /t/thierry-poibeau/",
        "bibtex": "@inproceedings{lasri-etal-2022-subject,\n    title = \"Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. {BERT}\",\n    author = \"Lasri, Karim  and\n      Seminck, Olga  and\n      Lenci, Alessandro  and\n      Poibeau, Thierry\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.4/\",\n    pages = \"37--43\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.4.pdf",
        "site": "https://aclanthology.org/2022.coling-1.4/",
        "pdf_size": 286428,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11708193563053373535&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle); Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle) + University of Pisa; University of Pisa; Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle)",
        "aff_domain": "ens.psl.eu;cnrs.fr;unipi.it;ens.psl.eu",
        "email": "ens.psl.eu;cnrs.fr;unipi.it;ens.psl.eu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;1;0",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure-PSL;University of Pisa",
        "aff_unique_dep": "Lattice;",
        "aff_unique_url": "https://www.ens.psl.eu;https://www.unipi.it",
        "aff_unique_abbr": "ENS-PSL;UNIP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;1;0",
        "aff_country_unique": "France;Italy"
    },
    {
        "id": "2022.coling-1.556",
        "title": "Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The difficulty of generating coherent long texts lies in the fact that existing models overwhelmingly focus on the tasks of local word prediction, and cannot make high level plans on what to generate or capture the high-level discourse dependencies between chunks of texts. Inspired by how humans write, where a list of bullet points or a catalog is first outlined, and then each bullet point is expanded to form the whole article, we propose SOE, a pipelined system that involves of summarizing, outlining and elaborating for long text generation: the model first outlines the summaries for different segments of long texts, and then elaborates on each bullet point to generate the corresponding segment. To avoid the labor-intensive process of summary soliciting, we propose the reconstruction strategy, which extracts segment summaries in an unsupervised manner by selecting its most informative part to reconstruct the segment. The proposed generation system comes with the following merits: (1) the summary provides high-level guidance for text generation and avoids the local minimum of individual word predictions; (2) the high-level discourse dependencies are captured in the conditional dependencies between summaries and are preserved during the summary expansion process and (3) additionally, we are able to consider significantly more contexts by representing contexts as concise summaries. Extensive experiments demonstrate that SOE produces long texts with significantly better quality, along with faster convergence speed.",
        "author": "Xiaofei Sun; Zijun Sun; Yuxian Meng; Jiwei Li; Chun Fan",
        "authorids": "/x/xiaofei-sun/; /z/zijun-sun/; /y/yuxian-meng/; /j/jiwei-li/; /c/chun-fan/",
        "bibtex": "@inproceedings{sun-etal-2022-summarize,\n    title = \"Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries\",\n    author = \"Sun, Xiaofei  and\n      Sun, Zijun  and\n      Meng, Yuxian  and\n      Li, Jiwei  and\n      Fan, Chun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.556/\",\n    pages = \"6392--6402\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.556.pdf",
        "site": "https://aclanthology.org/2022.coling-1.556/",
        "pdf_size": 506598,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7799055107711291137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University + Shannon.AI; Shannon.AI; Shannon.AI; Zhejiang University + Shannon.AI; Computer Center, Peking University + National Biomedical Imaging Center, Peking University + Peng Cheng Laboratory",
        "aff_domain": "zju.edu.cn; ; ;shannonai.com; ",
        "email": "zju.edu.cn; ; ;shannonai.com; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;0+1;2+2+3",
        "aff_unique_norm": "Zhejiang University;Shannon.AI;Peking University;Peng Cheng Laboratory",
        "aff_unique_dep": ";;Computer Center;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.shannon.ai;http://www.pku.edu.cn;http://www.pcl.ac.cn",
        "aff_unique_abbr": "ZJU;Shannon.AI;PKU;PCL",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+1;1;1;0+1;0+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.coling-1.528",
        "title": "Summarizing Dialogues with Negative Cues",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Abstractive dialogue summarization aims to convert a long dialogue content into its short form where the salient information is preserved while the redundant pieces are ignored. Different from the well-structured text, such as news and scientific articles, dialogues often consist of utterances coming from two or more interlocutors, where the conversations are often informal, verbose, and repetitive, sprinkled with false-starts, backchanneling, reconfirmations, hesitations, speaker interruptions and the salient information is often scattered across the whole chat. The above properties of conversations make it difficult to directly concentrate on scattered outstanding utterances and thus present new challenges of summarizing dialogues. In this work, rather than directly forcing a summarization system to merely pay more attention to the salient pieces, we propose to explicitly have the model perceive the redundant parts of an input dialogue history during the training phase. To be specific, we design two strategies to construct examples without salient pieces as negative cues. Then, the sequence-to-sequence likelihood loss is cooperated with the unlikelihood objective to drive the model to focus less on the unimportant information and also pay more attention to the salient pieces. Extensive experiments on the benchmark dataset demonstrate that our simple method significantly outperforms the baselines with regard to both semantic matching and factual consistent based metrics. The human evaluation also proves the performance gains.",
        "author": "Junpeng Liu; Yanyan Zou; Yuxuan Xi; Shengjie Li; Mian Ma; Zhuoye Ding",
        "authorids": "/j/junpeng-liu/; /y/yanyan-zou/; /y/yuxuan-xi/; /s/shengjie-li/; /m/mian-ma/; /z/zhuoye-ding/",
        "bibtex": "@inproceedings{liu-etal-2022-summarizing,\n    title = \"Summarizing Dialogues with Negative Cues\",\n    author = \"Liu, Junpeng  and\n      Zou, Yanyan  and\n      Xi, Yuxuan  and\n      Li, Shengjie  and\n      Ma, Mian  and\n      Ding, Zhuoye\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.528/\",\n    pages = \"6050--6056\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.528.pdf",
        "site": "https://aclanthology.org/2022.coling-1.528/",
        "pdf_size": 238756,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9093997533157893892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications, Beijing, China + JD.com, Beijing, China; JD.com, Beijing, China; Peking University, Beijing, China; JD.com, Beijing, China; JD.com, Beijing, China; JD.com, Beijing, China",
        "aff_domain": "bupt.edu.cn;jd.com;pku.edu.cn;jd.com;jd.com;jd.com",
        "email": "bupt.edu.cn;jd.com;pku.edu.cn;jd.com;jd.com;jd.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;2;1;1;1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;JD.com;Peking University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.jd.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "BUPT;JD;Peking U",
        "aff_campus_unique_index": "0+0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.264",
        "title": "Summarizing Patients\u2019 Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Automatically summarizing patients\u2019 main problems from daily progress notes using natural language processing methods helps to battle against information and cognitive overload in hospital settings and potentially assists providers with computerized diagnostic decision support. Problem list summarization requires a model to understand, abstract, and generate clinical documentation. In this work, we propose a new NLP task that aims to generate a list of problems in a patient\u2019s daily care plan using input from the provider\u2019s progress notes during hospitalization. We investigate the performance of T5 and BART, two state-of-the-art seq2seq transformer architectures, in solving this problem. We provide a corpus built on top of progress notes from publicly available electronic health record progress notes in the Medical Information Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain text, and we experiment with a data augmentation method and a domain adaptation pre-training method to increase exposure to medical vocabulary and knowledge. Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence embedding, and F-score on medical concepts. Results show that T5 with domain adaptive pre-training achieves significant performance gains compared to a rule-based system and general domain pre-trained language models, indicating a promising direction for tackling the problem summarization task.",
        "author": "Yanjun Gao; Dmitriy Dligach; Timothy Miller; Dongfang Xu; Matthew M. M. Churpek; Majid Afshar",
        "authorids": "/y/yanjun-gao/; /d/dmitriy-dligach/; /t/timothy-miller/; /d/dongfang-xu/; /m/matthew-m-m-churpek/; /m/majid-afshar/",
        "bibtex": "@inproceedings{gao-etal-2022-summarizing,\n    title = \"Summarizing Patients' Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models\",\n    author = \"Gao, Yanjun  and\n      Dligach, Dmitriy  and\n      Miller, Timothy  and\n      Xu, Dongfang  and\n      Churpek, Matthew M. M.  and\n      Afshar, Majid\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.264/\",\n    pages = \"2979--2991\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.264.pdf",
        "site": "https://aclanthology.org/2022.coling-1.264/",
        "pdf_size": 997977,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15684397922695475783&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "ICU Data Science Lab, School of Medicine and Public Health, University of Wisconsin-Madison; Loyola University Chicago; Boston Children\u2019s Hospital and Harvard Medical School; Boston Children\u2019s Hospital and Harvard Medical School; ICU Data Science Lab, School of Medicine and Public Health, University of Wisconsin-Madison; ICU Data Science Lab, School of Medicine and Public Health, University of Wisconsin-Madison",
        "aff_domain": "medicine.wisc.edu;luc.edu;childrens.harvard.edu;childrens.harvard.edu;medicine.wisc.edu;medicine.wisc.edu",
        "email": "medicine.wisc.edu;luc.edu;childrens.harvard.edu;childrens.harvard.edu;medicine.wisc.edu;medicine.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;2;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Loyola University Chicago;Boston Children's Hospital",
        "aff_unique_dep": "School of Medicine and Public Health;;",
        "aff_unique_url": "https://www.wisc.edu;https://www.luc.edu;https://www.childrenshospital.org",
        "aff_unique_abbr": "UW-Madison;LUC;BCH",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Madison;Chicago;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.216",
        "title": "Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Medical Relation Extraction (MRE) task aims to extract relations between entities in medical texts. Traditional relation extraction methods achieve impressive success by exploring the syntactic information, e.g., dependency tree. However, the quality of the 1-best dependency tree for medical texts produced by an out-of-domain parser is relatively limited so that the performance of medical relation extraction method may degenerate. To this end, we propose a method to jointly model semantic and syntactic information from medical texts based on causal explanation theory. We generate dependency forests consisting of the semantic-embedded 1-best dependency tree. Then, a task-specific causal explainer is adopted to prune the dependency forests, which are further fed into a designed graph convolutional network to learn the corresponding representation for downstream task. Empirically, the various comparisons on benchmark medical datasets demonstrate the effectiveness of our model.",
        "author": "Yifan Jin; Jiangmeng Li; Zheng Lian; Chengbo Jiao; Xiaohui Hu",
        "authorids": "/y/yifan-jin/; /j/jiangmeng-li/; /z/zheng-lian/; /c/chengbo-jiao/; /x/xiaohui-hu/",
        "bibtex": "@inproceedings{jin-etal-2022-supporting,\n    title = \"Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest\",\n    author = \"Jin, Yifan  and\n      Li, Jiangmeng  and\n      Lian, Zheng  and\n      Jiao, Chengbo  and\n      Hu, Xiaohui\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.216/\",\n    pages = \"2450--2460\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.216.pdf",
        "site": "https://aclanthology.org/2022.coling-1.216/",
        "pdf_size": 696080,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16389912591637085784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Chinese Academy of Sciences+Institute of Software Chinese Academy of Sciences; University of Chinese Academy of Sciences+Institute of Software Chinese Academy of Sciences; University of Chinese Academy of Sciences+Institute of Software Chinese Academy of Sciences; University of Electronic Science and Technology of China; University of Chinese Academy of Sciences+Institute of Software Chinese Academy of Sciences",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;hotmail.com;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;hotmail.com;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;2;0+1",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences;University of Electronic Science and Technology of China",
        "aff_unique_dep": ";Institute of Software;",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.is.cas.cn;https://www.uestc.edu.cn",
        "aff_unique_abbr": "UCAS;CAS;UESTC",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.493",
        "title": "Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In embodied cognition, physical experiences are believed to shape abstract cognition, such as natural language and reasoning. Image schemas were introduced as spatio-temporal cognitive building blocks that capture these recurring sensorimotor experiences. The few existing approaches for automatic detection of image schemas in natural language rely on specific assumptions about word classes as indicators of spatio-temporal events. Furthermore, the lack of sufficiently large, annotated datasets makes evaluation and supervised learning difficult. We propose to build on the recent success of large multilingual pretrained language models and a small dataset of examples from image schema literature to train a supervised classifier that classifies natural language expressions of varying lengths into image schemas. Despite most of the training data being in English with few examples for German, the model performs best in German. Additionally, we analyse the model\u2019s zero-shot performance in Russian, French, and Mandarin. To further investigate the model\u2019s behaviour, we utilize local linear approximations for prediction probabilities that indicate which words in a sentence the model relies on for its final classification decision. Code and dataset are publicly available.",
        "author": "Lennart Wachowiak; Dagmar Gromann",
        "authorids": "/l/lennart-wachowiak/; /d/dagmar-gromann/",
        "bibtex": "@inproceedings{wachowiak-gromann-2022-systematic,\n    title = \"Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing\",\n    author = \"Wachowiak, Lennart  and\n      Gromann, Dagmar\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.493/\",\n    pages = \"5571--5581\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.493.pdf",
        "site": "https://aclanthology.org/2022.coling-1.493/",
        "pdf_size": 351264,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12457781953835454616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "King\u2019s College London; University of Vienna",
        "aff_domain": "kcl.ac.uk;gmail.com",
        "email": "kcl.ac.uk;gmail.com",
        "github": "",
        "project": "https://tinyurl.com/24haedv5",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "King's College London;University of Vienna",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kcl.ac.uk;https://univie.ac.at",
        "aff_unique_abbr": "KCL;UV",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Austria"
    },
    {
        "id": "2022.coling-1.20",
        "title": "TAKE: Topic-shift Aware Knowledge sElection for Dialogue Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge-grounded dialogue generation consists of two subtasks: knowledge selection and response generation. The knowledge selector generally constructs a query based on the dialogue context and selects the most appropriate knowledge to help response generation. Recent work finds that realizing who (the user or the agent) holds the initiative and utilizing the role-initiative information to instruct the query construction can help select knowledge. It depends on whether the knowledge connection between two adjacent rounds is smooth to assign the role. However, whereby the user takes the initiative only when there is a strong semantic transition between two rounds, probably leading to initiative misjudgment. Therefore, it is necessary to seek a more sensitive reason beyond the initiative role for knowledge selection. To address the above problem, we propose a Topic-shift Aware Knowledge sElector(TAKE). Specifically, we first annotate the topic shift and topic inheritance labels in multi-round dialogues with distant supervision. Then, we alleviate the noise problem in pseudo labels through curriculum learning and knowledge distillation. Extensive experiments on WoW show that TAKE performs better than strong baselines.",
        "author": "Chenxu Yang; Zheng Lin; Jiangnan Li; Fandong Meng; Weiping Wang; Lanrui Wang; Jie Zhou",
        "authorids": "/c/chenxu-yang/; /z/zheng-lin/; /j/jiangnan-li/; /f/fandong-meng/; /w/weiping-wang/; /l/lanrui-wang/; /j/jie-zhou/",
        "bibtex": "@inproceedings{yang-etal-2022-take,\n    title = \"{TAKE}: Topic-shift Aware Knowledge s{E}lection for Dialogue Generation\",\n    author = \"Yang, Chenxu  and\n      Lin, Zheng  and\n      Li, Jiangnan  and\n      Meng, Fandong  and\n      Wang, Weiping  and\n      Wang, Lanrui  and\n      Zhou, Jie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.20/\",\n    pages = \"253--265\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.20.pdf",
        "site": "https://aclanthology.org/2022.coling-1.20/",
        "pdf_size": 531401,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8100350228935481446&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;mails.ucas.ac.cn;mails.ucas.ac.cn;tencent.com;tencent.com",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;mails.ucas.ac.cn;mails.ucas.ac.cn;tencent.com;tencent.com",
        "github": "https://github.com/iie-ycx/COLING2022-TAKE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;2;0+1;0+1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Tencent Inc",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.tencent.com",
        "aff_unique_abbr": "CAS;UCAS;Tencent",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.302",
        "title": "TERMinator: A System for Scientific Texts Processing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "This paper is devoted to the extraction of entities and semantic relations between them from scientific texts, where we consider scientific terms as entities. In this paper, we present a dataset that includes annotations for two tasks and develop a system called TERMinator for the study of the influence of language models on term recognition and comparison of different approaches for relation extraction. Experiments show that language models pre-trained on the target language are not always show the best performance. Also adding some heuristic approaches may improve the overall quality of the particular task. The developed tool and the annotated corpus are publicly available at https://github.com/iis-research-team/terminator and may be useful for other researchers.",
        "author": "Elena Bruches; Olga Tikhobaeva; Yana Dementyeva; Tatiana Batura",
        "authorids": "/e/elena-bruches/; /o/olga-tikhobaeva/; /y/yana-dementyeva/; /t/tatiana-batura/",
        "bibtex": "@inproceedings{bruches-etal-2022-terminator,\n    title = \"{TERM}inator: A System for Scientific Texts Processing\",\n    author = \"Bruches, Elena  and\n      Tikhobaeva, Olga  and\n      Dementyeva, Yana  and\n      Batura, Tatiana\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.302/\",\n    pages = \"3420--3426\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.302.pdf",
        "site": "https://aclanthology.org/2022.coling-1.302/",
        "pdf_size": 214822,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5017817070272226030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "A.P. Ershov Institute of Informatics Systems / Russia; Novosibirsk State University / Russia; Novosibirsk State University / Russia; A.P. Ershov Institute of Informatics Systems / Russia",
        "aff_domain": "bk.ru;g.nsu.ru;g.nsu.ru;iis.nsk.su",
        "email": "bk.ru;g.nsu.ru;g.nsu.ru;iis.nsk.su",
        "github": "https://github.com/iis-research-team/terminator",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "A.P. Ershov Institute of Informatics Systems;Novosibirsk State University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.nsu.ru",
        "aff_unique_abbr": ";NSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Russia"
    },
    {
        "id": "2022.coling-1.588",
        "title": "TSAM: A Two-Stream Attention Model for Causal Emotion Entailment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Causal Emotion Entailment (CEE) aims to discover the potential causes behind an emotion in a conversational utterance. Previous works formalize CEE as independent utterance pair classification problems, with emotion and speaker information neglected. From a new perspective, this paper considers CEE in a joint framework. We classify multiple utterances synchronously to capture the correlations between utterances in a global view and propose a Two-Stream Attention Model (TSAM) to effectively model the speaker\u2019s emotional influences in the conversational history. Specifically, the TSAM comprises three modules: Emotion Attention Network (EAN), Speaker Attention Network (SAN), and interaction module. The EAN and SAN incorporate emotion and speaker information in parallel, and the subsequent interaction module effectively interchanges relevant information between the EAN and SAN via a mutual BiAffine transformation. Extensive experimental results demonstrate that our model achieves new State-Of-The-Art (SOTA) performance and outperforms baselines remarkably.",
        "author": "Duzhen Zhang; Zhen Yang; Fandong Meng; Xiuyi Chen; Jie Zhou",
        "authorids": "/d/duzhen-zhang/; /z/zhen-yang/; /f/fandong-meng/; /x/xiuyi-chen/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2022-tsam,\n    title = \"{TSAM}: A Two-Stream Attention Model for Causal Emotion Entailment\",\n    author = \"Zhang, Duzhen  and\n      Yang, Zhen  and\n      Meng, Fandong  and\n      Chen, Xiuyi  and\n      Zhou, Jie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.588/\",\n    pages = \"6762--6772\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.588.pdf",
        "site": "https://aclanthology.org/2022.coling-1.588/",
        "pdf_size": 744905,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14539800578067736454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China",
        "aff_domain": "gmail.com;gmail.com;tencent.com;tencent.com;tencent.com",
        "email": "gmail.com;gmail.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tencent Inc",
        "aff_unique_dep": "Pattern Recognition Center",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.120",
        "title": "Table-based Fact Verification with Self-labeled Keypoint Alignment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Table-based fact verification aims to verify whether a statement sentence is trusted or fake. Most existing methods rely on graph feature or data augmentation but fail to investigate evidence correlation between the statement and table effectively. In this paper, we propose a self-Labeled Keypoint Alignment model, named LKA, to explore the correlation between the two. Specifically, a dual-view alignment module based on the statement and table views is designed to discriminate the salient words through multiple interactions, where one regular and one adversarial alignment network cooperatively character the alignment discrepancy. Considering the interaction characteristic inherent in the alignment module, we introduce a novel mixture-of experts block to elaborately integrate the interacted information for supporting the alignment and final classification. Furthermore, a contrastive learning loss is utilized to learn the precise representation of the structure-involved words, encouraging the words closer to words with the same table attribute and farther from the words with the unrelated attribute. Experimental results on three widely-studied datasets show that our model can outperform the state-of-the-art baselines and capture interpretable evidence words.",
        "author": "Guangzhen Zhao; Peng Yang",
        "authorids": "/g/guangzhen-zhao/; /p/peng-yang/",
        "bibtex": "@inproceedings{zhao-yang-2022-table,\n    title = \"Table-based Fact Verification with Self-labeled Keypoint Alignment\",\n    author = \"Zhao, Guangzhen  and\n      Yang, Peng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.120/\",\n    pages = \"1401--1411\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.120.pdf",
        "site": "https://aclanthology.org/2022.coling-1.120/",
        "pdf_size": 872072,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16563706060958383749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.330",
        "title": "Tafsir Dataset: A Novel Multi-Task Benchmark for Named Entity Recognition and Topic Modeling in Classical Arabic Literature",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Various historical languages, which used to be lingua franca of science and arts, deserve the attention of current NLP research. In this work, we take the first data-driven steps towards this research line for Classical Arabic (CA) by addressing named entity recognition (NER) and topic modeling (TM) on the example of CA literature. We manually annotate the encyclopedic work of Tafsir Al-Tabari with span-based NEs, sentence-based topics, and span-based subtopics, thus creating the Tafsir Dataset with over 51,000 sentences, the first large-scale multi-task benchmark for CA. Next, we analyze our newly generated dataset, which we make open-source available, with current language models (lightweight BiLSTM, transformer-based MaChAmP) along a novel script compression method, thereby achieving state-of-the-art performance for our target task CA-NER. We also show that CA-TM from the perspective of historical topic models, which are central to Arabic studies, is very challenging. With this interdisciplinary work, we lay the foundations for future research on automatic analysis of CA literature.",
        "author": "Sajawel Ahmed; Rob van der Goot; Misbahur Rehman; Carl Kruse; \u00d6mer \u00d6zsoy; Alexander Mehler; Gemma Roig",
        "authorids": "/s/sajawel-ahmed/; /r/rob-van-der-goot/; /m/misbahur-rehman/; /c/carl-kruse/; /o/omer-ozsoy/; /a/alexander-mehler/; /g/gemma-roig/",
        "bibtex": "@inproceedings{ahmed-etal-2022-tafsir,\n    title = \"Tafsir Dataset: A Novel Multi-Task Benchmark for Named Entity Recognition and Topic Modeling in Classical {A}rabic Literature\",\n    author = {Ahmed, Sajawel  and\n      van der Goot, Rob  and\n      Rehman, Misbahur  and\n      Kruse, Carl  and\n      {\\\"O}zsoy, {\\\"O}mer  and\n      Mehler, Alexander  and\n      Roig, Gemma},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.330/\",\n    pages = \"3753--3768\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.330.pdf",
        "site": "https://aclanthology.org/2022.coling-1.330/",
        "pdf_size": 897448,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8276072001783181547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Faculty for Computer Science and Mathematics, Goethe University Frankfurt + Department of Computer Science, IT University of Copenhagen; Department of Computer Science, IT University of Copenhagen; Faculty for Linguistics, Cultures, and Arts, Goethe University Frankfurt; Faculty for Linguistics, Cultures, and Arts, Goethe University Frankfurt; Faculty for Linguistics, Cultures, and Arts, Goethe University Frankfurt; Faculty for Computer Science and Mathematics, Goethe University Frankfurt; Faculty for Computer Science and Mathematics, Goethe University Frankfurt",
        "aff_domain": "em.uni-frankfurt.de; ; ; ; ; ; ",
        "email": "em.uni-frankfurt.de; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;0;0;0;0;0",
        "aff_unique_norm": "Goethe University Frankfurt;IT University of Copenhagen",
        "aff_unique_dep": "Faculty for Computer Science and Mathematics;Department of Computer Science",
        "aff_unique_url": "https://www.uni-frankfurt.de;https://itu.dk",
        "aff_unique_abbr": "Goethe U Frankfurt;ITU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Frankfurt;",
        "aff_country_unique_index": "0+1;1;0;0;0;0;0",
        "aff_country_unique": "Germany;Denmark"
    },
    {
        "id": "2022.coling-1.395",
        "title": "Taking Actions Separately: A Bidirectionally-Adaptive Transfer Learning Method for Low-Resource Neural Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Training Neural Machine Translation (NMT) models suffers from sparse parallel data, in the infrequent translation scenarios towards low-resource source languages. The existing solutions primarily concentrate on the utilization of Parent-Child (PC) transfer learning. It transfers well-trained NMT models on high-resource languages (namely Parent NMT) to low-resource languages, so as to produce Child NMT models by fine-tuning. It has been carefully demonstrated that a variety of PC variants yield significant improvements for low-resource NMT. In this paper, we intend to enhance PC-based NMT by a bidirectionally-adaptive learning strategy. Specifically, we divide inner constituents (6 transformers) of Parent encoder into two \u201cteams\u201d, i.e., T1 and T2. During representation learning, T1 learns to encode low-resource languages conditioned on bilingual shareable latent space. Generative adversarial network and masked language modeling are used for space-shareable encoding. On the other hand, T2 is straightforwardly transferred to low-resource languages, and fine-tuned together with T1 for low-resource translation. Briefly, T1 and T2 take actions separately for different goals. The former aims to adapt to characteristics of low-resource languages during encoding, while the latter adapts to translation experiences learned from high-resource languages. We experiment on benchmark corpora SETIMES, conducting low-resource NMT for Albanian (Sq), Macedonian (Mk), Croatian (Hr) and Romanian (Ro). Experimental results show that our method yields substantial improvements, which allows the NMT performance to reach BLEU4-scores of 62.24%, 56.93%, 50.53% and 54.65% for Sq, Mk, Hr and Ro, respectively.",
        "author": "Xiaolin Xing; Yu Hong; Minhan Xu; Jianmin Yao; Guodong Zhou",
        "authorids": "/x/xiaolin-xing/; /y/yu-hong/; /m/minhan-xu/; /j/jianmin-yao/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{xing-etal-2022-taking,\n    title = \"Taking Actions Separately: A Bidirectionally-Adaptive Transfer Learning Method for Low-Resource Neural Machine Translation\",\n    author = \"Xing, Xiaolin  and\n      Hong, Yu  and\n      Xu, Minhan  and\n      Yao, Jianmin  and\n      Zhou, Guodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.395/\",\n    pages = \"4481--4491\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.395.pdf",
        "site": "https://aclanthology.org/2022.coling-1.395/",
        "pdf_size": 455365,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9331273435693978813&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;suda.edu.cn;suda.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.273",
        "title": "Tales and Tropes: Gender Roles from Word Embeddings in a Century of Children\u2019s Books",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The manner in which gender is portrayed in materials used to teach children conveys messages about people\u2019s roles in society. In this paper, we measure the gendered depiction of central domains of social life in 100 years of highly influential children\u2019s books. We make two main contributions: (1) we find that the portrayal of gender in these books reproduces traditional gender norms in society, and (2) we publish StoryWords 1.0, the first word embeddings trained on such a large body of children\u2019s literature. We find that, relative to males, females are more likely to be represented in relation to their appearance than in relation to their competence; second, they are more likely to be represented in relation to their role in the family than their role in business. Finally, we find that non-binary or gender-fluid individuals are rarely mentioned. Our analysis advances understanding of the different messages contained in content commonly used to teach children, with immediate applications for practice, policy, and research.",
        "author": "Anjali Adukia; Patricia Chiril; Callista Christ; Anjali Das; Alex Eble; Emileigh Harrison; Hakizumwami Birali Runesha",
        "authorids": "/a/anjali-adukia/; /p/patricia-chiril/; /c/callista-christ/; /a/anjali-das/; /a/alex-eble/; /e/emileigh-harrison/; /h/hakizumwami-birali-runesha/",
        "bibtex": "@inproceedings{adukia-etal-2022-tales,\n    title = \"Tales and Tropes: Gender Roles from Word Embeddings in a Century of Children{'}s Books\",\n    author = \"Adukia, Anjali  and\n      Chiril, Patricia  and\n      Christ, Callista  and\n      Das, Anjali  and\n      Eble, Alex  and\n      Harrison, Emileigh  and\n      Runesha, Hakizumwami Birali\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.273/\",\n    pages = \"3086--3097\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.273.pdf",
        "site": "https://aclanthology.org/2022.coling-1.273/",
        "pdf_size": 374857,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9843945886291184205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Chicago; University of Chicago; University of Chicago; Columbia University; Columbia University; University of Chicago; University of Chicago",
        "aff_domain": "uchicago.edu;uchicago.edu;uchicago.edu;columbia.edu;tc.columbia.edu;uchicago.edu;uchicago.edu",
        "email": "uchicago.edu;uchicago.edu;uchicago.edu;columbia.edu;tc.columbia.edu;uchicago.edu;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;1;0;0",
        "aff_unique_norm": "University of Chicago;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UChicago;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.605",
        "title": "Target Really Matters: Target-aware Contrastive Learning and Consistency Regularization for Few-shot Stance Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Stance detection aims to identify the attitude from an opinion towards a certain target. Despite the significant progress on this task, it is extremely time-consuming and budget-unfriendly to collect sufficient high-quality labeled data for every new target under fully-supervised learning, whereas unlabeled data can be collected easier. Therefore, this paper is devoted to few-shot stance detection and investigating how to achieve satisfactory results in semi-supervised settings. As a target-oriented task, the core idea of semi-supervised few-shot stance detection is to make better use of target-relevant information from labeled and unlabeled data. Therefore, we develop a novel target-aware semi-supervised framework. Specifically, we propose a target-aware contrastive learning objective to learn more distinguishable representations for different targets. Such an objective can be easily applied with or without unlabeled data. Furthermore, to thoroughly exploit the unlabeled data and facilitate the model to learn target-relevant stance features in the opinion content, we explore a simple but effective target-aware consistency regularization combined with a self-training strategy. The experimental results demonstrate that our approach can achieve state-of-the-art performance on multiple benchmark datasets in the few-shot setting.",
        "author": "Rui Liu; Zheng Lin; Huishan Ji; Jiangnan Li; Peng Fu; Weiping Wang",
        "authorids": "/r/rui-liu/; /z/zheng-lin/; /h/huishan-ji/; /j/jiangnan-li/; /p/peng-fu/; /w/weiping-wang/",
        "bibtex": "@inproceedings{liu-etal-2022-target,\n    title = \"Target Really Matters: Target-aware Contrastive Learning and Consistency Regularization for Few-shot Stance Detection\",\n    author = \"Liu, Rui  and\n      Lin, Zheng  and\n      Ji, Huishan  and\n      Li, Jiangnan  and\n      Fu, Peng  and\n      Wang, Weiping\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.605/\",\n    pages = \"6944--6954\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.605.pdf",
        "site": "https://aclanthology.org/2022.coling-1.605/",
        "pdf_size": 5995067,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7033999065783280647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.55",
        "title": "Target-Guided Open-Domain Conversation Planning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prior studies addressing target-oriented conversational tasks lack a crucial notion that has been intensively studied in the context of goal-oriented artificial intelligence agents, namely, planning. In this study, we propose the task of Target-Guided Open-Domain Conversation Planning (TGCP) task to evaluate whether neural conversational agents have goal-oriented conversation planning abilities. Using the TGCP task, we investigate the conversation planning abilities of existing retrieval models and recent strong generative models. The experimental results reveal the challenges facing current technology.",
        "author": "Yosuke Kishinami; Reina Akama; Shiki Sato; Ryoko Tokuhisa; Jun Suzuki; Kentaro Inui",
        "authorids": "/y/yosuke-kishinami/; /r/reina-akama/; /s/shiki-sato/; /r/ryoko-tokuhisa/; /j/jun-suzuki/; /k/kentaro-inui/",
        "bibtex": "@inproceedings{kishinami-etal-2022-target,\n    title = \"Target-Guided Open-Domain Conversation Planning\",\n    author = \"Kishinami, Yosuke  and\n      Akama, Reina  and\n      Sato, Shiki  and\n      Tokuhisa, Ryoko  and\n      Suzuki, Jun  and\n      Inui, Kentaro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.55/\",\n    pages = \"660--668\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.55.pdf",
        "site": "https://aclanthology.org/2022.coling-1.55/",
        "pdf_size": 792663,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=142196728789566576&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Tohoku University; Tohoku University+RIKEN; Tohoku University; Tohoku University; Tohoku University+RIKEN; Tohoku University+RIKEN",
        "aff_domain": "dc.tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp",
        "email": "dc.tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;0;0+1;0+1",
        "aff_unique_norm": "Tohoku University;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "Tohoku U;RIKEN",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.129",
        "title": "Teaching Neural Module Networks to Do Arithmetic",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Answering complex questions that require multi-step multi-type reasoning over raw text is challenging, especially when conducting numerical reasoning. Neural Module Networks (NMNs), follow the programmer-interpreter framework and design trainable modules to learn different reasoning skills. However, NMNs only have limited reasoning abilities, and lack numerical reasoning capability. We upgrade NMNs by: (a) bridging the gap between its interpreter and the complex questions; (b) introducing addition and subtraction modules that perform numerical reasoning over numbers. On a subset of DROP, experimental results show that our proposed methods enhance NMNs\u2019 numerical reasoning skills by 17.7% improvement of F1 score and significantly outperform previous state-of-the-art models.",
        "author": "Jiayi Chen; Xiao-Yu Guo; Yuan-Fang Li; Gholamreza Haffari",
        "authorids": "/j/jiayi-chen/; /x/xiao-yu-guo/; /y/yuan-fang-li/; /g/gholamreza-haffari/",
        "bibtex": "@inproceedings{chen-etal-2022-teaching-neural,\n    title = \"Teaching Neural Module Networks to Do Arithmetic\",\n    author = \"Chen, Jiayi  and\n      Guo, Xiao-Yu  and\n      Li, Yuan-Fang  and\n      Haffari, Gholamreza\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.129/\",\n    pages = \"1502--1510\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.129.pdf",
        "site": "https://aclanthology.org/2022.coling-1.129/",
        "pdf_size": 930541,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16776135426550266884&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Faculty of Information Technology, Monash University, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia",
        "aff_domain": "student.monash.edu;monash.edu;monash.edu;monash.edu",
        "email": "student.monash.edu;monash.edu;monash.edu;monash.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Faculty of Information Technology",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.coling-1.296",
        "title": "TempoWiC: An Evaluation Benchmark for Detecting Meaning Shift in Social Media",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Language evolves over time, and word meaning changes accordingly. This is especially true in social media, since its dynamic nature leads to faster semantic shifts, making it challenging for NLP models to deal with new content and trends. However, the number of datasets and models that specifically address the dynamic nature of these social platforms is scarce. To bridge this gap, we present TempoWiC, a new benchmark especially aimed at accelerating research in social media-based meaning shift. Our results show that TempoWiC is a challenging benchmark, even for recently-released language models specialized in social media.",
        "author": "Daniel Loureiro; Aminette D\u2019Souza; Areej Nasser Muhajab; Isabella A. White; Gabriel Wong; Luis Espinosa-Anke; Leonardo Neves; Francesco Barbieri; Jose Camacho-Collados",
        "authorids": "/d/daniel-loureiro/; /a/aminette-dsouza/; /a/areej-nasser-muhajab/; /i/isabella-a-white/; /g/gabriel-wong/; /l/luis-espinosa-anke/; /l/leonardo-neves/; /f/francesco-barbieri/; /j/jose-camacho-collados/",
        "bibtex": "@inproceedings{loureiro-etal-2022-tempowic,\n    title = \"{T}empo{W}i{C}: An Evaluation Benchmark for Detecting Meaning Shift in Social Media\",\n    author = \"Loureiro, Daniel  and\n      D{'}Souza, Aminette  and\n      Muhajab, Areej Nasser  and\n      White, Isabella A.  and\n      Wong, Gabriel  and\n      Espinosa-Anke, Luis  and\n      Neves, Leonardo  and\n      Barbieri, Francesco  and\n      Camacho-Collados, Jose\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.296/\",\n    pages = \"3353--3359\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.296.pdf",
        "site": "https://aclanthology.org/2022.coling-1.296/",
        "pdf_size": 216032,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2735474647716371660&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK+AMPLYFI, UK; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK+AMPLYFI, UK; Snap Inc., Santa Monica, California, USA; Snap Inc., Santa Monica, California, USA; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK",
        "aff_domain": "cardiff.ac.uk; ; ; ; ; ; ; ;gmail.com",
        "email": "cardiff.ac.uk; ; ; ; ; ; ; ;gmail.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0+1;0;0;0;0+1;2;2;0",
        "aff_unique_norm": "Cardiff University;AMPLYFI;Snap Inc.",
        "aff_unique_dep": "School of Computer Science and Informatics;;",
        "aff_unique_url": "https://www.cardiff.ac.uk;https://www.amplyfi.com;https://www.snap.com",
        "aff_unique_abbr": "Cardiff;AMPLYFI;Snap",
        "aff_campus_unique_index": "0;0;0;0;0;0;2;2;0",
        "aff_campus_unique": "Cardiff;;Santa Monica",
        "aff_country_unique_index": "0;0+0;0;0;0;0+0;1;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.coling-1.416",
        "title": "Temporal Knowledge Graph Completion with Approximated Gaussian Process Embedding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Knowledge Graphs (KGs) stores world knowledge that benefits various reasoning-based applications. Due to their incompleteness, a fundamental task for KGs, which is known as Knowledge Graph Completion (KGC), is to perform link prediction and infer new facts based on the known facts. Recently, link prediction on the temporal KGs becomes an active research topic. Numerous Temporal Knowledge Graph Completion (TKGC) methods have been proposed by mapping the entities and relations in TKG to the high-dimensional representations. However, most existing TKGC methods are mainly based on deterministic vector embeddings, which are not flexible and expressive enough. In this paper, we propose a novel TKGC method, TKGC-AGP, by mapping the entities and relations in TKG to the approximations of multivariate Gaussian processes (MGPs). Equipped with the flexibility and capacity of MGP, the global trends as well as the local fluctuations in the TKGs can be simultaneously modeled. Moreover, the temporal uncertainties can be also captured with the kernel function and the covariance matrix of MGP. Moreover, a first-order Markov assumption-based training algorithm is proposed to effective optimize the proposed method. Experimental results show the effectiveness of the proposed approach on two real-world benchmark datasets compared with some state-of-the-art TKGC methods.",
        "author": "Linhai Zhang; Deyu Zhou",
        "authorids": "/l/linhai-zhang/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{zhang-zhou-2022-temporal,\n    title = \"Temporal Knowledge Graph Completion with Approximated {G}aussian Process Embedding\",\n    author = \"Zhang, Linhai  and\n      Zhou, Deyu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.416/\",\n    pages = \"4697--4706\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.416.pdf",
        "site": "https://aclanthology.org/2022.coling-1.416/",
        "pdf_size": 570372,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17744078146003759535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.307",
        "title": "TestAug: A Framework for Augmenting Capability-based NLP Tests",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The recently proposed capability-based NLP testing allows model developers to test the functional capabilities of NLP models, revealing functional failures for models with good held-out evaluation scores. However, existing work on capability-based testing requires the developer to compose each individual test template from scratch. Such approach thus requires extensive manual efforts and is less scalable. In this paper, we investigate a different approach that requires the developer to only annotate a few test templates, while leveraging the GPT-3 engine to generate the majority of test cases. While our approach saves the manual efforts by design, it guarantees the correctness of the generated suites with a validity checker. Moreover, our experimental results show that the test suites generated by GPT-3 are more diverse than the manually created ones; they can also be used to detect more errors compared to manually created counterparts. Our test suites can be downloaded at https://anonymous-researcher-nlp.github.io/testaug/.",
        "author": "Guanqun Yang; Mirazul Haque; Qiaochu Song; Wei Yang; Xueqing Liu",
        "authorids": "/g/guanqun-yang/; /m/mirazul-haque/; /q/qiaochu-song/; /w/wei-yang/; /x/xueqing-liu/",
        "bibtex": "@inproceedings{yang-etal-2022-testaug,\n    title = \"{T}est{A}ug: A Framework for Augmenting Capability-based {NLP} Tests\",\n    author = \"Yang, Guanqun  and\n      Haque, Mirazul  and\n      Song, Qiaochu  and\n      Yang, Wei  and\n      Liu, Xueqing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.307/\",\n    pages = \"3480--3495\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.307.pdf",
        "site": "https://aclanthology.org/2022.coling-1.307/",
        "pdf_size": 940560,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6154609743529244189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Stevens Institute of Technology; Department of Computer Science, The University of Texas at Dallas; Department of Computer Science, Stevens Institute of Technology; Department of Computer Science, The University of Texas at Dallas; Department of Computer Science, Stevens Institute of Technology",
        "aff_domain": "stevens.edu;utdallas.edu;stevens.com;utdallas.edu;stevens.edu",
        "email": "stevens.edu;utdallas.edu;stevens.com;utdallas.edu;stevens.edu",
        "github": "https://github.com/guanqun-yang/testaug",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Stevens Institute of Technology;The University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.stevens.edu;https://www.utdallas.edu",
        "aff_unique_abbr": "SIT;UT Dallas",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.359",
        "title": "Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous work has demonstrated that pre-trained large language models (LLM) acquire knowledge during pre-training which enables reasoning over relationships between words (e.g, hyponymy) and more complex inferences over larger units of meaning such as sentences. Here, we investigate whether lexical entailment (LE, i.e. hyponymy or the is a relation between words) can be generalised in a compositional manner. Accordingly, we introduce PLANE (Phrase-Level Adjective-Noun Entailment), a new benchmark to test models on fine-grained compositional entailment using adjective-noun phrases. Our experiments show that knowledge extracted via In\u2013Context and transfer learning is not enough to solve PLANE. However, a LLM trained on PLANE can generalise well to out\u2013of\u2013distribution sets, since the required knowledge can be stored in the representations of subwords (SW) tokens.",
        "author": "Lorenzo Bertolini; Julie Weeds; David Weir",
        "authorids": "/l/lorenzo-bertolini/; /j/julie-weeds/; /d/david-weir/",
        "bibtex": "@inproceedings{bertolini-etal-2022-testing,\n    title = \"Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment\",\n    author = \"Bertolini, Lorenzo  and\n      Weeds, Julie  and\n      Weir, David\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.359/\",\n    pages = \"4084--4100\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.359.pdf",
        "site": "https://aclanthology.org/2022.coling-1.359/",
        "pdf_size": 3213256,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15544552314782428048&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Sussex; University of Sussex; University of Sussex",
        "aff_domain": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk",
        "email": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Sussex",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sussex.ac.uk",
        "aff_unique_abbr": "Sussex",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.566",
        "title": "Text Simplification of College Admissions Instructions: A Professionally Simplified and Verified Corpus",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Access to higher education is critical for minority populations and emergent bilingual students. However, the language used by higher education institutions to communicate with prospective students is often too complex; concretely, many institutions in the US publish admissions application instructions far above the average reading level of a typical high school graduate, often near the 13th or 14th grade level. This leads to an unnecessary barrier between students and access to higher education. This work aims to tackle this challenge via text simplification. We present PSAT (Professionally Simplified Admissions Texts), a dataset with 112 admissions instructions randomly selected from higher education institutions across the US. These texts are then professionally simplified, and verified and accepted by subject-matter experts who are full-time employees in admissions offices at various institutions. Additionally, PSAT comes with manual alignments of 1,883 original-simplified sentence pairs. The result is a first-of-its-kind corpus for the evaluation and fine-tuning of text simplification systems in a high-stakes genre distinct from existing simplification resources.",
        "author": "Zachary W. Taylor; Maximus H. Chu; Junyi Jessy Li",
        "authorids": "/z/zachary-w-taylor/; /m/maximus-h-chu/; /j/junyi-jessy-li/",
        "bibtex": "@inproceedings{taylor-etal-2022-text,\n    title = \"Text Simplification of College Admissions Instructions: A Professionally Simplified and Verified Corpus\",\n    author = \"Taylor, Zachary W.  and\n      Chu, Maximus H.  and\n      Li, Junyi Jessy\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.566/\",\n    pages = \"6505--6515\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.566.pdf",
        "site": "https://aclanthology.org/2022.coling-1.566/",
        "pdf_size": 316049,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14767423612211487490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "College of Education and Human Sciences, University of Southern Mississippi; Department of Computer Science, The University of Texas at Austin; Department of Linguistics, The University of Texas at Austin",
        "aff_domain": "usm.edu;utexas.edu;utexas.edu",
        "email": "usm.edu;utexas.edu;utexas.edu",
        "github": "",
        "project": "https://doi.org/10.5281/zenodo.7055024",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Southern Mississippi;The University of Texas at Austin",
        "aff_unique_dep": "College of Education and Human Sciences;Department of Computer Science",
        "aff_unique_url": "https://www.usm.edu;https://www.utexas.edu",
        "aff_unique_abbr": "USM;UT Austin",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.238",
        "title": "Text-to-Text Extraction and Verbalization of Biomedical Event Graphs",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data.",
        "author": "Giacomo Frisoni; Gianluca Moro; Lorenzo Balzani",
        "authorids": "/g/giacomo-frisoni/; /g/gianluca-moro/; /l/lorenzo-balzani/",
        "bibtex": "@inproceedings{frisoni-etal-2022-text,\n    title = \"Text-to-Text Extraction and Verbalization of Biomedical Event Graphs\",\n    author = \"Frisoni, Giacomo  and\n      Moro, Gianluca  and\n      Balzani, Lorenzo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.238/\",\n    pages = \"2692--2710\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.238.pdf",
        "site": "https://aclanthology.org/2022.coling-1.238/",
        "pdf_size": 982893,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16058617773837068081&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Engineering (DISI), University of Bologna; Department of Computer Science and Engineering (DISI), University of Bologna; Department of Computer Science and Engineering (DISI), University of Bologna",
        "aff_domain": "unibo.it;unibo.it;icloud.com",
        "email": "unibo.it;unibo.it;icloud.com",
        "github": "https://github.com/disi-unibo-nlp/bio-ee-egv",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Bologna",
        "aff_unique_dep": "Department of Computer Science and Engineering (DISI)",
        "aff_unique_url": "https://www.unibo.it",
        "aff_unique_abbr": "UNIBO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.coling-1.475",
        "title": "The Fragility of Multi-Treebank Parsing Evaluation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Treebank selection for parsing evaluation and the spurious effects that might arise from a biased choice have not been explored in detail. This paper studies how evaluating on a single subset of treebanks can lead to weak conclusions. First, we take a few contrasting parsers, and run them on subsets of treebanks proposed in previous work, whose use was justified (or not) on criteria such as typology or data scarcity. Second, we run a large-scale version of this experiment, create vast amounts of random subsets of treebanks, and compare on them many parsers whose scores are available. The results show substantial variability across subsets and that although establishing guidelines for good treebank selection is hard, some inadequate strategies can be easily avoided.",
        "author": "Iago Alonso-Alonso; David Vilares; Carlos G\u00f3mez-Rodr\u00edguez",
        "authorids": "/i/iago-alonso-alonso/; /d/david-vilares/; /c/carlos-gomez-rodriguez/",
        "bibtex": "@inproceedings{alonso-alonso-etal-2022-fragility,\n    title = \"The Fragility of Multi-Treebank Parsing Evaluation\",\n    author = \"Alonso-Alonso, Iago  and\n      Vilares, David  and\n      G{\\'o}mez-Rodr{\\'i}guez, Carlos\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.475/\",\n    pages = \"5345--5359\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.475.pdf",
        "site": "https://aclanthology.org/2022.coling-1.475/",
        "pdf_size": 520749,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15669981498904580488&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Universidade da Coru\u00f1a, CITIC; Universidade da Coru\u00f1a, CITIC; Universidade da Coru\u00f1a, CITIC",
        "aff_domain": "udc.es;udc.es;udc.es",
        "email": "udc.es;udc.es;udc.es",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universidade da Coru\u00f1a",
        "aff_unique_dep": "CITIC",
        "aff_unique_url": "https://www.udc.es",
        "aff_unique_abbr": "UDC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2022.coling-1.67",
        "title": "The Role of Context and Uncertainty in Shallow Discourse Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Discourse parsing has proven to be useful for a number of NLP tasks that require complex reasoning. However, over a decade since the advent of the Penn Discourse Treebank, predicting implicit discourse relations in text remains challenging. There are several possible reasons for this, and we hypothesize that models should be exposed to more context as it plays an important role in accurate human annotation; meanwhile adding uncertainty measures can improve model accuracy and calibration. To thoroughly investigate this phenomenon, we perform a series of experiments to determine 1) the effects of context on human judgments, and 2) the effect of quantifying uncertainty with annotator confidence ratings on model accuracy and calibration (which we measure using the Brier score (Brier et al, 1950)). We find that including annotator accuracy and confidence improves model accuracy, and incorporating confidence in the model\u2019s temperature function can lead to models with significantly better-calibrated confidence measures. We also find some insightful qualitative results regarding human and model behavior on these datasets.",
        "author": "Katherine Atwell; Remi Choi; Junyi Jessy Li; Malihe Alikhani",
        "authorids": "/k/katherine-atwell/; /r/remi-choi/; /j/junyi-jessy-li/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{atwell-etal-2022-role,\n    title = \"The Role of Context and Uncertainty in Shallow Discourse Parsing\",\n    author = \"Atwell, Katherine  and\n      Choi, Remi  and\n      Li, Junyi Jessy  and\n      Alikhani, Malihe\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.67/\",\n    pages = \"797--811\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.67.pdf",
        "site": "https://aclanthology.org/2022.coling-1.67/",
        "pdf_size": 1064882,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mVuQ02IG3KQJ:scholar.google.com/&scioq=The+Role+of+Context+and+Uncertainty+in+Shallow+Discourse+Parsing&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Pittsburgh; Department of Computer Science, University of Pittsburgh; Department of Linguistics, The University of Texas at Austin; Department of Computer Science, University of Pittsburgh",
        "aff_domain": "pitt.edu;pitt.edu;utexas.edu;pitt.edu",
        "email": "pitt.edu;pitt.edu;utexas.edu;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Pittsburgh;The University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science;Department of Linguistics",
        "aff_unique_url": "https://www.pitt.edu;https://www.utexas.edu",
        "aff_unique_abbr": "Pitt;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.106",
        "title": "Threat Scenarios and Best Practices to Detect Neural Fake News",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this work, we discuss different threat scenarios from neural fake news generated by state-of-the-art language models. Through our experiments, we assess the performance of generated text detection systems under these threat scenarios. For each scenario, we also identify the minimax strategy for the detector that minimizes its worst-case performance. This constitutes a set of best practices that practitioners can rely on. In our analysis, we find that detectors are prone to shortcut learning (lack of out-of-distribution generalization) and discuss approaches to mitigate this problem and improve detectors more broadly. Finally, we argue that strong detectors should be released along with new generators.",
        "author": "Artidoro Pagnoni; Martin Graciarena; Yulia Tsvetkov",
        "authorids": "/a/artidoro-pagnoni/; /m/martin-graciarena/; /y/yulia-tsvetkov/",
        "bibtex": "@inproceedings{pagnoni-etal-2022-threat,\n    title = \"Threat Scenarios and Best Practices to Detect Neural Fake News\",\n    author = \"Pagnoni, Artidoro  and\n      Graciarena, Martin  and\n      Tsvetkov, Yulia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.106/\",\n    pages = \"1233--1249\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.106.pdf",
        "site": "https://aclanthology.org/2022.coling-1.106/",
        "pdf_size": 538088,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9509054601499312543&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Paul G. Allen School of CSE, University of Washington; SRI International; Paul G. Allen School of CSE, University of Washington",
        "aff_domain": "cs.washington.edu;sri.com;cs.washington.edu",
        "email": "cs.washington.edu;sri.com;cs.washington.edu",
        "github": "https://github.com/artidoro/detect-gentext",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;SRI International",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.cs.washington.edu;https://www.sri.com",
        "aff_unique_abbr": "UW CSE;SRI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.147",
        "title": "To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning.",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Reasoning and knowledge-related skills are considered as two fundamental skills for natural language understanding (NLU) tasks such as machine reading comprehension (MRC) and natural language inference (NLI). However, it is not clear to what extent an NLU task defined on a dataset correlates to a specific NLU skill. On the one hand, evaluating the correlation requires an understanding of the significance of the NLU skill in a dataset. Significance judges whether a dataset includes sufficient material to help the model master this skill. On the other hand, it is also necessary to evaluate the dependence of the task on the NLU skill. Dependence is a measure of how much the task defined on a dataset depends on the skill. In this paper, we propose a systematic method to diagnose the correlations between an NLU dataset and a specific skill, and then take a fundamental reasoning skill, logical reasoning, as an example for analysis. The method adopts a qualitative indicator to indicate the significance while adopting a quantitative indicator to measure the dependence. We perform diagnosis on 8 MRC datasets (including two types) and 3 NLI datasets and acquire intuitively reasonable results. We then perform the analysis to further understand the results and the proposed indicators. Based on the analysis, although the diagnostic method has some limitations, it is still an effective method to perform a basic diagnosis of the correlation between the dataset and logical reasoning skill, which also can be generalized to other NLU skills.",
        "author": "Yitian Li; Jidong Tian; Wenqing Chen; Caoyun Fan; Hao He; Yaohui Jin",
        "authorids": "/y/yitian-li/; /j/jidong-tian/; /w/wenqing-chen/; /c/caoyun-fan/; /h/hao-he/; /y/yaohui-jin/",
        "bibtex": "@inproceedings{li-etal-2022-extent,\n    title = \"To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning.\",\n    author = \"Li, Yitian  and\n      Tian, Jidong  and\n      Chen, Wenqing  and\n      Fan, Caoyun  and\n      He, Hao  and\n      Jin, Yaohui\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.147/\",\n    pages = \"1708--1717\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.147.pdf",
        "site": "https://aclanthology.org/2022.coling-1.147/",
        "pdf_size": 539415,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3268116480343650269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University+State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University+State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University+State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University+State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University+State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University+State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "AI Institute",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.404",
        "title": "Token and Head Adaptive Transformers for Efficient Natural Language Processing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "While pre-trained language models like BERT have achieved impressive results on various natural language processing tasks, deploying them on resource-restricted devices is challenging due to their intensive computational cost and memory footprint. Previous approaches mainly focused on training smaller versions of a BERT model with competitive accuracy under limited computational resources. In this paper, we extend Length Adaptive Transformer and propose to design Token and Head Adaptive Transformer, which can compress and accelerate various BERT-based models via simple fine-tuning. We train a transformer with a progressive token and head pruning scheme, eliminating a large number of redundant tokens and attention heads in the later layers. Then, we conduct a multi-objective evolutionary search with the overall number of floating point operations (FLOPs) as its efficiency constraint to find joint token and head pruning strategies that maximize accuracy and efficiency under various computational budgets. Empirical studies show that a large portion of tokens and attention heads could be pruned while achieving superior performance compared to the baseline BERT-based models and Length Adaptive Transformers in various downstream NLP tasks. MobileBERT trained with our joint token and head pruning scheme achieves a GLUE score of 83.0, which is 1.4 higher than Length Adaptive Transformer and 2.9 higher than the original model.",
        "author": "Chonghan Lee; Md Fahim Faysal Khan; Rita Brugarolas Brufau; Ke Ding; Vijaykrishnan Narayanan",
        "authorids": "/c/chonghan-lee/; /m/md-fahim-faysal-khan/; /r/rita-brugarolas-brufau/; /k/ke-ding/; /v/vijaykrishnan-narayanan/",
        "bibtex": "@inproceedings{lee-etal-2022-token,\n    title = \"Token and Head Adaptive Transformers for Efficient Natural Language Processing\",\n    author = \"Lee, Chonghan  and\n      Khan, Md Fahim Faysal  and\n      Brufau, Rita Brugarolas  and\n      Ding, Ke  and\n      Narayanan, Vijaykrishnan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.404/\",\n    pages = \"4575--4584\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.404.pdf",
        "site": "https://aclanthology.org/2022.coling-1.404/",
        "pdf_size": 372488,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4780862955377929695&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "The Pennsylvania State University; The Pennsylvania State University; Intel; Intel; The Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu;intel.com;intel.com;psu.edu",
        "email": "psu.edu;psu.edu;intel.com;intel.com;psu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "The Pennsylvania State University;Intel Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.psu.edu;https://www.intel.com",
        "aff_unique_abbr": "PSU;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.62",
        "title": "TopKG: Target-oriented Dialog via Global Planning on Knowledge Graph",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Target-oriented dialog aims to reach a global target through multi-turn conversation. The key to the task is the global planning towards the target, which flexibly guides the dialog concerning the context. However, existing target-oriented dialog works take a local and greedy strategy for response generation, where global planning is absent. In this work, we propose global planning for target-oriented dialog on a commonsense knowledge graph (KG). We design a global reinforcement learning with the planned paths to flexibly adjust the local response generation model towards the global target. We also propose a KG-based method to collect target-oriented samples automatically from the chit-chat corpus for model training. Experiments show that our method can reach the target with a higher success rate, fewer turns, and more coherent responses.",
        "author": "Zhitong Yang; Bo Wang; Jinfeng Zhou; Yue Tan; Dongming Zhao; Kun Huang; Ruifang He; Yuexian Hou",
        "authorids": "/z/zhitong-yang/; /b/bo-wang/; /j/jinfeng-zhou/; /y/yue-tan/; /d/dongming-zhao/; /k/kun-huang/; /r/ruifang-he/; /y/yuexian-hou/",
        "bibtex": "@inproceedings{yang-etal-2022-topkg,\n    title = \"{T}op{KG}: Target-oriented Dialog via Global Planning on Knowledge Graph\",\n    author = \"Yang, Zhitong  and\n      Wang, Bo  and\n      Zhou, Jinfeng  and\n      Tan, Yue  and\n      Zhao, Dongming  and\n      Huang, Kun  and\n      He, Ruifang  and\n      Hou, Yuexian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.62/\",\n    pages = \"745--755\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.62.pdf",
        "site": "https://aclanthology.org/2022.coling-1.62/",
        "pdf_size": 657280,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4373458124327690518&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2022.coling-1.71",
        "title": "Topicalization in Language Models: A Case Study on Japanese",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Humans use different wordings depending on the context to facilitate efficient communication. For example, instead of completely new information, information related to the preceding context is typically placed at the sentence-initial position. In this study, we analyze whether neural language models (LMs) can capture such discourse-level preferences in text generation. Specifically, we focus on a particular aspect of discourse, namely the topic-comment structure. To analyze the linguistic knowledge of LMs separately, we chose the Japanese language, a topic-prominent language, for designing probing tasks, and we created human topicalization judgment data by crowdsourcing. Our experimental results suggest that LMs have different generalizations from humans; LMs exhibited less context-dependent behaviors toward topicalization judgment. These results highlight the need for the additional inductive biases to guide LMs to achieve successful discourse-level generalization.",
        "author": "Riki Fujihara; Tatsuki Kuribayashi; Kaori Abe; Ryoko Tokuhisa; Kentaro Inui",
        "authorids": "/r/riki-fujihara/; /t/tatsuki-kuribayashi/; /k/kaori-abe/; /r/ryoko-tokuhisa/; /k/kentaro-inui/",
        "bibtex": "@inproceedings{fujihara-etal-2022-topicalization,\n    title = \"Topicalization in Language Models: A Case Study on {J}apanese\",\n    author = \"Fujihara, Riki  and\n      Kuribayashi, Tatsuki  and\n      Abe, Kaori  and\n      Tokuhisa, Ryoko  and\n      Inui, Kentaro\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.71/\",\n    pages = \"851--862\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.71.pdf",
        "site": "https://aclanthology.org/2022.coling-1.71/",
        "pdf_size": 464500,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10496049479728500628&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Tohoku University; Tohoku University+Langsmith Inc.; Tohoku University; Tohoku University; Tohoku University+RIKEN",
        "aff_domain": "dc.tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp",
        "email": "dc.tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0+2",
        "aff_unique_norm": "Tohoku University;Langsmith Inc.;RIKEN",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tohoku.ac.jp;;https://www.riken.jp",
        "aff_unique_abbr": "Tohoku U;;RIKEN",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0+0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2022.coling-1.415",
        "title": "Topology Imbalance and Relation Inauthenticity Aware Hierarchical Graph Attention Networks for Fake News Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Fake news detection is a challenging problem due to its tremendous real-world political and social impacts. Recent fake news detection works focus on learning news features from News Propagation Graph (NPG). However, little attention is paid to the issues of both authenticity of the relationships and topology imbalance in the structure of NPG, which trick existing methods and thus lead to incorrect prediction results. To tackle these issues, in this paper, we propose a novel Topology imbalance and Relation inauthenticity aware Hierarchical Graph Attention Networks (TR-HGAN) to identify fake news on social media. Specifically, we design a new topology imbalance smoothing strategy to measure the topology weight of each node. Besides, we adopt a hierarchical-level attention mechanism for graph convolutional learning, which can adaptively identify the authenticity of relationships by assigning appropriate weights to each of them. Experiments on real-world datasets demonstrate that TR-HGAN significantly outperforms state-of-the-art methods.",
        "author": "Li Gao; Lingyun Song; Jie Liu; Bolin Chen; Xuequn Shang",
        "authorids": "/l/li-gao/; /l/lingyun-song/; /j/jie-liu/; /b/bolin-chen/; /x/xuequn-shang/",
        "bibtex": "@inproceedings{gao-etal-2022-topology,\n    title = \"Topology Imbalance and Relation Inauthenticity Aware Hierarchical Graph Attention Networks for Fake News Detection\",\n    author = \"Gao, Li  and\n      Song, Lingyun  and\n      Liu, Jie  and\n      Chen, Bolin  and\n      Shang, Xuequn\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.415/\",\n    pages = \"4687--4696\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.415.pdf",
        "site": "https://aclanthology.org/2022.coling-1.415/",
        "pdf_size": 1261683,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11599416999779972919&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China+Key Laboratory of Big Data Storage and Management, Ministry of Industry and Information Technology, Xi\u2019an, China; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China+Key Laboratory of Big Data Storage and Management, Ministry of Industry and Information Technology, Xi\u2019an, China; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China+Key Laboratory of Big Data Storage and Management, Ministry of Industry and Information Technology, Xi\u2019an, China; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China+Key Laboratory of Big Data Storage and Management, Ministry of Industry and Information Technology, Xi\u2019an, China",
        "aff_domain": "nwpu.edu.com;nwpu.edu.com; ; ; ",
        "email": "nwpu.edu.com;nwpu.edu.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;0+1;0+1",
        "aff_unique_norm": "Northwestern Polytechnical University;Ministry of Industry and Information Technology",
        "aff_unique_dep": "School of Computer Science;Key Laboratory of Big Data Storage and Management",
        "aff_unique_url": "https://www.nwpu.edu.cn;",
        "aff_unique_abbr": "NPU;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.497",
        "title": "Towards Better Semantic Understanding of Mobile Interfaces",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Improving the accessibility and automation capabilities of mobile devices can have a significant positive impact on the daily lives of countless users. To stimulate research in this direction, we release a human-annotated dataset with approximately 500k unique annotations aimed at increasing the understanding of the functionality of UI elements. This dataset augments images and view hierarchies from RICO, a large dataset of mobile UIs, with annotations for icons based on their shapes and semantics, and associations between different elements and their corresponding text labels, resulting in a significant increase in the number of UI elements and the categories assigned to them. We also release models using image-only and multimodal inputs; we experiment with various architectures and study the benefits of using multimodal inputs on the new dataset. Our models demonstrate strong performance on an evaluation set of unseen apps, indicating their generalizability to newer screens. These models, combined with the new dataset, can enable innovative functionalities like referring to UI elements by their labels, improved coverage and better semantics for icons etc., which would go a long way in making UIs more usable for everyone.",
        "author": "Srinivas Sunkara; Maria Wang; Lijuan Liu; Gilles Baechler; Yu-Chung Hsiao; Jindong Chen; Abhanshu Sharma; James W. W. Stout",
        "authorids": "/s/srinivas-sunkara/; /m/maria-wang/; /l/lijuan-liu/; /g/gilles-baechler/; /y/yu-chung-hsiao/; /j/jindong-chen/; /a/abhanshu-sharma/; /j/james-w-w-stout/",
        "bibtex": "@inproceedings{sunkara-etal-2022-towards,\n    title = \"Towards Better Semantic Understanding of Mobile Interfaces\",\n    author = \"Sunkara, Srinivas  and\n      Wang, Maria  and\n      Liu, Lijuan  and\n      Baechler, Gilles  and\n      Hsiao, Yu-Chung  and\n      Chen, Jindong  and\n      Sharma, Abhanshu  and\n      Stout, James W. W.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.497/\",\n    pages = \"5636--5650\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.497.pdf",
        "site": "https://aclanthology.org/2022.coling-1.497/",
        "pdf_size": 2390743,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8529832378662029068&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google",
        "aff_domain": "google.com;google.com; ; ; ; ; ; ",
        "email": "google.com;google.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.58",
        "title": "Towards Enhancing Health Coaching Dialogue in Low-Resource Settings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Health coaching helps patients identify and accomplish lifestyle-related goals, effectively improving the control of chronic diseases and mitigating mental health conditions. However, health coaching is cost-prohibitive due to its highly personalized and labor-intensive nature. In this paper, we propose to build a dialogue system that converses with the patients, helps them create and accomplish specific goals, and can address their emotions with empathy. However, building such a system is challenging since real-world health coaching datasets are limited and empathy is subtle. Thus, we propose a modularized health coaching dialogue with simplified NLU and NLG frameworks combined with mechanism-conditioned empathetic response generation. Through automatic and human evaluation, we show that our system generates more empathetic, fluent, and coherent responses and outperforms the state-of-the-art in NLU tasks while requiring less annotation. We view our approach as a key step towards building automated and more accessible health coaching systems.",
        "author": "Yue Zhou; Barbara Di Eugenio; Brian Ziebart; Lisa Sharp; Bing Liu; Ben Gerber; Nikolaos Agadakos; Shweta Yadav",
        "authorids": "/y/yue-zhou/; /b/barbara-di-eugenio/; /b/brian-ziebart/; /l/lisa-sharp/; /b/bing-liu/; /b/ben-gerber/; /n/nikolaos-agadakos/; /s/shweta-yadav/",
        "bibtex": "@inproceedings{zhou-etal-2022-towards,\n    title = \"Towards Enhancing Health Coaching Dialogue in Low-Resource Settings\",\n    author = \"Zhou, Yue  and\n      Di Eugenio, Barbara  and\n      Ziebart, Brian  and\n      Sharp, Lisa  and\n      Liu, Bing  and\n      Gerber, Ben  and\n      Agadakos, Nikolaos  and\n      Yadav, Shweta\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.58/\",\n    pages = \"694--706\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.58.pdf",
        "site": "https://aclanthology.org/2022.coling-1.58/",
        "pdf_size": 469478,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3173713663446532605&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois at Chicago, IL, USA; University of Illinois at Chicago, IL, USA; University of Illinois at Chicago, IL, USA; University of Illinois at Chicago, IL, USA; University of Illinois at Chicago, IL, USA; UMass Chan Medical School, MA, USA; University of Illinois at Chicago, IL, USA; University of Illinois at Chicago, IL, USA",
        "aff_domain": "uic.edu;uic.edu;uic.edu;uic.edu;uic.edu;umassmed.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu;uic.edu;uic.edu;umassmed.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;1;0;0",
        "aff_unique_norm": "University of Illinois at Chicago;UMass Chan Medical School",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uic.edu;https://www.umassmed.edu/",
        "aff_unique_abbr": "UIC;UMass Chan",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.321",
        "title": "Towards Explainable Evaluation of Language Models on the Semantic Similarity of Visual Concepts",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent breakthroughs in NLP research, such as the advent of Transformer models have indisputably contributed to major advancements in several tasks. However, few works research robustness and explainability issues of their evaluation strategies. In this work, we examine the behavior of high-performing pre-trained language models, focusing on the task of semantic similarity for visual vocabularies. First, we address the need for explainable evaluation metrics, necessary for understanding the conceptual quality of retrieved instances. Our proposed metrics provide valuable insights in local and global level, showcasing the inabilities of widely used approaches. Secondly, adversarial interventions on salient query semantics expose vulnerabilities of opaque metrics and highlight patterns in learned linguistic representations.",
        "author": "Maria Lymperaiou; George Manoliadis; Orfeas Menis Mastromichalakis; Edmund G. Dervakos; Giorgos Stamou",
        "authorids": "/m/maria-lymperaiou/; /g/george-manoliadis/; /o/orfeas-menis-mastromichalakis/; /e/edmund-g-dervakos/; /g/giorgos-stamou/",
        "bibtex": "@inproceedings{lymperaiou-etal-2022-towards,\n    title = \"Towards Explainable Evaluation of Language Models on the Semantic Similarity of Visual Concepts\",\n    author = \"Lymperaiou, Maria  and\n      Manoliadis, George  and\n      Menis Mastromichalakis, Orfeas  and\n      Dervakos, Edmund G.  and\n      Stamou, Giorgos\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.321/\",\n    pages = \"3639--3658\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.321.pdf",
        "site": "https://aclanthology.org/2022.coling-1.321/",
        "pdf_size": 8833675,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14540214059409157156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Artificial Intelligence and Learning Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of Athens; Artificial Intelligence and Learning Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of Athens; Artificial Intelligence and Learning Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of Athens; Artificial Intelligence and Learning Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of Athens; Artificial Intelligence and Learning Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of Athens",
        "aff_domain": "islab.ntua.gr;islab.ntua.gr;mail.ntua.gr;ails.ece.ntua.gr;cs.ntua.gr",
        "email": "islab.ntua.gr;islab.ntua.gr;mail.ntua.gr;ails.ece.ntua.gr;cs.ntua.gr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National Technical University of Athens",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ntua.gr",
        "aff_unique_abbr": "NTUA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Greece"
    },
    {
        "id": "2022.coling-1.591",
        "title": "Towards Exploiting Sticker for Multimodal Sentiment Analysis in Social Media: A New Dataset and Baseline",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Sentiment analysis in social media is challenging since posts are short of context. As a popular way to express emotion on social media, stickers related to these posts can supplement missing sentiments and help identify sentiments precisely. However, research about stickers has not been investigated further. To this end, we present a Chinese sticker-based multimodal dataset for the sentiment analysis task (CSMSA). Compared with previous real-world photo-based multimodal datasets, the CSMSA dataset focuses on stickers, conveying more vivid and moving emotions. The sticker-based multimodal sentiment analysis task is challenging in three aspects: inherent multimodality of stickers, significant inter-series variations between stickers, and complex multimodal sentiment fusion. We propose SAMSAM to address the above three challenges. Our model introduces a flexible masked self-attention mechanism to allow the dynamic interaction between post texts and stickers. The experimental results indicate that our model performs best compared with other models. More researches need to be devoted to this field. The dataset is publicly available at https://github.com/Logos23333/CSMSA.",
        "author": "Feng Ge; Weizhao Li; Haopeng Ren; Yi Cai",
        "authorids": "/f/feng-ge/; /w/weizhao-li/; /h/haopeng-ren/; /y/yi-cai/",
        "bibtex": "@inproceedings{ge-etal-2022-towards,\n    title = \"Towards Exploiting Sticker for Multimodal Sentiment Analysis in Social Media: A New Dataset and Baseline\",\n    author = \"Ge, Feng  and\n      Li, Weizhao  and\n      Ren, Haopeng  and\n      Cai, Yi\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.591/\",\n    pages = \"6795--6804\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.591.pdf",
        "site": "https://aclanthology.org/2022.coling-1.591/",
        "pdf_size": 2176982,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14858809138180793360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "South China University of Technology; South China University of Technology; South China University of Technology; South China University of Technology",
        "aff_domain": "foxmail.com;mail.scut.edu.cn;foxmail.com;scut.edu.cn",
        "email": "foxmail.com;mail.scut.edu.cn;foxmail.com;scut.edu.cn",
        "github": "https://github.com/Logos23333/CSMSA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "South China University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.scut.edu.cn",
        "aff_unique_abbr": "SCUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.70",
        "title": "Towards Identifying Alternative-Lexicalization Signals of Discourse Relations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of shallow discourse parsing in the Penn Discourse Treebank (PDTB) framework has traditionally been restricted to identifying those relations that are signaled by a discourse connective (\u201cexplicit\u201d) and those that have no signal at all (\u201cimplicit\u201d). The third type, the more flexible group of \u201cAltLex\u201d realizations has been neglected because of its small amount of occurrences in the PDTB2 corpus. Their number has grown significantly in the recent PDTB3, and in this paper, we present the first approaches for recognizing these \u201calternative lexicalizations\u201d. We compare the performance of a pattern-based approach and a sequence labeling model, add an experiment on the pre-classification of candidate sentences, and provide an initial qualitative analysis of the error cases made by both models.",
        "author": "Ren\u00e9 Knaebel; Manfred Stede",
        "authorids": "/r/rene-knaebel/; /m/manfred-stede/",
        "bibtex": "@inproceedings{knaebel-stede-2022-towards,\n    title = \"Towards Identifying Alternative-Lexicalization Signals of Discourse Relations\",\n    author = \"Knaebel, Ren{\\'e}  and\n      Stede, Manfred\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.70/\",\n    pages = \"837--850\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.70.pdf",
        "site": "https://aclanthology.org/2022.coling-1.70/",
        "pdf_size": 614090,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10690432543010548243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Applied Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Applied Computational Linguistics, Department of Linguistics, University of Potsdam, Germany",
        "aff_domain": "uni-potsdam.de;uni-potsdam.de",
        "email": "uni-potsdam.de;uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.386",
        "title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively.",
        "author": "Linlin Liu; Thien Hai Nguyen; Shafiq Joty; Lidong Bing; Luo Si",
        "authorids": "/l/linlin-liu/; /t/thien-hai-nguyen/; /s/shafiq-joty/; /l/lidong-bing/; /l/luo-si/",
        "bibtex": "@inproceedings{liu-etal-2022-towards-multi,\n    title = \"Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings\",\n    author = \"Liu, Linlin  and\n      Nguyen, Thien Hai  and\n      Joty, Shafiq  and\n      Bing, Lidong  and\n      Si, Luo\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.386/\",\n    pages = \"4381--4396\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.386.pdf",
        "site": "https://aclanthology.org/2022.coling-1.386/",
        "pdf_size": 2274889,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1521819983980060256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "DAMO Academy, Alibaba Group+Joint PhD Program between Alibaba and Nanyang Technological University; DAMO Academy, Alibaba Group; Nanyang Technological University, Singapore; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;ntu.edu.sg;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;ntu.edu.sg;alibaba-inc.com;alibaba-inc.com",
        "github": "https://github.com/ntunlp/multisense_embedding_alignment.git",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;1;0;0",
        "aff_unique_norm": "Alibaba Group;Nanyang Technological University",
        "aff_unique_dep": "DAMO Academy;Joint PhD Program",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.ntu.edu.sg",
        "aff_unique_abbr": "Alibaba;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.coling-1.52",
        "title": "Towards Multi-label Unknown Intent Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-class unknown intent detection has made remarkable progress recently. However, it has a strong assumption that each utterance has only one intent, which does not conform to reality because utterances often have multiple intents. In this paper, we propose a more desirable task, multi-label unknown intent detection, to detect whether the utterance contains the unknown intent, in which each utterance may contain multiple intents. In this task, the unique utterances simultaneously containing known and unknown intents make existing multi-class methods easy to fail. To address this issue, we propose an intuitive and effective method to recognize whether All Intents contained in the utterance are Known (AIK). Our high-level idea is to predict the utterance\u2019s intent number, then check whether the utterance contains the same number of known intents. If the number of known intents is less than the number of intents, it implies that the utterance also contains unknown intents. We benchmark AIK over existing methods, and empirical results suggest that our method obtains state-of-the-art performances. For example, on the MultiWOZ 2.3 dataset, AIK significantly reduces the FPR95 by 12.25% compared to the best baseline.",
        "author": "Yawen Ouyang; Zhen Wu; Xinyu Dai; Shujian Huang; Jiajun Chen",
        "authorids": "/y/yawen-ouyang/; /z/zhen-wu/; /x/xinyu-dai/; /s/shujian-huang/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{ouyang-etal-2022-towards,\n    title = \"Towards Multi-label Unknown Intent Detection\",\n    author = \"Ouyang, Yawen  and\n      Wu, Zhen  and\n      Dai, Xinyu  and\n      Huang, Shujian  and\n      Chen, Jiajun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.52/\",\n    pages = \"626--635\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.52.pdf",
        "site": "https://aclanthology.org/2022.coling-1.52/",
        "pdf_size": 659582,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1035459264745025248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China",
        "aff_domain": "smail.nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/yawenouyang/AIK",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.468",
        "title": "Towards Robust Neural Machine Translation with Iterative Scheduled Data-Switch Training",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most existing methods on robust neural machine translation (NMT) construct adversarial examples by injecting noise into authentic examples and indiscriminately exploit two types of examples. They require the model to translate both the authentic source sentence and its adversarial counterpart into the identical target sentence within the same training stage, which may be a suboptimal choice to achieve robust NMT. In this paper, we first conduct a preliminary study to confirm this claim and further propose an Iterative Scheduled Data-switch Training Framework to mitigate this problem. Specifically, we introduce two training stages, iteratively switching between authentic and adversarial examples. Compared with previous studies, our model focuses more on just one type of examples at each single stage, which can better exploit authentic and adversarial examples, and thus obtaining a better robust NMT model. Moreover, we introduce an improved curriculum learning method with a sampling strategy to better schedule the process of noise injection. Experimental results show that our model significantly surpasses several competitive baselines on four translation benchmarks. Our source code is available at https://github.com/DeepLearnXMU/RobustNMT-ISDST.",
        "author": "Zhongjian Miao; Xiang Li; Liyan Kang; Wen Zhang; Chulun Zhou; Yidong Chen; Bin Wang; Min Zhang; Jinsong Su",
        "authorids": "/z/zhongjian-miao/; /x/xiang-li/; /l/liyan-kang/; /w/wen-zhang/; /c/chulun-zhou/; /y/yidong-chen/; /b/bin-wang/; /m/min-zhang/; /j/jinsong-su/",
        "bibtex": "@inproceedings{miao-etal-2022-towards,\n    title = \"Towards Robust Neural Machine Translation with Iterative Scheduled Data-Switch Training\",\n    author = \"Miao, Zhongjian  and\n      Li, Xiang  and\n      Kang, Liyan  and\n      Zhang, Wen  and\n      Zhou, Chulun  and\n      Chen, Yidong  and\n      Wang, Bin  and\n      Zhang, Min  and\n      Su, Jinsong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.468/\",\n    pages = \"5266--5277\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.468.pdf",
        "site": "https://aclanthology.org/2022.coling-1.468/",
        "pdf_size": 5027212,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12554746041363566632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Informatics, Xiamen University, China; Xiaomi AI Lab, China; School of Informatics, Xiamen University, China; Xiaomi AI Lab, China; School of Informatics, Xiamen University, China; School of Informatics, Xiamen University, China; Xiaomi AI Lab, China; Harbin Institute of Technology, Shenzhen, China; School of Informatics, Xiamen University, China + Pengcheng Lab, China",
        "aff_domain": "stu.xmu.edu.cn;xiaomi.com; ; ; ;xmu.edu.cn; ; ;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;xiaomi.com; ; ; ;xmu.edu.cn; ; ;xmu.edu.cn",
        "github": "https://github.com/DeepLearnXMU/RobustNMT-ISDST",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;0;1;0;0;1;2;0+3",
        "aff_unique_norm": "Xiamen University;Xiaomi AI Lab;Harbin Institute of Technology;Pengcheng Lab",
        "aff_unique_dep": "School of Informatics;AI Lab;;",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.xiaomi.com;http://en.hhit.edu.cn/;",
        "aff_unique_abbr": "XMU;Xiaomi AI Lab;HIT;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.89",
        "title": "Towards Robust Neural Retrieval with Source Domain Synthetic Pre-Finetuning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Research on neural IR has so far been focused primarily on standard supervised learning settings, where it outperforms traditional term matching baselines. Many practical use cases of such models, however, may involve previously unseen target domains. In this paper, we propose to improve the out-of-domain generalization of Dense Passage Retrieval (DPR) - a popular choice for neural IR - through synthetic data augmentation only in the source domain. We empirically show that pre-finetuning DPR with additional synthetic data in its source domain (Wikipedia), which we generate using a fine-tuned sequence-to-sequence generator, can be a low-cost yet effective first step towards its generalization. Across five different test sets, our augmented model shows more robust performance than DPR in both in-domain and zero-shot out-of-domain evaluation.",
        "author": "Revanth Gangi Reddy; Vikas Yadav; Md Arafat Sultan; Martin Franz; Vittorio Castelli; Heng Ji; Avirup Sil",
        "authorids": "/r/revanth-gangi-reddy/; /v/vikas-yadav/; /m/md-arafat-sultan/; /m/martin-franz/; /v/vittorio-castelli/; /h/heng-ji/; /a/avirup-sil/",
        "bibtex": "@inproceedings{gangi-reddy-etal-2022-towards,\n    title = \"Towards Robust Neural Retrieval with Source Domain Synthetic Pre-Finetuning\",\n    author = \"Gangi Reddy, Revanth  and\n      Yadav, Vikas  and\n      Sultan, Md Arafat  and\n      Franz, Martin  and\n      Castelli, Vittorio  and\n      Ji, Heng  and\n      Sil, Avirup\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.89/\",\n    pages = \"1065--1070\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.89.pdf",
        "site": "https://aclanthology.org/2022.coling-1.89/",
        "pdf_size": 270637,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5817339452464927431&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois at Urbana-Champaign; Amazon AWS AI; IBM Research AI; IBM Research AI; Amazon AWS AI; University of Illinois at Urbana-Champaign; IBM Research AI",
        "aff_domain": "illinois.edu;amazon.com;ibm.com;us.ibm.com;us.ibm.com;illinois.edu; ",
        "email": "illinois.edu;amazon.com;ibm.com;us.ibm.com;us.ibm.com;illinois.edu; ",
        "github": "https://github.com/primeqa/primeqa/tree/main/primeqa/qg",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;2;1;0;2",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Amazon;IBM Research",
        "aff_unique_dep": ";Amazon Web Services AI;AI",
        "aff_unique_url": "https://illinois.edu;https://aws.amazon.com;https://www.ibm.com/research",
        "aff_unique_abbr": "UIUC;AWS;IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.361",
        "title": "Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Previous works have demonstrated the effectiveness of utilising pre-trained sentence encoders based on their sentence representations for meaning comparison tasks. Though such representations are shown to capture hidden syntax structures, the direct similarity comparison between them exhibits weak sensitivity to word order and structural differences in given sentences. A single similarity score further makes the comparison process hard to interpret. Therefore, we here propose to combine sentence encoders with an alignment component by representing each sentence as a list of predicate-argument spans (where their span representations are derived from sentence encoders), and decomposing the sentence-level meaning comparison into the alignment between their spans for paraphrase identification tasks. Empirical results show that the alignment component brings in both improved performance and interpretability for various sentence encoders. After closer investigation, the proposed approach indicates increased sensitivity to structural difference and enhanced ability to distinguish non-paraphrases with high lexical overlap.",
        "author": "Qiwei Peng; David Weir; Julie Weeds",
        "authorids": "/q/qiwei-peng/; /d/david-weir/; /j/julie-weeds/",
        "bibtex": "@inproceedings{peng-etal-2022-towards,\n    title = \"Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders\",\n    author = \"Peng, Qiwei  and\n      Weir, David  and\n      Weeds, Julie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.361/\",\n    pages = \"4113--4123\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.361.pdf",
        "site": "https://aclanthology.org/2022.coling-1.361/",
        "pdf_size": 805862,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1716005182625389625&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Sussex; University of Sussex; University of Sussex",
        "aff_domain": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk",
        "email": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Sussex",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sussex.ac.uk",
        "aff_unique_abbr": "Sussex",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.coling-1.255",
        "title": "Towards Summarizing Healthcare Questions in Low-Resource Setting",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The current advancement in abstractive document summarization depends to a large extent on a considerable amount of human-annotated datasets. However, the creation of large-scale datasets is often not feasible in closed domains, such as medical and healthcare domains, where human annotation requires domain expertise. This paper presents a novel data selection strategy to generate diverse and semantic questions in a low-resource setting with the aim to summarize healthcare questions. Our method exploits the concept of guided semantic-overlap and diversity-based objective functions to optimally select the informative and diverse set of synthetic samples for data augmentation. Our extensive experiments on benchmark healthcare question summarization datasets demonstrate the effectiveness of our proposed data selection strategy by achieving new state-of-the-art results. Our human evaluation shows that our method generates diverse, fluent, and informative summarized questions.",
        "author": "Shweta Yadav; Cornelia Caragea",
        "authorids": "/s/shweta-yadav/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{yadav-caragea-2022-towards,\n    title = \"Towards Summarizing Healthcare Questions in Low-Resource Setting\",\n    author = \"Yadav, Shweta  and\n      Caragea, Cornelia\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.255/\",\n    pages = \"2892--2905\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.255.pdf",
        "site": "https://aclanthology.org/2022.coling-1.255/",
        "pdf_size": 2645841,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12020561990991709904&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of Illinois at Chicago, Illinois; Department of Computer Science, University of Illinois at Chicago, Illinois",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "",
        "project": "https://pewrsr.ch/3l6m3mv",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.488",
        "title": "Towards Understanding the Relation between Gestures and Language",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this paper, we explore the relation between gestures and language. Using a multimodal dataset, consisting of Ted talks where the language is aligned with the gestures made by the speakers, we adapt a semi-supervised multimodal model to learn gesture embeddings. We show that gestures are predictive of the native language of the speaker, and that gesture embeddings further improve language prediction result. In addition, gesture embeddings might contain some linguistic information, as we show by probing embeddings for psycholinguistic categories. Finally, we analyze the words that lead to the most expressive gestures and find that function words drive the expressiveness of gestures.",
        "author": "Artem Abzaliev; Andrew Owens; Rada Mihalcea",
        "authorids": "/a/artem-abzaliev/; /a/andrew-owens/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{abzaliev-etal-2022-towards,\n    title = \"Towards Understanding the Relation between Gestures and Language\",\n    author = \"Abzaliev, Artem  and\n      Owens, Andrew  and\n      Mihalcea, Rada\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.488/\",\n    pages = \"5507--5520\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.488.pdf",
        "site": "https://aclanthology.org/2022.coling-1.488/",
        "pdf_size": 3516892,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2922640487449404126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Michigan; University of Michigan; University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "https://github.com/MichiganNLP/gestures-language",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.51",
        "title": "Tracking Satisfaction States for Customer Satisfaction Prediction in E-commerce Service Chatbots",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Due to the increasing use of service chatbots in E-commerce platforms in recent years, customer satisfaction prediction (CSP) is gaining more and more attention. CSP is dedicated to evaluating subjective customer satisfaction in conversational service and thus helps improve customer service experience. However, previous methods focus on modeling customer-chatbot interaction across different turns, which are hard to represent the important dynamic satisfaction states throughout the customer journey. In this work, we investigate the problem of satisfaction states tracking and its effects on CSP in E-commerce service chatbots. To this end, we propose a dialogue-level classification model named DialogueCSP to track satisfaction states for CSP. In particular, we explore a novel two-step interaction module to represent the dynamic satisfaction states at each turn. In order to capture dialogue-level satisfaction states for CSP, we further introduce dialogue-aware attentions to integrate historical informative cues into the interaction module. To evaluate the proposed approach, we also build a Chinese E-commerce dataset for CSP. Experiment results demonstrate that our model significantly outperforms multiple baselines, illustrating the benefits of satisfaction states tracking on CSP.",
        "author": "Yang Sun; Liangqing Wu; Shuangyong Song; Xiaoguang Yu; Xiaodong He; Guohong Fu",
        "authorids": "/y/yang-sun/; /l/liangqing-wu/; /s/shuangyong-song/; /x/xiaoguang-yu/; /x/xiaodong-he/; /g/guohong-fu/",
        "bibtex": "https://aclanthology.org/2022.coling-1.51.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.51.pdf",
        "site": "https://aclanthology.org/2022.coling-1.51/",
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9441825440267132706&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.597",
        "title": "Transferring Confluent Knowledge to Argument Mining",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Relevant to all application domains where it is important to get at the reasons underlying sentiments and decisions, argument mining seeks to obtain structured arguments from unstructured text and has been addressed by approaches typically involving some feature and/or neural architecture engineering. By adopting a transfer learning methodology, and by means of a systematic study with a wide range of knowledge sources promisingly suitable to leverage argument mining, the aim of this paper is to empirically assess the potential of transferring such knowledge learned with confluent tasks. By adopting a lean approach that dispenses with heavier feature and model engineering, this study permitted both to gain novel empirically based insights into the argument mining task and to establish new state of the art levels of performance for its three main sub-tasks, viz. identification of argument components, classification of the components, and determination of the relation among them.",
        "author": "Jo\u00e3o Ant\u00f3nio Rodrigues; Ant\u00f3nio Branco",
        "authorids": "/j/joao-antonio-rodrigues/; /a/antonio-branco/",
        "bibtex": "@inproceedings{rodrigues-branco-2022-transferring,\n    title = \"Transferring Confluent Knowledge to Argument Mining\",\n    author = \"Rodrigues, Jo{\\~a}o Ant{\\'o}nio  and\n      Branco, Ant{\\'o}nio\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.597/\",\n    pages = \"6859--6874\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.597.pdf",
        "site": "https://aclanthology.org/2022.coling-1.597/",
        "pdf_size": 475110,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10304316972241860055&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Lisbon; University of Lisbon",
        "aff_domain": "fc.ul.pt;fc.ul.pt",
        "email": "fc.ul.pt;fc.ul.pt",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Lisbon",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ulisboa.pt",
        "aff_unique_abbr": "ULisbon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "2022.coling-1.280",
        "title": "Transferring Knowledge from Structure-aware Self-attention Language Model to Sequence-to-Sequence Semantic Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Semantic parsing considers the task of mapping a natural language sentence into a target formal representation, where various sophisticated sequence-to-sequence (seq2seq) models have been applied with promising results. Generally, these target representations follow a syntax formalism that limits permitted forms. However, it is neither easy nor flexible to explicitly integrate this syntax formalism into a neural seq2seq model. In this paper, we present a structure-aware self-attention language model to capture structural information of target representations and propose a knowledge distillation based approach to incorporating the target language model into a seq2seq model, where grammar rules or sketches are not required in the training process. An ablation study shows that the proposed language model can notably improve the performance of the baseline model. The experiments show that our method achieves new state-of-the-art performance among neural approaches on four semantic parsing (ATIS, GEO) and Python code generation (Django, CoNaLa) tasks.",
        "author": "Ran Ji; Jianmin Ji",
        "authorids": "/r/ran-ji/; /j/jianmin-ji/",
        "bibtex": "@inproceedings{ji-ji-2022-transferring,\n    title = \"Transferring Knowledge from Structure-aware Self-attention Language Model to Sequence-to-Sequence Semantic Parsing\",\n    author = \"Ji, Ran  and\n      Ji, Jianmin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.280/\",\n    pages = \"3164--3174\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.280.pdf",
        "site": "https://aclanthology.org/2022.coling-1.280/",
        "pdf_size": 625821,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3625944188873993692&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China",
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hefei",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.367",
        "title": "Transparent Semantic Parsing with Universal Dependencies Using Graph Transformations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Even though many recent semantic parsers are based on deep learning methods, we should not forget that rule-based alternatives might offer advantages over neural approaches with respect to transparency, portability, and explainability. Taking advantage of existing off-the-shelf Universal Dependency parsers, we present a method that maps a syntactic dependency tree to a formal meaning representation based on Discourse Representation Theory. Rather than using lambda calculus to manage variable bindings, our approach is novel in that it consists of using a series of graph transformations. The resulting UD semantic parser shows good performance for English, German, Italian and Dutch, with F-scores over 75%, outperforming a neural semantic parser for the lower-resourced languages. Unlike neural semantic parsers, our UD semantic parser does not hallucinate output, is relatively easy to port to other languages, and is completely transparent.",
        "author": "Wessel Poelman; Rik van Noord; Johan Bos",
        "authorids": "/w/wessel-poelman/; /r/rik-van-noord/; /j/johan-bos/",
        "bibtex": "@inproceedings{poelman-etal-2022-transparent,\n    title = \"Transparent Semantic Parsing with {U}niversal {D}ependencies Using Graph Transformations\",\n    author = \"Poelman, Wessel  and\n      van Noord, Rik  and\n      Bos, Johan\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.367/\",\n    pages = \"4186--4192\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.367.pdf",
        "site": "https://aclanthology.org/2022.coling-1.367/",
        "pdf_size": 257605,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5036420734537327576&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Center for Language and Cognition, University of Groningen; Center for Language and Cognition, University of Groningen; Center for Language and Cognition, University of Groningen",
        "aff_domain": "wesselpoelman.nl;rug.nl;rug.nl",
        "email": "wesselpoelman.nl;rug.nl;rug.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Groningen",
        "aff_unique_dep": "Center for Language and Cognition",
        "aff_unique_url": "https://www.rug.nl",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.coling-1.270",
        "title": "TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict ICD codes. Experiments on two MIMIC datasets show that our method outperforms prior state-of-the-art ICD coding approaches. The code is available at https://github.com/liu-zichen/TreeMAN.",
        "author": "Zichen Liu; Xuyuan Liu; Yanlong Wen; Guoqing Zhao; Fen Xia; Xiaojie Yuan",
        "authorids": "/z/zichen-liu/; /x/xuyuan-liu/; /y/yanlong-wen/; /g/guoqing-zhao/; /f/fen-xia/; /x/xiaojie-yuan/",
        "bibtex": "@inproceedings{liu-etal-2022-treeman,\n    title = \"{T}ree{MAN}: Tree-enhanced Multimodal Attention Network for {ICD} Coding\",\n    author = \"Liu, Zichen  and\n      Liu, Xuyuan  and\n      Wen, Yanlong  and\n      Zhao, Guoqing  and\n      Xia, Fen  and\n      Yuan, Xiaojie\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.270/\",\n    pages = \"3054--3063\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.270.pdf",
        "site": "https://aclanthology.org/2022.coling-1.270/",
        "pdf_size": 583363,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17539634880403124975&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "College of Computer Science, Nankai University; College of Computer Science, Nankai University; College of Computer Science, Nankai University+Mashang Consumer Finance Co.,Ltd.; Mashang Consumer Finance Co.,Ltd.; Mashang Consumer Finance Co.,Ltd.; College of Computer Science, Nankai University",
        "aff_domain": "dbis.nankai.edu.cn;mail.nankai.edu.cn;dbis.nankai.edu.cn;msxf.com;msxf.com;dbis.nankai.edu.cn",
        "email": "dbis.nankai.edu.cn;mail.nankai.edu.cn;dbis.nankai.edu.cn;msxf.com;msxf.com;dbis.nankai.edu.cn",
        "github": "https://github.com/liu-zichen/TreeMAN",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0+1;1;1;0",
        "aff_unique_norm": "Nankai University;Mashang Consumer Finance",
        "aff_unique_dep": "College of Computer Science;",
        "aff_unique_url": "http://www.nankai.edu.cn;",
        "aff_unique_abbr": "Nankai;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.299",
        "title": "Twitter Topic Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Social media platforms host discussions about a wide variety of topics that arise everyday. Making sense of all the content and organising it into categories is an arduous task. A common way to deal with this issue is relying on topic modeling, but topics discovered using this technique are difficult to interpret and can differ from corpus to corpus. In this paper, we present a new task based on tweet topic classification and release two associated datasets. Given a wide range of topics covering the most important discussion points in social media, we provide training and testing data from recent time periods that can be used to evaluate tweet classification models. Moreover, we perform a quantitative evaluation and analysis of current general- and domain-specific language models on the task, which provide more insights on the challenges and nature of the task.",
        "author": "Dimosthenis Antypas; Asahi Ushio; Jose Camacho-Collados; Vitor Silva; Leonardo Neves; Francesco Barbieri",
        "authorids": "/d/dimosthenis-antypas/; /a/asahi-ushio/; /j/jose-camacho-collados/; /v/vitor-silva/; /l/leonardo-neves/; /f/francesco-barbieri/",
        "bibtex": "@inproceedings{antypas-etal-2022-twitter,\n    title = \"{T}witter Topic Classification\",\n    author = \"Antypas, Dimosthenis  and\n      Ushio, Asahi  and\n      Camacho-Collados, Jose  and\n      Silva, Vitor  and\n      Neves, Leonardo  and\n      Barbieri, Francesco\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.299/\",\n    pages = \"3386--3400\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.299.pdf",
        "site": "https://aclanthology.org/2022.coling-1.299/",
        "pdf_size": 7918679,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2522857123834617604&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Cardiff NLP, School of Computer Science and Informatics, Cardiff University, United Kingdom; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, United Kingdom; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, United Kingdom; Snap Inc., Santa Monica, CA, United States; Snap Inc., Santa Monica, CA, United States; Snap Inc., Santa Monica, CA, United States",
        "aff_domain": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;snap.com;snap.com;snap.com",
        "email": "cardiff.ac.uk;cardiff.ac.uk;cardiff.ac.uk;snap.com;snap.com;snap.com",
        "github": "",
        "project": "https://huggingface.co/datasets/cardiffnlp/tweet_topic_single; https://huggingface.co/datasets/cardiffnlp/tweet_topic_multi",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;1;1",
        "aff_unique_norm": "Cardiff University;Snap Inc.",
        "aff_unique_dep": "School of Computer Science and Informatics;",
        "aff_unique_url": "https://www.cardiff.ac.uk;https://www.snapinc.com",
        "aff_unique_abbr": "Cardiff;Snap",
        "aff_campus_unique_index": "0;0;0;1;1;1",
        "aff_campus_unique": "Cardiff;Santa Monica",
        "aff_country_unique_index": "0;0;0;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.coling-1.176",
        "title": "Two Languages Are Better than One: Bilingual Enhancement for Chinese Named Entity Recognition",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Chinese Named Entity Recognition (NER) has continued to attract research attention. However, most existing studies only explore the internal features of the Chinese language but neglect other lingual modal features. Actually, as another modal knowledge of the Chinese language, English contains rich prompts about entities that can potentially be applied to improve the performance of Chinese NER. Therefore, in this study, we explore the bilingual enhancement for Chinese NER and propose a unified bilingual interaction module called the Adapted Cross-Transformers with Global Sparse Attention (ACT-S) to capture the interaction of bilingual information. We utilize a model built upon several different ACT-Ss to integrate the rich English information into the Chinese representation. Moreover, our model can learn the interaction of information between bilinguals (inter-features) and the dependency information within Chinese (intra-features). Compared with existing Chinese NER methods, our proposed model can better handle entities with complex structures. The English text that enhances the model is automatically generated by machine translation, avoiding high labour costs. Experimental results on four well-known benchmark datasets demonstrate the effectiveness and robustness of our proposed model.",
        "author": "Jinzhong Ning; Zhihao Yang; Zhizheng Wang; Yuanyuan Sun; Hongfei Lin; Jian Wang",
        "authorids": "/j/jinzhong-ning/; /z/zhihao-yang/; /z/zhizheng-wang/; /y/yuanyuan-sun/; /h/hongfei-lin/; /j/jian-wang/",
        "bibtex": "@inproceedings{ning-etal-2022-two,\n    title = \"Two Languages Are Better than One: Bilingual Enhancement for {C}hinese Named Entity Recognition\",\n    author = \"Ning, Jinzhong  and\n      Yang, Zhihao  and\n      Wang, Zhizheng  and\n      Sun, Yuanyuan  and\n      Lin, Hongfei  and\n      Wang, Jian\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.176/\",\n    pages = \"2024--2033\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.176.pdf",
        "site": "https://aclanthology.org/2022.coling-1.176/",
        "pdf_size": 947324,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4122589352074416535&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology; School of Computer Science and Technology, Dalian University of Technology",
        "aff_domain": "mail.dlut.edu.cn;gmail.com;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "email": "mail.dlut.edu.cn;gmail.com;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Dalian University of Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://en.dlut.edu.cn/",
        "aff_unique_abbr": "DUT",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Dalian",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.549",
        "title": "Type-dependent Prompt CycleQAG : Cycle Consistency for Multi-hop Question Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-hop question generation (QG) is the process of generating answer related questions, which requires aggregating multiple pieces of information and reasoning from different parts of the texts. This is opposed to single-hop QG which generates questions from sentences containing an answer in a given paragraph. Single-hop QG requires no reasoning or complexity, while multi-hop QG often requires logical reasoning to derive an answer related question, making it a dual task. Not enough research has been made on the multi-hop QG due to its complexity. Also, a question should be created using the question type and words related to the correct answer as a prompt so that multi-hop questions can get more information. In this view, we propose a new type-dependent prompt cycleQAG (cyclic question-answer-generation), with a cycle consistency loss in which QG and Question Answering (QA) are learnt in a cyclic manner. The novelty is that the cycle consistency loss uses the negative cross entropy to generate syntactically diverse questions that enable selecting different word representations. Empirical evaluation on the multi-hop dataset with automatic and human evaluation metrics outperforms the baseline model by about 10.38% based on ROUGE score.",
        "author": "Seungyeon Lee; Minho Lee",
        "authorids": "/s/seungyeon-lee/; /m/minho-lee/",
        "bibtex": "@inproceedings{lee-lee-2022-type,\n    title = \"Type-dependent Prompt {C}ycle{QAG} : Cycle Consistency for Multi-hop Question Generation\",\n    author = \"Lee, Seungyeon  and\n      Lee, Minho\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.549/\",\n    pages = \"6301--6314\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.549.pdf",
        "site": "https://aclanthology.org/2022.coling-1.549/",
        "pdf_size": 832518,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=20793692562112100&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Artificial Intelligence, Kyungpook National University + ALI Co., Ltd.; Department of Artificial Intelligence, Kyungpook National University + ALI Co., Ltd.",
        "aff_domain": "knu.ac.kr;knu.ac.kr",
        "email": "knu.ac.kr;knu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Kyungpook National University;ALI Co., Ltd.",
        "aff_unique_dep": "Department of Artificial Intelligence;",
        "aff_unique_url": "http://www.knu.ac.kr;",
        "aff_unique_abbr": "KNU;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "South Korea;China"
    },
    {
        "id": "2022.coling-1.212",
        "title": "Type-enriched Hierarchical Contrastive Strategy for Fine-Grained Entity Typing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Fine-grained entity typing (FET) aims to deduce specific semantic types of the entity mentions in the text. Modern methods for FET mainly focus on learning what a certain type looks like. And few works directly model the type differences, that is, let models know the extent that which one type is different from others. To alleviate this problem, we propose a type-enriched hierarchical contrastive strategy for FET. Our method can directly model the differences between hierarchical types and improve the ability to distinguish multi-grained similar types. On the one hand, we embed type into entity contexts to make type information directly perceptible. On the other hand, we design a constrained contrastive strategy on the hierarchical structure to directly model the type differences, which can simultaneously perceive the distinguishability between types at different granularity. Experimental results on three benchmarks, BBN, OntoNotes, and FIGER show that our method achieves significant performance on FET by effectively modeling type differences.",
        "author": "Xinyu Zuo; Haijin Liang; Ning Jing; Shuang Zeng; Zhou Fang; Yu Luo",
        "authorids": "/x/xinyu-zuo/; /h/haijin-liang/; /n/ning-jing/; /s/shuang-zeng/; /z/zhou-fang/; /y/yu-luo/",
        "bibtex": "@inproceedings{zuo-etal-2022-type,\n    title = \"Type-enriched Hierarchical Contrastive Strategy for Fine-Grained Entity Typing\",\n    author = \"Zuo, Xinyu  and\n      Liang, Haijin  and\n      Jing, Ning  and\n      Zeng, Shuang  and\n      Fang, Zhou  and\n      Luo, Yu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.212/\",\n    pages = \"2405--2417\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.212.pdf",
        "site": "https://aclanthology.org/2022.coling-1.212/",
        "pdf_size": 1001731,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8011401777128844677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent Inc.; Tencent Inc.; Tencent Inc. + USTC; Tencent Inc.; Tencent Inc.; Tencent Inc.",
        "aff_domain": "tencent.com;tencent.com;gmail.com;tencent.com;tencent.com;tencent.com",
        "email": "tencent.com;tencent.com;gmail.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0+1;0;0;0",
        "aff_unique_norm": "Tencent;University of Science and Technology of China",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tencent.com;https://www.ustc.edu.cn",
        "aff_unique_abbr": "Tencent;USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.613",
        "title": "UECA-Prompt: Universal Prompt for Emotion Cause Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Emotion cause analysis (ECA) aims to extract emotion clauses and find the corresponding cause of the emotion. Existing methods adopt fine-tuning paradigm to solve certain types of ECA tasks. These task-specific methods have a deficiency of universality. And the relations among multiple objectives in one task are not explicitly modeled. Moreover, the relative position information introduced in most existing methods may make the model suffer from dataset bias. To address the first two problems, this paper proposes a universal prompt tuning method to solve different ECA tasks in the unified framework. As for the third problem, this paper designs a directional constraint module and a sequential learning module to ease the bias. Considering the commonalities among different tasks, this paper proposes a cross-task training method to further explore the capability of the model. The experimental results show that our method achieves competitive performance on the ECA datasets.",
        "author": "Xiaopeng Zheng; Zhiyue Liu; Zizhen Zhang; Zhaoyang Wang; Jiahai Wang",
        "authorids": "/x/xiaopeng-zheng/; /z/zhiyue-liu/; /z/zizhen-zhang/; /z/zhaoyang-wang/; /j/jiahai-wang/",
        "bibtex": "@inproceedings{zheng-etal-2022-ueca,\n    title = \"{UECA}-Prompt: Universal Prompt for Emotion Cause Analysis\",\n    author = \"Zheng, Xiaopeng  and\n      Liu, Zhiyue  and\n      Zhang, Zizhen  and\n      Wang, Zhaoyang  and\n      Wang, Jiahai\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.613/\",\n    pages = \"7031--7041\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.613.pdf",
        "site": "https://aclanthology.org/2022.coling-1.613/",
        "pdf_size": 688832,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16328886295201695215&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University; School of Computer Science and Engineering, Sun Yat-sen University + School of Computer, Electronics and Information, Guangxi University; School of Computer Science and Engineering, Sun Yat-sen University; School of Computer Science and Engineering, Sun Yat-sen University; School of Computer Science and Engineering, Sun Yat-sen University",
        "aff_domain": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;mail.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Sun Yat-sen University;Guangxi University",
        "aff_unique_dep": "School of Computer Science and Engineering;School of Computer, Electronics and Information",
        "aff_unique_url": "http://www.sysu.edu.cn;https://www.gxu.edu.cn",
        "aff_unique_abbr": "SYSU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.550",
        "title": "UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm, which first extracts a relatively short meta-document, then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents. However, the trained scoring model is prone to under-fitting for low-resource settings, as it relies on the training data. To extract documents effectively, we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword\u2019s perplexity, which can assess the document\u2019s semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document\u2019s salience, thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets, 2 data settings, and 2 abstractive backbone models, showing our method\u2019s effectiveness. Our code is available at https://github.com/THU-KEG/UPER",
        "author": "Shangqing Tu; Jifan Yu; Fangwei Zhu; Juanzi Li; Lei Hou; Jian-Yun Nie",
        "authorids": "/s/shangqing-tu/; /j/jifan-yu/; /f/fangwei-zhu/; /j/juanzi-li/; /l/lei-hou/; /j/jian-yun-nie/",
        "bibtex": "https://aclanthology.org/2022.coling-1.550.bib",
        "pdf": "https://aclanthology.org/2022.coling-1.550.pdf",
        "site": "https://aclanthology.org/2022.coling-1.550/",
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5738210634283491401&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.243",
        "title": "Uncertainty-aware Propagation Structure Reconstruction for Fake News Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The widespread of fake news has detrimental societal effects. Recent works model information propagation as graph structure and aggregate structural features from user interactions for fake news detection. However, they usually neglect a broader propagation uncertainty issue, caused by some missing and unreliable interactions during actual spreading, and suffer from learning accurate and diverse structural properties. In this paper, we propose a novel dual graph-based model, Uncertainty-aware Propagation Structure Reconstruction (UPSR) for improving fake news detection. Specifically, after the original propagation modeling, we introduce propagation structure reconstruction to fully explore latent interactions in the actual propagation. We design a novel Gaussian Propagation Estimation to refine the original deterministic node representation by multiple Gaussian distributions and arise latent interactions with KL divergence between distributions in a multi-facet manner. Extensive experiments on two real-world datasets demonstrate the effectiveness and superiority of our model.",
        "author": "Lingwei Wei; Dou Hu; Wei Zhou; Songlin Hu",
        "authorids": "/l/lingwei-wei/; /d/dou-hu/; /w/wei-zhou/; /s/songlin-hu/",
        "bibtex": "@inproceedings{wei-etal-2022-uncertainty,\n    title = \"Uncertainty-aware Propagation Structure Reconstruction for Fake News Detection\",\n    author = \"Wei, Lingwei  and\n      Hu, Dou  and\n      Zhou, Wei  and\n      Hu, Songlin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.243/\",\n    pages = \"2759--2768\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.243.pdf",
        "site": "https://aclanthology.org/2022.coling-1.243/",
        "pdf_size": 2473100,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13618010920594327670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.304",
        "title": "Understanding Attention for Vision-and-Language Tasks",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Attention mechanism has been used as an important component across Vision-and-Language(VL) tasks in order to bridge the semantic gap between visual and textual features. While attention has been widely used in VL tasks, it has not been examined the capability of different attention alignment calculation in bridging the semantic gap between visual and textual clues. In this research, we conduct a comprehensive analysis on understanding the role of attention alignment by looking into the attention score calculation methods and check how it actually represents the visual region\u2019s and textual token\u2019s significance for the global assessment. We also analyse the conditions which attention score calculation mechanism would be more (or less) interpretable, and which may impact the model performance on three different VL tasks, including visual question answering, text-to-image generation, text-and-image matching (both sentence and image retrieval). Our analysis is the first of its kind and provides useful insights of the importance of each attention alignment score calculation when applied at the training phase of VL tasks, commonly ignored in attention-based cross modal models, and/or pretrained models. Our code is available at: https://github.com/adlnlp/Attention_VL",
        "author": "Feiqi Cao; Soyeon Caren Han; Siqu Long; Changwei Xu; Josiah Poon",
        "authorids": "/f/feiqi-cao/; /s/soyeon-caren-han/; /s/siqu-long/; /c/changwei-xu/; /j/josiah-poon/",
        "bibtex": "@inproceedings{cao-etal-2022-understanding,\n    title = \"Understanding Attention for Vision-and-Language Tasks\",\n    author = \"Cao, Feiqi  and\n      Han, Soyeon Caren  and\n      Long, Siqu  and\n      Xu, Changwei  and\n      Poon, Josiah\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.304/\",\n    pages = \"3438--3453\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.304.pdf",
        "site": "https://aclanthology.org/2022.coling-1.304/",
        "pdf_size": 4977262,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:a57vXM6wH44J:scholar.google.com/&scioq=Understanding+Attention+for+Vision-and-Language+Tasks&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "School of Computer Science, The University of Sydney, Australia; School of Computer Science, The University of Sydney, Australia + School of Physics, Maths and Computing, The University of Western Australia, Australia; School of Computer Science, The University of Sydney, Australia; School of Computer Science, The University of Sydney, Australia; School of Computer Science, The University of Sydney, Australia",
        "aff_domain": "uni.sydney.edu.au;sydney.edu.au;uni.sydney.edu.au;uni.sydney.edu.au;sydney.edu.au",
        "email": "uni.sydney.edu.au;sydney.edu.au;uni.sydney.edu.au;uni.sydney.edu.au;sydney.edu.au",
        "github": "https://github.com/adlnlp/Attention_VL",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "The University of Sydney;The University of Western Australia",
        "aff_unique_dep": "School of Computer Science;School of Physics, Maths and Computing",
        "aff_unique_url": "https://www.sydney.edu.au;https://www.uwa.edu.au",
        "aff_unique_abbr": "USYD;UWA",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Sydney;",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.coling-1.152",
        "title": "Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Generative question answering (QA) models generate answers to questions either solely based on the parameters of the model (the closed-book setting) or additionally retrieving relevant evidence (the open-book setting). Generative QA models can answer some relatively complex questions, but the mechanism through which they do so is still poorly understood. We perform several studies aimed at better understanding the multi-hop reasoning capabilities of generative QA models. First, we decompose multi-hop questions into multiple corresponding single-hop questions, and find marked inconsistency in QA models\u2019 answers on these pairs of ostensibly identical question chains. Second, we find that models lack zero-shot multi-hop reasoning ability: when trained only on single-hop questions, models generalize poorly to multi-hop questions. Finally, we demonstrate that it is possible to improve models\u2019 zero-shot multi-hop reasoning capacity through two methods that approximate real multi-hop natural language (NL) questions by training on either concatenation of single-hop questions or logical forms (SPARQL). In sum, these results demonstrate that multi-hop reasoning does not emerge naturally in generative QA models, but can be encouraged by advances in training or modeling techniques. Code is available at https://github.com/jzbjyb/multihop.",
        "author": "Zhengbao Jiang; Jun Araki; Haibo Ding; Graham Neubig",
        "authorids": "/z/zhengbao-jiang/; /j/jun-araki/; /h/haibo-ding/; /g/graham-neubig/",
        "bibtex": "@inproceedings{jiang-etal-2022-understanding,\n    title = \"Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering\",\n    author = \"Jiang, Zhengbao  and\n      Araki, Jun  and\n      Ding, Haibo  and\n      Neubig, Graham\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.152/\",\n    pages = \"1765--1775\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.152.pdf",
        "site": "https://aclanthology.org/2022.coling-1.152/",
        "pdf_size": 553332,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14440223540114399830&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/jzbjyb/multihop",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2022.coling-1.232",
        "title": "Unregulated Chinese-to-English Data Expansion Does NOT Work for Neural Event Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We leverage cross-language data expansion and retraining to enhance neural Event Detection (abbr., ED) on English ACE corpus. Machine translation is utilized for expanding English training set of ED from that of Chinese. However, experimental results illustrate that such strategy actually results in performance degradation. The survey of translations suggests that the mistakenly-aligned triggers in the expanded data negatively influences the retraining process. We refer this phenomenon to \u201ctrigger falsification\u201d. To overcome the issue, we apply heuristic rules for regulating the expanded data, fixing the distracting samples that contain the falsified triggers. The supplementary experiments show that the rule-based regulation is beneficial, yielding the improvement of about 1.6% F1-score for ED. We additionally prove that, instead of transfer learning from the translated ED data, the straight data combination by random pouring surprisingly performs better.",
        "author": "Zhongqiu Li; Yu Hong; Jie Wang; Shiming He; Jianmin Yao; Guodong Zhou",
        "authorids": "/z/zhongqiu-li/; /y/yu-hong/; /j/jie-wang/; /s/shiming-he/; /j/jianmin-yao/; /g/guodong-zhou/",
        "bibtex": "@inproceedings{li-etal-2022-unregulated,\n    title = \"Unregulated {C}hinese-to-{E}nglish Data Expansion Does {NOT} Work for Neural Event Detection\",\n    author = \"Li, Zhongqiu  and\n      Hong, Yu  and\n      Wang, Jie  and\n      He, Shiming  and\n      Yao, Jianmin  and\n      Zhou, Guodong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.232/\",\n    pages = \"2633--2638\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.232.pdf",
        "site": "https://aclanthology.org/2022.coling-1.232/",
        "pdf_size": 430440,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=283474604364447817&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;suda.edu.cn;suda.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.586",
        "title": "Unsupervised Data Augmentation for Aspect Based Sentiment Analysis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recent approaches to Aspect-based Sentiment Analysis (ABSA) take a co-extraction approach to this span-level classification task, performing the subtasks of aspect term extraction (ATE) and aspect sentiment classification (ASC) simultaneously. In this work, we build on recent progress in applying pre-training to this co-extraction task with the introduction of an adaptation of Unsupervised Data Augmentation in semi-supervised learning. As originally implemented, UDA cannot accommodate span-level classification since it relies on advanced data augmentation techniques, such as back-translation, that alter the sequence lengths of the original data and cause index mismatches. We introduce an adaptation of UDA using Masked Language Model (MLM) unmasking that accommodates this index-match constraint and test the approach on standard ABSA benchmark datasets. We show that simple augmentations applied to modest-sized datasets along with consistency training lead to competitive performance with the current ABSA state-of-the-art in the restaurant and laptop domains using only 75% of the training data.",
        "author": "David Z. Chen; Adam Faulkner; Sahil Badyal",
        "authorids": "/d/david-z-chen/; /a/adam-faulkner/; /s/sahil-badyal/",
        "bibtex": "@inproceedings{chen-etal-2022-unsupervised-data,\n    title = \"Unsupervised Data Augmentation for Aspect Based Sentiment Analysis\",\n    author = \"Chen, David Z.  and\n      Faulkner, Adam  and\n      Badyal, Sahil\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.586/\",\n    pages = \"6746--6751\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.586.pdf",
        "site": "https://aclanthology.org/2022.coling-1.586/",
        "pdf_size": 558673,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8088006548698403401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Capital One Servicing Intelligence, NLP; Capital One Servicing Intelligence, NLP; Capital One Servicing Intelligence, NLP",
        "aff_domain": "capitalone.com;capitalone.com;capitalone.com",
        "email": "capitalone.com;capitalone.com;capitalone.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Capital One",
        "aff_unique_dep": "Servicing Intelligence, NLP",
        "aff_unique_url": "https://www.capitalone.com",
        "aff_unique_abbr": "Capital One",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.420",
        "title": "Unsupervised Domain Adaptation for Text Classification via Meta Self-Paced Learning",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A shift in data distribution can have a significant impact on performance of a text classification model. Recent methods addressing unsupervised domain adaptation for textual tasks typically extracted domain-invariant representations through balancing between multiple objectives to align feature spaces between source and target domains. While effective, these methods induce various new domain-sensitive hyperparameters, thus are impractical as large-scale language models are drastically growing bigger to achieve optimal performance. To this end, we propose to leverage meta-learning framework to train a neural network-based self-paced learning procedure in an end-to-end manner. Our method, called Meta Self-Paced Domain Adaption (MSP-DA), follows a novel but intuitive domain-shift variation of cluster assumption to derive the meta train-test dataset split based on the self-pacing difficulties of source domain\u2019s examples. As a result, MSP-DA effectively leverages self-training and self-tuning domain-specific hyperparameters simultaneously throughout the learning process. Extensive experiments demonstrate our framework substantially improves performance on target domains, surpassing state-of-the-art approaches. Detailed analyses validate our method and provide insight into how each domain affects the learned hyperparameters.",
        "author": "Nghia Ngo Trung; Linh Ngo Van; Thien Huu Nguyen",
        "authorids": "/n/nghia-ngo-trung/; /l/linh-ngo-van/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{trung-etal-2022-unsupervised,\n    title = \"Unsupervised Domain Adaptation for Text Classification via Meta Self-Paced Learning\",\n    author = \"Trung, Nghia Ngo  and\n      Van, Linh Ngo  and\n      Nguyen, Thien Huu\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.420/\",\n    pages = \"4741--4752\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.420.pdf",
        "site": "https://aclanthology.org/2022.coling-1.420/",
        "pdf_size": 1580331,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8571945507848225024&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Hanoi University of Science and Technology, Vietnam; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA",
        "aff_domain": "uoregon.edu;soict.hust.edu.vn;cs.uoregon.edu",
        "email": "uoregon.edu;soict.hust.edu.vn;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Oregon;Hanoi University of Science and Technology",
        "aff_unique_dep": "Department of Computer and Information Science;",
        "aff_unique_url": "https://www.uoregon.edu;https://www.hust.edu.vn",
        "aff_unique_abbr": "UO;HUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Eugene;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Vietnam"
    },
    {
        "id": "2022.coling-1.366",
        "title": "Unsupervised Lexical Substitution with Decontextualised Embeddings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We propose a new unsupervised method for lexical substitution using pre-trained language models. Compared to previous approaches that use the generative capability of language models to predict substitutes, our method retrieves substitutes based on the similarity of contextualised and decontextualised word embeddings, i.e. the average contextual representation of a word in multiple contexts. We conduct experiments in English and Italian, and show that our method substantially outperforms strong baselines and establishes a new state-of-the-art without any explicit supervision or fine-tuning. We further show that our method performs particularly well at predicting low-frequency substitutes, and also generates a diverse list of substitute candidates, reducing morphophonetic or morphosyntactic biases induced by article-noun agreement.",
        "author": "Takashi Wada; Timothy Baldwin; Yuji Matsumoto; Jey Han Lau",
        "authorids": "/t/takashi-wada/; /t/timothy-baldwin/; /y/yuji-matsumoto/; /j/jey-han-lau/",
        "bibtex": "@inproceedings{wada-etal-2022-unsupervised,\n    title = \"Unsupervised Lexical Substitution with Decontextualised Embeddings\",\n    author = \"Wada, Takashi  and\n      Baldwin, Timothy  and\n      Matsumoto, Yuji  and\n      Lau, Jey Han\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.366/\",\n    pages = \"4172--4185\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.366.pdf",
        "site": "https://aclanthology.org/2022.coling-1.366/",
        "pdf_size": 679399,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10199989467400619963&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing and Information Systems, The University of Melbourne + RIKEN Center for Advanced Intelligence Project (AIP); School of Computing and Information Systems, The University of Melbourne + Department of Natural Language Processing, MBZUAI; RIKEN Center for Advanced Intelligence Project (AIP); School of Computing and Information Systems, The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;ldwin.net;riken.jp;gmail.com",
        "email": "student.unimelb.edu.au;ldwin.net;riken.jp;gmail.com",
        "github": "https://github.com/twadada/lexsub_decontextualised",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+2;1;0",
        "aff_unique_norm": "The University of Melbourne;RIKEN;MBZUAI",
        "aff_unique_dep": "School of Computing and Information Systems;Center for Advanced Intelligence Project (AIP);Department of Natural Language Processing",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.riken.jp/en/;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UniMelb;RIKEN;MBZUAI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne;",
        "aff_country_unique_index": "0+1;0+2;1;0",
        "aff_country_unique": "Australia;Japan;United Arab Emirates"
    },
    {
        "id": "2022.coling-1.630",
        "title": "Unsupervised Multi-scale Expressive Speaking Style Modeling with Hierarchical Context Information for Audiobook Speech Synthesis",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Naturalness and expressiveness are crucial for audiobook speech synthesis, but now are limited by the averaged global-scale speaking style representation. In this paper, we propose an unsupervised multi-scale context-sensitive text-to-speech model for audiobooks. A multi-scale hierarchical context encoder is specially designed to predict both global-scale context style embedding and local-scale context style embedding from a wider context of input text in a hierarchical manner. Likewise, a multi-scale reference encoder is introduced to extract reference style embeddings at both global and local scales from the reference speech, which is used to guide the prediction of speaking styles. On top of these, a bi-reference attention mechanism is used to align both local-scale reference style embedding sequence and local-scale context style embedding sequence with corresponding phoneme embedding sequence. Both objective and subjective experiment results on a real-world multi-speaker Mandarin novel audio dataset demonstrate the excellent performance of our proposed method over all baselines in terms of naturalness and expressiveness of the synthesized speech.",
        "author": "Xueyuan Chen; Shun Lei; Zhiyong Wu; Dong Xu; Weifeng Zhao; Helen Meng",
        "authorids": "/x/xueyuan-chen/; /s/shun-lei/; /z/zhiyong-wu/; /d/dong-xu/; /w/weifeng-zhao/; /h/helen-meng/",
        "bibtex": "@inproceedings{chen-etal-2022-unsupervised-multi,\n    title = \"Unsupervised Multi-scale Expressive Speaking Style Modeling with Hierarchical Context Information for Audiobook Speech Synthesis\",\n    author = \"Chen, Xueyuan  and\n      Lei, Shun  and\n      Wu, Zhiyong  and\n      Xu, Dong  and\n      Zhao, Weifeng  and\n      Meng, Helen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.630/\",\n    pages = \"7193--7202\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.630.pdf",
        "site": "https://aclanthology.org/2022.coling-1.630/",
        "pdf_size": 1413659,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1215224772843170777&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://thuhcsi.github.io/COLING2022-MSHCE-TTS",
        "author_num": 6
    },
    {
        "id": "2022.coling-1.149",
        "title": "Unsupervised Question Answering via Answer Diversifying",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Unsupervised question answering is an attractive task due to its independence on labeled data. Previous works usually make use of heuristic rules as well as pre-trained models to construct data and train QA models. However, most of these works regard named entity (NE) as the only answer type, which ignores the high diversity of answers in the real world. To tackle this problem, we propose a novel unsupervised method by diversifying answers, named DiverseQA. Specifically, the proposed method is composed of three modules: data construction, data augmentation and denoising filter. Firstly, the data construction module extends the extracted named entity into a longer sentence constituent as the new answer span to construct a QA dataset with diverse answers. Secondly, the data augmentation module adopts an answer-type dependent data augmentation process via adversarial training in the embedding level. Thirdly, the denoising filter module is designed to alleviate the noise in the constructed data. Extensive experiments show that the proposed method outperforms previous unsupervised models on five benchmark datasets, including SQuADv1.1, NewsQA, TriviaQA, BioASQ, and DuoRC. Besides, the proposed method shows strong performance in the few-shot learning setting.",
        "author": "Yuxiang Nie; Heyan Huang; Zewen Chi; Xian-Ling Mao",
        "authorids": "/y/yuxiang-nie/; /h/he-yan-huang/; /z/zewen-chi/; /x/xian-ling-mao/",
        "bibtex": "@inproceedings{nie-etal-2022-unsupervised,\n    title = \"Unsupervised Question Answering via Answer Diversifying\",\n    author = \"Nie, Yuxiang  and\n      Huang, Heyan  and\n      Chi, Zewen  and\n      Mao, Xian-Ling\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.149/\",\n    pages = \"1732--1742\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.149.pdf",
        "site": "https://aclanthology.org/2022.coling-1.149/",
        "pdf_size": 618542,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16467669296976596786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China + Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, Beijing, China + Beijing Institute of Technology Southeast Academy of Information Technology, Fujian, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China + Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, Beijing, China + Beijing Institute of Technology Southeast Academy of Information Technology, Fujian, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China + Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, Beijing, China + Beijing Institute of Technology Southeast Academy of Information Technology, Fujian, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China + Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, Beijing, China + Beijing Institute of Technology Southeast Academy of Information Technology, Fujian, China",
        "aff_domain": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "email": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "github": "https://github.com/JerrryNie/DiverseQA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0;0+1+0;0+1+0;0+1+0",
        "aff_unique_norm": "Beijing Institute of Technology;Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications",
        "aff_unique_dep": "School of Computer Science and Technology;",
        "aff_unique_url": "http://www.bit.edu.cn;",
        "aff_unique_abbr": "BIT;",
        "aff_campus_unique_index": "0+2;0+2;0+2;0+2",
        "aff_campus_unique": "Beijing;;Southeast Academy of Information Technology",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.441",
        "title": "Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Measuring Sentence Textual Similarity (STS) is a classic task that can be applied to many downstream NLP applications such as text generation and retrieval. In this paper, we focus on unsupervised STS that works on various domains but only requires minimal data and computational resources. Theoretically, we propose a light-weighted Expectation-Correction (EC) formulation for STS computation. EC formulation unifies unsupervised STS approaches including the cosine similarity of Additively Composed (AC) sentence embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the compositional phrase semantics by composing multiple recursive EC formulations. ROTS finishes in linear time and is faster than its predecessors. ROTS is empirically more effective and scalable than previous approaches. Extensive experiments on 29 STS tasks under various settings show the clear advantage of ROTS over existing approaches. Detailed ablation studies prove the effectiveness of our approaches.",
        "author": "Zihao Wang; Jiaheng Dou; Yong Zhang",
        "authorids": "/z/zihao-wang/; /j/jiaheng-dou/; /y/yong-zhang/",
        "bibtex": "@inproceedings{wang-etal-2022-unsupervised,\n    title = \"Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics\",\n    author = \"Wang, Zihao  and\n      Dou, Jiaheng  and\n      Zhang, Yong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.441/\",\n    pages = \"4976--4995\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.441.pdf",
        "site": "https://aclanthology.org/2022.coling-1.441/",
        "pdf_size": 729004,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18225407746195364536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of CSE, HKUST, Hong Kong, China; BNRist, RIIT, Institute of Internet Industry, Department of Computer Science and Technology, Tsinghua University, Beijing, China; BNRist, RIIT, Institute of Internet Industry, Department of Computer Science and Technology, Tsinghua University, Beijing, China",
        "aff_domain": "cse.ust.hk;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "cse.ust.hk;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/zihao-wang/rots",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tsinghua University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Technology",
        "aff_unique_url": "https://www.ust.hk;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "HKUST;THU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Hong Kong;Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.44",
        "title": "Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Personalized response selection systems are generally grounded on persona. However, a correlation exists between persona and empathy, which these systems do not explore well. Also, when a contradictory or off-topic response is selected, faithfulness to the conversation context plunges. This paper attempts to address these issues by proposing a suite of fusion strategies that capture the interaction between persona, emotion, and entailment information of the utterances. Ablation studies on the Persona-Chat dataset show that incorporating emotion and entailment improves the accuracy of response selection. We combine our fusion strategies and concept-flow encoding to train a BERT-based model which outperforms the previous methods by margins larger than 2.3% on original personas and 1.9% on revised personas in terms of hits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the Persona-Chat dataset",
        "author": "Souvik Das; Sougata Saha; Rohini K. Srihari",
        "authorids": "/s/souvik-das/; /s/sougata-saha/; /r/rohini-k-srihari/",
        "bibtex": "@inproceedings{das-etal-2022-using,\n    title = \"Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection\",\n    author = \"Das, Souvik  and\n      Saha, Sougata  and\n      Srihari, Rohini K.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.44/\",\n    pages = \"532--541\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.44.pdf",
        "site": "https://aclanthology.org/2022.coling-1.44/",
        "pdf_size": 795353,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5351437426380550681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2022.coling-1.514",
        "title": "Using Structured Content Plans for Fine-grained Syntactic Control in Pretrained Language Model Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Large pretrained language models offer powerful generation capabilities, but cannot be reliably controlled at a sub-sentential level. We propose to make such fine-grained control possible in pretrained LMs by generating text directly from a semantic representation, Abstract Meaning Representation (AMR), which is augmented at the node level with syntactic control tags. We experiment with English-language generation of three modes of syntax relevant to the framing of a sentence - verb voice, verb tense, and realization of human entities - and demonstrate that they can be reliably controlled, even in settings that diverge drastically from the training distribution. These syntactic aspects contribute to how information is framed in text, something that is important for applications such as summarization which aim to highlight salient information.",
        "author": "Fei-Tzin Lee; Miguel Ballesteros; Feng Nan; Kathleen McKeown",
        "authorids": "/f/fei-tzin-lee/; /m/miguel-ballesteros/; /f/feng-nan/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{lee-etal-2022-using,\n    title = \"Using Structured Content Plans for Fine-grained Syntactic Control in Pretrained Language Model Generation\",\n    author = \"Lee, Fei-Tzin  and\n      Ballesteros, Miguel  and\n      Nan, Feng  and\n      McKeown, Kathleen\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.514/\",\n    pages = \"5882--5895\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.514.pdf",
        "site": "https://aclanthology.org/2022.coling-1.514/",
        "pdf_size": 255347,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2301148169269470641&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science Department, Columbia University; Amazon AI Labs; Amazon AI Labs; Computer Science Department, Columbia University",
        "aff_domain": "cs.columbia.edu;amazon.com;amazon.com;cs.columbia.edu",
        "email": "cs.columbia.edu;amazon.com;amazon.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Columbia University;Amazon",
        "aff_unique_dep": "Computer Science Department;Amazon AI Labs",
        "aff_unique_url": "https://www.columbia.edu;https://www.amazon.com",
        "aff_unique_abbr": "Columbia;Amazon AI Labs",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.339",
        "title": "ViNLI: A Vietnamese Corpus for Studies on Open-Domain Natural Language Inference",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Over a decade, the research field of computational linguistics has witnessed the growth of corpora and models for natural language inference (NLI) for rich-resource languages such as English and Chinese. A large-scale and high-quality corpus is necessary for studies on NLI for Vietnamese, which can be considered a low-resource language. In this paper, we introduce ViNLI (Vietnamese Natural Language Inference), an open-domain and high-quality corpus for evaluating Vietnamese NLI models, which is created and evaluated with a strict process of quality control. ViNLI comprises over 30,000 human-annotated premise-hypothesis sentence pairs extracted from more than 800 online news articles on 13 distinct topics. In this paper, we introduce the guidelines for corpus creation which take the specific characteristics of the Vietnamese language in expressing entailment and contradiction into account. To evaluate the challenging level of our corpus, we conduct experiments with state-of-the-art deep neural networks and pre-trained models on our dataset. The best system performance is still far from human performance (a 14.20% gap in accuracy). The ViNLI corpus is a challenging corpus to accelerate progress in Vietnamese computational linguistics. Our corpus is available publicly for research purposes.",
        "author": "Tin Van Huynh; Kiet Van Nguyen; Ngan Luu-Thuy Nguyen",
        "authorids": "/t/tin-van-huynh/; /k/kiet-van-nguyen/; /n/ngan-luu-thuy-nguyen/",
        "bibtex": "@inproceedings{huynh-etal-2022-vinli,\n    title = \"{V}i{NLI}: A {V}ietnamese Corpus for Studies on Open-Domain Natural Language Inference\",\n    author = \"Huynh, Tin Van  and\n      Nguyen, Kiet Van  and\n      Nguyen, Ngan Luu-Thuy\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.339/\",\n    pages = \"3858--3872\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.339.pdf",
        "site": "https://aclanthology.org/2022.coling-1.339/",
        "pdf_size": 839050,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5416511881359589555&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam + Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam + Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam + Vietnam National University, Ho Chi Minh City, Vietnam",
        "aff_domain": "uit.edu.vn;uit.edu.vn;uit.edu.vn",
        "email": "uit.edu.vn;uit.edu.vn;uit.edu.vn",
        "github": "",
        "project": "https://nlp.uit.edu.vn/",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of Information Technology;Vietnam National University",
        "aff_unique_dep": "Faculty of Information Science and Engineering;",
        "aff_unique_url": ";https://www.vnu.edu.vn",
        "aff_unique_abbr": ";VNU",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Ho Chi Minh City",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2022.coling-1.531",
        "title": "View Dialogue in 2D: A Two-stream Model in Time-speaker Perspective for Dialogue Summarization and beyond",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Existing works on dialogue summarization often follow the common practice in document summarization and view the dialogue, which comprises utterances of different speakers, as a single utterance stream ordered by time. However, this single-stream approach without specific attention to the speaker-centered points has limitations in fully understanding the dialogue. To better capture the dialogue information, we propose a 2D view of dialogue based on a time-speaker perspective, where the time and speaker streams of dialogue can be obtained as strengthened input. Based on this 2D view, we present an effective two-stream model called ATM to combine the two streams. Extensive experiments on various summarization datasets demonstrate that ATM significantly surpasses other models regarding diverse metrics and beats the state-of-the-art models on the QMSum dataset in ROUGE scores. Besides, ATM achieves great improvements in summary faithfulness and human evaluation. Moreover, results on machine reading comprehension datasets show the generalization ability of the proposed methods and shed light on other dialogue-based tasks. Our code will be publicly available online.",
        "author": "Keli Xie; Dongchen He; Jiaxin Zhuang; Siyuan Lu; Zhongfeng Wang",
        "authorids": "/k/keli-xie/; /d/dongchen-he/; /j/jiaxin-zhuang/; /s/siyuan-lu/; /z/zhongfeng-wang/",
        "bibtex": "@inproceedings{xie-etal-2022-view,\n    title = \"View Dialogue in 2{D}: A Two-stream Model in Time-speaker Perspective for Dialogue Summarization and beyond\",\n    author = \"Xie, Keli  and\n      He, Dongchen  and\n      Zhuang, Jiaxin  and\n      Lu, Siyuan  and\n      Wang, Zhongfeng\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.531/\",\n    pages = \"6075--6088\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.531.pdf",
        "site": "https://aclanthology.org/2022.coling-1.531/",
        "pdf_size": 762104,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=156304280170758779&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/shakeley/View2dSum",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "School of Electronic Science and Engineering",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.101",
        "title": "Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Domain-specific documents cover terminologies and specialized knowledge. This has been the main challenge of domain-specific document retrieval systems. Previous approaches propose domain-adaptation and transfer learning methods to alleviate this problem. However, these approaches still follow the same document representation method in previous approaches; a document is embedded into a single vector. In this study, we propose VKGDR. VKGDR represents a given corpus into a graph of entities and their relations (known as a virtual knowledge graph) and computes the relevance between queries and documents based on the graph representation. We conduct three experiments 1) domain-specific document retrieval, 2) comparison of our virtual knowledge graph construction method with previous approaches, and 3) ablation study on each component of our virtual knowledge graph. From the results, we see that unsupervised VKGDR outperforms baselines in a zero-shot setting and even outperforms fully-supervised bi-encoder. We also verify that our virtual knowledge graph construction method results in better retrieval performance than previous approaches.",
        "author": "Yeon Seonwoo; Seunghyun Yoon; Franck Dernoncourt; Trung Bui; Alice Oh",
        "authorids": "/y/yeon-seonwoo/; /s/seunghyun-yoon/; /f/franck-dernoncourt/; /t/trung-bui/; /a/alice-oh/",
        "bibtex": "@inproceedings{seonwoo-etal-2022-virtual,\n    title = \"Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval\",\n    author = \"Seonwoo, Yeon  and\n      Yoon, Seunghyun  and\n      Dernoncourt, Franck  and\n      Bui, Trung  and\n      Oh, Alice\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.101/\",\n    pages = \"1169--1178\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.101.pdf",
        "site": "https://aclanthology.org/2022.coling-1.101/",
        "pdf_size": 423117,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1746308623206092228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "KAIST+Adobe Research; Adobe Research; Adobe Research; Adobe Research; KAIST",
        "aff_domain": "kaist.ac.kr;adobe.com;adobe.com;adobe.com;kaist.edu",
        "email": "kaist.ac.kr;adobe.com;adobe.com;adobe.com;kaist.edu",
        "github": "https://github.com/yeonsw/VKGDR",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.kaist.ac.kr;https://research.adobe.com",
        "aff_unique_abbr": "KAIST;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2022.coling-1.11",
        "title": "Visio-Linguistic Brain Encoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Brain encoding aims at reconstructing fMRI brain activity given a stimulus. There exists a plethora of neural encoding models which study brain encoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained language models). Few recent papers have also obtained separate visual and text representation models and performed late-fusion using simple heuristics. However, previous work has failed to explore the co-attentive multi-modal modeling for visual and text reasoning. In this paper, we systematically explore the efficacy of image and multi-modal Transformers for brain encoding. Extensive experiments on two popular datasets, BOLD5000 and Pereira, provide the following insights. (1) We find that VisualBERT, a multi-modal Transformer, significantly outperforms previously proposed single-mode CNNs, image Transformers as well as other previously proposed multi-modal models, thereby establishing new state-of-the-art. (2) The regions such as LPTG, LMTG, LIFG, and STS which have dual functionalities for language and vision, have higher correlation with multi-modal models which reinforces the fact that these models are good at mimicing the human brain behavior. (3) The supremacy of visio-linguistic models raises the question of whether the responses elicited in the visual regions are affected implicitly by linguistic processing even when passively viewing images. Future fMRI tasks can verify this computational insight in an appropriate experimental setting. We make our code publicly available.",
        "author": "Subba Reddy Oota; Jashn Arora; Vijay Rowtula; Manish Gupta; Raju S. Bapi",
        "authorids": "/s/subba-reddy-oota/; /j/jashn-arora/; /v/vijay-rowtula/; /m/manish-gupta/; /r/raju-s-bapi/",
        "bibtex": "@inproceedings{oota-etal-2022-visio,\n    title = \"Visio-Linguistic Brain Encoding\",\n    author = \"Oota, Subba Reddy  and\n      Arora, Jashn  and\n      Rowtula, Vijay  and\n      Gupta, Manish  and\n      Bapi, Raju S.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.11/\",\n    pages = \"116--133\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.11.pdf",
        "site": "https://aclanthology.org/2022.coling-1.11/",
        "pdf_size": 6537972,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17473532278228081138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Inria Bordeaux, France+IIIT Hyderabad, India; IIIT Hyderabad, India; IIIT Hyderabad, India; IIIT Hyderabad, India+Microsoft, Hyderabad, India; IIIT Hyderabad, India",
        "aff_domain": "inria.fr;research.iiit.ac.in;research.iiit.ac.in;microsoft.com;iiit.ac.in",
        "email": "inria.fr;research.iiit.ac.in;research.iiit.ac.in;microsoft.com;iiit.ac.in",
        "github": "",
        "project": "https://tinyurl.com/VLBEncoding",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1+2;1",
        "aff_unique_norm": "Inria Bordeaux;International Institute of Information Technology, Hyderabad;Microsoft",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr/en/centre/bordeaux;https://iiit Hyderabad.ac.in;https://www.microsoft.com",
        "aff_unique_abbr": "Inria;IIIT Hyderabad;Microsoft",
        "aff_campus_unique_index": "0+1;1;1;1+1;1",
        "aff_campus_unique": "Bordeaux;Hyderabad",
        "aff_country_unique_index": "0+1;1;1;1+1;1",
        "aff_country_unique": "France;India"
    },
    {
        "id": "2022.coling-1.568",
        "title": "Visual Information Guided Zero-Shot Paraphrase Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Zero-shot paraphrase generation has drawn much attention as the large-scale high-quality paraphrase corpus is limited. Back-translation, also known as the pivot-based method, is typical to this end. Several works leverage different information as \u201dpivot\u201d such as language, semantic representation and so on. In this paper, we explore using visual information such as image as the \u201dpivot\u201d of back-translation. Different with the pipeline back-translation method, we propose visual information guided zero-shot paraphrase generation (ViPG) based only on paired image-caption data. It jointly trains an image captioning model and a paraphrasing model and leverage the image captioning model to guide the training of the paraphrasing model. Both automatic evaluation and human evaluation show our model can generate paraphrase with good relevancy, fluency and diversity, and image is a promising kind of pivot for zero-shot paraphrase generation.",
        "author": "Zhe Lin; Xiaojun Wan",
        "authorids": "/z/zhe-lin/; /x/xiaojun-wan/",
        "bibtex": "@inproceedings{lin-wan-2022-visual,\n    title = \"Visual Information Guided Zero-Shot Paraphrase Generation\",\n    author = \"Lin, Zhe  and\n      Wan, Xiaojun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.568/\",\n    pages = \"6530--6539\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.568.pdf",
        "site": "https://aclanthology.org/2022.coling-1.568/",
        "pdf_size": 598553,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12371107957949341011&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University; Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0+0;0+0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.492",
        "title": "Visual Prompt Tuning for Few-Shot Text Classification",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Deploying large-scale pre-trained models in the prompt-tuning paradigm has demonstrated promising performance in few-shot learning. Particularly, vision-language pre-training models (VL-PTMs) have been intensively explored in various few-shot downstream tasks. However, most existing works only apply VL-PTMs to visual tasks like image classification, with few attempts being made on language tasks like text classification. In few-shot text classification, a feasible paradigm for deploying VL-PTMs is to align the input samples and their category names via the text encoders. However, it leads to the waste of visual information learned by the image encoders of VL-PTMs. To overcome this drawback, we propose a novel method named Visual Prompt Tuning (VPT). To our best knowledge, this method is the first attempt to deploy VL-PTM in few-shot text classification task. The main idea is to generate the image embeddings w.r.t. category names as visual prompt and then add them to the aligning process. Extensive experiments show that our VPT can achieve significant improvements under both zero-shot and few-shot settings. Importantly, our VPT even outperforms the most recent prompt-tuning methods on five public text classification datasets.",
        "author": "Jingyuan Wen; Yutian Luo; Nanyi Fei; Guoxing Yang; Zhiwu Lu; Hao Jiang; Jie Jiang; Zhao Cao",
        "authorids": "/j/jingyuan-wen/; /y/yutian-luo/; /n/nanyi-fei/; /g/guoxing-yang/; /z/zhiwu-lu/; /h/hao-jiang/; /j/jie-jiang/; /z/zhao-cao/",
        "bibtex": "@inproceedings{wen-etal-2022-visual,\n    title = \"Visual Prompt Tuning for Few-Shot Text Classification\",\n    author = \"Wen, Jingyuan  and\n      Luo, Yutian  and\n      Fei, Nanyi  and\n      Yang, Guoxing  and\n      Lu, Zhiwu  and\n      Jiang, Hao  and\n      Jiang, Jie  and\n      Cao, Zhao\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.492/\",\n    pages = \"5560--5570\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.492.pdf",
        "site": "https://aclanthology.org/2022.coling-1.492/",
        "pdf_size": 4403751,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=284321446196711647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China+Beijing Key Laboratory of Big Data Management and Analysis Methods; Huawei Poisson Lab, Hangzhou, Zhejiang; Huawei Poisson Lab, Hangzhou, Zhejiang; Huawei Poisson Lab, Hangzhou, Zhejiang",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn; ; ; ; ; ; ",
        "email": "ruc.edu.cn;ruc.edu.cn; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;2;2;2",
        "aff_unique_norm": "Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods;Huawei",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Big Data Management and Analysis;Poisson Lab",
        "aff_unique_url": "http://www.ruc.edu.cn;;https://www.huawei.com",
        "aff_unique_abbr": "RUC;;Huawei",
        "aff_campus_unique_index": "0;0;0;0;0;2;2;2",
        "aff_campus_unique": "Beijing;;Hangzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.315",
        "title": "Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "We present a new multimodal dataset called Visual Recipe Flow, which enables us to learn a cooking action result for each object in a recipe text. The dataset consists of object state changes and the workflow of the recipe text. The state change is represented as an image pair, while the workflow is represented as a recipe flow graph. We developed a web interface to reduce human annotation costs. The dataset allows us to try various applications, including multimodal information retrieval.",
        "author": "Keisuke Shirai; Atsushi Hashimoto; Taichi Nishimura; Hirotaka Kameko; Shuhei Kurita; Yoshitaka Ushiku; Shinsuke Mori",
        "authorids": "/k/keisuke-shirai/; /a/atsushi-hashimoto/; /t/taichi-nishimura/; /h/hirotaka-kameko/; /s/shuhei-kurita/; /y/yoshitaka-ushiku/; /s/shinsuke-mori/",
        "bibtex": "@inproceedings{shirai-etal-2022-visual,\n    title = \"Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows\",\n    author = \"Shirai, Keisuke  and\n      Hashimoto, Atsushi  and\n      Nishimura, Taichi  and\n      Kameko, Hirotaka  and\n      Kurita, Shuhei  and\n      Ushiku, Yoshitaka  and\n      Mori, Shinsuke\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.315/\",\n    pages = \"3570--3577\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.315.pdf",
        "site": "https://aclanthology.org/2022.coling-1.315/",
        "pdf_size": 3598281,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4999342221480774982&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Kyoto University; OMRON SINIC X Corporation; Kyoto University; Kyoto University; RIKEN AIP, JST PRESTO; OMRON SINIC X Corporation; Kyoto University",
        "aff_domain": "st.kyoto-u.ac.jp;sinicx.com;st.kyoto-u.ac.jp;i.kyoto-u.ac.jp;riken.jp;sinicx.com;i.kyoto-u.ac.jp",
        "email": "st.kyoto-u.ac.jp;sinicx.com;st.kyoto-u.ac.jp;i.kyoto-u.ac.jp;riken.jp;sinicx.com;i.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;0;2;1;0",
        "aff_unique_norm": "Kyoto University;OMRON Corporation;RIKEN",
        "aff_unique_dep": ";;Advanced Institute for Computational Science",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.omron.com;https://www.riken.jp/en/",
        "aff_unique_abbr": "Kyoto U;OMRON;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.432",
        "title": "Vocabulary-informed Language Encoding",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A Multilingual model relies on language encodings to identify input languages because the multilingual model has to distinguish between the input and output languages or among all the languages for cross-lingual tasks. Furthermore, we find that language encodings potentially refine multiple morphologies of different languages to form a better isomorphic space for multilinguality. To leverage this observation, we present a method to compute a vocabulary-informed language encoding as the language representation, for a required language, considering a local vocabulary covering an acceptable amount of the most frequent word embeddings in this language. In our experiments, our method can consistently improve the performance of multilingual models on unsupervised neural machine translation and cross-lingual embedding.",
        "author": "Xi Ai; Bin Fang",
        "authorids": "/x/xi-ai/; /b/bin-fang/",
        "bibtex": "@inproceedings{ai-fang-2022-vocabulary,\n    title = \"Vocabulary-informed Language Encoding\",\n    author = \"Ai, Xi  and\n      Fang, Bin\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.432/\",\n    pages = \"4883--4891\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.432.pdf",
        "site": "https://aclanthology.org/2022.coling-1.432/",
        "pdf_size": 472286,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1715200317005582670&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science, University of Chonqing; College of Computer Science, University of Chonqing",
        "aff_domain": "gmail.com;cqu.edu.cn",
        "email": "gmail.com;cqu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Chonqing",
        "aff_unique_dep": "College of Computer Science",
        "aff_unique_url": "http://www.cqu.edu.cn/",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.421",
        "title": "WARM: A Weakly (+Semi) Supervised Math Word Problem Solver",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solving MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the current state-of-the-art weakly-supervised approach, on the standard Math23K and AllArith datasets respectively. Additionally, we curate and release new datasets of roughly 10k MWPs each in English and in Hindi (a low-resource language). These datasets are suitable for training weakly supervised models. We also present an extension of our model to semi-supervised learning and present further improvements on results, along with insights.",
        "author": "Oishik Chatterjee; Isha Pandey; Aashish Waikar; Vishwajeet Kumar; Ganesh Ramakrishnan",
        "authorids": "/o/oishik-chatterjee/; /i/isha-pandey/; /a/aashish-waikar/; /v/vishwajeet-kumar/; /g/ganesh-ramakrishnan/",
        "bibtex": "@inproceedings{chatterjee-etal-2022-warm,\n    title = \"{WARM}: A Weakly (+{S}emi) Supervised Math Word Problem Solver\",\n    author = \"Chatterjee, Oishik  and\n      Pandey, Isha  and\n      Waikar, Aashish  and\n      Kumar, Vishwajeet  and\n      Ramakrishnan, Ganesh\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.421/\",\n    pages = \"4753--4764\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.421.pdf",
        "site": "https://aclanthology.org/2022.coling-1.421/",
        "pdf_size": 411710,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=312154593808729542&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Flipkart+IIT Bombay; IIT Bombay; Quadeye+IIT Bombay; IBM India Research Lab; IIT Bombay",
        "aff_domain": "gmail.com;gmail.com;gmail.com;in.ibm.com;cse.iitb.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;in.ibm.com;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;2+1;3;1",
        "aff_unique_norm": "Flipkart;Indian Institute of Technology Bombay;Quadeye;IBM Research",
        "aff_unique_dep": ";;;Research Lab",
        "aff_unique_url": "https://www.flipkart.com;https://www.iitb.ac.in;;https://www.ibm.com/research",
        "aff_unique_abbr": "Flipkart;IITB;;IBM",
        "aff_campus_unique_index": "1;1;1;2;1",
        "aff_campus_unique": ";Mumbai;India",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "India;"
    },
    {
        "id": "2022.coling-1.150",
        "title": "Weakly Supervised Formula Learner for Solving Mathematical Problems",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Mathematical reasoning task is a subset of the natural language question answering task. Existing work suggested solving this task with a two-phase approach, where the model first predicts formulas from questions and then calculates answers from such formulas. This approach achieved desirable performance in existing work. However, its reliance on annotated formulas as intermediate labels throughout its training limited its application. In this work, we put forward the idea to enable models to learn optimal formulas autonomously. We proposed Weakly Supervised Formula Learner, a learning framework that drives the formula exploration with weak supervision from the final answers to mathematical problems. Our experiments are conducted on two representative mathematical reasoning datasets MathQA and Math23K. On MathQA, our method outperformed baselines trained on complete yet imperfect formula annotations. On Math23K, our method outperformed other weakly supervised learning methods.",
        "author": "Yuxuan Wu; Hideki Nakayama",
        "authorids": "/y/yuxuan-wu/; /h/hideki-nakayama/",
        "bibtex": "@inproceedings{wu-nakayama-2022-weakly,\n    title = \"Weakly Supervised Formula Learner for Solving Mathematical Problems\",\n    author = \"Wu, Yuxuan  and\n      Nakayama, Hideki\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.150/\",\n    pages = \"1743--1752\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.150.pdf",
        "site": "https://aclanthology.org/2022.coling-1.150/",
        "pdf_size": 1024203,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3096951935982501780&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The University of Tokyo; The University of Tokyo",
        "aff_domain": "nlab.ci.i.u-tokyo.ac.jp;nlab.ci.i.u-tokyo.ac.jp",
        "email": "nlab.ci.i.u-tokyo.ac.jp;nlab.ci.i.u-tokyo.ac.jp",
        "github": "https://github.com/evan-ak/wsfl",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.coling-1.105",
        "title": "Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The world of pronouns is changing \u2013 from a closed word class with few members to an open set of terms to reflect identities. However, Natural Language Processing (NLP) barely reflects this linguistic shift, resulting in the possible exclusion of non-binary users, even though recent work outlined the harms of gender-exclusive language technology. The current modeling of 3rd person pronouns is particularly problematic. It largely ignores various phenomena like neopronouns, i.e., novel pronoun sets that are not (yet) widely established. This omission contributes to the discrimination of marginalized and underrepresented groups, e.g., non-binary individuals. It thus prevents gender equality, one of the UN\u2019s sustainable development goals (goal 5). Further, other identity-expressions beyond gender are ignored by current NLP technology. This paper provides an overview of 3rd person pronoun issues for NLP. Based on our observations and ethical considerations, we define a series of five desiderata for modeling pronouns in language technology, which we validate through a survey. We evaluate existing and novel modeling approaches w.r.t. these desiderata qualitatively and quantify the impact of a more discrimination-free approach on an established benchmark dataset.",
        "author": "Anne Lauscher; Archie Crowley; Dirk Hovy",
        "authorids": "/a/anne-lauscher/; /a/archie-crowley/; /d/dirk-hovy/",
        "bibtex": "@inproceedings{lauscher-etal-2022-welcome,\n    title = \"Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender\",\n    author = \"Lauscher, Anne  and\n      Crowley, Archie  and\n      Hovy, Dirk\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.105/\",\n    pages = \"1221--1232\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.105.pdf",
        "site": "https://aclanthology.org/2022.coling-1.105/",
        "pdf_size": 472608,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15229196487512780481&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "MilaNLP, Universit\u00e0 Luigi Bocconi, Milan, Italy; Linguistics, University of South Carolina, Columbia, SC, USA; MilaNLP, Universit\u00e0 Luigi Bocconi, Milan, Italy",
        "aff_domain": "unibocconi.it;sc.edu;unibocconi.it",
        "email": "unibocconi.it;sc.edu;unibocconi.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Universit\u00e0 Luigi Bocconi;University of South Carolina",
        "aff_unique_dep": "MilaNLP;Linguistics",
        "aff_unique_url": "https://www.unibocconi.eu;https://www.sc.edu",
        "aff_unique_abbr": ";USC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Milan;Columbia",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "2022.coling-1.391",
        "title": "When the Student Becomes the Master: Learning Better and Smaller Monolingual Models from mBERT",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this research, we present pilot experiments to distil monolingual models from a jointly trained model for 102 languages (mBERT). We demonstrate that it is possible for the target language to outperform the original model, even with a basic distillation setup. We evaluate our methodology for 6 languages with varying amounts of resources and language families.",
        "author": "Pranaydeep Singh; Els Lefever",
        "authorids": "/p/pranaydeep-singh/; /e/els-lefever/",
        "bibtex": "@inproceedings{singh-lefever-2022-student,\n    title = \"When the Student Becomes the Master: Learning Better and Smaller Monolingual Models from m{BERT}\",\n    author = \"Singh, Pranaydeep  and\n      Lefever, Els\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.391/\",\n    pages = \"4434--4441\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.391.pdf",
        "site": "https://aclanthology.org/2022.coling-1.391/",
        "pdf_size": 629882,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4749369463349793907&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "LT3, Language and Translation Technology Team, Department of Translation, Interpreting and Communication \u2013 Ghent University; LT3, Language and Translation Technology Team, Department of Translation, Interpreting and Communication \u2013 Ghent University",
        "aff_domain": "ugent.be;ugent.be",
        "email": "ugent.be;ugent.be",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "Department of Translation, Interpreting and Communication",
        "aff_unique_url": "https://www.ugent.be",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2022.coling-1.598",
        "title": "When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and Its Intensity",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Prerecorded laughter accompanying dialog in comedy TV shows encourages the audience to laugh by clearly marking humorous moments in the show. We present an approach for automatically detecting humor in the Friends TV show using multimodal data. Our model is capable of recognizing whether an utterance is humorous or not and assess the intensity of it. We use the prerecorded laughter in the show as annotation as it marks humor and the length of the audience\u2019s laughter tells us how funny a given joke is. We evaluate the model on episodes the model has not been exposed to during the training phase. Our results show that the model is capable of correctly detecting whether an utterance is humorous 78% of the time and how long the audience\u2019s laughter reaction should last with a mean absolute error of 600 milliseconds.",
        "author": "Khalid Alnajjar; Mika H\u00e4m\u00e4l\u00e4inen; J\u00f6rg Tiedemann; Jorma Laaksonen; Mikko Kurimo",
        "authorids": "/k/khalid-alnajjar/; /m/mika-hamalainen/; /j/jorg-tiedemann/; /j/jorma-laaksonen/; /m/mikko-kurimo/",
        "bibtex": "@inproceedings{alnajjar-etal-2022-laugh,\n    title = \"When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and Its Intensity\",\n    author = {Alnajjar, Khalid  and\n      H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika  and\n      Tiedemann, J{\\\"o}rg  and\n      Laaksonen, Jorma  and\n      Kurimo, Mikko},\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.598/\",\n    pages = \"6875--6886\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.598.pdf",
        "site": "https://aclanthology.org/2022.coling-1.598/",
        "pdf_size": 7570506,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16678750939438171705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of Helsinki, Finland + Rootroo Oy, Finland; University of Helsinki, Finland + Rootroo Oy, Finland; University of Helsinki, Finland; Aalto University, Finland; Aalto University, Finland",
        "aff_domain": "helsinki.fi;helsinki.fi;helsinki.fi;aalto.fi;aalto.fi",
        "email": "helsinki.fi;helsinki.fi;helsinki.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;2;2",
        "aff_unique_norm": "University of Helsinki;Rootroo Oy;Aalto University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.helsinki.fi;;https://www.aalto.fi",
        "aff_unique_abbr": "UH;;Aalto",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "2022.coling-1.413",
        "title": "Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Probing studies have extensively explored where in neural language models linguistic information is located. The standard approach to interpreting the results of a probing classifier is to focus on the layers whose representations give the highest performance on the probing task. We propose an alternative method that asks where the task-relevant information emerges in the model. Our framework consists of a family of metrics that explicitly model local information gain relative to the previous layer and each layer\u2019s contribution to the model\u2019s overall performance. We apply the new metrics to two pairs of syntactic probing tasks with different degrees of complexity and find that the metrics confirm the expected ordering only for one of the pairs. Our local metrics show a massive dominance of the first layers, indicating that the features that contribute the most to our probing tasks are not as high-level as global metrics suggest.",
        "author": "Jenny Kunz; Marco Kuhlmann",
        "authorids": "/j/jenny-kunz/; /m/marco-kuhlmann/",
        "bibtex": "@inproceedings{kunz-kuhlmann-2022-linguistic,\n    title = \"Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers\",\n    author = \"Kunz, Jenny  and\n      Kuhlmann, Marco\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.413/\",\n    pages = \"4664--4676\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.413.pdf",
        "site": "https://aclanthology.org/2022.coling-1.413/",
        "pdf_size": 985564,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17045998600749640107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of Computer and Information Science, Link\u00f6ping University; Dept. of Computer and Information Science, Link\u00f6ping University",
        "aff_domain": "liu.se;liu.se",
        "email": "liu.se;liu.se",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Link\u00f6ping University",
        "aff_unique_dep": "Dept. of Computer and Information Science",
        "aff_unique_url": "https://www.liu.se",
        "aff_unique_abbr": "LiU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "id": "2022.coling-1.82",
        "title": "Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Nowadays, deep-learning based NLP models are usually trained with large-scale third-party data which can be easily injected with malicious backdoors. Thus, BackDoor Attack (BDA) study has become a trending research to help promote the robustness of an NLP system. Text-based BDA aims to train a poisoned model with both clean and poisoned texts to perform normally on clean inputs while being misled to predict those trigger-embedded texts as target labels set by attackers. Previous works usually choose fixed Positions-to-Poison (P2P) first, then add triggers upon those positions such as letter insertion or deletion. However, considering the positions of words with important semantics may vary in different contexts, fixed P2P models are severely limited in flexibility and performance. We study the text-based BDA from the perspective of automatically and dynamically selecting P2P from contexts. We design a novel Locator model which can predict P2P dynamically without human intervention. Based on the predicted P2P, four effective strategies are introduced to show the BDA performance. Experiments on two public datasets show both tinier test accuracy gap on clean data and higher attack success rate on poisoned ones. Human evaluation with volunteers also shows the P2P predicted by our model are important for classification. Source code is available at https://github.com/jncsnlp/LocatorModel",
        "author": "Heng-yang Lu; Chenyou Fan; Jun Yang; Cong Hu; Wei Fang; Xiao-jun Wu",
        "authorids": "/h/heng-yang-lu/; /c/chenyou-fan/; /j/jun-yang/; /c/cong-hu/; /w/wei-fang/; /x/xiao-jun-wu/",
        "bibtex": "@inproceedings{lu-etal-2022-attack,\n    title = \"Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications\",\n    author = \"Lu, Heng-yang  and\n      Fan, Chenyou  and\n      Yang, Jun  and\n      Hu, Cong  and\n      Fang, Wei  and\n      Wu, Xiao-jun\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.82/\",\n    pages = \"984--993\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.82.pdf",
        "site": "https://aclanthology.org/2022.coling-1.82/",
        "pdf_size": 823975,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=56542590199951404&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, China; School of Artificial Intelligence, South China Normal University, China; Marcpoint Co.,Ltd., China; Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, China; Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, China; Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, China",
        "aff_domain": "jiangnan.edu.cn;scnu.edu.cn;126.com;jiangnan.edu.cn;jiangnan.edu.cn;jiangnan.edu.cn",
        "email": "jiangnan.edu.cn;scnu.edu.cn;126.com;jiangnan.edu.cn;jiangnan.edu.cn;jiangnan.edu.cn",
        "github": "https://github.com/jncsnlp/LocatorModel",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;0;0",
        "aff_unique_norm": "Jiangnan University;South China Normal University;Marcpoint Co., Ltd.",
        "aff_unique_dep": "Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.jiangnan.edu.cn/;http://www.scnu.edu.cn;",
        "aff_unique_abbr": ";SCNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.coling-1.577",
        "title": "Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In a hate speech detection model, we should consider two critical aspects in addition to detection performance\u2013bias and explainability. Hate speech cannot be identified based solely on the presence of specific words; the model should be able to reason like humans and be explainable. To improve the performance concerning the two aspects, we propose Masked Rationale Prediction (MRP) as an intermediate task. MRP is a task to predict the masked human rationales\u2013snippets of a sentence that are grounds for human judgment\u2013by referring to surrounding tokens combined with their unmasked rationales. As the model learns its reasoning ability based on rationales by MRP, it performs hate speech detection robustly in terms of bias and explainability. The proposed method generally achieves state-of-the-art performance in various metrics, demonstrating its effectiveness for hate speech detection. Warning: This paper contains samples that may be upsetting.",
        "author": "Jiyun Kim; Byounghan Lee; Kyung-Ah Sohn",
        "authorids": "/j/jiyun-kim/; /b/byounghan-lee/; /k/kyung-ah-sohn/",
        "bibtex": "@inproceedings{kim-etal-2022-hate,\n    title = \"Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection\",\n    author = \"Kim, Jiyun  and\n      Lee, Byounghan  and\n      Sohn, Kyung-Ah\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.577/\",\n    pages = \"6644--6655\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.577.pdf",
        "site": "https://aclanthology.org/2022.coling-1.577/",
        "pdf_size": 2969700,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7356282306710184267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Ajou University; Ajou University; Ajou University",
        "aff_domain": "ajou.ac.kr;ajou.ac.kr;ajou.ac.kr",
        "email": "ajou.ac.kr;ajou.ac.kr;ajou.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ajou University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ajou.ac.kr",
        "aff_unique_abbr": "Ajou",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.coling-1.314",
        "title": "WikiHan: A New Comparative Dataset for Chinese Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Most comparative datasets of Chinese varieties are not digital; however, Wiktionary includes a wealth of transcriptions of words from these varieties. The usefulness of these data is limited by the fact that they use a wide range of variety-specific romanizations, making data difficult to compare. The current work collects this data into a single constituent (IPA, or International Phonetic Alphabet) and structured form (TSV) for use in comparative linguistics and Chinese NLP. At the time of writing, the dataset contains 67,943 entries across 8 varieties and Middle Chinese. The dataset is validated on a protoform reconstruction task using an encoder-decoder cross-attention architecture (Meloni et al 2021), achieving an accuracy of 54.11%, a PER (phoneme error rate) of 17.69%, and a FER (feature error rate) of 6.60%.",
        "author": "Kalvin Chang; Chenxuan Cui; Youngmin Kim; David R. Mortensen",
        "authorids": "/k/kalvin-chang/; /c/chenxuan-cui/; /y/youngmin-kim/; /d/david-r-mortensen/",
        "bibtex": "@inproceedings{chang-etal-2022-wikihan,\n    title = \"{W}iki{H}an: A New Comparative Dataset for {C}hinese Languages\",\n    author = \"Chang, Kalvin  and\n      Cui, Chenxuan  and\n      Kim, Youngmin  and\n      Mortensen, David R.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.314/\",\n    pages = \"3563--3569\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.314.pdf",
        "site": "https://aclanthology.org/2022.coling-1.314/",
        "pdf_size": 296013,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16323942915385725534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/cmu-llab/wikihan",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.310",
        "title": "Wizard of Tasks: A Novel Conversational Dataset for Solving Real-World Tasks in Conversational Settings",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Conversational Task Assistants (CTAs) are conversational agents whose goal is to help humans perform real-world tasks. CTAs can help in exploring available tasks, answering task-specific questions and guiding users through step-by-step instructions. In this work, we present Wizard of Tasks, the first corpus of such conversations in two domains: Cooking and Home Improvement. We crowd-sourced a total of 549 conversations (18,077 utterances) with an asynchronous Wizard-of-Oz setup, relying on recipes from WholeFoods Market for the cooking domain, and WikiHow articles for the home improvement domain. We present a detailed data analysis and show that the collected data can be a valuable and challenging resource for CTAs in two tasks: Intent Classification (IC) and Abstractive Question Answering (AQA). While on IC we acquired a high performing model (>85% F1), on AQA the performance is far from being satisfactory (~27% BertScore-F1), suggesting that more work is needed to solve the task of low-resource AQA.",
        "author": "Jason Ingyu Choi; Saar Kuzi; Nikhita Vedula; Jie Zhao; Giuseppe Castellucci; Marcus Collins; Shervin Malmasi; Oleg Rokhlenko; Eugene Agichtein",
        "authorids": "/j/jason-ingyu-choi/; /s/saar-kuzi/; /n/nikhita-vedula/; /j/jie-zhao/; /g/giuseppe-castellucci/; /m/marcus-collins/; /s/shervin-malmasi/; /o/oleg-rokhlenko/; /e/eugene-agichtein/",
        "bibtex": "@inproceedings{choi-etal-2022-wizard,\n    title = \"Wizard of Tasks: A Novel Conversational Dataset for Solving Real-World Tasks in Conversational Settings\",\n    author = \"Choi, Jason Ingyu  and\n      Kuzi, Saar  and\n      Vedula, Nikhita  and\n      Zhao, Jie  and\n      Castellucci, Giuseppe  and\n      Collins, Marcus  and\n      Malmasi, Shervin  and\n      Rokhlenko, Oleg  and\n      Agichtein, Eugene\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.310/\",\n    pages = \"3514--3529\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.310.pdf",
        "site": "https://aclanthology.org/2022.coling-1.310/",
        "pdf_size": 1718439,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13093887562441864561&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon; Amazon; Amazon; Amazon; Amazon; Amazon; Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "https://registry.opendata.aws/wizard-of-tasks/",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Amazon.com, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.coling-1.357",
        "title": "Word Sense Disambiguation with Knowledge-Enhanced and Local Self-Attention-based Extractive Sense Comprehension",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Word sense disambiguation (WSD), identifying the most suitable meaning of ambiguous words in the given contexts according to a predefined sense inventory, is one of the most classical and challenging tasks in natural language processing. Benefiting from the powerful ability of deep neural networks, WSD has achieved a great advancement in recent years. Reformulating WSD as a text span extraction task is an effective approach, which accepts a sentence context of an ambiguous word together with all definitions of its candidate senses simultaneously, and requires to extract the text span corresponding with the right sense. However, the approach merely depends on a short definition to learn sense representation, which neglects abundant semantic knowledge from related senses and leads to data-inefficient learning and suboptimal WSD performance. To address the limitations, we propose a novel WSD method with Knowledge-Enhanced and Local Self-Attention-based Extractive Sense Comprehension (KELESC). Specifically, a knowledge-enhanced method is proposed to enrich semantic representation by incorporating additional examples and definitions of the related senses in WordNet. Then, in order to avoid the huge computing complexity induced by the additional information, a local self-attention mechanism is utilized to constrain attention to be local, which allows longer input texts without large-scale computing burdens. Extensive experimental results demonstrate that KELESC achieves better performance than baseline models on public benchmark datasets.",
        "author": "Guobiao Zhang; Wenpeng Lu; Xueping Peng; Shoujin Wang; Baoshuo Kan; Rui Yu",
        "authorids": "/g/guobiao-zhang/; /w/wenpeng-lu/; /x/xueping-peng/; /s/shoujin-wang/; /b/baoshuo-kan/; /r/rui-yu/",
        "bibtex": "@inproceedings{zhang-etal-2022-word,\n    title = \"Word Sense Disambiguation with Knowledge-Enhanced and Local Self-Attention-based Extractive Sense Comprehension\",\n    author = \"Zhang, Guobiao  and\n      Lu, Wenpeng  and\n      Peng, Xueping  and\n      Wang, Shoujin  and\n      Kan, Baoshuo  and\n      Yu, Rui\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.357/\",\n    pages = \"4061--4070\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.357.pdf",
        "site": "https://aclanthology.org/2022.coling-1.357/",
        "pdf_size": 2149218,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15473225995173807531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; School of Computer, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Australian Artificial Intelligence Institute, University of Technology Sydney, Sydney, Australia; Data Science Institute, University of Technology Sydney, Sydney, Australia; School of Computer, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; School of Computer, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China",
        "aff_domain": "foxmail.com;qlu.edu.cn;uts.edu.au;uts.edu.au;stu.qlu.edu.cn;foxmail.com",
        "email": "foxmail.com;qlu.edu.cn;uts.edu.au;uts.edu.au;stu.qlu.edu.cn;foxmail.com",
        "github": "https://github.com/Stubborn-z/KELESC",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Qilu University of Technology;University of Technology Sydney",
        "aff_unique_dep": "School of Computer;Australian Artificial Intelligence Institute",
        "aff_unique_url": "http://www.qilu.edu.cn;https://www.uts.edu.au",
        "aff_unique_abbr": "QUT;UTS",
        "aff_campus_unique_index": "0;0;1;1;0;0",
        "aff_campus_unique": "Jinan;Sydney",
        "aff_country_unique_index": "0;0;1;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.coling-1.390",
        "title": "WordNet-QU: Development of a Lexical Database for Quechua Varieties",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In the effort to minimize the risk of extinction of a language, linguistic resources are fundamental. Quechua, a low-resource language from South America, is a language spoken by millions but, despite several efforts in the past, still lacks the resources necessary to build high-performance computational systems. In this article, we present WordNet-QU which signifies the inclusion of Quechua in a well-known lexical database called wordnet. We propose WordNet-QU to be included as an extension to wordnet after demonstrating a manually-curated collection of multiple digital resources for lexical use in Quechua. Our work uses the synset alignment algorithm to compare Quechua to its geographically nearest high-resource language, Spanish. Altogether, we propose a total of 28,582 unique synset IDs divided according to region like so: 20510 for Southern Quechua, 5993 for Central Quechua, 1121 for Northern Quechua, and 958 for Amazonian Quechua.",
        "author": "Nelsi Melgarejo; Rodolfo Zevallos; Hector Gomez; John E. Ortega",
        "authorids": "/n/nelsi-melgarejo/; /r/rodolfo-zevallos/; /h/hector-gomez/; /j/john-e-ortega/",
        "bibtex": "@inproceedings{melgarejo-etal-2022-wordnet,\n    title = \"{W}ord{N}et-{QU}: Development of a Lexical Database for {Q}uechua Varieties\",\n    author = \"Melgarejo, Nelsi  and\n      Zevallos, Rodolfo  and\n      Gomez, Hector  and\n      Ortega, John E.\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.390/\",\n    pages = \"4429--4433\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.390.pdf",
        "site": "https://aclanthology.org/2022.coling-1.390/",
        "pdf_size": 173325,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5487093904949583692&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Pontifical Catholic University of Peru; Pompeu Fabra University; Pontifical Catholic University of Peru; Northeastern University",
        "aff_domain": "pucp.edu.pe;upf.edu;pucp.edu.pe;northeastern.edu",
        "email": "pucp.edu.pe;upf.edu;pucp.edu.pe;northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Pontifical Catholic University of Peru;Pompeu Fabra University;Northeastern University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://pucp.edu.pe;https://www.upf.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "PUCP;UPF;NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Peru;Spain;United States"
    },
    {
        "id": "2022.coling-1.482",
        "title": "Yet Another Format of Universal Dependencies for Korean",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "In this study, we propose a morpheme-based scheme for Korean dependency parsing and adopt the proposed scheme to Universal Dependencies. We present the linguistic rationale that illustrates the motivation and the necessity of adopting the morpheme-based format, and develop scripts that convert between the original format used by Universal Dependencies and the proposed morpheme-based format automatically. The effectiveness of the proposed format for Korean dependency parsing is then testified by both statistical and neural models, including UDPipe and Stanza, with our carefully constructed morpheme-based word embedding for Korean. morphUD outperforms parsing results for all Korean UD treebanks, and we also present detailed error analysis.",
        "author": "Yige Chen; Eunkyul Leah Jo; Yundong Yao; KyungTae Lim; Miikka Silfverberg; Francis M. Tyers; Jungyeul Park",
        "authorids": "/y/yige-chen/; /e/eunkyul-leah-jo/; /y/yundong-yao/; /k/kyungtae-lim/; /m/miikka-silfverberg/; /f/francis-tyers/; /j/jungyeul-park/",
        "bibtex": "@inproceedings{chen-etal-2022-yet,\n    title = \"Yet Another Format of {U}niversal {D}ependencies for {K}orean\",\n    author = \"Chen, Yige  and\n      Jo, Eunkyul Leah  and\n      Yao, Yundong  and\n      Lim, KyungTae  and\n      Silfverberg, Miikka  and\n      Tyers, Francis M.  and\n      Park, Jungyeul\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.482/\",\n    pages = \"5432--5437\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.482.pdf",
        "site": "https://aclanthology.org/2022.coling-1.482/",
        "pdf_size": 1755923,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17007367278300668269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Chinese University of Hong Kong, Hong Kong; The University of British Columbia, Canada; Hanbat National University & TeddySum, South Korea; Indiana University, USA; The University of British Columbia, Canada; Indiana University, USA; The University of British Columbia, Canada",
        "aff_domain": "link.cuhk.edu.hk;student.ubc.ca;student.ubc.ca;hanbat.ac.kr;iu.edu;mail.ubc.ca;mail.ubc.ca",
        "email": "link.cuhk.edu.hk;student.ubc.ca;student.ubc.ca;hanbat.ac.kr;iu.edu;mail.ubc.ca;mail.ubc.ca",
        "github": "",
        "project": "https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;1;3;1",
        "aff_unique_norm": "The Chinese University of Hong Kong;University of British Columbia;Hanbat National University;Indiana University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.ubc.ca;http://www.hanbat.ac.kr;https://www.indiana.edu",
        "aff_unique_abbr": "CUHK;UBC;HNU;IU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;2;3;1;3;1",
        "aff_country_unique": "China;Canada;South Korea;United States"
    },
    {
        "id": "2022.coling-1.392",
        "title": "Zero-shot Disfluency Detection for Indian Languages",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Disfluencies that appear in the transcriptions from automatic speech recognition systems tend to impair the performance of downstream NLP tasks. Disfluency correction models can help alleviate this problem. However, the unavailability of labeled data in low-resource languages impairs progress. We propose using a pretrained multilingual model, finetuned only on English disfluencies, for zero-shot disfluency detection in Indian languages. We present a detailed pipeline to synthetically generate disfluent text and create evaluation datasets for four Indian languages: Bengali, Hindi, Malayalam, and Marathi. Even in the zero-shot setting, we obtain F1 scores of 75 and higher on five disfluency types across all four languages. We also show the utility of synthetically generated disfluencies by evaluating on real disfluent text in Bengali, Hindi, and Marathi. Finetuning the multilingual model on additional synthetic Hindi disfluent text nearly doubles the number of exact matches and yields a 20-point boost in F1 scores when evaluated on real Hindi disfluent text, compared to training with only English disfluent text.",
        "author": "Rohit Kundu; Preethi Jyothi; Pushpak Bhattacharyya",
        "authorids": "/r/rohit-kundu/; /p/preethi-jyothi/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{kundu-etal-2022-zero,\n    title = \"Zero-shot Disfluency Detection for {I}ndian Languages\",\n    author = \"Kundu, Rohit  and\n      Jyothi, Preethi  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.392/\",\n    pages = \"4442--4454\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.392.pdf",
        "site": "https://aclanthology.org/2022.coling-1.392/",
        "pdf_size": 338377,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8296640049177359109&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Bombay; Department of Computer Science and Engineering, Indian Institute of Technology Bombay; Department of Computer Science and Engineering, Indian Institute of Technology Bombay",
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Bombay",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bombay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.coling-1.356",
        "title": "Zero-shot Script Parsing",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Script knowledge is useful to a variety of NLP tasks. However, existing resources only cover a small number of activities, limiting its practical usefulness. In this work, we propose a zero-shot learning approach to script parsing, the task of tagging texts with scenario-specific event and participant types, which enables us to acquire script knowledge without domain-specific annotations. We (1) learn representations of potential event and participant mentions by promoting cluster consistency according to the annotated data; (2) perform clustering on the event / participant candidates from unannotated texts that belongs to an unseen scenario. The model achieves 68.1/74.4 average F1 for event / participant parsing, respectively, outperforming a previous CRF model that, in contrast, has access to scenario-specific supervision. We also evaluate the model by testing on a different corpus, where it achieved 55.5/54.0 average F1 for event / participant parsing.",
        "author": "Fangzhou Zhai; Vera Demberg; Alexander Koller",
        "authorids": "/f/fangzhou-zhai/; /v/vera-demberg/; /a/alexander-koller/",
        "bibtex": "@inproceedings{zhai-etal-2022-zero,\n    title = \"Zero-shot Script Parsing\",\n    author = \"Zhai, Fangzhou  and\n      Demberg, Vera  and\n      Koller, Alexander\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.356/\",\n    pages = \"4049--4060\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.356.pdf",
        "site": "https://aclanthology.org/2022.coling-1.356/",
        "pdf_size": 658792,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2238790220553578263&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Language Science and Technology, Saarland Informatics Campus, Saarland University; Dept. of Language Science and Technology, Saarland Informatics Campus, Saarland University; Dept. of Language Science and Technology, Saarland Informatics Campus, Saarland University",
        "aff_domain": "coli.uni-saarland.de;coli.uni-saarland.de;coli.uni-saarland.de",
        "email": "coli.uni-saarland.de;coli.uni-saarland.de;coli.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "Dept. of Language Science and Technology",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "Saarland U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Saarland Informatics Campus",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.306",
        "title": "[RETRACTED] NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Driven by deep learning breakthroughs, natural language generation (NLG) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce NLG-Metricverse\u2014an end-to-end open-source library for NLG evaluation based on Python. Our framework provides a living collection of NLG metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. NLG-Metricverse aims to increase the comparability and replicability of NLG research, hopefully stimulating new contributions in the area.",
        "author": "Giacomo Frisoni; Antonella Carbonaro; Gianluca Moro; Andrea Zammarchi; Marco Avagnano",
        "authorids": "/g/giacomo-frisoni/; /a/antonella-carbonaro/; /g/gianluca-moro/; /a/andrea-zammarchi/; /m/marco-avagnano/",
        "bibtex": "",
        "pdf": "https://aclanthology.org/2022.coling-1.306.pdf",
        "site": "https://aclanthology.org/2022.coling-1.306/",
        "pdf_size": 1173084,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17212796302500899036&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering (DISI), University of Bologna; Department of Computer Science and Engineering (DISI), University of Bologna; Department of Computer Science and Engineering (DISI), University of Bologna; Department of Computer Science and Engineering (DISI), University of Bologna; Department of Computer Science and Engineering (DISI), University of Bologna",
        "aff_domain": "unibo.it;unibo.it;unibo.it;studio.unibo.it;studio.unibo.it",
        "email": "unibo.it;unibo.it;unibo.it;studio.unibo.it;studio.unibo.it",
        "github": "https://github.com/disi-unibo-nlp/nlg-metricverse",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Bologna",
        "aff_unique_dep": "Department of Computer Science and Engineering (DISI)",
        "aff_unique_url": "https://www.unibo.it",
        "aff_unique_abbr": "UNIBO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.coling-1.446",
        "title": "ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "Recently, a new training oaxe loss has proven effective to ameliorate the effect of multimodality for non-autoregressive translation (NAT), which removes the penalty of word order errors in the standard cross-entropy loss. Starting from the intuition that reordering generally occurs between phrases, we extend oaxe by only allowing reordering between ngram phrases and still requiring a strict match of word order within the phrases. Extensive experiments on NAT benchmarks across language pairs and data scales demonstrate the effectiveness and universality of our approach. Further analyses show that ngram noaxe indeed improves the translation of ngram phrases, and produces more fluent translation with a better modeling of sentence structure.",
        "author": "Cunxiao Du; Zhaopeng Tu; Longyue Wang; Jing Jiang",
        "authorids": "/c/cunxiao-du/; /z/zhaopeng-tu/; /l/longyue-wang/; /j/jing-jiang/",
        "bibtex": "@inproceedings{du-etal-2022-ngram,\n    title = \"ngram-{OAXE}: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation\",\n    author = \"Du, Cunxiao  and\n      Tu, Zhaopeng  and\n      Wang, Longyue  and\n      Jiang, Jing\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.446/\",\n    pages = \"5035--5045\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.446.pdf",
        "site": "https://aclanthology.org/2022.coling-1.446/",
        "pdf_size": 1342766,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11572949102318866095&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Singapore Management University; Tencent AI Lab; Tencent AI Lab; Singapore Management University",
        "aff_domain": "gmail.com;tencent.com;tencent.com;smu.edu.sg",
        "email": "gmail.com;tencent.com;tencent.com;smu.edu.sg",
        "github": "https://github.com/tencent-ailab/machine-translation/COLING22_ngram-OAXE/",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Singapore Management University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.smu.edu.sg;https://ai.tencent.com",
        "aff_unique_abbr": "SMU;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2022.coling-1.248",
        "title": "uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "The task of Chinese Spelling Check (CSC) is aiming to detect and correct spelling errors that can be found in the text. While manually annotating a high-quality dataset is expensive and time-consuming, thus the scale of the training dataset is usually very small (e.g., SIGHAN15 only contains 2339 samples for training), therefore supervised-learning based models usually suffer the data sparsity limitation and over-fitting issue, especially in the era of big language models. In this paper, we are dedicated to investigating the unsupervised paradigm to address the CSC problem and we propose a framework named uChecker to conduct unsupervised spelling error detection and correction. Masked pretrained language models such as BERT are introduced as the backbone model considering their powerful language diagnosis capability. Benefiting from the various and flexible MASKing operations, we propose a Confusionset-guided masking strategy to fine-train the masked language model to further improve the performance of unsupervised detection and correction. Experimental results on standard datasets demonstrate the effectiveness of our proposed model uChecker in terms of character-level and sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling error detection and correction respectively.",
        "author": "Piji Li",
        "authorids": "/p/piji-li/",
        "bibtex": "@inproceedings{li-2022-uchecker,\n    title = \"u{C}hecker: Masked Pretrained Language Models as Unsupervised {C}hinese Spelling Checkers\",\n    author = \"Li, Piji\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.248/\",\n    pages = \"2812--2822\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.248.pdf",
        "site": "https://aclanthology.org/2022.coling-1.248/",
        "pdf_size": 531106,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7983038176259350888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "gmail.com;tencent.com;tencent.com;smu.edu.sg",
        "email": "gmail.com;tencent.com;tencent.com;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2022.coling-1.27",
        "title": "\u201cMama Always Had a Way of Explaining Things So I Could Understand\u201d: A Dialogue Corpus for Learning to Construct Explanations",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "As AI is more and more pervasive in everyday life, humans have an increasing demand to understand its behavior and decisions. Most research on explainable AI builds on the premise that there is one ideal explanation to be found. In fact, however, everyday explanations are co-constructed in a dialogue between the person explaining (the explainer) and the specific person being explained to (the explainee). In this paper, we introduce a first corpus of dialogical explanations to enable NLP research on how humans explain as well as on how AI can learn to imitate this process. The corpus consists of 65 transcribed English dialogues from the Wired video series 5 Levels, explaining 13 topics to five explainees of different proficiency. All 1550 dialogue turns have been manually labeled by five independent professionals for the topic discussed as well as for the dialogue act and the explanation move performed. We analyze linguistic patterns of explainers and explainees, and we explore differences across proficiency levels. BERT-based baseline results indicate that sequence information helps predicting topics, acts, and moves effectively.",
        "author": "Henning Wachsmuth; Milad Alshomary",
        "authorids": "/h/henning-wachsmuth/; /m/milad-alshomary/",
        "bibtex": "@inproceedings{wachsmuth-alshomary-2022-mama,\n    title = \"{\\textquotedblleft}Mama Always Had a Way of Explaining Things So {I} Could Understand{\\textquotedblright}: A Dialogue Corpus for Learning to Construct Explanations\",\n    author = \"Wachsmuth, Henning  and\n      Alshomary, Milad\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.27/\",\n    pages = \"344--354\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.27.pdf",
        "site": "https://aclanthology.org/2022.coling-1.27/",
        "pdf_size": 940156,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5013511832700399093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Paderborn University; Department of Computer Science, Paderborn University",
        "aff_domain": "upb.de;upb.de",
        "email": "upb.de;upb.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Paderborn University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uni-paderborn.de",
        "aff_unique_abbr": "UPB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paderborn",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.coling-1.72",
        "title": "\u201cNo, They Did Not\u201d: Dialogue Response Dynamics in Pre-trained Language Models",
        "track": "main",
        "status": "Main",
        "award": false,
        "abstract": "A critical component of competence in language is being able to identify relevant components of an utterance and reply appropriately. In this paper we examine the extent of such dialogue response sensitivity in pre-trained language models, conducting a series of experiments with a particular focus on sensitivity to dynamics involving phenomena of at-issueness and ellipsis. We find that models show clear sensitivity to a distinctive role of embedded clauses, and a general preference for responses that target main clause content of prior utterances. However, the results indicate mixed and generally weak trends with respect to capturing the full range of dynamics involved in targeting at-issue versus not-at-issue content. Additionally, models show fundamental limitations in grasp of the dynamics governing ellipsis, and response selections show clear interference from superficial factors that outweigh the influence of principled discourse constraints.",
        "author": "Sanghee J. Kim; Lang Yu; Allyson Ettinger",
        "authorids": "/s/sanghee-j-kim/; /l/lang-yu/; /a/allyson-ettinger/",
        "bibtex": "@inproceedings{kim-etal-2022-dialogue,\n    title = \"{\\textquotedblleft}No, They Did Not{\\textquotedblright}: Dialogue Response Dynamics in Pre-trained Language Models\",\n    author = \"Kim, Sanghee J.  and\n      Yu, Lang  and\n      Ettinger, Allyson\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.72/\",\n    pages = \"863--874\"\n}",
        "pdf": "https://aclanthology.org/2022.coling-1.72.pdf",
        "site": "https://aclanthology.org/2022.coling-1.72/",
        "pdf_size": 374172,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9009555171516930425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Linguistics, University of Chicago; Meta; Department of Linguistics, University of Chicago",
        "aff_domain": "uchicago.edu;fb.com;uchicago.edu",
        "email": "uchicago.edu;fb.com;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Chicago;Meta Platforms, Inc.",
        "aff_unique_dep": "Department of Linguistics;",
        "aff_unique_url": "https://www.chicago.edu;https://meta.com",
        "aff_unique_abbr": "UChicago;Meta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    }
]